Commit Message,predict
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
def install(package):,0
"subprocess.check_call([sys.executable, ""-m"", ""pip"", ""install"", package])",0
"install(""tensorflow==1.13.1"")",0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
quicker variant,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Suppress tensorflow warning messages,0
"Train, evaluate & predict functions / all require TF",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Updating config file:,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path = auxfun_models.check_for_weights(pose_cfg['net_type'], parent_path)",0
Updating training and test pose_cfg:,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Read the config file,0
deal with user passing a single video to add,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
common parameters:,0
Write dictionary to yaml  config file,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
looking for the pseudo label path,0
spatial pyramid is not for adapted model,0
spatial pyramid can still be useful for reducing jittering and quantization error,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
ratio_h and ratio_w should match though in reality it does not match exactly,0
only keep the max,0
empty prediction if pred is not a dict,0
Compute cosine similarity,0
"len(frames) -> (n_scale,)",0
"frames[0].shape - > (batchsize, h, w, 3)",0
no crop needed,0
"batch full, start true inferencing",0
only do this when animal is detected,0
in case we reach the end of the video,0
add a temp folder for checkpoint,0
spatial pyramid can still be useful for reducing jittering and quantization error,0
extra data,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"TODO(stes) mocking a few modules to rely in fewer dependencies, without",1
causing import errors when using deeplabcut.,0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in.,0
TODO(jeylau) add affinity.setter to Assembly,1
TODO(stes): remove/rewrite,1
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"ignore the exception and continue with the next evaluation, without",0
yielding a result value.,0
"return the result value, with NaN as the result for all metrics that",0
could not be computed due to the error.,0
raise the error and stop evaluation,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Read the config file and related variables,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Flatten the list of bodyparts to connect,0
Format data,0
Trick to force equal aspect ratio of 3D plots,0
Set up the matplotlib figure beforehand,0
Set up skeleton LineCollections,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
Get track_method and do related checks,0
Get track method suffix,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
"## Assign nan to [X,Y] of low likelihood predictions ###",0
Convert the data to a np array to easily mask out the low likelihood predictions,0
"Assign [X,Y] = nan to low likelihood predictions",0
Reshape data back to original shape,0
put data back to the dataframes,0
Check individuals are the same in both views,0
Cross-view match individuals,0
Create a dummy variables for single-animal,0
Cleaner variable (since inds view1 == inds view2),0
"Reshape: (num_framex, num_individuals, num_bodyparts , 2)",0
Triangulate data,0
i is individual in view 1,0
voting[i] is the matched individual in view 2,0
Create 3D DataFrame column and row indices,0
Swap num_animals with num_frames axes to ensure well-behaving reshape,0
Fill up 3D dataframe,0
Reorder 2D dataframe in view 2 to match order of view 1,0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no intermediate saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Alternatively,0
Sort tracklets by length to prioritize greater continuity,0
Drop tracklets that are too short,0
"Where slots are available, copy the data over",0
"If about to overwrite data, keep tracklets with highest confidence",0
Squeeze some data into empty slots,0
"For the remaining data, overwrite where we are least confident",0
Find keypoints closest to the remaining data,0
Use Manhattan distance to avoid overflow,0
Only overwrite if improving confidence,0
Fill existing gaps,0
Retrieve original individual label indices,0
Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.,0
Fill existing gaps,0
Retrieve original individual label indices,0
ID swaps occur when X and Y simultaneously intersect each other.,0
Get only those bodypart pairs that belong to different individuals,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"See Li et al., 2012. Cross-view Activity Recognition using Hankelets.",0
"As proposed in the paper, the Hankel matrix can either be formed from",0
the tracklet's centroid or its normalized velocity.,0
"vel = np.diff(self.centroid, axis=0)",0
"vel /= np.linalg.norm(vel, axis=1, keepdims=True)",0
return self.hankelize(vel),0
TODO Add missing data imputation,1
"nrows, ncols = mat.shape",0
beta = nrows / ncols,0
omega = 0.56 * beta ** 3 - 0.95 * beta ** 2 + 1.82 * beta + 1.43,0
return np.argmin(s > omega * np.median(s)),0
"Note that if tracklets are very short, some may actually be part of the same track",0
and thus incorrectly reflect separate track endpoints...,0
"Map each Tracklet to an entry and output nodes and vice versa,",0
which is convenient once the tracklets are stitched.,0
Store tracklets and corresponding negatives (those that overlap in time),0
TODO Avoid looping over all pairs of tracklets,0
Pick the closest (spatially) overlapping tracklet,0
The algorithm works better with integer weights,0
Let us prune the graph by removing all source and sink edges,0
but those connecting the `n_tracks` first and last tracklets.,0
Preflow push seems to work slightly better than shortest,0
"augmentation path..., and is more computationally efficient.",0
Verify whether there are overlapping tracklets,0
"Pick the segment that minimizes ""smoothness"", computed here",0
with the coefficient of variation of the differences.,0
Rebuild a full graph from the remaining nodes without,0
temporal constraint on what tracklets can be stitched together.,0
Cycle through the residuals and incorporate back those,0
that only fit in a single tracklet.,0
Greedily add the remaining residuals,0
Refresh temporal bounds,0
Guarantee track data are sorted in the order defined in the config,0
"If there isn't a track for each animal, fill in the dataframe with NaNs",0
Default to the distance cost function,0
should only exist one,0
Add in identity weighing before building the graph,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
offset if the data was cropped,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implementation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
average distance and average # significant differences avg. over comparisonbodyparts,0
make sure coords and videos are a list,0
can we make a catch here? - in fact we should drop indices from DataCombined,0
if they are in CollectedData.. [ideal behavior; currently pretty unlikely],0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
###################################################,0
Dependencies,0
###################################################,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as,0
specified in the config file,0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
Prepare figure,0
parse the alpha selection function,0
Mask rather than drop unwanted bodyparts to ensure consistent coloring,0
cropping!,0
nx=X2-X1,0
!/usr/bin/env python3,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"more videos are in principle covered, as OpenCV is used and allows many formats.",0
The above already raises a ValueError if formatting is wrong,0
"Rotate, see: https://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg",0
interesting option to just update metadata.,0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
Read the video until a frame is successfully read,0
import platform,0
"if platform.system() == ""Darwin"":  # for OSX use WXAgg",0
fig.canvas.start_event_loop(timeout=-1),0
else:,0
#fig.canvas.stop_event_loop(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"Windows throws error if file path is > 260 characters, can fix with prefix.",0
See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
uses openCV,0
cap.release() >> still used in frame_extraction!,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
This dictionary maps the model types to the file locations where the models exist.,0
Exit the function early if an unknown modeltype is provided.,0
"The path is right, hence the weights are missing; we'll download them again.",0
Aliases for backwards-compatibility,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluding the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
Initialize costs matrix,0
get common bodypart detections in track pair,0
add 3rd dim to the points,0
cost for any point in time of t1 being the same,0
any point in time of t2,0
Get average cost of the entire track,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
check if it exists:,0
(average_degree / 2) as many edges as there are nodes is required,0
for undirected graphs to reach the target degree.,0
"Attention this order has to be consistent (for training set creation, training, inference etc.)",0
"-> adding (single,bpt) for uniquebodyparts",0
"-> adding (individual,bpt) for multianimalbodyparts",0
load defaults,0
set project specific parameters:,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
###################################################,0
Dependencies,0
###################################################,0
Pose X vs pose Y,0
Poses vs time,0
Likelihoods,0
Histograms,0
#################################################,0
Looping analysis over video,0
#################################################,0
"In a multi animal scenario, show more verbose errors.",0
Some vidoes were not evaluated.,0
Keep only the individuals and bodyparts that were labeled,0
Either display the animals defined in the config if they are found,0
"in the dataframe, or all the trajectories regardless of their names",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Determine whether the data are single- or multi-animal without loading into memory,0
simply by checking whether 'individuals' is in the second line of the CSV.,0
Load the config file,0
Get the Project path,0
Get the bodyparts,0
Iterate over each labeled data video,0
Use tqdm for a progress bar,0
discard the file extension,0
Load the csv file,0
get the scorer,0
Change the scorer in the dataframe,0
Get the individuals,0
Get the old bodyparts,0
Get the unmber of old bodyparts,0
Bodyparts to add,0
"If a bodypart is missing, add it to the dataframe",0
"create the columns for the bodypart, concatenate, the individual, the bodypart, and nan values",0
Insert the columns in the dataframe,0
"If the old bodyparts are not in the new project, remove them",0
Save the dataframe,0
Create/Update the h5 file,0
Make paths platform-agnostic if they are not already,0
Ensure folder names are strings,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
make list of full paths,0
filter list of videos,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
sort in ascending order of iteration number,0
"if ""MULTI!"" in allbpts:",0
Ensure same order as in config.yaml,0
ABBREVIATE NETWORK NAMES -- esp. for mobilenet!,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
Iterate over data files and stop as soon as one matching the scorer is found,0
If there was no match...,0
aliases for backwards-compatibility.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Set up a logger and add an stdout handler.,0
A single logger can have many handlers:,0
https://docs.python.org/3/howto/logging.html#handler-basic,0
TODO Dump to log file instead,1
"logger = logging.getLogger(""GUI"")",0
logger.setLevel(logging.DEBUG),0
handler = logging.StreamHandler(stream=sys.stdout),0
handler.setLevel(logging.DEBUG),0
formatter = logging.Formatter(,0
"""%(asctime)s - %(name)s - %(levelname)s - %(message)s"", ""%Y-%m-%d %H:%M:%S""",0
),0
handler.setFormatter(formatter),0
logger.addHandler(handler),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
restore the background region,0
Take into consideration imprecise OpenCV estimation of total number of frames,0
"Create dropdowns for selecting tracklets to swap, placing them near the swap button",0
Get tracklet indices for each individual,0
Frames to swap,0
Swap the tracklets,0
check that the input is a valid from the list of individuals,0
check that the input is a valid from the list of individuals,0
Automatically disable the draggable points,0
Save additional frames to the labeled-data directory,0
Store the newly-refined data,0
Uncertain keypoints are ignored,0
Merge with the already existing annotated data,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Thread-safe Stdout redirector,0
Creating action using the first constructor,0
Creating actions using the second constructor,0
File menu,0
View menu,0
Help menu,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
all_bodyparts: List,0
NOTE: Is there a case where a specific list should,0
have bodyparts other than the root? I don't think so.,0
Videotype selection,0
Select videos,0
Number of selected videos text,0
Clear video selection,0
Create a filter string with both lowercase and uppercase extensions,0
"Qt returns a tuple (list of files, filetype)",0
Add tab header,0
Add separating line,0
Look for any extension by default,0
This works both with e.g. .avi and avi,0
Choose multiple files by default,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Automatically activate the napari-deeplabcut plugin,0
TODO Insert new video,1
TODO Insert skeleton link,1
"Hack to make the first column read-only, as we do not want users to touch it.",1
"The cleaner solution would be to use a QTreeView and QAbstractItemModel,",0
but that is a lot of rework for little benefits.,0
Leave untouched when it is already a string,0
"Slashes also raise the error, but no need to print anything since it is then likely to be a path",0
Walk backwards across parents to get all keys,0
Read the video until a frame is successfully read,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
TODO: Multi video select.... have to change to single video!,1
Shuffle,0
Num animals,0
Min swap length,0
Max gap to fill,0
Trail length,0
Filter type,0
Filter window length,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
Tracker Type,0
Num animals,0
Num triplets,0
layout.addWidget(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
Overwrite videos,0
Trail Points,0
Plot all bodyparts,0
Skeleton,0
Filtered data,0
Plot trajectories,0
High quality video,0
Bodypart list,0
Multianimal scenario.,0
Color is based on individual or bodypart.,0
Single animal scenario.,0
Color is based on bodypart.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Extraction method,0
Frame extraction algorithm,0
Frame cropping,0
Cluster step,0
GUI Slider width,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
use the default pose_cfg file for default values,0
Shuffle,0
Display iterations,0
Save iterations,0
Max iterations,0
Max number snapshots to keep,0
layout.addWidget(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Single / Multi animal Only Layouts,0
Dynamic bodypart cropping,0
Save results as csv,0
Filter predictions,0
Plot Trajectories,0
Show trajectory plots,0
Shuffle,0
layout.addLayout(tmp_layout),0
tmp_layout = QtWidgets.QGridLayout(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
Augmentation method,0
Neural Network,0
TODO: finish model_comparison,1
Check that training data files were indeed created.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Add config text field and button,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
layout.addWidget(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Display all images,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"calling create_tracking_dataset, train_tracking_transformer, stitch_tracklets",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
only try to find these features if they are in the dictionary,0
1) reference to video folder and get the proper bpt_feature file for feature table,0
2) get either the path to gt or the path to track pickle,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
tracklets,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"with npy list form videos, split each to train and test",0
"video_name = '.'.join(video.split(""/"")[-1].split(""."")[:-1])",0
assuming there is only one match,0
make my own model factory,0
make my own loss factory,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights",0
"NOTE: drop path for stochastic depth, we shall see if this is better than dropout here",0
only channel pooling,0
"x -> (B, 12, 2048)",0
"x -> (B, 12, 768)",0
trying to keep everything simple,0
need to write my own keypoint embedding,0
we don't need a fc layer here,0
x: inputs,0
"(B, 12, 768)",0
added pos embed,0
remove all parts I don't understand,0
forwarding through blocks. But num of blocks should be reduced,0
i can't really tell what the difference is,0
will have to change some of these,0
For old models that I trained prior to conv based patchification,0
To resize pos embedding when using model at different size from pretrained weights,0
Rescale the grid of position embeddings when loading from state_dict. Adapted from,0
https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224,0
Cut & paste from PyTorch official master until it's in a few official releases - RW,0
Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf,0
Computes standard normal cumulative distribution function,0
Values are generated by using a truncated uniform distribution and,0
then using the inverse CDF for the normal distribution.,0
Get upper and lower cdf values,0
"Uniformly fill tensor with values from [l, u], then translate to",0
"[2l-1, 2u-1].",0
Use inverse cdf transform for normal distribution to get truncated,0
standard normal,0
"Transform to proper mean, std",0
Clamp to ensure it's in the proper range,0
"type: (Tensor, float, float, float, float) -> Tensor",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2019 Ross Wightman,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Hacked together by / Copyright 2020 Ross Wightman,1
https://github.com/rwightman/pytorch-image-models/blob/main/timm/scheduler/scheduler_factory.py,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"resample if noise out of percent limit, brute force but shouldn't spin much",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
distmat g,0
q    1 3 2 4,0
4 1 2 3,0
"0, 2, 1, 3",0
"1, 2, 3, 0",0
compute cmc curve for each query,0
get query pid and camid,0
remove gallery samples that have the same pid and camid with query,0
compute cmc curve,0
"binary vector, positions with value 1 are correct matches",0
this condition is true when query identity does not appear in gallery,0
compute average precision,0
reference: https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Average_precision,0
"tmp_cmc = [x / (i + 1.) for i, x in enumerate(tmp_cmc)]",0
query,0
gallery,0
"distmat = re_ranking(qf, gf, k1=20, k2=6, lambda_value=0.3)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"if feature vector is numpy, you should use 'torch.tensor' transform it to tensor",0
print('using GPU to compute original distance'),0
print('starting re_ranking'),0
k-reciprocal neighbors,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
More elegantly one can simply define:,0
take care of difference between feature map space and original image space,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
cos = torch.cdist,0
TODO: maybe find a better spot for this.,1
"dev =  device if torch.cuda.is_available() else ""cpu""",0
train,0
print (f'validation loss {val_loss/len(val_loader)}'),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
normalize vectors here,0
i th vector at j th kpt,0
maybe needs to convert them to embeddings and position token,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Drop missing body parts,0
Drop points lying outside the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
## dlcnet_ms5: backbone resnet50 + multi-fusion & multi-stage module,0
## dlcr101_ms5/dlcr152_ms5: backbone resnet101/152 + multi-fusion & multi-stage module,0
"If the graph is unnecessarily large (with 15+ keypoints by default),",0
we randomly prune it to a size guaranteeing an average node degree of 6;,0
"see Suppl. Fig S9c in Lauer et al., 2022.",0
Use the skeleton defined in the config file,0
Ignore possible connections between 'multi' and 'unique' body parts;,0
one can never be too careful...,0
Disable the prediction of PAFs if the graph is empty,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving metadata and data file (Pickle file),0
###############################################################################,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Setting inference cfg file:,0
Search a pose_config.yaml file to parse missing information,0
Clean the training-datasets folder prior to recreating the data pickles,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Load updated lists:,0
Find the corresponding video file,0
"print(key, ""dropping?"")",0
adding important values for multianiaml project:,0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Determine the index length required to guarantee,0
the train–test ratio is exactly the desired one.,0
Pad indices so lengths agree,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(name,test_video_name)",0
Blazing fast and does not load the image into memory,0
Check that points lie within the image,0
DeprecationWarnings are silenced since Python 3.2 unless triggered in __main__,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
updating variable if null/None! #backwardscompatability,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
these values are dropped as scalecrop,0
doesn't have rotation implemented,0
Also drop maDLC smart cropping augmentation parameters,0
sort the model directories,0
get the shuffle index and offset by 1.,0
read cfg file,0
create log file,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Check for variable correctness,0
Moviepy:,0
select crop method,0
"Find the matching video from the config `video_sets`,",0
as it may be stored elsewhere than in the `videos` directory.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
allow_growth must be true here because tensorflow does not automatically free gpu memory and setting it as false occupies all gpu memory so that pytorch cannot kick in,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
should close tensorflow session here in order to free gpu,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"Flip x, y, confidence and reshape",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html#videocapture-get,0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
TODO: add cropping as in video analysis!,1
if cropping is not None:,0
cfg['cropping']=True,0
"cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping",0
"print(""Overwriting cropping parameters:"", cropping)",0
"print(""These are used for all videos, but won't be save to the cfg file."")",0
"Also ensure `boundingboxslack` is greater than zero, otherwise overlap",0
"between trackers cannot be evaluated, resulting in empty tracklets.",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
NOTE: If dataname line above is changed then line below is obsolete?,0
"trackname = trackname.replace(videofolder, destfolder)",0
TODO: adjust this for multi + unique bodyparts!,1
this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. ),0
Optimal identity assignment based on soft voting,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
imagenet mean for resnet pretraining:,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/DeepLabCut/DeepLabCut/pull/324",0
for DLC 2.2. (here all set False to not use PAFs/pairwise fields),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
"reloading defaults, as they can bleed over from a previous run otherwise",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
dlc_cfg_train = load_config(str(path_train_config)),0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
########################## RESCALING (to global scale),0
Name for deeplabcut net (based on its parameters),0
"DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])",0
"print(""Extracting maps for "", DLCscorer, "" with # of trainingiterations:"", trainingsiterations)",0
if notanalyzed: #this only applies to ask if h5 exists...,0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"for multi animal, seems only this is used",0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Get animal IDs and corresponding indices in the arrays of detections,0
Form ground truth beforehand,0
Assemble animals on the full set of detections,0
get the indices of the images in the training set,0
Count the number of unassembled bodyparts,0
Handle unlabeled bodyparts...,0
"Only 1 animal, let us return the full graph indices only",0
Find minimal skeleton,0
Select optimal PAF graph,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Selective copy; deepcopy is >5x slower,0
Only keeps skeletons that are more than 90% complete,0
TODO Normalize dists by longest length?,1
TODO Smarter imputation technique (Bayesian? Grassmann averages?),1
Covariance matrix estimation fails due to numerical singularities,0
Distance is undefined if the assembly is empty,0
Correct distance to account for missing observations,0
"Alternatively, reduce contribution of missing values to the Mahalanobis",0
distance to zero by substituting the corresponding means.,0
"Fill the subsets with unambiguous, complete individuals",0
Fuse superfluous assemblies,0
Second pass without edge safety,0
Mark the unique body parts as assembled anyway so,0
they are not used later on to fill assemblies.,0
Store selected edges for subsequent frames,0
Remove invalid assemblies,0
Fill assemblies with unconnected body parts,0
Spawning (rather than forking) multiple processes does not,0
work nicely with the GUI or interactive sessions.,0
"In that case, we fall back to the serial assembly.",0
Swap corresponding keypoints,0
Only consider assemblies of at least two keypoints,0
Greedy assembly matching like in pycocotools,0
Global rather than greedy assembly matching,0
Cast columns of dtype 'object' to float to avoid TypeError,0
further down in _parse_ground_truth_data.,0
sigma is taken as the median of all COCO keypoint standard deviations,0
Sort matching score (OKS) in descending order of assembly affinity,0
Guarantee precision decreases monotonically,0
See https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Regularize by forcing AR <= 5,0
max_ar = 5,0
if el.aspect_ratio >= max_ar:,0
if el.height > el.width:,0
el.width = el.height / max_ar,0
else:,0
el.height = el.width / max_ar,0
Orient the ellipse such that it encompasses most points,0
"n_inside = el.contains_points(np.c_[self.x, self.y]).sum()",0
el.theta += 0.5 * np.pi,0
"if el.contains_points(np.c_[self.x, self.y]).sum() < n_inside:",0
el.theta -= 0.5 * np.pi,0
"r2 = chi2.ppf(2 * norm.cdf(sd) - 1, 2)",0
"height, width = np.sqrt(E * r2)",0
The general quadratic curve has the form:,0
ax^2 + 2bxy + cy^2 + 2dx + 2fy + g = 0,0
Ellipse center coordinates,0
Semi-axes lengths,0
Angle to the horizontal,0
High uncertainty to the unobservable initial velocities,0
Give high uncertainty to the unobservable initial velocities,0
diff = val - cost_matrix,0
"diff[row, col] += val",0
if (,0
val < self.iou_threshold,0
or np.any(diff[row] <= 0.2),0
"or np.any(diff[:, col] <= 0.2)",0
):,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
Modified from scipy source code:,0
- to restrict its use to 2D,0
- to get rid of shuffling (since arrays are only (nbodyparts * 3) element long),0
TODO - factor in keypoint confidence (and weight by # of observations??),1
"mat = self.calc_pairwise_oks(poses, poses_ref)",0
Remove matched detections with low OKS,0
matches = [],0
"for row, col in zip(row_indices, col_indices):",0
"if mat[row, col] < self.oks_threshold:",0
unmatched_poses.append(row),0
unmatched_trackers.append(col),0
else:,0
"matches.append([row, col])",0
if not len(matches):,0
"matches = np.empty((0, 2), dtype=int)",0
else:,0
matches = np.stack(matches),0
update matched trackers with assigned detections,0
create and initialise new trackers for unmatched detections,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
filter out matched with low IOU,0
TODO: get cropping parameters and utilize!,1
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Load meta data & annotations,0
get the graph!,0
"NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*",0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
name for deeplabcut net (based on its parameters),0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
"MA projects have column indices ""scorer"", ""individuals"" and ""bodyparts""",0
"Drop the scorer level, and put individuals in rows",0
The error rows are series; stack in axis 1 and pivot to get DF,0
TODO: Make this code not so redundant!,1
"If a string was passed in, auto-convert to True for backward compatibility",0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
TODO: Unlike using create_training_dataset() If create_training_model_comparison() is used there won't,1
necessarily be training fractions for every shuffle which will raise the FileNotFoundError..,0
Not sure if this should throw an exception or just be a warning...,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to initial folder,0
"# Also storing one ""large"" table with results:",0
note: evaluationfolder.parents[0] to get common folder above all shuffle evaluations.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
#################################################,0
Load data...,0
#################################################,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
TODO: Unlike using create_training_dataset() If create_training_model_comparison() is used there won't,1
necessarily be training fractions for every shuffle which will raise the FileNotFoundError..,0
Not sure if this should throw an exception or just be a warning...,0
Load meta data,0
TODO: IMPLEMENT for different batch sizes?,1
Ignore best edges possibly defined during a prior evaluation,0
Create folder structure to store results.,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
Pass the image and the keypoints through the resizer;,0
this has no effect if no augmenters were added to it.,0
FIXME Is having an empty array vs nan really that necessary?!,1
"Form 2D array of shape (n_rows, 4) where the last dim is",0
"(sample_index, peak_y, peak_x, bpt_index) to slice the PAFs.",0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in,0
Compute all distance statistics,0
Calculate overall prediction error,0
Skip data-driven skeleton selection unless,0
the model was trained on the full graph.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
in case there was already a graph,0
have to overwrite this,0
removing the decoding layer from the checkpoint,0
"loading backbone from ResNet, MobileNet etc.",0
"display_iters = max(1,int(displayiters))",0
"[_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})",0
Save snapshot,0
return to original path.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Clip peak locations to PAFs dimensions,0
"unit_vecs = vecs / lengths[:, np.newaxis]",0
"affinities = np.squeeze(y @ np.expand_dims(unit_vecs, axis=2)).sum(axis=1)",0
Form cost matrices,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"loading backbone from ResNet, MobileNet etc.",0
"Auto-switch to Adam on M1/M2 chips, as the momentum optimizer crashes",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Convert a frozen graph to OpenVINO IR format,0
Read network into memory,0
"For better efficiency, model is initialized for batch_size 1 and every sample processed independently",0
Load network to device,0
Prepare input data,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Change the stride from 2 to 1 to get 16x downscaling instead of 32x.,0
https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142,0
Multi stage is currently only implemented for resnets,0
Defining multi_fusion backbone,0
Attaching multi-stage decoder,0
stage_paf_output = stage_paf_output + pre_stage_paf_output,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
FIXME Fix wrong scope with Keras layers,1
"def prediction_layer(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2DTranspose(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=2,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
## New DLCNet Addition: multi-stage decoder,0
"def prediction_layer_stage(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2D(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=1,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Compute variance using: Var[X]= E[X^2] - E[X]^2.,0
Compute keep_prob,0
TODO(tanmingxing): add support for training progress.,1
Compute drop_connect tensor,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
turn into x times y time bs * bpts,0
extract corresponding locref x and y as well as probability,0
Filter predicted heatmaps with a 2D Gaussian kernel as in:,0
https://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_The_Devil_Is_in_the_Details_Delving_Into_Unbiased_Data_CVPR_2020_paper.pdf,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
defaults will be a public argument for namedtuple in Python 3.7,0
https://docs.python.org/3/library/collections.html#collections.namedtuple,0
Make sure that round down does not go down by more than 10%.,0
Builds the block accordings to arguments.,0
Expansion phase:,0
Depth-wise convolution phase:,0
Squeeze and Excitation layer.,0
Output phase:,0
only apply drop_connect if skip presents.,0
Expansion phase:,0
Output phase:,0
only apply drop_connect if skip presents.,0
Builds blocks.,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Stem part.,0
Head part.,0
Calls Stem layers,0
Calls blocks.,0
Calls final layers and returns logits.,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
"(width_coefficient, depth_coefficient, resolution, dropout_rate)",0
blocks_args = [,0
"'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',",0
"'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',",0
"'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',",0
"'r1_k3_s11_e6_i192_o320_se0.25',",0
],0
The default is TPU-specific batch norm.,0
The alternative is tf.layers.BatchNormalization.,0
"batch_norm=utils.TpuBatchNormalization,  # TPU-specific requirement.",0
ValueError will be raised here if override_params has fields not included,0
in global_params.,0
,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrunk when given a depth-multiplier < 1.0.,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Clamp to 40% of crop size to ensure that at least,0
the center keypoint remains visible after the offset is applied.,0
Points located close to one another are sampled preferentially,0
in order to augment crowded regions.,0
Include keypoints in the count to avoid null probabilities,0
Shift the crop center in both dimensions by random amounts,0
and normalize to the original image dimensions.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"for topview, it's safe to mask keypoints under threshold",0
Pickle the 'data' dictionary using the highest protocol available.,0
item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal,0
3 for likelihood,0
we only do single animal here,0
let's hard code it,0
"Add smart, keypoint-aware image cropping",0
"because of the existence of threshold, sampling population is adjusted to len(self.data)",0
empty prediction for this frame,0
in case it's empty prediction,0
Scale is sampled only once (per batch) to transform all of the images into same size.,0
Discard keypoints whose coordinates lie outside the cropped image,0
Invisible joints are represented by nans,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
Produce score maps and location refinement fields,0
Find indices of individuals in joint_id,0
WIP!,0
STD of gaussian is 1/4 of threshold,0
Grid of coordinates,0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Distance between the joint point and each coordinate,0
NEEDS TO BE DONE!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
TODO Finish implementing actual abstract class,1
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
parameter initialization for augmentation pipeline:,0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images of a batch into same size.,0
Approximating the scale,0
invisible joints are represented by nans,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"First, initializing variables (if they don't exist)",0
what is the fraction of training samples with scaling augmentation?,0
loading defaults for rotation range!,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"cfg[""rotateratio""] = cfg.get(",0
"""rotratio"", 0.4",0
)  # what is the fraction of training samples with rotation augmentation?,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"Auto cropping is new (was not in Nature Neuroscience 2018 paper, but introduced in Nath et al. Nat. Protocols 2019)",0
"and boosts performance by 2X, particularly on challenging datasets, like the cheetah in Nath et al.",0
Parameters for augmentation with regard to cropping:,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Get rid of the interpolation beyond the spline knots,0
Data has been filtered so continue to the next video,0
Data haven't been filtered yet,0
Retrieve original individual label indices,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Check the training image paths are correctly stored as arrays of strings,0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
TENSORPACK could fail in WINDOWS...,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
Sometimes tracks cannot be reconstructed as test data are randomly,0
"created; when this happens, we generate a fake h5 data file.",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Loading example data set,0
# Create one identical splits for 3 networks and 3 augmentations,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
Setting specific parameters for training,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"MODELS = [""dlcrnet_ms5"", ""dlcr101_ms5"", ""efficientnet-b0"", ""mobilenet_v2_0.35""]",0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
## adding it here,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Deleting unnecessary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Loading example data set,0
example how to set pose config variables:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
test if existing models are found:,0
Test if nonexisting models are not found,0
Add some other office files:,0
Add a .pickle and .h5 files,0
"By default, all videos with common extensions are taken from a directory",0
A list of extensions can also be passed in,0
"labeled_folders: (has_H5, H5_st_mtime, folder_name)",0
mock files,0
mock files,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Test bounding box from a single keypoint,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Test that shape was preserved after vectorization,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Guarantee that images smaller than crop size are handled fine,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Disable stochastic scale jitter,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Verify the aspect ratio is preserved,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Load sample multi animal data,0
Generate a random permutation and reorder data. Ignore the unique bodypart,0
Get inverse permutation and reorder the modified data to get back,0
to the original,0
Check,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"Check data comprise path, shape, and xy coordinates",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Check that we get the number of frames we asked for,0
Check that all indices are valid,0
Check that all frames are unique,0
Check that we get the number of frames we asked for,0
Check that all indices are valid,0
Check that all frames are unique,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Break the graph to test stitching failure,0
Add fake IDs,0
Split all tracklets in half,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"individual_1, individual2",0
"leftArm, rightArm, leftArm, rightArm",0
positive int,0
negative int,0
all snapshots,0
positive int,0
negative int,0
invalid str,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Test flipped keypoints,0
Original (cached) coordinates must have remained empty,0
Assemble based on the smallest graph to speed up testing,0
Generate fake identity predictions,0
Assemble based on the smallest graph to speed up testing,0
Test now with identity only and ensure assemblies,0
contain only parts of a single group ID.,0
Test empty assembly,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Verify that the Hugging Face folder was removed,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
def install(package):,0
"subprocess.check_call([sys.executable, ""-m"", ""pip"", ""install"", package])",0
"install(""tensorflow==1.13.1"")",0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
quicker variant,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Suppress tensorflow warning messages,0
"Train, evaluate & predict functions / all require TF",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Updating config file:,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path = auxfun_models.check_for_weights(pose_cfg['net_type'], parent_path)",0
Updating training and test pose_cfg:,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Read the config file,0
deal with user passing a single video to add,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
common parameters:,0
Write dictionary to yaml  config file,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
looking for the pseudo label path,0
spatial pyramid is not for adapted model,0
spatial pyramid can still be useful for reducing jittering and quantization error,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
ratio_h and ratio_w should match though in reality it does not match exactly,0
only keep the max,0
empty prediction if pred is not a dict,0
Compute cosine similarity,0
"len(frames) -> (n_scale,)",0
"frames[0].shape - > (batchsize, h, w, 3)",0
no crop needed,0
"batch full, start true inferencing",0
only do this when animal is detected,0
in case we reach the end of the video,0
add a temp folder for checkpoint,0
spatial pyramid can still be useful for reducing jittering and quantization error,0
extra data,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"TODO(stes) mocking a few modules to rely in fewer dependencies, without",1
causing import errors when using deeplabcut.,0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in.,0
TODO(jeylau) add affinity.setter to Assembly,1
TODO(stes): remove/rewrite,1
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"ignore the exception and continue with the next evaluation, without",0
yielding a result value.,0
"return the result value, with NaN as the result for all metrics that",0
could not be computed due to the error.,0
raise the error and stop evaluation,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Read the config file and related variables,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Flatten the list of bodyparts to connect,0
Format data,0
Trick to force equal aspect ratio of 3D plots,0
Set up the matplotlib figure beforehand,0
Set up skeleton LineCollections,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
Get track_method and do related checks,0
Get track method suffix,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
"## Assign nan to [X,Y] of low likelihood predictions ###",0
Convert the data to a np array to easily mask out the low likelihood predictions,0
"Assign [X,Y] = nan to low likelihood predictions",0
Reshape data back to original shape,0
put data back to the dataframes,0
Check individuals are the same in both views,0
Cross-view match individuals,0
Create a dummy variables for single-animal,0
Cleaner variable (since inds view1 == inds view2),0
"Reshape: (num_framex, num_individuals, num_bodyparts , 2)",0
Triangulate data,0
i is individual in view 1,0
voting[i] is the matched individual in view 2,0
Create 3D DataFrame column and row indices,0
Swap num_animals with num_frames axes to ensure well-behaving reshape,0
Fill up 3D dataframe,0
Reorder 2D dataframe in view 2 to match order of view 1,0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no intermediate saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Alternatively,0
Sort tracklets by length to prioritize greater continuity,0
Drop tracklets that are too short,0
"Where slots are available, copy the data over",0
"If about to overwrite data, keep tracklets with highest confidence",0
Squeeze some data into empty slots,0
"For the remaining data, overwrite where we are least confident",0
Find keypoints closest to the remaining data,0
Use Manhattan distance to avoid overflow,0
Only overwrite if improving confidence,0
Fill existing gaps,0
Retrieve original individual label indices,0
Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.,0
Fill existing gaps,0
Retrieve original individual label indices,0
ID swaps occur when X and Y simultaneously intersect each other.,0
Get only those bodypart pairs that belong to different individuals,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"See Li et al., 2012. Cross-view Activity Recognition using Hankelets.",0
"As proposed in the paper, the Hankel matrix can either be formed from",0
the tracklet's centroid or its normalized velocity.,0
"vel = np.diff(self.centroid, axis=0)",0
"vel /= np.linalg.norm(vel, axis=1, keepdims=True)",0
return self.hankelize(vel),0
TODO Add missing data imputation,1
"nrows, ncols = mat.shape",0
beta = nrows / ncols,0
omega = 0.56 * beta ** 3 - 0.95 * beta ** 2 + 1.82 * beta + 1.43,0
return np.argmin(s > omega * np.median(s)),0
"Note that if tracklets are very short, some may actually be part of the same track",0
and thus incorrectly reflect separate track endpoints...,0
"Map each Tracklet to an entry and output nodes and vice versa,",0
which is convenient once the tracklets are stitched.,0
Store tracklets and corresponding negatives (those that overlap in time),0
TODO Avoid looping over all pairs of tracklets,0
Pick the closest (spatially) overlapping tracklet,0
The algorithm works better with integer weights,0
Let us prune the graph by removing all source and sink edges,0
but those connecting the `n_tracks` first and last tracklets.,0
Preflow push seems to work slightly better than shortest,0
"augmentation path..., and is more computationally efficient.",0
Verify whether there are overlapping tracklets,0
"Pick the segment that minimizes ""smoothness"", computed here",0
with the coefficient of variation of the differences.,0
Rebuild a full graph from the remaining nodes without,0
temporal constraint on what tracklets can be stitched together.,0
Cycle through the residuals and incorporate back those,0
that only fit in a single tracklet.,0
Greedily add the remaining residuals,0
Refresh temporal bounds,0
Guarantee track data are sorted in the order defined in the config,0
"If there isn't a track for each animal, fill in the dataframe with NaNs",0
Default to the distance cost function,0
should only exist one,0
Add in identity weighing before building the graph,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
offset if the data was cropped,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implementation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
average distance and average # significant differences avg. over comparisonbodyparts,0
make sure coords and videos are a list,0
can we make a catch here? - in fact we should drop indices from DataCombined,0
if they are in CollectedData.. [ideal behavior; currently pretty unlikely],0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
###################################################,0
Dependencies,0
###################################################,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as,0
specified in the config file,0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
Prepare figure,0
parse the alpha selection function,0
Mask rather than drop unwanted bodyparts to ensure consistent coloring,0
cropping!,0
nx=X2-X1,0
!/usr/bin/env python3,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"more videos are in principle covered, as OpenCV is used and allows many formats.",0
The above already raises a ValueError if formatting is wrong,0
"Rotate, see: https://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg",0
interesting option to just update metadata.,0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
Read the video until a frame is successfully read,0
import platform,0
"if platform.system() == ""Darwin"":  # for OSX use WXAgg",0
fig.canvas.start_event_loop(timeout=-1),0
else:,0
#fig.canvas.stop_event_loop(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"Windows throws error if file path is > 260 characters, can fix with prefix.",0
See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
uses openCV,0
cap.release() >> still used in frame_extraction!,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
This dictionary maps the model types to the file locations where the models exist.,0
Exit the function early if an unknown modeltype is provided.,0
"The path is right, hence the weights are missing; we'll download them again.",0
Aliases for backwards-compatibility,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluding the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
Initialize costs matrix,0
get common bodypart detections in track pair,0
add 3rd dim to the points,0
cost for any point in time of t1 being the same,0
any point in time of t2,0
Get average cost of the entire track,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
check if it exists:,0
(average_degree / 2) as many edges as there are nodes is required,0
for undirected graphs to reach the target degree.,0
"Attention this order has to be consistent (for training set creation, training, inference etc.)",0
"-> adding (single,bpt) for uniquebodyparts",0
"-> adding (individual,bpt) for multianimalbodyparts",0
load defaults,0
set project specific parameters:,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
###################################################,0
Dependencies,0
###################################################,0
Pose X vs pose Y,0
Poses vs time,0
Likelihoods,0
Histograms,0
#################################################,0
Looping analysis over video,0
#################################################,0
"In a multi animal scenario, show more verbose errors.",0
Some vidoes were not evaluated.,0
Keep only the individuals and bodyparts that were labeled,0
Either display the animals defined in the config if they are found,0
"in the dataframe, or all the trajectories regardless of their names",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Determine whether the data are single- or multi-animal without loading into memory,0
simply by checking whether 'individuals' is in the second line of the CSV.,0
Make paths platform-agnostic if they are not already,0
Ensure folder names are strings,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
make list of full paths,0
filter list of videos,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"if ""MULTI!"" in allbpts:",0
Ensure same order as in config.yaml,0
ABBREVIATE NETWORK NAMES -- esp. for mobilenet!,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
Iterate over data files and stop as soon as one matching the scorer is found,0
If there was no match...,0
aliases for backwards-compatibility.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Set up a logger and add an stdout handler.,0
A single logger can have many handlers:,0
https://docs.python.org/3/howto/logging.html#handler-basic,0
TODO Dump to log file instead,1
"logger = logging.getLogger(""GUI"")",0
logger.setLevel(logging.DEBUG),0
handler = logging.StreamHandler(stream=sys.stdout),0
handler.setLevel(logging.DEBUG),0
formatter = logging.Formatter(,0
"""%(asctime)s - %(name)s - %(levelname)s - %(message)s"", ""%Y-%m-%d %H:%M:%S""",0
),0
handler.setFormatter(formatter),0
logger.addHandler(handler),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
restore the background region,0
Take into consideration imprecise OpenCV estimation of total number of frames,0
Automatically disable the draggable points,0
Save additional frames to the labeled-data directory,0
Store the newly-refined data,0
Uncertain keypoints are ignored,0
Merge with the already existing annotated data,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Thread-safe Stdout redirector,0
Creating action using the first constructor,0
Creating actions using the second constructor,0
File menu,0
View menu,0
Help menu,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
all_bodyparts: List,0
NOTE: Is there a case where a specific list should,0
have bodyparts other than the root? I don't think so.,0
Videotype selection,0
Select videos,0
Number of selected videos text,0
Clear video selection,0
"Qt returns a tuple (list of files, filetype)",0
Add tab header,0
Add separating line,0
Look for any extension by default,0
This works both with e.g. .avi and avi,0
Choose multiple files by default,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Automatically activate the napari-deeplabcut plugin,0
TODO Insert new video,1
TODO Insert skeleton link,1
"Hack to make the first column read-only, as we do not want users to touch it.",1
"The cleaner solution would be to use a QTreeView and QAbstractItemModel,",0
but that is a lot of rework for little benefits.,0
Leave untouched when it is already a string,0
"Slashes also raise the error, but no need to print anything since it is then likely to be a path",0
Walk backwards across parents to get all keys,0
Read the video until a frame is successfully read,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
TODO: Multi video select.... have to change to single video!,1
Shuffle,0
Num animals,0
Min swap length,0
Max gap to fill,0
Trail length,0
Filter type,0
Filter window length,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
Tracker Type,0
Num animals,0
Num triplets,0
layout.addWidget(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
Overwrite videos,0
Trail Points,0
Plot all bodyparts,0
Skeleton,0
Filtered data,0
Plot trajectories,0
High quality video,0
Bodypart list,0
Multianimal scenario.,0
Color is based on individual or bodypart.,0
Single animal scenario.,0
Color is based on bodypart.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Extraction method,0
Frame extraction algorithm,0
Frame cropping,0
Cluster step,0
GUI Slider width,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
use the default pose_cfg file for default values,0
Shuffle,0
Display iterations,0
Save iterations,0
Max iterations,0
Max number snapshots to keep,0
layout.addWidget(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Single / Multi animal Only Layouts,0
Dynamic bodypart cropping,0
Save results as csv,0
Filter predictions,0
Plot Trajectories,0
Show trajectory plots,0
Shuffle,0
layout.addLayout(tmp_layout),0
tmp_layout = QtWidgets.QGridLayout(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
Augmentation method,0
Neural Network,0
TODO: finish model_comparison,1
Check that training data files were indeed created.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Add config text field and button,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
layout.addWidget(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Display all images,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"calling create_tracking_dataset, train_tracking_transformer, stitch_tracklets",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
only try to find these features if they are in the dictionary,0
1) reference to video folder and get the proper bpt_feature file for feature table,0
2) get either the path to gt or the path to track pickle,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
tracklets,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"with npy list form videos, split each to train and test",0
"video_name = '.'.join(video.split(""/"")[-1].split(""."")[:-1])",0
assuming there is only one match,0
make my own model factory,0
make my own loss factory,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights",0
"NOTE: drop path for stochastic depth, we shall see if this is better than dropout here",0
only channel pooling,0
"x -> (B, 12, 2048)",0
"x -> (B, 12, 768)",0
trying to keep everything simple,0
need to write my own keypoint embedding,0
we don't need a fc layer here,0
x: inputs,0
"(B, 12, 768)",0
added pos embed,0
remove all parts I don't understand,0
forwarding through blocks. But num of blocks should be reduced,0
i can't really tell what the difference is,0
will have to change some of these,0
For old models that I trained prior to conv based patchification,0
To resize pos embedding when using model at different size from pretrained weights,0
Rescale the grid of position embeddings when loading from state_dict. Adapted from,0
https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224,0
Cut & paste from PyTorch official master until it's in a few official releases - RW,0
Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf,0
Computes standard normal cumulative distribution function,0
Values are generated by using a truncated uniform distribution and,0
then using the inverse CDF for the normal distribution.,0
Get upper and lower cdf values,0
"Uniformly fill tensor with values from [l, u], then translate to",0
"[2l-1, 2u-1].",0
Use inverse cdf transform for normal distribution to get truncated,0
standard normal,0
"Transform to proper mean, std",0
Clamp to ensure it's in the proper range,0
"type: (Tensor, float, float, float, float) -> Tensor",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2019 Ross Wightman,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Hacked together by / Copyright 2020 Ross Wightman,1
https://github.com/rwightman/pytorch-image-models/blob/main/timm/scheduler/scheduler_factory.py,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"resample if noise out of percent limit, brute force but shouldn't spin much",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
distmat g,0
q    1 3 2 4,0
4 1 2 3,0
"0, 2, 1, 3",0
"1, 2, 3, 0",0
compute cmc curve for each query,0
get query pid and camid,0
remove gallery samples that have the same pid and camid with query,0
compute cmc curve,0
"binary vector, positions with value 1 are correct matches",0
this condition is true when query identity does not appear in gallery,0
compute average precision,0
reference: https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Average_precision,0
"tmp_cmc = [x / (i + 1.) for i, x in enumerate(tmp_cmc)]",0
query,0
gallery,0
"distmat = re_ranking(qf, gf, k1=20, k2=6, lambda_value=0.3)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"if feature vector is numpy, you should use 'torch.tensor' transform it to tensor",0
print('using GPU to compute original distance'),0
print('starting re_ranking'),0
k-reciprocal neighbors,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
More elegantly one can simply define:,0
take care of difference between feature map space and original image space,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
cos = torch.cdist,0
TODO: maybe find a better spot for this.,1
"dev =  device if torch.cuda.is_available() else ""cpu""",0
train,0
print (f'validation loss {val_loss/len(val_loader)}'),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
normalize vectors here,0
i th vector at j th kpt,0
maybe needs to convert them to embeddings and position token,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Drop missing body parts,0
Drop points lying outside the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
## dlcnet_ms5: backbone resnet50 + multi-fusion & multi-stage module,0
## dlcr101_ms5/dlcr152_ms5: backbone resnet101/152 + multi-fusion & multi-stage module,0
"If the graph is unnecessarily large (with 15+ keypoints by default),",0
we randomly prune it to a size guaranteeing an average node degree of 6;,0
"see Suppl. Fig S9c in Lauer et al., 2022.",0
Use the skeleton defined in the config file,0
Ignore possible connections between 'multi' and 'unique' body parts;,0
one can never be too careful...,0
Disable the prediction of PAFs if the graph is empty,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving metadata and data file (Pickle file),0
###############################################################################,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Setting inference cfg file:,0
Search a pose_config.yaml file to parse missing information,0
Clean the training-datasets folder prior to recreating the data pickles,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Load updated lists:,0
Find the corresponding video file,0
"print(key, ""dropping?"")",0
adding important values for multianiaml project:,0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Determine the index length required to guarantee,0
the train–test ratio is exactly the desired one.,0
Pad indices so lengths agree,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(name,test_video_name)",0
Blazing fast and does not load the image into memory,0
Check that points lie within the image,0
DeprecationWarnings are silenced since Python 3.2 unless triggered in __main__,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
updating variable if null/None! #backwardscompatability,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
these values are dropped as scalecrop,0
doesn't have rotation implemented,0
Also drop maDLC smart cropping augmentation parameters,0
sort the model directories,0
get the shuffle index and offset by 1.,0
read cfg file,0
create log file,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Check for variable correctness,0
Moviepy:,0
select crop method,0
"Find the matching video from the config `video_sets`,",0
as it may be stored elsewhere than in the `videos` directory.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
allow_growth must be true here because tensorflow does not automatically free gpu memory and setting it as false occupies all gpu memory so that pytorch cannot kick in,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
should close tensorflow session here in order to free gpu,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"Flip x, y, confidence and reshape",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html#videocapture-get,0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
TODO: add cropping as in video analysis!,1
if cropping is not None:,0
cfg['cropping']=True,0
"cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping",0
"print(""Overwriting cropping parameters:"", cropping)",0
"print(""These are used for all videos, but won't be save to the cfg file."")",0
"Also ensure `boundingboxslack` is greater than zero, otherwise overlap",0
"between trackers cannot be evaluated, resulting in empty tracklets.",0
Check which snapshots are available and sort them by # iterations,0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
NOTE: If dataname line above is changed then line below is obsolete?,0
"trackname = trackname.replace(videofolder, destfolder)",0
TODO: adjust this for multi + unique bodyparts!,1
this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. ),0
Optimal identity assignment based on soft voting,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
imagenet mean for resnet pretraining:,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/DeepLabCut/DeepLabCut/pull/324",0
for DLC 2.2. (here all set False to not use PAFs/pairwise fields),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
"reloading defaults, as they can bleed over from a previous run otherwise",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
dlc_cfg_train = load_config(str(path_train_config)),0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
Name for deeplabcut net (based on its parameters),0
"DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])",0
"print(""Extracting maps for "", DLCscorer, "" with # of trainingiterations:"", trainingsiterations)",0
if notanalyzed: #this only applies to ask if h5 exists...,0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"for multi animal, seems only this is used",0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Get animal IDs and corresponding indices in the arrays of detections,0
Form ground truth beforehand,0
Assemble animals on the full set of detections,0
get the indices of the images in the training set,0
Count the number of unassembled bodyparts,0
Handle unlabeled bodyparts...,0
"Only 1 animal, let us return the full graph indices only",0
Find minimal skeleton,0
Select optimal PAF graph,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Selective copy; deepcopy is >5x slower,0
Only keeps skeletons that are more than 90% complete,0
TODO Normalize dists by longest length?,1
TODO Smarter imputation technique (Bayesian? Grassmann averages?),1
Covariance matrix estimation fails due to numerical singularities,0
Distance is undefined if the assembly is empty,0
Correct distance to account for missing observations,0
"Alternatively, reduce contribution of missing values to the Mahalanobis",0
distance to zero by substituting the corresponding means.,0
"Fill the subsets with unambiguous, complete individuals",0
Fuse superfluous assemblies,0
Second pass without edge safety,0
Mark the unique body parts as assembled anyway so,0
they are not used later on to fill assemblies.,0
Store selected edges for subsequent frames,0
Remove invalid assemblies,0
Fill assemblies with unconnected body parts,0
Spawning (rather than forking) multiple processes does not,0
work nicely with the GUI or interactive sessions.,0
"In that case, we fall back to the serial assembly.",0
Swap corresponding keypoints,0
Only consider assemblies of at least two keypoints,0
Greedy assembly matching like in pycocotools,0
Global rather than greedy assembly matching,0
Cast columns of dtype 'object' to float to avoid TypeError,0
further down in _parse_ground_truth_data.,0
sigma is taken as the median of all COCO keypoint standard deviations,0
Sort matching score (OKS) in descending order of assembly affinity,0
Guarantee precision decreases monotonically,0
See https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Regularize by forcing AR <= 5,0
max_ar = 5,0
if el.aspect_ratio >= max_ar:,0
if el.height > el.width:,0
el.width = el.height / max_ar,0
else:,0
el.height = el.width / max_ar,0
Orient the ellipse such that it encompasses most points,0
"n_inside = el.contains_points(np.c_[self.x, self.y]).sum()",0
el.theta += 0.5 * np.pi,0
"if el.contains_points(np.c_[self.x, self.y]).sum() < n_inside:",0
el.theta -= 0.5 * np.pi,0
"r2 = chi2.ppf(2 * norm.cdf(sd) - 1, 2)",0
"height, width = np.sqrt(E * r2)",0
The general quadratic curve has the form:,0
ax^2 + 2bxy + cy^2 + 2dx + 2fy + g = 0,0
Ellipse center coordinates,0
Semi-axes lengths,0
Angle to the horizontal,0
High uncertainty to the unobservable initial velocities,0
Give high uncertainty to the unobservable initial velocities,0
diff = val - cost_matrix,0
"diff[row, col] += val",0
if (,0
val < self.iou_threshold,0
or np.any(diff[row] <= 0.2),0
"or np.any(diff[:, col] <= 0.2)",0
):,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
Modified from scipy source code:,0
- to restrict its use to 2D,0
- to get rid of shuffling (since arrays are only (nbodyparts * 3) element long),0
TODO - factor in keypoint confidence (and weight by # of observations??),1
"mat = self.calc_pairwise_oks(poses, poses_ref)",0
Remove matched detections with low OKS,0
matches = [],0
"for row, col in zip(row_indices, col_indices):",0
"if mat[row, col] < self.oks_threshold:",0
unmatched_poses.append(row),0
unmatched_trackers.append(col),0
else:,0
"matches.append([row, col])",0
if not len(matches):,0
"matches = np.empty((0, 2), dtype=int)",0
else:,0
matches = np.stack(matches),0
update matched trackers with assigned detections,0
create and initialise new trackers for unmatched detections,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
filter out matched with low IOU,0
TODO: get cropping parameters and utilize!,1
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Load meta data & annotations,0
get the graph!,0
"NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*",0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
"MA projects have column indices ""scorer"", ""individuals"" and ""bodyparts""",0
"Drop the scorer level, and put individuals in rows",0
The error rows are series; stack in axis 1 and pivot to get DF,0
TODO: Make this code not so redundant!,1
"If a string was passed in, auto-convert to True for backward compatibility",0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
TODO: Unlike using create_training_dataset() If create_training_model_comparison() is used there won't,1
necessarily be training fractions for every shuffle which will raise the FileNotFoundError..,0
Not sure if this should throw an exception or just be a warning...,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to initial folder,0
"# Also storing one ""large"" table with results:",0
note: evaluationfolder.parents[0] to get common folder above all shuffle evaluations.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
#################################################,0
Load data...,0
#################################################,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
TODO: Unlike using create_training_dataset() If create_training_model_comparison() is used there won't,1
necessarily be training fractions for every shuffle which will raise the FileNotFoundError..,0
Not sure if this should throw an exception or just be a warning...,0
Load meta data,0
TODO: IMPLEMENT for different batch sizes?,1
Ignore best edges possibly defined during a prior evaluation,0
Create folder structure to store results.,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
Pass the image and the keypoints through the resizer;,0
this has no effect if no augmenters were added to it.,0
FIXME Is having an empty array vs nan really that necessary?!,1
"Form 2D array of shape (n_rows, 4) where the last dimension",0
"is (sample_index, peak_y, peak_x, bpt_index) to slice the PAFs.",0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in,0
Compute all distance statistics,0
Calculate overall prediction error,0
Skip data-driven skeleton selection unless,0
the model was trained on the full graph.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
in case there was already a graph,0
have to overwrite this,0
removing the decoding layer from the checkpoint,0
"loading backbone from ResNet, MobileNet etc.",0
"display_iters = max(1,int(displayiters))",0
"[_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})",0
Save snapshot,0
return to original path.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Clip peak locations to PAFs dimensions,0
"unit_vecs = vecs / lengths[:, np.newaxis]",0
"affinities = np.squeeze(y @ np.expand_dims(unit_vecs, axis=2)).sum(axis=1)",0
Form cost matrices,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"loading backbone from ResNet, MobileNet etc.",0
"Auto-switch to Adam on M1/M2 chips, as the momentum optimizer crashes",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Convert a frozen graph to OpenVINO IR format,0
Read network into memory,0
"For better efficiency, model is initialized for batch_size 1 and every sample processed independently",0
Load network to device,0
Prepare input data,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Change the stride from 2 to 1 to get 16x downscaling instead of 32x.,0
https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142,0
Multi stage is currently only implemented for resnets,0
Defining multi_fusion backbone,0
Attaching multi-stage decoder,0
stage_paf_output = stage_paf_output + pre_stage_paf_output,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
FIXME Fix wrong scope with Keras layers,1
"def prediction_layer(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2DTranspose(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=2,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
## New DLCNet Addition: multi-stage decoder,0
"def prediction_layer_stage(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2D(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=1,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Compute variance using: Var[X]= E[X^2] - E[X]^2.,0
Compute keep_prob,0
TODO(tanmingxing): add support for training progress.,1
Compute drop_connect tensor,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
turn into x times y time bs * bpts,0
extract corresponding locref x and y as well as probability,0
Filter predicted heatmaps with a 2D Gaussian kernel as in:,0
https://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_The_Devil_Is_in_the_Details_Delving_Into_Unbiased_Data_CVPR_2020_paper.pdf,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
defaults will be a public argument for namedtuple in Python 3.7,0
https://docs.python.org/3/library/collections.html#collections.namedtuple,0
Make sure that round down does not go down by more than 10%.,0
Builds the block accordings to arguments.,0
Expansion phase:,0
Depth-wise convolution phase:,0
Squeeze and Excitation layer.,0
Output phase:,0
only apply drop_connect if skip presents.,0
Expansion phase:,0
Output phase:,0
only apply drop_connect if skip presents.,0
Builds blocks.,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Stem part.,0
Head part.,0
Calls Stem layers,0
Calls blocks.,0
Calls final layers and returns logits.,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
"(width_coefficient, depth_coefficient, resolution, dropout_rate)",0
blocks_args = [,0
"'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',",0
"'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',",0
"'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',",0
"'r1_k3_s11_e6_i192_o320_se0.25',",0
],0
The default is TPU-specific batch norm.,0
The alternative is tf.layers.BatchNormalization.,0
"batch_norm=utils.TpuBatchNormalization,  # TPU-specific requirement.",0
ValueError will be raised here if override_params has fields not included,0
in global_params.,0
,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrunk when given a depth-multiplier < 1.0.,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Clamp to 40% of crop size to ensure that at least,0
the center keypoint remains visible after the offset is applied.,0
Points located close to one another are sampled preferentially,0
in order to augment crowded regions.,0
Include keypoints in the count to avoid null probabilities,0
Shift the crop center in both dimensions by random amounts,0
and normalize to the original image dimensions.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"for topview, it's safe to mask keypoints under threshold",0
Pickle the 'data' dictionary using the highest protocol available.,0
item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal,0
3 for likelihood,0
we only do single animal here,0
let's hard code it,0
"Add smart, keypoint-aware image cropping",0
"because of the existence of threshold, sampling population is adjusted to len(self.data)",0
empty prediction for this frame,0
in case it's empty prediction,0
Scale is sampled only once (per batch) to transform all of the images into same size.,0
Discard keypoints whose coordinates lie outside the cropped image,0
Invisible joints are represented by nans,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
Produce score maps and location refinement fields,0
Find indices of individuals in joint_id,0
WIP!,0
STD of gaussian is 1/4 of threshold,0
Grid of coordinates,0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Distance between the joint point and each coordinate,0
NEEDS TO BE DONE!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
TODO Finish implementing actual abstract class,1
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
parameter initialization for augmentation pipeline:,0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images of a batch into same size.,0
Approximating the scale,0
invisible joints are represented by nans,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"First, initializing variables (if they don't exist)",0
what is the fraction of training samples with scaling augmentation?,0
loading defaults for rotation range!,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"cfg[""rotateratio""] = cfg.get(",0
"""rotratio"", 0.4",0
)  # what is the fraction of training samples with rotation augmentation?,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"Auto cropping is new (was not in Nature Neuroscience 2018 paper, but introduced in Nath et al. Nat. Protocols 2019)",0
"and boosts performance by 2X, particularly on challenging datasets, like the cheetah in Nath et al.",0
Parameters for augmentation with regard to cropping:,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Get rid of the interpolation beyond the spline knots,0
Data has been filtered so continue to the next video,0
Data haven't been filtered yet,0
Retrieve original individual label indices,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Check the training image paths are correctly stored as arrays of strings,0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
TENSORPACK could fail in WINDOWS...,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
Sometimes tracks cannot be reconstructed as test data are randomly,0
"created; when this happens, we generate a fake h5 data file.",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Loading example data set,0
# Create one identical splits for 3 networks and 3 augmentations,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
Setting specific parameters for training,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"MODELS = [""dlcrnet_ms5"", ""dlcr101_ms5"", ""efficientnet-b0"", ""mobilenet_v2_0.35""]",0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
## adding it here,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Downloading the calibration images,0
Deleting unnecessary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Loading example data set,0
example how to set pose config variables:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
test if existing models are found:,0
Test if nonexisting models are not found,0
Add some other office files:,0
Add a .pickle and .h5 files,0
"By default, all videos with common extensions are taken from a directory",0
A list of extensions can also be passed in,0
"labeled_folders: (has_H5, H5_st_mtime, folder_name)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Test bounding box from a single keypoint,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Test that shape was preserved after vectorization,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Guarantee that images smaller than crop size are handled fine,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Disable stochastic scale jitter,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Verify the aspect ratio is preserved,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/main/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Load sample multi animal data,0
Generate a random permutation and reorder data. Ignore the unique bodypart,0
Get inverse permutation and reorder the modified data to get back,0
to the original,0
Check,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"Check data comprise path, shape, and xy coordinates",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Check that we get the number of frames we asked for,0
Check that all indices are valid,0
Check that all frames are unique,0
Check that we get the number of frames we asked for,0
Check that all indices are valid,0
Check that all frames are unique,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Break the graph to test stitching failure,0
Add fake IDs,0
Split all tracklets in half,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"individual_1, individual2",0
"leftArm, rightArm, leftArm, rightArm",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Test flipped keypoints,0
Original (cached) coordinates must have remained empty,0
Assemble based on the smallest graph to speed up testing,0
Generate fake identity predictions,0
Assemble based on the smallest graph to speed up testing,0
Test now with identity only and ensure assemblies,0
contain only parts of a single group ID.,0
Test empty assembly,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Verify that the Hugging Face folder was removed,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
def install(package):,0
"subprocess.check_call([sys.executable, ""-m"", ""pip"", ""install"", package])",0
"install(""tensorflow==1.13.1"")",0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
quicker variant,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Suppress tensorflow warning messages,0
"Train, evaluate & predict functions / all require TF",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Updating config file:,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path = auxfun_models.check_for_weights(pose_cfg['net_type'], parent_path)",0
Updating training and test pose_cfg:,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Read the config file,0
deal with user passing a single video to add,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
common parameters:,0
Write dictionary to yaml  config file,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
looking for the pseudo label path,0
spatial pyramid is not for adapted model,0
spatial pyramid can still be useful for reducing jittering and quantization error,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
ratio_h and ratio_w should match though in reality it does not match exactly,0
only keep the max,0
empty prediction if pred is not a dict,0
Compute cosine similarity,0
"len(frames) -> (n_scale,)",0
"frames[0].shape - > (batchsize, h, w, 3)",0
no crop needed,0
"batch full, start true inferencing",0
only do this when animal is detected,0
in case we reach the end of the video,0
add a temp folder for checkpoint,0
spatial pyramid can still be useful for reducing jittering and quantization error,0
extra data,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"TODO(stes) mocking a few modules to rely in fewer dependencies, without",1
causing import errors when using deeplabcut.,0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in.,0
TODO(jeylau) add affinity.setter to Assembly,1
TODO(stes): remove/rewrite,1
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"ignore the exception and continue with the next evaluation, without",0
yielding a result value.,0
"return the result value, with NaN as the result for all metrics that",0
could not be computed due to the error.,0
raise the error and stop evaluation,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Read the config file and related variables,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Flatten the list of bodyparts to connect,0
Format data,0
Trick to force equal aspect ratio of 3D plots,0
Set up the matplotlib figure beforehand,0
Set up skeleton LineCollections,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
Get track_method and do related checks,0
Get track method suffix,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
"## Assign nan to [X,Y] of low likelihood predictions ###",0
Convert the data to a np array to easily mask out the low likelihood predictions,0
"Assign [X,Y] = nan to low likelihood predictions",0
Reshape data back to original shape,0
put data back to the dataframes,0
Check individuals are the same in both views,0
Cross-view match individuals,0
Create a dummy variables for single-animal,0
Cleaner variable (since inds view1 == inds view2),0
"Reshape: (num_framex, num_individuals, num_bodyparts , 2)",0
Triangulate data,0
i is individual in view 1,0
voting[i] is the matched individual in view 2,0
Create 3D DataFrame column and row indices,0
Swap num_animals with num_frames axes to ensure well-behaving reshape,0
Fill up 3D dataframe,0
Reorder 2D dataframe in view 2 to match order of view 1,0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no intermediate saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Alternatively,0
Sort tracklets by length to prioritize greater continuity,0
Drop tracklets that are too short,0
"Where slots are available, copy the data over",0
"If about to overwrite data, keep tracklets with highest confidence",0
Squeeze some data into empty slots,0
"For the remaining data, overwrite where we are least confident",0
Find keypoints closest to the remaining data,0
Use Manhattan distance to avoid overflow,0
Only overwrite if improving confidence,0
Fill existing gaps,0
Retrieve original individual label indices,0
Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.,0
Fill existing gaps,0
Retrieve original individual label indices,0
ID swaps occur when X and Y simultaneously intersect each other.,0
Get only those bodypart pairs that belong to different individuals,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"See Li et al., 2012. Cross-view Activity Recognition using Hankelets.",0
"As proposed in the paper, the Hankel matrix can either be formed from",0
the tracklet's centroid or its normalized velocity.,0
"vel = np.diff(self.centroid, axis=0)",0
"vel /= np.linalg.norm(vel, axis=1, keepdims=True)",0
return self.hankelize(vel),0
TODO Add missing data imputation,1
"nrows, ncols = mat.shape",0
beta = nrows / ncols,0
omega = 0.56 * beta ** 3 - 0.95 * beta ** 2 + 1.82 * beta + 1.43,0
return np.argmin(s > omega * np.median(s)),0
"Note that if tracklets are very short, some may actually be part of the same track",0
and thus incorrectly reflect separate track endpoints...,0
"Map each Tracklet to an entry and output nodes and vice versa,",0
which is convenient once the tracklets are stitched.,0
Store tracklets and corresponding negatives (those that overlap in time),0
TODO Avoid looping over all pairs of tracklets,0
Pick the closest (spatially) overlapping tracklet,0
The algorithm works better with integer weights,0
Let us prune the graph by removing all source and sink edges,0
but those connecting the `n_tracks` first and last tracklets.,0
Preflow push seems to work slightly better than shortest,0
"augmentation path..., and is more computationally efficient.",0
Verify whether there are overlapping tracklets,0
"Pick the segment that minimizes ""smoothness"", computed here",0
with the coefficient of variation of the differences.,0
Rebuild a full graph from the remaining nodes without,0
temporal constraint on what tracklets can be stitched together.,0
Cycle through the residuals and incorporate back those,0
that only fit in a single tracklet.,0
Greedily add the remaining residuals,0
Refresh temporal bounds,0
Guarantee track data are sorted in the order defined in the config,0
"If there isn't a track for each animal, fill in the dataframe with NaNs",0
Default to the distance cost function,0
should only exist one,0
Add in identity weighing before building the graph,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implementation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
average distance and average # significant differences avg. over comparisonbodyparts,0
make sure coords and videos are a list,0
can we make a catch here? - in fact we should drop indices from DataCombined,0
if they are in CollectedData.. [ideal behavior; currently pretty unlikely],0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
###################################################,0
Dependencies,0
###################################################,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as,0
specified in the config file,0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
Prepare figure,0
parse the alpha selection function,0
Mask rather than drop unwanted bodyparts to ensure consistent coloring,0
cropping!,0
nx=X2-X1,0
!/usr/bin/env python3,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"more videos are in principle covered, as OpenCV is used and allows many formats.",0
The above already raises a ValueError if formatting is wrong,0
"Rotate, see: https://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg",0
interesting option to just update metadata.,0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
Read the video until a frame is successfully read,0
import platform,0
"if platform.system() == ""Darwin"":  # for OSX use WXAgg",0
fig.canvas.start_event_loop(timeout=-1),0
else:,0
#fig.canvas.stop_event_loop(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"Windows throws error if file path is > 260 characters, can fix with prefix.",0
See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
uses openCV,0
cap.release() >> still used in frame_extraction!,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
This dictionary maps the model types to the file locations where the models exist.,0
Exit the function early if an unknown modeltype is provided.,0
"The path is right, hence the weights are missing; we'll download them again.",0
Aliases for backwards-compatibility,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluding the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
Initialize costs matrix,0
get common bodypart detections in track pair,0
add 3rd dim to the points,0
cost for any point in time of t1 being the same,0
any point in time of t2,0
Get average cost of the entire track,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
check if it exists:,0
(average_degree / 2) as many edges as there are nodes is required,0
for undirected graphs to reach the target degree.,0
"Attention this order has to be consistent (for training set creation, training, inference etc.)",0
"-> adding (single,bpt) for uniquebodyparts",0
"-> adding (individual,bpt) for multianimalbodyparts",0
load defaults,0
set project specific parameters:,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
###################################################,0
Dependencies,0
###################################################,0
Pose X vs pose Y,0
Poses vs time,0
Likelihoods,0
Histograms,0
#################################################,0
Looping analysis over video,0
#################################################,0
"In a multi animal scenario, show more verbose errors.",0
Some vidoes were not evaluated.,0
Keep only the individuals and bodyparts that were labeled,0
Either display the animals defined in the config if they are found,0
"in the dataframe, or all the trajectories regardless of their names",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Determine whether the data are single- or multi-animal without loading into memory,0
simply by checking whether 'individuals' is in the second line of the CSV.,0
Make paths platform-agnostic if they are not already,0
Ensure folder names are strings,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
make list of full paths,0
filter list of videos,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"if ""MULTI!"" in allbpts:",0
Ensure same order as in config.yaml,0
ABBREVIATE NETWORK NAMES -- esp. for mobilenet!,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
Iterate over data files and stop as soon as one matching the scorer is found,0
If there was no match...,0
aliases for backwards-compatibility.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Set up a logger and add an stdout handler.,0
A single logger can have many handlers:,0
https://docs.python.org/3/howto/logging.html#handler-basic,0
TODO Dump to log file instead,1
"logger = logging.getLogger(""GUI"")",0
logger.setLevel(logging.DEBUG),0
handler = logging.StreamHandler(stream=sys.stdout),0
handler.setLevel(logging.DEBUG),0
formatter = logging.Formatter(,0
"""%(asctime)s - %(name)s - %(levelname)s - %(message)s"", ""%Y-%m-%d %H:%M:%S""",0
),0
handler.setFormatter(formatter),0
logger.addHandler(handler),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
restore the background region,0
Take into consideration imprecise OpenCV estimation of total number of frames,0
Automatically disable the draggable points,0
Save additional frames to the labeled-data directory,0
Store the newly-refined data,0
Uncertain keypoints are ignored,0
Merge with the already existing annotated data,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Thread-safe Stdout redirector,0
Creating action using the first constructor,0
Creating actions using the second constructor,0
File menu,0
View menu,0
Help menu,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
all_bodyparts: List,0
NOTE: Is there a case where a specific list should,0
have bodyparts other than the root? I don't think so.,0
Videotype selection,0
Select videos,0
Number of selected videos text,0
Clear video selection,0
"Qt returns a tuple (list of files, filetype)",0
Add tab header,0
Add separating line,0
Look for any extension by default,0
This works both with e.g. .avi and avi,0
Choose multiple files by default,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Automatically activate the napari-deeplabcut plugin,0
TODO Insert new video,1
TODO Insert skeleton link,1
"Hack to make the first column read-only, as we do not want users to touch it.",1
"The cleaner solution would be to use a QTreeView and QAbstractItemModel,",0
but that is a lot of rework for little benefits.,0
Leave untouched when it is already a string,0
"Slashes also raise the error, but no need to print anything since it is then likely to be a path",0
Walk backwards across parents to get all keys,0
Read the video until a frame is successfully read,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
TODO: Multi video select.... have to change to single video!,1
Shuffle,0
Num animals,0
Min swap length,0
Max gap to fill,0
Trail length,0
Filter type,0
Filter window length,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
Tracker Type,0
Num animals,0
Num triplets,0
layout.addWidget(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
Overwrite videos,0
Trail Points,0
Plot all bodyparts,0
Skeleton,0
Filtered data,0
Plot trajectories,0
High quality video,0
Bodypart list,0
Multianimal scenario.,0
Color is based on individual or bodypart.,0
Single animal scenario.,0
Color is based on bodypart.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Extraction method,0
Frame extraction algorithm,0
Frame cropping,0
Cluster step,0
GUI Slider width,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
use the default pose_cfg file for default values,0
Shuffle,0
Display iterations,0
Save iterations,0
Max iterations,0
Max number snapshots to keep,0
layout.addWidget(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Single / Multi animal Only Layouts,0
Dynamic bodypart cropping,0
Save results as csv,0
Filter predictions,0
Plot Trajectories,0
Show trajectory plots,0
Shuffle,0
layout.addLayout(tmp_layout),0
tmp_layout = QtWidgets.QGridLayout(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
Augmentation method,0
Neural Network,0
TODO: finish model_comparison,1
Check that training data files were indeed created.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Add config text field and button,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
layout.addWidget(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Display all images,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"calling create_tracking_dataset, train_tracking_transformer, stitch_tracklets",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
only try to find these features if they are in the dictionary,0
1) reference to video folder and get the proper bpt_feature file for feature table,0
2) get either the path to gt or the path to track pickle,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
tracklets,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"with npy list form videos, split each to train and test",0
"video_name = '.'.join(video.split(""/"")[-1].split(""."")[:-1])",0
assuming there is only one match,0
make my own model factory,0
make my own loss factory,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights",0
"NOTE: drop path for stochastic depth, we shall see if this is better than dropout here",0
only channel pooling,0
"x -> (B, 12, 2048)",0
"x -> (B, 12, 768)",0
trying to keep everything simple,0
need to write my own keypoint embedding,0
we don't need a fc layer here,0
x: inputs,0
"(B, 12, 768)",0
added pos embed,0
remove all parts I don't understand,0
forwarding through blocks. But num of blocks should be reduced,0
i can't really tell what the difference is,0
will have to change some of these,0
For old models that I trained prior to conv based patchification,0
To resize pos embedding when using model at different size from pretrained weights,0
Rescale the grid of position embeddings when loading from state_dict. Adapted from,0
https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224,0
Cut & paste from PyTorch official master until it's in a few official releases - RW,0
Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf,0
Computes standard normal cumulative distribution function,0
Values are generated by using a truncated uniform distribution and,0
then using the inverse CDF for the normal distribution.,0
Get upper and lower cdf values,0
"Uniformly fill tensor with values from [l, u], then translate to",0
"[2l-1, 2u-1].",0
Use inverse cdf transform for normal distribution to get truncated,0
standard normal,0
"Transform to proper mean, std",0
Clamp to ensure it's in the proper range,0
"type: (Tensor, float, float, float, float) -> Tensor",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2019 Ross Wightman,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Hacked together by / Copyright 2020 Ross Wightman,1
https://github.com/rwightman/pytorch-image-models/blob/main/timm/scheduler/scheduler_factory.py,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"resample if noise out of percent limit, brute force but shouldn't spin much",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
distmat g,0
q    1 3 2 4,0
4 1 2 3,0
"0, 2, 1, 3",0
"1, 2, 3, 0",0
compute cmc curve for each query,0
get query pid and camid,0
remove gallery samples that have the same pid and camid with query,0
compute cmc curve,0
"binary vector, positions with value 1 are correct matches",0
this condition is true when query identity does not appear in gallery,0
compute average precision,0
reference: https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Average_precision,0
"tmp_cmc = [x / (i + 1.) for i, x in enumerate(tmp_cmc)]",0
query,0
gallery,0
"distmat = re_ranking(qf, gf, k1=20, k2=6, lambda_value=0.3)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"if feature vector is numpy, you should use 'torch.tensor' transform it to tensor",0
print('using GPU to compute original distance'),0
print('starting re_ranking'),0
k-reciprocal neighbors,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
More elegantly one can simply define:,0
take care of difference between feature map space and original image space,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
cos = torch.cdist,0
TODO: maybe find a better spot for this.,1
"dev =  device if torch.cuda.is_available() else ""cpu""",0
train,0
print (f'validation loss {val_loss/len(val_loader)}'),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
normalize vectors here,0
i th vector at j th kpt,0
maybe needs to convert them to embeddings and position token,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Drop missing body parts,0
Drop points lying outside the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
## dlcnet_ms5: backbone resnet50 + multi-fusion & multi-stage module,0
## dlcr101_ms5/dlcr152_ms5: backbone resnet101/152 + multi-fusion & multi-stage module,0
"If the graph is unnecessarily large (with 15+ keypoints by default),",0
we randomly prune it to a size guaranteeing an average node degree of 6;,0
"see Suppl. Fig S9c in Lauer et al., 2022.",0
Use the skeleton defined in the config file,0
Ignore possible connections between 'multi' and 'unique' body parts;,0
one can never be too careful...,0
Disable the prediction of PAFs if the graph is empty,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving metadata and data file (Pickle file),0
###############################################################################,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Setting inference cfg file:,0
Search a pose_config.yaml file to parse missing information,0
Clean the training-datasets folder prior to recreating the data pickles,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Load updated lists:,0
Find the corresponding video file,0
"print(key, ""dropping?"")",0
adding important values for multianiaml project:,0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Determine the index length required to guarantee,0
the train–test ratio is exactly the desired one.,0
Pad indices so lengths agree,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(name,test_video_name)",0
Blazing fast and does not load the image into memory,0
Check that points lie within the image,0
DeprecationWarnings are silenced since Python 3.2 unless triggered in __main__,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
updating variable if null/None! #backwardscompatability,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
these values are dropped as scalecrop,0
doesn't have rotation implemented,0
Also drop maDLC smart cropping augmentation parameters,0
sort the model directories,0
get the shuffle index and offset by 1.,0
read cfg file,0
create log file,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Check for variable correctness,0
Moviepy:,0
select crop method,0
"Find the matching video from the config `video_sets`,",0
as it may be stored elsewhere than in the `videos` directory.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
allow_growth must be true here because tensorflow does not automatically free gpu memory and setting it as false occupies all gpu memory so that pytorch cannot kick in,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
should close tensorflow session here in order to free gpu,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"Flip x, y, confidence and reshape",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html#videocapture-get,0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
TODO: add cropping as in video analysis!,1
if cropping is not None:,0
cfg['cropping']=True,0
"cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping",0
"print(""Overwriting cropping parameters:"", cropping)",0
"print(""These are used for all videos, but won't be save to the cfg file."")",0
"Also ensure `boundingboxslack` is greater than zero, otherwise overlap",0
"between trackers cannot be evaluated, resulting in empty tracklets.",0
Check which snapshots are available and sort them by # iterations,0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
NOTE: If dataname line above is changed then line below is obsolete?,0
"trackname = trackname.replace(videofolder, destfolder)",0
TODO: adjust this for multi + unique bodyparts!,1
this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. ),0
Optimal identity assignment based on soft voting,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
imagenet mean for resnet pretraining:,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/DeepLabCut/DeepLabCut/pull/324",0
for DLC 2.2. (here all set False to not use PAFs/pairwise fields),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
"reloading defaults, as they can bleed over from a previous run otherwise",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
dlc_cfg_train = load_config(str(path_train_config)),0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
Name for deeplabcut net (based on its parameters),0
"DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])",0
"print(""Extracting maps for "", DLCscorer, "" with # of trainingiterations:"", trainingsiterations)",0
if notanalyzed: #this only applies to ask if h5 exists...,0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"for multi animal, seems only this is used",0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Get animal IDs and corresponding indices in the arrays of detections,0
Form ground truth beforehand,0
Assemble animals on the full set of detections,0
Count the number of unassembled bodyparts,0
Handle unlabeled bodyparts...,0
"Only 1 animal, let us return the full graph indices only",0
Find minimal skeleton,0
Select optimal PAF graph,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Selective copy; deepcopy is >5x slower,0
Only keeps skeletons that are more than 90% complete,0
TODO Normalize dists by longest length?,1
TODO Smarter imputation technique (Bayesian? Grassmann averages?),1
Covariance matrix estimation fails due to numerical singularities,0
Distance is undefined if the assembly is empty,0
Correct distance to account for missing observations,0
"Alternatively, reduce contribution of missing values to the Mahalanobis",0
distance to zero by substituting the corresponding means.,0
"Fill the subsets with unambiguous, complete individuals",0
Fuse superfluous assemblies,0
Second pass without edge safety,0
Mark the unique body parts as assembled anyway so,0
they are not used later on to fill assemblies.,0
Store selected edges for subsequent frames,0
Remove invalid assemblies,0
Fill assemblies with unconnected body parts,0
Spawning (rather than forking) multiple processes does not,0
work nicely with the GUI or interactive sessions.,0
"In that case, we fall back to the serial assembly.",0
Swap corresponding keypoints,0
Only consider assemblies of at least two keypoints,0
Greedy assembly matching like in pycocotools,0
Global rather than greedy assembly matching,0
Cast columns of dtype 'object' to float to avoid TypeError,0
further down in _parse_ground_truth_data.,0
sigma is taken as the median of all COCO keypoint standard deviations,0
Sort matching score (OKS) in descending order of assembly affinity,0
Guarantee precision decreases monotonically,0
See https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Regularize by forcing AR <= 5,0
max_ar = 5,0
if el.aspect_ratio >= max_ar:,0
if el.height > el.width:,0
el.width = el.height / max_ar,0
else:,0
el.height = el.width / max_ar,0
Orient the ellipse such that it encompasses most points,0
"n_inside = el.contains_points(np.c_[self.x, self.y]).sum()",0
el.theta += 0.5 * np.pi,0
"if el.contains_points(np.c_[self.x, self.y]).sum() < n_inside:",0
el.theta -= 0.5 * np.pi,0
"r2 = chi2.ppf(2 * norm.cdf(sd) - 1, 2)",0
"height, width = np.sqrt(E * r2)",0
The general quadratic curve has the form:,0
ax^2 + 2bxy + cy^2 + 2dx + 2fy + g = 0,0
Ellipse center coordinates,0
Semi-axes lengths,0
Angle to the horizontal,0
High uncertainty to the unobservable initial velocities,0
Give high uncertainty to the unobservable initial velocities,0
diff = val - cost_matrix,0
"diff[row, col] += val",0
if (,0
val < self.iou_threshold,0
or np.any(diff[row] <= 0.2),0
"or np.any(diff[:, col] <= 0.2)",0
):,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
Modified from scipy source code:,0
- to restrict its use to 2D,0
- to get rid of shuffling (since arrays are only (nbodyparts * 3) element long),0
TODO - factor in keypoint confidence (and weight by # of observations??),1
"mat = self.calc_pairwise_oks(poses, poses_ref)",0
Remove matched detections with low OKS,0
matches = [],0
"for row, col in zip(row_indices, col_indices):",0
"if mat[row, col] < self.oks_threshold:",0
unmatched_poses.append(row),0
unmatched_trackers.append(col),0
else:,0
"matches.append([row, col])",0
if not len(matches):,0
"matches = np.empty((0, 2), dtype=int)",0
else:,0
matches = np.stack(matches),0
update matched trackers with assigned detections,0
create and initialise new trackers for unmatched detections,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
filter out matched with low IOU,0
TODO: get cropping parameters and utilize!,1
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Load meta data & annotations,0
get the graph!,0
"NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*",0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
"MA projects have column indices ""scorer"", ""individuals"" and ""bodyparts""",0
"Drop the scorer level, and put individuals in rows",0
The error rows are series; stack in axis 1 and pivot to get DF,0
TODO: Make this code not so redundant!,1
"If a string was passed in, auto-convert to True for backward compatibility",0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
TODO: Unlike using create_training_dataset() If create_training_model_comparison() is used there won't,1
necessarily be training fractions for every shuffle which will raise the FileNotFoundError..,0
Not sure if this should throw an exception or just be a warning...,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to initial folder,0
"# Also storing one ""large"" table with results:",0
note: evaluationfolder.parents[0] to get common folder above all shuffle evaluations.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
#################################################,0
Load data...,0
#################################################,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
TODO: Unlike using create_training_dataset() If create_training_model_comparison() is used there won't,1
necessarily be training fractions for every shuffle which will raise the FileNotFoundError..,0
Not sure if this should throw an exception or just be a warning...,0
Load meta data,0
TODO: IMPLEMENT for different batch sizes?,1
Ignore best edges possibly defined during a prior evaluation,0
Create folder structure to store results.,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
Pass the image and the keypoints through the resizer;,0
this has no effect if no augmenters were added to it.,0
FIXME Is having an empty array vs nan really that necessary?!,1
"Form 2D array of shape (n_rows, 4) where the last dimension",0
"is (sample_index, peak_y, peak_x, bpt_index) to slice the PAFs.",0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in,0
Compute all distance statistics,0
Calculate overall prediction error,0
Skip data-driven skeleton selection unless,0
the model was trained on the full graph.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
in case there was already a graph,0
have to overwrite this,0
removing the decoding layer from the checkpoint,0
"loading backbone from ResNet, MobileNet etc.",0
"display_iters = max(1,int(displayiters))",0
"[_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})",0
Save snapshot,0
return to original path.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Clip peak locations to PAFs dimensions,0
"unit_vecs = vecs / lengths[:, np.newaxis]",0
"affinities = np.squeeze(y @ np.expand_dims(unit_vecs, axis=2)).sum(axis=1)",0
Form cost matrices,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"loading backbone from ResNet, MobileNet etc.",0
"Auto-switch to Adam on M1/M2 chips, as the momentum optimizer crashes",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Convert a frozen graph to OpenVINO IR format,0
Read network into memory,0
"For better efficiency, model is initialized for batch_size 1 and every sample processed independently",0
Load network to device,0
Prepare input data,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Change the stride from 2 to 1 to get 16x downscaling instead of 32x.,0
https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142,0
Multi stage is currently only implemented for resnets,0
Defining multi_fusion backbone,0
Attaching multi-stage decoder,0
stage_paf_output = stage_paf_output + pre_stage_paf_output,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
FIXME Fix wrong scope with Keras layers,1
"def prediction_layer(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2DTranspose(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=2,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
## New DLCNet Addition: multi-stage decoder,0
"def prediction_layer_stage(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2D(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=1,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Compute variance using: Var[X]= E[X^2] - E[X]^2.,0
Compute keep_prob,0
TODO(tanmingxing): add support for training progress.,1
Compute drop_connect tensor,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
turn into x times y time bs * bpts,0
extract corresponding locref x and y as well as probability,0
Filter predicted heatmaps with a 2D Gaussian kernel as in:,0
https://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_The_Devil_Is_in_the_Details_Delving_Into_Unbiased_Data_CVPR_2020_paper.pdf,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
defaults will be a public argument for namedtuple in Python 3.7,0
https://docs.python.org/3/library/collections.html#collections.namedtuple,0
Make sure that round down does not go down by more than 10%.,0
Builds the block accordings to arguments.,0
Expansion phase:,0
Depth-wise convolution phase:,0
Squeeze and Excitation layer.,0
Output phase:,0
only apply drop_connect if skip presents.,0
Expansion phase:,0
Output phase:,0
only apply drop_connect if skip presents.,0
Builds blocks.,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Stem part.,0
Head part.,0
Calls Stem layers,0
Calls blocks.,0
Calls final layers and returns logits.,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
"(width_coefficient, depth_coefficient, resolution, dropout_rate)",0
blocks_args = [,0
"'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',",0
"'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',",0
"'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',",0
"'r1_k3_s11_e6_i192_o320_se0.25',",0
],0
The default is TPU-specific batch norm.,0
The alternative is tf.layers.BatchNormalization.,0
"batch_norm=utils.TpuBatchNormalization,  # TPU-specific requirement.",0
ValueError will be raised here if override_params has fields not included,0
in global_params.,0
,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrunk when given a depth-multiplier < 1.0.,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Clamp to 40% of crop size to ensure that at least,0
the center keypoint remains visible after the offset is applied.,0
Points located close to one another are sampled preferentially,0
in order to augment crowded regions.,0
Include keypoints in the count to avoid null probabilities,0
Shift the crop center in both dimensions by random amounts,0
and normalize to the original image dimensions.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"for topview, it's safe to mask keypoints under threshold",0
Pickle the 'data' dictionary using the highest protocol available.,0
item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal,0
3 for likelihood,0
we only do single animal here,0
let's hard code it,0
"Add smart, keypoint-aware image cropping",0
"because of the existence of threshold, sampling population is adjusted to len(self.data)",0
empty prediction for this frame,0
in case it's empty prediction,0
Scale is sampled only once (per batch) to transform all of the images into same size.,0
Discard keypoints whose coordinates lie outside the cropped image,0
Invisible joints are represented by nans,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
Produce score maps and location refinement fields,0
Find indices of individuals in joint_id,0
WIP!,0
STD of gaussian is 1/4 of threshold,0
Grid of coordinates,0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Distance between the joint point and each coordinate,0
NEEDS TO BE DONE!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
TODO Finish implementing actual abstract class,1
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
parameter initialization for augmentation pipeline:,0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images of a batch into same size.,0
Approximating the scale,0
invisible joints are represented by nans,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"First, initializing variables (if they don't exist)",0
what is the fraction of training samples with scaling augmentation?,0
loading defaults for rotation range!,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"cfg[""rotateratio""] = cfg.get(",0
"""rotratio"", 0.4",0
)  # what is the fraction of training samples with rotation augmentation?,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"Auto cropping is new (was not in Nature Neuroscience 2018 paper, but introduced in Nath et al. Nat. Protocols 2019)",0
"and boosts performance by 2X, particularly on challenging datasets, like the cheetah in Nath et al.",0
Parameters for augmentation with regard to cropping:,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Get rid of the interpolation beyond the spline knots,0
Data has been filtered so continue to the next video,0
Data haven't been filtered yet,0
Retrieve original individual label indices,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Check the training image paths are correctly stored as arrays of strings,0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
TENSORPACK could fail in WINDOWS...,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
Sometimes tracks cannot be reconstructed as test data are randomly,0
"created; when this happens, we generate a fake h5 data file.",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Loading example data set,0
# Create one identical splits for 3 networks and 3 augmentations,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
Setting specific parameters for training,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"MODELS = [""dlcrnet_ms5"", ""dlcr101_ms5"", ""efficientnet-b0"", ""mobilenet_v2_0.35""]",0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
## adding it here,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Downloading the calibration images,0
Deleting unnecessary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Loading example data set,0
example how to set pose config variables:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
test if existing models are found:,0
Test if nonexisting models are not found,0
Add some other office files:,0
Add a .pickle and .h5 files,0
"By default, all videos with common extensions are taken from a directory",0
A list of extensions can also be passed in,0
"labeled_folders: (has_H5, H5_st_mtime, folder_name)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Test bounding box from a single keypoint,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Test that shape was preserved after vectorization,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Guarantee that images smaller than crop size are handled fine,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Disable stochastic scale jitter,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Verify the aspect ratio is preserved,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Load sample multi animal data,0
Generate a random permutation and reorder data. Ignore the unique bodypart,0
Get inverse permutation and reorder the modified data to get back,0
to the original,0
Check,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"Check data comprise path, shape, and xy coordinates",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Check that we get the number of frames we asked for,0
Check that all indices are valid,0
Check that all frames are unique,0
Check that we get the number of frames we asked for,0
Check that all indices are valid,0
Check that all frames are unique,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Break the graph to test stitching failure,0
Add fake IDs,0
Split all tracklets in half,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"individual_1, individual2",0
"leftArm, rightArm, leftArm, rightArm",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Test flipped keypoints,0
Original (cached) coordinates must have remained empty,0
Assemble based on the smallest graph to speed up testing,0
Generate fake identity predictions,0
Assemble based on the smallest graph to speed up testing,0
Test now with identity only and ensure assemblies,0
contain only parts of a single group ID.,0
Test empty assembly,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Verify that the Hugging Face folder was removed,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
def install(package):,0
"subprocess.check_call([sys.executable, ""-m"", ""pip"", ""install"", package])",0
"install(""tensorflow==1.13.1"")",0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
quicker variant,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Suppress tensorflow warning messages,0
"Train, evaluate & predict functions / all require TF",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Updating config file:,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path = auxfun_models.check_for_weights(pose_cfg['net_type'], parent_path)",0
Updating training and test pose_cfg:,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Read the config file,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
common parameters:,0
Write dictionary to yaml  config file,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
looking for the pseudo label path,0
spatial pyramid is not for adapted model,0
spatial pyramid can still be useful for reducing jittering and quantization error,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
ratio_h and ratio_w should match though in reality it does not match exactly,0
only keep the max,0
empty prediction if pred is not a dict,0
Compute cosine similarity,0
"len(frames) -> (n_scale,)",0
"frames[0].shape - > (batchsize, h, w, 3)",0
no crop needed,0
"batch full, start true inferencing",0
only do this when animal is detected,0
in case we reach the end of the video,0
add a temp folder for checkpoint,0
spatial pyramid can still be useful for reducing jittering and quantization error,0
extra data,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"TODO(stes) mocking a few modules to rely in fewer dependencies, without",1
causing import errors when using deeplabcut.,0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in.,0
TODO(jeylau) add affinity.setter to Assembly,1
TODO(stes): remove/rewrite,1
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"ignore the exception and continue with the next evaluation, without",0
yielding a result value.,0
"return the result value, with NaN as the result for all metrics that",0
could not be computed due to the error.,0
raise the error and stop evaluation,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Read the config file and related variables,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Flatten the list of bodyparts to connect,0
Format data,0
Trick to force equal aspect ratio of 3D plots,0
Set up the matplotlib figure beforehand,0
Set up skeleton LineCollections,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
Get track_method and do related checks,0
Get track method suffix,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
"## Assign nan to [X,Y] of low likelihood predictions ###",0
Convert the data to a np array to easily mask out the low likelihood predictions,0
"Assign [X,Y] = nan to low likelihood predictions",0
Reshape data back to original shape,0
put data back to the dataframes,0
Check individuals are the same in both views,0
Cross-view match individuals,0
Create a dummy variables for single-animal,0
Cleaner variable (since inds view1 == inds view2),0
"Reshape: (num_framex, num_individuals, num_bodyparts , 2)",0
Triangulate data,0
i is individual in view 1,0
voting[i] is the matched individual in view 2,0
Create 3D DataFrame column and row indices,0
Swap num_animals with num_frames axes to ensure well-behaving reshape,0
Fill up 3D dataframe,0
Reorder 2D dataframe in view 2 to match order of view 1,0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no intermediate saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Alternatively,0
Sort tracklets by length to prioritize greater continuity,0
Drop tracklets that are too short,0
"Where slots are available, copy the data over",0
"If about to overwrite data, keep tracklets with highest confidence",0
Squeeze some data into empty slots,0
"For the remaining data, overwrite where we are least confident",0
Find keypoints closest to the remaining data,0
Use Manhattan distance to avoid overflow,0
Only overwrite if improving confidence,0
Fill existing gaps,0
Retrieve original individual label indices,0
Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.,0
Fill existing gaps,0
Retrieve original individual label indices,0
ID swaps occur when X and Y simultaneously intersect each other.,0
Get only those bodypart pairs that belong to different individuals,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"See Li et al., 2012. Cross-view Activity Recognition using Hankelets.",0
"As proposed in the paper, the Hankel matrix can either be formed from",0
the tracklet's centroid or its normalized velocity.,0
"vel = np.diff(self.centroid, axis=0)",0
"vel /= np.linalg.norm(vel, axis=1, keepdims=True)",0
return self.hankelize(vel),0
TODO Add missing data imputation,1
"nrows, ncols = mat.shape",0
beta = nrows / ncols,0
omega = 0.56 * beta ** 3 - 0.95 * beta ** 2 + 1.82 * beta + 1.43,0
return np.argmin(s > omega * np.median(s)),0
"Note that if tracklets are very short, some may actually be part of the same track",0
and thus incorrectly reflect separate track endpoints...,0
"Map each Tracklet to an entry and output nodes and vice versa,",0
which is convenient once the tracklets are stitched.,0
Store tracklets and corresponding negatives (those that overlap in time),0
TODO Avoid looping over all pairs of tracklets,0
Pick the closest (spatially) overlapping tracklet,0
The algorithm works better with integer weights,0
Let us prune the graph by removing all source and sink edges,0
but those connecting the `n_tracks` first and last tracklets.,0
Preflow push seems to work slightly better than shortest,0
"augmentation path..., and is more computationally efficient.",0
Verify whether there are overlapping tracklets,0
"Pick the segment that minimizes ""smoothness"", computed here",0
with the coefficient of variation of the differences.,0
Rebuild a full graph from the remaining nodes without,0
temporal constraint on what tracklets can be stitched together.,0
Cycle through the residuals and incorporate back those,0
that only fit in a single tracklet.,0
Greedily add the remaining residuals,0
Refresh temporal bounds,0
Guarantee track data are sorted in the order defined in the config,0
"If there isn't a track for each animal, fill in the dataframe with NaNs",0
Default to the distance cost function,0
should only exist one,0
Add in identity weighing before building the graph,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implementation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
average distance and average # significant differences avg. over comparisonbodyparts,0
make sure coords and videos are a list,0
can we make a catch here? - in fact we should drop indices from DataCombined,0
if they are in CollectedData.. [ideal behavior; currently pretty unlikely],0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
###################################################,0
Dependencies,0
###################################################,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as,0
specified in the config file,0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
Prepare figure,0
parse the alpha selection function,0
Mask rather than drop unwanted bodyparts to ensure consistent coloring,0
cropping!,0
nx=X2-X1,0
!/usr/bin/env python3,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"more videos are in principle covered, as OpenCV is used and allows many formats.",0
The above already raises a ValueError if formatting is wrong,0
"Rotate, see: https://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg",0
interesting option to just update metadata.,0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
Read the video until a frame is successfully read,0
import platform,0
"if platform.system() == ""Darwin"":  # for OSX use WXAgg",0
fig.canvas.start_event_loop(timeout=-1),0
else:,0
#fig.canvas.stop_event_loop(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"Windows throws error if file path is > 260 characters, can fix with prefix.",0
See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
uses openCV,0
cap.release() >> still used in frame_extraction!,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
This dictionary maps the model types to the file locations where the models exist.,0
Exit the function early if an unknown modeltype is provided.,0
"The path is right, hence the weights are missing; we'll download them again.",0
Aliases for backwards-compatibility,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluding the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
Initialize costs matrix,0
get common bodypart detections in track pair,0
add 3rd dim to the points,0
cost for any point in time of t1 being the same,0
any point in time of t2,0
Get average cost of the entire track,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
check if it exists:,0
(average_degree / 2) as many edges as there are nodes is required,0
for undirected graphs to reach the target degree.,0
"Attention this order has to be consistent (for training set creation, training, inference etc.)",0
"-> adding (single,bpt) for uniquebodyparts",0
"-> adding (individual,bpt) for multianimalbodyparts",0
load defaults,0
set project specific parameters:,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
###################################################,0
Dependencies,0
###################################################,0
Pose X vs pose Y,0
Poses vs time,0
Likelihoods,0
Histograms,0
#################################################,0
Looping analysis over video,0
#################################################,0
"In a multi animal scenario, show more verbose errors.",0
Some vidoes were not evaluated.,0
Keep only the individuals and bodyparts that were labeled,0
Either display the animals defined in the config if they are found,0
"in the dataframe, or all the trajectories regardless of their names",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Determine whether the data are single- or multi-animal without loading into memory,0
simply by checking whether 'individuals' is in the second line of the CSV.,0
Make paths platform-agnostic if they are not already,0
Ensure folder names are strings,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
make list of full paths,0
filter list of videos,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"if ""MULTI!"" in allbpts:",0
Ensure same order as in config.yaml,0
ABBREVIATE NETWORK NAMES -- esp. for mobilenet!,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
Iterate over data files and stop as soon as one matching the scorer is found,0
If there was no match...,0
aliases for backwards-compatibility.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Set up a logger and add an stdout handler.,0
A single logger can have many handlers:,0
https://docs.python.org/3/howto/logging.html#handler-basic,0
TODO Dump to log file instead,1
"logger = logging.getLogger(""GUI"")",0
logger.setLevel(logging.DEBUG),0
handler = logging.StreamHandler(stream=sys.stdout),0
handler.setLevel(logging.DEBUG),0
formatter = logging.Formatter(,0
"""%(asctime)s - %(name)s - %(levelname)s - %(message)s"", ""%Y-%m-%d %H:%M:%S""",0
),0
handler.setFormatter(formatter),0
logger.addHandler(handler),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
restore the background region,0
Take into consideration imprecise OpenCV estimation of total number of frames,0
Automatically disable the draggable points,0
Save additional frames to the labeled-data directory,0
Store the newly-refined data,0
Uncertain keypoints are ignored,0
Merge with the already existing annotated data,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Thread-safe Stdout redirector,0
Creating action using the first constructor,0
Creating actions using the second constructor,0
File menu,0
View menu,0
Help menu,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
all_bodyparts: List,0
NOTE: Is there a case where a specific list should,0
have bodyparts other than the root? I don't think so.,0
Videotype selection,0
Select videos,0
Number of selected videos text,0
Clear video selection,0
"Qt returns a tuple (list of files, filetype)",0
Add tab header,0
Add separating line,0
Look for any extension by default,0
This works both with e.g. .avi and avi,0
Choose multiple files by default,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Automatically activate the napari-deeplabcut plugin,0
TODO Insert new video,1
TODO Insert skeleton link,1
"Hack to make the first column read-only, as we do not want users to touch it.",1
"The cleaner solution would be to use a QTreeView and QAbstractItemModel,",0
but that is a lot of rework for little benefits.,0
Leave untouched when it is already a string,0
"Slashes also raise the error, but no need to print anything since it is then likely to be a path",0
Walk backwards across parents to get all keys,0
Read the video until a frame is successfully read,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
TODO: Multi video select.... have to change to single video!,1
Shuffle,0
Num animals,0
Min swap length,0
Max gap to fill,0
Trail length,0
Filter type,0
Filter window length,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
Tracker Type,0
Num animals,0
Num triplets,0
layout.addWidget(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
Overwrite videos,0
Trail Points,0
Plot all bodyparts,0
Skeleton,0
Filtered data,0
Plot trajectories,0
High quality video,0
Bodypart list,0
Multianimal scenario.,0
Color is based on individual or bodypart.,0
Single animal scenario.,0
Color is based on bodypart.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Extraction method,0
Frame extraction algorithm,0
Frame cropping,0
Cluster step,0
GUI Slider width,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
use the default pose_cfg file for default values,0
Shuffle,0
Display iterations,0
Save iterations,0
Max iterations,0
Max number snapshots to keep,0
layout.addWidget(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Single / Multi animal Only Layouts,0
Dynamic bodypart cropping,0
Save results as csv,0
Filter predictions,0
Plot Trajectories,0
Show trajectory plots,0
Shuffle,0
layout.addLayout(tmp_layout),0
tmp_layout = QtWidgets.QGridLayout(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
Augmentation method,0
Neural Network,0
TODO: finish model_comparison,1
Check that training data files were indeed created.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Add config text field and button,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
layout.addWidget(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Display all images,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"calling create_tracking_dataset, train_tracking_transformer, stitch_tracklets",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
only try to find these features if they are in the dictionary,0
1) reference to video folder and get the proper bpt_feature file for feature table,0
2) get either the path to gt or the path to track pickle,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
tracklets,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"with npy list form videos, split each to train and test",0
"video_name = '.'.join(video.split(""/"")[-1].split(""."")[:-1])",0
assuming there is only one match,0
make my own model factory,0
make my own loss factory,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights",0
"NOTE: drop path for stochastic depth, we shall see if this is better than dropout here",0
only channel pooling,0
"x -> (B, 12, 2048)",0
"x -> (B, 12, 768)",0
trying to keep everything simple,0
need to write my own keypoint embedding,0
we don't need a fc layer here,0
x: inputs,0
"(B, 12, 768)",0
added pos embed,0
remove all parts I don't understand,0
forwarding through blocks. But num of blocks should be reduced,0
i can't really tell what the difference is,0
will have to change some of these,0
For old models that I trained prior to conv based patchification,0
To resize pos embedding when using model at different size from pretrained weights,0
Rescale the grid of position embeddings when loading from state_dict. Adapted from,0
https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224,0
Cut & paste from PyTorch official master until it's in a few official releases - RW,0
Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf,0
Computes standard normal cumulative distribution function,0
Values are generated by using a truncated uniform distribution and,0
then using the inverse CDF for the normal distribution.,0
Get upper and lower cdf values,0
"Uniformly fill tensor with values from [l, u], then translate to",0
"[2l-1, 2u-1].",0
Use inverse cdf transform for normal distribution to get truncated,0
standard normal,0
"Transform to proper mean, std",0
Clamp to ensure it's in the proper range,0
"type: (Tensor, float, float, float, float) -> Tensor",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2019 Ross Wightman,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Hacked together by / Copyright 2020 Ross Wightman,1
https://github.com/rwightman/pytorch-image-models/blob/main/timm/scheduler/scheduler_factory.py,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"resample if noise out of percent limit, brute force but shouldn't spin much",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
distmat g,0
q    1 3 2 4,0
4 1 2 3,0
"0, 2, 1, 3",0
"1, 2, 3, 0",0
compute cmc curve for each query,0
get query pid and camid,0
remove gallery samples that have the same pid and camid with query,0
compute cmc curve,0
"binary vector, positions with value 1 are correct matches",0
this condition is true when query identity does not appear in gallery,0
compute average precision,0
reference: https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Average_precision,0
"tmp_cmc = [x / (i + 1.) for i, x in enumerate(tmp_cmc)]",0
query,0
gallery,0
"distmat = re_ranking(qf, gf, k1=20, k2=6, lambda_value=0.3)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"if feature vector is numpy, you should use 'torch.tensor' transform it to tensor",0
print('using GPU to compute original distance'),0
print('starting re_ranking'),0
k-reciprocal neighbors,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
More elegantly one can simply define:,0
take care of difference between feature map space and original image space,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
cos = torch.cdist,0
TODO: maybe find a better spot for this.,1
"dev =  device if torch.cuda.is_available() else ""cpu""",0
train,0
print (f'validation loss {val_loss/len(val_loader)}'),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
normalize vectors here,0
i th vector at j th kpt,0
maybe needs to convert them to embeddings and position token,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Drop missing body parts,0
Drop points lying outside the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
## dlcnet_ms5: backbone resnet50 + multi-fusion & multi-stage module,0
## dlcr101_ms5/dlcr152_ms5: backbone resnet101/152 + multi-fusion & multi-stage module,0
"If the graph is unnecessarily large (with 15+ keypoints by default),",0
we randomly prune it to a size guaranteeing an average node degree of 6;,0
"see Suppl. Fig S9c in Lauer et al., 2022.",0
Use the skeleton defined in the config file,0
Ignore possible connections between 'multi' and 'unique' body parts;,0
one can never be too careful...,0
Disable the prediction of PAFs if the graph is empty,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving metadata and data file (Pickle file),0
###############################################################################,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Setting inference cfg file:,0
Search a pose_config.yaml file to parse missing information,0
Clean the training-datasets folder prior to recreating the data pickles,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Load updated lists:,0
Find the corresponding video file,0
"print(key, ""dropping?"")",0
adding important values for multianiaml project:,0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Determine the index length required to guarantee,0
the train–test ratio is exactly the desired one.,0
Pad indices so lengths agree,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(name,test_video_name)",0
Blazing fast and does not load the image into memory,0
Check that points lie within the image,0
DeprecationWarnings are silenced since Python 3.2 unless triggered in __main__,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
updating variable if null/None! #backwardscompatability,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
these values are dropped as scalecrop,0
doesn't have rotation implemented,0
Also drop maDLC smart cropping augmentation parameters,0
sort the model directories,0
get the shuffle index and offset by 1.,0
read cfg file,0
create log file,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Check for variable correctness,0
Moviepy:,0
select crop method,0
"Find the matching video from the config `video_sets`,",0
as it may be stored elsewhere than in the `videos` directory.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
allow_growth must be true here because tensorflow does not automatically free gpu memory and setting it as false occupies all gpu memory so that pytorch cannot kick in,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
should close tensorflow session here in order to free gpu,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"Flip x, y, confidence and reshape",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html#videocapture-get,0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
TODO: add cropping as in video analysis!,1
if cropping is not None:,0
cfg['cropping']=True,0
"cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping",0
"print(""Overwriting cropping parameters:"", cropping)",0
"print(""These are used for all videos, but won't be save to the cfg file."")",0
"Also ensure `boundingboxslack` is greater than zero, otherwise overlap",0
"between trackers cannot be evaluated, resulting in empty tracklets.",0
Check which snapshots are available and sort them by # iterations,0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
NOTE: If dataname line above is changed then line below is obsolete?,0
"trackname = trackname.replace(videofolder, destfolder)",0
TODO: adjust this for multi + unique bodyparts!,1
this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. ),0
Optimal identity assignment based on soft voting,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
imagenet mean for resnet pretraining:,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/DeepLabCut/DeepLabCut/pull/324",0
for DLC 2.2. (here all set False to not use PAFs/pairwise fields),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
"reloading defaults, as they can bleed over from a previous run otherwise",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
dlc_cfg_train = load_config(str(path_train_config)),0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
Name for deeplabcut net (based on its parameters),0
"DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])",0
"print(""Extracting maps for "", DLCscorer, "" with # of trainingiterations:"", trainingsiterations)",0
if notanalyzed: #this only applies to ask if h5 exists...,0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"for multi animal, seems only this is used",0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Get animal IDs and corresponding indices in the arrays of detections,0
Form ground truth beforehand,0
Assemble animals on the full set of detections,0
Count the number of unassembled bodyparts,0
Handle unlabeled bodyparts...,0
"Only 1 animal, let us return the full graph indices only",0
Find minimal skeleton,0
Select optimal PAF graph,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Selective copy; deepcopy is >5x slower,0
Only keeps skeletons that are more than 90% complete,0
TODO Normalize dists by longest length?,1
TODO Smarter imputation technique (Bayesian? Grassmann averages?),1
Covariance matrix estimation fails due to numerical singularities,0
Distance is undefined if the assembly is empty,0
Correct distance to account for missing observations,0
"Alternatively, reduce contribution of missing values to the Mahalanobis",0
distance to zero by substituting the corresponding means.,0
"Fill the subsets with unambiguous, complete individuals",0
Fuse superfluous assemblies,0
Second pass without edge safety,0
Mark the unique body parts as assembled anyway so,0
they are not used later on to fill assemblies.,0
Store selected edges for subsequent frames,0
Remove invalid assemblies,0
Fill assemblies with unconnected body parts,0
Spawning (rather than forking) multiple processes does not,0
work nicely with the GUI or interactive sessions.,0
"In that case, we fall back to the serial assembly.",0
Swap corresponding keypoints,0
Only consider assemblies of at least two keypoints,0
Greedy assembly matching like in pycocotools,0
Global rather than greedy assembly matching,0
Cast columns of dtype 'object' to float to avoid TypeError,0
further down in _parse_ground_truth_data.,0
sigma is taken as the median of all COCO keypoint standard deviations,0
Sort matching score (OKS) in descending order of assembly affinity,0
Guarantee precision decreases monotonically,0
See https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Regularize by forcing AR <= 5,0
max_ar = 5,0
if el.aspect_ratio >= max_ar:,0
if el.height > el.width:,0
el.width = el.height / max_ar,0
else:,0
el.height = el.width / max_ar,0
Orient the ellipse such that it encompasses most points,0
"n_inside = el.contains_points(np.c_[self.x, self.y]).sum()",0
el.theta += 0.5 * np.pi,0
"if el.contains_points(np.c_[self.x, self.y]).sum() < n_inside:",0
el.theta -= 0.5 * np.pi,0
"r2 = chi2.ppf(2 * norm.cdf(sd) - 1, 2)",0
"height, width = np.sqrt(E * r2)",0
The general quadratic curve has the form:,0
ax^2 + 2bxy + cy^2 + 2dx + 2fy + g = 0,0
Ellipse center coordinates,0
Semi-axes lengths,0
Angle to the horizontal,0
High uncertainty to the unobservable initial velocities,0
Give high uncertainty to the unobservable initial velocities,0
diff = val - cost_matrix,0
"diff[row, col] += val",0
if (,0
val < self.iou_threshold,0
or np.any(diff[row] <= 0.2),0
"or np.any(diff[:, col] <= 0.2)",0
):,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
Modified from scipy source code:,0
- to restrict its use to 2D,0
- to get rid of shuffling (since arrays are only (nbodyparts * 3) element long),0
TODO - factor in keypoint confidence (and weight by # of observations??),1
"mat = self.calc_pairwise_oks(poses, poses_ref)",0
Remove matched detections with low OKS,0
matches = [],0
"for row, col in zip(row_indices, col_indices):",0
"if mat[row, col] < self.oks_threshold:",0
unmatched_poses.append(row),0
unmatched_trackers.append(col),0
else:,0
"matches.append([row, col])",0
if not len(matches):,0
"matches = np.empty((0, 2), dtype=int)",0
else:,0
matches = np.stack(matches),0
update matched trackers with assigned detections,0
create and initialise new trackers for unmatched detections,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
filter out matched with low IOU,0
TODO: get cropping parameters and utilize!,1
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Load meta data & annotations,0
get the graph!,0
"NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*",0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
"MA projects have column indices ""scorer"", ""individuals"" and ""bodyparts""",0
"Drop the scorer level, and put individuals in rows",0
The error rows are series; stack in axis 1 and pivot to get DF,0
TODO: Make this code not so redundant!,1
"If a string was passed in, auto-convert to True for backward compatibility",0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to initial folder,0
"# Also storing one ""large"" table with results:",0
note: evaluationfolder.parents[0] to get common folder above all shuffle evaluations.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
#################################################,0
Load data...,0
#################################################,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
TODO: IMPLEMENT for different batch sizes?,1
Ignore best edges possibly defined during a prior evaluation,0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
Pass the image and the keypoints through the resizer;,0
this has no effect if no augmenters were added to it.,0
FIXME Is having an empty array vs nan really that necessary?!,1
"Form 2D array of shape (n_rows, 4) where the last dimension",0
"is (sample_index, peak_y, peak_x, bpt_index) to slice the PAFs.",0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in,0
Compute all distance statistics,0
Calculate overall prediction error,0
Skip data-driven skeleton selection unless,0
the model was trained on the full graph.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
in case there was already a graph,0
have to overwrite this,0
removing the decoding layer from the checkpoint,0
"loading backbone from ResNet, MobileNet etc.",0
"display_iters = max(1,int(displayiters))",0
"[_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})",0
Save snapshot,0
return to original path.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Clip peak locations to PAFs dimensions,0
"unit_vecs = vecs / lengths[:, np.newaxis]",0
"affinities = np.squeeze(y @ np.expand_dims(unit_vecs, axis=2)).sum(axis=1)",0
Form cost matrices,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"loading backbone from ResNet, MobileNet etc.",0
"Auto-switch to Adam on M1/M2 chips, as the momentum optimizer crashes",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Convert a frozen graph to OpenVINO IR format,0
Read network into memory,0
"For better efficiency, model is initialized for batch_size 1 and every sample processed independently",0
Load network to device,0
Prepare input data,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Change the stride from 2 to 1 to get 16x downscaling instead of 32x.,0
https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142,0
Multi stage is currently only implemented for resnets,0
Defining multi_fusion backbone,0
Attaching multi-stage decoder,0
stage_paf_output = stage_paf_output + pre_stage_paf_output,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
FIXME Fix wrong scope with Keras layers,1
"def prediction_layer(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2DTranspose(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=2,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
## New DLCNet Addition: multi-stage decoder,0
"def prediction_layer_stage(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2D(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=1,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Compute variance using: Var[X]= E[X^2] - E[X]^2.,0
Compute keep_prob,0
TODO(tanmingxing): add support for training progress.,1
Compute drop_connect tensor,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
turn into x times y time bs * bpts,0
extract corresponding locref x and y as well as probability,0
Filter predicted heatmaps with a 2D Gaussian kernel as in:,0
https://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_The_Devil_Is_in_the_Details_Delving_Into_Unbiased_Data_CVPR_2020_paper.pdf,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
defaults will be a public argument for namedtuple in Python 3.7,0
https://docs.python.org/3/library/collections.html#collections.namedtuple,0
Make sure that round down does not go down by more than 10%.,0
Builds the block accordings to arguments.,0
Expansion phase:,0
Depth-wise convolution phase:,0
Squeeze and Excitation layer.,0
Output phase:,0
only apply drop_connect if skip presents.,0
Expansion phase:,0
Output phase:,0
only apply drop_connect if skip presents.,0
Builds blocks.,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Stem part.,0
Head part.,0
Calls Stem layers,0
Calls blocks.,0
Calls final layers and returns logits.,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
"(width_coefficient, depth_coefficient, resolution, dropout_rate)",0
blocks_args = [,0
"'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',",0
"'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',",0
"'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',",0
"'r1_k3_s11_e6_i192_o320_se0.25',",0
],0
The default is TPU-specific batch norm.,0
The alternative is tf.layers.BatchNormalization.,0
"batch_norm=utils.TpuBatchNormalization,  # TPU-specific requirement.",0
ValueError will be raised here if override_params has fields not included,0
in global_params.,0
,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrunk when given a depth-multiplier < 1.0.,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Clamp to 40% of crop size to ensure that at least,0
the center keypoint remains visible after the offset is applied.,0
Points located close to one another are sampled preferentially,0
in order to augment crowded regions.,0
Include keypoints in the count to avoid null probabilities,0
Shift the crop center in both dimensions by random amounts,0
and normalize to the original image dimensions.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"for topview, it's safe to mask keypoints under threshold",0
Pickle the 'data' dictionary using the highest protocol available.,0
item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal,0
3 for likelihood,0
we only do single animal here,0
let's hard code it,0
"Add smart, keypoint-aware image cropping",0
"because of the existence of threshold, sampling population is adjusted to len(self.data)",0
empty prediction for this frame,0
in case it's empty prediction,0
Scale is sampled only once (per batch) to transform all of the images into same size.,0
Discard keypoints whose coordinates lie outside the cropped image,0
Invisible joints are represented by nans,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
Produce score maps and location refinement fields,0
Find indices of individuals in joint_id,0
WIP!,0
STD of gaussian is 1/4 of threshold,0
Grid of coordinates,0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Distance between the joint point and each coordinate,0
NEEDS TO BE DONE!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
TODO Finish implementing actual abstract class,1
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
parameter initialization for augmentation pipeline:,0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images of a batch into same size.,0
Approximating the scale,0
invisible joints are represented by nans,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"First, initializing variables (if they don't exist)",0
what is the fraction of training samples with scaling augmentation?,0
loading defaults for rotation range!,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"cfg[""rotateratio""] = cfg.get(",0
"""rotratio"", 0.4",0
)  # what is the fraction of training samples with rotation augmentation?,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"Auto cropping is new (was not in Nature Neuroscience 2018 paper, but introduced in Nath et al. Nat. Protocols 2019)",0
"and boosts performance by 2X, particularly on challenging datasets, like the cheetah in Nath et al.",0
Parameters for augmentation with regard to cropping:,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Get rid of the interpolation beyond the spline knots,0
Data has been filtered so continue to the next video,0
Data haven't been filtered yet,0
Retrieve original individual label indices,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Check the training image paths are correctly stored as arrays of strings,0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
TENSORPACK could fail in WINDOWS...,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
Sometimes tracks cannot be reconstructed as test data are randomly,0
"created; when this happens, we generate a fake h5 data file.",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Loading example data set,0
# Create one identical splits for 3 networks and 3 augmentations,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
Setting specific parameters for training,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"MODELS = [""dlcrnet_ms5"", ""dlcr101_ms5"", ""efficientnet-b0"", ""mobilenet_v2_0.35""]",0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
## adding it here,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Downloading the calibration images,0
Deleting unnecessary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Loading example data set,0
example how to set pose config variables:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
test if existing models are found:,0
Test if nonexisting models are not found,0
Add some other office files:,0
Add a .pickle and .h5 files,0
"By default, all videos with common extensions are taken from a directory",0
A list of extensions can also be passed in,0
"labeled_folders: (has_H5, H5_st_mtime, folder_name)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Test bounding box from a single keypoint,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Test that shape was preserved after vectorization,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Guarantee that images smaller than crop size are handled fine,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Disable stochastic scale jitter,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Verify the aspect ratio is preserved,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Load sample multi animal data,0
Generate a random permutation and reorder data. Ignore the unique bodypart,0
Get inverse permutation and reorder the modified data to get back,0
to the original,0
Check,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"Check data comprise path, shape, and xy coordinates",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Check that we get the number of frames we asked for,0
Check that all indices are valid,0
Check that all frames are unique,0
Check that we get the number of frames we asked for,0
Check that all indices are valid,0
Check that all frames are unique,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Break the graph to test stitching failure,0
Add fake IDs,0
Split all tracklets in half,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"individual_1, individual2",0
"leftArm, rightArm, leftArm, rightArm",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Test flipped keypoints,0
Original (cached) coordinates must have remained empty,0
Assemble based on the smallest graph to speed up testing,0
Generate fake identity predictions,0
Assemble based on the smallest graph to speed up testing,0
Test now with identity only and ensure assemblies,0
contain only parts of a single group ID.,0
Test empty assembly,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Verify that the Hugging Face folder was removed,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
def install(package):,0
"subprocess.check_call([sys.executable, ""-m"", ""pip"", ""install"", package])",0
"install(""tensorflow==1.13.1"")",0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
quicker variant,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Suppress tensorflow warning messages,0
"Train, evaluate & predict functions / all require TF",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Updating config file:,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path = auxfun_models.check_for_weights(pose_cfg['net_type'], parent_path)",0
Updating training and test pose_cfg:,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Read the config file,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
common parameters:,0
Write dictionary to yaml  config file,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
looking for the pseudo label path,0
spatial pyramid is not for adapted model,0
spatial pyramid can still be useful for reducing jittering and quantization error,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
ratio_h and ratio_w should match though in reality it does not match exactly,0
only keep the max,0
empty prediction if pred is not a dict,0
Compute cosine similarity,0
"len(frames) -> (n_scale,)",0
"frames[0].shape - > (batchsize, h, w, 3)",0
no crop needed,0
"batch full, start true inferencing",0
only do this when animal is detected,0
in case we reach the end of the video,0
add a temp folder for checkpoint,0
spatial pyramid can still be useful for reducing jittering and quantization error,0
extra data,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"TODO(stes) mocking a few modules to rely in fewer dependencies, without",1
causing import errors when using deeplabcut.,0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in.,0
TODO(jeylau) add affinity.setter to Assembly,1
TODO(stes): remove/rewrite,1
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"ignore the exception and continue with the next evaluation, without",0
yielding a result value.,0
"return the result value, with NaN as the result for all metrics that",0
could not be computed due to the error.,0
raise the error and stop evaluation,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Read the config file and related variables,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Flatten the list of bodyparts to connect,0
Format data,0
Trick to force equal aspect ratio of 3D plots,0
Set up the matplotlib figure beforehand,0
Set up skeleton LineCollections,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
Get track_method and do related checks,0
Get track method suffix,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
"## Assign nan to [X,Y] of low likelihood predictions ###",0
Convert the data to a np array to easily mask out the low likelihood predictions,0
"Assign [X,Y] = nan to low likelihood predictions",0
Reshape data back to original shape,0
put data back to the dataframes,0
Check individuals are the same in both views,0
Cross-view match individuals,0
Create a dummy variables for single-animal,0
Cleaner variable (since inds view1 == inds view2),0
"Reshape: (num_framex, num_individuals, num_bodyparts , 2)",0
Triangulate data,0
i is individual in view 1,0
voting[i] is the matched individual in view 2,0
Create 3D DataFrame column and row indices,0
Swap num_animals with num_frames axes to ensure well-behaving reshape,0
Fill up 3D dataframe,0
Reorder 2D dataframe in view 2 to match order of view 1,0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no intermediate saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Alternatively,0
Sort tracklets by length to prioritize greater continuity,0
Drop tracklets that are too short,0
"Where slots are available, copy the data over",0
"If about to overwrite data, keep tracklets with highest confidence",0
Squeeze some data into empty slots,0
"For the remaining data, overwrite where we are least confident",0
Find keypoints closest to the remaining data,0
Use Manhattan distance to avoid overflow,0
Only overwrite if improving confidence,0
Fill existing gaps,0
Retrieve original individual label indices,0
Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.,0
Fill existing gaps,0
Retrieve original individual label indices,0
ID swaps occur when X and Y simultaneously intersect each other.,0
Get only those bodypart pairs that belong to different individuals,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"See Li et al., 2012. Cross-view Activity Recognition using Hankelets.",0
"As proposed in the paper, the Hankel matrix can either be formed from",0
the tracklet's centroid or its normalized velocity.,0
"vel = np.diff(self.centroid, axis=0)",0
"vel /= np.linalg.norm(vel, axis=1, keepdims=True)",0
return self.hankelize(vel),0
TODO Add missing data imputation,1
"nrows, ncols = mat.shape",0
beta = nrows / ncols,0
omega = 0.56 * beta ** 3 - 0.95 * beta ** 2 + 1.82 * beta + 1.43,0
return np.argmin(s > omega * np.median(s)),0
"Note that if tracklets are very short, some may actually be part of the same track",0
and thus incorrectly reflect separate track endpoints...,0
"Map each Tracklet to an entry and output nodes and vice versa,",0
which is convenient once the tracklets are stitched.,0
Store tracklets and corresponding negatives (those that overlap in time),0
TODO Avoid looping over all pairs of tracklets,0
Pick the closest (spatially) overlapping tracklet,0
The algorithm works better with integer weights,0
Let us prune the graph by removing all source and sink edges,0
but those connecting the `n_tracks` first and last tracklets.,0
Preflow push seems to work slightly better than shortest,0
"augmentation path..., and is more computationally efficient.",0
Verify whether there are overlapping tracklets,0
"Pick the segment that minimizes ""smoothness"", computed here",0
with the coefficient of variation of the differences.,0
Rebuild a full graph from the remaining nodes without,0
temporal constraint on what tracklets can be stitched together.,0
Cycle through the residuals and incorporate back those,0
that only fit in a single tracklet.,0
Greedily add the remaining residuals,0
Refresh temporal bounds,0
Guarantee track data are sorted in the order defined in the config,0
"If there isn't a track for each animal, fill in the dataframe with NaNs",0
Default to the distance cost function,0
should only exist one,0
Add in identity weighing before building the graph,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implementation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
average distance and average # significant differences avg. over comparisonbodyparts,0
make sure coords and videos are a list,0
can we make a catch here? - in fact we should drop indices from DataCombined,0
if they are in CollectedData.. [ideal behavior; currently pretty unlikely],0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
###################################################,0
Dependencies,0
###################################################,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as,0
specified in the config file,0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
Prepare figure,0
parse the alpha selection function,0
Mask rather than drop unwanted bodyparts to ensure consistent coloring,0
cropping!,0
nx=X2-X1,0
!/usr/bin/env python3,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"more videos are in principle covered, as OpenCV is used and allows many formats.",0
The above already raises a ValueError if formatting is wrong,0
"Rotate, see: https://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg",0
interesting option to just update metadata.,0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
Read the video until a frame is successfully read,0
import platform,0
"if platform.system() == ""Darwin"":  # for OSX use WXAgg",0
fig.canvas.start_event_loop(timeout=-1),0
else:,0
#fig.canvas.stop_event_loop(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"Windows throws error if file path is > 260 characters, can fix with prefix.",0
See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
uses openCV,0
cap.release() >> still used in frame_extraction!,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
This dictionary maps the model types to the file locations where the models exist.,0
Exit the function early if an unknown modeltype is provided.,0
Aliases for backwards-compatibility,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluding the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
Initialize costs matrix,0
get common bodypart detections in track pair,0
add 3rd dim to the points,0
cost for any point in time of t1 being the same,0
any point in time of t2,0
Get average cost of the entire track,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
check if it exists:,0
(average_degree / 2) as many edges as there are nodes is required,0
for undirected graphs to reach the target degree.,0
"Attention this order has to be consistent (for training set creation, training, inference etc.)",0
"-> adding (single,bpt) for uniquebodyparts",0
"-> adding (individual,bpt) for multianimalbodyparts",0
load defaults,0
set project specific parameters:,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
###################################################,0
Dependencies,0
###################################################,0
Pose X vs pose Y,0
Poses vs time,0
Likelihoods,0
Histograms,0
#################################################,0
Looping analysis over video,0
#################################################,0
"In a multi animal scenario, show more verbose errors.",0
Some vidoes were not evaluated.,0
Keep only the individuals and bodyparts that were labeled,0
Either display the animals defined in the config if they are found,0
"in the dataframe, or all the trajectories regardless of their names",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Determine whether the data are single- or multi-animal without loading into memory,0
simply by checking whether 'individuals' is in the second line of the CSV.,0
Make paths platform-agnostic if they are not already,0
Ensure folder names are strings,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
make list of full paths,0
filter list of videos,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"if ""MULTI!"" in allbpts:",0
Ensure same order as in config.yaml,0
ABBREVIATE NETWORK NAMES -- esp. for mobilenet!,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
Iterate over data files and stop as soon as one matching the scorer is found,0
If there was no match...,0
aliases for backwards-compatibility.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Set up a logger and add an stdout handler.,0
A single logger can have many handlers:,0
https://docs.python.org/3/howto/logging.html#handler-basic,0
TODO Dump to log file instead,1
"logger = logging.getLogger(""GUI"")",0
logger.setLevel(logging.DEBUG),0
handler = logging.StreamHandler(stream=sys.stdout),0
handler.setLevel(logging.DEBUG),0
formatter = logging.Formatter(,0
"""%(asctime)s - %(name)s - %(levelname)s - %(message)s"", ""%Y-%m-%d %H:%M:%S""",0
),0
handler.setFormatter(formatter),0
logger.addHandler(handler),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
restore the background region,0
Take into consideration imprecise OpenCV estimation of total number of frames,0
Automatically disable the draggable points,0
Save additional frames to the labeled-data directory,0
Store the newly-refined data,0
Uncertain keypoints are ignored,0
Merge with the already existing annotated data,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Thread-safe Stdout redirector,0
Creating action using the first constructor,0
Creating actions using the second constructor,0
File menu,0
View menu,0
Help menu,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
all_bodyparts: List,0
NOTE: Is there a case where a specific list should,0
have bodyparts other than the root? I don't think so.,0
Videotype selection,0
Select videos,0
Number of selected videos text,0
Clear video selection,0
"Qt returns a tuple (list of files, filetype)",0
Add tab header,0
Add separating line,0
Look for any extension by default,0
This works both with e.g. .avi and avi,0
Choose multiple files by default,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Automatically activate the napari-deeplabcut plugin,0
TODO Insert new video,1
TODO Insert skeleton link,1
"Hack to make the first column read-only, as we do not want users to touch it.",1
"The cleaner solution would be to use a QTreeView and QAbstractItemModel,",0
but that is a lot of rework for little benefits.,0
Leave untouched when it is already a string,0
"Slashes also raise the error, but no need to print anything since it is then likely to be a path",0
Walk backwards across parents to get all keys,0
Read the video until a frame is successfully read,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
TODO: Multi video select.... have to change to single video!,1
Shuffle,0
Num animals,0
Min swap length,0
Max gap to fill,0
Trail length,0
Filter type,0
Filter window length,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
Tracker Type,0
Num animals,0
Num triplets,0
layout.addWidget(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
Overwrite videos,0
Trail Points,0
Plot all bodyparts,0
Skeleton,0
Filtered data,0
Plot trajectories,0
High quality video,0
Bodypart list,0
Multianimal scenario.,0
Color is based on individual or bodypart.,0
Single animal scenario.,0
Color is based on bodypart.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Extraction method,0
Frame extraction algorithm,0
Frame cropping,0
Cluster step,0
GUI Slider width,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
use the default pose_cfg file for default values,0
Shuffle,0
Display iterations,0
Save iterations,0
Max iterations,0
Max number snapshots to keep,0
layout.addWidget(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Single / Multi animal Only Layouts,0
Dynamic bodypart cropping,0
Save results as csv,0
Filter predictions,0
Plot Trajectories,0
Show trajectory plots,0
Shuffle,0
layout.addLayout(tmp_layout),0
tmp_layout = QtWidgets.QGridLayout(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
Augmentation method,0
Neural Network,0
TODO: finish model_comparison,1
Check that training data files were indeed created.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Add config text field and button,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
layout.addWidget(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Display all images,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"calling create_tracking_dataset, train_tracking_transformer, stitch_tracklets",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
only try to find these features if they are in the dictionary,0
1) reference to video folder and get the proper bpt_feature file for feature table,0
2) get either the path to gt or the path to track pickle,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
tracklets,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"with npy list form videos, split each to train and test",0
"video_name = '.'.join(video.split(""/"")[-1].split(""."")[:-1])",0
assuming there is only one match,0
make my own model factory,0
make my own loss factory,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights",0
"NOTE: drop path for stochastic depth, we shall see if this is better than dropout here",0
only channel pooling,0
"x -> (B, 12, 2048)",0
"x -> (B, 12, 768)",0
trying to keep everything simple,0
need to write my own keypoint embedding,0
we don't need a fc layer here,0
x: inputs,0
"(B, 12, 768)",0
added pos embed,0
remove all parts I don't understand,0
forwarding through blocks. But num of blocks should be reduced,0
i can't really tell what the difference is,0
will have to change some of these,0
For old models that I trained prior to conv based patchification,0
To resize pos embedding when using model at different size from pretrained weights,0
Rescale the grid of position embeddings when loading from state_dict. Adapted from,0
https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224,0
Cut & paste from PyTorch official master until it's in a few official releases - RW,0
Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf,0
Computes standard normal cumulative distribution function,0
Values are generated by using a truncated uniform distribution and,0
then using the inverse CDF for the normal distribution.,0
Get upper and lower cdf values,0
"Uniformly fill tensor with values from [l, u], then translate to",0
"[2l-1, 2u-1].",0
Use inverse cdf transform for normal distribution to get truncated,0
standard normal,0
"Transform to proper mean, std",0
Clamp to ensure it's in the proper range,0
"type: (Tensor, float, float, float, float) -> Tensor",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2019 Ross Wightman,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Hacked together by / Copyright 2020 Ross Wightman,1
https://github.com/rwightman/pytorch-image-models/blob/main/timm/scheduler/scheduler_factory.py,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"resample if noise out of percent limit, brute force but shouldn't spin much",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
distmat g,0
q    1 3 2 4,0
4 1 2 3,0
"0, 2, 1, 3",0
"1, 2, 3, 0",0
compute cmc curve for each query,0
get query pid and camid,0
remove gallery samples that have the same pid and camid with query,0
compute cmc curve,0
"binary vector, positions with value 1 are correct matches",0
this condition is true when query identity does not appear in gallery,0
compute average precision,0
reference: https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Average_precision,0
"tmp_cmc = [x / (i + 1.) for i, x in enumerate(tmp_cmc)]",0
query,0
gallery,0
"distmat = re_ranking(qf, gf, k1=20, k2=6, lambda_value=0.3)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"if feature vector is numpy, you should use 'torch.tensor' transform it to tensor",0
print('using GPU to compute original distance'),0
print('starting re_ranking'),0
k-reciprocal neighbors,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
More elegantly one can simply define:,0
take care of difference between feature map space and original image space,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
cos = torch.cdist,0
TODO: maybe find a better spot for this.,1
"dev =  device if torch.cuda.is_available() else ""cpu""",0
train,0
print (f'validation loss {val_loss/len(val_loader)}'),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
normalize vectors here,0
i th vector at j th kpt,0
maybe needs to convert them to embeddings and position token,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Drop missing body parts,0
Drop points lying outside the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
## dlcnet_ms5: backbone resnet50 + multi-fusion & multi-stage module,0
## dlcr101_ms5/dlcr152_ms5: backbone resnet101/152 + multi-fusion & multi-stage module,0
"If the graph is unnecessarily large (with 15+ keypoints by default),",0
we randomly prune it to a size guaranteeing an average node degree of 6;,0
"see Suppl. Fig S9c in Lauer et al., 2022.",0
Use the skeleton defined in the config file,0
Ignore possible connections between 'multi' and 'unique' body parts;,0
one can never be too careful...,0
Disable the prediction of PAFs if the graph is empty,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving metadata and data file (Pickle file),0
###############################################################################,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Setting inference cfg file:,0
Search a pose_config.yaml file to parse missing information,0
Clean the training-datasets folder prior to recreating the data pickles,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Load updated lists:,0
Find the corresponding video file,0
"print(key, ""dropping?"")",0
adding important values for multianiaml project:,0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Determine the index length required to guarantee,0
the train–test ratio is exactly the desired one.,0
Pad indices so lengths agree,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(name,test_video_name)",0
Blazing fast and does not load the image into memory,0
Check that points lie within the image,0
DeprecationWarnings are silenced since Python 3.2 unless triggered in __main__,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
updating variable if null/None! #backwardscompatability,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
these values are dropped as scalecrop,0
doesn't have rotation implemented,0
Also drop maDLC smart cropping augmentation parameters,0
sort the model directories,0
get the shuffle index and offset by 1.,0
read cfg file,0
create log file,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Check for variable correctness,0
Moviepy:,0
select crop method,0
"Find the matching video from the config `video_sets`,",0
as it may be stored elsewhere than in the `videos` directory.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
allow_growth must be true here because tensorflow does not automatically free gpu memory and setting it as false occupies all gpu memory so that pytorch cannot kick in,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
should close tensorflow session here in order to free gpu,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"Flip x, y, confidence and reshape",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html#videocapture-get,0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
TODO: add cropping as in video analysis!,1
if cropping is not None:,0
cfg['cropping']=True,0
"cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping",0
"print(""Overwriting cropping parameters:"", cropping)",0
"print(""These are used for all videos, but won't be save to the cfg file."")",0
"Also ensure `boundingboxslack` is greater than zero, otherwise overlap",0
"between trackers cannot be evaluated, resulting in empty tracklets.",0
Check which snapshots are available and sort them by # iterations,0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
NOTE: If dataname line above is changed then line below is obsolete?,0
"trackname = trackname.replace(videofolder, destfolder)",0
TODO: adjust this for multi + unique bodyparts!,1
this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. ),0
Optimal identity assignment based on soft voting,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
imagenet mean for resnet pretraining:,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/DeepLabCut/DeepLabCut/pull/324",0
for DLC 2.2. (here all set False to not use PAFs/pairwise fields),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
"reloading defaults, as they can bleed over from a previous run otherwise",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
dlc_cfg_train = load_config(str(path_train_config)),0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
Name for deeplabcut net (based on its parameters),0
"DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])",0
"print(""Extracting maps for "", DLCscorer, "" with # of trainingiterations:"", trainingsiterations)",0
if notanalyzed: #this only applies to ask if h5 exists...,0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"for multi animal, seems only this is used",0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Get animal IDs and corresponding indices in the arrays of detections,0
Form ground truth beforehand,0
Assemble animals on the full set of detections,0
Count the number of unassembled bodyparts,0
Handle unlabeled bodyparts...,0
"Only 1 animal, let us return the full graph indices only",0
Find minimal skeleton,0
Select optimal PAF graph,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Selective copy; deepcopy is >5x slower,0
Only keeps skeletons that are more than 90% complete,0
TODO Normalize dists by longest length?,1
TODO Smarter imputation technique (Bayesian? Grassmann averages?),1
Covariance matrix estimation fails due to numerical singularities,0
Distance is undefined if the assembly is empty,0
Correct distance to account for missing observations,0
"Alternatively, reduce contribution of missing values to the Mahalanobis",0
distance to zero by substituting the corresponding means.,0
"Fill the subsets with unambiguous, complete individuals",0
Fuse superfluous assemblies,0
Second pass without edge safety,0
Mark the unique body parts as assembled anyway so,0
they are not used later on to fill assemblies.,0
Store selected edges for subsequent frames,0
Remove invalid assemblies,0
Fill assemblies with unconnected body parts,0
Spawning (rather than forking) multiple processes does not,0
work nicely with the GUI or interactive sessions.,0
"In that case, we fall back to the serial assembly.",0
Swap corresponding keypoints,0
Only consider assemblies of at least two keypoints,0
Greedy assembly matching like in pycocotools,0
Global rather than greedy assembly matching,0
Cast columns of dtype 'object' to float to avoid TypeError,0
further down in _parse_ground_truth_data.,0
sigma is taken as the median of all COCO keypoint standard deviations,0
Sort matching score (OKS) in descending order of assembly affinity,0
Guarantee precision decreases monotonically,0
See https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Regularize by forcing AR <= 5,0
max_ar = 5,0
if el.aspect_ratio >= max_ar:,0
if el.height > el.width:,0
el.width = el.height / max_ar,0
else:,0
el.height = el.width / max_ar,0
Orient the ellipse such that it encompasses most points,0
"n_inside = el.contains_points(np.c_[self.x, self.y]).sum()",0
el.theta += 0.5 * np.pi,0
"if el.contains_points(np.c_[self.x, self.y]).sum() < n_inside:",0
el.theta -= 0.5 * np.pi,0
"r2 = chi2.ppf(2 * norm.cdf(sd) - 1, 2)",0
"height, width = np.sqrt(E * r2)",0
The general quadratic curve has the form:,0
ax^2 + 2bxy + cy^2 + 2dx + 2fy + g = 0,0
Ellipse center coordinates,0
Semi-axes lengths,0
Angle to the horizontal,0
High uncertainty to the unobservable initial velocities,0
Give high uncertainty to the unobservable initial velocities,0
diff = val - cost_matrix,0
"diff[row, col] += val",0
if (,0
val < self.iou_threshold,0
or np.any(diff[row] <= 0.2),0
"or np.any(diff[:, col] <= 0.2)",0
):,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
Modified from scipy source code:,0
- to restrict its use to 2D,0
- to get rid of shuffling (since arrays are only (nbodyparts * 3) element long),0
TODO - factor in keypoint confidence (and weight by # of observations??),1
"mat = self.calc_pairwise_oks(poses, poses_ref)",0
Remove matched detections with low OKS,0
matches = [],0
"for row, col in zip(row_indices, col_indices):",0
"if mat[row, col] < self.oks_threshold:",0
unmatched_poses.append(row),0
unmatched_trackers.append(col),0
else:,0
"matches.append([row, col])",0
if not len(matches):,0
"matches = np.empty((0, 2), dtype=int)",0
else:,0
matches = np.stack(matches),0
update matched trackers with assigned detections,0
create and initialise new trackers for unmatched detections,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
filter out matched with low IOU,0
TODO: get cropping parameters and utilize!,1
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Load meta data & annotations,0
get the graph!,0
"NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*",0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
"MA projects have column indices ""scorer"", ""individuals"" and ""bodyparts""",0
"Drop the scorer level, and put individuals in rows",0
The error rows are series; stack in axis 1 and pivot to get DF,0
TODO: Make this code not so redundant!,1
"If a string was passed in, auto-convert to True for backward compatibility",0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to initial folder,0
"# Also storing one ""large"" table with results:",0
note: evaluationfolder.parents[0] to get common folder above all shuffle evaluations.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
#################################################,0
Load data...,0
#################################################,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
TODO: IMPLEMENT for different batch sizes?,1
Ignore best edges possibly defined during a prior evaluation,0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
Pass the image and the keypoints through the resizer;,0
this has no effect if no augmenters were added to it.,0
FIXME Is having an empty array vs nan really that necessary?!,1
"Form 2D array of shape (n_rows, 4) where the last dimension",0
"is (sample_index, peak_y, peak_x, bpt_index) to slice the PAFs.",0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in,0
Compute all distance statistics,0
Calculate overall prediction error,0
Skip data-driven skeleton selection unless,0
the model was trained on the full graph.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
in case there was already a graph,0
have to overwrite this,0
"loading backbone from ResNet, MobileNet etc.",0
"display_iters = max(1,int(displayiters))",0
"[_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})",0
Save snapshot,0
return to original path.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Clip peak locations to PAFs dimensions,0
"unit_vecs = vecs / lengths[:, np.newaxis]",0
"affinities = np.squeeze(y @ np.expand_dims(unit_vecs, axis=2)).sum(axis=1)",0
Form cost matrices,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"loading backbone from ResNet, MobileNet etc.",0
"Auto-switch to Adam on M1/M2 chips, as the momentum optimizer crashes",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Convert a frozen graph to OpenVINO IR format,0
Read network into memory,0
"For better efficiency, model is initialized for batch_size 1 and every sample processed independently",0
Load network to device,0
Prepare input data,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Change the stride from 2 to 1 to get 16x downscaling instead of 32x.,0
https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142,0
Multi stage is currently only implemented for resnets,0
Defining multi_fusion backbone,0
Attaching multi-stage decoder,0
stage_paf_output = stage_paf_output + pre_stage_paf_output,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
FIXME Fix wrong scope with Keras layers,1
"def prediction_layer(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2DTranspose(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=2,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
## New DLCNet Addition: multi-stage decoder,0
"def prediction_layer_stage(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2D(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=1,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Compute variance using: Var[X]= E[X^2] - E[X]^2.,0
Compute keep_prob,0
TODO(tanmingxing): add support for training progress.,1
Compute drop_connect tensor,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
turn into x times y time bs * bpts,0
extract corresponding locref x and y as well as probability,0
Filter predicted heatmaps with a 2D Gaussian kernel as in:,0
https://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_The_Devil_Is_in_the_Details_Delving_Into_Unbiased_Data_CVPR_2020_paper.pdf,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
defaults will be a public argument for namedtuple in Python 3.7,0
https://docs.python.org/3/library/collections.html#collections.namedtuple,0
Make sure that round down does not go down by more than 10%.,0
Builds the block accordings to arguments.,0
Expansion phase:,0
Depth-wise convolution phase:,0
Squeeze and Excitation layer.,0
Output phase:,0
only apply drop_connect if skip presents.,0
Expansion phase:,0
Output phase:,0
only apply drop_connect if skip presents.,0
Builds blocks.,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Stem part.,0
Head part.,0
Calls Stem layers,0
Calls blocks.,0
Calls final layers and returns logits.,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
"(width_coefficient, depth_coefficient, resolution, dropout_rate)",0
blocks_args = [,0
"'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',",0
"'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',",0
"'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',",0
"'r1_k3_s11_e6_i192_o320_se0.25',",0
],0
The default is TPU-specific batch norm.,0
The alternative is tf.layers.BatchNormalization.,0
"batch_norm=utils.TpuBatchNormalization,  # TPU-specific requirement.",0
ValueError will be raised here if override_params has fields not included,0
in global_params.,0
,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrunk when given a depth-multiplier < 1.0.,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Clamp to 40% of crop size to ensure that at least,0
the center keypoint remains visible after the offset is applied.,0
Points located close to one another are sampled preferentially,0
in order to augment crowded regions.,0
Include keypoints in the count to avoid null probabilities,0
Shift the crop center in both dimensions by random amounts,0
and normalize to the original image dimensions.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"for topview, it's safe to mask keypoints under threshold",0
Pickle the 'data' dictionary using the highest protocol available.,0
item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal,0
3 for likelihood,0
we only do single animal here,0
let's hard code it,0
"Add smart, keypoint-aware image cropping",0
"because of the existence of threshold, sampling population is adjusted to len(self.data)",0
empty prediction for this frame,0
in case it's empty prediction,0
Scale is sampled only once (per batch) to transform all of the images into same size.,0
Discard keypoints whose coordinates lie outside the cropped image,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
Produce score maps and location refinement fields,0
Find indices of individuals in joint_id,0
WIP!,0
STD of gaussian is 1/4 of threshold,0
Grid of coordinates,0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Distance between the joint point and each coordinate,0
NEEDS TO BE DONE!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
TODO Finish implementing actual abstract class,1
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
parameter initialization for augmentation pipeline:,0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images of a batch into same size.,0
Approximating the scale,0
invisible joints are represented by nans,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"First, initializing variables (if they don't exist)",0
what is the fraction of training samples with scaling augmentation?,0
loading defaults for rotation range!,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"cfg[""rotateratio""] = cfg.get(",0
"""rotratio"", 0.4",0
)  # what is the fraction of training samples with rotation augmentation?,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"Auto cropping is new (was not in Nature Neuroscience 2018 paper, but introduced in Nath et al. Nat. Protocols 2019)",0
"and boosts performance by 2X, particularly on challenging datasets, like the cheetah in Nath et al.",0
Parameters for augmentation with regard to cropping:,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Get rid of the interpolation beyond the spline knots,0
Retrieve original individual label indices,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Check the training image paths are correctly stored as arrays of strings,0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
TENSORPACK could fail in WINDOWS...,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
Sometimes tracks cannot be reconstructed as test data are randomly,0
"created; when this happens, we generate a fake h5 data file.",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Loading example data set,0
# Create one identical splits for 3 networks and 3 augmentations,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
Setting specific parameters for training,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"MODELS = [""dlcrnet_ms5"", ""dlcr101_ms5"", ""efficientnet-b0"", ""mobilenet_v2_0.35""]",0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
## adding it here,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Downloading the calibration images,0
Deleting unnecessary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Loading example data set,0
example how to set pose config variables:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
test if existing models are found:,0
Test if nonexisting models are not found,0
Add some other office files:,0
Add a .pickle and .h5 files,0
"By default, all videos with common extensions are taken from a directory",0
A list of extensions can also be passed in,0
"labeled_folders: (has_H5, H5_st_mtime, folder_name)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Test bounding box from a single keypoint,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Test that shape was preserved after vectorization,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Guarantee that images smaller than crop size are handled fine,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Disable stochastic scale jitter,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Verify the aspect ratio is preserved,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Load sample multi animal data,0
Generate a random permutation and reorder data. Ignore the unique bodypart,0
Get inverse permutation and reorder the modified data to get back,0
to the original,0
Check,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"Check data comprise path, shape, and xy coordinates",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Check that we get the number of frames we asked for,0
Check that all indices are valid,0
Check that all frames are unique,0
Check that we get the number of frames we asked for,0
Check that all indices are valid,0
Check that all frames are unique,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Break the graph to test stitching failure,0
Add fake IDs,0
Split all tracklets in half,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"individual_1, individual2",0
"leftArm, rightArm, leftArm, rightArm",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Test flipped keypoints,0
Original (cached) coordinates must have remained empty,0
Assemble based on the smallest graph to speed up testing,0
Generate fake identity predictions,0
Assemble based on the smallest graph to speed up testing,0
Test now with identity only and ensure assemblies,0
contain only parts of a single group ID.,0
Test empty assembly,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Verify that the Hugging Face folder was removed,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
def install(package):,0
"subprocess.check_call([sys.executable, ""-m"", ""pip"", ""install"", package])",0
"install(""tensorflow==1.13.1"")",0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
quicker variant,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Suppress tensorflow warning messages,0
"Train, evaluate & predict functions / all require TF",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Updating config file:,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path = auxfun_models.check_for_weights(pose_cfg['net_type'], parent_path)",0
Updating training and test pose_cfg:,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Read the config file,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
common parameters:,0
Write dictionary to yaml  config file,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
looking for the pseudo label path,0
spatial pyramid is not for adapted model,0
spatial pyramid can still be useful for reducing jittering and quantization error,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
ratio_h and ratio_w should match though in reality it does not match exactly,0
only keep the max,0
empty prediction if pred is not a dict,0
Compute cosine similarity,0
"len(frames) -> (n_scale,)",0
"frames[0].shape - > (batchsize, h, w, 3)",0
no crop needed,0
"batch full, start true inferencing",0
only do this when animal is detected,0
in case we reach the end of the video,0
add a temp folder for checkpoint,0
spatial pyramid can still be useful for reducing jittering and quantization error,0
extra data,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"TODO(stes) mocking a few modules to rely in fewer dependencies, without",1
causing import errors when using deeplabcut.,0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in.,0
TODO(jeylau) add affinity.setter to Assembly,1
TODO(stes): remove/rewrite,1
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"ignore the exception and continue with the next evaluation, without",0
yielding a result value.,0
"return the result value, with NaN as the result for all metrics that",0
could not be computed due to the error.,0
raise the error and stop evaluation,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Read the config file and related variables,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Flatten the list of bodyparts to connect,0
Format data,0
Trick to force equal aspect ratio of 3D plots,0
Set up the matplotlib figure beforehand,0
Set up skeleton LineCollections,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
Get track_method and do related checks,0
Get track method suffix,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
"## Assign nan to [X,Y] of low likelihood predictions ###",0
Convert the data to a np array to easily mask out the low likelihood predictions,0
"Assign [X,Y] = nan to low likelihood predictions",0
Reshape data back to original shape,0
put data back to the dataframes,0
Check individuals are the same in both views,0
Cross-view match individuals,0
Create a dummy variables for single-animal,0
Cleaner variable (since inds view1 == inds view2),0
"Reshape: (num_framex, num_individuals, num_bodyparts , 2)",0
Triangulate data,0
i is individual in view 1,0
voting[i] is the matched individual in view 2,0
Create 3D DataFrame column and row indices,0
Swap num_animals with num_frames axes to ensure well-behaving reshape,0
Fill up 3D dataframe,0
Reorder 2D dataframe in view 2 to match order of view 1,0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no intermediate saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Alternatively,0
Sort tracklets by length to prioritize greater continuity,0
Drop tracklets that are too short,0
"Where slots are available, copy the data over",0
"If about to overwrite data, keep tracklets with highest confidence",0
Squeeze some data into empty slots,0
"For the remaining data, overwrite where we are least confident",0
Find keypoints closest to the remaining data,0
Use Manhattan distance to avoid overflow,0
Only overwrite if improving confidence,0
Fill existing gaps,0
Retrieve original individual label indices,0
Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.,0
Fill existing gaps,0
Retrieve original individual label indices,0
ID swaps occur when X and Y simultaneously intersect each other.,0
Get only those bodypart pairs that belong to different individuals,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"See Li et al., 2012. Cross-view Activity Recognition using Hankelets.",0
"As proposed in the paper, the Hankel matrix can either be formed from",0
the tracklet's centroid or its normalized velocity.,0
"vel = np.diff(self.centroid, axis=0)",0
"vel /= np.linalg.norm(vel, axis=1, keepdims=True)",0
return self.hankelize(vel),0
TODO Add missing data imputation,1
"nrows, ncols = mat.shape",0
beta = nrows / ncols,0
omega = 0.56 * beta ** 3 - 0.95 * beta ** 2 + 1.82 * beta + 1.43,0
return np.argmin(s > omega * np.median(s)),0
"Note that if tracklets are very short, some may actually be part of the same track",0
and thus incorrectly reflect separate track endpoints...,0
"Map each Tracklet to an entry and output nodes and vice versa,",0
which is convenient once the tracklets are stitched.,0
Store tracklets and corresponding negatives (those that overlap in time),0
TODO Avoid looping over all pairs of tracklets,0
Pick the closest (spatially) overlapping tracklet,0
The algorithm works better with integer weights,0
Let us prune the graph by removing all source and sink edges,0
but those connecting the `n_tracks` first and last tracklets.,0
Preflow push seems to work slightly better than shortest,0
"augmentation path..., and is more computationally efficient.",0
Verify whether there are overlapping tracklets,0
"Pick the segment that minimizes ""smoothness"", computed here",0
with the coefficient of variation of the differences.,0
Rebuild a full graph from the remaining nodes without,0
temporal constraint on what tracklets can be stitched together.,0
Cycle through the residuals and incorporate back those,0
that only fit in a single tracklet.,0
Greedily add the remaining residuals,0
Refresh temporal bounds,0
Guarantee track data are sorted in the order defined in the config,0
"If there isn't a track for each animal, fill in the dataframe with NaNs",0
Default to the distance cost function,0
should only exist one,0
Add in identity weighing before building the graph,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implementation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
average distance and average # significant differences avg. over comparisonbodyparts,0
make sure coords and videos are a list,0
can we make a catch here? - in fact we should drop indices from DataCombined,0
if they are in CollectedData.. [ideal behavior; currently pretty unlikely],0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
###################################################,0
Dependencies,0
###################################################,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as,0
specified in the config file,0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
Prepare figure,0
parse the alpha selection function,0
Mask rather than drop unwanted bodyparts to ensure consistent coloring,0
cropping!,0
nx=X2-X1,0
!/usr/bin/env python3,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"more videos are in principle covered, as OpenCV is used and allows many formats.",0
The above already raises a ValueError if formatting is wrong,0
"Rotate, see: https://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg",0
interesting option to just update metadata.,0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
Read the video until a frame is successfully read,0
import platform,0
"if platform.system() == ""Darwin"":  # for OSX use WXAgg",0
fig.canvas.start_event_loop(timeout=-1),0
else:,0
#fig.canvas.stop_event_loop(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"Windows throws error if file path is > 260 characters, can fix with prefix.",0
See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
uses openCV,0
cap.release() >> still used in frame_extraction!,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
This dictionary maps the model types to the file locations where the models exist.,0
Exit the function early if an unknown modeltype is provided.,0
Aliases for backwards-compatibility,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
Initialize costs matrix,0
get common bodypart detections in track pair,0
add 3rd dim to the points,0
cost for any point in time of t1 being the same,0
any point in time of t2,0
Get average cost of the entire track,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
check if it exists:,0
(average_degree / 2) as many edges as there are nodes is required,0
for undirected graphs to reach the target degree.,0
"Attention this order has to be consistent (for training set creation, training, inference etc.)",0
"-> adding (single,bpt) for uniquebodyparts",0
"-> adding (individual,bpt) for multianimalbodyparts",0
load defaults,0
set project specific parameters:,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
###################################################,0
Dependencies,0
###################################################,0
Pose X vs pose Y,0
Poses vs time,0
Likelihoods,0
Histograms,0
#################################################,0
Looping analysis over video,0
#################################################,0
"In a multi animal scenario, show more verbose errors.",0
Some vidoes were not evaluated.,0
Keep only the individuals and bodyparts that were labeled,0
Either display the animals defined in the config if they are found,0
"in the dataframe, or all the trajectories regardless of their names",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Determine whether the data are single- or multi-animal without loading into memory,0
simply by checking whether 'individuals' is in the second line of the CSV.,0
Make paths platform-agnostic if they are not already,0
Ensure folder names are strings,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
make list of full paths,0
filter list of videos,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"if ""MULTI!"" in allbpts:",0
Ensure same order as in config.yaml,0
ABBREVIATE NETWORK NAMES -- esp. for mobilenet!,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
Iterate over data files and stop as soon as one matching the scorer is found,0
If there was no match...,0
aliases for backwards-compatibility.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Set up a logger and add an stdout handler.,0
A single logger can have many handlers:,0
https://docs.python.org/3/howto/logging.html#handler-basic,0
TODO Dump to log file instead,1
"logger = logging.getLogger(""GUI"")",0
logger.setLevel(logging.DEBUG),0
handler = logging.StreamHandler(stream=sys.stdout),0
handler.setLevel(logging.DEBUG),0
formatter = logging.Formatter(,0
"""%(asctime)s - %(name)s - %(levelname)s - %(message)s"", ""%Y-%m-%d %H:%M:%S""",0
),0
handler.setFormatter(formatter),0
logger.addHandler(handler),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
restore the background region,0
Take into consideration imprecise OpenCV estimation of total number of frames,0
Automatically disable the draggable points,0
Save additional frames to the labeled-data directory,0
Store the newly-refined data,0
Uncertain keypoints are ignored,0
Merge with the already existing annotated data,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Thread-safe Stdout redirector,0
Creating action using the first constructor,0
Creating actions using the second constructor,0
File menu,0
View menu,0
Help menu,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
all_bodyparts: List,0
NOTE: Is there a case where a specific list should,0
have bodyparts other than the root? I don't think so.,0
Videotype selection,0
Select videos,0
Number of selected videos text,0
Clear video selection,0
"Qt returns a tuple (list of files, filetype)",0
Add tab header,0
Add separating line,0
Look for any extension by default,0
This works both with e.g. .avi and avi,0
Choose multiple files by default,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Automatically activate the napari-deeplabcut plugin,0
TODO Insert new video,1
TODO Insert skeleton link,1
"Hack to make the first column read-only, as we do not want users to touch it.",1
"The cleaner solution would be to use a QTreeView and QAbstractItemModel,",0
but that is a lot of rework for little benefits.,0
Leave untouched when it is already a string,0
"Slashes also raise the error, but no need to print anything since it is then likely to be a path",0
Walk backwards across parents to get all keys,0
Read the video until a frame is successfully read,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
TODO: Multi video select.... have to change to single video!,1
Shuffle,0
Num animals,0
Min swap length,0
Max gap to fill,0
Trail length,0
Filter type,0
Filter window length,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
Tracker Type,0
Num animals,0
Num triplets,0
layout.addWidget(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
Overwrite videos,0
Trail Points,0
Plot all bodyparts,0
Skeleton,0
Filtered data,0
Plot trajectories,0
High quality video,0
Bodypart list,0
Multianimal scenario.,0
Color is based on individual or bodypart.,0
Single animal scenario.,0
Color is based on bodypart.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Extraction method,0
Frame extraction algorithm,0
Frame cropping,0
Cluster step,0
GUI Slider width,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
use the default pose_cfg file for default values,0
Shuffle,0
Display iterations,0
Save iterations,0
Max iterations,0
Max number snapshots to keep,0
layout.addWidget(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Single / Multi animal Only Layouts,0
Dynamic bodypart cropping,0
Save results as csv,0
Filter predictions,0
Plot Trajectories,0
Show trajectory plots,0
Shuffle,0
layout.addLayout(tmp_layout),0
tmp_layout = QtWidgets.QGridLayout(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
Augmentation method,0
Neural Network,0
TODO: finish model_comparison,1
Check that training data files were indeed created.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Add config text field and button,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
layout.addWidget(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Display all images,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"calling create_tracking_dataset, train_tracking_transformer, stitch_tracklets",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
only try to find these features if they are in the dictionary,0
1) reference to video folder and get the proper bpt_feature file for feature table,0
2) get either the path to gt or the path to track pickle,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
tracklets,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"with npy list form videos, split each to train and test",0
"video_name = '.'.join(video.split(""/"")[-1].split(""."")[:-1])",0
assuming there is only one match,0
make my own model factory,0
make my own loss factory,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights",0
"NOTE: drop path for stochastic depth, we shall see if this is better than dropout here",0
only channel pooling,0
"x -> (B, 12, 2048)",0
"x -> (B, 12, 768)",0
trying to keep everything simple,0
need to write my own keypoint embedding,0
we don't need a fc layer here,0
x: inputs,0
"(B, 12, 768)",0
added pos embed,0
remove all parts I don't understand,0
forwarding through blocks. But num of blocks should be reduced,0
i can't really tell what the difference is,0
will have to change some of these,0
For old models that I trained prior to conv based patchification,0
To resize pos embedding when using model at different size from pretrained weights,0
Rescale the grid of position embeddings when loading from state_dict. Adapted from,0
https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224,0
Cut & paste from PyTorch official master until it's in a few official releases - RW,0
Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf,0
Computes standard normal cumulative distribution function,0
Values are generated by using a truncated uniform distribution and,0
then using the inverse CDF for the normal distribution.,0
Get upper and lower cdf values,0
"Uniformly fill tensor with values from [l, u], then translate to",0
"[2l-1, 2u-1].",0
Use inverse cdf transform for normal distribution to get truncated,0
standard normal,0
"Transform to proper mean, std",0
Clamp to ensure it's in the proper range,0
"type: (Tensor, float, float, float, float) -> Tensor",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2019 Ross Wightman,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Hacked together by / Copyright 2020 Ross Wightman,1
https://github.com/rwightman/pytorch-image-models/blob/main/timm/scheduler/scheduler_factory.py,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"resample if noise out of percent limit, brute force but shouldn't spin much",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
distmat g,0
q    1 3 2 4,0
4 1 2 3,0
"0, 2, 1, 3",0
"1, 2, 3, 0",0
compute cmc curve for each query,0
get query pid and camid,0
remove gallery samples that have the same pid and camid with query,0
compute cmc curve,0
"binary vector, positions with value 1 are correct matches",0
this condition is true when query identity does not appear in gallery,0
compute average precision,0
reference: https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Average_precision,0
"tmp_cmc = [x / (i + 1.) for i, x in enumerate(tmp_cmc)]",0
query,0
gallery,0
"distmat = re_ranking(qf, gf, k1=20, k2=6, lambda_value=0.3)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"if feature vector is numpy, you should use 'torch.tensor' transform it to tensor",0
print('using GPU to compute original distance'),0
print('starting re_ranking'),0
k-reciprocal neighbors,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
More elegantly one can simply define:,0
take care of difference between feature map space and original image space,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
cos = torch.cdist,0
TODO: maybe find a better spot for this.,1
"dev =  device if torch.cuda.is_available() else ""cpu""",0
train,0
print (f'validation loss {val_loss/len(val_loader)}'),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
normalize vectors here,0
i th vector at j th kpt,0
maybe needs to convert them to embeddings and position token,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Drop missing body parts,0
Drop points lying outside the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
## dlcnet_ms5: backbone resnet50 + multi-fusion & multi-stage module,0
## dlcr101_ms5/dlcr152_ms5: backbone resnet101/152 + multi-fusion & multi-stage module,0
"If the graph is unnecessarily large (with 15+ keypoints by default),",0
we randomly prune it to a size guaranteeing an average node degree of 6;,0
"see Suppl. Fig S9c in Lauer et al., 2022.",0
Use the skeleton defined in the config file,0
Ignore possible connections between 'multi' and 'unique' body parts;,0
one can never be too careful...,0
Disable the prediction of PAFs if the graph is empty,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving metadata and data file (Pickle file),0
###############################################################################,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Setting inference cfg file:,0
Search a pose_config.yaml file to parse missing information,0
Clean the training-datasets folder prior to recreating the data pickles,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Load updated lists:,0
Find the corresponding video file,0
"print(key, ""dropping?"")",0
adding important values for multianiaml project:,0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Determine the index length required to guarantee,0
the train–test ratio is exactly the desired one.,0
Pad indices so lengths agree,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(name,test_video_name)",0
Blazing fast and does not load the image into memory,0
Check that points lie within the image,0
DeprecationWarnings are silenced since Python 3.2 unless triggered in __main__,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
updating variable if null/None! #backwardscompatability,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
these values are dropped as scalecrop,0
doesn't have rotation implemented,0
Also drop maDLC smart cropping augmentation parameters,0
sort the model directories,0
get the shuffle index and offset by 1.,0
read cfg file,0
create log file,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Check for variable correctness,0
Moviepy:,0
select crop method,0
"Find the matching video from the config `video_sets`,",0
as it may be stored elsewhere than in the `videos` directory.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
allow_growth must be true here because tensorflow does not automatically free gpu memory and setting it as false occupies all gpu memory so that pytorch cannot kick in,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
should close tensorflow session here in order to free gpu,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"Flip x, y, confidence and reshape",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html#videocapture-get,0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
TODO: add cropping as in video analysis!,1
if cropping is not None:,0
cfg['cropping']=True,0
"cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping",0
"print(""Overwriting cropping parameters:"", cropping)",0
"print(""These are used for all videos, but won't be save to the cfg file."")",0
"Also ensure `boundingboxslack` is greater than zero, otherwise overlap",0
"between trackers cannot be evaluated, resulting in empty tracklets.",0
Check which snapshots are available and sort them by # iterations,0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
NOTE: If dataname line above is changed then line below is obsolete?,0
"trackname = trackname.replace(videofolder, destfolder)",0
TODO: adjust this for multi + unique bodyparts!,1
this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. ),0
Optimal identity assignment based on soft voting,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
imagenet mean for resnet pretraining:,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/DeepLabCut/DeepLabCut/pull/324",0
for DLC 2.2. (here all set False to not use PAFs/pairwise fields),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
"reloading defaults, as they can bleed over from a previous run otherwise",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
dlc_cfg_train = load_config(str(path_train_config)),0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
Name for deeplabcut net (based on its parameters),0
"DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])",0
"print(""Extracting maps for "", DLCscorer, "" with # of trainingiterations:"", trainingsiterations)",0
if notanalyzed: #this only applies to ask if h5 exists...,0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"for multi animal, seems only this is used",0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Get animal IDs and corresponding indices in the arrays of detections,0
Form ground truth beforehand,0
Assemble animals on the full set of detections,0
Count the number of unassembled bodyparts,0
Handle unlabeled bodyparts...,0
"Only 1 animal, let us return the full graph indices only",0
Find minimal skeleton,0
Select optimal PAF graph,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Selective copy; deepcopy is >5x slower,0
Only keeps skeletons that are more than 90% complete,0
TODO Normalize dists by longest length?,1
TODO Smarter imputation technique (Bayesian? Grassmann averages?),1
Covariance matrix estimation fails due to numerical singularities,0
Distance is undefined if the assembly is empty,0
Correct distance to account for missing observations,0
"Alternatively, reduce contribution of missing values to the Mahalanobis",0
distance to zero by substituting the corresponding means.,0
"Fill the subsets with unambiguous, complete individuals",0
Fuse superfluous assemblies,0
Second pass without edge safety,0
Mark the unique body parts as assembled anyway so,0
they are not used later on to fill assemblies.,0
Store selected edges for subsequent frames,0
Remove invalid assemblies,0
Fill assemblies with unconnected body parts,0
Spawning (rather than forking) multiple processes does not,0
work nicely with the GUI or interactive sessions.,0
"In that case, we fall back to the serial assembly.",0
Swap corresponding keypoints,0
Only consider assemblies of at least two keypoints,0
Greedy assembly matching like in pycocotools,0
Global rather than greedy assembly matching,0
Cast columns of dtype 'object' to float to avoid TypeError,0
further down in _parse_ground_truth_data.,0
sigma is taken as the median of all COCO keypoint standard deviations,0
Sort matching score (OKS) in descending order of assembly affinity,0
Guarantee precision decreases monotonically,0
See https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Regularize by forcing AR <= 5,0
max_ar = 5,0
if el.aspect_ratio >= max_ar:,0
if el.height > el.width:,0
el.width = el.height / max_ar,0
else:,0
el.height = el.width / max_ar,0
Orient the ellipse such that it encompasses most points,0
"n_inside = el.contains_points(np.c_[self.x, self.y]).sum()",0
el.theta += 0.5 * np.pi,0
"if el.contains_points(np.c_[self.x, self.y]).sum() < n_inside:",0
el.theta -= 0.5 * np.pi,0
"r2 = chi2.ppf(2 * norm.cdf(sd) - 1, 2)",0
"height, width = np.sqrt(E * r2)",0
The general quadratic curve has the form:,0
ax^2 + 2bxy + cy^2 + 2dx + 2fy + g = 0,0
Ellipse center coordinates,0
Semi-axes lengths,0
Angle to the horizontal,0
High uncertainty to the unobservable initial velocities,0
Give high uncertainty to the unobservable initial velocities,0
diff = val - cost_matrix,0
"diff[row, col] += val",0
if (,0
val < self.iou_threshold,0
or np.any(diff[row] <= 0.2),0
"or np.any(diff[:, col] <= 0.2)",0
):,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
Modified from scipy source code:,0
- to restrict its use to 2D,0
- to get rid of shuffling (since arrays are only (nbodyparts * 3) element long),0
TODO - factor in keypoint confidence (and weight by # of observations??),1
"mat = self.calc_pairwise_oks(poses, poses_ref)",0
Remove matched detections with low OKS,0
matches = [],0
"for row, col in zip(row_indices, col_indices):",0
"if mat[row, col] < self.oks_threshold:",0
unmatched_poses.append(row),0
unmatched_trackers.append(col),0
else:,0
"matches.append([row, col])",0
if not len(matches):,0
"matches = np.empty((0, 2), dtype=int)",0
else:,0
matches = np.stack(matches),0
update matched trackers with assigned detections,0
create and initialise new trackers for unmatched detections,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
filter out matched with low IOU,0
TODO: get cropping parameters and utilize!,1
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Load meta data & annotations,0
get the graph!,0
"NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*",0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
"MA projects have column indices ""scorer"", ""individuals"" and ""bodyparts""",0
"Drop the scorer level, and put individuals in rows",0
The error rows are series; stack in axis 1 and pivot to get DF,0
TODO: Make this code not so redundant!,1
"If a string was passed in, auto-convert to True for backward compatibility",0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to initial folder,0
"# Also storing one ""large"" table with results:",0
note: evaluationfolder.parents[0] to get common folder above all shuffle evaluations.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
#################################################,0
Load data...,0
#################################################,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
TODO: IMPLEMENT for different batch sizes?,1
Ignore best edges possibly defined during a prior evaluation,0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
Pass the image and the keypoints through the resizer;,0
this has no effect if no augmenters were added to it.,0
FIXME Is having an empty array vs nan really that necessary?!,1
"Form 2D array of shape (n_rows, 4) where the last dimension",0
"is (sample_index, peak_y, peak_x, bpt_index) to slice the PAFs.",0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in,0
Compute all distance statistics,0
Calculate overall prediction error,0
Skip data-driven skeleton selection unless,0
the model was trained on the full graph.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
in case there was already a graph,0
have to overwrite this,0
"loading backbone from ResNet, MobileNet etc.",0
"display_iters = max(1,int(displayiters))",0
"[_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})",0
Save snapshot,0
return to original path.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Clip peak locations to PAFs dimensions,0
"unit_vecs = vecs / lengths[:, np.newaxis]",0
"affinities = np.squeeze(y @ np.expand_dims(unit_vecs, axis=2)).sum(axis=1)",0
Form cost matrices,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"loading backbone from ResNet, MobileNet etc.",0
"Auto-switch to Adam on M1/M2 chips, as the momentum optimizer crashes",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Convert a frozen graph to OpenVINO IR format,0
Read network into memory,0
"For better efficiency, model is initialized for batch_size 1 and every sample processed independently",0
Load network to device,0
Prepare input data,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Change the stride from 2 to 1 to get 16x downscaling instead of 32x.,0
https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142,0
Multi stage is currently only implemented for resnets,0
Defining multi_fusion backbone,0
Attaching multi-stage decoder,0
stage_paf_output = stage_paf_output + pre_stage_paf_output,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
FIXME Fix wrong scope with Keras layers,1
"def prediction_layer(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2DTranspose(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=2,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
## New DLCNet Addition: multi-stage decoder,0
"def prediction_layer_stage(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2D(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=1,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Compute variance using: Var[X]= E[X^2] - E[X]^2.,0
Compute keep_prob,0
TODO(tanmingxing): add support for training progress.,1
Compute drop_connect tensor,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
turn into x times y time bs * bpts,0
extract corresponding locref x and y as well as probability,0
Filter predicted heatmaps with a 2D Gaussian kernel as in:,0
https://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_The_Devil_Is_in_the_Details_Delving_Into_Unbiased_Data_CVPR_2020_paper.pdf,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
defaults will be a public argument for namedtuple in Python 3.7,0
https://docs.python.org/3/library/collections.html#collections.namedtuple,0
Make sure that round down does not go down by more than 10%.,0
Builds the block accordings to arguments.,0
Expansion phase:,0
Depth-wise convolution phase:,0
Squeeze and Excitation layer.,0
Output phase:,0
only apply drop_connect if skip presents.,0
Expansion phase:,0
Output phase:,0
only apply drop_connect if skip presents.,0
Builds blocks.,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Stem part.,0
Head part.,0
Calls Stem layers,0
Calls blocks.,0
Calls final layers and returns logits.,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
"(width_coefficient, depth_coefficient, resolution, dropout_rate)",0
blocks_args = [,0
"'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',",0
"'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',",0
"'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',",0
"'r1_k3_s11_e6_i192_o320_se0.25',",0
],0
The default is TPU-specific batch norm.,0
The alternative is tf.layers.BatchNormalization.,0
"batch_norm=utils.TpuBatchNormalization,  # TPU-specific requirement.",0
ValueError will be raised here if override_params has fields not included,0
in global_params.,0
,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrunk when given a depth-multiplier < 1.0.,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Clamp to 40% of crop size to ensure that at least,0
the center keypoint remains visible after the offset is applied.,0
Points located close to one another are sampled preferentially,0
in order to augment crowded regions.,0
Include keypoints in the count to avoid null probabilities,0
Shift the crop center in both dimensions by random amounts,0
and normalize to the original image dimensions.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"for topview, it's safe to mask keypoints under threshold",0
Pickle the 'data' dictionary using the highest protocol available.,0
item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal,0
3 for likelihood,0
we only do single animal here,0
let's hard code it,0
"Add smart, keypoint-aware image cropping",0
"because of the existence of threshold, sampling population is adjusted to len(self.data)",0
empty prediction for this frame,0
in case it's empty prediction,0
Scale is sampled only once (per batch) to transform all of the images into same size.,0
Discard keypoints whose coordinates lie outside the cropped image,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
Produce score maps and location refinement fields,0
Find indices of individuals in joint_id,0
WIP!,0
STD of gaussian is 1/4 of threshold,0
Grid of coordinates,0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Distance between the joint point and each coordinate,0
NEEDS TO BE DONE!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
TODO Finish implementing actual abstract class,1
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
parameter initialization for augmentation pipeline:,0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images of a batch into same size.,0
Approximating the scale,0
invisible joints are represented by nans,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"First, initializing variables (if they don't exist)",0
what is the fraction of training samples with scaling augmentation?,0
loading defaults for rotation range!,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"cfg[""rotateratio""] = cfg.get(",0
"""rotratio"", 0.4",0
)  # what is the fraction of training samples with rotation augmentation?,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"Auto cropping is new (was not in Nature Neuroscience 2018 paper, but introduced in Nath et al. Nat. Protocols 2019)",0
"and boosts performance by 2X, particularly on challenging datasets, like the cheetah in Nath et al.",0
Parameters for augmentation with regard to cropping:,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Get rid of the interpolation beyond the spline knots,0
Retrieve original individual label indices,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Check the training image paths are correctly stored as arrays of strings,0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
TENSORPACK could fail in WINDOWS...,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
Sometimes tracks cannot be reconstructed as test data are randomly,0
"created; when this happens, we generate a fake h5 data file.",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Loading example data set,0
# Create one identical splits for 3 networks and 3 augmentations,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
Setting specific parameters for training,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"MODELS = [""dlcrnet_ms5"", ""dlcr101_ms5"", ""efficientnet-b0"", ""mobilenet_v2_0.35""]",0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
## adding it here,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Downloading the calibration images,0
Deleting unnecessary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Loading example data set,0
example how to set pose config variables:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
test if existing models are found:,0
Test if nonexisting models are not found,0
Add some other office files:,0
Add a .pickle and .h5 files,0
"By default, all videos with common extensions are taken from a directory",0
A list of extensions can also be passed in,0
"labeled_folders: (has_H5, H5_st_mtime, folder_name)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Test bounding box from a single keypoint,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Test that shape was preserved after vectorization,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Guarantee that images smaller than crop size are handled fine,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Disable stochastic scale jitter,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Verify the aspect ratio is preserved,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Load sample multi animal data,0
Generate a random permutation and reorder data. Ignore the unique bodypart,0
Get inverse permutation and reorder the modified data to get back,0
to the original,0
Check,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"Check data comprise path, shape, and xy coordinates",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Check that we get the number of frames we asked for,0
Check that all indices are valid,0
Check that all frames are unique,0
Check that we get the number of frames we asked for,0
Check that all indices are valid,0
Check that all frames are unique,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Break the graph to test stitching failure,0
Add fake IDs,0
Split all tracklets in half,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"individual_1, individual2",0
"leftArm, rightArm, leftArm, rightArm",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Test flipped keypoints,0
Original (cached) coordinates must have remained empty,0
Assemble based on the smallest graph to speed up testing,0
Generate fake identity predictions,0
Assemble based on the smallest graph to speed up testing,0
Test now with identity only and ensure assemblies,0
contain only parts of a single group ID.,0
Test empty assembly,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Verify that the Hugging Face folder was removed,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
def install(package):,0
"subprocess.check_call([sys.executable, ""-m"", ""pip"", ""install"", package])",0
"install(""tensorflow==1.13.1"")",0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
quicker variant,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Suppress tensorflow warning messages,0
"Train, evaluate & predict functions / all require TF",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Updating config file:,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path, num_shuffles=auxfun_models.Check4weights(pose_cfg['net_type'], parent_path, num_shuffles= 1)",0
Updating training and test pose_cfg:,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Read the config file,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
common parameters:,0
Write dictionary to yaml  config file,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
looking for the pseudo label path,0
spatial pyramid is not for adapted model,0
spatial pyramid can still be useful for reducing jittering and quantization error,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
ratio_h and ratio_w should match though in reality it does not match exactly,0
only keep the max,0
empty prediction if pred is not a dict,0
Compute cosine similarity,0
"len(frames) -> (n_scale,)",0
"frames[0].shape - > (batchsize, h, w, 3)",0
no crop needed,0
"batch full, start true inferencing",0
only do this when animal is detected,0
in case we reach the end of the video,0
add a temp folder for checkpoint,0
spatial pyramid can still be useful for reducing jittering and quantization error,0
extra data,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"TODO(stes) mocking a few modules to rely in fewer dependencies, without",1
causing import errors when using deeplabcut.,0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in.,0
TODO(jeylau) add affinity.setter to Assembly,1
TODO(stes): remove/rewrite,1
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"ignore the exception and continue with the next evaluation, without",0
yielding a result value.,0
"return the result value, with NaN as the result for all metrics that",0
could not be computed due to the error.,0
raise the error and stop evaluation,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Read the config file and related variables,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Flatten the list of bodyparts to connect,0
Format data,0
Trick to force equal aspect ratio of 3D plots,0
Set up the matplotlib figure beforehand,0
Set up skeleton LineCollections,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
Get track_method and do related checks,0
Get track method suffix,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
"## Assign nan to [X,Y] of low likelihood predictions ###",0
Convert the data to a np array to easily mask out the low likelihood predictions,0
"Assign [X,Y] = nan to low likelihood predictions",0
Reshape data back to original shape,0
put data back to the dataframes,0
Check individuals are the same in both views,0
Cross-view match individuals,0
Create a dummy variables for single-animal,0
Cleaner variable (since inds view1 == inds view2),0
"Reshape: (num_framex, num_individuals, num_bodyparts , 2)",0
Triangulate data,0
i is individual in view 1,0
voting[i] is the matched individual in view 2,0
Create 3D DataFrame column and row indices,0
Swap num_animals with num_frames axes to ensure well-behaving reshape,0
Fill up 3D dataframe,0
Reorder 2D dataframe in view 2 to match order of view 1,0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no intermediate saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Alternatively,0
Sort tracklets by length to prioritize greater continuity,0
Drop tracklets that are too short,0
"Where slots are available, copy the data over",0
"If about to overwrite data, keep tracklets with highest confidence",0
Squeeze some data into empty slots,0
"For the remaining data, overwrite where we are least confident",0
Find keypoints closest to the remaining data,0
Use Manhattan distance to avoid overflow,0
Only overwrite if improving confidence,0
Fill existing gaps,0
Retrieve original individual label indices,0
Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.,0
Fill existing gaps,0
Retrieve original individual label indices,0
ID swaps occur when X and Y simultaneously intersect each other.,0
Get only those bodypart pairs that belong to different individuals,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"See Li et al., 2012. Cross-view Activity Recognition using Hankelets.",0
"As proposed in the paper, the Hankel matrix can either be formed from",0
the tracklet's centroid or its normalized velocity.,0
"vel = np.diff(self.centroid, axis=0)",0
"vel /= np.linalg.norm(vel, axis=1, keepdims=True)",0
return self.hankelize(vel),0
TODO Add missing data imputation,1
"nrows, ncols = mat.shape",0
beta = nrows / ncols,0
omega = 0.56 * beta ** 3 - 0.95 * beta ** 2 + 1.82 * beta + 1.43,0
return np.argmin(s > omega * np.median(s)),0
"Note that if tracklets are very short, some may actually be part of the same track",0
and thus incorrectly reflect separate track endpoints...,0
"Map each Tracklet to an entry and output nodes and vice versa,",0
which is convenient once the tracklets are stitched.,0
Store tracklets and corresponding negatives (those that overlap in time),0
TODO Avoid looping over all pairs of tracklets,0
Pick the closest (spatially) overlapping tracklet,0
The algorithm works better with integer weights,0
Let us prune the graph by removing all source and sink edges,0
but those connecting the `n_tracks` first and last tracklets.,0
Preflow push seems to work slightly better than shortest,0
"augmentation path..., and is more computationally efficient.",0
Verify whether there are overlapping tracklets,0
"Pick the segment that minimizes ""smoothness"", computed here",0
with the coefficient of variation of the differences.,0
Rebuild a full graph from the remaining nodes without,0
temporal constraint on what tracklets can be stitched together.,0
Cycle through the residuals and incorporate back those,0
that only fit in a single tracklet.,0
Greedily add the remaining residuals,0
Refresh temporal bounds,0
Guarantee track data are sorted in the order defined in the config,0
Default to the distance cost function,0
should only exist one,0
Add in identity weighing before building the graph,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implementation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
average distance and average # significant differences avg. over comparisonbodyparts,0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
###################################################,0
Dependencies,0
###################################################,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as,0
specified in the config file,0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
Prepare figure,0
Mask rather than drop unwanted bodyparts to ensure consistent coloring,0
cropping!,0
nx=X2-X1,0
!/usr/bin/env python3,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"more videos are in principle covered, as OpenCV is used and allows many formats.",0
The above already raises a ValueError if formatting is wrong,0
"Rotate, see: https://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg",0
interesting option to just update metadata.,0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
Read the video until a frame is successfully read,0
import platform,0
"if platform.system() == ""Darwin"":  # for OSX use WXAgg",0
fig.canvas.start_event_loop(timeout=-1),0
else:,0
#fig.canvas.stop_event_loop(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"Windows throws error if file path is > 260 characters, can fix with prefix.",0
See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
uses openCV,0
cap.release() >> still used in frame_extraction!,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
This dictionary maps the model types to the file locations where the models exist.,0
Exit the function early if an unknown modeltype is provided.,0
Aliases for backwards-compatibility,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
Initialize costs matrix,0
get common bodypart detections in track pair,0
add 3rd dim to the points,0
cost for any point in time of t1 being the same,0
any point in time of t2,0
Get average cost of the entire track,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
check if it exists:,0
(average_degree / 2) as many edges as there are nodes is required,0
for undirected graphs to reach the target degree.,0
"Attention this order has to be consistent (for training set creation, training, inference etc.)",0
"-> adding (single,bpt) for uniquebodyparts",0
"-> adding (individual,bpt) for multianimalbodyparts",0
load defaults,0
set project specific parameters:,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
###################################################,0
Dependencies,0
###################################################,0
Pose X vs pose Y,0
Poses vs time,0
Likelihoods,0
Histograms,0
#################################################,0
Looping analysis over video,0
#################################################,0
"In a multi animal scenario, show more verbose errors.",0
Some vidoes were not evaluated.,0
Keep only the individuals and bodyparts that were labeled,0
Either display the animals defined in the config if they are found,0
"in the dataframe, or all the trajectories regardless of their names",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Determine whether the data are single- or multi-animal without loading into memory,0
simply by checking whether 'individuals' is in the second line of the CSV.,0
Make paths platform-agnostic if they are not already,0
Ensure folder names are strings,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
make list of full paths,0
filter list of videos,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"if ""MULTI!"" in allbpts:",0
Ensure same order as in config.yaml,0
ABBREVIATE NETWORK NAMES -- esp. for mobilenet!,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
Iterate over data files and stop as soon as one matching the scorer is found,0
If there was no match...,0
aliases for backwards-compatibility.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Set up a logger and add an stdout handler.,0
A single logger can have many handlers:,0
https://docs.python.org/3/howto/logging.html#handler-basic,0
TODO Dump to log file instead,1
"logger = logging.getLogger(""GUI"")",0
logger.setLevel(logging.DEBUG),0
handler = logging.StreamHandler(stream=sys.stdout),0
handler.setLevel(logging.DEBUG),0
formatter = logging.Formatter(,0
"""%(asctime)s - %(name)s - %(levelname)s - %(message)s"", ""%Y-%m-%d %H:%M:%S""",0
),0
handler.setFormatter(formatter),0
logger.addHandler(handler),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
restore the background region,0
Take into consideration imprecise OpenCV estimation of total number of frames,0
Automatically disable the draggable points,0
Save additional frames to the labeled-data directory,0
Store the newly-refined data,0
Uncertain keypoints are ignored,0
Merge with the already existing annotated data,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Thread-safe Stdout redirector,0
Creating action using the first constructor,0
Creating actions using the second constructor,0
File menu,0
View menu,0
Help menu,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
all_bodyparts: List,0
NOTE: Is there a case where a specific list should,0
have bodyparts other than the root? I don't think so.,0
Videotype selection,0
Select videos,0
Number of selected videos text,0
Clear video selection,0
"Qt returns a tuple (list of files, filetype)",0
Add tab header,0
Add separating line,0
Look for any extension by default,0
This works both with e.g. .avi and avi,0
Choose multiple files by default,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Automatically activate the napari-deeplabcut plugin,0
TODO Insert new video,1
TODO Insert skeleton link,1
"Hack to make the first column read-only, as we do not want users to touch it.",1
"The cleaner solution would be to use a QTreeView and QAbstractItemModel,",0
but that is a lot of rework for little benefits.,0
Leave untouched when it is already a string,0
"Slashes also raise the error, but no need to print anything since it is then likely to be a path",0
Walk backwards across parents to get all keys,0
Read the video until a frame is successfully read,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
TODO: Multi video select.... have to change to single video!,1
Shuffle,0
Num animals,0
Min swap length,0
Max gap to fill,0
Trail length,0
Filter type,0
Filter window length,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
Tracker Type,0
Num animals,0
Num triplets,0
layout.addWidget(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
Overwrite videos,0
Trail Points,0
Plot all bodyparts,0
Skeleton,0
Filtered data,0
Plot trajectories,0
High quality video,0
Bodypart list,0
Multianimal scenario.,0
Color is based on individual or bodypart.,0
Single animal scenario.,0
Color is based on bodypart.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Extraction method,0
Frame extraction algorithm,0
Frame cropping,0
Cluster step,0
GUI Slider width,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
use the default pose_cfg file for default values,0
Shuffle,0
Display iterations,0
Save iterations,0
Max iterations,0
Max number snapshots to keep,0
layout.addWidget(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Single / Multi animal Only Layouts,0
Dynamic bodypart cropping,0
Save results as csv,0
Filter predictions,0
Plot Trajectories,0
Show trajectory plots,0
Shuffle,0
layout.addLayout(tmp_layout),0
tmp_layout = QtWidgets.QGridLayout(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
Augmentation method,0
Neural Network,0
TODO: finish model_comparison,1
Check that training data files were indeed created.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Add config text field and button,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
layout.addWidget(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Display all images,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"calling create_tracking_dataset, train_tracking_transformer, stitch_tracklets",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
only try to find these features if they are in the dictionary,0
1) reference to video folder and get the proper bpt_feature file for feature table,0
2) get either the path to gt or the path to track pickle,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
tracklets,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"with npy list form videos, split each to train and test",0
"video_name = '.'.join(video.split(""/"")[-1].split(""."")[:-1])",0
assuming there is only one match,0
make my own model factory,0
make my own loss factory,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights",0
"NOTE: drop path for stochastic depth, we shall see if this is better than dropout here",0
only channel pooling,0
"x -> (B, 12, 2048)",0
"x -> (B, 12, 768)",0
trying to keep everything simple,0
need to write my own keypoint embedding,0
we don't need a fc layer here,0
x: inputs,0
"(B, 12, 768)",0
added pos embed,0
remove all parts I don't understand,0
forwarding through blocks. But num of blocks should be reduced,0
i can't really tell what the difference is,0
will have to change some of these,0
For old models that I trained prior to conv based patchification,0
To resize pos embedding when using model at different size from pretrained weights,0
Rescale the grid of position embeddings when loading from state_dict. Adapted from,0
https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224,0
Cut & paste from PyTorch official master until it's in a few official releases - RW,0
Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf,0
Computes standard normal cumulative distribution function,0
Values are generated by using a truncated uniform distribution and,0
then using the inverse CDF for the normal distribution.,0
Get upper and lower cdf values,0
"Uniformly fill tensor with values from [l, u], then translate to",0
"[2l-1, 2u-1].",0
Use inverse cdf transform for normal distribution to get truncated,0
standard normal,0
"Transform to proper mean, std",0
Clamp to ensure it's in the proper range,0
"type: (Tensor, float, float, float, float) -> Tensor",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2019 Ross Wightman,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Hacked together by / Copyright 2020 Ross Wightman,1
https://github.com/rwightman/pytorch-image-models/blob/main/timm/scheduler/scheduler_factory.py,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"resample if noise out of percent limit, brute force but shouldn't spin much",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
distmat g,0
q    1 3 2 4,0
4 1 2 3,0
"0, 2, 1, 3",0
"1, 2, 3, 0",0
compute cmc curve for each query,0
get query pid and camid,0
remove gallery samples that have the same pid and camid with query,0
compute cmc curve,0
"binary vector, positions with value 1 are correct matches",0
this condition is true when query identity does not appear in gallery,0
compute average precision,0
reference: https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Average_precision,0
"tmp_cmc = [x / (i + 1.) for i, x in enumerate(tmp_cmc)]",0
query,0
gallery,0
"distmat = re_ranking(qf, gf, k1=20, k2=6, lambda_value=0.3)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"if feature vector is numpy, you should use 'torch.tensor' transform it to tensor",0
print('using GPU to compute original distance'),0
print('starting re_ranking'),0
k-reciprocal neighbors,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
More elegantly one can simply define:,0
take care of difference between feature map space and original image space,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
cos = torch.cdist,0
TODO: maybe find a better spot for this.,1
"dev =  device if torch.cuda.is_available() else ""cpu""",0
train,0
print (f'validation loss {val_loss/len(val_loader)}'),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
normalize vectors here,0
i th vector at j th kpt,0
maybe needs to convert them to embeddings and position token,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Drop missing body parts,0
Drop points lying outside the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
## dlcnet_ms5: backbone resnet50 + multi-fusion & multi-stage module,0
## dlcr101_ms5/dlcr152_ms5: backbone resnet101/152 + multi-fusion & multi-stage module,0
"If the graph is unnecessarily large (with 15+ keypoints by default),",0
we randomly prune it to a size guaranteeing an average node degree of 6;,0
"see Suppl. Fig S9c in Lauer et al., 2022.",0
Use the skeleton defined in the config file,0
Ignore possible connections between 'multi' and 'unique' body parts;,0
one can never be too careful...,0
Disable the prediction of PAFs if the graph is empty,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving metadata and data file (Pickle file),0
###############################################################################,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Setting inference cfg file:,0
Search a pose_config.yaml file to parse missing information,0
Clean the training-datasets folder prior to recreating the data pickles,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Load updated lists:,0
Find the corresponding video file,0
"print(key, ""dropping?"")",0
adding important values for multianiaml project:,0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Determine the index length required to guarantee,0
the train–test ratio is exactly the desired one.,0
Pad indices so lengths agree,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(name,test_video_name)",0
Blazing fast and does not load the image into memory,0
Check that points lie within the image,0
DeprecationWarnings are silenced since Python 3.2 unless triggered in __main__,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
updating variable if null/None! #backwardscompatability,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
these values are dropped as scalecrop,0
doesn't have rotation implemented,0
Also drop maDLC smart cropping augmentation parameters,0
sort the model directories,0
get the shuffle index and offset by 1.,0
read cfg file,0
create log file,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Check for variable correctness,0
Moviepy:,0
select crop method,0
"Find the matching video from the config `video_sets`,",0
as it may be stored elsewhere than in the `videos` directory.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
allow_growth must be true here because tensorflow does not automatically free gpu memory and setting it as false occupies all gpu memory so that pytorch cannot kick in,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
should close tensorflow session here in order to free gpu,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"Flip x, y, confidence and reshape",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html#videocapture-get,0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
TODO: add cropping as in video analysis!,1
if cropping is not None:,0
cfg['cropping']=True,0
"cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping",0
"print(""Overwriting cropping parameters:"", cropping)",0
"print(""These are used for all videos, but won't be save to the cfg file."")",0
"Also ensure `boundingboxslack` is greater than zero, otherwise overlap",0
"between trackers cannot be evaluated, resulting in empty tracklets.",0
Check which snapshots are available and sort them by # iterations,0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
NOTE: If dataname line above is changed then line below is obsolete?,0
"trackname = trackname.replace(videofolder, destfolder)",0
TODO: adjust this for multi + unique bodyparts!,1
this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. ),0
Optimal identity assignment based on soft voting,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
imagenet mean for resnet pretraining:,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/DeepLabCut/DeepLabCut/pull/324",0
for DLC 2.2. (here all set False to not use PAFs/pairwise fields),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
"reloading defaults, as they can bleed over from a previous run otherwise",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
dlc_cfg_train = load_config(str(path_train_config)),0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
Name for deeplabcut net (based on its parameters),0
"DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])",0
"print(""Extracting maps for "", DLCscorer, "" with # of trainingiterations:"", trainingsiterations)",0
if notanalyzed: #this only applies to ask if h5 exists...,0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"for multi animal, seems only this is used",0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Get animal IDs and corresponding indices in the arrays of detections,0
Form ground truth beforehand,0
Assemble animals on the full set of detections,0
Count the number of unassembled bodyparts,0
Handle unlabeled bodyparts...,0
"Only 1 animal, let us return the full graph indices only",0
Find minimal skeleton,0
Select optimal PAF graph,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Selective copy; deepcopy is >5x slower,0
Only keeps skeletons that are more than 90% complete,0
TODO Normalize dists by longest length?,1
TODO Smarter imputation technique (Bayesian? Grassmann averages?),1
Covariance matrix estimation fails due to numerical singularities,0
Distance is undefined if the assembly is empty,0
Correct distance to account for missing observations,0
"Alternatively, reduce contribution of missing values to the Mahalanobis",0
distance to zero by substituting the corresponding means.,0
"Fill the subsets with unambiguous, complete individuals",0
Fuse superfluous assemblies,0
Second pass without edge safety,0
Mark the unique body parts as assembled anyway so,0
they are not used later on to fill assemblies.,0
Store selected edges for subsequent frames,0
Remove invalid assemblies,0
Fill assemblies with unconnected body parts,0
Spawning (rather than forking) multiple processes does not,0
work nicely with the GUI or interactive sessions.,0
"In that case, we fall back to the serial assembly.",0
Swap corresponding keypoints,0
Only consider assemblies of at least two keypoints,0
Greedy assembly matching like in pycocotools,0
Global rather than greedy assembly matching,0
Cast columns of dtype 'object' to float to avoid TypeError,0
further down in _parse_ground_truth_data.,0
sigma is taken as the median of all COCO keypoint standard deviations,0
Sort matching score (OKS) in descending order of assembly affinity,0
Guarantee precision decreases monotonically,0
See https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Regularize by forcing AR <= 5,0
max_ar = 5,0
if el.aspect_ratio >= max_ar:,0
if el.height > el.width:,0
el.width = el.height / max_ar,0
else:,0
el.height = el.width / max_ar,0
Orient the ellipse such that it encompasses most points,0
"n_inside = el.contains_points(np.c_[self.x, self.y]).sum()",0
el.theta += 0.5 * np.pi,0
"if el.contains_points(np.c_[self.x, self.y]).sum() < n_inside:",0
el.theta -= 0.5 * np.pi,0
"r2 = chi2.ppf(2 * norm.cdf(sd) - 1, 2)",0
"height, width = np.sqrt(E * r2)",0
The general quadratic curve has the form:,0
ax^2 + 2bxy + cy^2 + 2dx + 2fy + g = 0,0
Ellipse center coordinates,0
Semi-axes lengths,0
Angle to the horizontal,0
High uncertainty to the unobservable initial velocities,0
Give high uncertainty to the unobservable initial velocities,0
diff = val - cost_matrix,0
"diff[row, col] += val",0
if (,0
val < self.iou_threshold,0
or np.any(diff[row] <= 0.2),0
"or np.any(diff[:, col] <= 0.2)",0
):,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
Modified from scipy source code:,0
- to restrict its use to 2D,0
- to get rid of shuffling (since arrays are only (nbodyparts * 3) element long),0
TODO - factor in keypoint confidence (and weight by # of observations??),1
"mat = self.calc_pairwise_oks(poses, poses_ref)",0
Remove matched detections with low OKS,0
matches = [],0
"for row, col in zip(row_indices, col_indices):",0
"if mat[row, col] < self.oks_threshold:",0
unmatched_poses.append(row),0
unmatched_trackers.append(col),0
else:,0
"matches.append([row, col])",0
if not len(matches):,0
"matches = np.empty((0, 2), dtype=int)",0
else:,0
matches = np.stack(matches),0
update matched trackers with assigned detections,0
create and initialise new trackers for unmatched detections,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
filter out matched with low IOU,0
TODO: get cropping parameters and utilize!,1
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Load meta data & annotations,0
get the graph!,0
"NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*",0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
TODO: Make this code not so redundant!,1
"If a string was passed in, auto-convert to True for backward compatibility",0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to initial folder,0
"# Also storing one ""large"" table with results:",0
note: evaluationfolder.parents[0] to get common folder above all shuffle evaluations.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
#################################################,0
Load data...,0
#################################################,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
TODO: IMPLEMENT for different batch sizes?,1
Ignore best edges possibly defined during a prior evaluation,0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
Pass the image and the keypoints through the resizer;,0
this has no effect if no augmenters were added to it.,0
FIXME Is having an empty array vs nan really that necessary?!,1
"Form 2D array of shape (n_rows, 4) where the last dimension",0
"is (sample_index, peak_y, peak_x, bpt_index) to slice the PAFs.",0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in,0
Compute all distance statistics,0
Calculate overall prediction error,0
Skip data-driven skeleton selection unless,0
the model was trained on the full graph.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
in case there was already a graph,0
have to overwrite this,0
"loading backbone from ResNet, MobileNet etc.",0
"display_iters = max(1,int(displayiters))",0
"[_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})",0
Save snapshot,0
return to original path.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Clip peak locations to PAFs dimensions,0
"unit_vecs = vecs / lengths[:, np.newaxis]",0
"affinities = np.squeeze(y @ np.expand_dims(unit_vecs, axis=2)).sum(axis=1)",0
Form cost matrices,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"loading backbone from ResNet, MobileNet etc.",0
"Auto-switch to Adam on M1/M2 chips, as the momentum optimizer crashes",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Convert a frozen graph to OpenVINO IR format,0
Read network into memory,0
"For better efficiency, model is initialized for batch_size 1 and every sample processed independently",0
Load network to device,0
Prepare input data,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Change the stride from 2 to 1 to get 16x downscaling instead of 32x.,0
https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142,0
Multi stage is currently only implemented for resnets,0
Defining multi_fusion backbone,0
Attaching multi-stage decoder,0
stage_paf_output = stage_paf_output + pre_stage_paf_output,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
FIXME Fix wrong scope with Keras layers,1
"def prediction_layer(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2DTranspose(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=2,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
## New DLCNet Addition: multi-stage decoder,0
"def prediction_layer_stage(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2D(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=1,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Compute variance using: Var[X]= E[X^2] - E[X]^2.,0
Compute keep_prob,0
TODO(tanmingxing): add support for training progress.,1
Compute drop_connect tensor,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
turn into x times y time bs * bpts,0
extract corresponding locref x and y as well as probability,0
Filter predicted heatmaps with a 2D Gaussian kernel as in:,0
https://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_The_Devil_Is_in_the_Details_Delving_Into_Unbiased_Data_CVPR_2020_paper.pdf,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
defaults will be a public argument for namedtuple in Python 3.7,0
https://docs.python.org/3/library/collections.html#collections.namedtuple,0
Make sure that round down does not go down by more than 10%.,0
Builds the block accordings to arguments.,0
Expansion phase:,0
Depth-wise convolution phase:,0
Squeeze and Excitation layer.,0
Output phase:,0
only apply drop_connect if skip presents.,0
Expansion phase:,0
Output phase:,0
only apply drop_connect if skip presents.,0
Builds blocks.,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Stem part.,0
Head part.,0
Calls Stem layers,0
Calls blocks.,0
Calls final layers and returns logits.,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
"(width_coefficient, depth_coefficient, resolution, dropout_rate)",0
blocks_args = [,0
"'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',",0
"'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',",0
"'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',",0
"'r1_k3_s11_e6_i192_o320_se0.25',",0
],0
The default is TPU-specific batch norm.,0
The alternative is tf.layers.BatchNormalization.,0
"batch_norm=utils.TpuBatchNormalization,  # TPU-specific requirement.",0
ValueError will be raised here if override_params has fields not included,0
in global_params.,0
,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrunk when given a depth-multiplier < 1.0.,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Clamp to 40% of crop size to ensure that at least,0
the center keypoint remains visible after the offset is applied.,0
Points located close to one another are sampled preferentially,0
in order to augment crowded regions.,0
Include keypoints in the count to avoid null probabilities,0
Shift the crop center in both dimensions by random amounts,0
and normalize to the original image dimensions.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"for topview, it's safe to mask keypoints under threshold",0
Pickle the 'data' dictionary using the highest protocol available.,0
item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal,0
3 for likelihood,0
we only do single animal here,0
let's hard code it,0
"Add smart, keypoint-aware image cropping",0
"because of the existence of threshold, sampling population is adjusted to len(self.data)",0
empty prediction for this frame,0
in case it's empty prediction,0
Scale is sampled only once (per batch) to transform all of the images into same size.,0
Discard keypoints whose coordinates lie outside the cropped image,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
Produce score maps and location refinement fields,0
Find indices of individuals in joint_id,0
WIP!,0
STD of gaussian is 1/4 of threshold,0
Grid of coordinates,0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Distance between the joint point and each coordinate,0
NEEDS TO BE DONE!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
TODO Finish implementing actual abstract class,1
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
parameter initialization for augmentation pipeline:,0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images of a batch into same size.,0
Approximating the scale,0
invisible joints are represented by nans,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"First, initializing variables (if they don't exist)",0
what is the fraction of training samples with scaling augmentation?,0
loading defaults for rotation range!,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"cfg[""rotateratio""] = cfg.get(",0
"""rotratio"", 0.4",0
)  # what is the fraction of training samples with rotation augmentation?,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"Auto cropping is new (was not in Nature Neuroscience 2018 paper, but introduced in Nath et al. Nat. Protocols 2019)",0
"and boosts performance by 2X, particularly on challenging datasets, like the cheetah in Nath et al.",0
Parameters for augmentation with regard to cropping:,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Get rid of the interpolation beyond the spline knots,0
Retrieve original individual label indices,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Check the training image paths are correctly stored as arrays of strings,0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
TENSORPACK could fail in WINDOWS...,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
Sometimes tracks cannot be reconstructed as test data are randomly,0
"created; when this happens, we generate a fake h5 data file.",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Loading example data set,0
# Create one identical splits for 3 networks and 3 augmentations,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
Setting specific parameters for training,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"MODELS = [""dlcrnet_ms5"", ""dlcr101_ms5"", ""efficientnet-b0"", ""mobilenet_v2_0.35""]",0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
## adding it here,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Downloading the calibration images,0
Deleting unnecessary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Loading example data set,0
example how to set pose config variables:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
test if existing models are found:,0
Test if nonexisting models are not found,0
Add some other office files:,0
Add a .pickle and .h5 files,0
"By default, all videos with common extensions are taken from a directory",0
A list of extensions can also be passed in,0
"labeled_folders: (has_H5, H5_st_mtime, folder_name)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Test bounding box from a single keypoint,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Test that shape was preserved after vectorization,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Guarantee that images smaller than crop size are handled fine,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Disable stochastic scale jitter,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Verify the aspect ratio is preserved,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Load sample multi animal data,0
Generate a random permutation and reorder data. Ignore the unique bodypart,0
Get inverse permutation and reorder the modified data to get back,0
to the original,0
Check,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"Check data comprise path, shape, and xy coordinates",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Check that we get the number of frames we asked for,0
Check that all indices are valid,0
Check that all frames are unique,0
Check that we get the number of frames we asked for,0
Check that all indices are valid,0
Check that all frames are unique,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Break the graph to test stitching failure,0
Add fake IDs,0
Split all tracklets in half,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Test flipped keypoints,0
Original (cached) coordinates must have remained empty,0
Assemble based on the smallest graph to speed up testing,0
Generate fake identity predictions,0
Assemble based on the smallest graph to speed up testing,0
Test now with identity only and ensure assemblies,0
contain only parts of a single group ID.,0
Test empty assembly,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Verify that the Hugging Face folder was removed,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
def install(package):,0
"subprocess.check_call([sys.executable, ""-m"", ""pip"", ""install"", package])",0
"install(""tensorflow==1.13.1"")",0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
quicker variant,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Suppress tensorflow warning messages,0
"Train, evaluate & predict functions / all require TF",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Updating config file:,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path, num_shuffles=auxfun_models.Check4weights(pose_cfg['net_type'], parent_path, num_shuffles= 1)",0
Updating training and test pose_cfg:,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Read the config file,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
common parameters:,0
Write dictionary to yaml  config file,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
looking for the pseudo label path,0
spatial pyramid is not for adapted model,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
ratio_h and ratio_w should match though in reality it does not match exactly,0
only keep the max,0
Compute cosine similarity,0
"len(frames) -> (n_scale,)",0
"frames[0].shape - > (batchsize, h, w, 3)",0
no crop needed,0
"batch full, start true inferencing",0
only do this when animal is detected,0
in case we reach the end of the video,0
add a temp folder for checkpoint,0
"if the scale_list is empty, by default we use the original one",0
extra data,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"TODO(stes) mocking a few modules to rely in fewer dependencies, without",1
causing import errors when using deeplabcut.,0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in.,0
TODO(jeylau) add affinity.setter to Assembly,1
TODO(stes): remove/rewrite,1
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"ignore the exception and continue with the next evaluation, without",0
yielding a result value.,0
"return the result value, with NaN as the result for all metrics that",0
could not be computed due to the error.,0
raise the error and stop evaluation,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Read the config file and related variables,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Flatten the list of bodyparts to connect,0
Format data,0
Trick to force equal aspect ratio of 3D plots,0
Set up the matplotlib figure beforehand,0
Set up skeleton LineCollections,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
Get track_method and do related checks,0
Get track method suffix,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
"## Assign nan to [X,Y] of low likelihood predictions ###",0
Convert the data to a np array to easily mask out the low likelihood predictions,0
"Assign [X,Y] = nan to low likelihood predictions",0
Reshape data back to original shape,0
put data back to the dataframes,0
Check individuals are the same in both views,0
Cross-view match individuals,0
Create a dummy variables for single-animal,0
Cleaner variable (since inds view1 == inds view2),0
"Reshape: (num_framex, num_individuals, num_bodyparts , 2)",0
Triangulate data,0
i is individual in view 1,0
voting[i] is the matched individual in view 2,0
Create 3D DataFrame column and row indices,0
Swap num_animals with num_frames axes to ensure well-behaving reshape,0
Fill up 3D dataframe,0
Reorder 2D dataframe in view 2 to match order of view 1,0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no intermediate saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Alternatively,0
Sort tracklets by length to prioritize greater continuity,0
Drop tracklets that are too short,0
"Where slots are available, copy the data over",0
"If about to overwrite data, keep tracklets with highest confidence",0
Squeeze some data into empty slots,0
"For the remaining data, overwrite where we are least confident",0
Find keypoints closest to the remaining data,0
Use Manhattan distance to avoid overflow,0
Only overwrite if improving confidence,0
Fill existing gaps,0
Retrieve original individual label indices,0
Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.,0
Fill existing gaps,0
Retrieve original individual label indices,0
ID swaps occur when X and Y simultaneously intersect each other.,0
Get only those bodypart pairs that belong to different individuals,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"See Li et al., 2012. Cross-view Activity Recognition using Hankelets.",0
"As proposed in the paper, the Hankel matrix can either be formed from",0
the tracklet's centroid or its normalized velocity.,0
"vel = np.diff(self.centroid, axis=0)",0
"vel /= np.linalg.norm(vel, axis=1, keepdims=True)",0
return self.hankelize(vel),0
TODO Add missing data imputation,1
"nrows, ncols = mat.shape",0
beta = nrows / ncols,0
omega = 0.56 * beta ** 3 - 0.95 * beta ** 2 + 1.82 * beta + 1.43,0
return np.argmin(s > omega * np.median(s)),0
"Note that if tracklets are very short, some may actually be part of the same track",0
and thus incorrectly reflect separate track endpoints...,0
"Map each Tracklet to an entry and output nodes and vice versa,",0
which is convenient once the tracklets are stitched.,0
Store tracklets and corresponding negatives (those that overlap in time),0
TODO Avoid looping over all pairs of tracklets,0
Pick the closest (spatially) overlapping tracklet,0
The algorithm works better with integer weights,0
Let us prune the graph by removing all source and sink edges,0
but those connecting the `n_tracks` first and last tracklets.,0
Preflow push seems to work slightly better than shortest,0
"augmentation path..., and is more computationally efficient.",0
Verify whether there are overlapping tracklets,0
"Pick the segment that minimizes ""smoothness"", computed here",0
with the coefficient of variation of the differences.,0
Rebuild a full graph from the remaining nodes without,0
temporal constraint on what tracklets can be stitched together.,0
Cycle through the residuals and incorporate back those,0
that only fit in a single tracklet.,0
Greedily add the remaining residuals,0
Refresh temporal bounds,0
Guarantee track data are sorted in the order defined in the config,0
Default to the distance cost function,0
should only exist one,0
Add in identity weighing before building the graph,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implementation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
average distance and average # significant differences avg. over comparisonbodyparts,0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
###################################################,0
Dependencies,0
###################################################,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as,0
specified in the config file,0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
Prepare figure,0
Mask rather than drop unwanted bodyparts to ensure consistent coloring,0
cropping!,0
nx=X2-X1,0
!/usr/bin/env python3,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"more videos are in principle covered, as OpenCV is used and allows many formats.",0
The above already raises a ValueError if formatting is wrong,0
"Rotate, see: https://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg",0
interesting option to just update metadata.,0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
Read the video until a frame is successfully read,0
import platform,0
"if platform.system() == ""Darwin"":  # for OSX use WXAgg",0
fig.canvas.start_event_loop(timeout=-1),0
else:,0
#fig.canvas.stop_event_loop(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"Windows throws error if file path is > 260 characters, can fix with prefix.",0
See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
uses openCV,0
cap.release() >> still used in frame_extraction!,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
This dictionary maps the model types to the file locations where the models exist.,0
Exit the function early if an unknown modeltype is provided.,0
Aliases for backwards-compatibility,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
Initialize costs matrix,0
get common bodypart detections in track pair,0
add 3rd dim to the points,0
cost for any point in time of t1 being the same,0
any point in time of t2,0
Get average cost of the entire track,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
check if it exists:,0
(average_degree / 2) as many edges as there are nodes is required,0
for undirected graphs to reach the target degree.,0
"Attention this order has to be consistent (for training set creation, training, inference etc.)",0
"-> adding (single,bpt) for uniquebodyparts",0
"-> adding (individual,bpt) for multianimalbodyparts",0
load defaults,0
set project specific parameters:,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
###################################################,0
Dependencies,0
###################################################,0
Pose X vs pose Y,0
Poses vs time,0
Likelihoods,0
Histograms,0
#################################################,0
Looping analysis over video,0
#################################################,0
"In a multi animal scenario, show more verbose errors.",0
Some vidoes were not evaluated.,0
Keep only the individuals and bodyparts that were labeled,0
Either display the animals defined in the config if they are found,0
"in the dataframe, or all the trajectories regardless of their names",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Determine whether the data are single- or multi-animal without loading into memory,0
simply by checking whether 'individuals' is in the second line of the CSV.,0
Make paths platform-agnostic if they are not already,0
Ensure folder names are strings,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
make list of full paths,0
filter list of videos,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"if ""MULTI!"" in allbpts:",0
Ensure same order as in config.yaml,0
ABBREVIATE NETWORK NAMES -- esp. for mobilenet!,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
Iterate over data files and stop as soon as one matching the scorer is found,0
If there was no match...,0
aliases for backwards-compatibility.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Set up a logger and add an stdout handler.,0
A single logger can have many handlers:,0
https://docs.python.org/3/howto/logging.html#handler-basic,0
TODO Dump to log file instead,1
"logger = logging.getLogger(""GUI"")",0
logger.setLevel(logging.DEBUG),0
handler = logging.StreamHandler(stream=sys.stdout),0
handler.setLevel(logging.DEBUG),0
formatter = logging.Formatter(,0
"""%(asctime)s - %(name)s - %(levelname)s - %(message)s"", ""%Y-%m-%d %H:%M:%S""",0
),0
handler.setFormatter(formatter),0
logger.addHandler(handler),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
restore the background region,0
Take into consideration imprecise OpenCV estimation of total number of frames,0
Automatically disable the draggable points,0
Save additional frames to the labeled-data directory,0
Store the newly-refined data,0
Uncertain keypoints are ignored,0
Merge with the already existing annotated data,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Thread-safe Stdout redirector,0
Creating action using the first constructor,0
Creating actions using the second constructor,0
File menu,0
View menu,0
Help menu,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
all_bodyparts: List,0
NOTE: Is there a case where a specific list should,0
have bodyparts other than the root? I don't think so.,0
Videotype selection,0
Select videos,0
Number of selected videos text,0
Clear video selection,0
"Qt returns a tuple (list of files, filetype)",0
Add tab header,0
Add separating line,0
Look for any extension by default,0
This works both with e.g. .avi and avi,0
Choose multiple files by default,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Automatically activate the napari-deeplabcut plugin,0
TODO Insert new video,1
TODO Insert skeleton link,1
"Hack to make the first column read-only, as we do not want users to touch it.",1
"The cleaner solution would be to use a QTreeView and QAbstractItemModel,",0
but that is a lot of rework for little benefits.,0
Leave untouched when it is already a string,0
"Slashes also raise the error, but no need to print anything since it is then likely to be a path",0
Walk backwards across parents to get all keys,0
Read the video until a frame is successfully read,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
TODO: Multi video select.... have to change to single video!,1
Shuffle,0
Num animals,0
Min swap length,0
Max gap to fill,0
Trail length,0
Filter type,0
Filter window length,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
Tracker Type,0
Num animals,0
Num triplets,0
layout.addWidget(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
Overwrite videos,0
Trail Points,0
Plot all bodyparts,0
Skeleton,0
Filtered data,0
Plot trajectories,0
High quality video,0
Bodypart list,0
Multianimal scenario.,0
Color is based on individual or bodypart.,0
Single animal scenario.,0
Color is based on bodypart.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Extraction method,0
Frame extraction algorithm,0
Frame cropping,0
Cluster step,0
GUI Slider width,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
use the default pose_cfg file for default values,0
Shuffle,0
Display iterations,0
Save iterations,0
Max iterations,0
Max number snapshots to keep,0
layout.addWidget(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Single / Multi animal Only Layouts,0
Dynamic bodypart cropping,0
Save results as csv,0
Filter predictions,0
Plot Trajectories,0
Show trajectory plots,0
Shuffle,0
layout.addLayout(tmp_layout),0
tmp_layout = QtWidgets.QGridLayout(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
Augmentation method,0
Neural Network,0
TODO: finish model_comparison,1
Check that training data files were indeed created.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Add config text field and button,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
layout.addWidget(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Display all images,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"calling create_tracking_dataset, train_tracking_transformer, stitch_tracklets",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
only try to find these features if they are in the dictionary,0
1) reference to video folder and get the proper bpt_feature file for feature table,0
2) get either the path to gt or the path to track pickle,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
tracklets,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"with npy list form videos, split each to train and test",0
"video_name = '.'.join(video.split(""/"")[-1].split(""."")[:-1])",0
assuming there is only one match,0
make my own model factory,0
make my own loss factory,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights",0
"NOTE: drop path for stochastic depth, we shall see if this is better than dropout here",0
only channel pooling,0
"x -> (B, 12, 2048)",0
"x -> (B, 12, 768)",0
trying to keep everything simple,0
need to write my own keypoint embedding,0
we don't need a fc layer here,0
x: inputs,0
"(B, 12, 768)",0
added pos embed,0
remove all parts I don't understand,0
forwarding through blocks. But num of blocks should be reduced,0
i can't really tell what the difference is,0
will have to change some of these,0
For old models that I trained prior to conv based patchification,0
To resize pos embedding when using model at different size from pretrained weights,0
Rescale the grid of position embeddings when loading from state_dict. Adapted from,0
https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224,0
Cut & paste from PyTorch official master until it's in a few official releases - RW,0
Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf,0
Computes standard normal cumulative distribution function,0
Values are generated by using a truncated uniform distribution and,0
then using the inverse CDF for the normal distribution.,0
Get upper and lower cdf values,0
"Uniformly fill tensor with values from [l, u], then translate to",0
"[2l-1, 2u-1].",0
Use inverse cdf transform for normal distribution to get truncated,0
standard normal,0
"Transform to proper mean, std",0
Clamp to ensure it's in the proper range,0
"type: (Tensor, float, float, float, float) -> Tensor",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2019 Ross Wightman,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Hacked together by / Copyright 2020 Ross Wightman,1
https://github.com/rwightman/pytorch-image-models/blob/main/timm/scheduler/scheduler_factory.py,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"resample if noise out of percent limit, brute force but shouldn't spin much",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
distmat g,0
q    1 3 2 4,0
4 1 2 3,0
"0, 2, 1, 3",0
"1, 2, 3, 0",0
compute cmc curve for each query,0
get query pid and camid,0
remove gallery samples that have the same pid and camid with query,0
compute cmc curve,0
"binary vector, positions with value 1 are correct matches",0
this condition is true when query identity does not appear in gallery,0
compute average precision,0
reference: https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Average_precision,0
"tmp_cmc = [x / (i + 1.) for i, x in enumerate(tmp_cmc)]",0
query,0
gallery,0
"distmat = re_ranking(qf, gf, k1=20, k2=6, lambda_value=0.3)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"if feature vector is numpy, you should use 'torch.tensor' transform it to tensor",0
print('using GPU to compute original distance'),0
print('starting re_ranking'),0
k-reciprocal neighbors,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
More elegantly one can simply define:,0
take care of difference between feature map space and original image space,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
cos = torch.cdist,0
TODO: maybe find a better spot for this.,1
"dev =  device if torch.cuda.is_available() else ""cpu""",0
train,0
print (f'validation loss {val_loss/len(val_loader)}'),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
normalize vectors here,0
i th vector at j th kpt,0
maybe needs to convert them to embeddings and position token,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Drop missing body parts,0
Drop points lying outside the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
## dlcnet_ms5: backbone resnet50 + multi-fusion & multi-stage module,0
## dlcr101_ms5/dlcr152_ms5: backbone resnet101/152 + multi-fusion & multi-stage module,0
"If the graph is unnecessarily large (with 15+ keypoints by default),",0
we randomly prune it to a size guaranteeing an average node degree of 6;,0
"see Suppl. Fig S9c in Lauer et al., 2022.",0
Use the skeleton defined in the config file,0
Ignore possible connections between 'multi' and 'unique' body parts;,0
one can never be too careful...,0
Disable the prediction of PAFs if the graph is empty,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving metadata and data file (Pickle file),0
###############################################################################,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Setting inference cfg file:,0
Search a pose_config.yaml file to parse missing information,0
Clean the training-datasets folder prior to recreating the data pickles,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Load updated lists:,0
Find the corresponding video file,0
"print(key, ""dropping?"")",0
adding important values for multianiaml project:,0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Determine the index length required to guarantee,0
the train–test ratio is exactly the desired one.,0
Pad indices so lengths agree,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(name,test_video_name)",0
Blazing fast and does not load the image into memory,0
Check that points lie within the image,0
DeprecationWarnings are silenced since Python 3.2 unless triggered in __main__,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
updating variable if null/None! #backwardscompatability,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
these values are dropped as scalecrop,0
doesn't have rotation implemented,0
Also drop maDLC smart cropping augmentation parameters,0
sort the model directories,0
get the shuffle index and offset by 1.,0
read cfg file,0
create log file,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Check for variable correctness,0
Moviepy:,0
select crop method,0
"Find the matching video from the config `video_sets`,",0
as it may be stored elsewhere than in the `videos` directory.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
allow_growth must be true here because tensorflow does not automatically free gpu memory and setting it as false occupies all gpu memory so that pytorch cannot kick in,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
should close tensorflow session here in order to free gpu,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"Flip x, y, confidence and reshape",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html#videocapture-get,0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
TODO: add cropping as in video analysis!,1
if cropping is not None:,0
cfg['cropping']=True,0
"cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping",0
"print(""Overwriting cropping parameters:"", cropping)",0
"print(""These are used for all videos, but won't be save to the cfg file."")",0
"Also ensure `boundingboxslack` is greater than zero, otherwise overlap",0
"between trackers cannot be evaluated, resulting in empty tracklets.",0
Check which snapshots are available and sort them by # iterations,0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
NOTE: If dataname line above is changed then line below is obsolete?,0
"trackname = trackname.replace(videofolder, destfolder)",0
TODO: adjust this for multi + unique bodyparts!,1
this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. ),0
Optimal identity assignment based on soft voting,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
imagenet mean for resnet pretraining:,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/DeepLabCut/DeepLabCut/pull/324",0
for DLC 2.2. (here all set False to not use PAFs/pairwise fields),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
"reloading defaults, as they can bleed over from a previous run otherwise",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
dlc_cfg_train = load_config(str(path_train_config)),0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
Name for deeplabcut net (based on its parameters),0
"DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])",0
"print(""Extracting maps for "", DLCscorer, "" with # of trainingiterations:"", trainingsiterations)",0
if notanalyzed: #this only applies to ask if h5 exists...,0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"for multi animal, seems only this is used",0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Get animal IDs and corresponding indices in the arrays of detections,0
Form ground truth beforehand,0
Assemble animals on the full set of detections,0
Count the number of unassembled bodyparts,0
Handle unlabeled bodyparts...,0
"Only 1 animal, let us return the full graph indices only",0
Find minimal skeleton,0
Select optimal PAF graph,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Selective copy; deepcopy is >5x slower,0
Only keeps skeletons that are more than 90% complete,0
TODO Normalize dists by longest length?,1
TODO Smarter imputation technique (Bayesian? Grassmann averages?),1
Covariance matrix estimation fails due to numerical singularities,0
Distance is undefined if the assembly is empty,0
Correct distance to account for missing observations,0
"Alternatively, reduce contribution of missing values to the Mahalanobis",0
distance to zero by substituting the corresponding means.,0
"Fill the subsets with unambiguous, complete individuals",0
Fuse superfluous assemblies,0
Second pass without edge safety,0
Mark the unique body parts as assembled anyway so,0
they are not used later on to fill assemblies.,0
Store selected edges for subsequent frames,0
Remove invalid assemblies,0
Fill assemblies with unconnected body parts,0
Spawning (rather than forking) multiple processes does not,0
work nicely with the GUI or interactive sessions.,0
"In that case, we fall back to the serial assembly.",0
Swap corresponding keypoints,0
Only consider assemblies of at least two keypoints,0
Greedy assembly matching like in pycocotools,0
Global rather than greedy assembly matching,0
Cast columns of dtype 'object' to float to avoid TypeError,0
further down in _parse_ground_truth_data.,0
sigma is taken as the median of all COCO keypoint standard deviations,0
Sort matching score (OKS) in descending order of assembly affinity,0
Guarantee precision decreases monotonically,0
See https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Regularize by forcing AR <= 5,0
max_ar = 5,0
if el.aspect_ratio >= max_ar:,0
if el.height > el.width:,0
el.width = el.height / max_ar,0
else:,0
el.height = el.width / max_ar,0
Orient the ellipse such that it encompasses most points,0
"n_inside = el.contains_points(np.c_[self.x, self.y]).sum()",0
el.theta += 0.5 * np.pi,0
"if el.contains_points(np.c_[self.x, self.y]).sum() < n_inside:",0
el.theta -= 0.5 * np.pi,0
"r2 = chi2.ppf(2 * norm.cdf(sd) - 1, 2)",0
"height, width = np.sqrt(E * r2)",0
The general quadratic curve has the form:,0
ax^2 + 2bxy + cy^2 + 2dx + 2fy + g = 0,0
Ellipse center coordinates,0
Semi-axes lengths,0
Angle to the horizontal,0
High uncertainty to the unobservable initial velocities,0
Give high uncertainty to the unobservable initial velocities,0
diff = val - cost_matrix,0
"diff[row, col] += val",0
if (,0
val < self.iou_threshold,0
or np.any(diff[row] <= 0.2),0
"or np.any(diff[:, col] <= 0.2)",0
):,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
Modified from scipy source code:,0
- to restrict its use to 2D,0
- to get rid of shuffling (since arrays are only (nbodyparts * 3) element long),0
TODO - factor in keypoint confidence (and weight by # of observations??),1
"mat = self.calc_pairwise_oks(poses, poses_ref)",0
Remove matched detections with low OKS,0
matches = [],0
"for row, col in zip(row_indices, col_indices):",0
"if mat[row, col] < self.oks_threshold:",0
unmatched_poses.append(row),0
unmatched_trackers.append(col),0
else:,0
"matches.append([row, col])",0
if not len(matches):,0
"matches = np.empty((0, 2), dtype=int)",0
else:,0
matches = np.stack(matches),0
update matched trackers with assigned detections,0
create and initialise new trackers for unmatched detections,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
filter out matched with low IOU,0
TODO: get cropping parameters and utilize!,1
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Load meta data & annotations,0
get the graph!,0
"NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*",0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
TODO: Make this code not so redundant!,1
"If a string was passed in, auto-convert to True for backward compatibility",0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to initial folder,0
"# Also storing one ""large"" table with results:",0
note: evaluationfolder.parents[0] to get common folder above all shuffle evaluations.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
#################################################,0
Load data...,0
#################################################,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
TODO: IMPLEMENT for different batch sizes?,1
Ignore best edges possibly defined during a prior evaluation,0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
Pass the image and the keypoints through the resizer;,0
this has no effect if no augmenters were added to it.,0
FIXME Is having an empty array vs nan really that necessary?!,1
"Form 2D array of shape (n_rows, 4) where the last dimension",0
"is (sample_index, peak_y, peak_x, bpt_index) to slice the PAFs.",0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in,0
Compute all distance statistics,0
Calculate overall prediction error,0
Skip data-driven skeleton selection unless,0
the model was trained on the full graph.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
in case there was already a graph,0
have to overwrite this,0
"loading backbone from ResNet, MobileNet etc.",0
"display_iters = max(1,int(displayiters))",0
"[_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})",0
Save snapshot,0
return to original path.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Clip peak locations to PAFs dimensions,0
"unit_vecs = vecs / lengths[:, np.newaxis]",0
"affinities = np.squeeze(y @ np.expand_dims(unit_vecs, axis=2)).sum(axis=1)",0
Form cost matrices,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"loading backbone from ResNet, MobileNet etc.",0
"Auto-switch to Adam on M1/M2 chips, as the momentum optimizer crashes",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Convert a frozen graph to OpenVINO IR format,0
Read network into memory,0
"For better efficiency, model is initialized for batch_size 1 and every sample processed independently",0
Load network to device,0
Prepare input data,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Change the stride from 2 to 1 to get 16x downscaling instead of 32x.,0
https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142,0
Multi stage is currently only implemented for resnets,0
Defining multi_fusion backbone,0
Attaching multi-stage decoder,0
stage_paf_output = stage_paf_output + pre_stage_paf_output,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
FIXME Fix wrong scope with Keras layers,1
"def prediction_layer(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2DTranspose(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=2,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
## New DLCNet Addition: multi-stage decoder,0
"def prediction_layer_stage(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2D(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=1,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Compute variance using: Var[X]= E[X^2] - E[X]^2.,0
Compute keep_prob,0
TODO(tanmingxing): add support for training progress.,1
Compute drop_connect tensor,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
turn into x times y time bs * bpts,0
extract corresponding locref x and y as well as probability,0
Filter predicted heatmaps with a 2D Gaussian kernel as in:,0
https://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_The_Devil_Is_in_the_Details_Delving_Into_Unbiased_Data_CVPR_2020_paper.pdf,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
defaults will be a public argument for namedtuple in Python 3.7,0
https://docs.python.org/3/library/collections.html#collections.namedtuple,0
Make sure that round down does not go down by more than 10%.,0
Builds the block accordings to arguments.,0
Expansion phase:,0
Depth-wise convolution phase:,0
Squeeze and Excitation layer.,0
Output phase:,0
only apply drop_connect if skip presents.,0
Expansion phase:,0
Output phase:,0
only apply drop_connect if skip presents.,0
Builds blocks.,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Stem part.,0
Head part.,0
Calls Stem layers,0
Calls blocks.,0
Calls final layers and returns logits.,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
"(width_coefficient, depth_coefficient, resolution, dropout_rate)",0
blocks_args = [,0
"'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',",0
"'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',",0
"'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',",0
"'r1_k3_s11_e6_i192_o320_se0.25',",0
],0
The default is TPU-specific batch norm.,0
The alternative is tf.layers.BatchNormalization.,0
"batch_norm=utils.TpuBatchNormalization,  # TPU-specific requirement.",0
ValueError will be raised here if override_params has fields not included,0
in global_params.,0
,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrunk when given a depth-multiplier < 1.0.,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Clamp to 40% of crop size to ensure that at least,0
the center keypoint remains visible after the offset is applied.,0
Points located close to one another are sampled preferentially,0
in order to augment crowded regions.,0
Include keypoints in the count to avoid null probabilities,0
Shift the crop center in both dimensions by random amounts,0
and normalize to the original image dimensions.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Pickle the 'data' dictionary using the highest protocol available.,0
item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal,0
3 for likelihood,0
we only do single animal here,0
let's hard code it,0
"Add smart, keypoint-aware image cropping",0
empty prediction for this frame,0
in case it's empty prediction,0
Scale is sampled only once (per batch) to transform all of the images into same size.,0
Discard keypoints whose coordinates lie outside the cropped image,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
Produce score maps and location refinement fields,0
Find indices of individuals in joint_id,0
WIP!,0
STD of gaussian is 1/4 of threshold,0
Grid of coordinates,0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Distance between the joint point and each coordinate,0
NEEDS TO BE DONE!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
TODO Finish implementing actual abstract class,1
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
parameter initialization for augmentation pipeline:,0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images of a batch into same size.,0
Approximating the scale,0
invisible joints are represented by nans,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"First, initializing variables (if they don't exist)",0
what is the fraction of training samples with scaling augmentation?,0
loading defaults for rotation range!,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"cfg[""rotateratio""] = cfg.get(",0
"""rotratio"", 0.4",0
)  # what is the fraction of training samples with rotation augmentation?,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"Auto cropping is new (was not in Nature Neuroscience 2018 paper, but introduced in Nath et al. Nat. Protocols 2019)",0
"and boosts performance by 2X, particularly on challenging datasets, like the cheetah in Nath et al.",0
Parameters for augmentation with regard to cropping:,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Get rid of the interpolation beyond the spline knots,0
Retrieve original individual label indices,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Check the training image paths are correctly stored as arrays of strings,0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
TENSORPACK could fail in WINDOWS...,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Loading example data set,0
# Create one identical splits for 3 networks and 3 augmentations,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
Setting specific parameters for training,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"MODELS = [""dlcrnet_ms5"", ""dlcr101_ms5"", ""efficientnet-b0"", ""mobilenet_v2_0.35""]",0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
## adding it here,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Downloading the calibration images,0
Deleting unnecessary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Loading example data set,0
example how to set pose config variables:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
test if existing models are found:,0
Test if nonexisting models are not found,0
Add some other office files:,0
Add a .pickle and .h5 files,0
"By default, all videos with common extensions are taken from a directory",0
A list of extensions can also be passed in,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Test bounding box from a single keypoint,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Test that shape was preserved after vectorization,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Guarantee that images smaller than crop size are handled fine,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Disable stochastic scale jitter,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Verify the aspect ratio is preserved,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Load sample multi animal data,0
Generate a random permutation and reorder data. Ignore the unique bodypart,0
Get inverse permutation and reorder the modified data to get back,0
to the original,0
Check,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"Check data comprise path, shape, and xy coordinates",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Break the graph to test stitching failure,0
Add fake IDs,0
Split all tracklets in half,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Test flipped keypoints,0
Original (cached) coordinates must have remained empty,0
Assemble based on the smallest graph to speed up testing,0
Generate fake identity predictions,0
Assemble based on the smallest graph to speed up testing,0
Test now with identity only and ensure assemblies,0
contain only parts of a single group ID.,0
Test empty assembly,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Verify that the Hugging Face folder was removed,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
def install(package):,0
"subprocess.check_call([sys.executable, ""-m"", ""pip"", ""install"", package])",0
"install(""tensorflow==1.13.1"")",0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
quicker variant,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Suppress tensorflow warning messages,0
"Train, evaluate & predict functions / all require TF",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Updating config file:,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path, num_shuffles=auxfun_models.Check4weights(pose_cfg['net_type'], parent_path, num_shuffles= 1)",0
Updating training and test pose_cfg:,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Read the config file,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
common parameters:,0
Write dictionary to yaml  config file,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
looking for the pseudo label path,0
spatial pyramid is not for adapted model,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
ratio_h and ratio_w should match though in reality it does not match exactly,0
only keep the max,0
Compute cosine similarity,0
"len(frames) -> (n_scale,)",0
"frames[0].shape - > (batchsize, h, w, 3)",0
no crop needed,0
"batch full, start true inferencing",0
only do this when animal is detected,0
in case we reach the end of the video,0
add a temp folder for checkpoint,0
"if the scale_list is empty, by default we use the original one",0
extra data,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"TODO(stes) mocking a few modules to rely in fewer dependencies, without",1
causing import errors when using deeplabcut.,0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in.,0
TODO(jeylau) add affinity.setter to Assembly,1
TODO(stes): remove/rewrite,1
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"ignore the exception and continue with the next evaluation, without",0
yielding a result value.,0
"return the result value, with NaN as the result for all metrics that",0
could not be computed due to the error.,0
raise the error and stop evaluation,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Read the config file and related variables,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Flatten the list of bodyparts to connect,0
Format data,0
Trick to force equal aspect ratio of 3D plots,0
Set up the matplotlib figure beforehand,0
Set up skeleton LineCollections,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
Get track_method and do related checks,0
Get track method suffix,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
"## Assign nan to [X,Y] of low likelihood predictions ###",0
Convert the data to a np array to easily mask out the low likelihood predictions,0
"Assign [X,Y] = nan to low likelihood predictions",0
Reshape data back to original shape,0
put data back to the dataframes,0
Check individuals are the same in both views,0
Cross-view match individuals,0
Create a dummy variables for single-animal,0
Cleaner variable (since inds view1 == inds view2),0
"Reshape: (num_framex, num_individuals, num_bodyparts , 2)",0
Triangulate data,0
i is individual in view 1,0
voting[i] is the matched individual in view 2,0
Create 3D DataFrame column and row indices,0
Swap num_animals with num_frames axes to ensure well-behaving reshape,0
Fill up 3D dataframe,0
Reorder 2D dataframe in view 2 to match order of view 1,0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Alternatively,0
Sort tracklets by length to prioritize greater continuity,0
Drop tracklets that are too short,0
"Where slots are available, copy the data over",0
"If about to overwrite data, keep tracklets with highest confidence",0
Squeeze some data into empty slots,0
"For the remaining data, overwrite where we are least confident",0
Find keypoints closest to the remaining data,0
Use Manhattan distance to avoid overflow,0
Only overwrite if improving confidence,0
Fill existing gaps,0
Retrieve original individual label indices,0
Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.,0
Fill existing gaps,0
Retrieve original individual label indices,0
ID swaps occur when X and Y simultaneously intersect each other.,0
Get only those bodypart pairs that belong to different individuals,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"See Li et al., 2012. Cross-view Activity Recognition using Hankelets.",0
"As proposed in the paper, the Hankel matrix can either be formed from",0
the tracklet's centroid or its normalized velocity.,0
"vel = np.diff(self.centroid, axis=0)",0
"vel /= np.linalg.norm(vel, axis=1, keepdims=True)",0
return self.hankelize(vel),0
TODO Add missing data imputation,1
"nrows, ncols = mat.shape",0
beta = nrows / ncols,0
omega = 0.56 * beta ** 3 - 0.95 * beta ** 2 + 1.82 * beta + 1.43,0
return np.argmin(s > omega * np.median(s)),0
"Note that if tracklets are very short, some may actually be part of the same track",0
and thus incorrectly reflect separate track endpoints...,0
"Map each Tracklet to an entry and output nodes and vice versa,",0
which is convenient once the tracklets are stitched.,0
Store tracklets and corresponding negatives (those that overlap in time),0
TODO Avoid looping over all pairs of tracklets,0
Pick the closest (spatially) overlapping tracklet,0
The algorithm works better with integer weights,0
Let us prune the graph by removing all source and sink edges,0
but those connecting the `n_tracks` first and last tracklets.,0
Preflow push seems to work slightly better than shortest,0
"augmentation path..., and is more computationally efficient.",0
Verify whether there are overlapping tracklets,0
"Pick the segment that minimizes ""smoothness"", computed here",0
with the coefficient of variation of the differences.,0
Rebuild a full graph from the remaining nodes without,0
temporal constraint on what tracklets can be stitched together.,0
Cycle through the residuals and incorporate back those,0
that only fit in a single tracklet.,0
Greedily add the remaining residuals,0
Refresh temporal bounds,0
Default to the distance cost function,0
should only exist one,0
Add in identity weighing before building the graph,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implementation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
average distance and average # significant differences avg. over comparisonbodyparts,0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
###################################################,0
Dependencies,0
###################################################,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as,0
specified in the config file,0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
Prepare figure,0
will otherwise always return ellipse,0
Mask rather than drop unwanted bodyparts to ensure consistent coloring,0
cropping!,0
nx=X2-X1,0
!/usr/bin/env python3,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"more videos are in principle covered, as OpenCV is used and allows many formats.",0
The above already raises a ValueError if formatting is wrong,0
"Rotate, see: https://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg",0
interesting option to just update metadata.,0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
Read the video until a frame is successfully read,0
import platform,0
"if platform.system() == ""Darwin"":  # for OSX use WXAgg",0
fig.canvas.start_event_loop(timeout=-1),0
else:,0
#fig.canvas.stop_event_loop(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"Windows throws error if file path is > 260 characters, can fix with prefix.",0
See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
uses openCV,0
cap.release() >> still used in frame_extraction!,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
This dictionary maps the model types to the file locations where the models exist.,0
Exit the function early if an unknown modeltype is provided.,0
Aliases for backwards-compatibility,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
Initialize costs matrix,0
get common bodypart detections in track pair,0
add 3rd dim to the points,0
cost for any point in time of t1 being the same,0
any point in time of t2,0
Get average cost of the entire track,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
check if it exists:,0
(average_degree / 2) as many edges as there are nodes is required,0
for undirected graphs to reach the target degree.,0
"Attention this order has to be consistent (for training set creation, training, inference etc.)",0
"-> adding (single,bpt) for uniquebodyparts",0
"-> adding (individual,bpt) for multianimalbodyparts",0
load defaults,0
set project specific parameters:,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
###################################################,0
Dependencies,0
###################################################,0
Pose X vs pose Y,0
Poses vs time,0
Likelihoods,0
Histograms,0
#################################################,0
Looping analysis over video,0
#################################################,0
"In a multi animal scenario, show more verbose errors.",0
Some vidoes were not evaluated.,0
Keep only the individuals and bodyparts that were labeled,0
Either display the animals defined in the config if they are found,0
"in the dataframe, or all the trajectories regardless of their names",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Determine whether the data are single- or multi-animal without loading into memory,0
simply by checking whether 'individuals' is in the second line of the CSV.,0
Make paths platform-agnostic if they are not already,0
Ensure folder names are strings,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
make list of full paths,0
filter list of videos,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"if ""MULTI!"" in allbpts:",0
Ensure same order as in config.yaml,0
ABBREVIATE NETWORK NAMES -- esp. for mobilenet!,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
Iterate over data files and stop as soon as one matching the scorer is found,0
If there was no match...,0
aliases for backwards-compatibility.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Set up a logger and add an stdout handler.,0
A single logger can have many handlers:,0
https://docs.python.org/3/howto/logging.html#handler-basic,0
TODO Dump to log file instead,1
"logger = logging.getLogger(""GUI"")",0
logger.setLevel(logging.DEBUG),0
handler = logging.StreamHandler(stream=sys.stdout),0
handler.setLevel(logging.DEBUG),0
formatter = logging.Formatter(,0
"""%(asctime)s - %(name)s - %(levelname)s - %(message)s"", ""%Y-%m-%d %H:%M:%S""",0
),0
handler.setFormatter(formatter),0
logger.addHandler(handler),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
restore the background region,0
Take into consideration imprecise OpenCV estimation of total number of frames,0
Automatically disable the draggable points,0
Save additional frames to the labeled-data directory,0
Store the newly-refined data,0
Uncertain keypoints are ignored,0
Merge with the already existing annotated data,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Thread-safe Stdout redirector,0
Creating action using the first constructor,0
Creating actions using the second constructor,0
File menu,0
View menu,0
Help menu,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
all_bodyparts: List,0
NOTE: Is there a case where a specific list should,0
have bodyparts other than the root? I don't think so.,0
Videotype selection,0
Select videos,0
Number of selected videos text,0
Clear video selection,0
"Qt returns a tuple (list of files, filetype)",0
Add tab header,0
Add separating line,0
Look for any extension by default,0
This works both with e.g. .avi and avi,0
Choose multiple files by default,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Automatically activate the napari-deeplabcut plugin,0
TODO Insert new video,1
TODO Insert skeleton link,1
"Hack to make the first column read-only, as we do not want users to touch it.",1
"The cleaner solution would be to use a QTreeView and QAbstractItemModel,",0
but that is a lot of rework for little benefits.,0
Leave untouched when it is already a string,0
"Slashes also raise the error, but no need to print anything since it is then likely to be a path",0
Walk backwards across parents to get all keys,0
Read the video until a frame is successfully read,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
TODO: Multi video select.... have to change to single video!,1
Shuffle,0
Num animals,0
Min swap length,0
Max gap to fill,0
Trail length,0
Filter type,0
Filter window length,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
Tracker Type,0
Num animals,0
Num triplets,0
layout.addWidget(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
Overwrite videos,0
Trail Points,0
Plot all bodyparts,0
Skeleton,0
Filtered data,0
Plot trajectories,0
High quality video,0
Bodypart list,0
Multianimal scenario.,0
Color is based on individual or bodypart.,0
Single animal scenario.,0
Color is based on bodypart.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Extraction method,0
Frame extraction algorithm,0
Frame cropping,0
Cluster step,0
GUI Slider width,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
use the default pose_cfg file for default values,0
Shuffle,0
Display iterations,0
Save iterations,0
Max iterations,0
Max number snapshots to keep,0
layout.addWidget(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Single / Multi animal Only Layouts,0
Dynamic bodypart cropping,0
Save results as csv,0
Filter predictions,0
Plot Trajectories,0
Show trajectory plots,0
Shuffle,0
layout.addLayout(tmp_layout),0
tmp_layout = QtWidgets.QGridLayout(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
Augmentation method,0
Neural Network,0
TODO: finish model_comparison,1
Check that training data files were indeed created.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Add config text field and button,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
layout.addWidget(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Display all images,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"calling create_tracking_dataset, train_tracking_transformer, stitch_tracklets",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
only try to find these features if they are in the dictionary,0
1) reference to video folder and get the proper bpt_feature file for feature table,0
2) get either the path to gt or the path to track pickle,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
tracklets,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"with npy list form videos, split each to train and test",0
"video_name = '.'.join(video.split(""/"")[-1].split(""."")[:-1])",0
assuming there is only one match,0
make my own model factory,0
make my own loss factory,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights",0
"NOTE: drop path for stochastic depth, we shall see if this is better than dropout here",0
only channel pooling,0
"x -> (B, 12, 2048)",0
"x -> (B, 12, 768)",0
trying to keep everything simple,0
need to write my own keypoint embedding,0
we don't need a fc layer here,0
x: inputs,0
"(B, 12, 768)",0
added pos embed,0
remove all parts I don't understand,0
forwarding through blocks. But num of blocks should be reduced,0
i can't really tell what the difference is,0
will have to change some of these,0
For old models that I trained prior to conv based patchification,0
To resize pos embedding when using model at different size from pretrained weights,0
Rescale the grid of position embeddings when loading from state_dict. Adapted from,0
https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224,0
Cut & paste from PyTorch official master until it's in a few official releases - RW,0
Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf,0
Computes standard normal cumulative distribution function,0
Values are generated by using a truncated uniform distribution and,0
then using the inverse CDF for the normal distribution.,0
Get upper and lower cdf values,0
"Uniformly fill tensor with values from [l, u], then translate to",0
"[2l-1, 2u-1].",0
Use inverse cdf transform for normal distribution to get truncated,0
standard normal,0
"Transform to proper mean, std",0
Clamp to ensure it's in the proper range,0
"type: (Tensor, float, float, float, float) -> Tensor",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2019 Ross Wightman,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Hacked together by / Copyright 2020 Ross Wightman,1
https://github.com/rwightman/pytorch-image-models/blob/main/timm/scheduler/scheduler_factory.py,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"resample if noise out of percent limit, brute force but shouldn't spin much",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
distmat g,0
q    1 3 2 4,0
4 1 2 3,0
"0, 2, 1, 3",0
"1, 2, 3, 0",0
compute cmc curve for each query,0
get query pid and camid,0
remove gallery samples that have the same pid and camid with query,0
compute cmc curve,0
"binary vector, positions with value 1 are correct matches",0
this condition is true when query identity does not appear in gallery,0
compute average precision,0
reference: https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Average_precision,0
"tmp_cmc = [x / (i + 1.) for i, x in enumerate(tmp_cmc)]",0
query,0
gallery,0
"distmat = re_ranking(qf, gf, k1=20, k2=6, lambda_value=0.3)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"if feature vector is numpy, you should use 'torch.tensor' transform it to tensor",0
print('using GPU to compute original distance'),0
print('starting re_ranking'),0
k-reciprocal neighbors,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
More elegantly one can simply define:,0
take care of difference between feature map space and original image space,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
cos = torch.cdist,0
TODO: maybe find a better spot for this.,1
"dev =  device if torch.cuda.is_available() else ""cpu""",0
train,0
print (f'validation loss {val_loss/len(val_loader)}'),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
normalize vectors here,0
i th vector at j th kpt,0
maybe needs to convert them to embeddings and position token,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Drop missing body parts,0
Drop points lying outside the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
## dlcnet_ms5: backbone resnet50 + multi-fusion & multi-stage module,0
## dlcr101_ms5/dlcr152_ms5: backbone resnet101/152 + multi-fusion & multi-stage module,0
"If the graph is unnecessarily large (with 15+ keypoints by default),",0
we randomly prune it to a size guaranteeing an average node degree of 6;,0
"see Suppl. Fig S9c in Lauer et al., 2022.",0
Use the skeleton defined in the config file,0
Ignore possible connections between 'multi' and 'unique' body parts;,0
one can never be too careful...,0
Disable the prediction of PAFs if the graph is empty,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving metadata and data file (Pickle file),0
###############################################################################,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Setting inference cfg file:,0
Search a pose_config.yaml file to parse missing information,0
Clean the training-datasets folder prior to recreating the data pickles,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Load updated lists:,0
Find the corresponding video file,0
"print(key, ""dropping?"")",0
adding important values for multianiaml project:,0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Determine the index length required to guarantee,0
the train–test ratio is exactly the desired one.,0
Pad indices so lengths agree,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(name,test_video_name)",0
Blazing fast and does not load the image into memory,0
Check that points lie within the image,0
DeprecationWarnings are silenced since Python 3.2 unless triggered in __main__,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
updating variable if null/None! #backwardscompatability,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
these values are dropped as scalecrop,0
doesn't have rotation implemented,0
Also drop maDLC smart cropping augmentation parameters,0
sort the model directories,0
get the shuffle index and offset by 1.,0
read cfg file,0
create log file,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Check for variable correctness,0
Moviepy:,0
select crop method,0
"Find the matching video from the config `video_sets`,",0
as it may be stored elsewhere than in the `videos` directory.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
allow_growth must be true here because tensorflow does not automatically free gpu memory and setting it as false occupies all gpu memory so that pytorch cannot kick in,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
should close tensorflow session here in order to free gpu,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"Flip x, y, confidence and reshape",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html#videocapture-get,0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
TODO: add cropping as in video analysis!,1
if cropping is not None:,0
cfg['cropping']=True,0
"cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping",0
"print(""Overwriting cropping parameters:"", cropping)",0
"print(""These are used for all videos, but won't be save to the cfg file."")",0
"Also ensure `boundingboxslack` is greater than zero, otherwise overlap",0
"between trackers cannot be evaluated, resulting in empty tracklets.",0
Check which snapshots are available and sort them by # iterations,0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
NOTE: If dataname line above is changed then line below is obsolete?,0
"trackname = trackname.replace(videofolder, destfolder)",0
TODO: adjust this for multi + unique bodyparts!,1
this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. ),0
Optimal identity assignment based on soft voting,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
imagenet mean for resnet pretraining:,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/DeepLabCut/DeepLabCut/pull/324",0
for DLC 2.2. (here all set False to not use PAFs/pairwise fields),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
"reloading defaults, as they can bleed over from a previous run otherwise",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
dlc_cfg_train = load_config(str(path_train_config)),0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
Name for deeplabcut net (based on its parameters),0
"DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])",0
"print(""Extracting maps for "", DLCscorer, "" with # of trainingiterations:"", trainingsiterations)",0
if notanalyzed: #this only applies to ask if h5 exists...,0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"for multi animal, seems only this is used",0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Get animal IDs and corresponding indices in the arrays of detections,0
Form ground truth beforehand,0
Assemble animals on the full set of detections,0
Count the number of unassembled bodyparts,0
Handle unlabeled bodyparts...,0
"Only 1 animal, let us return the full graph indices only",0
Find minimal skeleton,0
Select optimal PAF graph,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Selective copy; deepcopy is >5x slower,0
Only keeps skeletons that are more than 90% complete,0
TODO Normalize dists by longest length?,1
TODO Smarter imputation technique (Bayesian? Grassmann averages?),1
Covariance matrix estimation fails due to numerical singularities,0
Distance is undefined if the assembly is empty,0
Correct distance to account for missing observations,0
"Alternatively, reduce contribution of missing values to the Mahalanobis",0
distance to zero by substituting the corresponding means.,0
"Fill the subsets with unambiguous, complete individuals",0
Fuse superfluous assemblies,0
Second pass without edge safety,0
Mark the unique body parts as assembled anyway so,0
they are not used later on to fill assemblies.,0
Store selected edges for subsequent frames,0
Remove invalid assemblies,0
Fill assemblies with unconnected body parts,0
Spawning (rather than forking) multiple processes does not,0
work nicely with the GUI or interactive sessions.,0
"In that case, we fall back to the serial assembly.",0
Swap corresponding keypoints,0
Only consider assemblies of at least two keypoints,0
Greedy assembly matching like in pycocotools,0
Global rather than greedy assembly matching,0
Cast columns of dtype 'object' to float to avoid TypeError,0
further down in _parse_ground_truth_data.,0
sigma is taken as the median of all COCO keypoint standard deviations,0
Sort matching score (OKS) in descending order of assembly affinity,0
Guarantee precision decreases monotonically,0
See https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Regularize by forcing AR <= 5,0
max_ar = 5,0
if el.aspect_ratio >= max_ar:,0
if el.height > el.width:,0
el.width = el.height / max_ar,0
else:,0
el.height = el.width / max_ar,0
Orient the ellipse such that it encompasses most points,0
"n_inside = el.contains_points(np.c_[self.x, self.y]).sum()",0
el.theta += 0.5 * np.pi,0
"if el.contains_points(np.c_[self.x, self.y]).sum() < n_inside:",0
el.theta -= 0.5 * np.pi,0
"r2 = chi2.ppf(2 * norm.cdf(sd) - 1, 2)",0
"height, width = np.sqrt(E * r2)",0
The general quadratic curve has the form:,0
ax^2 + 2bxy + cy^2 + 2dx + 2fy + g = 0,0
Ellipse center coordinates,0
Semi-axes lengths,0
Angle to the horizontal,0
High uncertainty to the unobservable initial velocities,0
Give high uncertainty to the unobservable initial velocities,0
diff = val - cost_matrix,0
"diff[row, col] += val",0
if (,0
val < self.iou_threshold,0
or np.any(diff[row] <= 0.2),0
"or np.any(diff[:, col] <= 0.2)",0
):,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
Modified from scipy source code:,0
- to restrict its use to 2D,0
- to get rid of shuffling (since arrays are only (nbodyparts * 3) element long),0
TODO - factor in keypoint confidence (and weight by # of observations??),1
"mat = self.calc_pairwise_oks(poses, poses_ref)",0
Remove matched detections with low OKS,0
matches = [],0
"for row, col in zip(row_indices, col_indices):",0
"if mat[row, col] < self.oks_threshold:",0
unmatched_poses.append(row),0
unmatched_trackers.append(col),0
else:,0
"matches.append([row, col])",0
if not len(matches):,0
"matches = np.empty((0, 2), dtype=int)",0
else:,0
matches = np.stack(matches),0
update matched trackers with assigned detections,0
create and initialise new trackers for unmatched detections,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
filter out matched with low IOU,0
TODO: get cropping parameters and utilize!,1
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Load meta data & annotations,0
get the graph!,0
"NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*",0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
TODO: Make this code not so redundant!,1
"If a string was passed in, auto-convert to True for backward compatibility",0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to initial folder,0
"# Also storing one ""large"" table with results:",0
note: evaluationfolder.parents[0] to get common folder above all shuffle evaluations.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
#################################################,0
Load data...,0
#################################################,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
TODO: IMPLEMENT for different batch sizes?,1
Ignore best edges possibly defined during a prior evaluation,0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
Pass the image and the keypoints through the resizer;,0
this has no effect if no augmenters were added to it.,0
FIXME Is having an empty array vs nan really that necessary?!,1
"Form 2D array of shape (n_rows, 4) where the last dimension",0
"is (sample_index, peak_y, peak_x, bpt_index) to slice the PAFs.",0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in,0
Compute all distance statistics,0
Calculate overall prediction error,0
Skip data-driven skeleton selection unless,0
the model was trained on the full graph.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
in case there was already a graph,0
have to overwrite this,0
"loading backbone from ResNet, MobileNet etc.",0
"display_iters = max(1,int(displayiters))",0
"[_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})",0
Save snapshot,0
return to original path.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Clip peak locations to PAFs dimensions,0
"unit_vecs = vecs / lengths[:, np.newaxis]",0
"affinities = np.squeeze(y @ np.expand_dims(unit_vecs, axis=2)).sum(axis=1)",0
Form cost matrices,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"loading backbone from ResNet, MobileNet etc.",0
"Auto-switch to Adam on M1/M2 chips, as the momentum optimizer crashes",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Convert a frozen graph to OpenVINO IR format,0
Read network into memory,0
"For better efficiency, model is initialized for batch_size 1 and every sample processed independently",0
Load network to device,0
Prepare input data,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Change the stride from 2 to 1 to get 16x downscaling instead of 32x.,0
https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142,0
Multi stage is currently only implemented for resnets,0
Defining multi_fusion backbone,0
Attaching multi-stage decoder,0
stage_paf_output = stage_paf_output + pre_stage_paf_output,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
FIXME Fix wrong scope with Keras layers,1
"def prediction_layer(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2DTranspose(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=2,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
## New DLCNet Addition: multi-stage decoder,0
"def prediction_layer_stage(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2D(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=1,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Compute variance using: Var[X]= E[X^2] - E[X]^2.,0
Compute keep_prob,0
TODO(tanmingxing): add support for training progress.,1
Compute drop_connect tensor,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
turn into x times y time bs * bpts,0
extract corresponding locref x and y as well as probability,0
Filter predicted heatmaps with a 2D Gaussian kernel as in:,0
https://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_The_Devil_Is_in_the_Details_Delving_Into_Unbiased_Data_CVPR_2020_paper.pdf,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
defaults will be a public argument for namedtuple in Python 3.7,0
https://docs.python.org/3/library/collections.html#collections.namedtuple,0
Make sure that round down does not go down by more than 10%.,0
Builds the block accordings to arguments.,0
Expansion phase:,0
Depth-wise convolution phase:,0
Squeeze and Excitation layer.,0
Output phase:,0
only apply drop_connect if skip presents.,0
Expansion phase:,0
Output phase:,0
only apply drop_connect if skip presents.,0
Builds blocks.,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Stem part.,0
Head part.,0
Calls Stem layers,0
Calls blocks.,0
Calls final layers and returns logits.,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
"(width_coefficient, depth_coefficient, resolution, dropout_rate)",0
blocks_args = [,0
"'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',",0
"'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',",0
"'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',",0
"'r1_k3_s11_e6_i192_o320_se0.25',",0
],0
The default is TPU-specific batch norm.,0
The alternative is tf.layers.BatchNormalization.,0
"batch_norm=utils.TpuBatchNormalization,  # TPU-specific requirement.",0
ValueError will be raised here if override_params has fields not included,0
in global_params.,0
,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrunk when given a depth-multiplier < 1.0.,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Clamp to 40% of crop size to ensure that at least,0
the center keypoint remains visible after the offset is applied.,0
Points located close to one another are sampled preferentially,0
in order to augment crowded regions.,0
Include keypoints in the count to avoid null probabilities,0
Shift the crop center in both dimensions by random amounts,0
and normalize to the original image dimensions.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Pickle the 'data' dictionary using the highest protocol available.,0
item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal,0
3 for likelihood,0
we only do single animal here,0
let's hard code it,0
"Add smart, keypoint-aware image cropping",0
empty prediction for this frame,0
in case it's empty prediction,0
Scale is sampled only once (per batch) to transform all of the images into same size.,0
Discard keypoints whose coordinates lie outside the cropped image,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
Produce score maps and location refinement fields,0
Find indices of individuals in joint_id,0
WIP!,0
STD of gaussian is 1/4 of threshold,0
Grid of coordinates,0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Distance between the joint point and each coordinate,0
NEEDS TO BE DONE!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
TODO Finish implementing actual abstract class,1
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
parameter initialization for augmentation pipeline:,0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images of a batch into same size.,0
Approximating the scale,0
invisible joints are represented by nans,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"First, initializing variables (if they don't exist)",0
what is the fraction of training samples with scaling augmentation?,0
loading defaults for rotation range!,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"cfg[""rotateratio""] = cfg.get(",0
"""rotratio"", 0.4",0
)  # what is the fraction of training samples with rotation augmentation?,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"Auto cropping is new (was not in Nature Neuroscience 2018 paper, but introduced in Nath et al. Nat. Protocols 2019)",0
"and boosts performance by 2X, particularly on challenging datasets, like the cheetah in Nath et al.",0
Parameters for augmentation with regard to cropping:,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Get rid of the interpolation beyond the spline knots,0
Retrieve original individual label indices,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Check the training image paths are correctly stored as arrays of strings,0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
TENSORPACK could fail in WINDOWS...,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Loading example data set,0
# Create one identical splits for 3 networks and 3 augmentations,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
Setting specific parameters for training,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"MODELS = [""dlcrnet_ms5"", ""dlcr101_ms5"", ""efficientnet-b0"", ""mobilenet_v2_0.35""]",0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
## adding it here,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Downloading the calibration images,0
Deleting unnecessary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Loading example data set,0
example how to set pose config variables:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
test if existing models are found:,0
Test if nonexisting models are not found,0
Add some other office files:,0
Add a .pickle and .h5 files,0
"By default, all videos with common extensions are taken from a directory",0
A list of extensions can also be passed in,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Test bounding box from a single keypoint,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Test that shape was preserved after vectorization,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Guarantee that images smaller than crop size are handled fine,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Disable stochastic scale jitter,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Verify the aspect ratio is preserved,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Load sample multi animal data,0
Generate a random permutation and reorder data. Ignore the unique bodypart,0
Get inverse permutation and reorder the modified data to get back,0
to the original,0
Check,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"Check data comprise path, shape, and xy coordinates",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Break the graph to test stitching failure,0
Add fake IDs,0
Split all tracklets in half,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Test flipped keypoints,0
Original (cached) coordinates must have remained empty,0
Assemble based on the smallest graph to speed up testing,0
Generate fake identity predictions,0
Assemble based on the smallest graph to speed up testing,0
Test now with identity only and ensure assemblies,0
contain only parts of a single group ID.,0
Test empty assembly,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Verify that the Hugging Face folder was removed,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
def install(package):,0
"subprocess.check_call([sys.executable, ""-m"", ""pip"", ""install"", package])",0
"install(""tensorflow==1.13.1"")",0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
quicker variant,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Suppress tensorflow warning messages,0
"Train, evaluate & predict functions / all require TF",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Updating config file:,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path, num_shuffles=auxfun_models.Check4weights(pose_cfg['net_type'], parent_path, num_shuffles= 1)",0
Updating training and test pose_cfg:,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Read the config file,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
common parameters:,0
Write dictionary to yaml  config file,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"TODO(stes) mocking a few modules to rely in fewer dependencies, without",1
causing import errors when using deeplabcut.,0
Map image paths from predicted data to GT as the first are typically,0
absolute whereas the latter are relative to the project path.,0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in.,0
TODO(jeylau) add affinity.setter to Assembly,1
TODO(stes): remove/rewrite,1
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"ignore the exception and continue with the next evaluation, without",0
yielding a result value.,0
"return the result value, with NaN as the result for all metrics that",0
could not be computed due to the error.,0
raise the error and stop evaluation,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Read the config file and related variables,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Flatten the list of bodyparts to connect,0
Format data,0
Trick to force equal aspect ratio of 3D plots,0
Set up the matplotlib figure beforehand,0
Set up skeleton LineCollections,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
Get track_method and do related checks,0
Get track method suffix,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
"## Assign nan to [X,Y] of low likelihood predictions ###",0
Convert the data to a np array to easily mask out the low likelihood predictions,0
"Assign [X,Y] = nan to low likelihood predictions",0
Reshape data back to original shape,0
put data back to the dataframes,0
Check individuals are the same in both views,0
Cross-view match individuals,0
Create a dummy variables for single-animal,0
Cleaner variable (since inds view1 == inds view2),0
"Reshape: (num_framex, num_individuals, num_bodyparts , 2)",0
Triangulate data,0
i is individual in view 1,0
voting[i] is the matched individual in view 2,0
Create 3D DataFrame column and row indices,0
Swap num_animals with num_frames axes to ensure well-behaving reshape,0
Fill up 3D dataframe,0
Reorder 2D dataframe in view 2 to match order of view 1,0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Alternatively,0
Sort tracklets by length to prioritize greater continuity,0
Drop tracklets that are too short,0
"Where slots are available, copy the data over",0
"If about to overwrite data, keep tracklets with highest confidence",0
Squeeze some data into empty slots,0
"For the remaining data, overwrite where we are least confident",0
Find keypoints closest to the remaining data,0
Use Manhattan distance to avoid overflow,0
Only overwrite if improving confidence,0
Fill existing gaps,0
Retrieve original individual label indices,0
Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.,0
Fill existing gaps,0
Retrieve original individual label indices,0
ID swaps occur when X and Y simultaneously intersect each other.,0
Get only those bodypart pairs that belong to different individuals,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"See Li et al., 2012. Cross-view Activity Recognition using Hankelets.",0
"As proposed in the paper, the Hankel matrix can either be formed from",0
the tracklet's centroid or its normalized velocity.,0
"vel = np.diff(self.centroid, axis=0)",0
"vel /= np.linalg.norm(vel, axis=1, keepdims=True)",0
return self.hankelize(vel),0
TODO Add missing data imputation,1
"nrows, ncols = mat.shape",0
beta = nrows / ncols,0
omega = 0.56 * beta ** 3 - 0.95 * beta ** 2 + 1.82 * beta + 1.43,0
return np.argmin(s > omega * np.median(s)),0
"Note that if tracklets are very short, some may actually be part of the same track",0
and thus incorrectly reflect separate track endpoints...,0
"Map each Tracklet to an entry and output nodes and vice versa,",0
which is convenient once the tracklets are stitched.,0
Store tracklets and corresponding negatives (those that overlap in time),0
TODO Avoid looping over all pairs of tracklets,0
Pick the closest (spatially) overlapping tracklet,0
The algorithm works better with integer weights,0
Let us prune the graph by removing all source and sink edges,0
but those connecting the `n_tracks` first and last tracklets.,0
Preflow push seems to work slightly better than shortest,0
"augmentation path..., and is more computationally efficient.",0
Verify whether there are overlapping tracklets,0
"Pick the segment that minimizes ""smoothness"", computed here",0
with the coefficient of variation of the differences.,0
Rebuild a full graph from the remaining nodes without,0
temporal constraint on what tracklets can be stitched together.,0
Cycle through the residuals and incorporate back those,0
that only fit in a single tracklet.,0
Greedily add the remaining residuals,0
Refresh temporal bounds,0
Default to the distance cost function,0
should only exist one,0
Add in identity weighing before building the graph,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implementation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
average distance and average # significant differences avg. over comparisonbodyparts,0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
###################################################,0
Dependencies,0
###################################################,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as,0
specified in the config file,0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
Prepare figure,0
Mask rather than drop unwanted bodyparts to ensure consistent coloring,0
cropping!,0
nx=X2-X1,0
!/usr/bin/env python3,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"more videos are in principle covered, as OpenCV is used and allows many formats.",0
The above already raises a ValueError if formatting is wrong,0
"Rotate, see: https://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg",0
interesting option to just update metadata.,0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
Read the video until a frame is successfully read,0
import platform,0
"if platform.system() == ""Darwin"":  # for OSX use WXAgg",0
fig.canvas.start_event_loop(timeout=-1),0
else:,0
#fig.canvas.stop_event_loop(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"Windows throws error if file path is > 260 characters, can fix with prefix.",0
See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
uses openCV,0
cap.release() >> still used in frame_extraction!,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
This dictionary maps the model types to the file locations where the models exist.,0
Exit the function early if an unknown modeltype is provided.,0
Aliases for backwards-compatibility,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
Initialize costs matrix,0
get common bodypart detections in track pair,0
add 3rd dim to the points,0
cost for any point in time of t1 being the same,0
any point in time of t2,0
Get average cost of the entire track,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
check if it exists:,0
(average_degree / 2) as many edges as there are nodes is required,0
for undirected graphs to reach the target degree.,0
"Attention this order has to be consistent (for training set creation, training, inference etc.)",0
"-> adding (single,bpt) for uniquebodyparts",0
"-> adding (individual,bpt) for multianimalbodyparts",0
load defaults,0
set project specific parameters:,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
###################################################,0
Dependencies,0
###################################################,0
Pose X vs pose Y,0
Poses vs time,0
Likelihoods,0
Histograms,0
#################################################,0
Looping analysis over video,0
#################################################,0
Keep only the individuals and bodyparts that were labeled,0
Either display the animals defined in the config if they are found,0
"in the dataframe, or all the trajectories regardless of their names",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Determine whether the data are single- or multi-animal without loading into memory,0
simply by checking whether 'individuals' is in the second line of the CSV.,0
Make paths platform-agnostic if they are not already,0
Ensure folder names are strings,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
make list of full paths,0
filter list of videos,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"if ""MULTI!"" in allbpts:",0
Ensure same order as in config.yaml,0
ABBREVIATE NETWORK NAMES -- esp. for mobilenet!,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
Iterate over data files and stop as soon as one matching the scorer is found,0
If there was no match...,0
aliases for backwards-compatibility.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Set up a logger and add an stdout handler.,0
A single logger can have many handlers:,0
https://docs.python.org/3/howto/logging.html#handler-basic,0
TODO Dump to log file instead,1
"logger = logging.getLogger(""GUI"")",0
logger.setLevel(logging.DEBUG),0
handler = logging.StreamHandler(stream=sys.stdout),0
handler.setLevel(logging.DEBUG),0
formatter = logging.Formatter(,0
"""%(asctime)s - %(name)s - %(levelname)s - %(message)s"", ""%Y-%m-%d %H:%M:%S""",0
),0
handler.setFormatter(formatter),0
logger.addHandler(handler),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
restore the background region,0
Take into consideration imprecise OpenCV estimation of total number of frames,0
Automatically disable the draggable points,0
Save additional frames to the labeled-data directory,0
Store the newly-refined data,0
Uncertain keypoints are ignored,0
Merge with the already existing annotated data,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Thread-safe Stdout redirector,0
Creating action using the first constructor,0
Creating actions using the second constructor,0
File menu,0
View menu,0
Help menu,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
all_bodyparts: List,0
NOTE: Is there a case where a specific list should,0
have bodyparts other than the root? I don't think so.,0
Videotype selection,0
Select videos,0
Number of selected videos text,0
Clear video selection,0
"Qt returns a tuple (list of files, filetype)",0
Add tab header,0
Add separating line,0
Look for any extension by default,0
This works both with e.g. .avi and avi,0
Choose multiple files by default,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Automatically activate the napari-deeplabcut plugin,0
TODO Insert new video,1
TODO Insert skeleton link,1
"Hack to make the first column read-only, as we do not want users to touch it.",1
"The cleaner solution would be to use a QTreeView and QAbstractItemModel,",0
but that is a lot of rework for little benefits.,0
Leave untouched when it is already a string,0
"Slashes also raise the error, but no need to print anything since it is then likely to be a path",0
Walk backwards across parents to get all keys,0
Read the video until a frame is successfully read,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
TODO: Multi video select.... have to change to single video!,1
Shuffle,0
Num animals,0
Min swap length,0
Max gap to fill,0
Trail length,0
Filter type,0
Filter window length,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
Tracker Type,0
Num animals,0
Num triplets,0
layout.addWidget(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
Overwrite videos,0
Trail Points,0
Plot all bodyparts,0
Skeleton,0
Filtered data,0
Plot trajectories,0
High quality video,0
Bodypart list,0
Multianimal scenario.,0
Color is based on individual or bodypart.,0
Single animal scenario.,0
Color is based on bodypart.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Extraction method,0
Frame extraction algorithm,0
Frame cropping,0
Cluster step,0
GUI Slider width,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
use the default pose_cfg file for default values,0
Shuffle,0
Display iterations,0
Save iterations,0
Max iterations,0
Max number snapshots to keep,0
layout.addWidget(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Single / Multi animal Only Layouts,0
Dynamic bodypart cropping,0
Save results as csv,0
Filter predictions,0
Plot Trajectories,0
Show trajectory plots,0
Shuffle,0
layout.addLayout(tmp_layout),0
tmp_layout = QtWidgets.QGridLayout(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
Augmentation method,0
Neural Network,0
TODO: finish model_comparison,1
Check that training data files were indeed created.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Add config text field and button,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
layout.addWidget(),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Display all images,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Shuffle,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"calling create_tracking_dataset, train_tracking_transformer, stitch_tracklets",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
only try to find these features if they are in the dictionary,0
1) reference to video folder and get the proper bpt_feature file for feature table,0
2) get either the path to gt or the path to track pickle,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
tracklets,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"with npy list form videos, split each to train and test",0
"video_name = '.'.join(video.split(""/"")[-1].split(""."")[:-1])",0
assuming there is only one match,0
make my own model factory,0
make my own loss factory,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights",0
"NOTE: drop path for stochastic depth, we shall see if this is better than dropout here",0
only channel pooling,0
"x -> (B, 12, 2048)",0
"x -> (B, 12, 768)",0
trying to keep everything simple,0
need to write my own keypoint embedding,0
we don't need a fc layer here,0
x: inputs,0
"(B, 12, 768)",0
added pos embed,0
remove all parts I don't understand,0
forwarding through blocks. But num of blocks should be reduced,0
i can't really tell what the difference is,0
will have to change some of these,0
For old models that I trained prior to conv based patchification,0
To resize pos embedding when using model at different size from pretrained weights,0
Rescale the grid of position embeddings when loading from state_dict. Adapted from,0
https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224,0
Cut & paste from PyTorch official master until it's in a few official releases - RW,0
Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf,0
Computes standard normal cumulative distribution function,0
Values are generated by using a truncated uniform distribution and,0
then using the inverse CDF for the normal distribution.,0
Get upper and lower cdf values,0
"Uniformly fill tensor with values from [l, u], then translate to",0
"[2l-1, 2u-1].",0
Use inverse cdf transform for normal distribution to get truncated,0
standard normal,0
"Transform to proper mean, std",0
Clamp to ensure it's in the proper range,0
"type: (Tensor, float, float, float, float) -> Tensor",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2019 Ross Wightman,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Hacked together by / Copyright 2020 Ross Wightman,1
https://github.com/rwightman/pytorch-image-models/blob/main/timm/scheduler/scheduler_factory.py,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"resample if noise out of percent limit, brute force but shouldn't spin much",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
distmat g,0
q    1 3 2 4,0
4 1 2 3,0
0 2 1 3,0
1 2 3 0,0
compute cmc curve for each query,0
get query pid and camid,0
remove gallery samples that have the same pid and camid with query,0
compute cmc curve,0
"binary vector, positions with value 1 are correct matches",0
this condition is true when query identity does not appear in gallery,0
compute average precision,0
reference: https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Average_precision,0
"tmp_cmc = [x / (i + 1.) for i, x in enumerate(tmp_cmc)]",0
query,0
gallery,0
"distmat = re_ranking(qf, gf, k1=20, k2=6, lambda_value=0.3)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"if feature vector is numpy, you should use 'torch.tensor' transform it to tensor",0
print('using GPU to compute original distance'),0
print('starting re_ranking'),0
k-reciprocal neighbors,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
More elegantly one can simply define:,0
take care of difference between feature map space and original image space,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
cos = torch.cdist,0
TODO: maybe find a better spot for this.,1
"dev =  device if torch.cuda.is_available() else ""cpu""",0
train,0
print (f'validation loss {val_loss/len(val_loader)}'),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
normalize vectors here,0
i th vector at j th kpt,0
maybe needs to convert them to embeddings and position token,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Drop missing body parts,0
Drop points lying outside the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
## dlcnet_ms5: backbone resnet50 + multi-fusion & multi-stage module,0
## dlcr101_ms5/dlcr152_ms5: backbone resnet101/152 + multi-fusion & multi-stage module,0
"If the graph is unnecessarily large (with 15+ keypoints by default),",0
we randomly prune it to a size guaranteeing an average node degree of 6;,0
"see Suppl. Fig S9c in Lauer et al., 2022.",0
Use the skeleton defined in the config file,0
Ignore possible connections between 'multi' and 'unique' body parts;,0
one can never be too careful...,0
Disable the prediction of PAFs if the graph is empty,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving metadata and data file (Pickle file),0
###############################################################################,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Setting inference cfg file:,0
Search a pose_config.yaml file to parse missing information,0
Clean the training-datasets folder prior to recreating the data pickles,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Load updated lists:,0
Find the corresponding video file,0
"print(key, ""dropping?"")",0
adding important values for multianiaml project:,0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Determine the index length required to guarantee,0
the train–test ratio is exactly the desired one.,0
Pad indices so lengths agree,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(name,test_video_name)",0
Blazing fast and does not load the image into memory,0
Check that points lie within the image,0
DeprecationWarnings are silenced since Python 3.2 unless triggered in __main__,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
updating variable if null/None! #backwardscompatability,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
these values are dropped as scalecrop,0
doesn't have rotation implemented,0
Also drop maDLC smart cropping augmentation parameters,0
sort the model directories,0
get the shuffle index and offset by 1.,0
read cfg file,0
create log file,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Check for variable correctness,0
Moviepy:,0
select crop method,0
"Find the matching video from the config `video_sets`,",0
as it may be stored elsewhere than in the `videos` directory.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
allow_growth must be true here because tensorflow does not automatically free gpu memory and setting it as false occupies all gpu memory so that pytorch cannot kick in,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
should close tensorflow session here in order to free gpu,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"Flip x, y, confidence and reshape",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html#videocapture-get,0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
TODO: add cropping as in video analysis!,1
if cropping is not None:,0
cfg['cropping']=True,0
"cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping",0
"print(""Overwriting cropping parameters:"", cropping)",0
"print(""These are used for all videos, but won't be save to the cfg file."")",0
"Also ensure `boundingboxslack` is greater than zero, otherwise overlap",0
"between trackers cannot be evaluated, resulting in empty tracklets.",0
Check which snapshots are available and sort them by # iterations,0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
NOTE: If dataname line above is changed then line below is obsolete?,0
"trackname = trackname.replace(videofolder, destfolder)",0
TODO: adjust this for multi + unique bodyparts!,1
this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. ),0
Optimal identity assignment based on soft voting,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
imagenet mean for resnet pretraining:,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/DeepLabCut/DeepLabCut/pull/324",0
for DLC 2.2. (here all set False to not use PAFs/pairwise fields),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
"reloading defaults, as they can bleed over from a previous run otherwise",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
dlc_cfg_train = load_config(str(path_train_config)),0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
Name for deeplabcut net (based on its parameters),0
"DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])",0
"print(""Extracting maps for "", DLCscorer, "" with # of trainingiterations:"", trainingsiterations)",0
if notanalyzed: #this only applies to ask if h5 exists...,0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"for multi animal, seems only this is used",0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Get animal IDs and corresponding indices in the arrays of detections,0
Form ground truth beforehand,0
Assemble animals on the full set of detections,0
Count the number of unassembled bodyparts,0
Handle unlabeled bodyparts...,0
"Only 1 animal, let us return the full graph indices only",0
Find minimal skeleton,0
Select optimal PAF graph,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Selective copy; deepcopy is >5x slower,0
Only keeps skeletons that are more than 90% complete,0
TODO Normalize dists by longest length?,1
TODO Smarter imputation technique (Bayesian? Grassmann averages?),1
Covariance matrix estimation fails due to numerical singularities,0
Distance is undefined if the assembly is empty,0
Correct distance to account for missing observations,0
"Alternatively, reduce contribution of missing values to the Mahalanobis",0
distance to zero by substituting the corresponding means.,0
"Fill the subsets with unambiguous, complete individuals",0
Fuse superfluous assemblies,0
Second pass without edge safety,0
Mark the unique body parts as assembled anyway so,0
they are not used later on to fill assemblies.,0
Store selected edges for subsequent frames,0
Remove invalid assemblies,0
Fill assemblies with unconnected body parts,0
Spawning (rather than forking) multiple processes does not,0
work nicely with the GUI or interactive sessions.,0
"In that case, we fall back to the serial assembly.",0
Swap corresponding keypoints,0
Only consider assemblies of at least two keypoints,0
Greedy assembly matching like in pycocotools,0
Global rather than greedy assembly matching,0
Cast columns of dtype 'object' to float to avoid TypeError,0
further down in _parse_ground_truth_data.,0
sigma is taken as the median of all COCO keypoint standard deviations,0
Sort matching score (OKS) in descending order of assembly affinity,0
Guarantee precision decreases monotonically,0
See https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Regularize by forcing AR <= 5,0
max_ar = 5,0
if el.aspect_ratio >= max_ar:,0
if el.height > el.width:,0
el.width = el.height / max_ar,0
else:,0
el.height = el.width / max_ar,0
Orient the ellipse such that it encompasses most points,0
"n_inside = el.contains_points(np.c_[self.x, self.y]).sum()",0
el.theta += 0.5 * np.pi,0
"if el.contains_points(np.c_[self.x, self.y]).sum() < n_inside:",0
el.theta -= 0.5 * np.pi,0
"r2 = chi2.ppf(2 * norm.cdf(sd) - 1, 2)",0
"height, width = np.sqrt(E * r2)",0
The general quadratic curve has the form:,0
ax^2 + 2bxy + cy^2 + 2dx + 2fy + g = 0,0
Ellipse center coordinates,0
Semi-axes lengths,0
Angle to the horizontal,0
High uncertainty to the unobservable initial velocities,0
Give high uncertainty to the unobservable initial velocities,0
diff = val - cost_matrix,0
"diff[row, col] += val",0
if (,0
val < self.iou_threshold,0
or np.any(diff[row] <= 0.2),0
"or np.any(diff[:, col] <= 0.2)",0
):,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
Modified from scipy source code:,0
- to restrict its use to 2D,0
- to get rid of shuffling (since arrays are only (nbodyparts * 3) element long),0
TODO - factor in keypoint confidence (and weight by # of observations??),1
"mat = self.calc_pairwise_oks(poses, poses_ref)",0
Remove matched detections with low OKS,0
matches = [],0
"for row, col in zip(row_indices, col_indices):",0
"if mat[row, col] < self.oks_threshold:",0
unmatched_poses.append(row),0
unmatched_trackers.append(col),0
else:,0
"matches.append([row, col])",0
if not len(matches):,0
"matches = np.empty((0, 2), dtype=int)",0
else:,0
matches = np.stack(matches),0
update matched trackers with assigned detections,0
create and initialise new trackers for unmatched detections,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
filter out matched with low IOU,0
TODO: get cropping parameters and utilize!,1
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Load meta data & annotations,0
get the graph!,0
"NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*",0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
TODO: Make this code not so redundant!,1
"If a string was passed in, auto-convert to True for backward compatibility",0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to initial folder,0
"# Also storing one ""large"" table with results:",0
note: evaluationfolder.parents[0] to get common folder above all shuffle evaluations.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
#################################################,0
Load data...,0
#################################################,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
TODO: IMPLEMENT for different batch sizes?,1
Ignore best edges possibly defined during a prior evaluation,0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
Pass the image and the keypoints through the resizer;,0
this has no effect if no augmenters were added to it.,0
FIXME Is having an empty array vs nan really that necessary?!,1
"Form 2D array of shape (n_rows, 4) where the last dimension",0
"is (sample_index, peak_y, peak_x, bpt_index) to slice the PAFs.",0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in,0
Compute all distance statistics,0
Calculate overall prediction error,0
Skip data-driven skeleton selection unless,0
the model was trained on the full graph.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"loading backbone from ResNet, MobileNet etc.",0
"display_iters = max(1,int(displayiters))",0
"[_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})",0
Save snapshot,0
return to original path.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Clip peak locations to PAFs dimensions,0
"unit_vecs = vecs / lengths[:, np.newaxis]",0
"affinities = np.squeeze(y @ np.expand_dims(unit_vecs, axis=2)).sum(axis=1)",0
Form cost matrices,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"loading backbone from ResNet, MobileNet etc.",0
"Auto-switch to Adam on M1/M2 chips, as the momentum optimizer crashes",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Convert a frozen graph to OpenVINO IR format,0
Read network into memory,0
"For better efficiency, model is initialized for batch_size 1 and every sample processed independently",0
Load network to device,0
Prepare input data,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Change the stride from 2 to 1 to get 16x downscaling instead of 32x.,0
https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142,0
Multi stage is currently only implemented for resnets,0
Defining multi_fusion backbone,0
Attaching multi-stage decoder,0
stage_paf_output = stage_paf_output + pre_stage_paf_output,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
FIXME Fix wrong scope with Keras layers,1
"def prediction_layer(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2DTranspose(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=2,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
## New DLCNet Addition: multi-stage decoder,0
"def prediction_layer_stage(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2D(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=1,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Compute variance using: Var[X]= E[X^2] - E[X]^2.,0
Compute keep_prob,0
TODO(tanmingxing): add support for training progress.,1
Compute drop_connect tensor,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
turn into x times y time bs * bpts,0
extract corresponding locref x and y as well as probability,0
Filter predicted heatmaps with a 2D Gaussian kernel as in:,0
https://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_The_Devil_Is_in_the_Details_Delving_Into_Unbiased_Data_CVPR_2020_paper.pdf,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
defaults will be a public argument for namedtuple in Python 3.7,0
https://docs.python.org/3/library/collections.html#collections.namedtuple,0
Make sure that round down does not go down by more than 10%.,0
Builds the block accordings to arguments.,0
Expansion phase:,0
Depth-wise convolution phase:,0
Squeeze and Excitation layer.,0
Output phase:,0
only apply drop_connect if skip presents.,0
Expansion phase:,0
Output phase:,0
only apply drop_connect if skip presents.,0
Builds blocks.,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Stem part.,0
Head part.,0
Calls Stem layers,0
Calls blocks.,0
Calls final layers and returns logits.,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
"(width_coefficient, depth_coefficient, resolution, dropout_rate)",0
blocks_args = [,0
"'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',",0
"'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',",0
"'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',",0
"'r1_k3_s11_e6_i192_o320_se0.25',",0
],0
The default is TPU-specific batch norm.,0
The alternative is tf.layers.BatchNormalization.,0
"batch_norm=utils.TpuBatchNormalization,  # TPU-specific requirement.",0
ValueError will be raised here if override_params has fields not included,0
in global_params.,0
,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrunk when given a depth-multiplier < 1.0.,0
,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Adapted from DeeperCut by Eldar Insafutdinov,0
https://github.com/eldar/pose-tensorflow,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Clamp to 40% of crop size to ensure that at least,0
the center keypoint remains visible after the offset is applied.,0
Points located close to one another are sampled preferentially,0
in order to augment crowded regions.,0
Include keypoints in the count to avoid null probabilities,0
Shift the crop center in both dimensions by random amounts,0
and normalize to the original image dimensions.,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Pickle the 'data' dictionary using the highest protocol available.,0
item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal,0
"Add smart, keypoint-aware image cropping",0
Scale is sampled only once (per batch) to transform all of the images into same size.,0
Discard keypoints whose coordinates lie outside the cropped image,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
Produce score maps and location refinement fields,0
Find indices of individuals in joint_id,0
WIP!,0
STD of gaussian is 1/4 of threshold,0
Grid of coordinates,0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Distance between the joint point and each coordinate,0
NEEDS TO BE DONE!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
TODO Finish implementing actual abstract class,1
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
parameter initialization for augmentation pipeline:,0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images of a batch into same size.,0
Approximating the scale,0
invisible joints are represented by nans,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"First, initializing variables (if they don't exist)",0
what is the fraction of training samples with scaling augmentation?,0
loading defaults for rotation range!,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"cfg[""rotateratio""] = cfg.get(",0
"""rotratio"", 0.4",0
)  # what is the fraction of training samples with rotation augmentation?,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"Auto cropping is new (was not in Nature Neuroscience 2018 paper, but introduced in Nath et al. Nat. Protocols 2019)",0
"and boosts performance by 2X, particularly on challenging datasets, like the cheetah in Nath et al.",0
Parameters for augmentation with regard to cropping:,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Get rid of the interpolation beyond the spline knots,0
Retrieve original individual label indices,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Check the training image paths are correctly stored as arrays of strings,0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
TENSORPACK could fail in WINDOWS...,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Loading example data set,0
# Create one identical splits for 3 networks and 3 augmentations,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
Setting specific parameters for training,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"MODELS = [""dlcrnet_ms5"", ""dlcr101_ms5"", ""efficientnet-b0"", ""mobilenet_v2_0.35""]",0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
## adding it here,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Downloading the calibration images,0
Deleting unnecessary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Loading example data set,0
example how to set pose config variables:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
test if existing models are found:,0
Test if nonexisting models are not found,0
Add some other office files:,0
Add a .pickle and .h5 files,0
"By default, all videos with common extensions are taken from a directory",0
A list of extensions can also be passed in,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Test bounding box from a single keypoint,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Test that shape was preserved after vectorization,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Guarantee that images smaller than crop size are handled fine,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Disable stochastic scale jitter,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Verify the aspect ratio is preserved,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Load sample multi animal data,0
Generate a random permutation and reorder data. Ignore the unique bodypart,0
Get inverse permutation and reorder the modified data to get back,0
to the original,0
Check,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
"Check data comprise path, shape, and xy coordinates",0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Break the graph to test stitching failure,0
Add fake IDs,0
Split all tracklets in half,0
,0
DeepLabCut Toolbox (deeplabcut.org),0
© A. & M.W. Mathis Labs,0
https://github.com/DeepLabCut/DeepLabCut,0
,0
Please see AUTHORS for contributors.,0
https://github.com/DeepLabCut/DeepLabCut/blob/master/AUTHORS,0
,0
Licensed under GNU Lesser General Public License v3.0,0
,0
Test flipped keypoints,0
Original (cached) coordinates must have remained empty,0
Assemble based on the smallest graph to speed up testing,0
Generate fake identity predictions,0
Assemble based on the smallest graph to speed up testing,0
Test now with identity only and ensure assemblies,0
contain only parts of a single group ID.,0
Test empty assembly,0
Verify that the Hugging Face folder was removed,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
def install(package):,0
"subprocess.check_call([sys.executable, ""-m"", ""pip"", ""install"", package])",0
"install(""tensorflow==1.13.1"")",0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
quicker variant,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
Suppress tensorflow warning messages,0
"Train, evaluate & predict functions / all require TF",0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
AM: Rowland server down...,0
"auxfun_models.download_model(model, train_dir)",0
Updating config file:,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path, num_shuffles=auxfun_models.Check4weights(pose_cfg['net_type'], parent_path, num_shuffles= 1)",0
Updating training and test pose_cfg:,0
Read the config file,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
common parameters:,0
Write dictionary to yaml  config file,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
DeepLabCut2.0 Toolbox (deeplabcut.org),0
© A. & M. Mathis Labs,0
https://github.com/AlexEMG/DeepLabCut,0
Please see AUTHORS for contributors.,0
,0
https://github.com/AlexEMG/DeepLabCut/blob/master/AUTHORS,0
Licensed under GNU Lesser General Public License v3.0,0
"TODO(stes) mocking a few modules to rely in fewer dependencies, without",1
causing import errors when using deeplabcut.,0
Map image paths from predicted data to GT as the first are typically,0
absolute whereas the latter are relative to the project path.,0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in.,0
TODO(jeylau) add affinity.setter to Assembly,1
TODO(stes): remove/rewrite,1
DeepLabCut2.0 Toolbox (deeplabcut.org),0
© A. & M. Mathis Labs,0
https://github.com/AlexEMG/DeepLabCut,0
Please see AUTHORS for contributors.,0
,0
https://github.com/AlexEMG/DeepLabCut/blob/master/AUTHORS,0
Licensed under GNU Lesser General Public License v3.0,0
DeepLabCut2.0 Toolbox (deeplabcut.org),0
© A. & M. Mathis Labs,0
https://github.com/AlexEMG/DeepLabCut,0
Please see AUTHORS for contributors.,0
,0
https://github.com/AlexEMG/DeepLabCut/blob/master/AUTHORS,0
Licensed under GNU Lesser General Public License v3.0,0
DeepLabCut2.0 Toolbox (deeplabcut.org),0
© A. & M. Mathis Labs,0
https://github.com/AlexEMG/DeepLabCut,0
Please see AUTHORS for contributors.,0
,0
https://github.com/AlexEMG/DeepLabCut/blob/master/AUTHORS,0
Licensed under GNU Lesser General Public License v3.0,0
DeepLabCut2.0 Toolbox (deeplabcut.org),0
© A. & M. Mathis Labs,0
https://github.com/AlexEMG/DeepLabCut,0
Please see AUTHORS for contributors.,0
,0
https://github.com/AlexEMG/DeepLabCut/blob/master/AUTHORS,0
Licensed under GNU Lesser General Public License v3.0,0
DeepLabCut2.0 Toolbox (deeplabcut.org),0
© A. & M. Mathis Labs,0
https://github.com/AlexEMG/DeepLabCut,0
Please see AUTHORS for contributors.,0
,0
https://github.com/AlexEMG/DeepLabCut/blob/master/AUTHORS,0
Licensed under GNU Lesser General Public License v3.0,0
"ignore the exception and continue with the next evaluation, without",0
yielding a result value.,0
"return the result value, with NaN as the result for all metrics that",0
could not be computed due to the error.,0
raise the error and stop evaluation,0
DeepLabCut2.0 Toolbox (deeplabcut.org),0
© A. & M. Mathis Labs,0
https://github.com/AlexEMG/DeepLabCut,0
Please see AUTHORS for contributors.,0
,0
https://github.com/AlexEMG/DeepLabCut/blob/master/AUTHORS,0
Licensed under GNU Lesser General Public License v3.0,0
Read the config file and related variables,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Flatten the list of bodyparts to connect,0
Format data,0
Trick to force equal aspect ratio of 3D plots,0
Set up the matplotlib figure beforehand,0
Set up skeleton LineCollections,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
Get track_method and do related checks,0
Get track method suffix,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
"## Assign nan to [X,Y] of low likelihood predictions ###",0
Convert the data to a np array to easily mask out the low likelihood predictions,0
"Assign [X,Y] = nan to low likelihood predictions",0
Reshape data back to original shape,0
put data back to the dataframes,0
Check individuals are the same in both views,0
Cross-view match individuals,0
Create a dummy variables for single-animal,0
Cleaner variable (since inds view1 == inds view2),0
"Reshape: (num_framex, num_individuals, num_bodyparts , 2)",0
Triangulate data,0
i is individual in view 1,0
voting[i] is the matched individual in view 2,0
Create 3D DataFrame column and row indices,0
Swap num_animals with num_frames axes to ensure well-behaving reshape,0
Fill up 3D dataframe,0
Reorder 2D dataframe in view 2 to match order of view 1,0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Alternatively,0
Sort tracklets by length to prioritize greater continuity,0
Drop tracklets that are too short,0
"Where slots are available, copy the data over",0
"If about to overwrite data, keep tracklets with highest confidence",0
Squeeze some data into empty slots,0
"For the remaining data, overwrite where we are least confident",0
Find keypoints closest to the remaining data,0
Use Manhattan distance to avoid overflow,0
Only overwrite if improving confidence,0
Fill existing gaps,0
Retrieve original individual label indices,0
Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.,0
Fill existing gaps,0
Retrieve original individual label indices,0
ID swaps occur when X and Y simultaneously intersect each other.,0
Get only those bodypart pairs that belong to different individuals,0
"See Li et al., 2012. Cross-view Activity Recognition using Hankelets.",0
"As proposed in the paper, the Hankel matrix can either be formed from",0
the tracklet's centroid or its normalized velocity.,0
"vel = np.diff(self.centroid, axis=0)",0
"vel /= np.linalg.norm(vel, axis=1, keepdims=True)",0
return self.hankelize(vel),0
TODO Add missing data imputation,1
"nrows, ncols = mat.shape",0
beta = nrows / ncols,0
omega = 0.56 * beta ** 3 - 0.95 * beta ** 2 + 1.82 * beta + 1.43,0
return np.argmin(s > omega * np.median(s)),0
"Note that if tracklets are very short, some may actually be part of the same track",0
and thus incorrectly reflect separate track endpoints...,0
"Map each Tracklet to an entry and output nodes and vice versa,",0
which is convenient once the tracklets are stitched.,0
Store tracklets and corresponding negatives (those that overlap in time),0
TODO Avoid looping over all pairs of tracklets,0
Pick the closest (spatially) overlapping tracklet,0
The algorithm works better with integer weights,0
Let us prune the graph by removing all source and sink edges,0
but those connecting the `n_tracks` first and last tracklets.,0
Preflow push seems to work slightly better than shortest,0
"augmentation path..., and is more computationally efficient.",0
Verify whether there are overlapping tracklets,0
"Pick the segment that minimizes ""smoothness"", computed here",0
with the coefficient of variation of the differences.,0
Rebuild a full graph from the remaining nodes without,0
temporal constraint on what tracklets can be stitched together.,0
Cycle through the residuals and incorporate back those,0
that only fit in a single tracklet.,0
Greedily add the remaining residuals,0
Refresh temporal bounds,0
Default to the distance cost function,0
should only exist one,0
Add in identity weighing before building the graph,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implementation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
average distance and average # significant differences avg. over comparisonbodyparts,0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
###################################################,0
Dependencies,0
###################################################,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as,0
specified in the config file,0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
Prepare figure,0
Mask rather than drop unwanted bodyparts to ensure consistent coloring,0
cropping!,0
nx=X2-X1,0
!/usr/bin/env python3,0
"more videos are in principle covered, as OpenCV is used and allows many formats.",0
The above already raises a ValueError if formatting is wrong,0
"Rotate, see: https://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg",0
interesting option to just update metadata.,0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
Read the video until a frame is successfully read,0
import platform,0
"if platform.system() == ""Darwin"":  # for OSX use WXAgg",0
fig.canvas.start_event_loop(timeout=-1),0
else:,0
#fig.canvas.stop_event_loop(),0
"Windows throws error if file path is > 260 characters, can fix with prefix.",0
See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation,0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
cap.release() >> still used in frame_extraction!,0
This dictionary maps the model types to the file locations where the models exist.,0
Exit the function early if an unknown modeltype is provided.,0
Loading urls of models,0
"creates a new subfolder as indicated below, unzipping from there and deleting this folder",0
Hack to get hf path ...,1
print('removing hf dir'),0
Aliases for backwards-compatibility,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
Initialize costs matrix,0
get common bodypart detections in track pair,0
add 3rd dim to the points,0
cost for any point in time of t1 being the same,0
any point in time of t2,0
Get average cost of the entire track,0
check if it exists:,0
(average_degree / 2) as many edges as there are nodes is required,0
for undirected graphs to reach the target degree.,0
"Attention this order has to be consistent (for training set creation, training, inference etc.)",0
"-> adding (single,bpt) for uniquebodyparts",0
"-> adding (individual,bpt) for multianimalbodyparts",0
load defaults,0
set project specific parameters:,0
###################################################,0
Dependencies,0
###################################################,0
Pose X vs pose Y,0
Poses vs time,0
Likelihoods,0
Histograms,0
#################################################,0
Looping analysis over video,0
#################################################,0
Keep only the individuals and bodyparts that were labeled,0
Either display the animals defined in the config if they are found,0
"in the dataframe, or all the trajectories regardless of their names",0
Determine whether the data are single- or multi-animal without loading into memory,0
simply by checking whether 'individuals' is in the second line of the CSV.,0
Make paths platform-agnostic if they are not already,0
Ensure folder names are strings,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
make list of full paths,0
filter list of videos,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"if ""MULTI!"" in allbpts:",0
Ensure same order as in config.yaml,0
ABBREVIATE NETWORK NAMES -- esp. for mobilenet!,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
Iterate over data files and stop as soon as one matching the scorer is found,0
If there was no match...,0
aliases for backwards-compatibility.,0
Set up a logger and add an stdout handler.,0
A single logger can have many handlers:,0
https://docs.python.org/3/howto/logging.html#handler-basic,0
TODO Dump to log file instead,1
"logger = logging.getLogger(""GUI"")",0
logger.setLevel(logging.DEBUG),0
handler = logging.StreamHandler(stream=sys.stdout),0
handler.setLevel(logging.DEBUG),0
formatter = logging.Formatter(,0
"""%(asctime)s - %(name)s - %(levelname)s - %(message)s"", ""%Y-%m-%d %H:%M:%S""",0
),0
handler.setFormatter(formatter),0
logger.addHandler(handler),0
restore the background region,0
Take into consideration imprecise OpenCV estimation of total number of frames,0
Automatically disable the draggable points,0
Save additional frames to the labeled-data directory,0
Store the newly-refined data,0
Uncertain keypoints are ignored,0
Merge with the already existing annotated data,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
Thread-safe Stdout redirector,0
Creating action using the first constructor,0
Creating actions using the second constructor,0
File menu,0
View menu,0
Help menu,0
all_bodyparts: List,0
NOTE: Is there a case where a specific list should,0
have bodyparts other than the root? I don't think so.,0
Videotype selection,0
Select videos,0
Number of selected videos text,0
Clear video selection,0
"Qt returns a tuple (list of files, filetype)",0
Add tab header,0
Add separating line,0
Look for any extension by default,0
This works both with e.g. .avi and avi,0
Choose multiple files by default,0
Automatically activate the napari-deeplabcut plugin,0
TODO Insert new video,1
TODO Insert skeleton link,1
"Hack to make the first column read-only, as we do not want users to touch it.",1
"The cleaner solution would be to use a QTreeView and QAbstractItemModel,",0
but that is a lot of rework for little benefits.,0
Leave untouched when it is already a string,0
"Slashes also raise the error, but no need to print anything since it is then likely to be a path",0
Walk backwards across parents to get all keys,0
Read the video until a frame is successfully read,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
TODO: Multi video select.... have to change to single video!,1
Shuffle,0
Num animals,0
Min swap length,0
Max gap to fill,0
Trail length,0
Filter type,0
Filter window length,0
Shuffle,0
Tracker Type,0
Num animals,0
Num triplets,0
layout.addWidget(),0
Shuffle,0
Overwrite videos,0
Trail Points,0
Plot all bodyparts,0
Skeleton,0
Filtered data,0
Plot trajectories,0
High quality video,0
Bodypart list,0
Extraction method,0
Frame extraction algorithm,0
Frame cropping,0
Cluster step,0
GUI Slider width,0
use the default pose_cfg file for default values,0
Shuffle,0
Display iterations,0
Save iterations,0
Max iterations,0
Max number snapshots to keep,0
layout.addWidget(),0
Single / Multi animal Only Layouts,0
Dynamic bodypart cropping,0
Save results as csv,0
Filter predictions,0
Plot Trajectories,0
Show trajectory plots,0
Shuffle,0
layout.addLayout(tmp_layout),0
tmp_layout = QtWidgets.QGridLayout(),0
Shuffle,0
Augmentation method,0
Neural Network,0
TODO: finish model_comparison,1
Check that training data files were indeed created.,0
Add config text field and button,0
layout.addWidget(),0
Display all images,0
Shuffle,0
"calling create_tracking_dataset, train_tracking_transformer, stitch_tracklets",0
only try to find these features if they are in the dictionary,0
1) reference to video folder and get the proper bpt_feature file for feature table,0
2) get either the path to gt or the path to track pickle,0
tracklets,0
"with npy list form videos, split each to train and test",0
"video_name = '.'.join(video.split(""/"")[-1].split(""."")[:-1])",0
assuming there is only one match,0
make my own model factory,0
make my own loss factory,0
"NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights",0
"NOTE: drop path for stochastic depth, we shall see if this is better than dropout here",0
only channel pooling,0
"x -> (B, 12, 2048)",0
"x -> (B, 12, 768)",0
trying to keep everything simple,0
need to write my own keypoint embedding,0
we don't need a fc layer here,0
x: inputs,0
"(B, 12, 768)",0
added pos embed,0
remove all parts I don't understand,0
forwarding through blocks. But num of blocks should be reduced,0
i can't really tell what the difference is,0
will have to change some of these,0
For old models that I trained prior to conv based patchification,0
To resize pos embedding when using model at different size from pretrained weights,0
Rescale the grid of position embeddings when loading from state_dict. Adapted from,0
https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224,0
Cut & paste from PyTorch official master until it's in a few official releases - RW,0
Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf,0
Computes standard normal cumulative distribution function,0
Values are generated by using a truncated uniform distribution and,0
then using the inverse CDF for the normal distribution.,0
Get upper and lower cdf values,0
"Uniformly fill tensor with values from [l, u], then translate to",0
"[2l-1, 2u-1].",0
Use inverse cdf transform for normal distribution to get truncated,0
standard normal,0
"Transform to proper mean, std",0
Clamp to ensure it's in the proper range,0
"type: (Tensor, float, float, float, float) -> Tensor",0
"resample if noise out of percent limit, brute force but shouldn't spin much",0
distmat g,0
q    1 3 2 4,0
4 1 2 3,0
0 2 1 3,0
1 2 3 0,0
compute cmc curve for each query,0
get query pid and camid,0
remove gallery samples that have the same pid and camid with query,0
compute cmc curve,0
"binary vector, positions with value 1 are correct matches",0
this condition is true when query identity does not appear in gallery,0
compute average precision,0
reference: https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Average_precision,0
"tmp_cmc = [x / (i + 1.) for i, x in enumerate(tmp_cmc)]",0
query,0
gallery,0
"distmat = re_ranking(qf, gf, k1=20, k2=6, lambda_value=0.3)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
"if feature vector is numpy, you should use 'torch.tensor' transform it to tensor",0
print('using GPU to compute original distance'),0
print('starting re_ranking'),0
k-reciprocal neighbors,0
More elegantly one can simply define:,0
take care of difference between feature map space and original image space,0
cos = torch.cdist,0
TODO: maybe find a better spot for this.,1
"dev =  device if torch.cuda.is_available() else ""cpu""",0
train,0
print (f'validation loss {val_loss/len(val_loader)}'),0
normalize vectors here,0
i th vector at j th kpt,0
maybe needs to convert them to embeddings and position token,0
Drop missing body parts,0
Drop points lying outside the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
## dlcnet_ms5: backbone resnet50 + multi-fusion & multi-stage module,0
## dlcr101_ms5/dlcr152_ms5: backbone resnet101/152 + multi-fusion & multi-stage module,0
"If the graph is unnecessarily large (with 15+ keypoints by default),",0
we randomly prune it to a size guaranteeing an average node degree of 6;,0
"see Suppl. Fig S9c in Lauer et al., 2022.",0
Use the skeleton defined in the config file,0
Ignore possible connections between 'multi' and 'unique' body parts;,0
one can never be too careful...,0
Disable the prediction of PAFs if the graph is empty,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving metadata and data file (Pickle file),0
###############################################################################,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Setting inference cfg file:,0
Search a pose_config.yaml file to parse missing information,0
Clean the training-datasets folder prior to recreating the data pickles,0
Load updated lists:,0
Find the corresponding video file,0
"print(key, ""dropping?"")",0
adding important values for multianiaml project:,0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Determine the index length required to guarantee,0
the train–test ratio is exactly the desired one.,0
Pad indices so lengths agree,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(name,test_video_name)",0
Blazing fast and does not load the image into memory,0
Check that points lie within the image,0
DeprecationWarnings are silenced since Python 3.2 unless triggered in __main__,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
updating variable if null/None! #backwardscompatability,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
these values are dropped as scalecrop,0
doesn't have rotation implemented,0
Also drop maDLC smart cropping augmentation parameters,0
sort the model directories,0
get the shuffle index and offset by 1.,0
read cfg file,0
create log file,0
Check for variable correctness,0
Moviepy:,0
select crop method,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
allow_growth must be true here because tensorflow does not automatically free gpu memory and setting it as false occupies all gpu memory so that pytorch cannot kick in,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
should close tensorflow session here in order to free gpu,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"Flip x, y, confidence and reshape",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html#videocapture-get,0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
TODO: add cropping as in video analysis!,1
if cropping is not None:,0
cfg['cropping']=True,0
"cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping",0
"print(""Overwriting cropping parameters:"", cropping)",0
"print(""These are used for all videos, but won't be save to the cfg file."")",0
"Also ensure `boundingboxslack` is greater than zero, otherwise overlap",0
"between trackers cannot be evaluated, resulting in empty tracklets.",0
Check which snapshots are available and sort them by # iterations,0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
NOTE: If dataname line above is changed then line below is obsolete?,0
"trackname = trackname.replace(videofolder, destfolder)",0
TODO: adjust this for multi + unique bodyparts!,1
this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. ),0
Optimal identity assignment based on soft voting,0
imagenet mean for resnet pretraining:,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/DeepLabCut/DeepLabCut/pull/324",0
for DLC 2.2. (here all set False to not use PAFs/pairwise fields),0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
"reloading defaults, as they can bleed over from a previous run otherwise",0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
dlc_cfg_train = load_config(str(path_train_config)),0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
Name for deeplabcut net (based on its parameters),0
"DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])",0
"print(""Extracting maps for "", DLCscorer, "" with # of trainingiterations:"", trainingsiterations)",0
if notanalyzed: #this only applies to ask if h5 exists...,0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
"for multi animal, seems only this is used",0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Get animal IDs and corresponding indices in the arrays of detections,0
Form ground truth beforehand,0
Assemble animals on the full set of detections,0
Count the number of unassembled bodyparts,0
Handle unlabeled bodyparts...,0
"Only 1 animal, let us return the full graph indices only",0
Find minimal skeleton,0
Select optimal PAF graph,0
Selective copy; deepcopy is >5x slower,0
Only keeps skeletons that are more than 90% complete,0
TODO Normalize dists by longest length?,1
TODO Smarter imputation technique (Bayesian? Grassmann averages?),1
Covariance matrix estimation fails due to numerical singularities,0
Distance is undefined if the assembly is empty,0
Correct distance to account for missing observations,0
"Alternatively, reduce contribution of missing values to the Mahalanobis",0
distance to zero by substituting the corresponding means.,0
"Fill the subsets with unambiguous, complete individuals",0
Fuse superfluous assemblies,0
Second pass without edge safety,0
Mark the unique body parts as assembled anyway so,0
they are not used later on to fill assemblies.,0
Store selected edges for subsequent frames,0
Remove invalid assemblies,0
Fill assemblies with unconnected body parts,0
Spawning (rather than forking) multiple processes does not,0
work nicely with the GUI or interactive sessions.,0
"In that case, we fall back to the serial assembly.",0
Swap corresponding keypoints,0
Only consider assemblies of at least two keypoints,0
Greedy assembly matching like in pycocotools,0
Global rather than greedy assembly matching,0
Cast columns of dtype 'object' to float to avoid TypeError,0
further down in _parse_ground_truth_data.,0
sigma is taken as the median of all COCO keypoint standard deviations,0
Sort matching score (OKS) in descending order of assembly affinity,0
Guarantee precision decreases monotonically,0
See https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173),0
Regularize by forcing AR <= 5,0
max_ar = 5,0
if el.aspect_ratio >= max_ar:,0
if el.height > el.width:,0
el.width = el.height / max_ar,0
else:,0
el.height = el.width / max_ar,0
Orient the ellipse such that it encompasses most points,0
"n_inside = el.contains_points(np.c_[self.x, self.y]).sum()",0
el.theta += 0.5 * np.pi,0
"if el.contains_points(np.c_[self.x, self.y]).sum() < n_inside:",0
el.theta -= 0.5 * np.pi,0
"r2 = chi2.ppf(2 * norm.cdf(sd) - 1, 2)",0
"height, width = np.sqrt(E * r2)",0
The general quadratic curve has the form:,0
ax^2 + 2bxy + cy^2 + 2dx + 2fy + g = 0,0
Ellipse center coordinates,0
Semi-axes lengths,0
Angle to the horizontal,0
High uncertainty to the unobservable initial velocities,0
Give high uncertainty to the unobservable initial velocities,0
diff = val - cost_matrix,0
"diff[row, col] += val",0
if (,0
val < self.iou_threshold,0
or np.any(diff[row] <= 0.2),0
"or np.any(diff[:, col] <= 0.2)",0
):,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
Modified from scipy source code:,0
- to restrict its use to 2D,0
- to get rid of shuffling (since arrays are only (nbodyparts * 3) element long),0
TODO - factor in keypoint confidence (and weight by # of observations??),1
"mat = self.calc_pairwise_oks(poses, poses_ref)",0
Remove matched detections with low OKS,0
matches = [],0
"for row, col in zip(row_indices, col_indices):",0
"if mat[row, col] < self.oks_threshold:",0
unmatched_poses.append(row),0
unmatched_trackers.append(col),0
else:,0
"matches.append([row, col])",0
if not len(matches):,0
"matches = np.empty((0, 2), dtype=int)",0
else:,0
matches = np.stack(matches),0
update matched trackers with assigned detections,0
create and initialise new trackers for unmatched detections,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
filter out matched with low IOU,0
TODO: get cropping parameters and utilize!,1
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Load meta data & annotations,0
get the graph!,0
"NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*",0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
TODO: Make this code not so redundant!,1
"If a string was passed in, auto-convert to True for backward compatibility",0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to initial folder,0
"# Also storing one ""large"" table with results:",0
note: evaluationfolder.parents[0] to get common folder above all shuffle evaluations.,0
#################################################,0
Load data...,0
#################################################,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
TODO: IMPLEMENT for different batch sizes?,1
Ignore best edges possibly defined during a prior evaluation,0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
Pass the image and the keypoints through the resizer;,0
this has no effect if no augmenters were added to it.,0
FIXME Is having an empty array vs nan really that necessary?!,1
"Form 2D array of shape (n_rows, 4) where the last dimension",0
"is (sample_index, peak_y, peak_x, bpt_index) to slice the PAFs.",0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in,0
Compute all distance statistics,0
Calculate overall prediction error,0
Skip data-driven skeleton selection unless,0
the model was trained on the full graph.,0
"loading backbone from ResNet, MobileNet etc.",0
"display_iters = max(1,int(displayiters))",0
"[_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})",0
Save snapshot,0
return to original path.,0
Clip peak locations to PAFs dimensions,0
"unit_vecs = vecs / lengths[:, np.newaxis]",0
"affinities = np.squeeze(y @ np.expand_dims(unit_vecs, axis=2)).sum(axis=1)",0
Form cost matrices,0
"loading backbone from ResNet, MobileNet etc.",0
"Auto-switch to Adam on M1/M2 chips, as the momentum optimizer crashes",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
Convert a frozen graph to OpenVINO IR format,0
Read network into memory,0
"For better efficiency, model is initialized for batch_size 1 and every sample processed independently",0
Load network to device,0
Prepare input data,0
Change the stride from 2 to 1 to get 16x downscaling instead of 32x.,0
https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142,0
Multi stage is currently only implemented for resnets,0
Defining multi_fusion backbone,0
Attaching multi-stage decoder,0
stage_paf_output = stage_paf_output + pre_stage_paf_output,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
FIXME Fix wrong scope with Keras layers,1
"def prediction_layer(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2DTranspose(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=2,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
## New DLCNet Addition: multi-stage decoder,0
"def prediction_layer_stage(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2D(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=1,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Compute variance using: Var[X]= E[X^2] - E[X]^2.,0
Compute keep_prob,0
TODO(tanmingxing): add support for training progress.,1
Compute drop_connect tensor,0
turn into x times y time bs * bpts,0
extract corresponding locref x and y as well as probability,0
Filter predicted heatmaps with a 2D Gaussian kernel as in:,0
https://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_The_Devil_Is_in_the_Details_Delving_Into_Unbiased_Data_CVPR_2020_paper.pdf,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
defaults will be a public argument for namedtuple in Python 3.7,0
https://docs.python.org/3/library/collections.html#collections.namedtuple,0
Make sure that round down does not go down by more than 10%.,0
Builds the block accordings to arguments.,0
Expansion phase:,0
Depth-wise convolution phase:,0
Squeeze and Excitation layer.,0
Output phase:,0
only apply drop_connect if skip presents.,0
Expansion phase:,0
Output phase:,0
only apply drop_connect if skip presents.,0
Builds blocks.,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Stem part.,0
Head part.,0
Calls Stem layers,0
Calls blocks.,0
Calls final layers and returns logits.,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"(width_coefficient, depth_coefficient, resolution, dropout_rate)",0
blocks_args = [,0
"'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',",0
"'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',",0
"'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',",0
"'r1_k3_s11_e6_i192_o320_se0.25',",0
],0
The default is TPU-specific batch norm.,0
The alternative is tf.layers.BatchNormalization.,0
"batch_norm=utils.TpuBatchNormalization,  # TPU-specific requirement.",0
ValueError will be raised here if override_params has fields not included,0
in global_params.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrunk when given a depth-multiplier < 1.0.,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
Clamp to 40% of crop size to ensure that at least,0
the center keypoint remains visible after the offset is applied.,0
Points located close to one another are sampled preferentially,0
in order to augment crowded regions.,0
Include keypoints in the count to avoid null probabilities,0
Shift the crop center in both dimensions by random amounts,0
and normalize to the original image dimensions.,0
Pickle the 'data' dictionary using the highest protocol available.,0
item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal,0
"Add smart, keypoint-aware image cropping",0
Scale is sampled only once (per batch) to transform all of the images into same size.,0
Discard keypoints whose coordinates lie outside the cropped image,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
Produce score maps and location refinement fields,0
Find indices of individuals in joint_id,0
WIP!,0
STD of gaussian is 1/4 of threshold,0
Grid of coordinates,0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Distance between the joint point and each coordinate,0
NEEDS TO BE DONE!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
TODO Finish implementing actual abstract class,1
parameter initialization for augmentation pipeline:,0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images of a batch into same size.,0
Approximating the scale,0
invisible joints are represented by nans,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"First, initializing variables (if they don't exist)",0
what is the fraction of training samples with scaling augmentation?,0
loading defaults for rotation range!,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"cfg[""rotateratio""] = cfg.get(",0
"""rotratio"", 0.4",0
)  # what is the fraction of training samples with rotation augmentation?,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"Auto cropping is new (was not in Nature Neuroscience 2018 paper, but introduced in Nath et al. Nat. Protocols 2019)",0
"and boosts performance by 2X, particularly on challenging datasets, like the cheetah in Nath et al.",0
Parameters for augmentation with regard to cropping:,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
Get rid of the interpolation beyond the spline knots,0
Retrieve original individual label indices,0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Check the training image paths are correctly stored as arrays of strings,0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
TENSORPACK could fail in WINDOWS...,0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
# Create one identical splits for 3 networks and 3 augmentations,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
Setting specific parameters for training,0
"MODELS = [""dlcrnet_ms5"", ""dlcr101_ms5"", ""efficientnet-b0"", ""mobilenet_v2_0.35""]",0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
## adding it here,0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Downloading the calibration images,0
Deleting unnecessary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
example how to set pose config variables:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
test if existing models are found:,0
Test if nonexisting models are not found,0
Add some other office files:,0
Add a .pickle and .h5 files,0
"By default, all videos with common extensions are taken from a directory",0
A list of extensions can also be passed in,0
Test bounding box from a single keypoint,0
Test that shape was preserved after vectorization,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Guarantee that images smaller than crop size are handled fine,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Disable stochastic scale jitter,0
Verify the aspect ratio is preserved,0
Load sample multi animal data,0
Generate a random permutation and reorder data. Ignore the unique bodypart,0
Get inverse permutation and reorder the modified data to get back,0
to the original,0
Check,0
"Check data comprise path, shape, and xy coordinates",0
Break the graph to test stitching failure,0
Add fake IDs,0
Split all tracklets in half,0
Test flipped keypoints,0
Original (cached) coordinates must have remained empty,0
Assemble based on the smallest graph to speed up testing,0
Generate fake identity predictions,0
Assemble based on the smallest graph to speed up testing,0
Test now with identity only and ensure assemblies,0
contain only parts of a single group ID.,0
Test empty assembly,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
def install(package):,0
"subprocess.check_call([sys.executable, ""-m"", ""pip"", ""install"", package])",0
"install(""tensorflow==1.13.1"")",0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
quicker variant,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
Suppress tensorflow warning messages,0
"Train, evaluate & predict functions / all require TF",0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Updating config file:,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path, num_shuffles=auxfun_models.Check4weights(pose_cfg['net_type'], parent_path, num_shuffles= 1)",0
Updating training and test pose_cfg:,0
Read the config file,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
common parameters:,0
Write dictionary to yaml  config file,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Flatten the list of bodyparts to connect,0
Format data,0
Trick to force equal aspect ratio of 3D plots,0
Set up the matplotlib figure beforehand,0
Set up skeleton LineCollections,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
Get track_method and do related checks,0
Get track method suffix,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
"## Assign nan to [X,Y] of low likelihood predictions ###",0
Convert the data to a np array to easily mask out the low likelihood predictions,0
"Assign [X,Y] = nan to low likelihood predictions",0
Reshape data back to original shape,0
put data back to the dataframes,0
Check individuals are the same in both views,0
Cross-view match individuals,0
Create a dummy variables for single-animal,0
Cleaner variable (since inds view1 == inds view2),0
"Reshape: (num_framex, num_individuals, num_bodyparts , 2)",0
Triangulate data,0
i is individual in view 1,0
voting[i] is the matched individual in view 2,0
Create 3D DataFrame column and row indices,0
Swap num_animals with num_frames axes to ensure well-behaving reshape,0
Fill up 3D dataframe,0
Reorder 2D dataframe in view 2 to match order of view 1,0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Alternatively,0
Sort tracklets by length to prioritize greater continuity,0
Drop tracklets that are too short,0
"Where slots are available, copy the data over",0
"If about to overwrite data, keep tracklets with highest confidence",0
Squeeze some data into empty slots,0
"For the remaining data, overwrite where we are least confident",0
Find keypoints closest to the remaining data,0
Use Manhattan distance to avoid overflow,0
Only overwrite if improving confidence,0
Fill existing gaps,0
Retrieve original individual label indices,0
Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.,0
Fill existing gaps,0
Retrieve original individual label indices,0
ID swaps occur when X and Y simultaneously intersect each other.,0
Get only those bodypart pairs that belong to different individuals,0
"See Li et al., 2012. Cross-view Activity Recognition using Hankelets.",0
"As proposed in the paper, the Hankel matrix can either be formed from",0
the tracklet's centroid or its normalized velocity.,0
"vel = np.diff(self.centroid, axis=0)",0
"vel /= np.linalg.norm(vel, axis=1, keepdims=True)",0
return self.hankelize(vel),0
TODO Add missing data imputation,1
"nrows, ncols = mat.shape",0
beta = nrows / ncols,0
omega = 0.56 * beta ** 3 - 0.95 * beta ** 2 + 1.82 * beta + 1.43,0
return np.argmin(s > omega * np.median(s)),0
"Note that if tracklets are very short, some may actually be part of the same track",0
and thus incorrectly reflect separate track endpoints...,0
"Map each Tracklet to an entry and output nodes and vice versa,",0
which is convenient once the tracklets are stitched.,0
Store tracklets and corresponding negatives (those that overlap in time),0
TODO Avoid looping over all pairs of tracklets,0
Pick the closest (spatially) overlapping tracklet,0
The algorithm works better with integer weights,0
Let us prune the graph by removing all source and sink edges,0
but those connecting the `n_tracks` first and last tracklets.,0
Preflow push seems to work slightly better than shortest,0
"augmentation path..., and is more computationally efficient.",0
Verify whether there are overlapping tracklets,0
"Pick the segment that minimizes ""smoothness"", computed here",0
with the coefficient of variation of the differences.,0
Rebuild a full graph from the remaining nodes without,0
temporal constraint on what tracklets can be stitched together.,0
Cycle through the residuals and incorporate back those,0
that only fit in a single tracklet.,0
Greedily add the remaining residuals,0
Refresh temporal bounds,0
Default to the distance cost function,0
should only exist one,0
Add in identity weighing before building the graph,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implementation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
average distance and average # significant differences avg. over comparisonbodyparts,0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
###################################################,0
Dependencies,0
###################################################,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as,0
specified in the config file,0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
Prepare figure,0
Mask rather than drop unwanted bodyparts to ensure consistent coloring,0
cropping!,0
nx=X2-X1,0
!/usr/bin/env python3,0
"more videos are in principle covered, as OpenCV is used and allows many formats.",0
The above already raises a ValueError if formatting is wrong,0
"Rotate, see: https://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg",0
interesting option to just update metadata.,0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
Read the video until a frame is successfully read,0
import platform,0
"if platform.system() == ""Darwin"":  # for OSX use WXAgg",0
fig.canvas.start_event_loop(timeout=-1),0
else:,0
#fig.canvas.stop_event_loop(),0
"Windows throws error if file path is > 260 characters, can fix with prefix.",0
See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation,0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
This dictionary maps the model types to the file locations where the models exist.,0
Exit the function early if an unknown modeltype is provided.,0
Aliases for backwards-compatibility,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
Initialize costs matrix,0
get common bodypart detections in track pair,0
add 3rd dim to the points,0
cost for any point in time of t1 being the same,0
any point in time of t2,0
Get average cost of the entire track,0
check if it exists:,0
(average_degree / 2) as many edges as there are nodes is required,0
for undirected graphs to reach the target degree.,0
"Attention this order has to be consistent (for training set creation, training, inference etc.)",0
"-> adding (single,bpt) for uniquebodyparts",0
"-> adding (individual,bpt) for multianimalbodyparts",0
load defaults,0
set project specific parameters:,0
###################################################,0
Dependencies,0
###################################################,0
Pose X vs pose Y,0
Poses vs time,0
Likelihoods,0
Histograms,0
#################################################,0
Looping analysis over video,0
#################################################,0
Keep only the individuals and bodyparts that were labeled,0
Either display the animals defined in the config if they are found,0
"in the dataframe, or all the trajectories regardless of their names",0
Determine whether the data are single- or multi-animal without loading into memory,0
simply by checking whether 'individuals' is in the second line of the CSV.,0
Make paths platform-agnostic if they are not already,0
Ensure folder names are strings,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
make list of full paths,0
filter list of videos,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"if ""MULTI!"" in allbpts:",0
Ensure same order as in config.yaml,0
ABBREVIATE NETWORK NAMES -- esp. for mobilenet!,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
Iterate over data files and stop as soon as one matching the scorer is found,0
If there was no match...,0
aliases for backwards-compatibility.,0
Set up a logger and add an stdout handler.,0
A single logger can have many handlers:,0
https://docs.python.org/3/howto/logging.html#handler-basic,0
TODO Dump to log file instead,1
"logger = logging.getLogger(""GUI"")",0
logger.setLevel(logging.DEBUG),0
handler = logging.StreamHandler(stream=sys.stdout),0
handler.setLevel(logging.DEBUG),0
formatter = logging.Formatter(,0
"""%(asctime)s - %(name)s - %(levelname)s - %(message)s"", ""%Y-%m-%d %H:%M:%S""",0
),0
handler.setFormatter(formatter),0
logger.addHandler(handler),0
restore the background region,0
Take into consideration imprecise OpenCV estimation of total number of frames,0
Automatically disable the draggable points,0
Save additional frames to the labeled-data directory,0
Store the newly-refined data,0
Uncertain keypoints are ignored,0
Merge with the already existing annotated data,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
Thread-safe Stdout redirector,0
Creating action using the first constructor,0
Creating actions using the second constructor,0
File menu,0
View menu,0
Help menu,0
all_bodyparts: List,0
NOTE: Is there a case where a specific list should,0
have bodyparts other than the root? I don't think so.,0
Videotype selection,0
Select videos,0
Number of selected videos text,0
Clear video selection,0
"Qt returns a tuple (list of files, filetype)",0
Add tab header,0
Add separating line,0
Look for any extension by default,0
This works both with e.g. .avi and avi,0
Choose multiple files by default,0
Automatically activate the napari-deeplabcut plugin,0
TODO Insert new video,1
TODO Insert skeleton link,1
"Hack to make the first column read-only, as we do not want users to touch it.",1
"The cleaner solution would be to use a QTreeView and QAbstractItemModel,",0
but that is a lot of rework for little benefits.,0
Leave untouched when it is already a string,0
"Slashes also raise the error, but no need to print anything since it is then likely to be a path",0
Walk backwards across parents to get all keys,0
Read the video until a frame is successfully read,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
TODO: Multi video select.... have to change to single video!,1
Shuffle,0
Num animals,0
Min swap length,0
Max gap to fill,0
Trail length,0
Filter type,0
Filter window length,0
Shuffle,0
Tracker Type,0
Num animals,0
Num triplets,0
layout.addWidget(),0
Shuffle,0
Overwrite videos,0
Trail Points,0
Plot all bodyparts,0
Skeleton,0
Filtered data,0
Plot trajectories,0
High quality video,0
Bodypart list,0
Extraction method,0
Frame extraction algorithm,0
Frame cropping,0
Cluster step,0
GUI Slider width,0
use the default pose_cfg file for default values,0
Shuffle,0
Display iterations,0
Save iterations,0
Max iterations,0
Max number snapshots to keep,0
layout.addWidget(),0
Single / Multi animal Only Layouts,0
Dynamic bodypart cropping,0
Save results as csv,0
Filter predictions,0
Plot Trajectories,0
Show trajectory plots,0
Shuffle,0
layout.addLayout(tmp_layout),0
tmp_layout = QtWidgets.QGridLayout(),0
Shuffle,0
Augmentation method,0
Neural Network,0
TODO: finish model_comparison,1
Check that training data files were indeed created.,0
Add config text field and button,0
layout.addWidget(),0
Display all images,0
Shuffle,0
"calling create_tracking_dataset, train_tracking_transformer, stitch_tracklets",0
only try to find these features if they are in the dictionary,0
1) reference to video folder and get the proper bpt_feature file for feature table,0
2) get either the path to gt or the path to track pickle,0
tracklets,0
"with npy list form videos, split each to train and test",0
"video_name = '.'.join(video.split(""/"")[-1].split(""."")[:-1])",0
assuming there is only one match,0
make my own model factory,0
make my own loss factory,0
"NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights",0
"NOTE: drop path for stochastic depth, we shall see if this is better than dropout here",0
only channel pooling,0
"x -> (B, 12, 2048)",0
"x -> (B, 12, 768)",0
trying to keep everything simple,0
need to write my own keypoint embedding,0
we don't need a fc layer here,0
x: inputs,0
"(B, 12, 768)",0
added pos embed,0
remove all parts I don't understand,0
forwarding through blocks. But num of blocks should be reduced,0
i can't really tell what the difference is,0
will have to change some of these,0
For old models that I trained prior to conv based patchification,0
To resize pos embedding when using model at different size from pretrained weights,0
Rescale the grid of position embeddings when loading from state_dict. Adapted from,0
https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224,0
Cut & paste from PyTorch official master until it's in a few official releases - RW,0
Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf,0
Computes standard normal cumulative distribution function,0
Values are generated by using a truncated uniform distribution and,0
then using the inverse CDF for the normal distribution.,0
Get upper and lower cdf values,0
"Uniformly fill tensor with values from [l, u], then translate to",0
"[2l-1, 2u-1].",0
Use inverse cdf transform for normal distribution to get truncated,0
standard normal,0
"Transform to proper mean, std",0
Clamp to ensure it's in the proper range,0
"type: (Tensor, float, float, float, float) -> Tensor",0
"resample if noise out of percent limit, brute force but shouldn't spin much",0
distmat g,0
q    1 3 2 4,0
4 1 2 3,0
0 2 1 3,0
1 2 3 0,0
compute cmc curve for each query,0
get query pid and camid,0
remove gallery samples that have the same pid and camid with query,0
compute cmc curve,0
"binary vector, positions with value 1 are correct matches",0
this condition is true when query identity does not appear in gallery,0
compute average precision,0
reference: https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Average_precision,0
"tmp_cmc = [x / (i + 1.) for i, x in enumerate(tmp_cmc)]",0
query,0
gallery,0
"distmat = re_ranking(qf, gf, k1=20, k2=6, lambda_value=0.3)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
"if feature vector is numpy, you should use 'torch.tensor' transform it to tensor",0
print('using GPU to compute original distance'),0
print('starting re_ranking'),0
k-reciprocal neighbors,0
More elegantly one can simply define:,0
take care of difference between feature map space and original image space,0
cos = torch.cdist,0
TODO: maybe find a better spot for this.,1
"dev =  device if torch.cuda.is_available() else ""cpu""",0
train,0
print (f'validation loss {val_loss/len(val_loader)}'),0
normalize vectors here,0
i th vector at j th kpt,0
maybe needs to convert them to embeddings and position token,0
Drop missing body parts,0
Drop points lying outside the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
## dlcnet_ms5: backbone resnet50 + multi-fusion & multi-stage module,0
## dlcr101_ms5/dlcr152_ms5: backbone resnet101/152 + multi-fusion & multi-stage module,0
"If the graph is unnecessarily large (with 15+ keypoints by default),",0
we randomly prune it to a size guaranteeing an average node degree of 6;,0
"see Suppl. Fig S9c in Lauer et al., 2022.",0
Use the skeleton defined in the config file,0
Ignore possible connections between 'multi' and 'unique' body parts;,0
one can never be too careful...,0
Disable the prediction of PAFs if the graph is empty,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving metadata and data file (Pickle file),0
###############################################################################,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Setting inference cfg file:,0
Search a pose_config.yaml file to parse missing information,0
Clean the training-datasets folder prior to recreating the data pickles,0
Load updated lists:,0
Find the corresponding video file,0
"print(key, ""dropping?"")",0
adding important values for multianiaml project:,0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Determine the index length required to guarantee,0
the train–test ratio is exactly the desired one.,0
Pad indices so lengths agree,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(name,test_video_name)",0
Blazing fast and does not load the image into memory,0
Check that points lie within the image,0
DeprecationWarnings are silenced since Python 3.2 unless triggered in __main__,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
updating variable if null/None! #backwardscompatability,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
these values are dropped as scalecrop,0
doesn't have rotation implemented,0
Also drop maDLC smart cropping augmentation parameters,0
sort the model directories,0
get the shuffle index and offset by 1.,0
read cfg file,0
create log file,0
Check for variable correctness,0
Moviepy:,0
select crop method,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
allow_growth must be true here because tensorflow does not automatically free gpu memory and setting it as false occupies all gpu memory so that pytorch cannot kick in,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
should close tensorflow session here in order to free gpu,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html#videocapture-get,0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
TODO: add cropping as in video analysis!,1
if cropping is not None:,0
cfg['cropping']=True,0
"cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping",0
"print(""Overwriting cropping parameters:"", cropping)",0
"print(""These are used for all videos, but won't be save to the cfg file."")",0
"Also ensure `boundingboxslack` is greater than zero, otherwise overlap",0
"between trackers cannot be evaluated, resulting in empty tracklets.",0
Check which snapshots are available and sort them by # iterations,0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
NOTE: If dataname line above is changed then line below is obsolete?,0
"trackname = trackname.replace(videofolder, destfolder)",0
TODO: adjust this for multi + unique bodyparts!,1
this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. ),0
Optimal identity assignment based on soft voting,0
imagenet mean for resnet pretraining:,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/DeepLabCut/DeepLabCut/pull/324",0
for DLC 2.2. (here all set False to not use PAFs/pairwise fields),0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
"reloading defaults, as they can bleed over from a previous run otherwise",0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
dlc_cfg_train = load_config(str(path_train_config)),0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
Name for deeplabcut net (based on its parameters),0
"DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])",0
"print(""Extracting maps for "", DLCscorer, "" with # of trainingiterations:"", trainingsiterations)",0
if notanalyzed: #this only applies to ask if h5 exists...,0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
"for multi animal, seems only this is used",0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Get animal IDs and corresponding indices in the arrays of detections,0
Form ground truth beforehand,0
Assemble animals on the full set of detections,0
Count the number of unassembled bodyparts,0
Handle unlabeled bodyparts...,0
"Only 1 animal, let us return the full graph indices only",0
Find minimal skeleton,0
Select optimal PAF graph,0
Selective copy; deepcopy is >5x slower,0
Only keeps skeletons that are more than 90% complete,0
TODO Normalize dists by longest length?,1
TODO Smarter imputation technique (Bayesian? Grassmann averages?),1
Covariance matrix estimation fails due to numerical singularities,0
Distance is undefined if the assembly is empty,0
Correct distance to account for missing observations,0
"Alternatively, reduce contribution of missing values to the Mahalanobis",0
distance to zero by substituting the corresponding means.,0
"Fill the subsets with unambiguous, complete individuals",0
Fuse superfluous assemblies,0
Second pass without edge safety,0
Mark the unique body parts as assembled anyway so,0
they are not used later on to fill assemblies.,0
Store selected edges for subsequent frames,0
Remove invalid assemblies,0
Fill assemblies with unconnected body parts,0
Spawning (rather than forking) multiple processes does not,0
work nicely with the GUI or interactive sessions.,0
"In that case, we fall back to the serial assembly.",0
Swap corresponding keypoints,0
Only consider assemblies of at least two keypoints,0
Greedy assembly matching like in pycocotools,0
Global rather than greedy assembly matching,0
Cast columns of dtype 'object' to float to avoid TypeError,0
further down in _parse_ground_truth_data.,0
sigma is taken as the median of all COCO keypoint standard deviations,0
Sort matching score (OKS) in descending order of assembly affinity,0
Guarantee precision decreases monotonically,0
See https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173),0
Regularize by forcing AR <= 5,0
max_ar = 5,0
if el.aspect_ratio >= max_ar:,0
if el.height > el.width:,0
el.width = el.height / max_ar,0
else:,0
el.height = el.width / max_ar,0
Orient the ellipse such that it encompasses most points,0
"n_inside = el.contains_points(np.c_[self.x, self.y]).sum()",0
el.theta += 0.5 * np.pi,0
"if el.contains_points(np.c_[self.x, self.y]).sum() < n_inside:",0
el.theta -= 0.5 * np.pi,0
"r2 = chi2.ppf(2 * norm.cdf(sd) - 1, 2)",0
"height, width = np.sqrt(E * r2)",0
The general quadratic curve has the form:,0
ax^2 + 2bxy + cy^2 + 2dx + 2fy + g = 0,0
Ellipse center coordinates,0
Semi-axes lengths,0
Angle to the horizontal,0
High uncertainty to the unobservable initial velocities,0
Give high uncertainty to the unobservable initial velocities,0
diff = val - cost_matrix,0
"diff[row, col] += val",0
if (,0
val < self.iou_threshold,0
or np.any(diff[row] <= 0.2),0
"or np.any(diff[:, col] <= 0.2)",0
):,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
Modified from scipy source code:,0
- to restrict its use to 2D,0
- to get rid of shuffling (since arrays are only (nbodyparts * 3) element long),0
TODO - factor in keypoint confidence (and weight by # of observations??),1
"mat = self.calc_pairwise_oks(poses, poses_ref)",0
Remove matched detections with low OKS,0
matches = [],0
"for row, col in zip(row_indices, col_indices):",0
"if mat[row, col] < self.oks_threshold:",0
unmatched_poses.append(row),0
unmatched_trackers.append(col),0
else:,0
"matches.append([row, col])",0
if not len(matches):,0
"matches = np.empty((0, 2), dtype=int)",0
else:,0
matches = np.stack(matches),0
update matched trackers with assigned detections,0
create and initialise new trackers for unmatched detections,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
filter out matched with low IOU,0
TODO: get cropping parameters and utilize!,1
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Load meta data & annotations,0
get the graph!,0
"NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*",0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
TODO: Make this code not so redundant!,1
"If a string was passed in, auto-convert to True for backward compatibility",0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to initial folder,0
"# Also storing one ""large"" table with results:",0
note: evaluationfolder.parents[0] to get common folder above all shuffle evaluations.,0
#################################################,0
Load data...,0
#################################################,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
TODO: IMPLEMENT for different batch sizes?,1
Ignore best edges possibly defined during a prior evaluation,0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
Pass the image and the keypoints through the resizer;,0
this has no effect if no augmenters were added to it.,0
FIXME Is having an empty array vs nan really that necessary?!,1
"Form 2D array of shape (n_rows, 4) where the last dimension",0
"is (sample_index, peak_y, peak_x, bpt_index) to slice the PAFs.",0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in,0
Compute all distance statistics,0
Calculate overall prediction error,0
Skip data-driven skeleton selection unless,0
the model was trained on the full graph.,0
"loading backbone from ResNet, MobileNet etc.",0
"display_iters = max(1,int(displayiters))",0
"[_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})",0
Save snapshot,0
return to original path.,0
Clip peak locations to PAFs dimensions,0
"unit_vecs = vecs / lengths[:, np.newaxis]",0
"affinities = np.squeeze(y @ np.expand_dims(unit_vecs, axis=2)).sum(axis=1)",0
Form cost matrices,0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
Convert a frozen graph to OpenVINO IR format,0
Read network into memory,0
"For better efficiency, model is initialized for batch_size 1 and every sample processed independently",0
Load network to device,0
Prepare input data,0
Change the stride from 2 to 1 to get 16x downscaling instead of 32x.,0
https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142,0
Multi stage is currently only implemented for resnets,0
Defining multi_fusion backbone,0
Attaching multi-stage decoder,0
stage_paf_output = stage_paf_output + pre_stage_paf_output,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
FIXME Fix wrong scope with Keras layers,1
"def prediction_layer(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2DTranspose(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=2,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
## New DLCNet Addition: multi-stage decoder,0
"def prediction_layer_stage(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2D(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=1,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Compute variance using: Var[X]= E[X^2] - E[X]^2.,0
Compute keep_prob,0
TODO(tanmingxing): add support for training progress.,1
Compute drop_connect tensor,0
turn into x times y time bs * bpts,0
extract corresponding locref x and y as well as probability,0
Filter predicted heatmaps with a 2D Gaussian kernel as in:,0
https://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_The_Devil_Is_in_the_Details_Delving_Into_Unbiased_Data_CVPR_2020_paper.pdf,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
defaults will be a public argument for namedtuple in Python 3.7,0
https://docs.python.org/3/library/collections.html#collections.namedtuple,0
Make sure that round down does not go down by more than 10%.,0
Builds the block accordings to arguments.,0
Expansion phase:,0
Depth-wise convolution phase:,0
Squeeze and Excitation layer.,0
Output phase:,0
only apply drop_connect if skip presents.,0
Expansion phase:,0
Output phase:,0
only apply drop_connect if skip presents.,0
Builds blocks.,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Stem part.,0
Head part.,0
Calls Stem layers,0
Calls blocks.,0
Calls final layers and returns logits.,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"(width_coefficient, depth_coefficient, resolution, dropout_rate)",0
blocks_args = [,0
"'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',",0
"'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',",0
"'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',",0
"'r1_k3_s11_e6_i192_o320_se0.25',",0
],0
The default is TPU-specific batch norm.,0
The alternative is tf.layers.BatchNormalization.,0
"batch_norm=utils.TpuBatchNormalization,  # TPU-specific requirement.",0
ValueError will be raised here if override_params has fields not included,0
in global_params.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrunk when given a depth-multiplier < 1.0.,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
Clamp to 40% of crop size to ensure that at least,0
the center keypoint remains visible after the offset is applied.,0
Points located close to one another are sampled preferentially,0
in order to augment crowded regions.,0
Include keypoints in the count to avoid null probabilities,0
Shift the crop center in both dimensions by random amounts,0
and normalize to the original image dimensions.,0
Pickle the 'data' dictionary using the highest protocol available.,0
item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal,0
"Add smart, keypoint-aware image cropping",0
Scale is sampled only once (per batch) to transform all of the images into same size.,0
Discard keypoints whose coordinates lie outside the cropped image,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
Produce score maps and location refinement fields,0
Find indices of individuals in joint_id,0
WIP!,0
STD of gaussian is 1/4 of threshold,0
Grid of coordinates,0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Distance between the joint point and each coordinate,0
NEEDS TO BE DONE!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
TODO Finish implementing actual abstract class,1
parameter initialization for augmentation pipeline:,0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images of a batch into same size.,0
Approximating the scale,0
invisible joints are represented by nans,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"First, initializing variables (if they don't exist)",0
what is the fraction of training samples with scaling augmentation?,0
loading defaults for rotation range!,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"cfg[""rotateratio""] = cfg.get(",0
"""rotratio"", 0.4",0
)  # what is the fraction of training samples with rotation augmentation?,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"Auto cropping is new (was not in Nature Neuroscience 2018 paper, but introduced in Nath et al. Nat. Protocols 2019)",0
"and boosts performance by 2X, particularly on challenging datasets, like the cheetah in Nath et al.",0
Parameters for augmentation with regard to cropping:,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
Get rid of the interpolation beyond the spline knots,0
Retrieve original individual label indices,0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Check the training image paths are correctly stored as arrays of strings,0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
TENSORPACK could fail in WINDOWS...,0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
# Create one identical splits for 3 networks and 3 augmentations,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
Setting specific parameters for training,0
"MODELS = [""dlcrnet_ms5"", ""dlcr101_ms5"", ""efficientnet-b0"", ""mobilenet_v2_0.35""]",0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
## adding it here,0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Downloading the calibration images,0
Deleting unnecessary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
example how to set pose config variables:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
test if existing models are found:,0
Test if nonexisting models are not found,0
Add some other office files:,0
Add a .pickle and .h5 files,0
"By default, all videos with common extensions are taken from a directory",0
A list of extensions can also be passed in,0
Test bounding box from a single keypoint,0
Test that shape was preserved after vectorization,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Guarantee that images smaller than crop size are handled fine,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Disable stochastic scale jitter,0
Verify the aspect ratio is preserved,0
Load sample multi animal data,0
Generate a random permutation and reorder data. Ignore the unique bodypart,0
Get inverse permutation and reorder the modified data to get back,0
to the original,0
Check,0
"Check data comprise path, shape, and xy coordinates",0
Break the graph to test stitching failure,0
Add fake IDs,0
Split all tracklets in half,0
Test flipped keypoints,0
Original (cached) coordinates must have remained empty,0
Assemble based on the smallest graph to speed up testing,0
Generate fake identity predictions,0
Assemble based on the smallest graph to speed up testing,0
Test now with identity only and ensure assemblies,0
contain only parts of a single group ID.,0
Test empty assembly,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
def install(package):,0
"subprocess.check_call([sys.executable, ""-m"", ""pip"", ""install"", package])",0
"install(""tensorflow==1.13.1"")",0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
quicker variant,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
Suppress tensorflow warning messages,0
"Train, evaluate & predict functions / all require TF",0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Updating config file:,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path, num_shuffles=auxfun_models.Check4weights(pose_cfg['net_type'], parent_path, num_shuffles= 1)",0
Updating training and test pose_cfg:,0
Read the config file,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
common parameters:,0
Write dictionary to yaml  config file,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Flatten the list of bodyparts to connect,0
Format data,0
Trick to force equal aspect ratio of 3D plots,0
Set up the matplotlib figure beforehand,0
Set up skeleton LineCollections,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
Get track_method and do related checks,0
Get track method suffix,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
"## Assign nan to [X,Y] of low likelihood predictions ###",0
Convert the data to a np array to easily mask out the low likelihood predictions,0
"Assign [X,Y] = nan to low likelihood predictions",0
Reshape data back to original shape,0
put data back to the dataframes,0
Check individuals are the same in both views,0
Cross-view match individuals,0
Create a dummy variables for single-animal,0
Cleaner variable (since inds view1 == inds view2),0
"Reshape: (num_framex, num_individuals, num_bodyparts , 2)",0
Triangulate data,0
i is individual in view 1,0
voting[i] is the matched individual in view 2,0
Create 3D DataFrame column and row indices,0
Swap num_animals with num_frames axes to ensure well-behaving reshape,0
Fill up 3D dataframe,0
Reorder 2D dataframe in view 2 to match order of view 1,0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Alternatively,0
Sort tracklets by length to prioritize greater continuity,0
Drop tracklets that are too short,0
"Where slots are available, copy the data over",0
"If about to overwrite data, keep tracklets with highest confidence",0
Squeeze some data into empty slots,0
"For the remaining data, overwrite where we are least confident",0
Find keypoints closest to the remaining data,0
Use Manhattan distance to avoid overflow,0
Only overwrite if improving confidence,0
Fill existing gaps,0
Retrieve original individual label indices,0
Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.,0
Fill existing gaps,0
Retrieve original individual label indices,0
ID swaps occur when X and Y simultaneously intersect each other.,0
Get only those bodypart pairs that belong to different individuals,0
"See Li et al., 2012. Cross-view Activity Recognition using Hankelets.",0
"As proposed in the paper, the Hankel matrix can either be formed from",0
the tracklet's centroid or its normalized velocity.,0
"vel = np.diff(self.centroid, axis=0)",0
"vel /= np.linalg.norm(vel, axis=1, keepdims=True)",0
return self.hankelize(vel),0
TODO Add missing data imputation,1
"nrows, ncols = mat.shape",0
beta = nrows / ncols,0
omega = 0.56 * beta ** 3 - 0.95 * beta ** 2 + 1.82 * beta + 1.43,0
return np.argmin(s > omega * np.median(s)),0
"Note that if tracklets are very short, some may actually be part of the same track",0
and thus incorrectly reflect separate track endpoints...,0
"Map each Tracklet to an entry and output nodes and vice versa,",0
which is convenient once the tracklets are stitched.,0
Store tracklets and corresponding negatives (those that overlap in time),0
TODO Avoid looping over all pairs of tracklets,0
Pick the closest (spatially) overlapping tracklet,0
The algorithm works better with integer weights,0
Let us prune the graph by removing all source and sink edges,0
but those connecting the `n_tracks` first and last tracklets.,0
Preflow push seems to work slightly better than shortest,0
"augmentation path..., and is more computationally efficient.",0
Verify whether there are overlapping tracklets,0
"Pick the segment that minimizes ""smoothness"", computed here",0
with the coefficient of variation of the differences.,0
Rebuild a full graph from the remaining nodes without,0
temporal constraint on what tracklets can be stitched together.,0
Cycle through the residuals and incorporate back those,0
that only fit in a single tracklet.,0
Greedily add the remaining residuals,0
Refresh temporal bounds,0
Default to the distance cost function,0
should only exist one,0
Add in identity weighing before building the graph,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implementation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
average distance and average # significant differences avg. over comparisonbodyparts,0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
###################################################,0
Dependencies,0
###################################################,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as,0
specified in the config file,0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
Prepare figure,0
Mask rather than drop unwanted bodyparts to ensure consistent coloring,0
cropping!,0
nx=X2-X1,0
!/usr/bin/env python3,0
"more videos are in principle covered, as OpenCV is used and allows many formats.",0
The above already raises a ValueError if formatting is wrong,0
"Rotate, see: https://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg",0
interesting option to just update metadata.,0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
Read the video until a frame is successfully read,0
import platform,0
"if platform.system() == ""Darwin"":  # for OSX use WXAgg",0
fig.canvas.start_event_loop(timeout=-1),0
else:,0
#fig.canvas.stop_event_loop(),0
"Windows throws error if file path is > 260 characters, can fix with prefix.",0
See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation,0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
This dictionary maps the model types to the file locations where the models exist.,0
Exit the function early if an unknown modeltype is provided.,0
Aliases for backwards-compatibility,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
Initialize costs matrix,0
get common bodypart detections in track pair,0
add 3rd dim to the points,0
cost for any point in time of t1 being the same,0
any point in time of t2,0
Get average cost of the entire track,0
check if it exists:,0
(average_degree / 2) as many edges as there are nodes is required,0
for undirected graphs to reach the target degree.,0
"Attention this order has to be consistent (for training set creation, training, inference etc.)",0
"-> adding (single,bpt) for uniquebodyparts",0
"-> adding (individual,bpt) for multianimalbodyparts",0
load defaults,0
set project specific parameters:,0
###################################################,0
Dependencies,0
###################################################,0
Pose X vs pose Y,0
Poses vs time,0
Likelihoods,0
Histograms,0
#################################################,0
Looping analysis over video,0
#################################################,0
Keep only the individuals and bodyparts that were labeled,0
Either display the animals defined in the config if they are found,0
"in the dataframe, or all the trajectories regardless of their names",0
Determine whether the data are single- or multi-animal without loading into memory,0
simply by checking whether 'individuals' is in the second line of the CSV.,0
Make paths platform-agnostic if they are not already,0
Ensure folder names are strings,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
make list of full paths,0
filter list of videos,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"if ""MULTI!"" in allbpts:",0
Ensure same order as in config.yaml,0
ABBREVIATE NETWORK NAMES -- esp. for mobilenet!,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
Iterate over data files and stop as soon as one matching the scorer is found,0
If there was no match...,0
aliases for backwards-compatibility.,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"trainingset = wx.StaticBox(self, label=""Specify the trainingset index"")",0
"trainingset_boxsizer = wx.StaticBoxSizer(trainingset, wx.VERTICAL)",0
"self.trainingset = wx.SpinCtrl(self, value=""0"", min=0, max=100)",0
trainingset_boxsizer.Add(,0
"self.trainingset, 1, wx.EXPAND | wx.TOP | wx.BOTTOM, 10",0
),0
"hbox_.Add(trainingset_boxsizer, 5, wx.EXPAND | wx.TOP | wx.BOTTOM, 5)",0
Read the infer config file,0
trainingsetindex = self.trainingset.GetValue(),0
let the user open the file with default text editor. Also make it mac compatible,0
trainingsetindex = self.trainingset.GetValue(),0
"trainingsetindex=trainingsetindex,",0
"trainingsetindex=self.trainingset.GetValue(),",0
self.save.Enable(False),0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
topSplitter = wx.SplitterWindow(self),0
,0
"self.image_panel = ImagePanel(topSplitter, config,video,shuffle,Dataframe,self.gui_size)",0
self.widget_panel = WidgetPanel(topSplitter),0
,0
"topSplitter.SplitHorizontally(self.image_panel, self.widget_panel,sashPosition=self.gui_size[1]*0.83)#0.9",0
topSplitter.SetSashGravity(1),0
sizer = wx.BoxSizer(wx.VERTICAL),0
"sizer.Add(topSplitter, 1, wx.EXPAND)",0
self.SetSizer(sizer),0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
Read config file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
variable initialization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0,10), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=10)",0
Add all the options,0
line = wx.StaticLine(self),0
"self.sizer.Add(line, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=10)",0
,0
"self.multi_choice.Bind(wx.EVT_CHECKBOX,self.activate_copy_videos)",0
Hide the button as this is not the default option,0
self.vids.Enable(False),0
self.SetSizer(self.sizer),0
"self.sizer.Add(self.sizer, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=15)",0
self.ok.Enable(False),0
else:,0
self.ok.Enable(True),0
For mac compatibility,0
Remove the pages in case the user goes back to the create new project and creates/load a new project,0
Add all the other pages,0
"page10 = Refine_labels(self.parent, self.gui_size, self.cfg, page5)",0
"self.parent.AddPage(page10, ""OPT: Refine Labels"")",0
self.yes.Enable(False),0
self.no.Enable(False),0
Here we create a panel and a notebook on the panel,0
create the page windows as children of the notebook and add the pages to the notebook with the label to show on the tab,0
Take into consideration imprecise OpenCV estimation of total number of frames,0
Automatically disable the draggable points,0
Save additional frames to the labeled-data directory,0
Store the newly-refined data,0
Uncertain keypoints are ignored,0
Merge with the already existing annotated data,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.buttonCounter = [],0
self.bodyparts = [],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
load labeledPoints and fundamental Matrix,0
Find offset terms for drawing epipolar Lines,0
Get crop params for camera being labeled,0
Get crop params for other camera,0
convert the image to RGB as you are showing the image with matplotlib,0
draw epipolar lines,0
##################################################################################################################################################,0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
variable initialization,0
design the panel,0
Add logo of DLC,0
self.filelist = self.filelist + self.vids,0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
variable initialization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initialization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
## train and go ###,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
load labeledPoints and fundamental Matrix,0
Find offset terms for drawing epipolar Lines,0
Get crop params for camera being labeled,0
Get crop params for other camera,0
convert the image to RGB as you are showing the image with matplotlib,0
draw epipolar lines,0
divider = make_axes_locatable(self.axes),0
"colorIndex = np.linspace(np.min(im),np.max(im),len(bodyparts))",0
"cax = divider.append_axes(""right"", size=""5%"", pad=0.05)",0
"cbar = self.figure.colorbar(ax, cax=cax,spacing='proportional', ticks=colorIndex)",0
cbar.set_ticklabels(bodyparts[::-1]),0
self.figure.canvas.draw(),0
##################################################################################################################################################,0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"MainFrame.saveDataSet(self, event)",0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
checks for unique bodyparts,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Create an empty data frame,0
Cache original bodyparts,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Rearrange bodypart columns in config order,0
Test whether there are missing frames and superfluous data,0
Check whether new individuals or body parts were added,0
Checking if user added a new label,0
Found new labels in either multiple bodyparts or unique bodyparts,0
check if single is selected when radio buttons are changed,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Discard data associated with bodyparts that are no longer in the config,0
Re-organize the dataframe so the CSV looks consistent with the config,0
restore the background region,0
variable initialization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.build = wx.Button(self, label=""Build skeleton"")",0
"sizer.Add(self.build, pos=(4, 3), flag=wx.BOTTOM | wx.RIGHT, border=10)",0
"self.build.Bind(wx.EVT_BUTTON, self.build_skeleton)",0
self.build.Enable(True),0
"def build_skeleton(self, event):",0
skeleton.SkeletonBuilder(self.config),0
Setting the GUI size and panels design,0
This sets the minimum size of the GUI. It can scale now!,0
Checks if zoom/pan button is ON,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
variable initialization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"trainingindex = wx.StaticBox(self, label=""Specify the trainingset index"")",0
"trainingindex_boxsizer = wx.StaticBoxSizer(trainingindex, wx.VERTICAL)",0
"self.trainingindex = wx.SpinCtrl(self, value=""0"", min=0, max=100)",0
trainingindex_boxsizer.Add(,0
"self.trainingindex, 1, wx.EXPAND | wx.TOP | wx.BOTTOM, 10)",0
use the default pose_cfg file for default values,0
self.display_iters.Enable(False),0
self.save_iters.Enable(False),0
self.max_iters.Enable(False),0
self.snapshots.Enable(False),0
"hbox1.Add(trainingindex_boxsizer, 10, wx.EXPAND | wx.TOP | wx.BOTTOM, 5)",0
self.trainingindex.Enable(False),0
self.trainingindex.Enable(True),0
self.trainingindex.Enable(True),0
Read the pose config file,0
print(trainFraction[-1]),0
"print(os.path.join(cfg['project_path'],auxiliaryfunctions.get_model_folder(trainFraction, self.shuffles.GetValue(),cfg),'train','pose_cfg.yaml'))",0
let the user open the file with default text editor. Also make it mac compatible,0
update the variables with the edited values in the pose config file,0
self.trainingindex.Enable(True),0
variable initialization,0
design the panel,0
Add logo of DLC,0
### BOX 1 ####,0
### BOX 2 ####,0
### BOX 3 ####,0
Trigger chooseOption as if the radio button had been clicked,0
Read the infer config file,0
"trainFraction = cfg[""TrainingFraction""][trainingsetindex]",0
let the user open the file with default text editor. Also make it mac compatible,0
"self.select_destfolder.SetPath(""None"")",0
self.config = [],0
"self.sel_config.SetPath("""")",0
variable initialization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"trainingindex_box = wx.StaticBox(self, label=""Specify the trainingset index"")",0
"trainingindex_boxsizer = wx.StaticBoxSizer(trainingindex_box, wx.VERTICAL)",0
"self.trainingindex = wx.SpinCtrl(self, value=""0"", min=0, max=100)",0
trainingindex_boxsizer.Add(,0
"self.trainingindex, 0, wx.EXPAND | wx.TOP | wx.BOTTOM, 10",0
),0
"self.hbox2.Add(trainingindex_boxsizer, 10, wx.EXPAND | wx.TOP | wx.BOTTOM, 5)",0
trainindex = self.trainingindex.GetValue(),0
"trainindex=trainindex,",0
"self.shuffles.SetValue(""1"")",0
"DownSampleVideo(vname,width=-1,height=200,outsuffix='downsampled',outpath=None,rotatecw=False):",0
variable initialization,0
design the panel,0
Add logo of DLC,0
"majorDimension=0,",0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
"self.choiceBox,self.visualization_rdb = self.choice_panel.addRadioButtons()",0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
Overwrite the config-defined individual names,0
with those actually present in the annotated data,0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
print(self.Dataframe.head()),0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
fix move to corner,0
fix move to corner,0
fix move to corner,0
fix move to corner,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
variable initialization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"trainingset = wx.StaticBox(self, label=""Specify the trainingset index"")",0
"trainingset_boxsizer = wx.StaticBoxSizer(trainingset, wx.VERTICAL)",0
"self.trainingset = wx.SpinCtrl(self, value=""0"", min=0, max=100)",0
trainingset_boxsizer.Add(,0
"self.trainingset, 1, wx.EXPAND | wx.TOP | wx.BOTTOM, 10",0
),0
"self.hbox1.Add(trainingset_boxsizer, 5, wx.EXPAND | wx.TOP | wx.BOTTOM, 5)",0
"if self.plot_scoremaps.GetStringSelection() == ""Yes"":",0
Read the infer config file,0
trainingsetindex = self.trainingset.GetValue(),0
let the user open the file with default text editor. Also make it mac compatible,0
shuffle = self.shuffle.GetValue(),0
trainingsetindex = self.trainingset.GetValue(),0
"trainingsetindex=trainingsetindex,",0
variable initialization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
## LABELING ###,0
"self.sel_config.SetPath("""")",0
##################################################################################################################################################,0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
self.cfg = auxiliaryfunctions.read_config(config),0
"self.statusbar.SetStatusText("""")",0
"dlg = wx.MessageDialog(None,""Are you sure?"", ""Quit!"",wx.YES_NO | wx.ICON_WARNING)",0
result = dlg.ShowModal(),0
if result == wx.ID_YES:,0
variable initialization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0, 4), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=5)",0
variable initialization,0
design the panel,0
Add logo of DLC,0
#         design the panel,0
Add image of DLC,0
if editing this text make sure you add the '\n' to get the new line. The sizer is unable to format lines correctly.,0
"calling create_tracking_dataset, train_tracking_transformer, stitch_tracklets",0
only try to find these features if they are in the dictionary,0
1) reference to video folder and get the proper bpt_feature file for feature table,0
2) get either the path to gt or the path to track pickle,0
tracklets,0
"with npy list form videos, split each to train and test",0
"video_name = '.'.join(video.split(""/"")[-1].split(""."")[:-1])",0
assuming there is only one match,0
make my own model factory,0
make my own loss factory,0
"NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights",0
"NOTE: drop path for stochastic depth, we shall see if this is better than dropout here",0
only channel pooling,0
"x -> (B, 12, 2048)",0
"x -> (B, 12, 768)",0
trying to keep everything simple,0
need to write my own keypoint embedding,0
we don't need a fc layer here,0
x: inputs,0
"(B, 12, 768)",0
added pos embed,0
remove all parts I don't understand,0
forwarding through blocks. But num of blocks should be reduced,0
i can't really tell what the difference is,0
will have to change some of these,0
For old models that I trained prior to conv based patchification,0
To resize pos embedding when using model at different size from pretrained weights,0
Rescale the grid of position embeddings when loading from state_dict. Adapted from,0
https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224,0
Cut & paste from PyTorch official master until it's in a few official releases - RW,0
Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf,0
Computes standard normal cumulative distribution function,0
Values are generated by using a truncated uniform distribution and,0
then using the inverse CDF for the normal distribution.,0
Get upper and lower cdf values,0
"Uniformly fill tensor with values from [l, u], then translate to",0
"[2l-1, 2u-1].",0
Use inverse cdf transform for normal distribution to get truncated,0
standard normal,0
"Transform to proper mean, std",0
Clamp to ensure it's in the proper range,0
"type: (Tensor, float, float, float, float) -> Tensor",0
"resample if noise out of percent limit, brute force but shouldn't spin much",0
distmat g,0
q    1 3 2 4,0
4 1 2 3,0
0 2 1 3,0
1 2 3 0,0
compute cmc curve for each query,0
get query pid and camid,0
remove gallery samples that have the same pid and camid with query,0
compute cmc curve,0
"binary vector, positions with value 1 are correct matches",0
this condition is true when query identity does not appear in gallery,0
compute average precision,0
reference: https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Average_precision,0
"tmp_cmc = [x / (i + 1.) for i, x in enumerate(tmp_cmc)]",0
query,0
gallery,0
"distmat = re_ranking(qf, gf, k1=20, k2=6, lambda_value=0.3)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
"if feature vector is numpy, you should use 'torch.tensor' transform it to tensor",0
print('using GPU to compute original distance'),0
print('starting re_ranking'),0
k-reciprocal neighbors,0
More elegantly one can simply define:,0
take care of difference between feature map space and original image space,0
cos = torch.cdist,0
TODO: maybe find a better spot for this.,1
"dev =  device if torch.cuda.is_available() else ""cpu""",0
train,0
print (f'validation loss {val_loss/len(val_loader)}'),0
normalize vectors here,0
i th vector at j th kpt,0
maybe needs to convert them to embeddings and position token,0
Drop missing body parts,0
Drop points lying outside the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
## dlcnet_ms5: backbone resnet50 + multi-fusion & multi-stage module,0
## dlcr101_ms5/dlcr152_ms5: backbone resnet101/152 + multi-fusion & multi-stage module,0
"If the graph is unnecessarily large (with 15+ keypoints by default),",0
we randomly prune it to a size guaranteeing an average node degree of 6;,0
"see Suppl. Fig S9c in Lauer et al., 2022.",0
Use the skeleton defined in the config file,0
Ignore possible connections between 'multi' and 'unique' body parts;,0
one can never be too careful...,0
Disable the prediction of PAFs if the graph is empty,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving metadata and data file (Pickle file),0
###############################################################################,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Setting inference cfg file:,0
Search a pose_config.yaml file to parse missing information,0
Clean the training-datasets folder prior to recreating the data pickles,0
Load updated lists:,0
Find the corresponding video file,0
"print(key, ""dropping?"")",0
adding important values for multianiaml project:,0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Determine the index length required to guarantee,0
the train–test ratio is exactly the desired one.,0
Pad indices so lengths agree,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(name,test_video_name)",0
Blazing fast and does not load the image into memory,0
Check that points lie within the image,0
DeprecationWarnings are silenced since Python 3.2 unless triggered in __main__,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
updating variable if null/None! #backwardscompatability,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
these values are dropped as scalecrop,0
doesn't have rotation implemented,0
Also drop maDLC smart cropping augmentation parameters,0
sort the model directories,0
get the shuffle index and offset by 1.,0
read cfg file,0
create log file,0
Check for variable correctness,0
Moviepy:,0
select crop method,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
allow_growth must be true here because tensorflow does not automatically free gpu memory and setting it as false occupies all gpu memory so that pytorch cannot kick in,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
should close tensorflow session here in order to free gpu,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html#videocapture-get,0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
TODO: add cropping as in video analysis!,1
if cropping is not None:,0
cfg['cropping']=True,0
"cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping",0
"print(""Overwriting cropping parameters:"", cropping)",0
"print(""These are used for all videos, but won't be save to the cfg file."")",0
"Also ensure `boundingboxslack` is greater than zero, otherwise overlap",0
"between trackers cannot be evaluated, resulting in empty tracklets.",0
Check which snapshots are available and sort them by # iterations,0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
NOTE: If dataname line above is changed then line below is obsolete?,0
"trackname = trackname.replace(videofolder, destfolder)",0
TODO: adjust this for multi + unique bodyparts!,1
this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. ),0
Optimal identity assignment based on soft voting,0
imagenet mean for resnet pretraining:,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/DeepLabCut/DeepLabCut/pull/324",0
for DLC 2.2. (here all set False to not use PAFs/pairwise fields),0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
"reloading defaults, as they can bleed over from a previous run otherwise",0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
dlc_cfg_train = load_config(str(path_train_config)),0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
Name for deeplabcut net (based on its parameters),0
"DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])",0
"print(""Extracting maps for "", DLCscorer, "" with # of trainingiterations:"", trainingsiterations)",0
if notanalyzed: #this only applies to ask if h5 exists...,0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
"for multi animal, seems only this is used",0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Get animal IDs and corresponding indices in the arrays of detections,0
Form ground truth beforehand,0
Assemble animals on the full set of detections,0
Count the number of unassembled bodyparts,0
Handle unlabeled bodyparts...,0
"Only 1 animal, let us return the full graph indices only",0
Find minimal skeleton,0
Select optimal PAF graph,0
Selective copy; deepcopy is >5x slower,0
Only keeps skeletons that are more than 90% complete,0
TODO Normalize dists by longest length?,1
TODO Smarter imputation technique (Bayesian? Grassmann averages?),1
Covariance matrix estimation fails due to numerical singularities,0
Distance is undefined if the assembly is empty,0
Correct distance to account for missing observations,0
"Alternatively, reduce contribution of missing values to the Mahalanobis",0
distance to zero by substituting the corresponding means.,0
"Fill the subsets with unambiguous, complete individuals",0
Fuse superfluous assemblies,0
Second pass without edge safety,0
Mark the unique body parts as assembled anyway so,0
they are not used later on to fill assemblies.,0
Store selected edges for subsequent frames,0
Remove invalid assemblies,0
Fill assemblies with unconnected body parts,0
Spawning (rather than forking) multiple processes does not,0
work nicely with the GUI or interactive sessions.,0
"In that case, we fall back to the serial assembly.",0
Swap corresponding keypoints,0
Only consider assemblies of at least two keypoints,0
Greedy assembly matching like in pycocotools,0
Global rather than greedy assembly matching,0
Cast columns of dtype 'object' to float to avoid TypeError,0
further down in _parse_ground_truth_data.,0
sigma is taken as the median of all COCO keypoint standard deviations,0
Sort matching score (OKS) in descending order of assembly affinity,0
Guarantee precision decreases monotonically,0
See https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173),0
Regularize by forcing AR <= 5,0
max_ar = 5,0
if el.aspect_ratio >= max_ar:,0
if el.height > el.width:,0
el.width = el.height / max_ar,0
else:,0
el.height = el.width / max_ar,0
Orient the ellipse such that it encompasses most points,0
"n_inside = el.contains_points(np.c_[self.x, self.y]).sum()",0
el.theta += 0.5 * np.pi,0
"if el.contains_points(np.c_[self.x, self.y]).sum() < n_inside:",0
el.theta -= 0.5 * np.pi,0
"r2 = chi2.ppf(2 * norm.cdf(sd) - 1, 2)",0
"height, width = np.sqrt(E * r2)",0
The general quadratic curve has the form:,0
ax^2 + 2bxy + cy^2 + 2dx + 2fy + g = 0,0
Ellipse center coordinates,0
Semi-axes lengths,0
Angle to the horizontal,0
High uncertainty to the unobservable initial velocities,0
Give high uncertainty to the unobservable initial velocities,0
diff = val - cost_matrix,0
"diff[row, col] += val",0
if (,0
val < self.iou_threshold,0
or np.any(diff[row] <= 0.2),0
"or np.any(diff[:, col] <= 0.2)",0
):,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
Modified from scipy source code:,0
- to restrict its use to 2D,0
- to get rid of shuffling (since arrays are only (nbodyparts * 3) element long),0
TODO - factor in keypoint confidence (and weight by # of observations??),1
"mat = self.calc_pairwise_oks(poses, poses_ref)",0
Remove matched detections with low OKS,0
matches = [],0
"for row, col in zip(row_indices, col_indices):",0
"if mat[row, col] < self.oks_threshold:",0
unmatched_poses.append(row),0
unmatched_trackers.append(col),0
else:,0
"matches.append([row, col])",0
if not len(matches):,0
"matches = np.empty((0, 2), dtype=int)",0
else:,0
matches = np.stack(matches),0
update matched trackers with assigned detections,0
create and initialise new trackers for unmatched detections,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
filter out matched with low IOU,0
TODO: get cropping parameters and utilize!,1
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Load meta data & annotations,0
get the graph!,0
"NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*",0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
TODO: Make this code not so redundant!,1
"If a string was passed in, auto-convert to True for backward compatibility",0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to initial folder,0
"# Also storing one ""large"" table with results:",0
note: evaluationfolder.parents[0] to get common folder above all shuffle evaluations.,0
#################################################,0
Load data...,0
#################################################,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
TODO: IMPLEMENT for different batch sizes?,1
Ignore best edges possibly defined during a prior evaluation,0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
Pass the image and the keypoints through the resizer;,0
this has no effect if no augmenters were added to it.,0
FIXME Is having an empty array vs nan really that necessary?!,1
"Form 2D array of shape (n_rows, 4) where the last dimension",0
"is (sample_index, peak_y, peak_x, bpt_index) to slice the PAFs.",0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in,0
Compute all distance statistics,0
Calculate overall prediction error,0
Skip data-driven skeleton selection unless,0
the model was trained on the full graph.,0
"loading backbone from ResNet, MobileNet etc.",0
"display_iters = max(1,int(displayiters))",0
"[_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})",0
Save snapshot,0
return to original path.,0
Clip peak locations to PAFs dimensions,0
"unit_vecs = vecs / lengths[:, np.newaxis]",0
"affinities = np.squeeze(y @ np.expand_dims(unit_vecs, axis=2)).sum(axis=1)",0
Form cost matrices,0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
Convert a frozen graph to OpenVINO IR format,0
Read network into memory,0
"For better efficiency, model is initialized for batch_size 1 and every sample processed independently",0
Load network to device,0
Prepare input data,0
Change the stride from 2 to 1 to get 16x downscaling instead of 32x.,0
https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142,0
Multi stage is currently only implemented for resnets,0
Defining multi_fusion backbone,0
Attaching multi-stage decoder,0
stage_paf_output = stage_paf_output + pre_stage_paf_output,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
FIXME Fix wrong scope with Keras layers,1
"def prediction_layer(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2DTranspose(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=2,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
## New DLCNet Addition: multi-stage decoder,0
"def prediction_layer_stage(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2D(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=1,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Compute variance using: Var[X]= E[X^2] - E[X]^2.,0
Compute keep_prob,0
TODO(tanmingxing): add support for training progress.,1
Compute drop_connect tensor,0
turn into x times y time bs * bpts,0
extract corresponding locref x and y as well as probability,0
Filter predicted heatmaps with a 2D Gaussian kernel as in:,0
https://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_The_Devil_Is_in_the_Details_Delving_Into_Unbiased_Data_CVPR_2020_paper.pdf,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
defaults will be a public argument for namedtuple in Python 3.7,0
https://docs.python.org/3/library/collections.html#collections.namedtuple,0
Make sure that round down does not go down by more than 10%.,0
Builds the block accordings to arguments.,0
Expansion phase:,0
Depth-wise convolution phase:,0
Squeeze and Excitation layer.,0
Output phase:,0
only apply drop_connect if skip presents.,0
Expansion phase:,0
Output phase:,0
only apply drop_connect if skip presents.,0
Builds blocks.,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Stem part.,0
Head part.,0
Calls Stem layers,0
Calls blocks.,0
Calls final layers and returns logits.,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"(width_coefficient, depth_coefficient, resolution, dropout_rate)",0
blocks_args = [,0
"'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',",0
"'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',",0
"'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',",0
"'r1_k3_s11_e6_i192_o320_se0.25',",0
],0
The default is TPU-specific batch norm.,0
The alternative is tf.layers.BatchNormalization.,0
"batch_norm=utils.TpuBatchNormalization,  # TPU-specific requirement.",0
ValueError will be raised here if override_params has fields not included,0
in global_params.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrunk when given a depth-multiplier < 1.0.,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
Clamp to 40% of crop size to ensure that at least,0
the center keypoint remains visible after the offset is applied.,0
Points located close to one another are sampled preferentially,0
in order to augment crowded regions.,0
Include keypoints in the count to avoid null probabilities,0
Shift the crop center in both dimensions by random amounts,0
and normalize to the original image dimensions.,0
Pickle the 'data' dictionary using the highest protocol available.,0
item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal,0
"Add smart, keypoint-aware image cropping",0
Scale is sampled only once (per batch) to transform all of the images into same size.,0
Discard keypoints whose coordinates lie outside the cropped image,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
Produce score maps and location refinement fields,0
Find indices of individuals in joint_id,0
WIP!,0
STD of gaussian is 1/4 of threshold,0
Grid of coordinates,0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Distance between the joint point and each coordinate,0
NEEDS TO BE DONE!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
TODO Finish implementing actual abstract class,1
parameter initialization for augmentation pipeline:,0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images of a batch into same size.,0
Approximating the scale,0
invisible joints are represented by nans,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"First, initializing variables (if they don't exist)",0
what is the fraction of training samples with scaling augmentation?,0
loading defaults for rotation range!,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"cfg[""rotateratio""] = cfg.get(",0
"""rotratio"", 0.4",0
)  # what is the fraction of training samples with rotation augmentation?,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"Auto cropping is new (was not in Nature Neuroscience 2018 paper, but introduced in Nath et al. Nat. Protocols 2019)",0
"and boosts performance by 2X, particularly on challenging datasets, like the cheetah in Nath et al.",0
Parameters for augmentation with regard to cropping:,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
Get rid of the interpolation beyond the spline knots,0
Retrieve original individual label indices,0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Check the training image paths are correctly stored as arrays of strings,0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
TENSORPACK could fail in WINDOWS...,0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
# Create one identical splits for 3 networks and 3 augmentations,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
Setting specific parameters for training,0
"MODELS = [""dlcrnet_ms5"", ""dlcr101_ms5"", ""efficientnet-b0"", ""mobilenet_v2_0.35""]",0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
## adding it here,0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Downloading the calibration images,0
Deleting unnecessary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
example how to set pose config variables:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
test if existing models are found:,0
Test if nonexisting models are not found,0
Add some other office files:,0
Add a .pickle and .h5 files,0
"By default, all videos with common extensions are taken from a directory",0
A list of extensions can also be passed in,0
Test bounding box from a single keypoint,0
Test that shape was preserved after vectorization,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Guarantee that images smaller than crop size are handled fine,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Disable stochastic scale jitter,0
Verify the aspect ratio is preserved,0
Load sample multi animal data,0
Generate a random permutation and reorder data. Ignore the unique bodypart,0
Get inverse permutation and reorder the modified data to get back,0
to the original,0
Check,0
"Check data comprise path, shape, and xy coordinates",0
Break the graph to test stitching failure,0
Add fake IDs,0
Split all tracklets in half,0
Test flipped keypoints,0
Original (cached) coordinates must have remained empty,0
Assemble based on the smallest graph to speed up testing,0
Generate fake identity predictions,0
Assemble based on the smallest graph to speed up testing,0
Test now with identity only and ensure assemblies,0
contain only parts of a single group ID.,0
Test empty assembly,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
def install(package):,0
"subprocess.check_call([sys.executable, ""-m"", ""pip"", ""install"", package])",0
"install(""tensorflow==1.13.1"")",0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
quicker variant,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
Suppress tensorflow warning messages,0
"Train, evaluate & predict functions / all require TF",0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Updating config file:,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path, num_shuffles=auxfun_models.Check4weights(pose_cfg['net_type'], parent_path, num_shuffles= 1)",0
Updating training and test pose_cfg:,0
Read the config file,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
common parameters:,0
Write dictionary to yaml  config file,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Flatten the list of bodyparts to connect,0
Format data,0
Trick to force equal aspect ratio of 3D plots,0
Set up the matplotlib figure beforehand,0
Set up skeleton LineCollections,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
Get track_method and do related checks,0
Get track method suffix,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
"## Assign nan to [X,Y] of low likelihood predictions ###",0
Convert the data to a np array to easily mask out the low likelihood predictions,0
"Assign [X,Y] = nan to low likelihood predictions",0
Reshape data back to original shape,0
put data back to the dataframes,0
Check individuals are the same in both views,0
Cross-view match individuals,0
Create a dummy variables for single-animal,0
Cleaner variable (since inds view1 == inds view2),0
"Reshape: (num_framex, num_individuals, num_bodyparts , 2)",0
Triangulate data,0
i is individual in view 1,0
voting[i] is the matched individual in view 2,0
Create 3D DataFrame column and row indices,0
Swap num_animals with num_frames axes to ensure well-behaving reshape,0
Fill up 3D dataframe,0
Reorder 2D dataframe in view 2 to match order of view 1,0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Alternatively,0
Sort tracklets by length to prioritize greater continuity,0
Drop tracklets that are too short,0
"Where slots are available, copy the data over",0
"If about to overwrite data, keep tracklets with highest confidence",0
Squeeze some data into empty slots,0
"For the remaining data, overwrite where we are least confident",0
Find keypoints closest to the remaining data,0
Use Manhattan distance to avoid overflow,0
Only overwrite if improving confidence,0
Fill existing gaps,0
Retrieve original individual label indices,0
Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.,0
Fill existing gaps,0
Retrieve original individual label indices,0
ID swaps occur when X and Y simultaneously intersect each other.,0
Get only those bodypart pairs that belong to different individuals,0
"See Li et al., 2012. Cross-view Activity Recognition using Hankelets.",0
"As proposed in the paper, the Hankel matrix can either be formed from",0
the tracklet's centroid or its normalized velocity.,0
"vel = np.diff(self.centroid, axis=0)",0
"vel /= np.linalg.norm(vel, axis=1, keepdims=True)",0
return self.hankelize(vel),0
TODO Add missing data imputation,1
"nrows, ncols = mat.shape",0
beta = nrows / ncols,0
omega = 0.56 * beta ** 3 - 0.95 * beta ** 2 + 1.82 * beta + 1.43,0
return np.argmin(s > omega * np.median(s)),0
"Note that if tracklets are very short, some may actually be part of the same track",0
and thus incorrectly reflect separate track endpoints...,0
"Map each Tracklet to an entry and output nodes and vice versa,",0
which is convenient once the tracklets are stitched.,0
Store tracklets and corresponding negatives (those that overlap in time),0
TODO Avoid looping over all pairs of tracklets,0
Pick the closest (spatially) overlapping tracklet,0
The algorithm works better with integer weights,0
Let us prune the graph by removing all source and sink edges,0
but those connecting the `n_tracks` first and last tracklets.,0
Preflow push seems to work slightly better than shortest,0
"augmentation path..., and is more computationally efficient.",0
Verify whether there are overlapping tracklets,0
"Pick the segment that minimizes ""smoothness"", computed here",0
with the coefficient of variation of the differences.,0
Rebuild a full graph from the remaining nodes without,0
temporal constraint on what tracklets can be stitched together.,0
Cycle through the residuals and incorporate back those,0
that only fit in a single tracklet.,0
Greedily add the remaining residuals,0
Refresh temporal bounds,0
Default to the distance cost function,0
should only exist one,0
Add in identity weighing before building the graph,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implementation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
average distance and average # significant differences avg. over comparisonbodyparts,0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
###################################################,0
Dependencies,0
###################################################,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as,0
specified in the config file,0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
Prepare figure,0
Mask rather than drop unwanted bodyparts to ensure consistent coloring,0
cropping!,0
nx=X2-X1,0
!/usr/bin/env python3,0
"more videos are in principle covered, as OpenCV is used and allows many formats.",0
The above already raises a ValueError if formatting is wrong,0
"Rotate, see: https://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg",0
interesting option to just update metadata.,0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
Read the video until a frame is successfully read,0
import platform,0
"if platform.system() == ""Darwin"":  # for OSX use WXAgg",0
fig.canvas.start_event_loop(timeout=-1),0
else:,0
#fig.canvas.stop_event_loop(),0
"Windows throws error if file path is > 260 characters, can fix with prefix.",0
See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation,0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
This dictionary maps the model types to the file locations where the models exist.,0
Exit the function early if an unknown modeltype is provided.,0
Aliases for backwards-compatibility,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
Initialize costs matrix,0
get common bodypart detections in track pair,0
add 3rd dim to the points,0
cost for any point in time of t1 being the same,0
any point in time of t2,0
Get average cost of the entire track,0
check if it exists:,0
(average_degree / 2) as many edges as there are nodes is required,0
for undirected graphs to reach the target degree.,0
"Attention this order has to be consistent (for training set creation, training, inference etc.)",0
"-> adding (single,bpt) for uniquebodyparts",0
"-> adding (individual,bpt) for multianimalbodyparts",0
load defaults,0
set project specific parameters:,0
###################################################,0
Dependencies,0
###################################################,0
Pose X vs pose Y,0
Poses vs time,0
Likelihoods,0
Histograms,0
#################################################,0
Looping analysis over video,0
#################################################,0
Keep only the individuals and bodyparts that were labeled,0
Either display the animals defined in the config if they are found,0
"in the dataframe, or all the trajectories regardless of their names",0
Determine whether the data are single- or multi-animal without loading into memory,0
simply by checking whether 'individuals' is in the second line of the CSV.,0
Make paths platform-agnostic if they are not already,0
Ensure folder names are strings,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
make list of full paths,0
filter list of videos,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"if ""MULTI!"" in allbpts:",0
Ensure same order as in config.yaml,0
ABBREVIATE NETWORK NAMES -- esp. for mobilenet!,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
Iterate over data files and stop as soon as one matching the scorer is found,0
If there was no match...,0
aliases for backwards-compatibility.,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"trainingset = wx.StaticBox(self, label=""Specify the trainingset index"")",0
"trainingset_boxsizer = wx.StaticBoxSizer(trainingset, wx.VERTICAL)",0
"self.trainingset = wx.SpinCtrl(self, value=""0"", min=0, max=100)",0
trainingset_boxsizer.Add(,0
"self.trainingset, 1, wx.EXPAND | wx.TOP | wx.BOTTOM, 10",0
),0
"hbox_.Add(trainingset_boxsizer, 5, wx.EXPAND | wx.TOP | wx.BOTTOM, 5)",0
Read the infer config file,0
trainingsetindex = self.trainingset.GetValue(),0
let the user open the file with default text editor. Also make it mac compatible,0
trainingsetindex = self.trainingset.GetValue(),0
"trainingsetindex=trainingsetindex,",0
"trainingsetindex=self.trainingset.GetValue(),",0
self.save.Enable(False),0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
topSplitter = wx.SplitterWindow(self),0
,0
"self.image_panel = ImagePanel(topSplitter, config,video,shuffle,Dataframe,self.gui_size)",0
self.widget_panel = WidgetPanel(topSplitter),0
,0
"topSplitter.SplitHorizontally(self.image_panel, self.widget_panel,sashPosition=self.gui_size[1]*0.83)#0.9",0
topSplitter.SetSashGravity(1),0
sizer = wx.BoxSizer(wx.VERTICAL),0
"sizer.Add(topSplitter, 1, wx.EXPAND)",0
self.SetSizer(sizer),0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
Read config file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
variable initialization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0,10), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=10)",0
Add all the options,0
line = wx.StaticLine(self),0
"self.sizer.Add(line, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=10)",0
,0
"self.multi_choice.Bind(wx.EVT_CHECKBOX,self.activate_copy_videos)",0
Hide the button as this is not the default option,0
self.vids.Enable(False),0
self.SetSizer(self.sizer),0
"self.sizer.Add(self.sizer, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=15)",0
self.ok.Enable(False),0
else:,0
self.ok.Enable(True),0
For mac compatibility,0
Remove the pages in case the user goes back to the create new project and creates/load a new project,0
Add all the other pages,0
"page10 = Refine_labels(self.parent, self.gui_size, self.cfg, page5)",0
"self.parent.AddPage(page10, ""OPT: Refine Labels"")",0
self.yes.Enable(False),0
self.no.Enable(False),0
Here we create a panel and a notebook on the panel,0
create the page windows as children of the notebook and add the pages to the notebook with the label to show on the tab,0
Take into consideration imprecise OpenCV estimation of total number of frames,0
Automatically disable the draggable points,0
Save additional frames to the labeled-data directory,0
Store the newly-refined data,0
Uncertain keypoints are ignored,0
Merge with the already existing annotated data,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.buttonCounter = [],0
self.bodyparts = [],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
load labeledPoints and fundamental Matrix,0
Find offset terms for drawing epipolar Lines,0
Get crop params for camera being labeled,0
Get crop params for other camera,0
convert the image to RGB as you are showing the image with matplotlib,0
draw epipolar lines,0
##################################################################################################################################################,0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
variable initialization,0
design the panel,0
Add logo of DLC,0
self.filelist = self.filelist + self.vids,0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
variable initialization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initialization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
## train and go ###,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
load labeledPoints and fundamental Matrix,0
Find offset terms for drawing epipolar Lines,0
Get crop params for camera being labeled,0
Get crop params for other camera,0
convert the image to RGB as you are showing the image with matplotlib,0
draw epipolar lines,0
divider = make_axes_locatable(self.axes),0
"colorIndex = np.linspace(np.min(im),np.max(im),len(bodyparts))",0
"cax = divider.append_axes(""right"", size=""5%"", pad=0.05)",0
"cbar = self.figure.colorbar(ax, cax=cax,spacing='proportional', ticks=colorIndex)",0
cbar.set_ticklabels(bodyparts[::-1]),0
self.figure.canvas.draw(),0
##################################################################################################################################################,0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"MainFrame.saveDataSet(self, event)",0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
checks for unique bodyparts,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Create an empty data frame,0
Cache original bodyparts,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Rearrange bodypart columns in config order,0
Test whether there are missing frames and superfluous data,0
Check whether new individuals or body parts were added,0
Checking if user added a new label,0
Found new labels in either multiple bodyparts or unique bodyparts,0
check if single is selected when radio buttons are changed,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Discard data associated with bodyparts that are no longer in the config,0
Re-organize the dataframe so the CSV looks consistent with the config,0
restore the background region,0
variable initialization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.build = wx.Button(self, label=""Build skeleton"")",0
"sizer.Add(self.build, pos=(4, 3), flag=wx.BOTTOM | wx.RIGHT, border=10)",0
"self.build.Bind(wx.EVT_BUTTON, self.build_skeleton)",0
self.build.Enable(True),0
"def build_skeleton(self, event):",0
skeleton.SkeletonBuilder(self.config),0
Setting the GUI size and panels design,0
This sets the minimum size of the GUI. It can scale now!,0
Checks if zoom/pan button is ON,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
variable initialization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"trainingindex = wx.StaticBox(self, label=""Specify the trainingset index"")",0
"trainingindex_boxsizer = wx.StaticBoxSizer(trainingindex, wx.VERTICAL)",0
"self.trainingindex = wx.SpinCtrl(self, value=""0"", min=0, max=100)",0
trainingindex_boxsizer.Add(,0
"self.trainingindex, 1, wx.EXPAND | wx.TOP | wx.BOTTOM, 10)",0
use the default pose_cfg file for default values,0
self.display_iters.Enable(False),0
self.save_iters.Enable(False),0
self.max_iters.Enable(False),0
self.snapshots.Enable(False),0
"hbox1.Add(trainingindex_boxsizer, 10, wx.EXPAND | wx.TOP | wx.BOTTOM, 5)",0
self.trainingindex.Enable(False),0
self.trainingindex.Enable(True),0
self.trainingindex.Enable(True),0
Read the pose config file,0
print(trainFraction[-1]),0
"print(os.path.join(cfg['project_path'],auxiliaryfunctions.get_model_folder(trainFraction, self.shuffles.GetValue(),cfg),'train','pose_cfg.yaml'))",0
let the user open the file with default text editor. Also make it mac compatible,0
update the variables with the edited values in the pose config file,0
self.trainingindex.Enable(True),0
variable initialization,0
design the panel,0
Add logo of DLC,0
### BOX 1 ####,0
### BOX 2 ####,0
### BOX 3 ####,0
Trigger chooseOption as if the radio button had been clicked,0
Read the infer config file,0
"trainFraction = cfg[""TrainingFraction""][trainingsetindex]",0
let the user open the file with default text editor. Also make it mac compatible,0
"self.select_destfolder.SetPath(""None"")",0
self.config = [],0
"self.sel_config.SetPath("""")",0
variable initialization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"trainingindex_box = wx.StaticBox(self, label=""Specify the trainingset index"")",0
"trainingindex_boxsizer = wx.StaticBoxSizer(trainingindex_box, wx.VERTICAL)",0
"self.trainingindex = wx.SpinCtrl(self, value=""0"", min=0, max=100)",0
trainingindex_boxsizer.Add(,0
"self.trainingindex, 0, wx.EXPAND | wx.TOP | wx.BOTTOM, 10",0
),0
"self.hbox2.Add(trainingindex_boxsizer, 10, wx.EXPAND | wx.TOP | wx.BOTTOM, 5)",0
trainindex = self.trainingindex.GetValue(),0
"trainindex=trainindex,",0
"self.shuffles.SetValue(""1"")",0
"DownSampleVideo(vname,width=-1,height=200,outsuffix='downsampled',outpath=None,rotatecw=False):",0
variable initialization,0
design the panel,0
Add logo of DLC,0
"majorDimension=0,",0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
"self.choiceBox,self.visualization_rdb = self.choice_panel.addRadioButtons()",0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
Overwrite the config-defined individual names,0
with those actually present in the annotated data,0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
print(self.Dataframe.head()),0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
fix move to corner,0
fix move to corner,0
fix move to corner,0
fix move to corner,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
variable initialization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"trainingset = wx.StaticBox(self, label=""Specify the trainingset index"")",0
"trainingset_boxsizer = wx.StaticBoxSizer(trainingset, wx.VERTICAL)",0
"self.trainingset = wx.SpinCtrl(self, value=""0"", min=0, max=100)",0
trainingset_boxsizer.Add(,0
"self.trainingset, 1, wx.EXPAND | wx.TOP | wx.BOTTOM, 10",0
),0
"self.hbox1.Add(trainingset_boxsizer, 5, wx.EXPAND | wx.TOP | wx.BOTTOM, 5)",0
"if self.plot_scoremaps.GetStringSelection() == ""Yes"":",0
Read the infer config file,0
trainingsetindex = self.trainingset.GetValue(),0
let the user open the file with default text editor. Also make it mac compatible,0
shuffle = self.shuffle.GetValue(),0
trainingsetindex = self.trainingset.GetValue(),0
"trainingsetindex=trainingsetindex,",0
variable initialization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
## LABELING ###,0
"self.sel_config.SetPath("""")",0
##################################################################################################################################################,0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
self.cfg = auxiliaryfunctions.read_config(config),0
"self.statusbar.SetStatusText("""")",0
"dlg = wx.MessageDialog(None,""Are you sure?"", ""Quit!"",wx.YES_NO | wx.ICON_WARNING)",0
result = dlg.ShowModal(),0
if result == wx.ID_YES:,0
variable initialization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0, 4), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=5)",0
variable initialization,0
design the panel,0
Add logo of DLC,0
#         design the panel,0
Add image of DLC,0
if editing this text make sure you add the '\n' to get the new line. The sizer is unable to format lines correctly.,0
"calling create_tracking_dataset, train_tracking_transformer, stitch_tracklets",0
only try to find these features if they are in the dictionary,0
1) reference to video folder and get the proper bpt_feature file for feature table,0
2) get either the path to gt or the path to track pickle,0
tracklets,0
"with npy list form videos, split each to train and test",0
"video_name = '.'.join(video.split(""/"")[-1].split(""."")[:-1])",0
assuming there is only one match,0
make my own model factory,0
make my own loss factory,0
"NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights",0
"NOTE: drop path for stochastic depth, we shall see if this is better than dropout here",0
only channel pooling,0
"x -> (B, 12, 2048)",0
"x -> (B, 12, 768)",0
trying to keep everything simple,0
need to write my own keypoint embedding,0
we don't need a fc layer here,0
x: inputs,0
"(B, 12, 768)",0
added pos embed,0
remove all parts I don't understand,0
forwarding through blocks. But num of blocks should be reduced,0
i can't really tell what the difference is,0
will have to change some of these,0
For old models that I trained prior to conv based patchification,0
To resize pos embedding when using model at different size from pretrained weights,0
Rescale the grid of position embeddings when loading from state_dict. Adapted from,0
https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224,0
Cut & paste from PyTorch official master until it's in a few official releases - RW,0
Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf,0
Computes standard normal cumulative distribution function,0
Values are generated by using a truncated uniform distribution and,0
then using the inverse CDF for the normal distribution.,0
Get upper and lower cdf values,0
"Uniformly fill tensor with values from [l, u], then translate to",0
"[2l-1, 2u-1].",0
Use inverse cdf transform for normal distribution to get truncated,0
standard normal,0
"Transform to proper mean, std",0
Clamp to ensure it's in the proper range,0
"type: (Tensor, float, float, float, float) -> Tensor",0
"resample if noise out of percent limit, brute force but shouldn't spin much",0
distmat g,0
q    1 3 2 4,0
4 1 2 3,0
0 2 1 3,0
1 2 3 0,0
compute cmc curve for each query,0
get query pid and camid,0
remove gallery samples that have the same pid and camid with query,0
compute cmc curve,0
"binary vector, positions with value 1 are correct matches",0
this condition is true when query identity does not appear in gallery,0
compute average precision,0
reference: https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Average_precision,0
"tmp_cmc = [x / (i + 1.) for i, x in enumerate(tmp_cmc)]",0
query,0
gallery,0
"distmat = re_ranking(qf, gf, k1=20, k2=6, lambda_value=0.3)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
"if feature vector is numpy, you should use 'torch.tensor' transform it to tensor",0
print('using GPU to compute original distance'),0
print('starting re_ranking'),0
k-reciprocal neighbors,0
More elegantly one can simply define:,0
take care of difference between feature map space and original image space,0
cos = torch.cdist,0
TODO: maybe find a better spot for this.,1
"dev =  device if torch.cuda.is_available() else ""cpu""",0
train,0
print (f'validation loss {val_loss/len(val_loader)}'),0
normalize vectors here,0
i th vector at j th kpt,0
maybe needs to convert them to embeddings and position token,0
Drop missing body parts,0
Drop points lying outside the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
## dlcnet_ms5: backbone resnet50 + multi-fusion & multi-stage module,0
## dlcr101_ms5/dlcr152_ms5: backbone resnet101/152 + multi-fusion & multi-stage module,0
"If the graph is unnecessarily large (with 15+ keypoints by default),",0
we randomly prune it to a size guaranteeing an average node degree of 6;,0
"see Suppl. Fig S9c in Lauer et al., 2022.",0
Use the skeleton defined in the config file,0
Ignore possible connections between 'multi' and 'unique' body parts;,0
one can never be too careful...,0
Disable the prediction of PAFs if the graph is empty,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving metadata and data file (Pickle file),0
###############################################################################,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Setting inference cfg file:,0
Search a pose_config.yaml file to parse missing information,0
Clean the training-datasets folder prior to recreating the data pickles,0
Load updated lists:,0
Find the corresponding video file,0
"print(key, ""dropping?"")",0
adding important values for multianiaml project:,0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Determine the index length required to guarantee,0
the train–test ratio is exactly the desired one.,0
Pad indices so lengths agree,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(name,test_video_name)",0
Blazing fast and does not load the image into memory,0
Check that points lie within the image,0
DeprecationWarnings are silenced since Python 3.2 unless triggered in __main__,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
updating variable if null/None! #backwardscompatability,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
these values are dropped as scalecrop,0
doesn't have rotation implemented,0
Also drop maDLC smart cropping augmentation parameters,0
sort the model directories,0
get the shuffle index and offset by 1.,0
read cfg file,0
create log file,0
Check for variable correctness,0
Moviepy:,0
select crop method,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
allow_growth must be true here because tensorflow does not automatically free gpu memory and setting it as false occupies all gpu memory so that pytorch cannot kick in,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
should close tensorflow session here in order to free gpu,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html#videocapture-get,0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
TODO: add cropping as in video analysis!,1
if cropping is not None:,0
cfg['cropping']=True,0
"cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping",0
"print(""Overwriting cropping parameters:"", cropping)",0
"print(""These are used for all videos, but won't be save to the cfg file."")",0
"Also ensure `boundingboxslack` is greater than zero, otherwise overlap",0
"between trackers cannot be evaluated, resulting in empty tracklets.",0
Check which snapshots are available and sort them by # iterations,0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
NOTE: If dataname line above is changed then line below is obsolete?,0
"trackname = trackname.replace(videofolder, destfolder)",0
TODO: adjust this for multi + unique bodyparts!,1
this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. ),0
Optimal identity assignment based on soft voting,0
imagenet mean for resnet pretraining:,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/DeepLabCut/DeepLabCut/pull/324",0
for DLC 2.2. (here all set False to not use PAFs/pairwise fields),0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
"reloading defaults, as they can bleed over from a previous run otherwise",0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
dlc_cfg_train = load_config(str(path_train_config)),0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
Name for deeplabcut net (based on its parameters),0
"DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])",0
"print(""Extracting maps for "", DLCscorer, "" with # of trainingiterations:"", trainingsiterations)",0
if notanalyzed: #this only applies to ask if h5 exists...,0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
"for multi animal, seems only this is used",0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Get animal IDs and corresponding indices in the arrays of detections,0
Form ground truth beforehand,0
Assemble animals on the full set of detections,0
Count the number of unassembled bodyparts,0
Handle unlabeled bodyparts...,0
"Only 1 animal, let us return the full graph indices only",0
Find minimal skeleton,0
Select optimal PAF graph,0
Selective copy; deepcopy is >5x slower,0
Only keeps skeletons that are more than 90% complete,0
TODO Normalize dists by longest length?,1
TODO Smarter imputation technique (Bayesian? Grassmann averages?),1
Covariance matrix estimation fails due to numerical singularities,0
Distance is undefined if the assembly is empty,0
Correct distance to account for missing observations,0
"Alternatively, reduce contribution of missing values to the Mahalanobis",0
distance to zero by substituting the corresponding means.,0
"Fill the subsets with unambiguous, complete individuals",0
Fuse superfluous assemblies,0
Second pass without edge safety,0
Mark the unique body parts as assembled anyway so,0
they are not used later on to fill assemblies.,0
Store selected edges for subsequent frames,0
Remove invalid assemblies,0
Fill assemblies with unconnected body parts,0
Spawning (rather than forking) multiple processes does not,0
work nicely with the GUI or interactive sessions.,0
"In that case, we fall back to the serial assembly.",0
Swap corresponding keypoints,0
Only consider assemblies of at least two keypoints,0
Greedy assembly matching like in pycocotools,0
Global rather than greedy assembly matching,0
Cast columns of dtype 'object' to float to avoid TypeError,0
further down in _parse_ground_truth_data.,0
sigma is taken as the median of all COCO keypoint standard deviations,0
Sort matching score (OKS) in descending order of assembly affinity,0
Guarantee precision decreases monotonically,0
See https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173),0
Regularize by forcing AR <= 5,0
max_ar = 5,0
if el.aspect_ratio >= max_ar:,0
if el.height > el.width:,0
el.width = el.height / max_ar,0
else:,0
el.height = el.width / max_ar,0
Orient the ellipse such that it encompasses most points,0
"n_inside = el.contains_points(np.c_[self.x, self.y]).sum()",0
el.theta += 0.5 * np.pi,0
"if el.contains_points(np.c_[self.x, self.y]).sum() < n_inside:",0
el.theta -= 0.5 * np.pi,0
"r2 = chi2.ppf(2 * norm.cdf(sd) - 1, 2)",0
"height, width = np.sqrt(E * r2)",0
The general quadratic curve has the form:,0
ax^2 + 2bxy + cy^2 + 2dx + 2fy + g = 0,0
Ellipse center coordinates,0
Semi-axes lengths,0
Angle to the horizontal,0
High uncertainty to the unobservable initial velocities,0
Give high uncertainty to the unobservable initial velocities,0
diff = val - cost_matrix,0
"diff[row, col] += val",0
if (,0
val < self.iou_threshold,0
or np.any(diff[row] <= 0.2),0
"or np.any(diff[:, col] <= 0.2)",0
):,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
Modified from scipy source code:,0
- to restrict its use to 2D,0
- to get rid of shuffling (since arrays are only (nbodyparts * 3) element long),0
TODO - factor in keypoint confidence (and weight by # of observations??),1
"mat = self.calc_pairwise_oks(poses, poses_ref)",0
Remove matched detections with low OKS,0
matches = [],0
"for row, col in zip(row_indices, col_indices):",0
"if mat[row, col] < self.oks_threshold:",0
unmatched_poses.append(row),0
unmatched_trackers.append(col),0
else:,0
"matches.append([row, col])",0
if not len(matches):,0
"matches = np.empty((0, 2), dtype=int)",0
else:,0
matches = np.stack(matches),0
update matched trackers with assigned detections,0
create and initialise new trackers for unmatched detections,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
filter out matched with low IOU,0
TODO: get cropping parameters and utilize!,1
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Load meta data & annotations,0
get the graph!,0
"NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*",0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
TODO: Make this code not so redundant!,1
"If a string was passed in, auto-convert to True for backward compatibility",0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to initial folder,0
"# Also storing one ""large"" table with results:",0
note: evaluationfolder.parents[0] to get common folder above all shuffle evaluations.,0
#################################################,0
Load data...,0
#################################################,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
TODO: IMPLEMENT for different batch sizes?,1
Ignore best edges possibly defined during a prior evaluation,0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
Pass the image and the keypoints through the resizer;,0
this has no effect if no augmenters were added to it.,0
FIXME Is having an empty array vs nan really that necessary?!,1
"Form 2D array of shape (n_rows, 4) where the last dimension",0
"is (sample_index, peak_y, peak_x, bpt_index) to slice the PAFs.",0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in,0
Compute all distance statistics,0
Calculate overall prediction error,0
Skip data-driven skeleton selection unless,0
the model was trained on the full graph.,0
"loading backbone from ResNet, MobileNet etc.",0
"display_iters = max(1,int(displayiters))",0
"[_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})",0
Save snapshot,0
return to original path.,0
Clip peak locations to PAFs dimensions,0
"unit_vecs = vecs / lengths[:, np.newaxis]",0
"affinities = np.squeeze(y @ np.expand_dims(unit_vecs, axis=2)).sum(axis=1)",0
Form cost matrices,0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
Convert a frozen graph to OpenVINO IR format,0
Read network into memory,0
"For better efficiency, model is initialized for batch_size 1 and every sample processed independently",0
Load network to device,0
Prepare input data,0
Change the stride from 2 to 1 to get 16x downscaling instead of 32x.,0
https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142,0
Multi stage is currently only implemented for resnets,0
Defining multi_fusion backbone,0
Attaching multi-stage decoder,0
stage_paf_output = stage_paf_output + pre_stage_paf_output,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
FIXME Fix wrong scope with Keras layers,1
"def prediction_layer(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2DTranspose(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=2,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
## New DLCNet Addition: multi-stage decoder,0
"def prediction_layer_stage(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2D(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=1,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Compute variance using: Var[X]= E[X^2] - E[X]^2.,0
Compute keep_prob,0
TODO(tanmingxing): add support for training progress.,1
Compute drop_connect tensor,0
turn into x times y time bs * bpts,0
extract corresponding locref x and y as well as probability,0
Filter predicted heatmaps with a 2D Gaussian kernel as in:,0
https://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_The_Devil_Is_in_the_Details_Delving_Into_Unbiased_Data_CVPR_2020_paper.pdf,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
defaults will be a public argument for namedtuple in Python 3.7,0
https://docs.python.org/3/library/collections.html#collections.namedtuple,0
Make sure that round down does not go down by more than 10%.,0
Builds the block accordings to arguments.,0
Expansion phase:,0
Depth-wise convolution phase:,0
Squeeze and Excitation layer.,0
Output phase:,0
only apply drop_connect if skip presents.,0
Expansion phase:,0
Output phase:,0
only apply drop_connect if skip presents.,0
Builds blocks.,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Stem part.,0
Head part.,0
Calls Stem layers,0
Calls blocks.,0
Calls final layers and returns logits.,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"(width_coefficient, depth_coefficient, resolution, dropout_rate)",0
blocks_args = [,0
"'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',",0
"'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',",0
"'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',",0
"'r1_k3_s11_e6_i192_o320_se0.25',",0
],0
The default is TPU-specific batch norm.,0
The alternative is tf.layers.BatchNormalization.,0
"batch_norm=utils.TpuBatchNormalization,  # TPU-specific requirement.",0
ValueError will be raised here if override_params has fields not included,0
in global_params.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrunk when given a depth-multiplier < 1.0.,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
Clamp to 40% of crop size to ensure that at least,0
the center keypoint remains visible after the offset is applied.,0
Points located close to one another are sampled preferentially,0
in order to augment crowded regions.,0
Include keypoints in the count to avoid null probabilities,0
Shift the crop center in both dimensions by random amounts,0
and normalize to the original image dimensions.,0
Pickle the 'data' dictionary using the highest protocol available.,0
item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal,0
"Add smart, keypoint-aware image cropping",0
Scale is sampled only once (per batch) to transform all of the images into same size.,0
Discard keypoints whose coordinates lie outside the cropped image,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
Produce score maps and location refinement fields,0
Find indices of individuals in joint_id,0
WIP!,0
STD of gaussian is 1/4 of threshold,0
Grid of coordinates,0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Distance between the joint point and each coordinate,0
NEEDS TO BE DONE!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
TODO Finish implementing actual abstract class,1
parameter initialization for augmentation pipeline:,0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images of a batch into same size.,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"First, initializing variables (if they don't exist)",0
what is the fraction of training samples with scaling augmentation?,0
loading defaults for rotation range!,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"cfg[""rotateratio""] = cfg.get(",0
"""rotratio"", 0.4",0
)  # what is the fraction of training samples with rotation augmentation?,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"Auto cropping is new (was not in Nature Neuroscience 2018 paper, but introduced in Nath et al. Nat. Protocols 2019)",0
"and boosts performance by 2X, particularly on challenging datasets, like the cheetah in Nath et al.",0
Parameters for augmentation with regard to cropping:,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
Get rid of the interpolation beyond the spline knots,0
Retrieve original individual label indices,0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Check the training image paths are correctly stored as arrays of strings,0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
TENSORPACK could fail in WINDOWS...,0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
# Create one identical splits for 3 networks and 3 augmentations,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
Setting specific parameters for training,0
"MODELS = [""dlcrnet_ms5"", ""dlcr101_ms5"", ""efficientnet-b0"", ""mobilenet_v2_0.35""]",0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
## adding it here,0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Downloading the calibration images,0
Deleting unnecessary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
example how to set pose config variables:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
test if existing models are found:,0
Test if nonexisting models are not found,0
Add some other office files:,0
Add a .pickle and .h5 files,0
"By default, all videos with common extensions are taken from a directory",0
A list of extensions can also be passed in,0
Test bounding box from a single keypoint,0
Test that shape was preserved after vectorization,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Guarantee that images smaller than crop size are handled fine,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Disable stochastic scale jitter,0
Verify the aspect ratio is preserved,0
Load sample multi animal data,0
Generate a random permutation and reorder data. Ignore the unique bodypart,0
Get inverse permutation and reorder the modified data to get back,0
to the original,0
Check,0
"Check data comprise path, shape, and xy coordinates",0
Break the graph to test stitching failure,0
Add fake IDs,0
Split all tracklets in half,0
Test flipped keypoints,0
Original (cached) coordinates must have remained empty,0
Assemble based on the smallest graph to speed up testing,0
Generate fake identity predictions,0
Assemble based on the smallest graph to speed up testing,0
Test now with identity only and ensure assemblies,0
contain only parts of a single group ID.,0
Test empty assembly,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
def install(package):,0
"subprocess.check_call([sys.executable, ""-m"", ""pip"", ""install"", package])",0
"install(""tensorflow==1.13.1"")",0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
quicker variant,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
Suppress tensorflow warning messages,0
"Train, evaluate & predict functions / all require TF",0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Updating config file:,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path, num_shuffles=auxfun_models.Check4weights(pose_cfg['net_type'], parent_path, num_shuffles= 1)",0
Updating training and test pose_cfg:,0
Read the config file,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
common parameters:,0
Write dictionary to yaml  config file,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Flatten the list of bodyparts to connect,0
Format data,0
Trick to force equal aspect ratio of 3D plots,0
Set up the matplotlib figure beforehand,0
Set up skeleton LineCollections,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
Get track_method and do related checks,0
Get track method suffix,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
"## Assign nan to [X,Y] of low likelihood predictions ###",0
Convert the data to a np array to easily mask out the low likelihood predictions,0
"Assign [X,Y] = nan to low likelihood predictions",0
Reshape data back to original shape,0
put data back to the dataframes,0
Check individuals are the same in both views,0
Cross-view match individuals,0
Create a dummy variables for single-animal,0
Cleaner variable (since inds view1 == inds view2),0
"Reshape: (num_framex, num_individuals, num_bodyparts , 2)",0
Triangulate data,0
i is individual in view 1,0
voting[i] is the matched individual in view 2,0
Create 3D DataFrame column and row indices,0
Swap num_animals with num_frames axes to ensure well-behaving reshape,0
Fill up 3D dataframe,0
Reorder 2D dataframe in view 2 to match order of view 1,0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Alternatively,0
Sort tracklets by length to prioritize greater continuity,0
Drop tracklets that are too short,0
"Where slots are available, copy the data over",0
"If about to overwrite data, keep tracklets with highest confidence",0
Squeeze some data into empty slots,0
"For the remaining data, overwrite where we are least confident",0
Find keypoints closest to the remaining data,0
Use Manhattan distance to avoid overflow,0
Only overwrite if improving confidence,0
Fill existing gaps,0
Retrieve original individual label indices,0
Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.,0
Fill existing gaps,0
Retrieve original individual label indices,0
ID swaps occur when X and Y simultaneously intersect each other.,0
Get only those bodypart pairs that belong to different individuals,0
"See Li et al., 2012. Cross-view Activity Recognition using Hankelets.",0
"As proposed in the paper, the Hankel matrix can either be formed from",0
the tracklet's centroid or its normalized velocity.,0
"vel = np.diff(self.centroid, axis=0)",0
"vel /= np.linalg.norm(vel, axis=1, keepdims=True)",0
return self.hankelize(vel),0
TODO Add missing data imputation,1
"nrows, ncols = mat.shape",0
beta = nrows / ncols,0
omega = 0.56 * beta ** 3 - 0.95 * beta ** 2 + 1.82 * beta + 1.43,0
return np.argmin(s > omega * np.median(s)),0
"Note that if tracklets are very short, some may actually be part of the same track",0
and thus incorrectly reflect separate track endpoints...,0
"Map each Tracklet to an entry and output nodes and vice versa,",0
which is convenient once the tracklets are stitched.,0
Store tracklets and corresponding negatives (those that overlap in time),0
TODO Avoid looping over all pairs of tracklets,0
Pick the closest (spatially) overlapping tracklet,0
The algorithm works better with integer weights,0
Let us prune the graph by removing all source and sink edges,0
but those connecting the `n_tracks` first and last tracklets.,0
Preflow push seems to work slightly better than shortest,0
"augmentation path..., and is more computationally efficient.",0
Verify whether there are overlapping tracklets,0
"Pick the segment that minimizes ""smoothness"", computed here",0
with the coefficient of variation of the differences.,0
Rebuild a full graph from the remaining nodes without,0
temporal constraint on what tracklets can be stitched together.,0
Cycle through the residuals and incorporate back those,0
that only fit in a single tracklet.,0
Greedily add the remaining residuals,0
Refresh temporal bounds,0
Default to the distance cost function,0
should only exist one,0
Add in identity weighing before building the graph,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implementation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
average distance and average # significant differences avg. over comparisonbodyparts,0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
###################################################,0
Dependencies,0
###################################################,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as,0
specified in the config file,0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
Prepare figure,0
Mask rather than drop unwanted bodyparts to ensure consistent coloring,0
cropping!,0
nx=X2-X1,0
!/usr/bin/env python3,0
"more videos are in principle covered, as OpenCV is used and allows many formats.",0
The above already raises a ValueError if formatting is wrong,0
"Rotate, see: https://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg",0
interesting option to just update metadata.,0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
Read the video until a frame is successfully read,0
import platform,0
"if platform.system() == ""Darwin"":  # for OSX use WXAgg",0
fig.canvas.start_event_loop(timeout=-1),0
else:,0
#fig.canvas.stop_event_loop(),0
"Windows throws error if file path is > 260 characters, can fix with prefix.",0
See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation,0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
This dictionary maps the model types to the file locations where the models exist.,0
Exit the function early if an unknown modeltype is provided.,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
Initialize costs matrix,0
get common bodypart detections in track pair,0
add 3rd dim to the points,0
cost for any point in time of t1 being the same,0
any point in time of t2,0
Get average cost of the entire track,0
check if it exists:,0
(average_degree / 2) as many edges as there are nodes is required,0
for undirected graphs to reach the target degree.,0
"Attention this order has to be consistent (for training set creation, training, inference etc.)",0
"-> adding (single,bpt) for uniquebodyparts",0
"-> adding (individual,bpt) for multianimalbodyparts",0
load defaults,0
set project specific parameters:,0
###################################################,0
Dependencies,0
###################################################,0
Pose X vs pose Y,0
Poses vs time,0
Likelihoods,0
Histograms,0
#################################################,0
Looping analysis over video,0
#################################################,0
Keep only the individuals and bodyparts that were labeled,0
Either display the animals defined in the config if they are found,0
"in the dataframe, or all the trajectories regardless of their names",0
Determine whether the data are single- or multi-animal without loading into memory,0
simply by checking whether 'individuals' is in the second line of the CSV.,0
Make paths platform-agnostic if they are not already,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
make list of full paths,0
filter list of videos,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"if ""MULTI!"" in allbpts:",0
Ensure same order as in config.yaml,0
ABBREVIATE NETWORK NAMES -- esp. for mobilenet!,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
Iterate over data files and stop as soon as one matching the scorer is found,0
If there was no match...,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"trainingset = wx.StaticBox(self, label=""Specify the trainingset index"")",0
"trainingset_boxsizer = wx.StaticBoxSizer(trainingset, wx.VERTICAL)",0
"self.trainingset = wx.SpinCtrl(self, value=""0"", min=0, max=100)",0
trainingset_boxsizer.Add(,0
"self.trainingset, 1, wx.EXPAND | wx.TOP | wx.BOTTOM, 10",0
),0
"hbox_.Add(trainingset_boxsizer, 5, wx.EXPAND | wx.TOP | wx.BOTTOM, 5)",0
Read the infer config file,0
trainingsetindex = self.trainingset.GetValue(),0
let the user open the file with default text editor. Also make it mac compatible,0
trainingsetindex = self.trainingset.GetValue(),0
"trainingsetindex=trainingsetindex,",0
"trainingsetindex=self.trainingset.GetValue(),",0
self.save.Enable(False),0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
topSplitter = wx.SplitterWindow(self),0
,0
"self.image_panel = ImagePanel(topSplitter, config,video,shuffle,Dataframe,self.gui_size)",0
self.widget_panel = WidgetPanel(topSplitter),0
,0
"topSplitter.SplitHorizontally(self.image_panel, self.widget_panel,sashPosition=self.gui_size[1]*0.83)#0.9",0
topSplitter.SetSashGravity(1),0
sizer = wx.BoxSizer(wx.VERTICAL),0
"sizer.Add(topSplitter, 1, wx.EXPAND)",0
self.SetSizer(sizer),0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
variable initialization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0,10), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=10)",0
Add all the options,0
line = wx.StaticLine(self),0
"self.sizer.Add(line, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=10)",0
,0
"self.multi_choice.Bind(wx.EVT_CHECKBOX,self.activate_copy_videos)",0
Hide the button as this is not the default option,0
self.vids.Enable(False),0
self.SetSizer(self.sizer),0
"self.sizer.Add(self.sizer, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=15)",0
self.ok.Enable(False),0
else:,0
self.ok.Enable(True),0
For mac compatibility,0
Remove the pages in case the user goes back to the create new project and creates/load a new project,0
Add all the other pages,0
"page10 = Refine_labels(self.parent, self.gui_size, self.cfg, page5)",0
"self.parent.AddPage(page10, ""OPT: Refine Labels"")",0
self.yes.Enable(False),0
self.no.Enable(False),0
Here we create a panel and a notebook on the panel,0
create the page windows as children of the notebook and add the pages to the notebook with the label to show on the tab,0
Take into consideration imprecise OpenCV estimation of total number of frames,0
Automatically disable the draggable points,0
Save additional frames to the labeled-data directory,0
Store the newly-refined data,0
Uncertain keypoints are ignored,0
Merge with the already existing annotated data,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.buttonCounter = [],0
self.bodyparts = [],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
load labeledPoints and fundamental Matrix,0
Find offset terms for drawing epipolar Lines,0
Get crop params for camera being labeled,0
Get crop params for other camera,0
convert the image to RGB as you are showing the image with matplotlib,0
draw epipolar lines,0
##################################################################################################################################################,0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
variable initialization,0
design the panel,0
Add logo of DLC,0
self.filelist = self.filelist + self.vids,0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
variable initialization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initialization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
## train and go ###,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
load labeledPoints and fundamental Matrix,0
Find offset terms for drawing epipolar Lines,0
Get crop params for camera being labeled,0
Get crop params for other camera,0
convert the image to RGB as you are showing the image with matplotlib,0
draw epipolar lines,0
divider = make_axes_locatable(self.axes),0
"colorIndex = np.linspace(np.min(im),np.max(im),len(bodyparts))",0
"cax = divider.append_axes(""right"", size=""5%"", pad=0.05)",0
"cbar = self.figure.colorbar(ax, cax=cax,spacing='proportional', ticks=colorIndex)",0
cbar.set_ticklabels(bodyparts[::-1]),0
self.figure.canvas.draw(),0
##################################################################################################################################################,0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"MainFrame.saveDataSet(self, event)",0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
checks for unique bodyparts,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Create an empty data frame,0
Cache original bodyparts,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Rearrange bodypart columns in config order,0
Test whether there are missing frames and superfluous data,0
Check whether new individuals or body parts were added,0
Checking if user added a new label,0
Found new labels in either multiple bodyparts or unique bodyparts,0
check if single is selected when radio buttons are changed,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Discard data associated with bodyparts that are no longer in the config,0
Re-organize the dataframe so the CSV looks consistent with the config,0
restore the background region,0
variable initialization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.build = wx.Button(self, label=""Build skeleton"")",0
"sizer.Add(self.build, pos=(4, 3), flag=wx.BOTTOM | wx.RIGHT, border=10)",0
"self.build.Bind(wx.EVT_BUTTON, self.build_skeleton)",0
self.build.Enable(True),0
"def build_skeleton(self, event):",0
skeleton.SkeletonBuilder(self.config),0
Setting the GUI size and panels design,0
This sets the minimum size of the GUI. It can scale now!,0
Checks if zoom/pan button is ON,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
variable initialization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"trainingindex = wx.StaticBox(self, label=""Specify the trainingset index"")",0
"trainingindex_boxsizer = wx.StaticBoxSizer(trainingindex, wx.VERTICAL)",0
"self.trainingindex = wx.SpinCtrl(self, value=""0"", min=0, max=100)",0
trainingindex_boxsizer.Add(,0
"self.trainingindex, 1, wx.EXPAND | wx.TOP | wx.BOTTOM, 10)",0
use the default pose_cfg file for default values,0
self.display_iters.Enable(False),0
self.save_iters.Enable(False),0
self.max_iters.Enable(False),0
self.snapshots.Enable(False),0
"hbox1.Add(trainingindex_boxsizer, 10, wx.EXPAND | wx.TOP | wx.BOTTOM, 5)",0
self.trainingindex.Enable(False),0
self.trainingindex.Enable(True),0
self.trainingindex.Enable(True),0
Read the pose config file,0
print(trainFraction[-1]),0
"print(os.path.join(cfg['project_path'],auxiliaryfunctions.get_model_folder(trainFraction, self.shuffles.GetValue(),cfg),'train','pose_cfg.yaml'))",0
let the user open the file with default text editor. Also make it mac compatible,0
update the variables with the edited values in the pose config file,0
self.trainingindex.Enable(True),0
variable initialization,0
design the panel,0
Add logo of DLC,0
### BOX 1 ####,0
### BOX 2 ####,0
### BOX 3 ####,0
Read the infer config file,0
"trainFraction = cfg[""TrainingFraction""][trainingsetindex]",0
let the user open the file with default text editor. Also make it mac compatible,0
"self.select_destfolder.SetPath(""None"")",0
self.config = [],0
"self.sel_config.SetPath("""")",0
variable initialization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"trainingindex_box = wx.StaticBox(self, label=""Specify the trainingset index"")",0
"trainingindex_boxsizer = wx.StaticBoxSizer(trainingindex_box, wx.VERTICAL)",0
"self.trainingindex = wx.SpinCtrl(self, value=""0"", min=0, max=100)",0
trainingindex_boxsizer.Add(,0
"self.trainingindex, 0, wx.EXPAND | wx.TOP | wx.BOTTOM, 10",0
),0
"self.hbox2.Add(trainingindex_boxsizer, 10, wx.EXPAND | wx.TOP | wx.BOTTOM, 5)",0
trainindex = self.trainingindex.GetValue(),0
"trainindex=trainindex,",0
"self.shuffles.SetValue(""1"")",0
"DownSampleVideo(vname,width=-1,height=200,outsuffix='downsampled',outpath=None,rotatecw=False):",0
variable initialization,0
design the panel,0
Add logo of DLC,0
"majorDimension=0,",0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
"self.choiceBox,self.visualization_rdb = self.choice_panel.addRadioButtons()",0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
Overwrite the config-defined individual names,0
with those actually present in the annotated data,0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
print(self.Dataframe.head()),0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
fix move to corner,0
fix move to corner,0
fix move to corner,0
fix move to corner,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
variable initialization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"trainingset = wx.StaticBox(self, label=""Specify the trainingset index"")",0
"trainingset_boxsizer = wx.StaticBoxSizer(trainingset, wx.VERTICAL)",0
"self.trainingset = wx.SpinCtrl(self, value=""0"", min=0, max=100)",0
trainingset_boxsizer.Add(,0
"self.trainingset, 1, wx.EXPAND | wx.TOP | wx.BOTTOM, 10",0
),0
"self.hbox1.Add(trainingset_boxsizer, 5, wx.EXPAND | wx.TOP | wx.BOTTOM, 5)",0
"if self.plot_scoremaps.GetStringSelection() == ""Yes"":",0
Read the infer config file,0
trainingsetindex = self.trainingset.GetValue(),0
let the user open the file with default text editor. Also make it mac compatible,0
shuffle = self.shuffle.GetValue(),0
trainingsetindex = self.trainingset.GetValue(),0
"trainingsetindex=trainingsetindex,",0
variable initialization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
## LABELING ###,0
"self.sel_config.SetPath("""")",0
##################################################################################################################################################,0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
self.cfg = auxiliaryfunctions.read_config(config),0
"self.statusbar.SetStatusText("""")",0
"dlg = wx.MessageDialog(None,""Are you sure?"", ""Quit!"",wx.YES_NO | wx.ICON_WARNING)",0
result = dlg.ShowModal(),0
if result == wx.ID_YES:,0
variable initialization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0, 4), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=5)",0
variable initialization,0
design the panel,0
Add logo of DLC,0
#         design the panel,0
Add image of DLC,0
if editing this text make sure you add the '\n' to get the new line. The sizer is unable to format lines correctly.,0
"calling create_tracking_dataset, train_tracking_transformer, stitch_tracklets",0
only try to find these features if they are in the dictionary,0
1) reference to video folder and get the proper bpt_feature file for feature table,0
2) get either the path to gt or the path to track pickle,0
tracklets,0
"with npy list form videos, split each to train and test",0
"video_name = '.'.join(video.split(""/"")[-1].split(""."")[:-1])",0
assuming there is only one match,0
make my own model factory,0
make my own loss factory,0
"NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights",0
"NOTE: drop path for stochastic depth, we shall see if this is better than dropout here",0
only channel pooling,0
"x -> (B, 12, 2048)",0
"x -> (B, 12, 768)",0
trying to keep everything simple,0
need to write my own keypoint embedding,0
we don't need a fc layer here,0
x: inputs,0
"(B, 12, 768)",0
added pos embed,0
remove all parts I don't understand,0
forwarding through blocks. But num of blocks should be reduced,0
i can't really tell what the difference is,0
will have to change some of these,0
For old models that I trained prior to conv based patchification,0
To resize pos embedding when using model at different size from pretrained weights,0
Rescale the grid of position embeddings when loading from state_dict. Adapted from,0
https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224,0
Cut & paste from PyTorch official master until it's in a few official releases - RW,0
Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf,0
Computes standard normal cumulative distribution function,0
Values are generated by using a truncated uniform distribution and,0
then using the inverse CDF for the normal distribution.,0
Get upper and lower cdf values,0
"Uniformly fill tensor with values from [l, u], then translate to",0
"[2l-1, 2u-1].",0
Use inverse cdf transform for normal distribution to get truncated,0
standard normal,0
"Transform to proper mean, std",0
Clamp to ensure it's in the proper range,0
"type: (Tensor, float, float, float, float) -> Tensor",0
"resample if noise out of percent limit, brute force but shouldn't spin much",0
distmat g,0
q    1 3 2 4,0
4 1 2 3,0
0 2 1 3,0
1 2 3 0,0
compute cmc curve for each query,0
get query pid and camid,0
remove gallery samples that have the same pid and camid with query,0
compute cmc curve,0
"binary vector, positions with value 1 are correct matches",0
this condition is true when query identity does not appear in gallery,0
compute average precision,0
reference: https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Average_precision,0
"tmp_cmc = [x / (i + 1.) for i, x in enumerate(tmp_cmc)]",0
query,0
gallery,0
"distmat = re_ranking(qf, gf, k1=20, k2=6, lambda_value=0.3)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
"if feature vector is numpy, you should use 'torch.tensor' transform it to tensor",0
print('using GPU to compute original distance'),0
print('starting re_ranking'),0
k-reciprocal neighbors,0
More elegantly one can simply define:,0
take care of difference between feature map space and original image space,0
cos = torch.cdist,0
TODO: maybe find a better spot for this.,1
"dev =  device if torch.cuda.is_available() else ""cpu""",0
train,0
print (f'validation loss {val_loss/len(val_loader)}'),0
normalize vectors here,0
i th vector at j th kpt,0
maybe needs to convert them to embeddings and position token,0
Drop missing body parts,0
Drop points lying outside the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
## dlcnet_ms5: backbone resnet50 + multi-fusion & multi-stage module,0
## dlcr101_ms5/dlcr152_ms5: backbone resnet101/152 + multi-fusion & multi-stage module,0
"If the graph is unnecessarily large (with 15+ keypoints by default),",0
we randomly prune it to a size guaranteeing an average node degree of 6;,0
"see Suppl. Fig S9c in Lauer et al., 2022.",0
Use the skeleton defined in the config file,0
Ignore possible connections between 'multi' and 'unique' body parts;,0
one can never be too careful...,0
Disable the prediction of PAFs if the graph is empty,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving metadata and data file (Pickle file),0
###############################################################################,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Setting inference cfg file:,0
Search a pose_config.yaml file to parse missing information,0
Clean the training-datasets folder prior to recreating the data pickles,0
Load updated lists:,0
Find the corresponding video file,0
"print(key, ""dropping?"")",0
adding important values for multianiaml project:,0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Determine the index length required to guarantee,0
the train–test ratio is exactly the desired one.,0
Pad indices so lengths agree,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(name,test_video_name)",0
Blazing fast and does not load the image into memory,0
Check that points lie within the image,0
DeprecationWarnings are silenced since Python 3.2 unless triggered in __main__,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
updating variable if null/None! #backwardscompatability,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
these values are dropped as scalecrop,0
doesn't have rotation implemented,0
Also drop maDLC smart cropping augmentation parameters,0
sort the model directories,0
get the shuffle index and offset by 1.,0
read cfg file,0
create log file,0
Check for variable correctness,0
Moviepy:,0
select crop method,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
allow_growth must be true here because tensorflow does not automatically free gpu memory and setting it as false occupies all gpu memory so that pytorch cannot kick in,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
should close tensorflow session here in order to free gpu,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html#videocapture-get,0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
TODO: add cropping as in video analysis!,1
if cropping is not None:,0
cfg['cropping']=True,0
"cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping",0
"print(""Overwriting cropping parameters:"", cropping)",0
"print(""These are used for all videos, but won't be save to the cfg file."")",0
"Also ensure `boundingboxslack` is greater than zero, otherwise overlap",0
"between trackers cannot be evaluated, resulting in empty tracklets.",0
Check which snapshots are available and sort them by # iterations,0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
NOTE: If dataname line above is changed then line below is obsolete?,0
"trackname = trackname.replace(videofolder, destfolder)",0
TODO: adjust this for multi + unique bodyparts!,1
this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. ),0
Optimal identity assignment based on soft voting,0
imagenet mean for resnet pretraining:,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/DeepLabCut/DeepLabCut/pull/324",0
for DLC 2.2. (here all set False to not use PAFs/pairwise fields),0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
"reloading defaults, as they can bleed over from a previous run otherwise",0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
dlc_cfg_train = load_config(str(path_train_config)),0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
Name for deeplabcut net (based on its parameters),0
"DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])",0
"print(""Extracting maps for "", DLCscorer, "" with # of trainingiterations:"", trainingsiterations)",0
if notanalyzed: #this only applies to ask if h5 exists...,0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
"for multi animal, seems only this is used",0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Get animal IDs and corresponding indices in the arrays of detections,0
Form ground truth beforehand,0
Assemble animals on the full set of detections,0
Count the number of unassembled bodyparts,0
Handle unlabeled bodyparts...,0
"Only 1 animal, let us return the full graph indices only",0
Find minimal skeleton,0
Select optimal PAF graph,0
Selective copy; deepcopy is >5x slower,0
Only keeps skeletons that are more than 90% complete,0
TODO Normalize dists by longest length?,1
TODO Smarter imputation technique (Bayesian? Grassmann averages?),1
Covariance matrix estimation fails due to numerical singularities,0
Distance is undefined if the assembly is empty,0
Correct distance to account for missing observations,0
"Alternatively, reduce contribution of missing values to the Mahalanobis",0
distance to zero by substituting the corresponding means.,0
"Fill the subsets with unambiguous, complete individuals",0
Fuse superfluous assemblies,0
Second pass without edge safety,0
Mark the unique body parts as assembled anyway so,0
they are not used later on to fill assemblies.,0
Store selected edges for subsequent frames,0
Remove invalid assemblies,0
Fill assemblies with unconnected body parts,0
Spawning (rather than forking) multiple processes does not,0
work nicely with the GUI or interactive sessions.,0
"In that case, we fall back to the serial assembly.",0
Swap corresponding keypoints,0
Only consider assemblies of at least two keypoints,0
Greedy assembly matching like in pycocotools,0
Global rather than greedy assembly matching,0
Cast columns of dtype 'object' to float to avoid TypeError,0
further down in _parse_ground_truth_data.,0
sigma is taken as the median of all COCO keypoint standard deviations,0
Sort matching score (OKS) in descending order of assembly affinity,0
Guarantee precision decreases monotonically,0
See https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173),0
Regularize by forcing AR <= 5,0
max_ar = 5,0
if el.aspect_ratio >= max_ar:,0
if el.height > el.width:,0
el.width = el.height / max_ar,0
else:,0
el.height = el.width / max_ar,0
Orient the ellipse such that it encompasses most points,0
"n_inside = el.contains_points(np.c_[self.x, self.y]).sum()",0
el.theta += 0.5 * np.pi,0
"if el.contains_points(np.c_[self.x, self.y]).sum() < n_inside:",0
el.theta -= 0.5 * np.pi,0
"r2 = chi2.ppf(2 * norm.cdf(sd) - 1, 2)",0
"height, width = np.sqrt(E * r2)",0
The general quadratic curve has the form:,0
ax^2 + 2bxy + cy^2 + 2dx + 2fy + g = 0,0
Ellipse center coordinates,0
Semi-axes lengths,0
Angle to the horizontal,0
High uncertainty to the unobservable initial velocities,0
Give high uncertainty to the unobservable initial velocities,0
diff = val - cost_matrix,0
"diff[row, col] += val",0
if (,0
val < self.iou_threshold,0
or np.any(diff[row] <= 0.2),0
"or np.any(diff[:, col] <= 0.2)",0
):,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
Modified from scipy source code:,0
- to restrict its use to 2D,0
- to get rid of shuffling (since arrays are only (nbodyparts * 3) element long),0
TODO - factor in keypoint confidence (and weight by # of observations??),1
"mat = self.calc_pairwise_oks(poses, poses_ref)",0
Remove matched detections with low OKS,0
matches = [],0
"for row, col in zip(row_indices, col_indices):",0
"if mat[row, col] < self.oks_threshold:",0
unmatched_poses.append(row),0
unmatched_trackers.append(col),0
else:,0
"matches.append([row, col])",0
if not len(matches):,0
"matches = np.empty((0, 2), dtype=int)",0
else:,0
matches = np.stack(matches),0
update matched trackers with assigned detections,0
create and initialise new trackers for unmatched detections,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
filter out matched with low IOU,0
TODO: get cropping parameters and utilize!,1
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Load meta data & annotations,0
get the graph!,0
"NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*",0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
TODO: Make this code not so redundant!,1
"If a string was passed in, auto-convert to True for backward compatibility",0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to initial folder,0
"# Also storing one ""large"" table with results:",0
note: evaluationfolder.parents[0] to get common folder above all shuffle evaluations.,0
#################################################,0
Load data...,0
#################################################,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
TODO: IMPLEMENT for different batch sizes?,1
Ignore best edges possibly defined during a prior evaluation,0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
Pass the image and the keypoints through the resizer;,0
this has no effect if no augmenters were added to it.,0
FIXME Is having an empty array vs nan really that necessary?!,1
"Form 2D array of shape (n_rows, 4) where the last dimension",0
"is (sample_index, peak_y, peak_x, bpt_index) to slice the PAFs.",0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in,0
Compute all distance statistics,0
Calculate overall prediction error,0
Skip data-driven skeleton selection unless,0
the model was trained on the full graph.,0
"loading backbone from ResNet, MobileNet etc.",0
"display_iters = max(1,int(displayiters))",0
"[_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})",0
Save snapshot,0
return to original path.,0
Clip peak locations to PAFs dimensions,0
"unit_vecs = vecs / lengths[:, np.newaxis]",0
"affinities = np.squeeze(y @ np.expand_dims(unit_vecs, axis=2)).sum(axis=1)",0
Form cost matrices,0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
Convert a frozen graph to OpenVINO IR format,0
Read network into memory,0
"For better efficiency, model is initialized for batch_size 1 and every sample processed independently",0
Load network to device,0
Prepare input data,0
Change the stride from 2 to 1 to get 16x downscaling instead of 32x.,0
https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142,0
Multi stage is currently only implemented for resnets,0
Defining multi_fusion backbone,0
Attaching multi-stage decoder,0
stage_paf_output = stage_paf_output + pre_stage_paf_output,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
FIXME Fix wrong scope with Keras layers,1
"def prediction_layer(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2DTranspose(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=2,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
## New DLCNet Addition: multi-stage decoder,0
"def prediction_layer_stage(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2D(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=1,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Compute variance using: Var[X]= E[X^2] - E[X]^2.,0
Compute keep_prob,0
TODO(tanmingxing): add support for training progress.,1
Compute drop_connect tensor,0
turn into x times y time bs * bpts,0
extract corresponding locref x and y as well as probability,0
Filter predicted heatmaps with a 2D Gaussian kernel as in:,0
https://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_The_Devil_Is_in_the_Details_Delving_Into_Unbiased_Data_CVPR_2020_paper.pdf,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
defaults will be a public argument for namedtuple in Python 3.7,0
https://docs.python.org/3/library/collections.html#collections.namedtuple,0
Make sure that round down does not go down by more than 10%.,0
Builds the block accordings to arguments.,0
Expansion phase:,0
Depth-wise convolution phase:,0
Squeeze and Excitation layer.,0
Output phase:,0
only apply drop_connect if skip presents.,0
Expansion phase:,0
Output phase:,0
only apply drop_connect if skip presents.,0
Builds blocks.,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Stem part.,0
Head part.,0
Calls Stem layers,0
Calls blocks.,0
Calls final layers and returns logits.,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"(width_coefficient, depth_coefficient, resolution, dropout_rate)",0
blocks_args = [,0
"'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',",0
"'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',",0
"'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',",0
"'r1_k3_s11_e6_i192_o320_se0.25',",0
],0
The default is TPU-specific batch norm.,0
The alternative is tf.layers.BatchNormalization.,0
"batch_norm=utils.TpuBatchNormalization,  # TPU-specific requirement.",0
ValueError will be raised here if override_params has fields not included,0
in global_params.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrunk when given a depth-multiplier < 1.0.,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
Clamp to 40% of crop size to ensure that at least,0
the center keypoint remains visible after the offset is applied.,0
Points located close to one another are sampled preferentially,0
in order to augment crowded regions.,0
Include keypoints in the count to avoid null probabilities,0
Shift the crop center in both dimensions by random amounts,0
and normalize to the original image dimensions.,0
Pickle the 'data' dictionary using the highest protocol available.,0
item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal,0
"Add smart, keypoint-aware image cropping",0
Scale is sampled only once (per batch) to transform all of the images into same size.,0
Discard keypoints whose coordinates lie outside the cropped image,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
"imageio.imwrite(data_items[i].im_path.split('/')[-1],im)",0
Produce score maps and location refinement fields,0
Find indices of individuals in joint_id,0
WIP!,0
STD of gaussian is 1/4 of threshold,0
Grid of coordinates,0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Distance between the joint point and each coordinate,0
NEEDS TO BE DONE!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
TODO Finish implementing actual abstract class,1
parameter initialization for augmentation pipeline:,0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images of a batch into same size.,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"First, initializing variables (if they don't exist)",0
what is the fraction of training samples with scaling augmentation?,0
loading defaults for rotation range!,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"cfg[""rotateratio""] = cfg.get(",0
"""rotratio"", 0.4",0
)  # what is the fraction of training samples with rotation augmentation?,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"Auto cropping is new (was not in Nature Neuroscience 2018 paper, but introduced in Nath et al. Nat. Protocols 2019)",0
"and boosts performance by 2X, particularly on challenging datasets, like the cheetah in Nath et al.",0
Parameters for augmentation with regard to cropping:,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
Get rid of the interpolation beyond the spline knots,0
Retrieve original individual label indices,0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Check the training image paths are correctly stored as arrays of strings,0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
TENSORPACK could fail in WINDOWS...,0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
# Create one identical splits for 3 networks and 3 augmentations,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
Setting specific parameters for training,0
"MODELS = [""dlcrnet_ms5"", ""dlcr101_ms5"", ""efficientnet-b0"", ""mobilenet_v2_0.35""]",0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
## adding it here,0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Downloading the calibration images,0
Deleting unnecessary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
example how to set pose config variables:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
test if existing models are found:,0
Test if nonexisting models are not found,0
Add some other office files:,0
Add a .pickle and .h5 files,0
"By default, all videos with common extensions are taken from a directory",0
A list of extensions can also be passed in,0
Test bounding box from a single keypoint,0
Test that shape was preserved after vectorization,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Guarantee that images smaller than crop size are handled fine,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Disable stochastic scale jitter,0
Verify the aspect ratio is preserved,0
Load sample multi animal data,0
Generate a random permutation and reorder data. Ignore the unique bodypart,0
Get inverse permutation and reorder the modified data to get back,0
to the original,0
Check,0
"Check data comprise path, shape, and xy coordinates",0
Break the graph to test stitching failure,0
Add fake IDs,0
Split all tracklets in half,0
Test flipped keypoints,0
Original (cached) coordinates must have remained empty,0
Assemble based on the smallest graph to speed up testing,0
Generate fake identity predictions,0
Assemble based on the smallest graph to speed up testing,0
Test now with identity only and ensure assemblies,0
contain only parts of a single group ID.,0
Test empty assembly,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
def install(package):,0
"subprocess.check_call([sys.executable, ""-m"", ""pip"", ""install"", package])",0
"install(""tensorflow==1.13.1"")",0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
quicker variant,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
Suppress tensorflow warning messages,0
"Train, evaluate & predict functions / all require TF",0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Updating config file:,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path, num_shuffles=auxfun_models.Check4weights(pose_cfg['net_type'], parent_path, num_shuffles= 1)",0
Updating training and test pose_cfg:,0
Read the config file,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
common parameters:,0
Write dictionary to yaml  config file,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Flatten the list of bodyparts to connect,0
Format data,0
Trick to force equal aspect ratio of 3D plots,0
Set up the matplotlib figure beforehand,0
Set up skeleton LineCollections,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
Get track_method and do related checks,0
Get track method suffix,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
"## Assign nan to [X,Y] of low likelihood predictions ###",0
Convert the data to a np array to easily mask out the low likelihood predictions,0
"Assign [X,Y] = nan to low likelihood predictions",0
Reshape data back to original shape,0
put data back to the dataframes,0
Check individuals are the same in both views,0
Cross-view match individuals,0
Create a dummy variables for single-animal,0
Cleaner variable (since inds view1 == inds view2),0
"Reshape: (num_framex, num_individuals, num_bodyparts , 2)",0
Triangulate data,0
i is individual in view 1,0
voting[i] is the matched individual in view 2,0
Create 3D DataFrame column and row indices,0
Swap num_animals with num_frames axes to ensure well-behaving reshape,0
Fill up 3D dataframe,0
Reorder 2D dataframe in view 2 to match order of view 1,0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Alternatively,0
Sort tracklets by length to prioritize greater continuity,0
Drop tracklets that are too short,0
"Where slots are available, copy the data over",0
"If about to overwrite data, keep tracklets with highest confidence",0
Squeeze some data into empty slots,0
"For the remaining data, overwrite where we are least confident",0
Find keypoints closest to the remaining data,0
Use Manhattan distance to avoid overflow,0
Only overwrite if improving confidence,0
Fill existing gaps,0
Retrieve original individual label indices,0
Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.,0
Fill existing gaps,0
Retrieve original individual label indices,0
ID swaps occur when X and Y simultaneously intersect each other.,0
Get only those bodypart pairs that belong to different individuals,0
"See Li et al., 2012. Cross-view Activity Recognition using Hankelets.",0
"As proposed in the paper, the Hankel matrix can either be formed from",0
the tracklet's centroid or its normalized velocity.,0
"vel = np.diff(self.centroid, axis=0)",0
"vel /= np.linalg.norm(vel, axis=1, keepdims=True)",0
return self.hankelize(vel),0
TODO Add missing data imputation,1
"nrows, ncols = mat.shape",0
beta = nrows / ncols,0
omega = 0.56 * beta ** 3 - 0.95 * beta ** 2 + 1.82 * beta + 1.43,0
return np.argmin(s > omega * np.median(s)),0
"Note that if tracklets are very short, some may actually be part of the same track",0
and thus incorrectly reflect separate track endpoints...,0
"Map each Tracklet to an entry and output nodes and vice versa,",0
which is convenient once the tracklets are stitched.,0
Store tracklets and corresponding negatives (those that overlap in time),0
TODO Avoid looping over all pairs of tracklets,0
Pick the closest (spatially) overlapping tracklet,0
The algorithm works better with integer weights,0
Let us prune the graph by removing all source and sink edges,0
but those connecting the `n_tracks` first and last tracklets.,0
Preflow push seems to work slightly better than shortest,0
"augmentation path..., and is more computationally efficient.",0
Verify whether there are overlapping tracklets,0
"Pick the segment that minimizes ""smoothness"", computed here",0
with the coefficient of variation of the differences.,0
Rebuild a full graph from the remaining nodes without,0
temporal constraint on what tracklets can be stitched together.,0
Cycle through the residuals and incorporate back those,0
that only fit in a single tracklet.,0
Greedily add the remaining residuals,0
Refresh temporal bounds,0
Default to the distance cost function,0
should only exist one,0
Add in identity weighing before building the graph,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implementation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
average distance and average # significant differences avg. over comparisonbodyparts,0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
###################################################,0
Dependencies,0
###################################################,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as,0
specified in the config file,0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
Prepare figure,0
Mask rather than drop unwanted bodyparts to ensure consistent coloring,0
cropping!,0
nx=X2-X1,0
!/usr/bin/env python3,0
"more videos are in principle covered, as OpenCV is used and allows many formats.",0
The above already raises a ValueError if formatting is wrong,0
"Rotate, see: https://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg",0
interesting option to just update metadata.,0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
Read the video until a frame is successfully read,0
import platform,0
"if platform.system() == ""Darwin"":  # for OSX use WXAgg",0
fig.canvas.start_event_loop(timeout=-1),0
else:,0
#fig.canvas.stop_event_loop(),0
"Windows throws error if file path is > 260 characters, can fix with prefix.",0
See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation,0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
Initialize costs matrix,0
get common bodypart detections in track pair,0
add 3rd dim to the points,0
cost for any point in time of t1 being the same,0
any point in time of t2,0
Get average cost of the entire track,0
check if it exists:,0
(average_degree / 2) as many edges as there are nodes is required,0
for undirected graphs to reach the target degree.,0
"Attention this order has to be consistent (for training set creation, training, inference etc.)",0
"-> adding (single,bpt) for uniquebodyparts",0
"-> adding (individual,bpt) for multianimalbodyparts",0
load defaults,0
set project specific parameters:,0
###################################################,0
Dependencies,0
###################################################,0
Pose X vs pose Y,0
Poses vs time,0
Likelihoods,0
Histograms,0
#################################################,0
Looping analysis over video,0
#################################################,0
Keep only the individuals and bodyparts that were labeled,0
Either display the animals defined in the config if they are found,0
"in the dataframe, or all the trajectories regardless of their names",0
Determine whether the data are single- or multi-animal without loading into memory,0
simply by checking whether 'individuals' is in the second line of the CSV.,0
Make paths platform-agnostic if they are not already,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Fraction of video to start/stop when extracting frames for labeling/refinement,0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
make list of full paths,0
filter list of videos,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"if ""MULTI!"" in allbpts:",0
Ensure same order as in config.yaml,0
ABBREVIATE NETWORK NAMES -- esp. for mobilenet!,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
Iterate over data files and stop as soon as one matching the scorer is found,0
If there was no match...,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"trainingset = wx.StaticBox(self, label=""Specify the trainingset index"")",0
"trainingset_boxsizer = wx.StaticBoxSizer(trainingset, wx.VERTICAL)",0
"self.trainingset = wx.SpinCtrl(self, value=""0"", min=0, max=100)",0
trainingset_boxsizer.Add(,0
"self.trainingset, 1, wx.EXPAND | wx.TOP | wx.BOTTOM, 10",0
),0
"hbox_.Add(trainingset_boxsizer, 5, wx.EXPAND | wx.TOP | wx.BOTTOM, 5)",0
Read the infer config file,0
trainingsetindex = self.trainingset.GetValue(),0
let the user open the file with default text editor. Also make it mac compatible,0
trainingsetindex = self.trainingset.GetValue(),0
"trainingsetindex=trainingsetindex,",0
"trainingsetindex=self.trainingset.GetValue(),",0
self.save.Enable(False),0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
topSplitter = wx.SplitterWindow(self),0
,0
"self.image_panel = ImagePanel(topSplitter, config,video,shuffle,Dataframe,self.gui_size)",0
self.widget_panel = WidgetPanel(topSplitter),0
,0
"topSplitter.SplitHorizontally(self.image_panel, self.widget_panel,sashPosition=self.gui_size[1]*0.83)#0.9",0
topSplitter.SetSashGravity(1),0
sizer = wx.BoxSizer(wx.VERTICAL),0
"sizer.Add(topSplitter, 1, wx.EXPAND)",0
self.SetSizer(sizer),0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
variable initialization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0,10), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=10)",0
Add all the options,0
line = wx.StaticLine(self),0
"self.sizer.Add(line, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=10)",0
,0
"self.multi_choice.Bind(wx.EVT_CHECKBOX,self.activate_copy_videos)",0
Hide the button as this is not the default option,0
self.vids.Enable(False),0
self.SetSizer(self.sizer),0
"self.sizer.Add(self.sizer, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=15)",0
self.ok.Enable(False),0
else:,0
self.ok.Enable(True),0
For mac compatibility,0
Remove the pages in case the user goes back to the create new project and creates/load a new project,0
Add all the other pages,0
"page10 = Refine_labels(self.parent, self.gui_size, self.cfg, page5)",0
"self.parent.AddPage(page10, ""OPT: Refine Labels"")",0
self.yes.Enable(False),0
self.no.Enable(False),0
Here we create a panel and a notebook on the panel,0
create the page windows as children of the notebook and add the pages to the notebook with the label to show on the tab,0
Take into consideration imprecise OpenCV estimation of total number of frames,0
Automatically disable the draggable points,0
Save additional frames to the labeled-data directory,0
Store the newly-refined data,0
Uncertain keypoints are ignored,0
Merge with the already existing annotated data,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.buttonCounter = [],0
self.bodyparts = [],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
load labeledPoints and fundamental Matrix,0
Find offset terms for drawing epipolar Lines,0
Get crop params for camera being labeled,0
Get crop params for other camera,0
convert the image to RGB as you are showing the image with matplotlib,0
draw epipolar lines,0
##################################################################################################################################################,0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
variable initialization,0
design the panel,0
Add logo of DLC,0
self.filelist = self.filelist + self.vids,0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
variable initialization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initialization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
## train and go ###,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
load labeledPoints and fundamental Matrix,0
Find offset terms for drawing epipolar Lines,0
Get crop params for camera being labeled,0
Get crop params for other camera,0
convert the image to RGB as you are showing the image with matplotlib,0
draw epipolar lines,0
divider = make_axes_locatable(self.axes),0
"colorIndex = np.linspace(np.min(im),np.max(im),len(bodyparts))",0
"cax = divider.append_axes(""right"", size=""5%"", pad=0.05)",0
"cbar = self.figure.colorbar(ax, cax=cax,spacing='proportional', ticks=colorIndex)",0
cbar.set_ticklabels(bodyparts[::-1]),0
self.figure.canvas.draw(),0
##################################################################################################################################################,0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"MainFrame.saveDataSet(self, event)",0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
checks for unique bodyparts,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Create an empty data frame,0
Cache original bodyparts,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Rearrange bodypart columns in config order,0
Test whether there are missing frames and superfluous data,0
Check whether new individuals or body parts were added,0
Checking if user added a new label,0
Found new labels in either multiple bodyparts or unique bodyparts,0
check if single is selected when radio buttons are changed,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Discard data associated with bodyparts that are no longer in the config,0
Re-organize the dataframe so the CSV looks consistent with the config,0
restore the background region,0
variable initialization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.build = wx.Button(self, label=""Build skeleton"")",0
"sizer.Add(self.build, pos=(4, 3), flag=wx.BOTTOM | wx.RIGHT, border=10)",0
"self.build.Bind(wx.EVT_BUTTON, self.build_skeleton)",0
self.build.Enable(True),0
"def build_skeleton(self, event):",0
skeleton.SkeletonBuilder(self.config),0
Setting the GUI size and panels design,0
This sets the minimum size of the GUI. It can scale now!,0
Checks if zoom/pan button is ON,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
variable initialization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"trainingindex = wx.StaticBox(self, label=""Specify the trainingset index"")",0
"trainingindex_boxsizer = wx.StaticBoxSizer(trainingindex, wx.VERTICAL)",0
"self.trainingindex = wx.SpinCtrl(self, value=""0"", min=0, max=100)",0
trainingindex_boxsizer.Add(,0
"self.trainingindex, 1, wx.EXPAND | wx.TOP | wx.BOTTOM, 10)",0
use the default pose_cfg file for default values,0
self.display_iters.Enable(False),0
self.save_iters.Enable(False),0
self.max_iters.Enable(False),0
self.snapshots.Enable(False),0
"hbox1.Add(trainingindex_boxsizer, 10, wx.EXPAND | wx.TOP | wx.BOTTOM, 5)",0
self.trainingindex.Enable(False),0
self.trainingindex.Enable(True),0
self.trainingindex.Enable(True),0
Read the pose config file,0
print(trainFraction[-1]),0
"print(os.path.join(cfg['project_path'],auxiliaryfunctions.get_model_folder(trainFraction, self.shuffles.GetValue(),cfg),'train','pose_cfg.yaml'))",0
let the user open the file with default text editor. Also make it mac compatible,0
update the variables with the edited values in the pose config file,0
self.trainingindex.Enable(True),0
variable initialization,0
design the panel,0
Add logo of DLC,0
### BOX 1 ####,0
### BOX 2 ####,0
### BOX 3 ####,0
Read the infer config file,0
"trainFraction = cfg[""TrainingFraction""][trainingsetindex]",0
let the user open the file with default text editor. Also make it mac compatible,0
"self.select_destfolder.SetPath(""None"")",0
self.config = [],0
"self.sel_config.SetPath("""")",0
variable initialization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"trainingindex_box = wx.StaticBox(self, label=""Specify the trainingset index"")",0
"trainingindex_boxsizer = wx.StaticBoxSizer(trainingindex_box, wx.VERTICAL)",0
"self.trainingindex = wx.SpinCtrl(self, value=""0"", min=0, max=100)",0
trainingindex_boxsizer.Add(,0
"self.trainingindex, 0, wx.EXPAND | wx.TOP | wx.BOTTOM, 10",0
),0
"self.hbox2.Add(trainingindex_boxsizer, 10, wx.EXPAND | wx.TOP | wx.BOTTOM, 5)",0
trainindex = self.trainingindex.GetValue(),0
"self.shuffles.SetValue(""1"")",0
"DownSampleVideo(vname,width=-1,height=200,outsuffix='downsampled',outpath=None,rotatecw=False):",0
variable initialization,0
design the panel,0
Add logo of DLC,0
"majorDimension=0,",0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
"self.choiceBox,self.visualization_rdb = self.choice_panel.addRadioButtons()",0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
Overwrite the config-defined individual names,0
with those actually present in the annotated data,0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
print(self.Dataframe.head()),0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
fix move to corner,0
fix move to corner,0
fix move to corner,0
fix move to corner,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
variable initialization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"trainingset = wx.StaticBox(self, label=""Specify the trainingset index"")",0
"trainingset_boxsizer = wx.StaticBoxSizer(trainingset, wx.VERTICAL)",0
"self.trainingset = wx.SpinCtrl(self, value=""0"", min=0, max=100)",0
trainingset_boxsizer.Add(,0
"self.trainingset, 1, wx.EXPAND | wx.TOP | wx.BOTTOM, 10",0
),0
"self.hbox1.Add(trainingset_boxsizer, 5, wx.EXPAND | wx.TOP | wx.BOTTOM, 5)",0
"if self.plot_scoremaps.GetStringSelection() == ""Yes"":",0
Read the infer config file,0
trainingsetindex = self.trainingset.GetValue(),0
let the user open the file with default text editor. Also make it mac compatible,0
shuffle = self.shuffle.GetValue(),0
trainingsetindex = self.trainingset.GetValue(),0
"trainingsetindex=trainingsetindex,",0
variable initialization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
## LABELING ###,0
"self.sel_config.SetPath("""")",0
##################################################################################################################################################,0
Splitting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
self.cfg = auxiliaryfunctions.read_config(config),0
"self.statusbar.SetStatusText("""")",0
"dlg = wx.MessageDialog(None,""Are you sure?"", ""Quit!"",wx.YES_NO | wx.ICON_WARNING)",0
result = dlg.ShowModal(),0
if result == wx.ID_YES:,0
variable initialization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0, 4), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=5)",0
variable initialization,0
design the panel,0
Add logo of DLC,0
#         design the panel,0
Add image of DLC,0
if editing this text make sure you add the '\n' to get the new line. The sizer is unable to format lines correctly.,0
"calling create_tracking_dataset, train_tracking_transformer, stitch_tracklets",0
if animal == 'pup' and anchor_coord.shape[0]!=5:,0
continue,0
1) reference to video folder and get the proper bpt_feature file for feature table,0
2) get either the path to gt or the path to track pickle,0
tracklets,0
"with npy list form videos, split each to train and test",0
"video_name = '.'.join(video.split(""/"")[-1].split(""."")[:-1])",0
assuming there is only one match,0
make my own model factory,0
make my own loss factory,0
"NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights",0
"NOTE: drop path for stochastic depth, we shall see if this is better than dropout here",0
only channel pooling,0
"x -> (B, 12, 2048)",0
"x -> (B, 12, 768)",0
trying to keep everything simple,0
need to write my own keypoint embedding,0
we don't need a fc layer here,0
x: inputs,0
"(B, 12, 768)",0
added pos embed,0
remove all parts I don't understand,0
forwarding through blocks. But num of blocks should be reduced,0
i can't really tell what the difference is,0
will have to change some of these,0
For old models that I trained prior to conv based patchification,0
To resize pos embedding when using model at different size from pretrained weights,0
Rescale the grid of position embeddings when loading from state_dict. Adapted from,0
https://github.com/google-research/vision_transformer/blob/00883dd691c63a6830751563748663526e811cee/vit_jax/checkpoint.py#L224,0
Cut & paste from PyTorch official master until it's in a few official releases - RW,0
Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf,0
Computes standard normal cumulative distribution function,0
Values are generated by using a truncated uniform distribution and,0
then using the inverse CDF for the normal distribution.,0
Get upper and lower cdf values,0
"Uniformly fill tensor with values from [l, u], then translate to",0
"[2l-1, 2u-1].",0
Use inverse cdf transform for normal distribution to get truncated,0
standard normal,0
"Transform to proper mean, std",0
Clamp to ensure it's in the proper range,0
"type: (Tensor, float, float, float, float) -> Tensor",0
"resample if noise out of percent limit, brute force but shouldn't spin much",0
distmat g,0
q    1 3 2 4,0
4 1 2 3,0
0 2 1 3,0
1 2 3 0,0
compute cmc curve for each query,0
get query pid and camid,0
remove gallery samples that have the same pid and camid with query,0
compute cmc curve,0
"binary vector, positions with value 1 are correct matches",0
this condition is true when query identity does not appear in gallery,0
compute average precision,0
reference: https://en.wikipedia.org/wiki/Evaluation_measures_(information_retrieval)#Average_precision,0
"tmp_cmc = [x / (i + 1.) for i, x in enumerate(tmp_cmc)]",0
query,0
gallery,0
"distmat = re_ranking(qf, gf, k1=20, k2=6, lambda_value=0.3)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
"if feature vector is numpy, you should use 'torch.tensor' transform it to tensor",0
print('using GPU to compute original distance'),0
print('starting re_ranking'),0
k-reciprocal neighbors,0
More elegantly one can simply define:,0
take care of difference between feature map space and original image space,0
cos = torch.cdist,0
TODO: maybe find a better spot for this.,1
"dev =  device if torch.cuda.is_available() else ""cpu""",0
train,0
print (f'validation loss {val_loss/len(val_loader)}'),0
normalize vectors here,0
i th vector at j th kpt,0
maybe needs to convert them to embeddings and position token,0
Drop missing body parts,0
Drop points lying outside the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
## dlcnet_ms5: backbone resnet50 + multi-fusion & multi-stage module,0
## dlcr101_ms5/dlcr152_ms5: backbone resnet101/152 + multi-fusion & multi-stage module,0
"If the graph is unnecessarily large (with 15+ keypoints by default),",0
we randomly prune it to a size guaranteeing an average node degree of 6;,0
"see Suppl. Fig S9c in Lauer et al., 2022.",0
Use the skeleton defined in the config file,0
Ignore possible connections between 'multi' and 'unique' body parts;,0
one can never be too careful...,0
Disable the prediction of PAFs if the graph is empty,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving metadata and data file (Pickle file),0
###############################################################################,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Setting inference cfg file:,0
Search a pose_config.yaml file to parse missing information,0
Clean the training-datasets folder prior to recreating the data pickles,0
Load updated lists:,0
Find the corresponding video file,0
"print(key, ""dropping?"")",0
adding important values for multianiaml project:,0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Determine the index length required to guarantee,0
the train–test ratio is exactly the desired one.,0
Pad indices so lengths agree,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(name,test_video_name)",0
Blazing fast and does not load the image into memory,0
Check that points lie within the image,0
DeprecationWarnings are silenced since Python 3.2 unless triggered in __main__,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
updating variable if null/None! #backwardscompatability,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
these values are dropped as scalecrop,0
doesn't have rotation implemented,0
Also drop maDLC smart cropping augmentation parameters,0
sort the model directories,0
get the shuffle index and offset by 1.,0
read cfg file,0
create log file,0
Check for variable correctness,0
Moviepy:,0
select crop method,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
allow_growth must be true here because tensorflow does not automatically free gpu memory and setting it as false occupies all gpu memory so that pytorch cannot kick in,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
should close tensorflow session here in order to free gpu,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
TODO: add cropping as in video analysis!,1
if cropping is not None:,0
cfg['cropping']=True,0
"cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping",0
"print(""Overwriting cropping parameters:"", cropping)",0
"print(""These are used for all videos, but won't be save to the cfg file."")",0
"Also ensure `boundingboxslack` is greater than zero, otherwise overlap",0
"between trackers cannot be evaluated, resulting in empty tracklets.",0
Check which snapshots are available and sort them by # iterations,0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
NOTE: If dataname line above is changed then line below is obsolete?,0
"trackname = trackname.replace(videofolder, destfolder)",0
TODO: adjust this for multi + unique bodyparts!,1
this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. ),0
Optimal identity assignment based on soft voting,0
imagenet mean for resnet pretraining:,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/AlexEMG/DeepLabCut/pull/324",0
for DLC 2.2. (here all set False to not use PAFs/pairwise fields),0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
"reloading defaults, as they can bleed over from a previous run otherwise",0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
dlc_cfg_train = load_config(str(path_train_config)),0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
Name for deeplabcut net (based on its parameters),0
"DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])",0
"print(""Extracting maps for "", DLCscorer, "" with # of trainingiterations:"", trainingsiterations)",0
if notanalyzed: #this only applies to ask if h5 exists...,0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
"for multi animal, seems only this is used",0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Get animal IDs and corresponding indices in the arrays of detections,0
Form ground truth beforehand,0
Assemble animals on the full set of detections,0
Count the number of unassembled bodyparts,0
Handle unlabeled bodyparts...,0
"Only 1 animal, let us return the full graph indices only",0
Find minimal skeleton,0
Select optimal PAF graph,0
Selective copy; deepcopy is >5x slower,0
Only keeps skeletons that are more than 90% complete,0
TODO Normalize dists by longest length?,1
TODO Smarter imputation technique (Bayesian? Grassmann averages?),1
Covariance matrix estimation fails due to numerical singularities,0
Distance is undefined if the assembly is empty,0
Correct distance to account for missing observations,0
"Alternatively, reduce contribution of missing values to the Mahalanobis",0
distance to zero by substituting the corresponding means.,0
"Fill the subsets with unambiguous, complete individuals",0
Fuse superfluous assemblies,0
Second pass without edge safety,0
Mark the unique body parts as assembled anyway so,0
they are not used later on to fill assemblies.,0
Store selected edges for subsequent frames,0
Remove invalid assemblies,0
Fill assemblies with unconnected body parts,0
Spawning (rather than forking) multiple processes does not,0
work nicely with the GUI or interactive sessions.,0
"In that case, we fall back to the serial assembly.",0
Swap corresponding keypoints,0
Only consider assemblies of at least two keypoints,0
Greedy assembly matching like in pycocotools,0
Global rather than greedy assembly matching,0
Cast columns of dtype 'object' to float to avoid TypeError,0
further down in _parse_ground_truth_data.,0
sigma is taken as the median of all COCO keypoint standard deviations,0
Sort matching score (OKS) in descending order of assembly affinity,0
Guarantee precision decreases monotonically,0
See https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173),0
Regularize by forcing AR <= 5,0
max_ar = 5,0
if el.aspect_ratio >= max_ar:,0
if el.height > el.width:,0
el.width = el.height / max_ar,0
else:,0
el.height = el.width / max_ar,0
Orient the ellipse such that it encompasses most points,0
"n_inside = el.contains_points(np.c_[self.x, self.y]).sum()",0
el.theta += 0.5 * np.pi,0
"if el.contains_points(np.c_[self.x, self.y]).sum() < n_inside:",0
el.theta -= 0.5 * np.pi,0
"r2 = chi2.ppf(2 * norm.cdf(sd) - 1, 2)",0
"height, width = np.sqrt(E * r2)",0
The general quadratic curve has the form:,0
ax^2 + 2bxy + cy^2 + 2dx + 2fy + g = 0,0
Ellipse center coordinates,0
Semi-axes lengths,0
Angle to the horizontal,0
High uncertainty to the unobservable initial velocities,0
Give high uncertainty to the unobservable initial velocities,0
diff = val - cost_matrix,0
"diff[row, col] += val",0
if (,0
val < self.iou_threshold,0
or np.any(diff[row] <= 0.2),0
"or np.any(diff[:, col] <= 0.2)",0
):,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
Modified from scipy source code:,0
- to restrict its use to 2D,0
- to get rid of shuffling (since arrays are only (nbodyparts * 3) element long),0
TODO - factor in keypoint confidence (and weight by # of observations??),1
"mat = self.calc_pairwise_oks(poses, poses_ref)",0
Remove matched detections with low OKS,0
matches = [],0
"for row, col in zip(row_indices, col_indices):",0
"if mat[row, col] < self.oks_threshold:",0
unmatched_poses.append(row),0
unmatched_trackers.append(col),0
else:,0
"matches.append([row, col])",0
if not len(matches):,0
"matches = np.empty((0, 2), dtype=int)",0
else:,0
matches = np.stack(matches),0
update matched trackers with assigned detections,0
create and initialise new trackers for unmatched detections,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
filter out matched with low IOU,0
TODO: get cropping parameters and utilize!,1
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Load meta data & annotations,0
get the graph!,0
"NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*",0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
TODO: Make this code not so redundant!,1
"If a string was passed in, auto-convert to True for backward compatibility",0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to initial folder,0
"# Also storing one ""large"" table with results:",0
note: evaluationfolder.parents[0] to get common folder above all shuffle evaluations.,0
#################################################,0
Load data...,0
#################################################,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
TODO: IMPLEMENT for different batch sizes?,1
Ignore best edges possibly defined during a prior evaluation,0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
Pass the image and the keypoints through the resizer;,0
this has no effect if no augmenters were added to it.,0
FIXME Is having an empty array vs nan really that necessary?!,1
"Form 2D array of shape (n_rows, 4) where the last dimension",0
"is (sample_index, peak_y, peak_x, bpt_index) to slice the PAFs.",0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in,0
Compute all distance statistics,0
Calculate overall prediction error,0
Skip data-driven skeleton selection unless,0
the model was trained on the full graph.,0
"loading backbone from ResNet, MobileNet etc.",0
"display_iters = max(1,int(displayiters))",0
"[_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})",0
Save snapshot,0
return to original path.,0
Clip peak locations to PAFs dimensions,0
"unit_vecs = vecs / lengths[:, np.newaxis]",0
"affinities = np.squeeze(y @ np.expand_dims(unit_vecs, axis=2)).sum(axis=1)",0
Form cost matrices,0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
Convert a frozen graph to OpenVINO IR format,0
Read network into memory,0
"For better efficiency, model is initialized for batch_size 1 and every sample processed independently",0
Load network to device,0
Prepare input data,0
Change the stride from 2 to 1 to get 16x downscaling instead of 32x.,0
https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142,0
Multi stage is currently only implemented for resnets,0
Defining multi_fusion backbone,0
Attaching multi-stage decoder,0
stage_paf_output = stage_paf_output + pre_stage_paf_output,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
FIXME Fix wrong scope with Keras layers,1
"def prediction_layer(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2DTranspose(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=2,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
## New DLCNet Addition: multi-stage decoder,0
"def prediction_layer_stage(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2D(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=1,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Compute variance using: Var[X]= E[X^2] - E[X]^2.,0
Compute keep_prob,0
TODO(tanmingxing): add support for training progress.,1
Compute drop_connect tensor,0
turn into x times y time bs * bpts,0
extract corresponding locref x and y as well as probability,0
Filter predicted heatmaps with a 2D Gaussian kernel as in:,0
https://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_The_Devil_Is_in_the_Details_Delving_Into_Unbiased_Data_CVPR_2020_paper.pdf,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
defaults will be a public argument for namedtuple in Python 3.7,0
https://docs.python.org/3/library/collections.html#collections.namedtuple,0
Make sure that round down does not go down by more than 10%.,0
Builds the block accordings to arguments.,0
Expansion phase:,0
Depth-wise convolution phase:,0
Squeeze and Excitation layer.,0
Output phase:,0
only apply drop_connect if skip presents.,0
Expansion phase:,0
Output phase:,0
only apply drop_connect if skip presents.,0
Builds blocks.,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Stem part.,0
Head part.,0
Calls Stem layers,0
Calls blocks.,0
Calls final layers and returns logits.,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"(width_coefficient, depth_coefficient, resolution, dropout_rate)",0
blocks_args = [,0
"'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',",0
"'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',",0
"'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',",0
"'r1_k3_s11_e6_i192_o320_se0.25',",0
],0
The default is TPU-specific batch norm.,0
The alternative is tf.layers.BatchNormalization.,0
"batch_norm=utils.TpuBatchNormalization,  # TPU-specific requirement.",0
ValueError will be raised here if override_params has fields not included,0
in global_params.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrunk when given a depth-multiplier < 1.0.,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
Clamp to 40% of crop size to ensure that at least,0
the center keypoint remains visible after the offset is applied.,0
Points located close to one another are sampled preferentially,0
in order to augment crowded regions.,0
Include keypoints in the count to avoid null probabilities,0
Shift the crop center in both dimensions by random amounts,0
and normalize to the original image dimensions.,0
Pickle the 'data' dictionary using the highest protocol available.,0
item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal,0
"Add smart, keypoint-aware image cropping",0
Scale is sampled only once (per batch) to transform all of the images into same size.,0
Discard keypoints whose coordinates lie outside the cropped image,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
"imageio.imwrite(data_items[i].im_path.split('/')[-1],im)",0
Produce score maps and location refinement fields,0
Find indices of individuals in joint_id,0
WIP!,0
STD of gaussian is 1/4 of threshold,0
Grid of coordinates,0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Distance between the joint point and each coordinate,0
NEEDS TO BE DONE!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
TODO Finish implementing actual abstract class,1
parameter initialization for augmentation pipeline:,0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images of a batch into same size.,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"First, initializing variables (if they don't exist)",0
what is the fraction of training samples with scaling augmentation?,0
loading defaults for rotation range!,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"cfg[""rotateratio""] = cfg.get(",0
"""rotratio"", 0.4",0
)  # what is the fraction of training samples with rotation augmentation?,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"Auto cropping is new (was not in Nature Neuroscience 2018 paper, but introduced in Nath et al. Nat. Protocols 2019)",0
"and boosts performance by 2X, particularly on challenging datasets, like the cheetah in Nath et al.",0
Parameters for augmentation with regard to cropping:,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
Get rid of the interpolation beyond the spline knots,0
Retrieve original individual label indices,0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Check the training image paths are correctly stored as arrays of strings,0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
TENSORPACK could fail in WINDOWS...,0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
# Create one identical splits for 3 networks and 3 augmentations,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
Setting specific parameters for training,0
"MODELS = [""dlcrnet_ms5"", ""dlcr101_ms5"", ""efficientnet-b0"", ""mobilenet_v2_0.35""]",0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
## adding it here,0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Downloading the calibration images,0
Deleting unnecessary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
example how to set pose config variables:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
test if existing models are found:,0
Test if nonexisting models are not found,0
Add some other office files:,0
Add a .pickle and .h5 files,0
"By default, all videos with common extensions are taken from a directory",0
A list of extensions can also be passed in,0
Test bounding box from a single keypoint,0
Test that shape was preserved after vectorization,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Guarantee that images smaller than crop size are handled fine,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Disable stochastic scale jitter,0
Verify the aspect ratio is preserved,0
Load sample multi animal data,0
Generate a random permutation and reorder data. Ignore the unique bodypart,0
Get inverse permutation and reorder the modified data to get back,0
to the original,0
Check,0
"Check data comprise path, shape, and xy coordinates",0
Break the graph to test stitching failure,0
Add fake IDs,0
Split all tracklets in half,0
Test flipped keypoints,0
Original (cached) coordinates must have remained empty,0
Assemble based on the smallest graph to speed up testing,0
Generate fake identity predictions,0
Assemble based on the smallest graph to speed up testing,0
Test now with identity only and ensure assemblies,0
contain only parts of a single group ID.,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
def install(package):,0
"subprocess.check_call([sys.executable, ""-m"", ""pip"", ""install"", package])",0
"install(""tensorflow==1.13.1"")",0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
quicker variant,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
Suppress tensorflow warning messages,0
DLClight version does not support GUIs. Importing accordingly,0
"Train, evaluate & predict functions / all require TF",0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Updating config file:,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path, num_shuffles=auxfun_models.Check4weights(pose_cfg['net_type'], parent_path, num_shuffles= 1)",0
Updating training and test pose_cfg:,0
Read the config file,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
common parameters:,0
Write dictionary to yaml  config file,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
Flatten the list of bodyparts to connect,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Start plotting for every frame,0
"Once all the frames are saved, then make a movie using ffmpeg.",0
Create the fig and define the axes,0
Clear plot and initialize the variables,0
Initialize arrays for appending the 3d data for actual plotting the points,0
Initialize arrays for appending the 3d data for drawing the lines,0
Initialize arrays for appending the 2d data from cam1 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam2 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam1 for drawing the lines,0
Initialize arrays for appending the 2d data from cam2 for drawing the lines,0
Get the scorer names from the dataframe,0
"Set the x,y, and z limits for the 3d view",0
"Set the frame number to read#max(0,index-trailpoints):index",0
Plot the labels for each body part,0
Connecting the bodyparts specified in the config file.3d file is created based on the likelihoods of cam1 and cam2. Using 3d file and check if the body part is nan then dont plot skeleton,0
Saving the frames,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
Extract the indices of frames where the likelihood of a bodypart for both cameras are less than pvalue,0
Extract frames where likelihood for both the views is less than the pcutoff,0
"low_likelihood_frames = np.all(likelihoods < pcutoff, axis=1)",0
For cam1 camera: Assign nans to x and y values of a bodypart where the likelihood for is less than pvalue,0
For cam2 camera: Assign nans to x and y values of a bodypart where the likelihood is less than pvalue,0
ToDo: speed up func. below by saving in numpy.array,1
"Create an empty dataframe to store x,y,z of 3d data",0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Undistorting the points from cam2 camera,0
Save the undistorted files,0
Alternatively,0
Sort tracklets by length to prioritize greater continuity,0
Drop tracklets that are too short,0
"Where slots are available, copy the data over",0
"If about to overwrite data, keep tracklets with highest confidence",0
Squeeze some data into empty slots,0
"For the remaining data, overwrite where we are least confident",0
Find keypoints closest to the remaining data,0
Use Manhattan distance to avoid overflow,0
Only overwrite if improving confidence,0
Fill existing gaps,0
Retrieve original individual label indices,0
Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.,0
Fill existing gaps,0
Retrieve original individual label indices,0
ID swaps occur when X and Y simultaneously intersect each other.,0
Get only those bodypart pairs that belong to different individuals,0
"See Li et al., 2012. Cross-view Activity Recognition using Hankelets.",0
"As proposed in the paper, the Hankel matrix can either be formed from",0
the tracklet's centroid or its normalized velocity.,0
"vel = np.diff(self.centroid, axis=0)",0
"vel /= np.linalg.norm(vel, axis=1, keepdims=True)",0
return self.hankelize(vel),0
TODO Add missing data imputation,1
"nrows, ncols = mat.shape",0
beta = nrows / ncols,0
omega = 0.56 * beta ** 3 - 0.95 * beta ** 2 + 1.82 * beta + 1.43,0
return np.argmin(s > omega * np.median(s)),0
"Note that if tracklets are very short, some may actually be part of the same track",0
and thus incorrectly reflect separate track endpoints...,0
"Map each Tracklet to an entry and output nodes and vice versa,",0
which is convenient once the tracklets are stitched.,0
Store tracklets and corresponding negatives (those that overlap in time),0
TODO Avoid looping over all pairs of tracklets,0
Pick the closest (spatially) overlapping tracklet,0
The algorithm works better with integer weights,0
Let us prune the graph by removing all source and sink edges,0
but those connecting the `n_tracks` first and last tracklets.,0
Preflow push seems to work slightly better than shortest,0
"augmentation path..., and is more computationally efficient.",0
Verify whether there are overlapping tracklets,0
"Pick the segment that minimizes ""smoothness"", computed here",0
with the coefficient of variation of the differences.,0
Rebuild a full graph from the remaining nodes without,0
temporal constraint on what tracklets can be stitched together.,0
Cycle through the residuals and incorporate back those,0
that only fit in a single tracklet.,0
Greedily add the remaining residuals,0
Refresh temporal bounds,0
Default to the distance cost function,0
Add in identity weighing before building the graph,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
average distance and average # significant differences avg. over comparisonbodyparts,0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
###################################################,0
Dependencies,0
###################################################,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as,0
specified in the config file,0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
Prepare figure,0
Mask rather than drop unwanted bodyparts to ensure consistent coloring,0
cropping!,0
nx=X2-X1,0
!/usr/bin/env python3,0
from deeplabcut.utils.auxfun_videos import imread,0
"auxfun_videos.imread(image_path, mode=""skimage"")",0
The above already raises a ValueError if formatting is wrong,0
"Rotate, see: https://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg",0
interesting option to just update metadata.,0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
Read the video until a frame is successfully read,0
import platform,0
"if platform.system() == ""Darwin"":  # for OSX use WXAgg",0
fig.canvas.start_event_loop(timeout=-1),0
else:,0
#fig.canvas.stop_event_loop(),0
"Windows throws error if file path is > 260 characters, can fix with prefix.",0
See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation,0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
check if it exists:,0
"Attention this order has to be consistent (for training set creation, training, inference etc.)",0
"-> adding (single,bpt) for uniquebodyparts",0
"-> adding (indivdual,bpt) for multianimalbodyparts",0
load defaults,0
set project specific parameters:,0
###################################################,0
Dependencies,0
###################################################,0
Pose X vs pose Y,0
Poses vs time,0
Likelihoods,0
Histograms,0
#################################################,0
Looping analysis over video,0
#################################################,0
Keep only the individuals and bodyparts that were labeled,0
Either display the animals defined in the config if they are found,0
"in the dataframe, or all the trajectories regardless of their names",0
Determine whether the data are single- or multi-animal without loading into memory,0
simply by checking whether 'individuals' is in the second line of the CSV.,0
Make paths platform-agnostic if they are not already,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"if ""MULTI!"" in allbpts:",0
Ensure same order as in config.yaml,0
ABBREVIATE NETWORK NAMES -- esp. for mobilenet!,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
Iterate over data files and stop as soon as one matching the scorer is found,0
If there was no match...,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"trainingset = wx.StaticBox(self, label=""Specify the trainingset index"")",0
"trainingset_boxsizer = wx.StaticBoxSizer(trainingset, wx.VERTICAL)",0
"self.trainingset = wx.SpinCtrl(self, value=""0"", min=0, max=100)",0
trainingset_boxsizer.Add(,0
"self.trainingset, 1, wx.EXPAND | wx.TOP | wx.BOTTOM, 10",0
),0
"hbox_.Add(trainingset_boxsizer, 5, wx.EXPAND | wx.TOP | wx.BOTTOM, 5)",0
Read the infer config file,0
trainingsetindex = self.trainingset.GetValue(),0
let the user open the file with default text editor. Also make it mac compatible,0
trainingsetindex = self.trainingset.GetValue(),0
"trainingsetindex=trainingsetindex,",0
"trainingsetindex=self.trainingset.GetValue(),",0
self.save.Enable(False),0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
topSplitter = wx.SplitterWindow(self),0
,0
"self.image_panel = ImagePanel(topSplitter, config,video,shuffle,Dataframe,self.gui_size)",0
self.widget_panel = WidgetPanel(topSplitter),0
,0
"topSplitter.SplitHorizontally(self.image_panel, self.widget_panel,sashPosition=self.gui_size[1]*0.83)#0.9",0
topSplitter.SetSashGravity(1),0
sizer = wx.BoxSizer(wx.VERTICAL),0
"sizer.Add(topSplitter, 1, wx.EXPAND)",0
self.SetSizer(sizer),0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
"cv2.imwrite(img_name, frame)",0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0,10), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=10)",0
Add all the options,0
line = wx.StaticLine(self),0
"self.sizer.Add(line, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=10)",0
,0
"self.multi_choice.Bind(wx.EVT_CHECKBOX,self.activate_copy_videos)",0
Hide the button as this is not the default option,0
self.vids.Enable(False),0
self.SetSizer(self.sizer),0
"self.sizer.Add(self.sizer, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=15)",0
self.ok.Enable(False),0
else:,0
self.ok.Enable(True),0
For mac compatibility,0
Remove the pages in case the user goes back to the create new project and creates/load a new project,0
Add all the other pages,0
"page10 = Refine_labels(self.parent, self.gui_size, self.cfg, page5)",0
"self.parent.AddPage(page10, ""OPT: Refine Labels"")",0
self.yes.Enable(False),0
self.no.Enable(False),0
Here we create a panel and a notebook on the panel,0
create the page windows as children of the notebook and add the pages to the notebook with the label to show on the tab,0
Take into consideration imprecise OpenCV estimation of total number of frames,0
Automatically disable the draggable points,0
Save additional frames to the labeled-data directory,0
Store the newly-refined data,0
Uncertain keypoints are ignored,0
Merge with the already existing annotated data,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.buttonCounter = [],0
self.bodyparts = [],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
load labeledPoints and fundamental Matrix,0
Find offset terms for drawing epipolar Lines,0
Get crop params for camera being labeled,0
Get crop params for other camera,0
convert the image to RGB as you are showing the image with matplotlib,0
draw epipolar lines,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
variable initilization,0
design the panel,0
Add logo of DLC,0
self.filelist = self.filelist + self.vids,0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
load labeledPoints and fundamental Matrix,0
Find offset terms for drawing epipolar Lines,0
Get crop params for camera being labeled,0
Get crop params for other camera,0
convert the image to RGB as you are showing the image with matplotlib,0
draw epipolar lines,0
divider = make_axes_locatable(self.axes),0
"colorIndex = np.linspace(np.min(im),np.max(im),len(bodyparts))",0
"cax = divider.append_axes(""right"", size=""5%"", pad=0.05)",0
"cbar = self.figure.colorbar(ax, cax=cax,spacing='proportional', ticks=colorIndex)",0
cbar.set_ticklabels(bodyparts[::-1]),0
self.figure.canvas.draw(),0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"MainFrame.saveDataSet(self, event)",0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
checks for unique bodyparts,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Create an empty data frame,0
Cache original bodyparts,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Rearrange bodypart columns in config order,0
Test whether there are missing frames and superfluous data,0
Check whether new individuals or body parts were added,0
Checking if user added a new label,0
Found new labels in either multiple bodyparts or unique bodyparts,0
check if single is slected when radio buttons are changed,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Discard data associated with bodyparts that are no longer in the config,0
Re-organize the dataframe so the CSV looks consistent with the config,0
restore the background region,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.build = wx.Button(self, label=""Build skeleton"")",0
"sizer.Add(self.build, pos=(4, 3), flag=wx.BOTTOM | wx.RIGHT, border=10)",0
"self.build.Bind(wx.EVT_BUTTON, self.build_skeleton)",0
self.build.Enable(True),0
"def build_skeleton(self, event):",0
skeleton.SkeletonBuilder(self.config),0
Settting the GUI size and panels design,0
Checks if zoom/pan button is ON,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"trainingindex = wx.StaticBox(self, label=""Specify the trainingset index"")",0
"trainingindex_boxsizer = wx.StaticBoxSizer(trainingindex, wx.VERTICAL)",0
"self.trainingindex = wx.SpinCtrl(self, value=""0"", min=0, max=100)",0
trainingindex_boxsizer.Add(,0
"self.trainingindex, 1, wx.EXPAND | wx.TOP | wx.BOTTOM, 10)",0
use the default pose_cfg file for default values,0
self.display_iters.Enable(False),0
self.save_iters.Enable(False),0
self.max_iters.Enable(False),0
self.snapshots.Enable(False),0
"hbox1.Add(trainingindex_boxsizer, 10, wx.EXPAND | wx.TOP | wx.BOTTOM, 5)",0
self.trainingindex.Enable(False),0
self.trainingindex.Enable(True),0
self.trainingindex.Enable(True),0
Read the pose config file,0
print(trainFraction[-1]),0
"print(os.path.join(cfg['project_path'],auxiliaryfunctions.GetModelFolder(trainFraction, self.shuffles.GetValue(),cfg),'train','pose_cfg.yaml'))",0
let the user open the file with default text editor. Also make it mac compatible,0
update the variables with the edited values in the pose config file,0
self.trainingindex.Enable(True),0
variable initilization,0
design the panel,0
Add logo of DLC,0
### BOX 1 ####,0
### BOX 2 ####,0
### BOX 3 ####,0
Read the infer config file,0
"trainFraction = cfg[""TrainingFraction""][trainingsetindex]",0
let the user open the file with default text editor. Also make it mac compatible,0
"self.select_destfolder.SetPath(""None"")",0
self.config = [],0
"self.sel_config.SetPath("""")",0
"self.videotype.SetStringSelection("".avi"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"trainingindex_box = wx.StaticBox(self, label=""Specify the trainingset index"")",0
"trainingindex_boxsizer = wx.StaticBoxSizer(trainingindex_box, wx.VERTICAL)",0
"self.trainingindex = wx.SpinCtrl(self, value=""0"", min=0, max=100)",0
trainingindex_boxsizer.Add(,0
"self.trainingindex, 0, wx.EXPAND | wx.TOP | wx.BOTTOM, 10",0
),0
"self.hbox2.Add(trainingindex_boxsizer, 10, wx.EXPAND | wx.TOP | wx.BOTTOM, 5)",0
trainindex = self.trainingindex.GetValue(),0
"self.shuffles.SetValue(""1"")",0
"DownSampleVideo(vname,width=-1,height=200,outsuffix='downsampled',outpath=None,rotatecw=False):",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"majorDimension=0,",0
"self.videotype.SetStringSelection("".avi"")",0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
"self.choiceBox,self.visualization_rdb = self.choice_panel.addRadioButtons()",0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
Overwrite the config-defined individual names,0
with those actually present in the annotated data,0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
print(self.Dataframe.head()),0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
fix move to corner,0
fix move to corner,0
fix move to corner,0
fix move to corner,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"trainingset = wx.StaticBox(self, label=""Specify the trainingset index"")",0
"trainingset_boxsizer = wx.StaticBoxSizer(trainingset, wx.VERTICAL)",0
"self.trainingset = wx.SpinCtrl(self, value=""0"", min=0, max=100)",0
trainingset_boxsizer.Add(,0
"self.trainingset, 1, wx.EXPAND | wx.TOP | wx.BOTTOM, 10",0
),0
"self.hbox1.Add(trainingset_boxsizer, 5, wx.EXPAND | wx.TOP | wx.BOTTOM, 5)",0
"if self.plot_scoremaps.GetStringSelection() == ""Yes"":",0
Read the infer config file,0
trainingsetindex = self.trainingset.GetValue(),0
let the user open the file with default text editor. Also make it mac compatible,0
shuffle = self.shuffle.GetValue(),0
trainingsetindex = self.trainingset.GetValue(),0
"trainingsetindex=trainingsetindex,",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
## LABELING ###,0
"self.sel_config.SetPath("""")",0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
self.cfg = auxiliaryfunctions.read_config(config),0
"self.statusbar.SetStatusText("""")",0
"dlg = wx.MessageDialog(None,""Are you sure?"", ""Quit!"",wx.YES_NO | wx.ICON_WARNING)",0
result = dlg.ShowModal(),0
if result == wx.ID_YES:,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0, 4), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=5)",0
variable initilization,0
design the panel,0
Add logo of DLC,0
#         design the panel,0
Add image of DLC,0
if editing this text make sure you add the '\n' to get the new line. The sizer is unable to format lines correctly.,0
Drop missing body parts,0
Drop points lying outside the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
## dlcnet_ms5: backbone resnet50 + multi-fusion & multi-stage module,0
## dlcr101_ms5/dlcr152_ms5: backbone resnet101/152 + multi-fusion & multi-stage module,0
Ignore possible connections between 'multi' and 'unique' body parts;,0
one can never be too careful...,0
Disable the prediction of PAFs if the graph is empty,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving metadata and data file (Pickle file),0
###############################################################################,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Setting inference cfg file:,0
Search a pose_config.yaml file to parse missing information,0
Clean the training-datasets folder prior to recreating the data pickles,0
Load updated lists:,0
Find the corresponding video file,0
"print(key, ""dropping?"")",0
adding important values for multianiaml project:,0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Determine the index length required to guarantee,0
the train–test ratio is exactly the desired one.,0
Pad indices so lengths agree,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(name,test_video_name)",0
Blazing fast and does not load the image into memory,0
Check that points lie within the image,0
DeprecationWarnings are silenced since Python 3.2 unless triggered in __main__,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
updating variable if null/None! #backwardscompatability,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
these values are dropped as scalecrop,0
doesn't have rotation implemented,0
sort the model directories,0
get the shuffle index and offset by 1.,0
read cfg file,0
create log file,0
Check for variable correctness,0
Moviepy:,0
select crop method,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
TODO: addd cropping as in video analysis!,1
if cropping is not None:,0
cfg['cropping']=True,0
"cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping",0
"print(""Overwriting cropping parameters:"", cropping)",0
"print(""These are used for all videos, but won't be save to the cfg file."")",0
"Also ensure `boundingboxslack` is greater than zero, otherwise overlap",0
"between trackers cannot be evaluated, resulting in empty tracklets.",0
Check which snapshots are available and sort them by # iterations,0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
NOTE: If dataname line above is changed then line below is obsolete?,0
"trackname = trackname.replace(videofolder, destfolder)",0
TODO: adjust this for multi + unique bodyparts!,1
this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. ),0
Optimal identity assignment based on soft voting,0
imagenet mean for resnet pretraining:,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/AlexEMG/DeepLabCut/pull/324",0
for DLC 2.2. (here all set False to not use PAFs/pairwise fields),0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
"reloading defaults, as they can bleed over from a previous run otherwise",0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
dlc_cfg_train = load_config(str(path_train_config)),0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
Name for deeplabcut net (based on its parameters),0
"DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])",0
"print(""Extracting maps for "", DLCscorer, "" with # of trainingiterations:"", trainingsiterations)",0
if notanalyzed: #this only applies to ask if h5 exists...,0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Get animal IDs and corresponding indices in the arrays of detections,0
Form ground truth beforehand,0
Assemble animals on the full set of detections,0
Count the number of unassembled bodyparts,0
Handle unlabeled bodyparts...,0
"Only 1 animal, let us return the full graph indices only",0
Find minimal skeleton,0
Select optimal PAF graph,0
Selective copy; deepcopy is >5x slower,0
Only keeps skeletons that are more than 90% complete,0
TODO Normalize dists by longest length?,1
TODO Smarter imputation technique (Bayesian? Grassmann averages?),1
Covariance matrix estimation fails due to numerical singularities,0
Distance is undefined if the assembly is empty,0
Correct distance to account for missing observations,0
"Alternatively, reduce contribution of missing values to the Mahalanobis",0
distance to zero by substituting the corresponding means.,0
"Fill the subsets with unambiguous, complete individuals",0
Fuse superfluous assemblies,0
Second pass without edge safety,0
Mark the unique body parts as assembled anyway so,0
they are not used later on to fill assemblies.,0
Store selected edges for subsequent frames,0
Remove invalid assemblies,0
Fill assemblies with unconnected body parts,0
Spawning (rather than forking) multiple processes does not,0
work nicely with the GUI or interactive sessions.,0
"In that case, we fall back to the serial assembly.",0
Swap corresponding keypoints,0
Only consider assemblies of at least two keypoints,0
sigma is taken as the median of all COCO keypoint standard deviations,0
Sort matching score (OKS) in descending order of assembly affinity,0
Guarantee precision decreases monotonically,0
See https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173),0
Regularize by forcing AR <= 5,0
max_ar = 5,0
if el.aspect_ratio >= max_ar:,0
if el.height > el.width:,0
el.width = el.height / max_ar,0
else:,0
el.height = el.width / max_ar,0
Orient the ellipse such that it encompasses most points,0
"n_inside = el.contains_points(np.c_[self.x, self.y]).sum()",0
el.theta += 0.5 * np.pi,0
"if el.contains_points(np.c_[self.x, self.y]).sum() < n_inside:",0
el.theta -= 0.5 * np.pi,0
"r2 = chi2.ppf(2 * norm.cdf(sd) - 1, 2)",0
"height, width = np.sqrt(E * r2)",0
The general quadratic curve has the form:,0
ax^2 + 2bxy + cy^2 + 2dx + 2fy + g = 0,0
Ellipse center coordinates,0
Semi-axes lengths,0
Angle to the horizontal,0
High uncertainty to the unobservable initial velocities,0
Give high uncertainty to the unobservable initial velocities,0
diff = val - cost_matrix,0
"diff[row, col] += val",0
if (,0
val < self.iou_threshold,0
or np.any(diff[row] <= 0.2),0
"or np.any(diff[:, col] <= 0.2)",0
):,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
Modified from scipy source code:,0
- to restrict its use to 2D,0
- to get rid of shuffling (since arrays are only (nbodyparts * 3) element long),0
TODO - factor in keypoint confidence (and weight by # of observations??),1
"mat = self.calc_pairwise_oks(poses, poses_ref)",0
Remove matched detections with low OKS,0
matches = [],0
"for row, col in zip(row_indices, col_indices):",0
"if mat[row, col] < self.oks_threshold:",0
unmatched_poses.append(row),0
unmatched_trackers.append(col),0
else:,0
"matches.append([row, col])",0
if not len(matches):,0
"matches = np.empty((0, 2), dtype=int)",0
else:,0
matches = np.stack(matches),0
update matched trackers with assigned detections,0
create and initialise new trackers for unmatched detections,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
filter out matched with low IOU,0
TODO: get cropping parameters and utilize!,1
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Load meta data & annotations,0
get the graph!,0
"NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*",0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
TODO: Make this code not so redundant!,1
"If a string was passed in, auto-convert to True for backward compatibility",0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
"# Also storing one ""large"" table with results:",0
note: evaluationfolder.parents[0] to get common folder above all shuffle evaluations.,0
#################################################,0
Load data...,0
#################################################,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
TODO: IMPLEMENT for different batch sizes?,1
Ignore best edges possibly defined during a prior evaluation,0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
FIXME Is having an empty array vs nan really that necessary?!,1
"Form 2D array of shape (n_rows, 4) where the last dimension",0
"is (sample_index, peak_y, peak_x, bpt_index) to slice the PAFs.",0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in,0
Compute all distance statistics,0
Calculate overall prediction error,0
Skip data-driven skeleton selection unless,0
the model was trained on the full graph.,0
"loading backbone from ResNet, MobileNet etc.",0
"display_iters = max(1,int(displayiters))",0
"[_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})",0
Save snapshot,0
return to original path.,0
Clip peak locations to PAFs dimensions,0
"unit_vecs = vecs / lengths[:, np.newaxis]",0
"affinities = np.squeeze(y @ np.expand_dims(unit_vecs, axis=2)).sum(axis=1)",0
Form cost matrices,0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
Change the stride from 2 to 1 to get 16x downscaling instead of 32x.,0
https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142,0
Multi stage is currently only implemented for resnets,0
Defining multi_fusion backbone,0
Attaching multi-stage decoder,0
stage_paf_output = stage_paf_output + pre_stage_paf_output,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
FIXME Fix wrong scope with Keras layers,1
"def prediction_layer(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2DTranspose(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=2,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
## New DLCNet Addition: multi-stage decoder,0
"def prediction_layer_stage(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2D(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=1,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Compute variance using: Var[X]= E[X^2] - E[X]^2.,0
Compute keep_prob,0
TODO(tanmingxing): add support for training progress.,1
Compute drop_connect tensor,0
turn into x times y time bs * bpts,0
extract corresponding locref x and y as well as probability,0
Filter predicted heatmaps with a 2D Gaussian kernel as in:,0
https://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_The_Devil_Is_in_the_Details_Delving_Into_Unbiased_Data_CVPR_2020_paper.pdf,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
defaults will be a public argument for namedtuple in Python 3.7,0
https://docs.python.org/3/library/collections.html#collections.namedtuple,0
Make sure that round down does not go down by more than 10%.,0
Builds the block accordings to arguments.,0
Expansion phase:,0
Depth-wise convolution phase:,0
Squeeze and Excitation layer.,0
Output phase:,0
only apply drop_connect if skip presents.,0
Expansion phase:,0
Output phase:,0
only apply drop_connect if skip presents.,0
Builds blocks.,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Stem part.,0
Head part.,0
Calls Stem layers,0
Calls blocks.,0
Calls final layers and returns logits.,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"(width_coefficient, depth_coefficient, resolution, dropout_rate)",0
blocks_args = [,0
"'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',",0
"'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',",0
"'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',",0
"'r1_k3_s11_e6_i192_o320_se0.25',",0
],0
The default is TPU-specific batch norm.,0
The alternative is tf.layers.BatchNormalization.,0
"batch_norm=utils.TpuBatchNormalization,  # TPU-specific requirement.",0
ValueError will be raised here if override_params has fields not included,0
in global_params.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
Clamp to 40% of crop size to ensure that at least,0
the center keypoint remains visible after the offset is applied.,0
Points located close to one another are sampled preferentially,0
in order to augment crowded regions.,0
Include keypoints in the count to avoid null probabilities,0
Shift the crop center in both dimensions by random amounts,0
and normalize to the original image dimensions.,0
Pickle the 'data' dictionary using the highest protocol available.,0
item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal,0
"Add smart, keypoint-aware image cropping",0
Scale is sampled only once (per batch) to transform all of the images into same size.,0
Discard keypoints whose coordinates lie outside the cropped image,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
"imageio.imwrite(data_items[i].im_path.split('/')[-1],im)",0
Produce score maps and location refinement fields,0
Find indices of individuals in joint_id,0
WIP!,0
STD of gaussian is 1/4 of threshold,0
Grid of coordinates,0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Distance between the joint point and each coordinate,0
NEEDS TO BE DONE!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
TODO Finish implementing actual abstract class,1
parameter initialization for augmentation pipeline:,0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images of a batch into same size.,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"First, initializing variables (if they don't exist)",0
what is the fraction of training samples with scaling augmentation?,0
loading defaults for rotation range!,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"cfg[""rotateratio""] = cfg.get(",0
"""rotratio"", 0.4",0
)  # what is the fraction of training samples with rotation augmentation?,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"Auto cropping is new (was not in Nature Neuroscience 2018 paper, but introduced in Nath et al. Nat. Protocols 2019)",0
"and boosts performance by 2X, particularly on challenging datasets, like the cheetah in Nath et al.",0
Parameters for augmentation with regard to cropping:,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
Get rid of the interpolation beyond the spline knots,0
Retrieve original individual label indices,0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Check the training image paths are correctly stored as arrays of strings,0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
TENSORPACK could fail in WINDOWS...,0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
# Create one identical splits for 3 networks and 3 augmentations,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
Setting specific parameters for training,0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Dowloading the calibration images,0
Deleting unneccesary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
example how to set pose config variables:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
Test bounding box from a single keypoint,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Guarantee that images smaller than crop size are handled fine,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Disable stochastic scale jitter,0
Verify the aspect ratio is preserved,0
"Check data comprise path, shape, and xy coordinates",0
Break the graph to test stitching failure,0
Add fake IDs,0
Split all tracklets in half,0
Test flipped keypoints,0
Original (cached) coordinates must have remained empty,0
Assemble based on the smallest graph to speed up testing,0
Generate fake identity predictions,0
Assemble based on the smallest graph to speed up testing,0
Test now with identity only and ensure assemblies,0
contain only parts of a single group ID.,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
def install(package):,0
"subprocess.check_call([sys.executable, ""-m"", ""pip"", ""install"", package])",0
"install(""tensorflow==1.13.1"")",0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
quicker variant,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
Suppress tensorflow warning messages,0
DLClight version does not support GUIs. Importing accordingly,0
"Train, evaluate & predict functions / all require TF",0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Updating config file:,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path, num_shuffles=auxfun_models.Check4weights(pose_cfg['net_type'], parent_path, num_shuffles= 1)",0
Updating training and test pose_cfg:,0
Read the config file,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
common parameters:,0
Write dictionary to yaml  config file,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
Flatten the list of bodyparts to connect,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Start plotting for every frame,0
"Once all the frames are saved, then make a movie using ffmpeg.",0
Create the fig and define the axes,0
Clear plot and initialize the variables,0
Initialize arrays for appending the 3d data for actual plotting the points,0
Initialize arrays for appending the 3d data for drawing the lines,0
Initialize arrays for appending the 2d data from cam1 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam2 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam1 for drawing the lines,0
Initialize arrays for appending the 2d data from cam2 for drawing the lines,0
Get the scorer names from the dataframe,0
"Set the x,y, and z limits for the 3d view",0
"Set the frame number to read#max(0,index-trailpoints):index",0
Plot the labels for each body part,0
Connecting the bodyparts specified in the config file.3d file is created based on the likelihoods of cam1 and cam2. Using 3d file and check if the body part is nan then dont plot skeleton,0
Saving the frames,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
Extract the indices of frames where the likelihood of a bodypart for both cameras are less than pvalue,0
Extract frames where likelihood for both the views is less than the pcutoff,0
"low_likelihood_frames = np.all(likelihoods < pcutoff, axis=1)",0
For cam1 camera: Assign nans to x and y values of a bodypart where the likelihood for is less than pvalue,0
For cam2 camera: Assign nans to x and y values of a bodypart where the likelihood is less than pvalue,0
ToDo: speed up func. below by saving in numpy.array,1
"Create an empty dataframe to store x,y,z of 3d data",0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Undistorting the points from cam2 camera,0
Save the undistorted files,0
Alternatively,0
Sort tracklets by length to prioritize greater continuity,0
Drop tracklets that are too short,0
"Where slots are available, copy the data over",0
"If about to overwrite data, keep tracklets with highest confidence",0
Squeeze some data into empty slots,0
"For the remaining data, overwrite where we are least confident",0
Find keypoints closest to the remaining data,0
Use Manhattan distance to avoid overflow,0
Only overwrite if improving confidence,0
Fill existing gaps,0
Retrieve original individual label indices,0
Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.,0
Fill existing gaps,0
Retrieve original individual label indices,0
ID swaps occur when X and Y simultaneously intersect each other.,0
Get only those bodypart pairs that belong to different individuals,0
"See Li et al., 2012. Cross-view Activity Recognition using Hankelets.",0
"As proposed in the paper, the Hankel matrix can either be formed from",0
the tracklet's centroid or its normalized velocity.,0
"vel = np.diff(self.centroid, axis=0)",0
"vel /= np.linalg.norm(vel, axis=1, keepdims=True)",0
return self.hankelize(vel),0
TODO Add missing data imputation,1
"nrows, ncols = mat.shape",0
beta = nrows / ncols,0
omega = 0.56 * beta ** 3 - 0.95 * beta ** 2 + 1.82 * beta + 1.43,0
return np.argmin(s > omega * np.median(s)),0
"Note that if tracklets are very short, some may actually be part of the same track",0
and thus incorrectly reflect separate track endpoints...,0
"Map each Tracklet to an entry and output nodes and vice versa,",0
which is convenient once the tracklets are stitched.,0
Store tracklets and corresponding negatives (those that overlap in time),0
TODO Avoid looping over all pairs of tracklets,0
Pick the closest (spatially) overlapping tracklet,0
The algorithm works better with integer weights,0
Let us prune the graph by removing all source and sink edges,0
but those connecting the `n_tracks` first and last tracklets.,0
Preflow push seems to work slightly better than shortest,0
"augmentation path..., and is more computationally efficient.",0
Verify whether there are overlapping tracklets,0
"Pick the segment that minimizes ""smoothness"", computed here",0
with the coefficient of variation of the differences.,0
Rebuild a full graph from the remaining nodes without,0
temporal constraint on what tracklets can be stitched together.,0
Cycle through the residuals and incorporate back those,0
that only fit in a single tracklet.,0
Greedily add the remaining residuals,0
Refresh temporal bounds,0
Default to the distance cost function,0
Add in identity weighing before building the graph,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
average distance and average # significant differences avg. over comparisonbodyparts,0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
###################################################,0
Dependencies,0
###################################################,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as,0
specified in the config file,0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
Prepare figure,0
Mask rather than drop unwanted bodyparts to ensure consistent coloring,0
cropping!,0
nx=X2-X1,0
!/usr/bin/env python3,0
from deeplabcut.utils.auxfun_videos import imread,0
"auxfun_videos.imread(image_path, mode=""skimage"")",0
The above already raises a ValueError if formatting is wrong,0
"Rotate, see: https://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg",0
interesting option to just update metadata.,0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
Read the video until a frame is successfully read,0
import platform,0
"if platform.system() == ""Darwin"":  # for OSX use WXAgg",0
fig.canvas.start_event_loop(timeout=-1),0
else:,0
#fig.canvas.stop_event_loop(),0
"Windows throws error if file path is > 260 characters, can fix with prefix.",0
See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation,0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
check if it exists:,0
"Attention this order has to be consistent (for training set creation, training, inference etc.)",0
"-> adding (single,bpt) for uniquebodyparts",0
"-> adding (indivdual,bpt) for multianimalbodyparts",0
load defaults,0
set project specific parameters:,0
###################################################,0
Dependencies,0
###################################################,0
Pose X vs pose Y,0
Poses vs time,0
Likelihoods,0
Histograms,0
#################################################,0
Looping analysis over video,0
#################################################,0
Keep only the individuals and bodyparts that were labeled,0
Either display the animals defined in the config if they are found,0
"in the dataframe, or all the trajectories regardless of their names",0
Determine whether the data are single- or multi-animal without loading into memory,0
simply by checking whether 'individuals' is in the second line of the CSV.,0
Make paths platform-agnostic if they are not already,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"if ""MULTI!"" in allbpts:",0
Ensure same order as in config.yaml,0
ABBREVIATE NETWORK NAMES -- esp. for mobilenet!,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
Iterate over data files and stop as soon as one matching the scorer is found,0
If there was no match...,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"trainingset = wx.StaticBox(self, label=""Specify the trainingset index"")",0
"trainingset_boxsizer = wx.StaticBoxSizer(trainingset, wx.VERTICAL)",0
"self.trainingset = wx.SpinCtrl(self, value=""0"", min=0, max=100)",0
trainingset_boxsizer.Add(,0
"self.trainingset, 1, wx.EXPAND | wx.TOP | wx.BOTTOM, 10",0
),0
"hbox_.Add(trainingset_boxsizer, 5, wx.EXPAND | wx.TOP | wx.BOTTOM, 5)",0
Read the infer config file,0
trainingsetindex = self.trainingset.GetValue(),0
let the user open the file with default text editor. Also make it mac compatible,0
trainingsetindex = self.trainingset.GetValue(),0
"trainingsetindex=trainingsetindex,",0
"trainingsetindex=self.trainingset.GetValue(),",0
self.save.Enable(False),0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
topSplitter = wx.SplitterWindow(self),0
,0
"self.image_panel = ImagePanel(topSplitter, config,video,shuffle,Dataframe,self.gui_size)",0
self.widget_panel = WidgetPanel(topSplitter),0
,0
"topSplitter.SplitHorizontally(self.image_panel, self.widget_panel,sashPosition=self.gui_size[1]*0.83)#0.9",0
topSplitter.SetSashGravity(1),0
sizer = wx.BoxSizer(wx.VERTICAL),0
"sizer.Add(topSplitter, 1, wx.EXPAND)",0
self.SetSizer(sizer),0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
"cv2.imwrite(img_name, frame)",0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0,10), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=10)",0
Add all the options,0
line = wx.StaticLine(self),0
"self.sizer.Add(line, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=10)",0
,0
"self.multi_choice.Bind(wx.EVT_CHECKBOX,self.activate_copy_videos)",0
Hide the button as this is not the default option,0
self.vids.Enable(False),0
self.SetSizer(self.sizer),0
"self.sizer.Add(self.sizer, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=15)",0
self.ok.Enable(False),0
else:,0
self.ok.Enable(True),0
For mac compatibility,0
Remove the pages in case the user goes back to the create new project and creates/load a new project,0
Add all the other pages,0
"page10 = Refine_labels(self.parent, self.gui_size, self.cfg, page5)",0
"self.parent.AddPage(page10, ""OPT: Refine Labels"")",0
self.yes.Enable(False),0
self.no.Enable(False),0
Here we create a panel and a notebook on the panel,0
create the page windows as children of the notebook and add the pages to the notebook with the label to show on the tab,0
Take into consideration imprecise OpenCV estimation of total number of frames,0
Automatically disable the draggable points,0
Save additional frames to the labeled-data directory,0
Store the newly-refined data,0
Uncertain keypoints are ignored,0
Merge with the already existing annotated data,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.buttonCounter = [],0
self.bodyparts = [],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
load labeledPoints and fundamental Matrix,0
Find offset terms for drawing epipolar Lines,0
Get crop params for camera being labeled,0
Get crop params for other camera,0
convert the image to RGB as you are showing the image with matplotlib,0
draw epipolar lines,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
variable initilization,0
design the panel,0
Add logo of DLC,0
self.filelist = self.filelist + self.vids,0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
load labeledPoints and fundamental Matrix,0
Find offset terms for drawing epipolar Lines,0
Get crop params for camera being labeled,0
Get crop params for other camera,0
convert the image to RGB as you are showing the image with matplotlib,0
draw epipolar lines,0
divider = make_axes_locatable(self.axes),0
"colorIndex = np.linspace(np.min(im),np.max(im),len(bodyparts))",0
"cax = divider.append_axes(""right"", size=""5%"", pad=0.05)",0
"cbar = self.figure.colorbar(ax, cax=cax,spacing='proportional', ticks=colorIndex)",0
cbar.set_ticklabels(bodyparts[::-1]),0
self.figure.canvas.draw(),0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"MainFrame.saveDataSet(self, event)",0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
checks for unique bodyparts,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Create an empty data frame,0
Cache original bodyparts,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Rearrange bodypart columns in config order,0
Test whether there are missing frames and superfluous data,0
Check whether new individuals or body parts were added,0
Checking if user added a new label,0
Found new labels in either multiple bodyparts or unique bodyparts,0
check if single is slected when radio buttons are changed,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Discard data associated with bodyparts that are no longer in the config,0
Re-organize the dataframe so the CSV looks consistent with the config,0
restore the background region,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.build = wx.Button(self, label=""Build skeleton"")",0
"sizer.Add(self.build, pos=(4, 3), flag=wx.BOTTOM | wx.RIGHT, border=10)",0
"self.build.Bind(wx.EVT_BUTTON, self.build_skeleton)",0
self.build.Enable(True),0
"def build_skeleton(self, event):",0
skeleton.SkeletonBuilder(self.config),0
Settting the GUI size and panels design,0
Checks if zoom/pan button is ON,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"trainingindex = wx.StaticBox(self, label=""Specify the trainingset index"")",0
"trainingindex_boxsizer = wx.StaticBoxSizer(trainingindex, wx.VERTICAL)",0
"self.trainingindex = wx.SpinCtrl(self, value=""0"", min=0, max=100)",0
trainingindex_boxsizer.Add(,0
"self.trainingindex, 1, wx.EXPAND | wx.TOP | wx.BOTTOM, 10)",0
use the default pose_cfg file for default values,0
self.display_iters.Enable(False),0
self.save_iters.Enable(False),0
self.max_iters.Enable(False),0
self.snapshots.Enable(False),0
"hbox1.Add(trainingindex_boxsizer, 10, wx.EXPAND | wx.TOP | wx.BOTTOM, 5)",0
self.trainingindex.Enable(False),0
self.trainingindex.Enable(True),0
self.trainingindex.Enable(True),0
Read the pose config file,0
print(trainFraction[-1]),0
"print(os.path.join(cfg['project_path'],auxiliaryfunctions.GetModelFolder(trainFraction, self.shuffles.GetValue(),cfg),'train','pose_cfg.yaml'))",0
let the user open the file with default text editor. Also make it mac compatible,0
update the variables with the edited values in the pose config file,0
self.trainingindex.Enable(True),0
variable initilization,0
design the panel,0
Add logo of DLC,0
### BOX 1 ####,0
### BOX 2 ####,0
### BOX 3 ####,0
Read the infer config file,0
"trainFraction = cfg[""TrainingFraction""][trainingsetindex]",0
let the user open the file with default text editor. Also make it mac compatible,0
"self.select_destfolder.SetPath(""None"")",0
self.config = [],0
"self.sel_config.SetPath("""")",0
"self.videotype.SetStringSelection("".avi"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"trainingindex_box = wx.StaticBox(self, label=""Specify the trainingset index"")",0
"trainingindex_boxsizer = wx.StaticBoxSizer(trainingindex_box, wx.VERTICAL)",0
"self.trainingindex = wx.SpinCtrl(self, value=""0"", min=0, max=100)",0
trainingindex_boxsizer.Add(,0
"self.trainingindex, 0, wx.EXPAND | wx.TOP | wx.BOTTOM, 10",0
),0
"self.hbox2.Add(trainingindex_boxsizer, 10, wx.EXPAND | wx.TOP | wx.BOTTOM, 5)",0
trainindex = self.trainingindex.GetValue(),0
"self.shuffles.SetValue(""1"")",0
"DownSampleVideo(vname,width=-1,height=200,outsuffix='downsampled',outpath=None,rotatecw=False):",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"majorDimension=0,",0
"self.videotype.SetStringSelection("".avi"")",0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
"self.choiceBox,self.visualization_rdb = self.choice_panel.addRadioButtons()",0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
Overwrite the config-defined individual names,0
with those actually present in the annotated data,0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
print(self.Dataframe.head()),0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
fix move to corner,0
fix move to corner,0
fix move to corner,0
fix move to corner,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"trainingset = wx.StaticBox(self, label=""Specify the trainingset index"")",0
"trainingset_boxsizer = wx.StaticBoxSizer(trainingset, wx.VERTICAL)",0
"self.trainingset = wx.SpinCtrl(self, value=""0"", min=0, max=100)",0
trainingset_boxsizer.Add(,0
"self.trainingset, 1, wx.EXPAND | wx.TOP | wx.BOTTOM, 10",0
),0
"self.hbox1.Add(trainingset_boxsizer, 5, wx.EXPAND | wx.TOP | wx.BOTTOM, 5)",0
"if self.plot_scoremaps.GetStringSelection() == ""Yes"":",0
Read the infer config file,0
trainingsetindex = self.trainingset.GetValue(),0
let the user open the file with default text editor. Also make it mac compatible,0
shuffle = self.shuffle.GetValue(),0
trainingsetindex = self.trainingset.GetValue(),0
"trainingsetindex=trainingsetindex,",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
## LABELING ###,0
"self.sel_config.SetPath("""")",0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
self.cfg = auxiliaryfunctions.read_config(config),0
"self.statusbar.SetStatusText("""")",0
"dlg = wx.MessageDialog(None,""Are you sure?"", ""Quit!"",wx.YES_NO | wx.ICON_WARNING)",0
result = dlg.ShowModal(),0
if result == wx.ID_YES:,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0, 4), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=5)",0
variable initilization,0
design the panel,0
Add logo of DLC,0
#         design the panel,0
Add image of DLC,0
if editing this text make sure you add the '\n' to get the new line. The sizer is unable to format lines correctly.,0
Drop missing body parts,0
Drop points lying outside the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
## dlcnet_ms5: backbone resnet50 + multi-fusion & multi-stage module,0
## dlcr101_ms5/dlcr152_ms5: backbone resnet101/152 + multi-fusion & multi-stage module,0
Ignore possible connections between 'multi' and 'unique' body parts;,0
one can never be too careful...,0
Disable the prediction of PAFs if the graph is empty,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving metadata and data file (Pickle file),0
###############################################################################,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Setting inference cfg file:,0
Search a pose_config.yaml file to parse missing information,0
Clean the training-datasets folder prior to recreating the data pickles,0
Load updated lists:,0
Find the corresponding video file,0
"print(key, ""dropping?"")",0
adding important values for multianiaml project:,0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Determine the index length required to guarantee,0
the train–test ratio is exactly the desired one.,0
Pad indices so lengths agree,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(name,test_video_name)",0
Blazing fast and does not load the image into memory,0
Check that points lie within the image,0
DeprecationWarnings are silenced since Python 3.2 unless triggered in __main__,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
updating variable if null/None! #backwardscompatability,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
these values are dropped as scalecrop,0
doesn't have rotation implemented,0
sort the model directories,0
get the shuffle index and offset by 1.,0
read cfg file,0
create log file,0
Check for variable correctness,0
Moviepy:,0
select crop method,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
TODO: addd cropping as in video analysis!,1
if cropping is not None:,0
cfg['cropping']=True,0
"cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping",0
"print(""Overwriting cropping parameters:"", cropping)",0
"print(""These are used for all videos, but won't be save to the cfg file."")",0
"Also ensure `boundingboxslack` is greater than zero, otherwise overlap",0
"between trackers cannot be evaluated, resulting in empty tracklets.",0
Check which snapshots are available and sort them by # iterations,0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
NOTE: If dataname line above is changed then line below is obsolete?,0
"trackname = trackname.replace(videofolder, destfolder)",0
TODO: adjust this for multi + unique bodyparts!,1
this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. ),0
Optimal identity assignment based on soft voting,0
imagenet mean for resnet pretraining:,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/AlexEMG/DeepLabCut/pull/324",0
for DLC 2.2. (here all set False to not use PAFs/pairwise fields),0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
"reloading defaults, as they can bleed over from a previous run otherwise",0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
dlc_cfg_train = load_config(str(path_train_config)),0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
Name for deeplabcut net (based on its parameters),0
"DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])",0
"print(""Extracting maps for "", DLCscorer, "" with # of trainingiterations:"", trainingsiterations)",0
if notanalyzed: #this only applies to ask if h5 exists...,0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Get animal IDs and corresponding indices in the arrays of detections,0
Form ground truth beforehand,0
Assemble animals on the full set of detections,0
Count the number of unassembled bodyparts,0
Handle unlabeled bodyparts...,0
"Only 1 animal, let us return the full graph indices only",0
Find minimal skeleton,0
Select optimal PAF graph,0
Selective copy; deepcopy is >5x slower,0
Only keeps skeletons that are more than 90% complete,0
TODO Normalize dists by longest length?,1
TODO Smarter imputation technique (Bayesian? Grassmann averages?),1
Covariance matrix estimation fails due to numerical singularities,0
Correct distance to account for missing observations,0
"Alternatively, reduce contribution of missing values to the Mahalanobis",0
distance to zero by substituting the corresponding means.,0
"Fill the subsets with unambiguous, complete individuals",0
Fuse superfluous assemblies,0
Second pass without edge safety,0
Mark the unique body parts as assembled anyway so,0
they are not used later on to fill assemblies.,0
Store selected edges for subsequent frames,0
Remove invalid assemblies,0
Fill assemblies with unconnected body parts,0
Spawning (rather than forking) multiple processes does not,0
work nicely with the GUI or interactive sessions.,0
"In that case, we fall back to the serial assembly.",0
Swap corresponding keypoints,0
Only consider assemblies of at least two keypoints,0
sigma is taken as the median of all COCO keypoint standard deviations,0
Sort matching score (OKS) in descending order of assembly affinity,0
Guarantee precision decreases monotonically,0
See https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173),0
Regularize by forcing AR <= 5,0
max_ar = 5,0
if el.aspect_ratio >= max_ar:,0
if el.height > el.width:,0
el.width = el.height / max_ar,0
else:,0
el.height = el.width / max_ar,0
Orient the ellipse such that it encompasses most points,0
"n_inside = el.contains_points(np.c_[self.x, self.y]).sum()",0
el.theta += 0.5 * np.pi,0
"if el.contains_points(np.c_[self.x, self.y]).sum() < n_inside:",0
el.theta -= 0.5 * np.pi,0
"r2 = chi2.ppf(2 * norm.cdf(sd) - 1, 2)",0
"height, width = np.sqrt(E * r2)",0
The general quadratic curve has the form:,0
ax^2 + 2bxy + cy^2 + 2dx + 2fy + g = 0,0
Ellipse center coordinates,0
Semi-axes lengths,0
Angle to the horizontal,0
High uncertainty to the unobservable initial velocities,0
Give high uncertainty to the unobservable initial velocities,0
diff = val - cost_matrix,0
"diff[row, col] += val",0
if (,0
val < self.iou_threshold,0
or np.any(diff[row] <= 0.2),0
"or np.any(diff[:, col] <= 0.2)",0
):,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
Modified from scipy source code:,0
- to restrict its use to 2D,0
- to get rid of shuffling (since arrays are only (nbodyparts * 3) element long),0
TODO - factor in keypoint confidence (and weight by # of observations??),1
"mat = self.calc_pairwise_oks(poses, poses_ref)",0
Remove matched detections with low OKS,0
matches = [],0
"for row, col in zip(row_indices, col_indices):",0
"if mat[row, col] < self.oks_threshold:",0
unmatched_poses.append(row),0
unmatched_trackers.append(col),0
else:,0
"matches.append([row, col])",0
if not len(matches):,0
"matches = np.empty((0, 2), dtype=int)",0
else:,0
matches = np.stack(matches),0
update matched trackers with assigned detections,0
create and initialise new trackers for unmatched detections,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
filter out matched with low IOU,0
TODO: get cropping parameters and utilize!,1
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Load meta data & annotations,0
get the graph!,0
"NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*",0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
TODO: Make this code not so redundant!,1
"If a string was passed in, auto-convert to True for backward compatibility",0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
"# Also storing one ""large"" table with results:",0
note: evaluationfolder.parents[0] to get common folder above all shuffle evaluations.,0
#################################################,0
Load data...,0
#################################################,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
TODO: IMPLEMENT for different batch sizes?,1
Ignore best edges possibly defined during a prior evaluation,0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
FIXME Is having an empty array vs nan really that necessary?!,1
"Form 2D array of shape (n_rows, 4) where the last dimension",0
"is (sample_index, peak_y, peak_x, bpt_index) to slice the PAFs.",0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in,0
Compute all distance statistics,0
Calculate overall prediction error,0
Skip data-driven skeleton selection unless,0
the model was trained on the full graph.,0
"loading backbone from ResNet, MobileNet etc.",0
"display_iters = max(1,int(displayiters))",0
"[_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})",0
Save snapshot,0
return to original path.,0
Clip peak locations to PAFs dimensions,0
"unit_vecs = vecs / lengths[:, np.newaxis]",0
"affinities = np.squeeze(y @ np.expand_dims(unit_vecs, axis=2)).sum(axis=1)",0
Form cost matrices,0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
Change the stride from 2 to 1 to get 16x downscaling instead of 32x.,0
https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142,0
Multi stage is currently only implemented for resnets,0
Defining multi_fusion backbone,0
Attaching multi-stage decoder,0
stage_paf_output = stage_paf_output + pre_stage_paf_output,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
FIXME Fix wrong scope with Keras layers,1
"def prediction_layer(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2DTranspose(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=2,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
## New DLCNet Addition: multi-stage decoder,0
"def prediction_layer_stage(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2D(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=1,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Compute variance using: Var[X]= E[X^2] - E[X]^2.,0
Compute keep_prob,0
TODO(tanmingxing): add support for training progress.,1
Compute drop_connect tensor,0
turn into x times y time bs * bpts,0
extract corresponding locref x and y as well as probability,0
Filter predicted heatmaps with a 2D Gaussian kernel as in:,0
https://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_The_Devil_Is_in_the_Details_Delving_Into_Unbiased_Data_CVPR_2020_paper.pdf,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
defaults will be a public argument for namedtuple in Python 3.7,0
https://docs.python.org/3/library/collections.html#collections.namedtuple,0
Make sure that round down does not go down by more than 10%.,0
Builds the block accordings to arguments.,0
Expansion phase:,0
Depth-wise convolution phase:,0
Squeeze and Excitation layer.,0
Output phase:,0
only apply drop_connect if skip presents.,0
Expansion phase:,0
Output phase:,0
only apply drop_connect if skip presents.,0
Builds blocks.,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Stem part.,0
Head part.,0
Calls Stem layers,0
Calls blocks.,0
Calls final layers and returns logits.,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"(width_coefficient, depth_coefficient, resolution, dropout_rate)",0
blocks_args = [,0
"'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',",0
"'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',",0
"'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',",0
"'r1_k3_s11_e6_i192_o320_se0.25',",0
],0
The default is TPU-specific batch norm.,0
The alternative is tf.layers.BatchNormalization.,0
"batch_norm=utils.TpuBatchNormalization,  # TPU-specific requirement.",0
ValueError will be raised here if override_params has fields not included,0
in global_params.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
Clamp to 40% of crop size to ensure that at least,0
the center keypoint remains visible after the offset is applied.,0
Points located close to one another are sampled preferentially,0
in order to augment crowded regions.,0
Include keypoints in the count to avoid null probabilities,0
Shift the crop center in both dimensions by random amounts,0
and normalize to the original image dimensions.,0
Pickle the 'data' dictionary using the highest protocol available.,0
item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal,0
"Add smart, keypoint-aware image cropping",0
Scale is sampled only once (per batch) to transform all of the images into same size.,0
Discard keypoints whose coordinates lie outside the cropped image,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
"imageio.imwrite(data_items[i].im_path.split('/')[-1],im)",0
Produce score maps and location refinement fields,0
Find indices of individuals in joint_id,0
WIP!,0
STD of gaussian is 1/4 of threshold,0
Grid of coordinates,0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Distance between the joint point and each coordinate,0
NEEDS TO BE DONE!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
TODO Finish implementing actual abstract class,1
parameter initialization for augmentation pipeline:,0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images of a batch into same size.,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"First, initializing variables (if they don't exist)",0
what is the fraction of training samples with scaling augmentation?,0
loading defaults for rotation range!,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"cfg[""rotateratio""] = cfg.get(",0
"""rotratio"", 0.4",0
)  # what is the fraction of training samples with rotation augmentation?,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"Auto cropping is new (was not in Nature Neuroscience 2018 paper, but introduced in Nath et al. Nat. Protocols 2019)",0
"and boosts performance by 2X, particularly on challenging datasets, like the cheetah in Nath et al.",0
Parameters for augmentation with regard to cropping:,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
Get rid of the interpolation beyond the spline knots,0
Retrieve original individual label indices,0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Check the training image paths are correctly stored as arrays of strings,0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
TENSORPACK could fail in WINDOWS...,0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
# Create one identical splits for 3 networks and 3 augmentations,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
Setting specific parameters for training,0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Dowloading the calibration images,0
Deleting unneccesary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
example how to set pose config variables:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
Test bounding box from a single keypoint,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Guarantee that images smaller than crop size are handled fine,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Disable stochastic scale jitter,0
Verify the aspect ratio is preserved,0
"Check data comprise path, shape, and xy coordinates",0
Break the graph to test stitching failure,0
Add fake IDs,0
Split all tracklets in half,0
Test flipped keypoints,0
Original (cached) coordinates must have remained empty,0
Assemble based on the smallest graph to speed up testing,0
Generate fake identity predictions,0
Assemble based on the smallest graph to speed up testing,0
Test now with identity only and ensure assemblies,0
contain only parts of a single group ID.,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
def install(package):,0
"subprocess.check_call([sys.executable, ""-m"", ""pip"", ""install"", package])",0
"install(""tensorflow==1.13.1"")",0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
quicker variant,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
Suppress tensorflow warning messages,0
DLClight version does not support GUIs. Importing accordingly,0
"Train, evaluate & predict functions / all require TF",0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Updating config file:,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path, num_shuffles=auxfun_models.Check4weights(pose_cfg['net_type'], parent_path, num_shuffles= 1)",0
Updating training and test pose_cfg:,0
Read the config file,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
common parameters:,0
Write dictionary to yaml  config file,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
Flatten the list of bodyparts to connect,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Start plotting for every frame,0
"Once all the frames are saved, then make a movie using ffmpeg.",0
Create the fig and define the axes,0
Clear plot and initialize the variables,0
Initialize arrays for appending the 3d data for actual plotting the points,0
Initialize arrays for appending the 3d data for drawing the lines,0
Initialize arrays for appending the 2d data from cam1 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam2 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam1 for drawing the lines,0
Initialize arrays for appending the 2d data from cam2 for drawing the lines,0
Get the scorer names from the dataframe,0
"Set the x,y, and z limits for the 3d view",0
"Set the frame number to read#max(0,index-trailpoints):index",0
Plot the labels for each body part,0
Connecting the bodyparts specified in the config file.3d file is created based on the likelihoods of cam1 and cam2. Using 3d file and check if the body part is nan then dont plot skeleton,0
Saving the frames,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
Extract the indices of frames where the likelihood of a bodypart for both cameras are less than pvalue,0
Extract frames where likelihood for both the views is less than the pcutoff,0
"low_likelihood_frames = np.all(likelihoods < pcutoff, axis=1)",0
For cam1 camera: Assign nans to x and y values of a bodypart where the likelihood for is less than pvalue,0
For cam2 camera: Assign nans to x and y values of a bodypart where the likelihood is less than pvalue,0
ToDo: speed up func. below by saving in numpy.array,1
"Create an empty dataframe to store x,y,z of 3d data",0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Undistorting the points from cam2 camera,0
Save the undistorted files,0
Alternatively,0
Sort tracklets by length to prioritize greater continuity,0
Drop tracklets that are too short,0
"Where slots are available, copy the data over",0
"If about to overwrite data, keep tracklets with highest confidence",0
Squeeze some data into empty slots,0
"For the remaining data, overwrite where we are least confident",0
Find keypoints closest to the remaining data,0
Use Manhattan distance to avoid overflow,0
Only overwrite if improving confidence,0
Fill existing gaps,0
Retrieve original individual label indices,0
Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.,0
Fill existing gaps,0
Retrieve original individual label indices,0
ID swaps occur when X and Y simultaneously intersect each other.,0
Get only those bodypart pairs that belong to different individuals,0
"See Li et al., 2012. Cross-view Activity Recognition using Hankelets.",0
"As proposed in the paper, the Hankel matrix can either be formed from",0
the tracklet's centroid or its normalized velocity.,0
"vel = np.diff(self.centroid, axis=0)",0
"vel /= np.linalg.norm(vel, axis=1, keepdims=True)",0
return self.hankelize(vel),0
TODO Add missing data imputation,1
"nrows, ncols = mat.shape",0
beta = nrows / ncols,0
omega = 0.56 * beta ** 3 - 0.95 * beta ** 2 + 1.82 * beta + 1.43,0
return np.argmin(s > omega * np.median(s)),0
"Note that if tracklets are very short, some may actually be part of the same track",0
and thus incorrectly reflect separate track endpoints...,0
"Map each Tracklet to an entry and output nodes and vice versa,",0
which is convenient once the tracklets are stitched.,0
TODO Avoid looping over all pairs of tracklets,0
The algorithm works better with integer weights,0
Let us prune the graph by removing all source and sink edges,0
but those connecting the `n_tracks` first and last tracklets.,0
Preflow push seems to work slightly better than shortest,0
"augmentation path..., and is more computationally efficient.",0
Verify whether there are overlapping tracklets,0
"Pick the segment that minimizes ""smoothness"", computed here",0
with the coefficient of variation of the differences.,0
Rebuild a full graph from the remaining nodes without,0
temporal constraint on what tracklets can be stitched together.,0
Cycle through the residuals and incorporate back those,0
that only fit in a single tracklet.,0
Greedily add the remaining residuals,0
Refresh temporal bounds,0
Default to the distance cost function,0
Add in identity weighing before building the graph,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
average distance and average # significant differences avg. over comparisonbodyparts,0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
###################################################,0
Dependencies,0
###################################################,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as,0
specified in the config file,0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
Prepare figure,0
Mask rather than drop unwanted bodyparts to ensure consistent coloring,0
cropping!,0
nx=X2-X1,0
!/usr/bin/env python3,0
from deeplabcut.utils.auxfun_videos import imread,0
"auxfun_videos.imread(image_path, mode=""skimage"")",0
The above already raises a ValueError if formatting is wrong,0
"Rotate, see: https://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg",0
interesting option to just update metadata.,0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
Read the video until a frame is successfully read,0
import platform,0
"if platform.system() == ""Darwin"":  # for OSX use WXAgg",0
fig.canvas.start_event_loop(timeout=-1),0
else:,0
#fig.canvas.stop_event_loop(),0
"Windows throws error if file path is > 260 characters, can fix with prefix.",0
See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation,0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
TODO: check if this tracker actually exists?,1
"Attention this order has to be consistent (for training set creation, training, inference etc.)",0
"-> adding (single,bpt) for uniquebodyparts",0
"-> adding (indivdual,bpt) for multianimalbodyparts",0
load defaults,0
set project specific parameters:,0
###################################################,0
Dependencies,0
###################################################,0
Pose X vs pose Y,0
Poses vs time,0
Likelihoods,0
Histograms,0
#################################################,0
Looping analysis over video,0
#################################################,0
Keep only the individuals and bodyparts that were labeled,0
Either display the animals defined in the config if they are found,0
"in the dataframe, or all the trajectories regardless of their names",0
Determine whether the data are single- or multi-animal without loading into memory,0
simply by checking whether 'individuals' is in the second line of the CSV.,0
Make paths platform-agnostic if they are not already,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"if ""MULTI!"" in allbpts:",0
Ensure same order as in config.yaml,0
ABBREVIATE NETWORK NAMES -- esp. for mobilenet!,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
Iterate over data files and stop as soon as one matching the scorer is found,0
If there was no match...,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"trainingset = wx.StaticBox(self, label=""Specify the trainingset index"")",0
"trainingset_boxsizer = wx.StaticBoxSizer(trainingset, wx.VERTICAL)",0
"self.trainingset = wx.SpinCtrl(self, value=""0"", min=0, max=100)",0
trainingset_boxsizer.Add(,0
"self.trainingset, 1, wx.EXPAND | wx.TOP | wx.BOTTOM, 10",0
),0
"hbox_.Add(trainingset_boxsizer, 5, wx.EXPAND | wx.TOP | wx.BOTTOM, 5)",0
Read the infer config file,0
trainingsetindex = self.trainingset.GetValue(),0
let the user open the file with default text editor. Also make it mac compatible,0
trainingsetindex = self.trainingset.GetValue(),0
"trainingsetindex=trainingsetindex,",0
"trainingsetindex=self.trainingset.GetValue(),",0
self.save.Enable(False),0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
topSplitter = wx.SplitterWindow(self),0
,0
"self.image_panel = ImagePanel(topSplitter, config,video,shuffle,Dataframe,self.gui_size)",0
self.widget_panel = WidgetPanel(topSplitter),0
,0
"topSplitter.SplitHorizontally(self.image_panel, self.widget_panel,sashPosition=self.gui_size[1]*0.83)#0.9",0
topSplitter.SetSashGravity(1),0
sizer = wx.BoxSizer(wx.VERTICAL),0
"sizer.Add(topSplitter, 1, wx.EXPAND)",0
self.SetSizer(sizer),0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
"cv2.imwrite(img_name, frame)",0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0,10), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=10)",0
Add all the options,0
line = wx.StaticLine(self),0
"self.sizer.Add(line, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=10)",0
,0
"self.multi_choice.Bind(wx.EVT_CHECKBOX,self.activate_copy_videos)",0
Hide the button as this is not the default option,0
self.vids.Enable(False),0
self.SetSizer(self.sizer),0
"self.sizer.Add(self.sizer, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=15)",0
self.ok.Enable(False),0
else:,0
self.ok.Enable(True),0
For mac compatibility,0
Remove the pages in case the user goes back to the create new project and creates/load a new project,0
Add all the other pages,0
"page10 = Refine_labels(self.parent, self.gui_size, self.cfg, page5)",0
"self.parent.AddPage(page10, ""OPT: Refine Labels"")",0
self.yes.Enable(False),0
self.no.Enable(False),0
Here we create a panel and a notebook on the panel,0
create the page windows as children of the notebook and add the pages to the notebook with the label to show on the tab,0
Take into consideration imprecise OpenCV estimation of total number of frames,0
Automatically disable the draggable points,0
Save additional frames to the labeled-data directory,0
Store the newly-refined data,0
Uncertain keypoints are ignored,0
Merge with the already existing annotated data,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.buttonCounter = [],0
self.bodyparts = [],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
load labeledPoints and fundamental Matrix,0
Find offset terms for drawing epipolar Lines,0
Get crop params for camera being labeled,0
Get crop params for other camera,0
convert the image to RGB as you are showing the image with matplotlib,0
draw epipolar lines,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
variable initilization,0
design the panel,0
Add logo of DLC,0
self.filelist = self.filelist + self.vids,0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
load labeledPoints and fundamental Matrix,0
Find offset terms for drawing epipolar Lines,0
Get crop params for camera being labeled,0
Get crop params for other camera,0
convert the image to RGB as you are showing the image with matplotlib,0
draw epipolar lines,0
divider = make_axes_locatable(self.axes),0
"colorIndex = np.linspace(np.min(im),np.max(im),len(bodyparts))",0
"cax = divider.append_axes(""right"", size=""5%"", pad=0.05)",0
"cbar = self.figure.colorbar(ax, cax=cax,spacing='proportional', ticks=colorIndex)",0
cbar.set_ticklabels(bodyparts[::-1]),0
self.figure.canvas.draw(),0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"MainFrame.saveDataSet(self, event)",0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
checks for unique bodyparts,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Create an empty data frame,0
Cache original bodyparts,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Rearrange bodypart columns in config order,0
Test whether there are missing frames and superfluous data,0
Check whether new individuals or body parts were added,0
Checking if user added a new label,0
Found new labels in either multiple bodyparts or unique bodyparts,0
check if single is slected when radio buttons are changed,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Discard data associated with bodyparts that are no longer in the config,0
Re-organize the dataframe so the CSV looks consistent with the config,0
restore the background region,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.build = wx.Button(self, label=""Build skeleton"")",0
"sizer.Add(self.build, pos=(4, 3), flag=wx.BOTTOM | wx.RIGHT, border=10)",0
"self.build.Bind(wx.EVT_BUTTON, self.build_skeleton)",0
self.build.Enable(True),0
"def build_skeleton(self, event):",0
skeleton.SkeletonBuilder(self.config),0
Settting the GUI size and panels design,0
Checks if zoom/pan button is ON,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"trainingindex = wx.StaticBox(self, label=""Specify the trainingset index"")",0
"trainingindex_boxsizer = wx.StaticBoxSizer(trainingindex, wx.VERTICAL)",0
"self.trainingindex = wx.SpinCtrl(self, value=""0"", min=0, max=100)",0
trainingindex_boxsizer.Add(,0
"self.trainingindex, 1, wx.EXPAND | wx.TOP | wx.BOTTOM, 10)",0
use the default pose_cfg file for default values,0
self.display_iters.Enable(False),0
self.save_iters.Enable(False),0
self.max_iters.Enable(False),0
self.snapshots.Enable(False),0
"hbox1.Add(trainingindex_boxsizer, 10, wx.EXPAND | wx.TOP | wx.BOTTOM, 5)",0
self.trainingindex.Enable(False),0
self.trainingindex.Enable(True),0
self.trainingindex.Enable(True),0
Read the pose config file,0
print(trainFraction[-1]),0
"print(os.path.join(cfg['project_path'],auxiliaryfunctions.GetModelFolder(trainFraction, self.shuffles.GetValue(),cfg),'train','pose_cfg.yaml'))",0
let the user open the file with default text editor. Also make it mac compatible,0
update the variables with the edited values in the pose config file,0
self.trainingindex.Enable(True),0
variable initilization,0
design the panel,0
Add logo of DLC,0
### BOX 1 ####,0
### BOX 2 ####,0
### BOX 3 ####,0
Read the infer config file,0
"trainFraction = cfg[""TrainingFraction""][trainingsetindex]",0
let the user open the file with default text editor. Also make it mac compatible,0
"self.select_destfolder.SetPath(""None"")",0
self.config = [],0
"self.sel_config.SetPath("""")",0
"self.videotype.SetStringSelection("".avi"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"trainingindex_box = wx.StaticBox(self, label=""Specify the trainingset index"")",0
"trainingindex_boxsizer = wx.StaticBoxSizer(trainingindex_box, wx.VERTICAL)",0
"self.trainingindex = wx.SpinCtrl(self, value=""0"", min=0, max=100)",0
trainingindex_boxsizer.Add(,0
"self.trainingindex, 0, wx.EXPAND | wx.TOP | wx.BOTTOM, 10",0
),0
"self.hbox2.Add(trainingindex_boxsizer, 10, wx.EXPAND | wx.TOP | wx.BOTTOM, 5)",0
trainindex = self.trainingindex.GetValue(),0
"self.shuffles.SetValue(""1"")",0
"DownSampleVideo(vname,width=-1,height=200,outsuffix='downsampled',outpath=None,rotatecw=False):",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"majorDimension=0,",0
"self.videotype.SetStringSelection("".avi"")",0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
"self.choiceBox,self.visualization_rdb = self.choice_panel.addRadioButtons()",0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
Overwrite the config-defined individual names,0
with those actually present in the annotated data,0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
print(self.Dataframe.head()),0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
fix move to corner,0
fix move to corner,0
fix move to corner,0
fix move to corner,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"trainingset = wx.StaticBox(self, label=""Specify the trainingset index"")",0
"trainingset_boxsizer = wx.StaticBoxSizer(trainingset, wx.VERTICAL)",0
"self.trainingset = wx.SpinCtrl(self, value=""0"", min=0, max=100)",0
trainingset_boxsizer.Add(,0
"self.trainingset, 1, wx.EXPAND | wx.TOP | wx.BOTTOM, 10",0
),0
"self.hbox1.Add(trainingset_boxsizer, 5, wx.EXPAND | wx.TOP | wx.BOTTOM, 5)",0
"if self.plot_scoremaps.GetStringSelection() == ""Yes"":",0
Read the infer config file,0
trainingsetindex = self.trainingset.GetValue(),0
let the user open the file with default text editor. Also make it mac compatible,0
shuffle = self.shuffle.GetValue(),0
trainingsetindex = self.trainingset.GetValue(),0
"trainingsetindex=trainingsetindex,",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
## LABELING ###,0
"self.sel_config.SetPath("""")",0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
self.cfg = auxiliaryfunctions.read_config(config),0
"self.statusbar.SetStatusText("""")",0
"dlg = wx.MessageDialog(None,""Are you sure?"", ""Quit!"",wx.YES_NO | wx.ICON_WARNING)",0
result = dlg.ShowModal(),0
if result == wx.ID_YES:,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0, 4), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=5)",0
variable initilization,0
design the panel,0
Add logo of DLC,0
#         design the panel,0
Add image of DLC,0
if editing this text make sure you add the '\n' to get the new line. The sizer is unable to format lines correctly.,0
Drop missing body parts,0
Drop points lying outside the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
## dlcnet_ms5: backbone resnet50 + multi-fusion & multi-stage module,0
## dlcr101_ms5/dlcr152_ms5: backbone resnet101/152 + multi-fusion & multi-stage module,0
Ignore possible connections between 'multi' and 'unique' body parts;,0
one can never be too careful...,0
Disable the prediction of PAFs if the graph is empty,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving metadata and data file (Pickle file),0
###############################################################################,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Setting inference cfg file:,0
Search a pose_config.yaml file to parse missing information,0
Clean the training-datasets folder prior to recreating the data pickles,0
Load updated lists:,0
Find the corresponding video file,0
"print(key, ""dropping?"")",0
adding important values for multianiaml project:,0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Determine the index length required to guarantee,0
the train–test ratio is exactly the desired one.,0
Pad indices so lengths agree,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(name,test_video_name)",0
Blazing fast and does not load the image into memory,0
Check that points lie within the image,0
DeprecationWarnings are silenced since Python 3.2 unless triggered in __main__,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
updating variable if null/None! #backwardscompatability,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
these values are dropped as scalecrop,0
doesn't have rotation implemented,0
sort the model directories,0
get the shuffle index and offset by 1.,0
read cfg file,0
create log file,0
Check for variable correctness,0
Moviepy:,0
select crop method,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
TODO: addd cropping as in video analysis!,1
if cropping is not None:,0
cfg['cropping']=True,0
"cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping",0
"print(""Overwriting cropping parameters:"", cropping)",0
"print(""These are used for all videos, but won't be save to the cfg file."")",0
"Also ensure `boundingboxslack` is greater than zero, otherwise overlap",0
"between trackers cannot be evaluated, resulting in empty tracklets.",0
Check which snapshots are available and sort them by # iterations,0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
NOTE: If dataname line above is changed then line below is obsolete?,0
"trackname = trackname.replace(videofolder, destfolder)",0
TODO: adjust this for multi + unique bodyparts!,1
this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. ),0
Optimal identity assignment based on soft voting,0
imagenet mean for resnet pretraining:,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/AlexEMG/DeepLabCut/pull/324",0
for DLC 2.2. (here all set False to not use PAFs/pairwise fields),0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
"reloading defaults, as they can bleed over from a previous run otherwise",0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
dlc_cfg_train = load_config(str(path_train_config)),0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
Name for deeplabcut net (based on its parameters),0
"DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])",0
"print(""Extracting maps for "", DLCscorer, "" with # of trainingiterations:"", trainingsiterations)",0
if notanalyzed: #this only applies to ask if h5 exists...,0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
Avoid overwriting data already on the shelf,0
Avoid overwriting data already on the shelf,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Get animal IDs and corresponding indices in the arrays of detections,0
Form ground truth beforehand,0
Assemble animals on the full set of detections,0
Count the number of unassembled bodyparts,0
Handle unlabeled bodyparts...,0
"Only 1 animal, let us return the full graph indices only",0
Find minimal skeleton,0
Select optimal PAF graph,0
Selective copy; deepcopy is >5x slower,0
Only keeps skeletons that are more than 90% complete,0
TODO Normalize dists by longest length?,1
TODO Smarter imputation technique (Bayesian? Grassmann averages?),1
Covariance matrix estimation fails due to numerical singularities,0
Correct distance to account for missing observations,0
"Alternatively, reduce contribution of missing values to the Mahalanobis",0
distance to zero by substituting the corresponding means.,0
"Fill the subsets with unambiguous, complete individuals",0
Fuse superfluous assemblies,0
Second pass without edge safety,0
Mark the unique body parts as assembled anyway so,0
they are not used later on to fill assemblies.,0
Store selected edges for subsequent frames,0
Remove invalid assemblies,0
Fill assemblies with unconnected body parts,0
Spawning (rather than forking) multiple processes does not,0
work nicely with the GUI or interactive sessions.,0
"In that case, we fall back to the serial assembly.",0
Swap corresponding keypoints,0
Only consider assemblies of at least two keypoints,0
sigma is taken as the median of all COCO keypoint standard deviations,0
Sort matching score (OKS) in descending order of assembly affinity,0
Guarantee precision decreases monotonically,0
See https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173),0
Regularize by forcing AR <= 5,0
max_ar = 5,0
if el.aspect_ratio >= max_ar:,0
if el.height > el.width:,0
el.width = el.height / max_ar,0
else:,0
el.height = el.width / max_ar,0
Orient the ellipse such that it encompasses most points,0
"n_inside = el.contains_points(np.c_[self.x, self.y]).sum()",0
el.theta += 0.5 * np.pi,0
"if el.contains_points(np.c_[self.x, self.y]).sum() < n_inside:",0
el.theta -= 0.5 * np.pi,0
"r2 = chi2.ppf(2 * norm.cdf(sd) - 1, 2)",0
"height, width = np.sqrt(E * r2)",0
The general quadratic curve has the form:,0
ax^2 + 2bxy + cy^2 + 2dx + 2fy + g = 0,0
Ellipse center coordinates,0
Semi-axes lengths,0
Angle to the horizontal,0
High uncertainty to the unobservable initial velocities,0
Give high uncertainty to the unobservable initial velocities,0
diff = val - cost_matrix,0
"diff[row, col] += val",0
if (,0
val < self.iou_threshold,0
or np.any(diff[row] <= 0.2),0
"or np.any(diff[:, col] <= 0.2)",0
):,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
Modified from scipy source code:,0
- to restrict its use to 2D,0
- to get rid of shuffling (since arrays are only (nbodyparts * 3) element long),0
TODO - factor in keypoint confidence (and weight by # of observations??),1
"mat = self.calc_pairwise_oks(poses, poses_ref)",0
Remove matched detections with low OKS,0
matches = [],0
"for row, col in zip(row_indices, col_indices):",0
"if mat[row, col] < self.oks_threshold:",0
unmatched_poses.append(row),0
unmatched_trackers.append(col),0
else:,0
"matches.append([row, col])",0
if not len(matches):,0
"matches = np.empty((0, 2), dtype=int)",0
else:,0
matches = np.stack(matches),0
update matched trackers with assigned detections,0
create and initialise new trackers for unmatched detections,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
filter out matched with low IOU,0
TODO: get cropping parameters and utilize!,1
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Load meta data & annotations,0
get the graph!,0
"NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*",0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
TODO: Make this code not so redundant!,1
"If a string was passed in, auto-convert to True for backward compatibility",0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
"# Also storing one ""large"" table with results:",0
note: evaluationfolder.parents[0] to get common folder above all shuffle evaluations.,0
#################################################,0
Load data...,0
#################################################,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
TODO: IMPLEMENT for different batch sizes?,1
Ignore best edges possibly defined during a prior evaluation,0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
FIXME Is having an empty array vs nan really that necessary?!,1
"Form 2D array of shape (n_rows, 4) where the last dimension",0
"is (sample_index, peak_y, peak_x, bpt_index) to slice the PAFs.",0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in,0
Compute all distance statistics,0
Calculate overall prediction error,0
Skip data-driven skeleton selection unless,0
the model was trained on the full graph.,0
"loading backbone from ResNet, MobileNet etc.",0
"display_iters = max(1,int(displayiters))",0
"[_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})",0
Save snapshot,0
return to original path.,0
Clip peak locations to PAFs dimensions,0
"unit_vecs = vecs / lengths[:, np.newaxis]",0
"affinities = np.squeeze(y @ np.expand_dims(unit_vecs, axis=2)).sum(axis=1)",0
Form cost matrices,0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
Change the stride from 2 to 1 to get 16x downscaling instead of 32x.,0
https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142,0
Multi stage is currently only implemented for resnets,0
Defining multi_fusion backbone,0
Attaching multi-stage decoder,0
stage_paf_output = stage_paf_output + pre_stage_paf_output,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
FIXME Fix wrong scope with Keras layers,1
"def prediction_layer(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2DTranspose(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=2,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
## New DLCNet Addition: multi-stage decoder,0
"def prediction_layer_stage(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2D(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=1,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Compute variance using: Var[X]= E[X^2] - E[X]^2.,0
Compute keep_prob,0
TODO(tanmingxing): add support for training progress.,1
Compute drop_connect tensor,0
turn into x times y time bs * bpts,0
extract corresponding locref x and y as well as probability,0
Filter predicted heatmaps with a 2D Gaussian kernel as in:,0
https://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_The_Devil_Is_in_the_Details_Delving_Into_Unbiased_Data_CVPR_2020_paper.pdf,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
defaults will be a public argument for namedtuple in Python 3.7,0
https://docs.python.org/3/library/collections.html#collections.namedtuple,0
Make sure that round down does not go down by more than 10%.,0
Builds the block accordings to arguments.,0
Expansion phase:,0
Depth-wise convolution phase:,0
Squeeze and Excitation layer.,0
Output phase:,0
only apply drop_connect if skip presents.,0
Expansion phase:,0
Output phase:,0
only apply drop_connect if skip presents.,0
Builds blocks.,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Stem part.,0
Head part.,0
Calls Stem layers,0
Calls blocks.,0
Calls final layers and returns logits.,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"(width_coefficient, depth_coefficient, resolution, dropout_rate)",0
blocks_args = [,0
"'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',",0
"'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',",0
"'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',",0
"'r1_k3_s11_e6_i192_o320_se0.25',",0
],0
The default is TPU-specific batch norm.,0
The alternative is tf.layers.BatchNormalization.,0
"batch_norm=utils.TpuBatchNormalization,  # TPU-specific requirement.",0
ValueError will be raised here if override_params has fields not included,0
in global_params.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
Clamp to 40% of crop size to ensure that at least,0
the center keypoint remains visible after the offset is applied.,0
Points located close to one another are sampled preferentially,0
in order to augment crowded regions.,0
Include keypoints in the count to avoid null probabilities,0
Shift the crop center in both dimensions by random amounts,0
and normalize to the original image dimensions.,0
Pickle the 'data' dictionary using the highest protocol available.,0
item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal,0
"Add smart, keypoint-aware image cropping",0
Scale is sampled only once (per batch) to transform all of the images into same size.,0
Discard keypoints whose coordinates lie outside the cropped image,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
"imageio.imwrite(data_items[i].im_path.split('/')[-1],im)",0
Produce score maps and location refinement fields,0
Find indices of individuals in joint_id,0
WIP!,0
STD of gaussian is 1/4 of threshold,0
Grid of coordinates,0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Distance between the joint point and each coordinate,0
NEEDS TO BE DONE!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
TODO Finish implementing actual abstract class,1
parameter initialization for augmentation pipeline:,0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images of a batch into same size.,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"First, initializing variables (if they don't exist)",0
what is the fraction of training samples with scaling augmentation?,0
loading defaults for rotation range!,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"cfg[""rotateratio""] = cfg.get(",0
"""rotratio"", 0.4",0
)  # what is the fraction of training samples with rotation augmentation?,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"Auto cropping is new (was not in Nature Neuroscience 2018 paper, but introduced in Nath et al. Nat. Protocols 2019)",0
"and boosts performance by 2X, particularly on challenging datasets, like the cheetah in Nath et al.",0
Parameters for augmentation with regard to cropping:,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
Get rid of the interpolation beyond the spline knots,0
Retrieve original individual label indices,0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Check the training image paths are correctly stored as arrays of strings,0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
TENSORPACK could fail in WINDOWS...,0
"NET = ""dlcr101_ms5""",0
Always test a different model from list above,0
Check the training image paths are correctly stored as arrays of strings,0
"""multi_step"": [[0.001, N_ITER]],",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
# Create one identical splits for 3 networks and 3 augmentations,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
Setting specific parameters for training,0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Dowloading the calibration images,0
Deleting unneccesary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
example how to set pose config variables:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
Test bounding box from a single keypoint,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Guarantee that images smaller than crop size are handled fine,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Disable stochastic scale jitter,0
Verify the aspect ratio is preserved,0
"Check data comprise path, shape, and xy coordinates",0
Break the graph to test stitching failure,0
Add fake IDs,0
Split all tracklets in half,0
Test flipped keypoints,0
Original (cached) coordinates must have remained empty,0
Assemble based on the smallest graph to speed up testing,0
Generate fake identity predictions,0
Assemble based on the smallest graph to speed up testing,0
Test now with identity only and ensure assemblies,0
contain only parts of a single group ID.,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
def install(package):,0
"subprocess.check_call([sys.executable, ""-m"", ""pip"", ""install"", package])",0
"install(""tensorflow==1.13.1"")",0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
quicker variant,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
Suppress tensorflow warning messages,0
DLClight version does not support GUIs. Importing accordingly,0
"Train, evaluate & predict functions / all require TF",0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Updating config file:,0
Create the pose_config.yaml files,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path, num_shuffles=auxfun_models.Check4weights(pose_cfg['net_type'], parent_path, num_shuffles= 1)",0
Updating training and test pose_cfg:,0
Read the config file,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
common parameters:,0
Write dictionary to yaml  config file,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
Flatten the list of bodyparts to connect,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Start plotting for every frame,0
"Once all the frames are saved, then make a movie using ffmpeg.",0
Create the fig and define the axes,0
Clear plot and initialize the variables,0
Initialize arrays for appending the 3d data for actual plotting the points,0
Initialize arrays for appending the 3d data for drawing the lines,0
Initialize arrays for appending the 2d data from cam1 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam2 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam1 for drawing the lines,0
Initialize arrays for appending the 2d data from cam2 for drawing the lines,0
Get the scorer names from the dataframe,0
"Set the x,y, and z limits for the 3d view",0
"Set the frame number to read#max(0,index-trailpoints):index",0
Plot the labels for each body part,0
Connecting the bodyparts specified in the config file.3d file is created based on the likelihoods of cam1 and cam2. Using 3d file and check if the body part is nan then dont plot skeleton,0
Saving the frames,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
Extract the indices of frames where the likelihood of a bodypart for both cameras are less than pvalue,0
Extract frames where likelihood for both the views is less than the pcutoff,0
"low_likelihood_frames = np.all(likelihoods < pcutoff, axis=1)",0
For cam1 camera: Assign nans to x and y values of a bodypart where the likelihood for is less than pvalue,0
For cam2 camera: Assign nans to x and y values of a bodypart where the likelihood is less than pvalue,0
ToDo: speed up func. below by saving in numpy.array,1
"Create an empty dataframe to store x,y,z of 3d data",0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Undistorting the points from cam2 camera,0
Save the undistorted files,0
Alternatively,0
Sort tracklets by length to prioritize greater continuity,0
Drop tracklets that are too short,0
"Where slots are available, copy the data over",0
"If about to overwrite data, keep tracklets with highest confidence",0
Squeeze some data into empty slots,0
"For the remaining data, overwrite where we are least confident",0
Find keypoints closest to the remaining data,0
Use Manhattan distance to avoid overflow,0
Only overwrite if improving confidence,0
Fill existing gaps,0
Retrieve original individual label indices,0
Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.,0
Fill existing gaps,0
Retrieve original individual label indices,0
ID swaps occur when X and Y simultaneously intersect each other.,0
Get only those bodypart pairs that belong to different individuals,0
"See Li et al., 2012. Cross-view Activity Recognition using Hankelets.",0
"As proposed in the paper, the Hankel matrix can either be formed from",0
the tracklet's centroid or its normalized velocity.,0
"vel = np.diff(self.centroid, axis=0)",0
"vel /= np.linalg.norm(vel, axis=1, keepdims=True)",0
return self.hankelize(vel),0
TODO Add missing data imputation,1
"nrows, ncols = mat.shape",0
beta = nrows / ncols,0
omega = 0.56 * beta ** 3 - 0.95 * beta ** 2 + 1.82 * beta + 1.43,0
return np.argmin(s > omega * np.median(s)),0
"Note that if tracklets are very short, some may actually be part of the same track",0
and thus incorrectly reflect separate track endpoints...,0
"Map each Tracklet to an entry and output nodes and vice versa,",0
which is convenient once the tracklets are stitched.,0
TODO Avoid looping over all pairs of tracklets,0
The algorithm works better with integer weights,0
Let us prune the graph by removing all source and sink edges,0
but those connecting the `n_tracks` first and last tracklets.,0
Preflow push seems to work slightly better than shortest,0
"augmentation path..., and is more computationally efficient.",0
Verify whether there are overlapping tracklets,0
"Pick the segment that minimizes ""smoothness"", computed here",0
with the coefficient of variation of the differences.,0
Rebuild a full graph from the remaining nodes without,0
temporal constraint on what tracklets can be stitched together.,0
Cycle through the residuals and incorporate back those,0
that only fit in a single tracklet.,0
Greedily add the remaining residuals,0
Refresh temporal bounds,0
Default to the distance cost function,0
Add in identity weighing before building the graph,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
average distance and average # significant differences avg. over comparisonbodyparts,0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
###################################################,0
Dependencies,0
###################################################,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as,0
specified in the config file,0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
Prepare figure,0
Mask rather than drop unwanted bodyparts to ensure consistent coloring,0
cropping!,0
nx=X2-X1,0
!/usr/bin/env python3,0
The above already raises a ValueError if formatting is wrong,0
"Rotate, see: https://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg",0
interesting option to just update metadata.,0
"Historically DLC used: from scipy.misc import imread, imresize >> deprecated functions",0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
Read the video until a frame is successfully read,0
import platform,0
"if platform.system() == ""Darwin"":  # for OSX use WXAgg",0
fig.canvas.start_event_loop(timeout=-1),0
else:,0
#fig.canvas.stop_event_loop(),0
"Windows throws error if file path is > 260 characters, can fix with prefix.",0
See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation,0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
"Attention this order has to be consistent (for training set creation, training, inference etc.)",0
"-> adding (single,bpt) for uniquebodyparts",0
"-> adding (indivdual,bpt) for multianimalbodyparts",0
load defaults,0
set project specific parameters:,0
###################################################,0
Dependencies,0
###################################################,0
Pose X vs pose Y,0
Poses vs time,0
Likelihoods,0
Histograms,0
#################################################,0
Looping analysis over video,0
#################################################,0
Keep only the individuals and bodyparts that were labeled,0
Either display the animals defined in the config if they are found,0
"in the dataframe, or all the trajectories regardless of their names",0
Determine whether the data are single- or multi-animal without loading into memory,0
simply by checking whether 'individuals' is in the second line of the CSV.,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"if ""MULTI!"" in allbpts:",0
Ensure same order as in config.yaml,0
ABBREVIATE NETWORK NAMES -- esp. for mobilenet!,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
Iterate over data files and stop as soon as one matching the scorer is found,0
If there was no match...,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
Read the infer config file,0
let the user open the file with default text editor. Also make it mac compatible,0
self.save.Enable(False),0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = io.imread(img),0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
topSplitter = wx.SplitterWindow(self),0
,0
"self.image_panel = ImagePanel(topSplitter, config,video,shuffle,Dataframe,self.gui_size)",0
self.widget_panel = WidgetPanel(topSplitter),0
,0
"topSplitter.SplitHorizontally(self.image_panel, self.widget_panel,sashPosition=self.gui_size[1]*0.83)#0.9",0
topSplitter.SetSashGravity(1),0
sizer = wx.BoxSizer(wx.VERTICAL),0
"sizer.Add(topSplitter, 1, wx.EXPAND)",0
self.SetSizer(sizer),0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
"cv2.imwrite(img_name, frame)",0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0,10), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=10)",0
Add all the options,0
line = wx.StaticLine(self),0
"self.sizer.Add(line, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=10)",0
,0
"self.multi_choice.Bind(wx.EVT_CHECKBOX,self.activate_copy_videos)",0
Hide the button as this is not the default option,0
self.vids.Enable(False),0
self.SetSizer(self.sizer),0
"self.sizer.Add(self.sizer, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=15)",0
self.ok.Enable(False),0
else:,0
self.ok.Enable(True),0
For mac compatibility,0
Remove the pages in case the user goes back to the create new project and creates/load a new project,0
Add all the other pages,0
self.yes.Enable(False),0
self.no.Enable(False),0
Here we create a panel and a notebook on the panel,0
create the page windows as children of the notebook and add the pages to the notebook with the label to show on the tab,0
Take into consideration imprecise OpenCV estimation of total number of frames,0
Automatically disable the draggable points,0
Save additional frames to the labeled-data directory,0
Store the newly-refined data,0
Uncertain keypoints are ignored,0
Merge with the already existing annotated data,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.buttonCounter = [],0
self.bodyparts = [],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
load labeledPoints and fundamental Matrix,0
Find offset terms for drawing epipolar Lines,0
Get crop params for camera being labeled,0
Get crop params for other camera,0
convert the image to RGB as you are showing the image with matplotlib,0
draw epipolar lines,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
variable initilization,0
design the panel,0
Add logo of DLC,0
self.filelist = self.filelist + self.vids,0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
load labeledPoints and fundamental Matrix,0
Find offset terms for drawing epipolar Lines,0
Get crop params for camera being labeled,0
Get crop params for other camera,0
convert the image to RGB as you are showing the image with matplotlib,0
draw epipolar lines,0
divider = make_axes_locatable(self.axes),0
"colorIndex = np.linspace(np.min(im),np.max(im),len(bodyparts))",0
"cax = divider.append_axes(""right"", size=""5%"", pad=0.05)",0
"cbar = self.figure.colorbar(ax, cax=cax,spacing='proportional', ticks=colorIndex)",0
cbar.set_ticklabels(bodyparts[::-1]),0
self.figure.canvas.draw(),0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"MainFrame.saveDataSet(self, event)",0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
checks for unique bodyparts,0
"Reading the existing dataset,if already present",0
Handle data previously labeled on a different platform,0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Create an empty data frame,0
Cache original bodyparts,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Rearrange bodypart columns in config order,0
Test whether there are missing frames and superfluous data,0
Check whether new individuals or body parts were added,0
Checking if user added a new label,0
Found new labels in either multiple bodyparts or unique bodyparts,0
check if single is slected when radio buttons are changed,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Discard data associated with bodyparts that are no longer in the config,0
Re-organize the dataframe so the CSV looks consistent with the config,0
restore the background region,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.build = wx.Button(self, label=""Build skeleton"")",0
"sizer.Add(self.build, pos=(4, 3), flag=wx.BOTTOM | wx.RIGHT, border=10)",0
"self.build.Bind(wx.EVT_BUTTON, self.build_skeleton)",0
self.build.Enable(True),0
"def build_skeleton(self, event):",0
skeleton.SkeletonBuilder(self.config),0
Settting the GUI size and panels design,0
Checks if zoom/pan button is ON,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
use the default pose_cfg file for default values,0
self.display_iters.Enable(False),0
self.save_iters.Enable(False),0
self.max_iters.Enable(False),0
self.snapshots.Enable(False),0
Read the pose config file,0
"print(os.path.join(cfg['project_path'],auxiliaryfunctions.GetModelFolder(trainFraction, self.shuffles.GetValue(),cfg),'train','pose_cfg.yaml'))",0
let the user open the file with default text editor. Also make it mac compatible,0
update the variables with the edited values in the pose config file,0
variable initilization,0
design the panel,0
Add logo of DLC,0
Read the infer config file,0
let the user open the file with default text editor. Also make it mac compatible,0
"def video_tracklets(self,event):",0
shuffle = self.shuffle.GetValue(),0
trainingsetindex = self.trainingset.GetValue(),0
"deeplabcut.create_video_from_pickled_tracks(self.filelist, picklefile, pcutoff=0.6)",0
"self.select_destfolder.SetPath(""None"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.shuffles.SetValue(""1"")",0
"DownSampleVideo(vname,width=-1,height=200,outsuffix='downsampled',outpath=None,rotatecw=False):",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"majorDimension=0,",0
"self.videotype.SetStringSelection("".avi"")",0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
"self.choiceBox,self.visualization_rdb = self.choice_panel.addRadioButtons()",0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
Overwrite the config-defined individual names,0
with those actually present in the annotated data,0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
print(self.Dataframe.head()),0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
fix move to corner,0
fix move to corner,0
fix move to corner,0
fix move to corner,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"if self.plot_scoremaps.GetStringSelection() == ""Yes"":",0
Read the infer config file,0
let the user open the file with default text editor. Also make it mac compatible,0
shuffle = self.shuffle.GetValue(),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
self.cfg = auxiliaryfunctions.read_config(config),0
"self.statusbar.SetStatusText("""")",0
"dlg = wx.MessageDialog(None,""Are you sure?"", ""Quit!"",wx.YES_NO | wx.ICON_WARNING)",0
result = dlg.ShowModal(),0
if result == wx.ID_YES:,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0, 4), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=5)",0
variable initilization,0
design the panel,0
Add logo of DLC,0
#         design the panel,0
Add image of DLC,0
if editing this text make sure you add the '\n' to get the new line. The sizer is unable to format lines correctly.,0
Drop missing body parts,0
Drop points lying outside the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
## dlcnet_ms5: backbone resnet50 + multi-fusion & multi-stage module,0
## dlcr101_ms5/dlcr152_ms5: backbone resnet101/152 + multi-fusion & multi-stage module,0
Ignore possible connections between 'multi' and 'unique' body parts;,0
one can never be too careful...,0
Disable the prediction of PAFs if the graph is empty,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving metadata and data file (Pickle file),0
###############################################################################,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Setting inference cfg file:,0
Search a pose_config.yaml file to parse missing information,0
Clean the training-datasets folder prior to recreating the data pickles,0
Load updated lists:,0
Find the corresponding video file,0
"print(key, ""dropping?"")",0
adding important values for multianiaml project:,0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Let's check if the code is *not* run on windows (Source: #https://stackoverflow.com/questions/1325581/how-do-i-check-if-im-running-on-windows-in-python),0
but the paths are in windows format...,0
Determine the index length required to guarantee,0
the train–test ratio is exactly the desired one.,0
Pad indices so lengths agree,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(index,name.split(os.sep)[1])",0
"print(name,test_video_name)",0
Blazing fast and does not load the image into memory,0
Check that points lie within the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
updating variable if null/None! #backwardscompatability,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
these values are dropped as scalecrop,0
doesn't have rotation implemented,0
sort the models directories,0
get the shuffle index,0
read cfg file,0
create log file,0
Check for variable correctness,0
Moviepy:,0
select crop method,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
from skimage.io import imread,0
"frame=imread(os.path.join(directory,framename),mode='RGB')",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
TODO: addd cropping as in video analysis!,1
if cropping is not None:,0
cfg['cropping']=True,0
"cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping",0
"print(""Overwriting cropping parameters:"", cropping)",0
"print(""These are used for all videos, but won't be save to the cfg file."")",0
Check which snapshots are available and sort them by # iterations,0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
NOTE: If dataname line above is changed then line below is obsolete?,0
"trackname = trackname.replace(videofolder, destfolder)",0
TODO: adjust this for multi + unique bodyparts!,1
this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. ),0
Optimal identity assignment based on soft voting,0
imagenet mean for resnet pretraining:,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/AlexEMG/DeepLabCut/pull/324",0
for DLC 2.2. (here all set False to not use PAFs/pairwise fields),0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
"reloading defaults, as they can bleed over from a previous run otherwise",0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
dlc_cfg_train = load_config(str(path_train_config)),0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
Name for deeplabcut net (based on its parameters),0
"DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])",0
"print(""Extracting maps for "", DLCscorer, "" with # of trainingiterations:"", trainingsiterations)",0
if notanalyzed: #this only applies to ask if h5 exists...,0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Get animal IDs and corresponding indices in the arrays of detections,0
Form ground truth beforehand,0
Assemble animals on the full set of detections,0
Count the number of unassembled bodyparts,0
Handle unlabeled bodyparts...,0
"Only 1 animal, let us return the full graph indices only",0
Find minimal skeleton,0
Select optimal PAF graph,0
Selective copy; deepcopy is >5x slower,0
Only keeps skeletons that are more than 90% complete,0
TODO Normalize dists by longest length?,1
TODO Smarter imputation technique (Bayesian? Grassmann averages?),1
Covariance matrix estimation fails due to numerical singularities,0
Correct distance to account for missing observations,0
"Alternatively, reduce contribution of missing values to the Mahalanobis",0
distance to zero by substituting the corresponding means.,0
"Fill the subsets with unambiguous, complete individuals",0
Fuse superfluous assemblies,0
Second pass without edge safety,0
Mark the unique body parts as assembled anyway so,0
they are not used later on to fill assemblies.,0
Store selected edges for subsequent frames,0
Remove invalid assemblies,0
Fill assemblies with unconnected body parts,0
Spawning (rather than forking) multiple processes does not,0
work nicely with the GUI or interactive sessions.,0
"In that case, we fall back to the serial assembly.",0
Swap corresponding keypoints,0
sigma is taken as the median of all COCO keypoint standard deviations,0
Sort matching score (OKS) in descending order of assembly affinity,0
Guarantee precision decreases monotonically,0
See https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173),0
define constant velocity model,0
Regularize by forcing AR <= 5,0
max_ar = 5,0
if el.aspect_ratio >= max_ar:,0
if el.height > el.width:,0
el.width = el.height / max_ar,0
else:,0
el.height = el.width / max_ar,0
Orient the ellipse such that it encompasses most points,0
"n_inside = el.contains_points(np.c_[self.x, self.y]).sum()",0
el.theta += 0.5 * np.pi,0
"if el.contains_points(np.c_[self.x, self.y]).sum() < n_inside:",0
el.theta -= 0.5 * np.pi,0
"r2 = chi2.ppf(2 * norm.cdf(sd) - 1, 2)",0
"height, width = np.sqrt(E * r2)",0
The general quadratic curve has the form:,0
ax^2 + 2bxy + cy^2 + 2dx + 2fy + g = 0,0
Ellipse center coordinates,0
Semi-axes lengths,0
Angle to the horizontal,0
Reset tracker IDs,0
diff = val - cost_matrix,0
"diff[row, col] += val",0
if (,0
val < self.iou_threshold,0
or np.any(diff[row] <= 0.2),0
"or np.any(diff[:, col] <= 0.2)",0
):,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
TODO Try particle filter (since we already have the keypoints),1
Modified from scipy source code:,0
- to restrict its use to 2D,0
- to get rid of shuffling (since arrays are only (nbodyparts * 3) element long),0
TODO - factor in keypoint confidence (and weight by # of observations??),1
"mat = self.calc_pairwise_oks(poses, poses_ref)",0
Remove matched detections with low OKS,0
matches = [],0
"for row, col in zip(row_indices, col_indices):",0
"if mat[row, col] < self.oks_threshold:",0
unmatched_poses.append(row),0
unmatched_trackers.append(col),0
else:,0
"matches.append([row, col])",0
if not len(matches):,0
"matches = np.empty((0, 2), dtype=int)",0
else:,0
matches = np.stack(matches),0
filter out matched with low IOU,0
get predicted locations from existing trackers.,0
update matched trackers with assigned detections,0
create and initialise new trackers for unmatched detections,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
TODO: get cropping parameters and utilize!,1
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Load meta data & annotations,0
get the graph!,0
"NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*",0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
TODO: Make this code not so redundant!,1
"If a string was passed in, auto-convert to True for backward compatibility",0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
"# Also storing one ""large"" table with results:",0
note: evaluationfolder.parents[0] to get common folder above all shuffle evaluations.,0
#################################################,0
Load data...,0
#################################################,0
Loading human annotatated data,0
Handle data previously annotated on a different platform,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
TODO: IMPLEMENT for different batch sizes?,1
Ignore best edges possibly defined during a prior evaluation,0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
FIXME Is having an empty array vs nan really that necessary?!,1
"Form 2D array of shape (n_rows, 4) where the last dimension",0
"is (sample_index, peak_y, peak_x, bpt_index) to slice the PAFs.",0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in,0
Compute all distance statistics,0
Calculate overall prediction error,0
Skip data-driven skeleton selection unless,0
the model was trained on the full graph.,0
"loading backbone from ResNet, MobileNet etc.",0
"display_iters = max(1,int(displayiters))",0
"[_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})",0
Save snapshot,0
return to original path.,0
Clip peak locations to PAFs dimensions,0
"unit_vecs = vecs / lengths[:, np.newaxis]",0
"affinities = np.squeeze(y @ np.expand_dims(unit_vecs, axis=2)).sum(axis=1)",0
Form cost matrices,0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
Change the stride from 2 to 1 to get 16x downscaling instead of 32x.,0
https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142,0
Multi stage is currently only implemented for resnets,0
Defining multi_fusion backbone,0
Attaching multi-stage decoder,0
stage_paf_output = stage_paf_output + pre_stage_paf_output,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
FIXME Fix wrong scope with Keras layers,1
"def prediction_layer(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2DTranspose(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=2,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
## New DLCNet Addition: multi-stage decoder,0
"def prediction_layer_stage(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2D(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=1,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Compute variance using: Var[X]= E[X^2] - E[X]^2.,0
Compute keep_prob,0
TODO(tanmingxing): add support for training progress.,1
Compute drop_connect tensor,0
turn into x times y time bs * bpts,0
extract corresponding locref x and y as well as probability,0
Filter predicted heatmaps with a 2D Gaussian kernel as in:,0
https://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_The_Devil_Is_in_the_Details_Delving_Into_Unbiased_Data_CVPR_2020_paper.pdf,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
defaults will be a public argument for namedtuple in Python 3.7,0
https://docs.python.org/3/library/collections.html#collections.namedtuple,0
Make sure that round down does not go down by more than 10%.,0
Builds the block accordings to arguments.,0
Expansion phase:,0
Depth-wise convolution phase:,0
Squeeze and Excitation layer.,0
Output phase:,0
only apply drop_connect if skip presents.,0
Expansion phase:,0
Output phase:,0
only apply drop_connect if skip presents.,0
Builds blocks.,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Stem part.,0
Head part.,0
Calls Stem layers,0
Calls blocks.,0
Calls final layers and returns logits.,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"(width_coefficient, depth_coefficient, resolution, dropout_rate)",0
blocks_args = [,0
"'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',",0
"'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',",0
"'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',",0
"'r1_k3_s11_e6_i192_o320_se0.25',",0
],0
The default is TPU-specific batch norm.,0
The alternative is tf.layers.BatchNormalization.,0
"batch_norm=utils.TpuBatchNormalization,  # TPU-specific requirement.",0
ValueError will be raised here if override_params has fields not included,0
in global_params.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
Clamp to 40% of crop size to ensure that at least,0
the center keypoint remains visible after the offset is applied.,0
Points located close to one another are sampled preferentially,0
in order to augment crowded regions.,0
Include keypoints in the count to avoid null probabilities,0
Shift the crop center in both dimensions by random amounts,0
and normalize to the original image dimensions.,0
Pickle the 'data' dictionary using the highest protocol available.,0
item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal,0
"Add smart, keypoint-aware image cropping",0
Scale is sampled only once (per batch) to transform all of the images into same size.,0
Discard keypoints whose coordinates lie outside the cropped image,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
"imageio.imwrite(data_items[i].im_path.split('/')[-1],im)",0
Produce score maps and location refinement fields,0
Find indices of individuals in joint_id,0
WIP!,0
STD of gaussian is 1/4 of threshold,0
Grid of coordinates,0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Distance between the joint point and each coordinate,0
NEEDS TO BE DONE!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
TODO Finish implementing actual abstract class,1
parameter initialization for augmentation pipeline:,0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images of a batch into same size.,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"First, initializing variables (if they don't exist)",0
what is the fraction of training samples with scaling augmentation?,0
loading defaults for rotation range!,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"cfg[""rotateratio""] = cfg.get(",0
"""rotratio"", 0.4",0
)  # what is the fraction of training samples with rotation augmentation?,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"Auto cropping is new (was not in Nature Neuroscience 2018 paper, but introduced in Nath et al. Nat. Protocols 2019)",0
"and boosts performance by 2X, particularly on challenging datasets, like the cheetah in Nath et al.",0
Parameters for augmentation with regard to cropping:,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
Get rid of the interpolation beyond the spline knots,0
Retrieve original individual label indices,0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
"net_type = ""mobilenet_v2_0.35""",0
"net_type = ""efficientnet-b0""  # to -b6",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
TENSORPACK could fail in WINDOWS...,0
"NET = ""dlcr101_ms5""",0
"NET = ""resnet_152""",0
"NET = ""efficientnet-b0""",0
"NET = ""mobilenet_v2_0.35"" # should be fixed",0
"""multi_step"": [[0.001, N_ITER]],",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
# Create one split and make Shuffle 2 and 3 have the same split.,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
for EfficientNet,0
imgaug,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
# Create one split and make Shuffle 2 and 3 have the same split.,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Dowloading the calibration images,0
Deleting unneccesary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
example how to set pose config variables:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
Test bounding box from a single keypoint,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Guarantee that images smaller than crop size are handled fine,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Disable stochastic scale jitter,0
Verify the aspect ratio is preserved,0
Break the graph to test stitching failure,0
Add fake IDs,0
Split all tracklets in half,0
Test flipped keypoints,0
Assemble based on the smallest graph to speed up testing,0
Generate fake identity predictions,0
Assemble based on the smallest graph to speed up testing,0
Test now with identity only and ensure assemblies,0
contain only parts of a single group ID.,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
def install(package):,0
"subprocess.check_call([sys.executable, ""-m"", ""pip"", ""install"", package])",0
"install(""tensorflow==1.13.1"")",0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
quicker variant,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
Suppress tensorflow warning messages,0
DLClight version does not support GUIs. Importing accordingly,0
"Train, evaluate & predict functions / all require TF",0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Updating config file:,0
Create the pose_config.yaml files,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path, num_shuffles=auxfun_models.Check4weights(pose_cfg['net_type'], parent_path, num_shuffles= 1)",0
Updating training and test pose_cfg:,0
Read the config file,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
common parameters:,0
Write dictionary to yaml  config file,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
Flatten the list of bodyparts to connect,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Start plotting for every frame,0
"Once all the frames are saved, then make a movie using ffmpeg.",0
Create the fig and define the axes,0
Clear plot and initialize the variables,0
Initialize arrays for appending the 3d data for actual plotting the points,0
Initialize arrays for appending the 3d data for drawing the lines,0
Initialize arrays for appending the 2d data from cam1 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam2 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam1 for drawing the lines,0
Initialize arrays for appending the 2d data from cam2 for drawing the lines,0
Get the scorer names from the dataframe,0
"Set the x,y, and z limits for the 3d view",0
"Set the frame number to read#max(0,index-trailpoints):index",0
Plot the labels for each body part,0
Connecting the bodyparts specified in the config file.3d file is created based on the likelihoods of cam1 and cam2. Using 3d file and check if the body part is nan then dont plot skeleton,0
Saving the frames,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
Extract the indices of frames where the likelihood of a bodypart for both cameras are less than pvalue,0
Extract frames where likelihood for both the views is less than the pcutoff,0
"low_likelihood_frames = np.all(likelihoods < pcutoff, axis=1)",0
For cam1 camera: Assign nans to x and y values of a bodypart where the likelihood for is less than pvalue,0
For cam2 camera: Assign nans to x and y values of a bodypart where the likelihood is less than pvalue,0
ToDo: speed up func. below by saving in numpy.array,1
"Create an empty dataframe to store x,y,z of 3d data",0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Undistorting the points from cam2 camera,0
Save the undistorted files,0
Alternatively,0
Sort tracklets by length to prioritize greater continuity,0
Drop tracklets that are too short,0
"Where slots are available, copy the data over",0
"If about to overwrite data, keep tracklets with highest confidence",0
Squeeze some data into empty slots,0
"For the remaining data, overwrite where we are least confident",0
Find keypoints closest to the remaining data,0
Use Manhattan distance to avoid overflow,0
Only overwrite if improving confidence,0
Fill existing gaps,0
Retrieve original individual label indices,0
Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.,0
Fill existing gaps,0
Retrieve original individual label indices,0
ID swaps occur when X and Y simultaneously intersect each other.,0
Get only those bodypart pairs that belong to different individuals,0
"See Li et al., 2012. Cross-view Activity Recognition using Hankelets.",0
"As proposed in the paper, the Hankel matrix can either be formed from",0
the tracklet's centroid or its normalized velocity.,0
"vel = np.diff(self.centroid, axis=0)",0
"vel /= np.linalg.norm(vel, axis=1, keepdims=True)",0
return self.hankelize(vel),0
TODO Add missing data imputation,1
"nrows, ncols = mat.shape",0
beta = nrows / ncols,0
omega = 0.56 * beta ** 3 - 0.95 * beta ** 2 + 1.82 * beta + 1.43,0
return np.argmin(s > omega * np.median(s)),0
"Note that if tracklets are very short, some may actually be part of the same track",0
and thus incorrectly reflect separate track endpoints...,0
"Map each Tracklet to an entry and output nodes and vice versa,",0
which is convenient once the tracklets are stitched.,0
TODO Avoid looping over all pairs of tracklets,0
The algorithm works better with integer weights,0
Let us prune the graph by removing all source and sink edges,0
but those connecting the `n_tracks` first and last tracklets.,0
Preflow push seems to work slightly better than shortest,0
"augmentation path..., and is more computationally efficient.",0
Verify whether there are overlapping tracklets,0
"Pick the segment that minimizes ""smoothness"", computed here",0
with the coefficient of variation of the differences.,0
Rebuild a full graph from the remaining nodes without,0
temporal constraint on what tracklets can be stitched together.,0
Cycle through the residuals and incorporate back those,0
that only fit in a single tracklet.,0
Greedily add the remaining residuals,0
Refresh temporal bounds,0
Default to the distance cost function,0
Add in identity weighing before building the graph,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
average distance and average # significant differences avg. over comparisonbodyparts,0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
###################################################,0
Dependencies,0
###################################################,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as,0
specified in the config file,0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
Prepare figure,0
Mask rather than drop unwanted bodyparts to ensure consistent coloring,0
cropping!,0
nx=X2-X1,0
!/usr/bin/env python3,0
The above already raises a ValueError if formatting is wrong,0
"Rotate, see: https://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg",0
interesting option to just update metadata.,0
"Historically DLC used: from scipy.misc import imread, imresize >> deprecated functions",0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
Read the video until a frame is successfully read,0
import platform,0
"if platform.system() == ""Darwin"":  # for OSX use WXAgg",0
fig.canvas.start_event_loop(timeout=-1),0
else:,0
#fig.canvas.stop_event_loop(),0
"Windows throws error if file path is > 260 characters, can fix with prefix.",0
See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation,0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
"Attention this order has to be consistent (for training set creation, training, inference etc.)",0
"-> adding (single,bpt) for uniquebodyparts",0
"-> adding (indivdual,bpt) for multianimalbodyparts",0
load defaults,0
set project specific parameters:,0
###################################################,0
Dependencies,0
###################################################,0
Pose X vs pose Y,0
Poses vs time,0
Likelihoods,0
Histograms,0
#################################################,0
Looping analysis over video,0
#################################################,0
Keep only the individuals and bodyparts that were labeled,0
Either display the animals defined in the config if they are found,0
"in the dataframe, or all the trajectories regardless of their names",0
Determine whether the data are single- or multi-animal without loading into memory,0
simply by checking whether 'individuals' is in the second line of the CSV.,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"if ""MULTI!"" in allbpts:",0
Ensure same order as in config.yaml,0
ABBREVIATE NETWORK NAMES -- esp. for mobilenet!,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
Iterate over data files and stop as soon as one matching the scorer is found,0
If there was no match...,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
Read the infer config file,0
let the user open the file with default text editor. Also make it mac compatible,0
self.save.Enable(False),0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = io.imread(img),0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
topSplitter = wx.SplitterWindow(self),0
,0
"self.image_panel = ImagePanel(topSplitter, config,video,shuffle,Dataframe,self.gui_size)",0
self.widget_panel = WidgetPanel(topSplitter),0
,0
"topSplitter.SplitHorizontally(self.image_panel, self.widget_panel,sashPosition=self.gui_size[1]*0.83)#0.9",0
topSplitter.SetSashGravity(1),0
sizer = wx.BoxSizer(wx.VERTICAL),0
"sizer.Add(topSplitter, 1, wx.EXPAND)",0
self.SetSizer(sizer),0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
"cv2.imwrite(img_name, frame)",0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0,10), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=10)",0
Add all the options,0
line = wx.StaticLine(self),0
"self.sizer.Add(line, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=10)",0
,0
"self.multi_choice.Bind(wx.EVT_CHECKBOX,self.activate_copy_videos)",0
Hide the button as this is not the default option,0
self.vids.Enable(False),0
self.SetSizer(self.sizer),0
"self.sizer.Add(self.sizer, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=15)",0
self.ok.Enable(False),0
else:,0
self.ok.Enable(True),0
For mac compatibility,0
Remove the pages in case the user goes back to the create new project and creates/load a new project,0
Add all the other pages,0
self.yes.Enable(False),0
self.no.Enable(False),0
Here we create a panel and a notebook on the panel,0
create the page windows as children of the notebook and add the pages to the notebook with the label to show on the tab,0
Take into consideration imprecise OpenCV estimation of total number of frames,0
Automatically disable the draggable points,0
Save additional frames to the labeled-data directory,0
Store the newly-refined data,0
Uncertain keypoints are ignored,0
Merge with the already existing annotated data,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.buttonCounter = [],0
self.bodyparts = [],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
load labeledPoints and fundamental Matrix,0
Find offset terms for drawing epipolar Lines,0
Get crop params for camera being labeled,0
Get crop params for other camera,0
draw epipolar lines,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
variable initilization,0
design the panel,0
Add logo of DLC,0
self.filelist = self.filelist + self.vids,0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
load labeledPoints and fundamental Matrix,0
Find offset terms for drawing epipolar Lines,0
Get crop params for camera being labeled,0
Get crop params for other camera,0
im = cv2.imread(img),0
convert the image to RGB as you are showing the image with matplotlib,0
draw epipolar lines,0
divider = make_axes_locatable(self.axes),0
"colorIndex = np.linspace(np.min(im),np.max(im),len(bodyparts))",0
"cax = divider.append_axes(""right"", size=""5%"", pad=0.05)",0
"cbar = self.figure.colorbar(ax, cax=cax,spacing='proportional', ticks=colorIndex)",0
cbar.set_ticklabels(bodyparts[::-1]),0
self.figure.canvas.draw(),0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"MainFrame.saveDataSet(self, event)",0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
checks for unique bodyparts,0
"Reading the existing dataset,if already present",0
Handle data previously labeled on a different platform,0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Create an empty data frame,0
Cache original bodyparts,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Rearrange bodypart columns in config order,0
Test whether there are missing frames and superfluous data,0
Check whether new individuals or body parts were added,0
Checking if user added a new label,0
Found new labels in either multiple bodyparts or unique bodyparts,0
check if single is slected when radio buttons are changed,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Discard data associated with bodyparts that are no longer in the config,0
Re-organize the dataframe so the CSV looks consistent with the config,0
restore the background region,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.build = wx.Button(self, label=""Build skeleton"")",0
"sizer.Add(self.build, pos=(4, 3), flag=wx.BOTTOM | wx.RIGHT, border=10)",0
"self.build.Bind(wx.EVT_BUTTON, self.build_skeleton)",0
self.build.Enable(True),0
"def build_skeleton(self, event):",0
skeleton.SkeletonBuilder(self.config),0
Settting the GUI size and panels design,0
Checks if zoom/pan button is ON,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
use the default pose_cfg file for default values,0
self.display_iters.Enable(False),0
self.save_iters.Enable(False),0
self.max_iters.Enable(False),0
self.snapshots.Enable(False),0
Read the pose config file,0
"print(os.path.join(cfg['project_path'],auxiliaryfunctions.GetModelFolder(trainFraction, self.shuffles.GetValue(),cfg),'train','pose_cfg.yaml'))",0
let the user open the file with default text editor. Also make it mac compatible,0
update the variables with the edited values in the pose config file,0
variable initilization,0
design the panel,0
Add logo of DLC,0
Read the infer config file,0
let the user open the file with default text editor. Also make it mac compatible,0
"def video_tracklets(self,event):",0
shuffle = self.shuffle.GetValue(),0
trainingsetindex = self.trainingset.GetValue(),0
"deeplabcut.create_video_from_pickled_tracks(self.filelist, picklefile, pcutoff=0.6)",0
"self.select_destfolder.SetPath(""None"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.shuffles.SetValue(""1"")",0
"DownSampleVideo(vname,width=-1,height=200,outsuffix='downsampled',outpath=None,rotatecw=False):",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"majorDimension=0,",0
"self.videotype.SetStringSelection("".avi"")",0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
"self.choiceBox,self.visualization_rdb = self.choice_panel.addRadioButtons()",0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
Overwrite the config-defined individual names,0
with those actually present in the annotated data,0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
print(self.Dataframe.head()),0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
fix move to corner,0
fix move to corner,0
fix move to corner,0
fix move to corner,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"if self.plot_scoremaps.GetStringSelection() == ""Yes"":",0
Read the infer config file,0
let the user open the file with default text editor. Also make it mac compatible,0
shuffle = self.shuffle.GetValue(),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
self.cfg = auxiliaryfunctions.read_config(config),0
"self.statusbar.SetStatusText("""")",0
"dlg = wx.MessageDialog(None,""Are you sure?"", ""Quit!"",wx.YES_NO | wx.ICON_WARNING)",0
result = dlg.ShowModal(),0
if result == wx.ID_YES:,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0, 4), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=5)",0
variable initilization,0
design the panel,0
Add logo of DLC,0
#         design the panel,0
Add image of DLC,0
if editing this text make sure you add the '\n' to get the new line. The sizer is unable to format lines correctly.,0
Drop missing body parts,0
Drop points lying outside the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
## dlcnet_ms5: backbone resnet50 + multi-fusion & multi-stage module,0
## dlcr101_ms5/dlcr152_ms5: backbone resnet101/152 + multi-fusion & multi-stage module,0
Ignore possible connections between 'multi' and 'unique' body parts;,0
one can never be too careful...,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving metadata and data file (Pickle file),0
###############################################################################,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Setting inference cfg file:,0
Search a pose_config.yaml file to parse missing information,0
Clean the training-datasets folder prior to recreating the data pickles,0
Load updated lists:,0
Find the corresponding video file,0
"print(key, ""dropping?"")",0
adding important values for multianiaml project:,0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Let's check if the code is *not* run on windows (Source: #https://stackoverflow.com/questions/1325581/how-do-i-check-if-im-running-on-windows-in-python),0
but the paths are in windows format...,0
Determine the index length required to guarantee,0
the train–test ratio is exactly the desired one.,0
Pad indices so lengths agree,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(index,name.split(os.sep)[1])",0
"print(name,test_video_name)",0
Blazing fast and does not load the image into memory,0
Check that points lie within the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
updating variable if null/None! #backwardscompatability,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
these values are dropped as scalecrop,0
doesn't have rotation implemented,0
sort the models directories,0
get the shuffle index,0
read cfg file,0
create log file,0
Check for variable correctness,0
Moviepy:,0
select crop method,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
from skimage.io import imread,0
"frame=imread(os.path.join(directory,framename),mode='RGB')",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
TODO: addd cropping as in video analysis!,1
if cropping is not None:,0
cfg['cropping']=True,0
"cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping",0
"print(""Overwriting cropping parameters:"", cropping)",0
"print(""These are used for all videos, but won't be save to the cfg file."")",0
Check which snapshots are available and sort them by # iterations,0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
NOTE: If dataname line above is changed then line below is obsolete?,0
"trackname = trackname.replace(videofolder, destfolder)",0
TODO: adjust this for multi + unique bodyparts!,1
this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. ),0
Optimal identity assignment based on soft voting,0
"print(""If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract any outlier frames!"")",0
imagenet mean for resnet pretraining:,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/AlexEMG/DeepLabCut/pull/324",0
for DLC 2.2. (here all set False to not use PAFs/pairwise fields),0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
"reloading defaults, as they can bleed over from a previous run otherwise",0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
dlc_cfg_train = load_config(str(path_train_config)),0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
Name for deeplabcut net (based on its parameters),0
"DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])",0
"print(""Extracting maps for "", DLCscorer, "" with # of trainingiterations:"", trainingsiterations)",0
if notanalyzed: #this only applies to ask if h5 exists...,0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Get animal IDs and corresponding indices in the arrays of detections,0
Form ground truth beforehand,0
Assemble animals on the full set of detections,0
Count the number of unassembled bodyparts,0
Handle unlabeled bodyparts...,0
"Only 1 animal, let us return the full graph indices only",0
Find minimal skeleton,0
Select optimal PAF graph,0
Selective copy; deepcopy is >5x slower,0
Only keeps skeletons that are more than 90% complete,0
TODO Normalize dists by longest length?,1
TODO Smarter imputation technique (Bayesian? Grassmann averages?),1
Covariance matrix estimation fails due to numerical singularities,0
Correct distance to account for missing observations,0
"Alternatively, reduce contribution of missing values to the Mahalanobis",0
distance to zero by substituting the corresponding means.,0
"Fill the subsets with unambiguous, complete individuals",0
Fuse superfluous assemblies,0
Second pass without edge safety,0
Mark the unique body parts as assembled anyway so,0
they are not used later on to fill assemblies.,0
Store selected edges for subsequent frames,0
Remove invalid assemblies,0
Fill assemblies with unconnected body parts,0
Spawning (rather than forking) multiple processes does not,0
work nicely with the GUI or interactive sessions.,0
"In that case, we fall back to the serial assembly.",0
sigma is taken as the median of all COCO keypoint standard deviations,0
define constant velocity model,0
Regularize by forcing AR <= 5,0
max_ar = 5,0
if el.aspect_ratio >= max_ar:,0
if el.height > el.width:,0
el.width = el.height / max_ar,0
else:,0
el.height = el.width / max_ar,0
Orient the ellipse such that it encompasses most points,0
"n_inside = el.contains_points(np.c_[self.x, self.y]).sum()",0
el.theta += 0.5 * np.pi,0
"if el.contains_points(np.c_[self.x, self.y]).sum() < n_inside:",0
el.theta -= 0.5 * np.pi,0
"r2 = chi2.ppf(2 * norm.cdf(sd) - 1, 2)",0
"height, width = np.sqrt(E * r2)",0
The general quadratic curve has the form:,0
ax^2 + 2bxy + cy^2 + 2dx + 2fy + g = 0,0
Ellipse center coordinates,0
Semi-axes lengths,0
Angle to the horizontal,0
Reset tracker IDs,0
diff = val - cost_matrix,0
"diff[row, col] += val",0
if (,0
val < self.iou_threshold,0
or np.any(diff[row] <= 0.2),0
"or np.any(diff[:, col] <= 0.2)",0
):,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
TODO Try particle filter (since we already have the keypoints),1
Modified from scipy source code:,0
- to restrict its use to 2D,0
- to get rid of shuffling (since arrays are only (nbodyparts * 3) element long),0
TODO - factor in keypoint confidence (and weight by # of observations??),1
"mat = self.calc_pairwise_oks(poses, poses_ref)",0
Remove matched detections with low OKS,0
matches = [],0
"for row, col in zip(row_indices, col_indices):",0
"if mat[row, col] < self.oks_threshold:",0
unmatched_poses.append(row),0
unmatched_trackers.append(col),0
else:,0
"matches.append([row, col])",0
if not len(matches):,0
"matches = np.empty((0, 2), dtype=int)",0
else:,0
matches = np.stack(matches),0
filter out matched with low IOU,0
get predicted locations from existing trackers.,0
update matched trackers with assigned detections,0
create and initialise new trackers for unmatched detections,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
TODO: get cropping parameters and utilize!,1
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Load meta data & annotations,0
get the graph!,0
"NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*",0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
TODO: Make this code not so redundant!,1
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
"# Also storing one ""large"" table with results:",0
note: evaluationfolder.parents[0] to get common folder above all shuffle evaluations.,0
#################################################,0
Load data...,0
#################################################,0
Loading human annotatated data,0
Handle data previously annotated on a different platform,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
TODO: IMPLEMENT for different batch sizes?,1
Ignore best edges possibly defined during a prior evaluation,0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
FIXME Is having an empty array vs nan really that necessary?!,1
"Form 2D array of shape (n_rows, 4) where the last dimension",0
"is (sample_index, peak_y, peak_x, bpt_index) to slice the PAFs.",0
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in,0
Compute all distance statistics,0
Calculate overall prediction error,0
Skip data-driven skeleton selection unless,0
the model was trained on the full graph.,0
"loading backbone from ResNet, MobileNet etc.",0
"display_iters = max(1,int(displayiters))",0
"[_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})",0
Save snapshot,0
return to original path.,0
Clip peak locations to PAFs dimensions,0
"unit_vecs = vecs / lengths[:, np.newaxis]",0
"affinities = np.squeeze(y @ np.expand_dims(unit_vecs, axis=2)).sum(axis=1)",0
Form cost matrices,0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
Change the stride from 2 to 1 to get 16x downscaling instead of 32x.,0
https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142,0
Multi stage is currently only implemented for resnets,0
Defining multi_fusion backbone,0
Attaching multi-stage decoder,0
stage_paf_output = stage_paf_output + pre_stage_paf_output,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
FIXME Fix wrong scope with Keras layers,1
"def prediction_layer(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2DTranspose(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=2,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
## New DLCNet Addition: multi-stage decoder,0
"def prediction_layer_stage(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2D(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=1,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Compute variance using: Var[X]= E[X^2] - E[X]^2.,0
Compute keep_prob,0
TODO(tanmingxing): add support for training progress.,1
Compute drop_connect tensor,0
turn into x times y time bs * bpts,0
extract corresponding locref x and y as well as probability,0
Filter predicted heatmaps with a 2D Gaussian kernel as in:,0
https://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_The_Devil_Is_in_the_Details_Delving_Into_Unbiased_Data_CVPR_2020_paper.pdf,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
defaults will be a public argument for namedtuple in Python 3.7,0
https://docs.python.org/3/library/collections.html#collections.namedtuple,0
Make sure that round down does not go down by more than 10%.,0
Builds the block accordings to arguments.,0
Expansion phase:,0
Depth-wise convolution phase:,0
Squeeze and Excitation layer.,0
Output phase:,0
only apply drop_connect if skip presents.,0
Expansion phase:,0
Output phase:,0
only apply drop_connect if skip presents.,0
Builds blocks.,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Stem part.,0
Head part.,0
Calls Stem layers,0
Calls blocks.,0
Calls final layers and returns logits.,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"(width_coefficient, depth_coefficient, resolution, dropout_rate)",0
blocks_args = [,0
"'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',",0
"'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',",0
"'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',",0
"'r1_k3_s11_e6_i192_o320_se0.25',",0
],0
The default is TPU-specific batch norm.,0
The alternative is tf.layers.BatchNormalization.,0
"batch_norm=utils.TpuBatchNormalization,  # TPU-specific requirement.",0
ValueError will be raised here if override_params has fields not included,0
in global_params.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
Clamp to 40% of crop size to ensure that at least,0
the center keypoint remains visible after the offset is applied.,0
Points located close to one another are sampled preferentially,0
in order to augment crowded regions.,0
Include keypoints in the count to avoid null probabilities,0
Shift the crop center in both dimensions by random amounts,0
and normalize to the original image dimensions.,0
Pickle the 'data' dictionary using the highest protocol available.,0
item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal,0
"Add smart, keypoint-aware image cropping",0
Scale is sampled only once (per batch) to transform all of the images into same size.,0
Discard keypoints whose coordinates lie outside the cropped image,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
"imageio.imwrite(data_items[i].im_path.split('/')[-1],im)",0
Produce score maps and location refinement fields,0
Find indices of individuals in joint_id,0
WIP!,0
STD of gaussian is 1/4 of threshold,0
Grid of coordinates,0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Distance between the joint point and each coordinate,0
NEEDS TO BE DONE!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
TODO Finish implementing actual abstract class,1
parameter initialization for augmentation pipeline:,0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images of a batch into same size.,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"First, initializing variables (if they don't exist)",0
what is the fraction of training samples with scaling augmentation?,0
loading defaults for rotation range!,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"cfg[""rotateratio""] = cfg.get(",0
"""rotratio"", 0.4",0
)  # what is the fraction of training samples with rotation augmentation?,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"Auto cropping is new (was not in Nature Neuroscience 2018 paper, but introduced in Nath et al. Nat. Protocols 2019)",0
"and boosts performance by 2X, particularly on challenging datasets, like the cheetah in Nath et al.",0
Parameters for augmentation with regard to cropping:,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
Get rid of the interpolation beyond the spline knots,0
Retrieve original individual label indices,0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
"net_type = ""mobilenet_v2_0.35""",0
"net_type = ""efficientnet-b0""  # to -b6",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
TENSORPACK could fail in WINDOWS...,0
"NET = ""dlcrnet_ms5""",0
"NET = ""resnet_152""",0
"NET = ""efficientnet-b0""",0
"NET = ""mobilenet_v2_0.35"" # should be fixed",0
"""multi_step"": [[0.001, N_ITER]],",0
Copy over meaningful tracklets to test stitching,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
# Create one split and make Shuffle 2 and 3 have the same split.,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
for EfficientNet,0
imgaug,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
# Create one split and make Shuffle 2 and 3 have the same split.,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Dowloading the calibration images,0
Deleting unneccesary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
example how to set pose config variables:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Guarantee that images smaller than crop size are handled fine,0
Ensure at least a keypoint is visible in each crop,0
Test passing in a batch of frames,0
Verify the aspect ratio is preserved,0
Break the graph to test stitching failure,0
Add fake IDs,0
Split all tracklets in half,0
Assemble based on the smallest graph to speed up testing,0
Generate fake identity predictions,0
Assemble based on the smallest graph to speed up testing,0
Test now with identity only and ensure assemblies,0
contain only parts of a single group ID.,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
def install(package):,0
"subprocess.check_call([sys.executable, ""-m"", ""pip"", ""install"", package])",0
"install(""tensorflow==1.13.1"")",0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
quicker variant,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
Suppress tensorflow warning messages,0
DLClight version does not support GUIs. Importing accordingly,0
"Train, evaluate & predict functions / all require TF",0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Updating config file:,0
Create the pose_config.yaml files,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path, num_shuffles=auxfun_models.Check4weights(pose_cfg['net_type'], parent_path, num_shuffles= 1)",0
Updating training and test pose_cfg:,0
Read the config file,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
common parameters:,0
Write dictionary to yaml  config file,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
Flatten the list of bodyparts to connect,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Start plotting for every frame,0
"Once all the frames are saved, then make a movie using ffmpeg.",0
Create the fig and define the axes,0
Clear plot and initialize the variables,0
Initialize arrays for appending the 3d data for actual plotting the points,0
Initialize arrays for appending the 3d data for drawing the lines,0
Initialize arrays for appending the 2d data from cam1 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam2 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam1 for drawing the lines,0
Initialize arrays for appending the 2d data from cam2 for drawing the lines,0
Get the scorer names from the dataframe,0
"Set the x,y, and z limits for the 3d view",0
"Set the frame number to read#max(0,index-trailpoints):index",0
Plot the labels for each body part,0
Connecting the bodyparts specified in the config file.3d file is created based on the likelihoods of cam1 and cam2. Using 3d file and check if the body part is nan then dont plot skeleton,0
Saving the frames,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
Extract the indices of frames where the likelihood of a bodypart for both cameras are less than pvalue,0
Extract frames where likelihood for both the views is less than the pcutoff,0
"low_likelihood_frames = np.all(likelihoods < pcutoff, axis=1)",0
For cam1 camera: Assign nans to x and y values of a bodypart where the likelihood for is less than pvalue,0
For cam2 camera: Assign nans to x and y values of a bodypart where the likelihood is less than pvalue,0
ToDo: speed up func. below by saving in numpy.array,1
"Create an empty dataframe to store x,y,z of 3d data",0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Undistorting the points from cam2 camera,0
Save the undistorted files,0
Alternatively,0
Sort tracklets by length to prioritize greater continuity,0
Drop tracklets that are too short,0
"Where slots are available, copy the data over",0
"If about to overwrite data, keep tracklets with highest confidence",0
Squeeze some data into empty slots,0
"For the remaining data, overwrite where we are least confident",0
Find keypoints closest to the remaining data,0
Use Manhattan distance to avoid overflow,0
Only overwrite if improving confidence,0
Fill existing gaps,0
Retrieve original individual label indices,0
Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.,0
Fill existing gaps,0
Retrieve original individual label indices,0
ID swaps occur when X and Y simultaneously intersect each other.,0
Get only those bodypart pairs that belong to different individuals,0
"See Li et al., 2012. Cross-view Activity Recognition using Hankelets.",0
"As proposed in the paper, the Hankel matrix can either be formed from",0
the tracklet's centroid or its normalized velocity.,0
"vel = np.diff(self.centroid, axis=0)",0
"vel /= np.linalg.norm(vel, axis=1, keepdims=True)",0
return self.hankelize(vel),0
TODO Add missing data imputation,1
"nrows, ncols = mat.shape",0
beta = nrows / ncols,0
omega = 0.56 * beta ** 3 - 0.95 * beta ** 2 + 1.82 * beta + 1.43,0
return np.argmin(s > omega * np.median(s)),0
"Note that if tracklets are very short, some may actually be part of the same track",0
and thus incorrectly reflect separate track endpoints...,0
"Map each Tracklet to an entry and output nodes and vice versa,",0
which is convenient once the tracklets are stitched.,0
TODO Avoid looping over all pairs of tracklets,0
The algorithm works better with integer weights,0
Let us prune the graph by removing all source and sink edges,0
but those connecting the `n_tracks` first and last tracklets.,0
Preflow push seems to work slightly better than shortest,0
"augmentation path..., and is more computationally efficient.",0
Verify whether there are overlapping tracklets,0
"Pick the segment that minimizes ""smoothness"", computed here",0
with the coefficient of variation of the differences.,0
Rebuild a full graph from the remaining nodes without,0
temporal constraint on what tracklets can be stitched together.,0
Cycle through the residuals and incorporate back those,0
that only fit in a single tracklet.,0
Greedily add the remaining residuals,0
Refresh temporal bounds,0
Default to the distance cost function,0
Add in identity weighing before building the graph,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
average distance and average # significant differences avg. over comparisonbodyparts,0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
###################################################,0
Dependencies,0
###################################################,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as,0
specified in the config file,0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
Prepare figure,0
Mask rather than drop unwanted bodyparts to ensure consistent coloring,0
cropping!,0
nx=X2-X1,0
!/usr/bin/env python3,0
The above already raises a ValueError if formatting is wrong,0
"Rotate, see: https://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg",0
interesting option to just update metadata.,0
"Historically DLC used: from scipy.misc import imread, imresize >> deprecated functions",0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
Read the video until a frame is successfully read,0
import platform,0
"if platform.system() == ""Darwin"":  # for OSX use WXAgg",0
fig.canvas.start_event_loop(timeout=-1),0
else:,0
#fig.canvas.stop_event_loop(),0
"Windows throws error if file path is > 260 characters, can fix with prefix.",0
See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation,0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
"Attention this order has to be consistent (for training set creation, training, inference etc.)",0
"-> adding (single,bpt) for uniquebodyparts",0
"-> adding (indivdual,bpt) for multianimalbodyparts",0
load defaults,0
set project specific parameters:,0
###################################################,0
Dependencies,0
###################################################,0
Pose X vs pose Y,0
Poses vs time,0
Likelihoods,0
Histograms,0
#################################################,0
Looping analysis over video,0
#################################################,0
Keep only the individuals and bodyparts that were labeled,0
Determine whether the data are single- or multi-animal without loading into memory,0
simply by checking whether 'individuals' is in the second line of the CSV.,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"if ""MULTI!"" in allbpts:",0
Ensure same order as in config.yaml,0
ABBREVIATE NETWORK NAMES -- esp. for mobilenet!,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
Iterate over data files and stop as soon as one matching the scorer is found,0
If there was no match...,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
Read the infer config file,0
let the user open the file with default text editor. Also make it mac compatible,0
self.save.Enable(False),0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = io.imread(img),0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
topSplitter = wx.SplitterWindow(self),0
,0
"self.image_panel = ImagePanel(topSplitter, config,video,shuffle,Dataframe,self.gui_size)",0
self.widget_panel = WidgetPanel(topSplitter),0
,0
"topSplitter.SplitHorizontally(self.image_panel, self.widget_panel,sashPosition=self.gui_size[1]*0.83)#0.9",0
topSplitter.SetSashGravity(1),0
sizer = wx.BoxSizer(wx.VERTICAL),0
"sizer.Add(topSplitter, 1, wx.EXPAND)",0
self.SetSizer(sizer),0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
"cv2.imwrite(img_name, frame)",0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0,10), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=10)",0
Add all the options,0
line = wx.StaticLine(self),0
"self.sizer.Add(line, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=10)",0
,0
"self.multi_choice.Bind(wx.EVT_CHECKBOX,self.activate_copy_videos)",0
Hide the button as this is not the default option,0
self.vids.Enable(False),0
self.SetSizer(self.sizer),0
"self.sizer.Add(self.sizer, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=15)",0
self.ok.Enable(False),0
else:,0
self.ok.Enable(True),0
For mac compatibility,0
Remove the pages in case the user goes back to the create new project and creates/load a new project,0
Add all the other pages,0
self.yes.Enable(False),0
self.no.Enable(False),0
Here we create a panel and a notebook on the panel,0
create the page windows as children of the notebook and add the pages to the notebook with the label to show on the tab,0
Take into consideration imprecise OpenCV estimation of total number of frames,0
Automatically disable the draggable points,0
Save additional frames to the labeled-data directory,0
Store the newly-refined data,0
Uncertain keypoints are ignored,0
Merge with the already existing annotated data,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.buttonCounter = [],0
self.bodyparts = [],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
load labeledPoints and fundamental Matrix,0
Find offset terms for drawing epipolar Lines,0
Get crop params for camera being labeled,0
Get crop params for other camera,0
draw epipolar lines,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
variable initilization,0
design the panel,0
Add logo of DLC,0
self.filelist = self.filelist + self.vids,0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
load labeledPoints and fundamental Matrix,0
Find offset terms for drawing epipolar Lines,0
Get crop params for camera being labeled,0
Get crop params for other camera,0
im = cv2.imread(img),0
convert the image to RGB as you are showing the image with matplotlib,0
draw epipolar lines,0
divider = make_axes_locatable(self.axes),0
"colorIndex = np.linspace(np.min(im),np.max(im),len(bodyparts))",0
"cax = divider.append_axes(""right"", size=""5%"", pad=0.05)",0
"cbar = self.figure.colorbar(ax, cax=cax,spacing='proportional', ticks=colorIndex)",0
cbar.set_ticklabels(bodyparts[::-1]),0
self.figure.canvas.draw(),0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"MainFrame.saveDataSet(self, event)",0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
checks for unique bodyparts,0
"Reading the existing dataset,if already present",0
Handle data previously labeled on a different platform,0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Create an empty data frame,0
Cache original bodyparts,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Rearrange bodypart columns in config order,0
Test whether there are missing frames and superfluous data,0
Check whether new individuals or body parts were added,0
Checking if user added a new label,0
Found new labels in either multiple bodyparts or unique bodyparts,0
check if single is slected when radio buttons are changed,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Discard data associated with bodyparts that are no longer in the config,0
Re-organize the dataframe so the CSV looks consistent with the config,0
restore the background region,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.build = wx.Button(self, label=""Build skeleton"")",0
"sizer.Add(self.build, pos=(4, 3), flag=wx.BOTTOM | wx.RIGHT, border=10)",0
"self.build.Bind(wx.EVT_BUTTON, self.build_skeleton)",0
self.build.Enable(True),0
"def build_skeleton(self, event):",0
skeleton.SkeletonBuilder(self.config),0
Settting the GUI size and panels design,0
Checks if zoom/pan button is ON,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
use the default pose_cfg file for default values,0
self.display_iters.Enable(False),0
self.save_iters.Enable(False),0
self.max_iters.Enable(False),0
self.snapshots.Enable(False),0
Read the pose config file,0
"print(os.path.join(cfg['project_path'],auxiliaryfunctions.GetModelFolder(trainFraction, self.shuffles.GetValue(),cfg),'train','pose_cfg.yaml'))",0
let the user open the file with default text editor. Also make it mac compatible,0
update the variables with the edited values in the pose config file,0
variable initilization,0
design the panel,0
Add logo of DLC,0
Read the infer config file,0
let the user open the file with default text editor. Also make it mac compatible,0
"def video_tracklets(self,event):",0
shuffle = self.shuffle.GetValue(),0
trainingsetindex = self.trainingset.GetValue(),0
"deeplabcut.create_video_from_pickled_tracks(self.filelist, picklefile, pcutoff=0.6)",0
"self.select_destfolder.SetPath(""None"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.shuffles.SetValue(""1"")",0
"DownSampleVideo(vname,width=-1,height=200,outsuffix='downsampled',outpath=None,rotatecw=False):",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"majorDimension=0,",0
"self.videotype.SetStringSelection("".avi"")",0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
"self.choiceBox,self.visualization_rdb = self.choice_panel.addRadioButtons()",0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
Overwrite the config-defined individual names,0
with those actually present in the annotated data,0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
print(self.Dataframe.head()),0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
fix move to corner,0
fix move to corner,0
fix move to corner,0
fix move to corner,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"if self.plot_scoremaps.GetStringSelection() == ""Yes"":",0
Read the infer config file,0
let the user open the file with default text editor. Also make it mac compatible,0
shuffle = self.shuffle.GetValue(),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
self.cfg = auxiliaryfunctions.read_config(config),0
"self.statusbar.SetStatusText("""")",0
"dlg = wx.MessageDialog(None,""Are you sure?"", ""Quit!"",wx.YES_NO | wx.ICON_WARNING)",0
result = dlg.ShowModal(),0
if result == wx.ID_YES:,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0, 4), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=5)",0
variable initilization,0
design the panel,0
Add logo of DLC,0
#         design the panel,0
Add image of DLC,0
if editing this text make sure you add the '\n' to get the new line. The sizer is unable to format lines correctly.,0
Drop missing body parts,0
Drop points lying outside the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
utility function to split different crops from same image into either train or test!,0
Ignore possible connections between 'multi' and 'unique' body parts;,0
one can never be too careful...,0
Loading the encoder (if necessary downloading from TF),0
Map back to the original indices.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving metadata and data file (Pickle file),0
###############################################################################,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Setting inference cfg file:,0
Load updated lists:,0
Find the corresponding video file,0
Avoid cropping already cropped images,0
initialize a all zero image container with target crop size,0
"new upper left, border protection",0
be careful crop side can be smaller than one side of the image,0
"new bottom right, border protection",0
avoid to exceed the container's border,0
fill original image to the container image,0
use safe upper left and bottom right to crop original image and fill it to the container,0
all keypoints are shifted by +x0 and +y0,0
possibly out of border again,0
some keypoints are out of the borders,0
save the padded img,0
"moving old entry to _original, dropping it from video_set and update crop parameters",0
"print(key, ""dropping?"")",0
adding important values for multianiaml project:,0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Let's check if the code is *not* run on windows (Source: #https://stackoverflow.com/questions/1325581/how-do-i-check-if-im-running-on-windows-in-python),0
but the paths are in windows format...,0
Determine the index length required to guarantee,0
the train–test ratio is exactly the desired one.,0
Pad indices so lengths agree,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(index,name.split(os.sep)[1])",0
"print(name,test_video_name)",0
Blazing fast and does not load the image into memory,0
Check that points lie within the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
updating variable if null/None! #backwardscompatability,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
"Now that the training fraction is guaranteed to be correct,",0
the values added to pad the indices are removed.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
these values are dropped as scalecrop,0
doesn't have rotation implemented,0
sort the models directories,0
get the shuffle index,0
read cfg file,0
create log file,0
Check for variable correctness,0
Moviepy:,0
select crop method,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
Re-use data-driven PAF graph for video analysis. Note that this must,0
happen after setting up the TF session to avoid graph mismatch.,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
from skimage.io import imread,0
"frame=imread(os.path.join(directory,framename),mode='RGB')",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
TODO: addd cropping as in video analysis!,1
if cropping is not None:,0
cfg['cropping']=True,0
"cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping",0
"print(""Overwriting cropping parameters:"", cropping)",0
"print(""These are used for all videos, but won't be save to the cfg file."")",0
Check which snapshots are available and sort them by # iterations,0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
NOTE: If dataname line above is changed then line below is obsolete?,0
"trackname = trackname.replace(videofolder, destfolder)",0
TODO: adjust this for multi + unique bodyparts!,1
this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. ),0
Optimal identity assignment based on soft voting,0
"print(""If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract any outlier frames!"")",0
imagenet mean for resnet pretraining:,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/AlexEMG/DeepLabCut/pull/324",0
for DLC 2.2. (here all set False to not use PAFs/pairwise fields),0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
"reloading defaults, as they can bleed over from a previous run otherwise",0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
dlc_cfg_train = load_config(str(path_train_config)),0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
Name for deeplabcut net (based on its parameters),0
"DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])",0
"print(""Extracting maps for "", DLCscorer, "" with # of trainingiterations:"", trainingsiterations)",0
if notanalyzed: #this only applies to ask if h5 exists...,0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
initializing constants,0
"PredicteData['frame'+str(counter)]=predict.get_detectionswithcosts(frame, dlc_cfg, sess, inputs, outputs, outall=False,nms_radius=dlc_cfg.nmsradius,det_min_score=dlc_cfg.minconfidence)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
"PredicteData[batch_num*batchsize:batch_num*batchsize+batch_ind, :] = pose[:batch_ind,:]",0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Sort crop patches to maximize the probability they overlap with others,0
Form the ground truth back,0
Match detections across crops,0
Store the costs associated with the retained candidates,0
Form ground truth beforehand,0
Assemble animals on the full set of detections,0
Count the number of unassembled bodyparts,0
Handle unlabeled bodyparts...,0
"Only 1 animal, let us return the full graph indices only",0
Find minimal skeleton,0
Select optimal PAF graph,0
Selective copy; deepcopy is >5x slower,0
Only keeps skeletons that are more than 90% complete,0
TODO Normalize dists by longest length?,1
TODO Smarter imputation technique (Bayesian? Grassmann averages?),1
Covariance matrix estimation fails due to numerical singularities,0
Correct distance to account for missing observations,0
"Alternatively, reduce contribution of missing values to the Mahalanobis",0
distance to zero by substituting the corresponding means.,0
"Fill the subsets with unambiguous, complete individuals",0
Fuse superfluous assemblies,0
Second pass without edge safety,0
Store selected edges for subsequent frames,0
Remove invalid assemblies,0
Fill assemblies with unconnected body parts,0
Spawning (rather than forking) multiple processes does not,0
work nicely with the GUI or interactive sessions.,0
"In that case, we fall back to the serial assembly.",0
sigma is taken as the median of all COCO keypoint standard deviations,0
define constant velocity model,0
Regularize by forcing AR <= 5,0
max_ar = 5,0
if el.aspect_ratio >= max_ar:,0
if el.height > el.width:,0
el.width = el.height / max_ar,0
else:,0
el.height = el.width / max_ar,0
Orient the ellipse such that it encompasses most points,0
"n_inside = el.contains_points(np.c_[self.x, self.y]).sum()",0
el.theta += 0.5 * np.pi,0
"if el.contains_points(np.c_[self.x, self.y]).sum() < n_inside:",0
el.theta -= 0.5 * np.pi,0
"r2 = chi2.ppf(2 * norm.cdf(sd) - 1, 2)",0
"height, width = np.sqrt(E * r2)",0
The general quadratic curve has the form:,0
ax^2 + 2bxy + cy^2 + 2dx + 2fy + g = 0,0
Ellipse center coordinates,0
Semi-axes lengths,0
Angle to the horizontal,0
Reset tracker IDs,0
diff = val - cost_matrix,0
"diff[row, col] += val",0
if (,0
val < self.iou_threshold,0
or np.any(diff[row] <= 0.2),0
"or np.any(diff[:, col] <= 0.2)",0
):,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
TODO Try particle filter (since we already have the keypoints),1
Modified from scipy source code:,0
- to restrict its use to 2D,0
- to get rid of shuffling (since arrays are only (nbodyparts * 3) element long),0
TODO - factor in keypoint confidence (and weight by # of observations??),1
"mat = self.calc_pairwise_oks(poses, poses_ref)",0
Remove matched detections with low OKS,0
matches = [],0
"for row, col in zip(row_indices, col_indices):",0
"if mat[row, col] < self.oks_threshold:",0
unmatched_poses.append(row),0
unmatched_trackers.append(col),0
else:,0
"matches.append([row, col])",0
if not len(matches):,0
"matches = np.empty((0, 2), dtype=int)",0
else:,0
matches = np.stack(matches),0
filter out matched with low IOU,0
get predicted locations from existing trackers.,0
update matched trackers with assigned detections,0
create and initialise new trackers for unmatched detections,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
TODO: get cropping parameters and utilize!,1
openmp_arg = '-fopenmp',0
"if _platform == ""win32"":",0
openmp_arg = '-openmp',0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Load meta data & annotations,0
get the graph!,0
"NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*",0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
TODO: Make this code not so redundant!,1
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
"# Also storing one ""large"" table with results:",0
note: evaluationfolder.parents[0] to get common folder above all shuffle evaluations.,0
#################################################,0
Load data...,0
#################################################,0
Loading human annotatated data,0
Handle data previously annotated on a different platform,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
TODO: IMPLEMENT for different batch sizes?,1
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
print(dlc_cfg),0
Specifying state of model (snapshot / training state),0
Evaluate PAF edge lengths to calibrate `distnorm`,0
FIXME Is having an empty array vs nan really that necessary?!,1
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in,0
Compute all distance statistics,0
Calculate overall prediction error,0
"For OKS/PCK, compute the standard deviation error across all frames",0
Skip data-driven skeleton selection unless,0
the model was trained on the full graph.,0
returning to intial folder,0
"loading backbone from ResNet, MobileNet etc.",0
"display_iters = max(1,int(displayiters))",0
"[_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})",0
Save snapshot,0
return to original path.,0
get coordinates for bp1 and bp2,0
'm2',0
"distscalarproduct=np.zeros((len(C1),len(C2)))*np.nan",0
"distscalarproduct[c1i,c2i]=dy*v[1]+dx*v[0] #scalar product [v unit vector dx,dy in pixel coordinats from partaffinitymap]",0
Distances[l]['m2'] = distscalarproduct,0
TODO: compute all the grids etc. once for the video analysis method,1
(and then just pass on the variables),0
apply nms,0
"IMPORTANT, as C++ function expects row-major",0
These two functions are for evaluation specifically (one also calculates integral between gt points),0
get dist_grid,0
apply nms,0
"IMPORTANT, as C++ function expects row-major",0
if 'eval_scale' in cfg.keys():,0
import imgaug.augmenters as iaa,0
im = iaa.Resize(float(cfg['eval_scale']))(images=im),0
# Functions below implement are for batch sizes > 1:,0
get dist_grid,0
apply nms,0
"IMPORTANT, as C++ function expects row-major",0
"batchsize,ny,nx,num_joints = scmap.shape",0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
Change the stride from 2 to 1 to get 16x downscaling instead of 32x.,0
https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142,0
Multi stage is currently only implemented for resnets,0
Defining multi_fusion backbone,0
Attaching multi-stage decoder,0
stage_paf_output = stage_paf_output + pre_stage_paf_output,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
FIXME Fix wrong scope with Keras layers,1
"def prediction_layer(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2DTranspose(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=2,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
## New DLCNet Addition: multi-stage decoder,0
"def prediction_layer_stage(cfg, input, name, num_outputs):",0
with tf.compat.v1.variable_scope(name):,0
layer = tf.keras.layers.Conv2D(,0
"filters=num_outputs,",0
"kernel_size=(3, 3),",0
"strides=1,",0
"padding=""same"",",0
"kernel_regularizer=tf.keras.regularizers.l2(0.5 * (cfg['weight_decay'])),",0
"name=name,",0
"dtype=input.dtype.base_dtype,",0
),0
return layer(input),0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Compute variance using: Var[X]= E[X^2] - E[X]^2.,0
Compute keep_prob,0
TODO(tanmingxing): add support for training progress.,1
Compute drop_connect tensor,0
turn into x times y time bs * bpts,0
extract corresponding locref x and y as well as probability,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
defaults will be a public argument for namedtuple in Python 3.7,0
https://docs.python.org/3/library/collections.html#collections.namedtuple,0
Make sure that round down does not go down by more than 10%.,0
Builds the block accordings to arguments.,0
Expansion phase:,0
Depth-wise convolution phase:,0
Squeeze and Excitation layer.,0
Output phase:,0
only apply drop_connect if skip presents.,0
Expansion phase:,0
Output phase:,0
only apply drop_connect if skip presents.,0
Builds blocks.,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Stem part.,0
Head part.,0
Calls Stem layers,0
Calls blocks.,0
Calls final layers and returns logits.,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"(width_coefficient, depth_coefficient, resolution, dropout_rate)",0
blocks_args = [,0
"'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',",0
"'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',",0
"'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',",0
"'r1_k3_s11_e6_i192_o320_se0.25',",0
],0
The default is TPU-specific batch norm.,0
The alternative is tf.layers.BatchNormalization.,0
"batch_norm=utils.TpuBatchNormalization,  # TPU-specific requirement.",0
ValueError will be raised here if override_params has fields not included,0
in global_params.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
Pickle the 'data' dictionary using the highest protocol available.,0
item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal,0
Scale is sampled only once (per batch) to transform all of the images into same size.,0
assert len(batch_images) == self.batch_size,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
"imageio.imwrite(data_items[i].im_path.split('/')[-1],im)",0
if returndata:,0
"return batch_images,batch_joints,targetmaps",0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Find indices of individuals in joint_id,0
if self.cfg.partaffinityfield_predict:,0
"print(""hello"",joint_id)",0
print(np.concatenate(joint_id)) #this is all joint_ids for all individuals!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
WIP!,0
STD of gaussian is 1/4 of threshold,0
Grid of coordinates,0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Distance between the joint point and each coordinate,0
NEEDS TO BE DONE!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
TODO Finish implementing actual abstract class,1
parameter initialization for augmentation pipeline:,0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images of a batch into same size.,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"First, initializing variables (if they don't exist)",0
what is the fraction of training samples with scaling augmentation?,0
loading defaults for rotation range!,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"cfg[""rotateratio""] = cfg.get(",0
"""rotratio"", 0.4",0
)  # what is the fraction of training samples with rotation augmentation?,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"Auto cropping is new (was not in Nature Neuroscience 2018 paper, but introduced in Nath et al. Nat. Protocols 2019)",0
"and boosts performance by 2X, particularly on challenging datasets, like the cheetah in Nath et al.",0
Parameters for augmentation with regard to cropping:,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
Get rid of the interpolation beyond the spline knots,0
Retrieve original individual label indices,0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
"net_type = ""mobilenet_v2_0.35""",0
"net_type = ""efficientnet-b0""  # to -b6",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
TENSORPACK could fail in WINDOWS...,0
"NET = ""resnet_152""",0
"NET = ""efficientnet-b0""",0
"NET = ""mobilenet_v2_0.35"" # should be fixed",0
"""multi_step"": [[0.001, N_ITER]],",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
# Create one split and make Shuffle 2 and 3 have the same split.,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
for EfficientNet,0
imgaug,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
# Create one split and make Shuffle 2 and 3 have the same split.,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Dowloading the calibration images,0
Deleting unneccesary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
example how to set pose config variables:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
Verify the aspect ratio is preserved,0
Break the graph to test stitching failure,0
Add fake IDs,0
Split all tracklets in half,0
Assemble based on the smallest graph to speed up testing,0
Generate fake identity predictions,0
Assemble based on the smallest graph to speed up testing,0
Test now with identity only and ensure assemblies,0
contain only parts of a single group ID.,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
quicker variant,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
Suppress tensorflow warning messages,0
DLClight version does not support GUIs. Importing accordingly,0
"Train, evaluate & predict functions / all require TF",0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Updating config file:,0
Create the pose_config.yaml files,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path, num_shuffles=auxfun_models.Check4weights(pose_cfg['net_type'], parent_path, num_shuffles= 1)",0
Updating training and test pose_cfg:,0
Read the config file,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
common parameters:,0
Write dictionary to yaml  config file,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
Flatten the list of bodyparts to connect,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Start plotting for every frame,0
"Once all the frames are saved, then make a movie using ffmpeg.",0
Create the fig and define the axes,0
Clear plot and initialize the variables,0
Initialize arrays for appending the 3d data for actual plotting the points,0
Initialize arrays for appending the 3d data for drawing the lines,0
Initialize arrays for appending the 2d data from cam1 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam2 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam1 for drawing the lines,0
Initialize arrays for appending the 2d data from cam2 for drawing the lines,0
Get the scorer names from the dataframe,0
"Set the x,y, and z limits for the 3d view",0
"Set the frame number to read#max(0,index-trailpoints):index",0
Plot the labels for each body part,0
Connecting the bodyparts specified in the config file.3d file is created based on the likelihoods of cam1 and cam2. Using 3d file and check if the body part is nan then dont plot skeleton,0
Saving the frames,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
Extract the indices of frames where the likelihood of a bodypart for both cameras are less than pvalue,0
Extract frames where likelihood for both the views is less than the pcutoff,0
"low_likelihood_frames = np.all(likelihoods < pcutoff, axis=1)",0
For cam1 camera: Assign nans to x and y values of a bodypart where the likelihood for is less than pvalue,0
For cam2 camera: Assign nans to x and y values of a bodypart where the likelihood is less than pvalue,0
ToDo: speed up func. below by saving in numpy.array,1
"Create an empty dataframe to store x,y,z of 3d data",0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Undistorting the points from cam2 camera,0
Save the undistorted files,0
Alternatively,0
Sort tracklets by length to prioritize greater continuity,0
Drop tracklets that are too short,0
"Where slots are available, copy the data over",0
"If about to overwrite data, keep tracklets with highest confidence",0
Squeeze some data into empty slots,0
"For the remaining data, overwrite where we are least confident",0
Find keypoints closest to the remaining data,0
Use Manhattan distance to avoid overflow,0
Only overwrite if improving confidence,0
Fill existing gaps,0
Retrieve original individual label indices,0
Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.,0
Fill existing gaps,0
Retrieve original individual label indices,0
ID swaps occur when X and Y simultaneously intersect each other.,0
Get only those bodypart pairs that belong to different individuals,0
"See Li et al., 2012. Cross-view Activity Recognition using Hankelets.",0
"As proposed in the paper, the Hankel matrix can either be formed from",0
the tracklet's centroid or its normalized velocity.,0
"vel = np.diff(self.centroid, axis=0)",0
"vel /= np.linalg.norm(vel, axis=1, keepdims=True)",0
return self.hankelize(vel),0
TODO Add missing data imputation,1
"nrows, ncols = mat.shape",0
beta = nrows / ncols,0
omega = 0.56 * beta ** 3 - 0.95 * beta ** 2 + 1.82 * beta + 1.43,0
return np.argmin(s > omega * np.median(s)),0
"Note that if tracklets are very short, some may actually be part of the same track",0
and thus incorrectly reflect separate track endpoints...,0
"Map each Tracklet to an entry and output nodes and vice versa,",0
which is convenient once the tracklets are stitched.,0
TODO Avoid looping over all pairs of tracklets,0
The algorithm works better with integer weights,0
Let us prune the graph by removing all source and sink edges,0
but those connecting the `n_tracks` first and last tracklets.,0
Preflow push seems to work slightly better than shortest,0
"augmentation path..., and is more computationally efficient.",0
Verify whether there are overlapping tracklets,0
"Pick the segment that minimizes ""smoothness"", computed here",0
with the coefficient of variation of the differences.,0
Rebuild a full graph from the remaining nodes without,0
temporal constraint on what tracklets can be stitched together.,0
Cycle through the residuals and incorporate back those,0
that only fit in a single tracklet.,0
Greedily add the remaining residuals,0
Refresh temporal bounds,0
Default to the distance cost function,0
Add in identity weighing before building the graph,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
average distance and average # significant differences avg. over comparisonbodyparts,0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
###################################################,0
Dependencies,0
###################################################,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as,0
specified in the config file,0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
Prepare figure,0
Mask rather than drop unwanted bodyparts to ensure consistent coloring,0
cropping!,0
nx=X2-X1,0
!/usr/bin/env python3,0
The above already raises a ValueError if formatting is wrong,0
"Rotate, see: https://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg",0
interesting option to just update metadata.,0
"Historically DLC used: from scipy.misc import imread, imresize >> deprecated functions",0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
Read the video until a frame is successfully read,0
import platform,0
"if platform.system() == ""Darwin"":  # for OSX use WXAgg",0
fig.canvas.start_event_loop(timeout=-1),0
else:,0
#fig.canvas.stop_event_loop(),0
"Windows throws error if file path is > 260 characters, can fix with prefix.",0
See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation,0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
"Attention this order has to be consistent (for training set creation, training, inference etc.)",0
"-> adding (single,bpt) for uniquebodyparts",0
"-> adding (indivdual,bpt) for multianimalbodyparts",0
load defaults,0
set project specific parameters:,0
###################################################,0
Dependencies,0
###################################################,0
Pose X vs pose Y,0
Poses vs time,0
Likelihoods,0
Histograms,0
#################################################,0
Looping analysis over video,0
#################################################,0
Keep only the individuals and bodyparts that were labeled,0
Determine whether the data are single- or multi-animal without loading into memory,0
simply by checking whether 'individuals' is in the second line of the CSV.,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"if ""MULTI!"" in allbpts:",0
Ensure same order as in config.yaml,0
ABBREVIATE NETWORK NAMES -- esp. for mobilenet!,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
Iterate over data files and stop as soon as one matching the scorer is found,0
If there was no match...,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
Read the infer config file,0
let the user open the file with default text editor. Also make it mac compatible,0
self.save.Enable(False),0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = io.imread(img),0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
topSplitter = wx.SplitterWindow(self),0
,0
"self.image_panel = ImagePanel(topSplitter, config,video,shuffle,Dataframe,self.gui_size)",0
self.widget_panel = WidgetPanel(topSplitter),0
,0
"topSplitter.SplitHorizontally(self.image_panel, self.widget_panel,sashPosition=self.gui_size[1]*0.83)#0.9",0
topSplitter.SetSashGravity(1),0
sizer = wx.BoxSizer(wx.VERTICAL),0
"sizer.Add(topSplitter, 1, wx.EXPAND)",0
self.SetSizer(sizer),0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
"cv2.imwrite(img_name, frame)",0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0,10), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=10)",0
Add all the options,0
line = wx.StaticLine(self),0
"self.sizer.Add(line, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=10)",0
,0
"self.multi_choice.Bind(wx.EVT_CHECKBOX,self.activate_copy_videos)",0
Hide the button as this is not the default option,0
self.vids.Enable(False),0
self.SetSizer(self.sizer),0
"self.sizer.Add(self.sizer, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=15)",0
self.ok.Enable(False),0
else:,0
self.ok.Enable(True),0
For mac compatibility,0
Remove the pages in case the user goes back to the create new project and creates/load a new project,0
Add all the other pages,0
self.yes.Enable(False),0
self.no.Enable(False),0
Here we create a panel and a notebook on the panel,0
create the page windows as children of the notebook and add the pages to the notebook with the label to show on the tab,0
Take into consideration imprecise OpenCV estimation of total number of frames,0
Automatically disable the draggable points,0
Save additional frames to the labeled-data directory,0
Store the newly-refined data,0
Uncertain keypoints are ignored,0
Merge with the already existing annotated data,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.buttonCounter = [],0
self.bodyparts = [],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
load labeledPoints and fundamental Matrix,0
Find offset terms for drawing epipolar Lines,0
Get crop params for camera being labeled,0
Get crop params for other camera,0
draw epipolar lines,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
variable initilization,0
design the panel,0
Add logo of DLC,0
self.filelist = self.filelist + self.vids,0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
load labeledPoints and fundamental Matrix,0
Find offset terms for drawing epipolar Lines,0
Get crop params for camera being labeled,0
Get crop params for other camera,0
im = cv2.imread(img),0
convert the image to RGB as you are showing the image with matplotlib,0
draw epipolar lines,0
divider = make_axes_locatable(self.axes),0
"colorIndex = np.linspace(np.min(im),np.max(im),len(bodyparts))",0
"cax = divider.append_axes(""right"", size=""5%"", pad=0.05)",0
"cbar = self.figure.colorbar(ax, cax=cax,spacing='proportional', ticks=colorIndex)",0
cbar.set_ticklabels(bodyparts[::-1]),0
self.figure.canvas.draw(),0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"MainFrame.saveDataSet(self, event)",0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
checks for unique bodyparts,0
"Reading the existing dataset,if already present",0
Handle data previously labeled on a different platform,0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Create an empty data frame,0
Cache original bodyparts,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Rearrange bodypart columns in config order,0
Test whether there are missing frames and superfluous data,0
Check whether new individuals or body parts were added,0
Checking if user added a new label,0
Found new labels in either multiple bodyparts or unique bodyparts,0
check if single is slected when radio buttons are changed,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Discard data associated with bodyparts that are no longer in the config,0
Re-organize the dataframe so the CSV looks consistent with the config,0
restore the background region,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.build = wx.Button(self, label=""Build skeleton"")",0
"sizer.Add(self.build, pos=(4, 3), flag=wx.BOTTOM | wx.RIGHT, border=10)",0
"self.build.Bind(wx.EVT_BUTTON, self.build_skeleton)",0
self.build.Enable(True),0
"def build_skeleton(self, event):",0
skeleton.SkeletonBuilder(self.config),0
Settting the GUI size and panels design,0
Checks if zoom/pan button is ON,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
use the default pose_cfg file for default values,0
self.display_iters.Enable(False),0
self.save_iters.Enable(False),0
self.max_iters.Enable(False),0
self.snapshots.Enable(False),0
Read the pose config file,0
"print(os.path.join(cfg['project_path'],auxiliaryfunctions.GetModelFolder(trainFraction, self.shuffles.GetValue(),cfg),'train','pose_cfg.yaml'))",0
let the user open the file with default text editor. Also make it mac compatible,0
update the variables with the edited values in the pose config file,0
variable initilization,0
design the panel,0
Add logo of DLC,0
Read the infer config file,0
let the user open the file with default text editor. Also make it mac compatible,0
"def video_tracklets(self,event):",0
shuffle = self.shuffle.GetValue(),0
trainingsetindex = self.trainingset.GetValue(),0
"deeplabcut.create_video_from_pickled_tracks(self.filelist, picklefile, pcutoff=0.6)",0
"self.select_destfolder.SetPath(""None"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.shuffles.SetValue(""1"")",0
"DownSampleVideo(vname,width=-1,height=200,outsuffix='downsampled',outpath=None,rotateccw=False):",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"majorDimension=0,",0
"self.videotype.SetStringSelection("".avi"")",0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
"self.choiceBox,self.visualization_rdb = self.choice_panel.addRadioButtons()",0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
Overwrite the config-defined individual names,0
with those actually present in the annotated data,0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
print(self.Dataframe.head()),0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
fix move to corner,0
fix move to corner,0
fix move to corner,0
fix move to corner,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"if self.plot_scoremaps.GetStringSelection() == ""Yes"":",0
Read the infer config file,0
let the user open the file with default text editor. Also make it mac compatible,0
shuffle = self.shuffle.GetValue(),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
self.cfg = auxiliaryfunctions.read_config(config),0
"self.statusbar.SetStatusText("""")",0
"dlg = wx.MessageDialog(None,""Are you sure?"", ""Quit!"",wx.YES_NO | wx.ICON_WARNING)",0
result = dlg.ShowModal(),0
if result == wx.ID_YES:,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0, 4), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=5)",0
variable initilization,0
design the panel,0
Add logo of DLC,0
#         design the panel,0
Add image of DLC,0
if editing this text make sure you add the '\n' to get the new line. The sizer is unable to format lines correctly.,0
Drop missing body parts,0
Drop points lying outside the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
utility function to split different crops from same image into either train or test!,0
Ignore possible connections between 'multi' and 'unique' body parts;,0
one can never be too careful...,0
Loading the encoder (if necessary downloading from TF),0
Map back to the original indices.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving metadata and data file (Pickle file),0
###############################################################################,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Setting inference cfg file:,0
Load updated lists:,0
Find the corresponding video file,0
Avoid cropping already cropped images,0
"moving old entry to _original, dropping it from video_set and update crop parameters",0
"print(key, ""dropping?"")",0
adding important values for multianiaml project:,0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Let's check if the code is *not* run on windows (Source: #https://stackoverflow.com/questions/1325581/how-do-i-check-if-im-running-on-windows-in-python),0
but the paths are in windows format...,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(index,name.split(os.sep)[1])",0
"print(name,test_video_name)",0
Blazing fast and does not load the image into memory,0
Check that points lie within the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
updating variable if null/None! #backwardscompatability,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
these values are dropped as scalecrop,0
doesn't have rotation implemented,0
sort the models directories,0
get the shuffle index,0
read cfg file,0
create log file,0
Check for variable correctness,0
Moviepy:,0
select crop method,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
Re-use data-driven PAF graph for video analysis. Note that this must,0
happen after setting up the TF session to avoid graph mismatch.,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
from skimage.io import imread,0
"frame=imread(os.path.join(directory,framename),mode='RGB')",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
TODO: addd cropping as in video analysis!,1
if cropping is not None:,0
cfg['cropping']=True,0
"cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping",0
"print(""Overwriting cropping parameters:"", cropping)",0
"print(""These are used for all videos, but won't be save to the cfg file."")",0
Check which snapshots are available and sort them by # iterations,0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
"NOTE: this can be used if only a subset is relevant. I.e. [0,1] for only first and second joint!",0
TODO: adjust this for multi + unique bodyparts!,1
this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. ),0
Optimal identity assignment based on soft voting,0
"print(""If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract any outlier frames!"")",0
imagenet mean for resnet pretraining:,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/AlexEMG/DeepLabCut/pull/324",0
for DLC 2.2. (here all set False to not use PAFs/pairwise fields),0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
"reloading defaults, as they can bleed over from a previous run otherwise",0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
dlc_cfg_train = load_config(str(path_train_config)),0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
Name for deeplabcut net (based on its parameters),0
"DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])",0
"print(""Extracting maps for "", DLCscorer, "" with # of trainingiterations:"", trainingsiterations)",0
if notanalyzed: #this only applies to ask if h5 exists...,0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
Dependencies for anaysis,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Load meta data & annotations,0
get the graph!,0
"NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*",0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
TODO: Make this code not so redundant!,1
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
"# Also storing one ""large"" table with results:",0
note: evaluationfolder.parents[0] to get common folder above all shuffle evaluations.,0
#################################################,0
Load data...,0
#################################################,0
Loading human annotatated data,0
Handle data previously annotated on a different platform,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
TODO: IMPLEMENT for different batch sizes?,1
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
print(dlc_cfg),0
Specifying state of model (snapshot / training state),0
Evaluate PAF edge lengths to calibrate `distnorm`,0
FIXME Is having an empty array vs nan really that necessary?!,1
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in,0
Compute all distance statistics,0
Calculate overall prediction error,0
"For OKS/PCK, compute the standard deviation error across all frames",0
Skip data-driven skeleton selection unless,0
the model was trained on the full graph.,0
returning to intial folder,0
"loading backbone from ResNet, MobileNet etc.",0
"display_iters = max(1,int(displayiters))",0
"[_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})",0
Save snapshot,0
return to original path.,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
from deeplabcut.pose_estimation_tensorflow.predict_multianimal import convert_detections2tracklet,0
initializing constants,0
"PredicteData['frame'+str(counter)]=predict.get_detectionswithcosts(frame, dlc_cfg, sess, inputs, outputs, outall=False,nms_radius=dlc_cfg.nmsradius,det_min_score=dlc_cfg.minconfidence)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
"PredicteData[batch_num*batchsize:batch_num*batchsize+batch_ind, :] = pose[:batch_ind,:]",0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Sort crop patches to maximize the probability they overlap with others,0
Form the ground truth back,0
Match detections across crops,0
Store the costs associated with the retained candidates,0
Form ground truth beforehand,0
Assemble animals on the full set of detections,0
Count the number of unassembled bodyparts,0
Handle unlabeled bodyparts...,0
Find minimal skeleton,0
Select optimal PAF graph,0
Selective copy; deepcopy is >5x slower,0
Only keeps skeletons that are more than 90% complete,0
TODO Normalize dists by longest length?,1
TODO Smarter imputation technique (Bayesian? Grassmann averages?),1
Correct distance to account for missing observations,0
"Alternatively, reduce contribution of missing values to the Mahalanobis",0
distance to zero by substituting the corresponding means.,0
"Fill the subsets with unambiguous, complete individuals",0
Fuse superfluous assemblies,0
Second pass without edge safety,0
Store selected edges for subsequent frames,0
Remove invalid assemblies,0
Fill assemblies with unconnected body parts,0
sigma is taken as the median of all COCO keypoint standard deviations,0
define constant velocity model,0
Regularize by forcing AR <= 5,0
max_ar = 5,0
if el.aspect_ratio >= max_ar:,0
if el.height > el.width:,0
el.width = el.height / max_ar,0
else:,0
el.height = el.width / max_ar,0
Orient the ellipse such that it encompasses most points,0
"n_inside = el.contains_points(np.c_[self.x, self.y]).sum()",0
el.theta += 0.5 * np.pi,0
"if el.contains_points(np.c_[self.x, self.y]).sum() < n_inside:",0
el.theta -= 0.5 * np.pi,0
"r2 = chi2.ppf(2 * norm.cdf(sd) - 1, 2)",0
"height, width = np.sqrt(E * r2)",0
The general quadratic curve has the form:,0
ax^2 + 2bxy + cy^2 + 2dx + 2fy + g = 0,0
Ellipse center coordinates,0
Semi-axes lengths,0
Angle to the horizontal,0
Reset tracker IDs,0
diff = val - cost_matrix,0
"diff[row, col] += val",0
if (,0
val < self.iou_threshold,0
or np.any(diff[row] <= 0.2),0
"or np.any(diff[:, col] <= 0.2)",0
):,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
TODO Try particle filter (since we already have the keypoints),1
Modified from scipy source code:,0
- to restrict its use to 2D,0
- to get rid of shuffling (since arrays are only (nbodyparts * 3) element long),0
TODO - factor in keypoint confidence (and weight by # of observations??),1
"mat = self.calc_pairwise_oks(poses, poses_ref)",0
Remove matched detections with low OKS,0
matches = [],0
"for row, col in zip(row_indices, col_indices):",0
"if mat[row, col] < self.oks_threshold:",0
unmatched_poses.append(row),0
unmatched_trackers.append(col),0
else:,0
"matches.append([row, col])",0
if not len(matches):,0
"matches = np.empty((0, 2), dtype=int)",0
else:,0
matches = np.stack(matches),0
filter out matched with low IOU,0
get predicted locations from existing trackers.,0
update matched trackers with assigned detections,0
create and initialise new trackers for unmatched detections,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
TODO: get cropping parameters and utilize!,1
openmp_arg = '-fopenmp',0
"if _platform == ""win32"":",0
openmp_arg = '-openmp',0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
assuming batchsize 1 here!,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
setting pairwise bodypart loss,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
defaults will be a public argument for namedtuple in Python 3.7,0
https://docs.python.org/3/library/collections.html#collections.namedtuple,0
Make sure that round down does not go down by more than 10%.,0
Builds the block accordings to arguments.,0
Expansion phase:,0
Depth-wise convolution phase:,0
Squeeze and Excitation layer.,0
Output phase:,0
only apply drop_connect if skip presents.,0
Expansion phase:,0
Output phase:,0
only apply drop_connect if skip presents.,0
Builds blocks.,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Stem part.,0
Head part.,0
Calls Stem layers,0
Calls blocks.,0
Calls final layers and returns logits.,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"(width_coefficient, depth_coefficient, resolution, dropout_rate)",0
blocks_args = [,0
"'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',",0
"'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',",0
"'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',",0
"'r1_k3_s11_e6_i192_o320_se0.25',",0
],0
The default is TPU-specific batch norm.,0
The alternative is tf.layers.BatchNormalization.,0
"batch_norm=utils.TpuBatchNormalization,  # TPU-specific requirement.",0
ValueError will be raised here if override_params has fields not included,0
in global_params.,0
print(end_points.keys()) >> to see what else is available.,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
setting pairwise bodypart loss,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
class TpuBatchNormalization(tf.layers.BatchNormalization):,0
Compute variance using: Var[X]= E[X^2] - E[X]^2.,0
Compute keep_prob,0
TODO(tanmingxing): add support for training progress.,1
Compute drop_connect tensor,0
Clear the old folder.,0
Write checkpoints.,0
Update the best objective.,0
We maintain mva for batch norm moving mean and variance as well.,0
Return the top 5 predictions (idx and prob) for each image.,0
Change the stride from 2 to 1 to get 16x downscaling instead of 32x.,0
https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142,0
## New DLCNet Addition: multi-stage decoder,0
standard stride 2 decoder,0
Multi stage is currently only implemented for resnets,0
The next part of the code depends upon the tensorflow version,0
Defining multi_fusion backbone,0
Attaching multi-stage decoder,0
stage_paf_output = stage_paf_output + pre_stage_paf_output,0
"return self.prediction_layers(net, end_points)",0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
from nets.mobilenet import conv_blocks as ops,0
from nets.mobilenet import mobilenet as lib,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
The next part of the code depends upon which tensorflow version you have.,0
assuming batchsize 1 here!,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
setting pairwise bodypart loss,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
get coordinates for bp1 and bp2,0
'm2',0
"distscalarproduct=np.zeros((len(C1),len(C2)))*np.nan",0
"distscalarproduct[c1i,c2i]=dy*v[1]+dx*v[0] #scalar product [v unit vector dx,dy in pixel coordinats from partaffinitymap]",0
Distances[l]['m2'] = distscalarproduct,0
TODO: compute all the grids etc. once for the video analysis method,1
(and then just pass on the variables),0
apply nms,0
"IMPORTANT, as C++ function expects row-major",0
These two functions are for evaluation specifically (one also calculates integral between gt points),0
get dist_grid,0
apply nms,0
"IMPORTANT, as C++ function expects row-major",0
if 'eval_scale' in cfg.keys():,0
import imgaug.augmenters as iaa,0
im = iaa.Resize(float(cfg['eval_scale']))(images=im),0
# Functions below implement are for batch sizes > 1:,0
get dist_grid,0
apply nms,0
"IMPORTANT, as C++ function expects row-major",0
"batchsize,ny,nx,num_joints = scmap.shape",0
Augmentation functions,0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"First, initializing variables (if they don't exist)",0
what is the fraction of training samples with scaling augmentation?,0
loading defaults for rotation range!,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"cfg[""rotateratio""] = cfg.get(",0
"""rotratio"", 0.4",0
)  # what is the fraction of training samples with rotation augmentation?,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"Auto cropping is new (was not in Nature Neuroscience 2018 paper, but introduced in Nath et al. Nat. Protocols 2019)",0
"and boosts performance by 2X, particularly on challenging datasets, like the cheetah in Nath et al.",0
Parameters for augmentation with regard to cropping:,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
Pickle the 'data' dictionary using the highest protocol available.,0
item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal,0
Scale is sampled only once (per batch) to transform all of the images into same size.,0
assert len(batch_images) == self.batch_size,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
"imageio.imwrite(data_items[i].im_path.split('/')[-1],im)",0
if returndata:,0
"return batch_images,batch_joints,targetmaps",0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
"print(person_id,joint_ids,data_item.im_path.split('/')[-1])",0
if self.cfg.partaffinityfield_predict:,0
"print(""hello"",joint_id)",0
print(np.concatenate(joint_id)) #this is all joint_ids for all individuals!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
"plt.savefig(""along""+str(data_item.im_path.split('/')[-1]))",0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
"print(person_id,k, j_id, coords[0])",0
assert(0==1),0
"A[:,:,2]=partaffinityfield_map[:,:,j]*0",0
"A[:,:,2]=partaffinityfield_map[:,:,j]*0",0
"plt.savefig(""SomePafs""+str(data_item.im_path.split('/')[-1]))",0
WIP!,0
STD of gaussian is 1/4 of threshold,0
Grid of coordinates,0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Distance between the joint point and each coordinate,0
NEEDS TO BE DONE!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
parameter initialization for augmentation pipeline:,0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images of a batch into same size.,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
"from scipy.misc import imread, imresize",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
"from scipy.misc import imread, imresize",0
"from dataset.pose_dataset import Batch, data_to_input, mirror_joints_map, CropImage, DataItem",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
Get rid of the interpolation beyond the spline knots,0
Retrieve original individual label indices,0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
"net_type = ""mobilenet_v2_0.35""",0
"net_type = ""efficientnet-b0""  # to -b6",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
TENSORPACK could fail in WINDOWS...,0
"NET = ""resnet_152""",0
"NET = ""efficientnet-b0""",0
"NET = ""mobilenet_v2_0.35"" # should be fixed",0
"""multi_step"": [[0.001, N_ITER]],",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
# Create one split and make Shuffle 2 and 3 have the same split.,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
for EfficientNet,0
imgaug,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
# Create one split and make Shuffle 2 and 3 have the same split.,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Dowloading the calibration images,0
Deleting unneccesary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
example how to set pose config variables:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
Verify the aspect ratio is preserved,0
Break the graph to test stitching failure,0
Add fake IDs,0
Split all tracklets in half,0
Assemble based on the smallest graph to speed up testing,0
Generate fake identity predictions,0
Assemble based on the smallest graph to speed up testing,0
Test now with identity only and ensure assemblies,0
contain only parts of a single group ID.,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
quicker variant,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
Suppress tensorflow warning messages,0
DLClight version does not support GUIs. Importing accordingly,0
"Train, evaluate & predict functions / all require TF",0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Updating config file:,0
Create the pose_config.yaml files,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path, num_shuffles=auxfun_models.Check4weights(pose_cfg['net_type'], parent_path, num_shuffles= 1)",0
Updating training and test pose_cfg:,0
Read the config file,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
common parameters:,0
Write dictionary to yaml  config file,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
Flatten the list of bodyparts to connect,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Start plotting for every frame,0
"Once all the frames are saved, then make a movie using ffmpeg.",0
Create the fig and define the axes,0
Clear plot and initialize the variables,0
Initialize arrays for appending the 3d data for actual plotting the points,0
Initialize arrays for appending the 3d data for drawing the lines,0
Initialize arrays for appending the 2d data from cam1 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam2 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam1 for drawing the lines,0
Initialize arrays for appending the 2d data from cam2 for drawing the lines,0
Get the scorer names from the dataframe,0
"Set the x,y, and z limits for the 3d view",0
"Set the frame number to read#max(0,index-trailpoints):index",0
Plot the labels for each body part,0
Connecting the bodyparts specified in the config file.3d file is created based on the likelihoods of cam1 and cam2. Using 3d file and check if the body part is nan then dont plot skeleton,0
Saving the frames,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
Extract the indices of frames where the likelihood of a bodypart for both cameras are less than pvalue,0
Extract frames where likelihood for both the views is less than the pcutoff,0
"low_likelihood_frames = np.all(likelihoods < pcutoff, axis=1)",0
For cam1 camera: Assign nans to x and y values of a bodypart where the likelihood for is less than pvalue,0
For cam2 camera: Assign nans to x and y values of a bodypart where the likelihood is less than pvalue,0
ToDo: speed up func. below by saving in numpy.array,1
"Create an empty dataframe to store x,y,z of 3d data",0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Undistorting the points from cam2 camera,0
Save the undistorted files,0
Alternatively,0
Sort tracklets by length to prioritize greater continuity,0
Drop tracklets that are too short,0
"Where slots are available, copy the data over",0
"If about to overwrite data, keep tracklets with highest confidence",0
Squeeze some data into empty slots,0
"For the remaining data, overwrite where we are least confident",0
Find keypoints closest to the remaining data,0
Use Manhattan distance to avoid overflow,0
Only overwrite if improving confidence,0
Fill existing gaps,0
Retrieve original individual label indices,0
Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.,0
Fill existing gaps,0
Retrieve original individual label indices,0
ID swaps occur when X and Y simultaneously intersect each other.,0
Get only those bodypart pairs that belong to different individuals,0
"See Li et al., 2012. Cross-view Activity Recognition using Hankelets.",0
"As proposed in the paper, the Hankel matrix can either be formed from",0
the tracklet's centroid or its normalized velocity.,0
"vel = np.diff(self.centroid, axis=0)",0
"vel /= np.linalg.norm(vel, axis=1, keepdims=True)",0
return self.hankelize(vel),0
TODO Add missing data imputation,1
"nrows, ncols = mat.shape",0
beta = nrows / ncols,0
omega = 0.56 * beta ** 3 - 0.95 * beta ** 2 + 1.82 * beta + 1.43,0
return np.argmin(s > omega * np.median(s)),0
"Note that if tracklets are very short, some may actually be part of the same track",0
and thus incorrectly reflect separate track endpoints...,0
"Map each Tracklet to an entry and output nodes and vice versa,",0
which is convenient once the tracklets are stitched.,0
TODO Avoid looping over all pairs of tracklets,0
The algorithm works better with integer weights,0
Let us prune the graph by removing all source and sink edges,0
but those connecting the `n_tracks` first and last tracklets.,0
Preflow push seems to work slightly better than shortest,0
"augmentation path..., and is more computationally efficient.",0
Verify whether there are overlapping tracklets,0
"Pick the segment that minimizes ""smoothness"", computed here",0
with the coefficient of variation of the differences.,0
Cycle through the residuals and incorporate back those,0
that only fit in a single tracklet.,0
Greedily add the remaining residuals,0
Refresh temporal bounds,0
Default to the distance cost function,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
average distance and average # significant differences avg. over comparisonbodyparts,0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
###################################################,0
Dependencies,0
###################################################,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as,0
specified in the config file,0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
Prepare figure,0
Mask rather than drop unwanted bodyparts to ensure consistent coloring,0
cropping!,0
nx=X2-X1,0
!/usr/bin/env python3,0
The above already raises a ValueError if formatting is wrong,0
"Rotate, see: https://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg",0
interesting option to just update metadata.,0
"Historically DLC used: from scipy.misc import imread, imresize >> deprecated functions",0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
Read the video until a frame is successfully read,0
import platform,0
"if platform.system() == ""Darwin"":  # for OSX use WXAgg",0
fig.canvas.start_event_loop(timeout=-1),0
else:,0
#fig.canvas.stop_event_loop(),0
"Windows throws error if file path is > 260 characters, can fix with prefix.",0
See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation,0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
"Attention this order has to be consistent (for training set creation, training, inference etc.)",0
"-> adding (single,bpt) for uniquebodyparts",0
"-> adding (indivdual,bpt) for multianimalbodyparts",0
load defaults,0
set project specific parameters:,0
###################################################,0
Dependencies,0
###################################################,0
Pose X vs pose Y,0
Poses vs time,0
Likelihoods,0
Histograms,0
#################################################,0
Looping analysis over video,0
#################################################,0
Keep only the individuals and bodyparts that were labeled,0
Determine whether the data are single- or multi-animal without loading into memory,0
simply by checking whether 'individuals' is in the second line of the CSV.,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"if ""MULTI!"" in allbpts:",0
Ensure same order as in config.yaml,0
ABBREVIATE NETWORK NAMES -- esp. for mobilenet!,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
Iterate over data files and stop as soon as one matching the scorer is found,0
If there was no match...,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
Read the infer config file,0
let the user open the file with default text editor. Also make it mac compatible,0
self.save.Enable(False),0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = io.imread(img),0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
topSplitter = wx.SplitterWindow(self),0
,0
"self.image_panel = ImagePanel(topSplitter, config,video,shuffle,Dataframe,self.gui_size)",0
self.widget_panel = WidgetPanel(topSplitter),0
,0
"topSplitter.SplitHorizontally(self.image_panel, self.widget_panel,sashPosition=self.gui_size[1]*0.83)#0.9",0
topSplitter.SetSashGravity(1),0
sizer = wx.BoxSizer(wx.VERTICAL),0
"sizer.Add(topSplitter, 1, wx.EXPAND)",0
self.SetSizer(sizer),0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
"cv2.imwrite(img_name, frame)",0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0,10), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=10)",0
Add all the options,0
line = wx.StaticLine(self),0
"self.sizer.Add(line, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=10)",0
,0
"self.multi_choice.Bind(wx.EVT_CHECKBOX,self.activate_copy_videos)",0
Hide the button as this is not the default option,0
self.vids.Enable(False),0
self.SetSizer(self.sizer),0
"self.sizer.Add(self.sizer, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=15)",0
self.ok.Enable(False),0
else:,0
self.ok.Enable(True),0
For mac compatibility,0
Remove the pages in case the user goes back to the create new project and creates/load a new project,0
Add all the other pages,0
self.yes.Enable(False),0
self.no.Enable(False),0
Here we create a panel and a notebook on the panel,0
create the page windows as children of the notebook and add the pages to the notebook with the label to show on the tab,0
Take into consideration imprecise OpenCV estimation of total number of frames,0
Automatically disable the draggable points,0
Save additional frames to the labeled-data directory,0
Store the newly-refined data,0
Uncertain keypoints are ignored,0
Merge with the already existing annotated data,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.buttonCounter = [],0
self.bodyparts = [],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
load labeledPoints and fundamental Matrix,0
Find offset terms for drawing epipolar Lines,0
Get crop params for camera being labeled,0
Get crop params for other camera,0
draw epipolar lines,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
variable initilization,0
design the panel,0
Add logo of DLC,0
self.filelist = self.filelist + self.vids,0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
load labeledPoints and fundamental Matrix,0
Find offset terms for drawing epipolar Lines,0
Get crop params for camera being labeled,0
Get crop params for other camera,0
im = cv2.imread(img),0
convert the image to RGB as you are showing the image with matplotlib,0
draw epipolar lines,0
divider = make_axes_locatable(self.axes),0
"colorIndex = np.linspace(np.min(im),np.max(im),len(bodyparts))",0
"cax = divider.append_axes(""right"", size=""5%"", pad=0.05)",0
"cbar = self.figure.colorbar(ax, cax=cax,spacing='proportional', ticks=colorIndex)",0
cbar.set_ticklabels(bodyparts[::-1]),0
self.figure.canvas.draw(),0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"MainFrame.saveDataSet(self, event)",0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
checks for unique bodyparts,0
"Reading the existing dataset,if already present",0
Handle data previously labeled on a different platform,0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Create an empty data frame,0
Cache original bodyparts,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Rearrange bodypart columns in config order,0
Test whether there are missing frames and superfluous data,0
Check whether new labels were added,0
Checking if user added a new label,0
Found new labels in either multiple bodyparts or unique bodyparts,0
check if single is slected when radio buttons are changed,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Discard data associated with bodyparts that are no longer in the config,0
Re-organize the dataframe so the CSV looks consistent with the config,0
restore the background region,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.build = wx.Button(self, label=""Build skeleton"")",0
"sizer.Add(self.build, pos=(4, 3), flag=wx.BOTTOM | wx.RIGHT, border=10)",0
"self.build.Bind(wx.EVT_BUTTON, self.build_skeleton)",0
self.build.Enable(True),0
"def build_skeleton(self, event):",0
skeleton.SkeletonBuilder(self.config),0
Settting the GUI size and panels design,0
Checks if zoom/pan button is ON,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
use the default pose_cfg file for default values,0
self.display_iters.Enable(False),0
self.save_iters.Enable(False),0
self.max_iters.Enable(False),0
self.snapshots.Enable(False),0
Read the pose config file,0
"print(os.path.join(cfg['project_path'],auxiliaryfunctions.GetModelFolder(trainFraction, self.shuffles.GetValue(),cfg),'train','pose_cfg.yaml'))",0
let the user open the file with default text editor. Also make it mac compatible,0
update the variables with the edited values in the pose config file,0
variable initilization,0
design the panel,0
Add logo of DLC,0
Read the infer config file,0
let the user open the file with default text editor. Also make it mac compatible,0
"def video_tracklets(self,event):",0
shuffle = self.shuffle.GetValue(),0
trainingsetindex = self.trainingset.GetValue(),0
"deeplabcut.create_video_from_pickled_tracks(self.filelist, picklefile, pcutoff=0.6)",0
"self.select_destfolder.SetPath(""None"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.shuffles.SetValue(""1"")",0
"DownSampleVideo(vname,width=-1,height=200,outsuffix='downsampled',outpath=None,rotateccw=False):",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"majorDimension=0,",0
"self.videotype.SetStringSelection("".avi"")",0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
"self.choiceBox,self.visualization_rdb = self.choice_panel.addRadioButtons()",0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
print(self.Dataframe.head()),0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
fix move to corner,0
fix move to corner,0
fix move to corner,0
fix move to corner,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"if self.plot_scoremaps.GetStringSelection() == ""Yes"":",0
Read the infer config file,0
let the user open the file with default text editor. Also make it mac compatible,0
shuffle = self.shuffle.GetValue(),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
self.cfg = auxiliaryfunctions.read_config(config),0
"self.statusbar.SetStatusText("""")",0
"dlg = wx.MessageDialog(None,""Are you sure?"", ""Quit!"",wx.YES_NO | wx.ICON_WARNING)",0
result = dlg.ShowModal(),0
if result == wx.ID_YES:,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0, 4), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=5)",0
variable initilization,0
design the panel,0
Add logo of DLC,0
#         design the panel,0
Add image of DLC,0
if editing this text make sure you add the '\n' to get the new line. The sizer is unable to format lines correctly.,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
actualbpts=set(Data.columns.get_level_values(0)),0
utility function to split different crops from same image into either train or test!,0
loading & linking pretrained models,0
CURRENTLY ONLY ResNet supported!,0
multianimal case:,0
Automatically form a complete PAF graph,0
Loading the encoder (if necessary downloading from TF),0
Map back to the original indices.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
load image to get dimensions:,0
"print ""Grayscale!""",0
if socialbdpt in actualbpts:,0
if socialbdpt in actualbpts:,0
Drop missing body parts,0
Drop points lying outside the image,0
###############################################################################,0
Saving metadata and data file (Pickle file),0
###############################################################################,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Setting inference cfg file:,0
Load updated lists:,0
Find the corresponding video file,0
Avoid cropping already cropped images,0
"moving old entry to _original, dropping it from video_set and update crop parameters",0
"print(key, ""dropping?"")",0
adding important values for multianiaml project:,0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Let's check if the code is *not* run on windows (Source: #https://stackoverflow.com/questions/1325581/how-do-i-check-if-im-running-on-windows-in-python),0
but the paths are in windows format...,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(index,name.split(os.sep)[1])",0
"print(name,test_video_name)",0
Check that points lie within the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
updating variable if null/None! #backwardscompatability,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
these values are dropped as scalecrop,0
doesn't have rotation implemented,0
sort the models directories,0
get the shuffle index,0
read cfg file,0
create log file,0
Check for variable correctness,0
Moviepy:,0
select crop method,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
"sess, inputs, outputs = predict.setup_pose_prediction(dlc_cfg)",0
#################################################,0
Looping over videos,0
#################################################,0
Re-use data-driven PAF graph for video analysis. Note that this must,0
happen after setting up the TF session to avoid graph mismatch.,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
from skimage.io import imread,0
"frame=imread(os.path.join(directory,framename),mode='RGB')",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
TODO: addd cropping as in video analysis!,1
if cropping is not None:,0
cfg['cropping']=True,0
"cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping",0
"print(""Overwriting cropping parameters:"", cropping)",0
"print(""These are used for all videos, but won't be save to the cfg file."")",0
Check which snapshots are available and sort them by # iterations,0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
"NOTE: this can be used if only a subset is relevant. I.e. [0,1] for only first and second joint!",0
TODO: adjust this for multi + unique bodyparts!,1
this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. ),0
"print(""If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract any outlier frames!"")",0
imagenet mean for resnet pretraining:,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/AlexEMG/DeepLabCut/pull/324",0
for DLC 2.2. (here all set False to not use PAFs/pairwise fields),0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
"reloading defaults, as they can bleed over from a previous run otherwise",0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
dlc_cfg_train = load_config(str(path_train_config)),0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
Name for deeplabcut net (based on its parameters),0
"DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])",0
"print(""Extracting maps for "", DLCscorer, "" with # of trainingiterations:"", trainingsiterations)",0
if notanalyzed: #this only applies to ask if h5 exists...,0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
Dependencies for anaysis,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Load meta data & annotations,0
get the graph!,0
"NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*",0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
TODO: Make this code not so redundant!,1
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
"# Also storing one ""large"" table with results:",0
note: evaluationfolder.parents[0] to get common folder above all shuffle evaluations.,0
#################################################,0
Load data...,0
#################################################,0
Loading human annotatated data,0
Handle data previously annotated on a different platform,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
TODO: IMPLEMENT for different batch sizes?,1
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
print(dlc_cfg),0
Specifying state of model (snapshot / training state),0
Evaluate PAF edge lengths to calibrate `distnorm`,0
FIXME Is having an empty array vs nan really that necessary?!,1
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in,0
Compute all distance statistics,0
Calculate overall prediction error,0
"For OKS/PCK, compute the standard deviation error across all frames",0
Data-driven skeleton selection,0
returning to intial folder,0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
"[_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})",0
Save snapshot,0
return to original path.,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
from deeplabcut.pose_estimation_tensorflow.predict_multianimal import convert_detections2tracklet,0
initializing constants,0
"PredicteData['frame'+str(counter)]=predict.get_detectionswithcosts(frame, dlc_cfg, sess, inputs, outputs, outall=False,nms_radius=dlc_cfg.nmsradius,det_min_score=dlc_cfg.minconfidence)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
"PredicteData[batch_num*batchsize:batch_num*batchsize+batch_ind, :] = pose[:batch_ind,:]",0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Sort crop patches to maximize the probability they overlap with others,0
Form the ground truth back,0
Match detections across crops,0
Store the costs associated with the retained candidates,0
Form ground truth beforehand,0
Assemble animals on the full set of detections,0
Count the number of unassembled bodyparts,0
Handle unlabeled bodyparts...,0
Find minimal skeleton,0
Select optimal PAF graph,0
Selective copy; deepcopy is >5x slower,0
Only keeps skeletons that are more than 90% complete,0
TODO Normalize dists by longest length?,1
TODO Smarter imputation technique (Bayesian? Grassmann averages?),1
Correct distance to account for missing observations,0
"Alternatively, reduce contribution of missing values to the Mahalanobis",0
distance to zero by substituting the corresponding means.,0
"Fill the subsets with unambiguous, complete individuals",0
Fuse superfluous assemblies,0
Second pass without edge safety,0
Store selected edges for subsequent frames,0
Remove invalid assemblies,0
Fill assemblies with unconnected body parts,0
sigma is taken as the median of all COCO keypoint standard deviations,0
define constant velocity model,0
Regularize by forcing AR <= 5,0
max_ar = 5,0
if el.aspect_ratio >= max_ar:,0
if el.height > el.width:,0
el.width = el.height / max_ar,0
else:,0
el.height = el.width / max_ar,0
Orient the ellipse such that it encompasses most points,0
"n_inside = el.contains_points(np.c_[self.x, self.y]).sum()",0
el.theta += 0.5 * np.pi,0
"if el.contains_points(np.c_[self.x, self.y]).sum() < n_inside:",0
el.theta -= 0.5 * np.pi,0
"r2 = chi2.ppf(2 * norm.cdf(sd) - 1, 2)",0
"height, width = np.sqrt(E * r2)",0
The general quadratic curve has the form:,0
ax^2 + 2bxy + cy^2 + 2dx + 2fy + g = 0,0
Ellipse center coordinates,0
Semi-axes lengths,0
Angle to the horizontal,0
Reset tracker IDs,0
diff = val - cost_matrix,0
"diff[row, col] += val",0
if (,0
val < self.iou_threshold,0
or np.any(diff[row] <= 0.2),0
"or np.any(diff[:, col] <= 0.2)",0
):,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
TODO Try particle filter (since we already have the keypoints),1
Modified from scipy source code:,0
- to restrict its use to 2D,0
- to get rid of shuffling (since arrays are only (nbodyparts * 3) element long),0
TODO - factor in keypoint confidence (and weight by # of observations??),1
"mat = self.calc_pairwise_oks(poses, poses_ref)",0
Remove matched detections with low OKS,0
matches = [],0
"for row, col in zip(row_indices, col_indices):",0
"if mat[row, col] < self.oks_threshold:",0
unmatched_poses.append(row),0
unmatched_trackers.append(col),0
else:,0
"matches.append([row, col])",0
if not len(matches):,0
"matches = np.empty((0, 2), dtype=int)",0
else:,0
matches = np.stack(matches),0
filter out matched with low IOU,0
get predicted locations from existing trackers.,0
update matched trackers with assigned detections,0
create and initialise new trackers for unmatched detections,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
TODO: get cropping parameters and utilize!,1
openmp_arg = '-fopenmp',0
"if _platform == ""win32"":",0
openmp_arg = '-openmp',0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
assuming batchsize 1 here!,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
setting pairwise bodypart loss,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
defaults will be a public argument for namedtuple in Python 3.7,0
https://docs.python.org/3/library/collections.html#collections.namedtuple,0
Make sure that round down does not go down by more than 10%.,0
Builds the block accordings to arguments.,0
Expansion phase:,0
Depth-wise convolution phase:,0
Squeeze and Excitation layer.,0
Output phase:,0
only apply drop_connect if skip presents.,0
Expansion phase:,0
Output phase:,0
only apply drop_connect if skip presents.,0
Builds blocks.,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Stem part.,0
Head part.,0
Calls Stem layers,0
Calls blocks.,0
Calls final layers and returns logits.,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"(width_coefficient, depth_coefficient, resolution, dropout_rate)",0
blocks_args = [,0
"'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',",0
"'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',",0
"'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',",0
"'r1_k3_s11_e6_i192_o320_se0.25',",0
],0
The default is TPU-specific batch norm.,0
The alternative is tf.layers.BatchNormalization.,0
"batch_norm=utils.TpuBatchNormalization,  # TPU-specific requirement.",0
ValueError will be raised here if override_params has fields not included,0
in global_params.,0
print(end_points.keys()) >> to see what else is available.,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
setting pairwise bodypart loss,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
class TpuBatchNormalization(tf.layers.BatchNormalization):,0
Compute variance using: Var[X]= E[X^2] - E[X]^2.,0
Compute keep_prob,0
TODO(tanmingxing): add support for training progress.,1
Compute drop_connect tensor,0
Clear the old folder.,0
Write checkpoints.,0
Update the best objective.,0
We maintain mva for batch norm moving mean and variance as well.,0
Return the top 5 predictions (idx and prob) for each image.,0
Change the stride from 2 to 1 to get 16x downscaling instead of 32x.,0
https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142,0
## New DLCNet Addition: multi-stage decoder,0
standard stride 2 decoder,0
Multi stage is currently only implemented for resnets,0
The next part of the code depends upon the tensorflow version,0
Defining multi_fusion backbone,0
Attaching multi-stage decoder,0
stage_paf_output = stage_paf_output + pre_stage_paf_output,0
"return self.prediction_layers(net, end_points)",0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
from nets.mobilenet import conv_blocks as ops,0
from nets.mobilenet import mobilenet as lib,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
The next part of the code depends upon which tensorflow version you have.,0
assuming batchsize 1 here!,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
setting pairwise bodypart loss,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
get coordinates for bp1 and bp2,0
'm2',0
"distscalarproduct=np.zeros((len(C1),len(C2)))*np.nan",0
"distscalarproduct[c1i,c2i]=dy*v[1]+dx*v[0] #scalar product [v unit vector dx,dy in pixel coordinats from partaffinitymap]",0
Distances[l]['m2'] = distscalarproduct,0
TODO: compute all the grids etc. once for the video analysis method,1
(and then just pass on the variables),0
apply nms,0
"IMPORTANT, as C++ function expects row-major",0
These two functions are for evaluation specifically (one also calculates integral between gt points),0
get dist_grid,0
apply nms,0
"IMPORTANT, as C++ function expects row-major",0
if 'eval_scale' in cfg.keys():,0
import imgaug.augmenters as iaa,0
im = iaa.Resize(float(cfg['eval_scale']))(images=im),0
# Functions below implement are for batch sizes > 1:,0
get dist_grid,0
apply nms,0
"IMPORTANT, as C++ function expects row-major",0
"batchsize,ny,nx,num_joints = scmap.shape",0
Augmentation functions,0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"First, initializing variables (if they don't exist)",0
what is the fraction of training samples with scaling augmentation?,0
loading defaults for rotation range!,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"cfg[""rotateratio""] = cfg.get(",0
"""rotratio"", 0.4",0
)  # what is the fraction of training samples with rotation augmentation?,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"Auto cropping is new (was not in Nature Neuroscience 2018 paper, but introduced in Nath et al. Nat. Protocols 2019)",0
"and boosts performance by 2X, particularly on challenging datasets, like the cheetah in Nath et al.",0
Parameters for augmentation with regard to cropping:,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
Pickle the 'data' dictionary using the highest protocol available.,0
item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal,0
Scale is sampled only once (per batch) to transform all of the images into same size.,0
assert len(batch_images) == self.batch_size,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
"imageio.imwrite(data_items[i].im_path.split('/')[-1],im)",0
if returndata:,0
"return batch_images,batch_joints,targetmaps",0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
"print(person_id,joint_ids,data_item.im_path.split('/')[-1])",0
if self.cfg.partaffinityfield_predict:,0
"print(""hello"",joint_id)",0
print(np.concatenate(joint_id)) #this is all joint_ids for all individuals!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
"plt.savefig(""along""+str(data_item.im_path.split('/')[-1]))",0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
"print(person_id,k, j_id, coords[0])",0
assert(0==1),0
"A[:,:,2]=partaffinityfield_map[:,:,j]*0",0
"A[:,:,2]=partaffinityfield_map[:,:,j]*0",0
"plt.savefig(""SomePafs""+str(data_item.im_path.split('/')[-1]))",0
WIP!,0
STD of gaussian is 1/4 of threshold,0
Grid of coordinates,0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Distance between the joint point and each coordinate,0
NEEDS TO BE DONE!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
parameter initialization for augmentation pipeline:,0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images of a batch into same size.,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
"from scipy.misc import imread, imresize",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
"from scipy.misc import imread, imresize",0
"from dataset.pose_dataset import Batch, data_to_input, mirror_joints_map, CropImage, DataItem",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
Get rid of the interpolation beyond the spline knots,0
Retrieve original individual label indices,0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
"net_type = ""mobilenet_v2_0.35""",0
"net_type = ""efficientnet-b0""  # to -b6",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
TENSORPACK could fail in WINDOWS...,0
"NET = ""efficientnet-b0""",0
"""multi_step"": [[0.001, N_ITER]],",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
# Create one split and make Shuffle 2 and 3 have the same split.,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
for EfficientNet,0
imgaug,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
# Create one split and make Shuffle 2 and 3 have the same split.,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Dowloading the calibration images,0
Deleting unneccesary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
example how to set pose config variables:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
Verify the aspect ratio is preserved,0
Break the graph to test stitching failure,0
Assemble based on the smallest graph to speed up testing,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
quicker variant,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
Suppress tensorflow warning messages,0
DLClight version does not support GUIs. Importing accordingly,0
"Train, evaluate & predict functions / all require TF",0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Updating config file:,0
Create the pose_config.yaml files,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path, num_shuffles=auxfun_models.Check4weights(pose_cfg['net_type'], parent_path, num_shuffles= 1)",0
Updating training and test pose_cfg:,0
Read the config file,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
common parameters:,0
Write dictionary to yaml  config file,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
Flatten the list of bodyparts to connect,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Start plotting for every frame,0
"Once all the frames are saved, then make a movie using ffmpeg.",0
Create the fig and define the axes,0
Clear plot and initialize the variables,0
Initialize arrays for appending the 3d data for actual plotting the points,0
Initialize arrays for appending the 3d data for drawing the lines,0
Initialize arrays for appending the 2d data from cam1 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam2 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam1 for drawing the lines,0
Initialize arrays for appending the 2d data from cam2 for drawing the lines,0
Get the scorer names from the dataframe,0
"Set the x,y, and z limits for the 3d view",0
"Set the frame number to read#max(0,index-trailpoints):index",0
Plot the labels for each body part,0
Connecting the bodyparts specified in the config file.3d file is created based on the likelihoods of cam1 and cam2. Using 3d file and check if the body part is nan then dont plot skeleton,0
Saving the frames,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
Extract the indices of frames where the likelihood of a bodypart for both cameras are less than pvalue,0
Extract frames where likelihood for both the views is less than the pcutoff,0
"low_likelihood_frames = np.all(likelihoods < pcutoff, axis=1)",0
For cam1 camera: Assign nans to x and y values of a bodypart where the likelihood for is less than pvalue,0
For cam2 camera: Assign nans to x and y values of a bodypart where the likelihood is less than pvalue,0
ToDo: speed up func. below by saving in numpy.array,1
"Create an empty dataframe to store x,y,z of 3d data",0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Undistorting the points from cam2 camera,0
Save the undistorted files,0
Alternatively,0
Sort tracklets by length to prioritize greater continuity,0
Drop tracklets that are too short,0
"Where slots are available, copy the data over",0
"If about to overwrite data, keep tracklets with highest confidence",0
Squeeze some data into empty slots,0
"For the remaining data, overwrite where we are least confident",0
Find keypoints closest to the remaining data,0
Use Manhattan distance to avoid overflow,0
Only overwrite if improving confidence,0
Fill existing gaps,0
Retrieve original individual label indices,0
Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.,0
ID swaps occur when X and Y simultaneously intersect each other.,0
Get only those bodypart pairs that belong to different individuals,0
"See Li et al., 2012. Cross-view Activity Recognition using Hankelets.",0
"As proposed in the paper, the Hankel matrix can either be formed from",0
the tracklet's centroid or its normalized velocity.,0
"vel = np.diff(self.centroid, axis=0)",0
"vel /= np.linalg.norm(vel, axis=1, keepdims=True)",0
return self.hankelize(vel),0
TODO Add missing data imputation,1
"nrows, ncols = mat.shape",0
beta = nrows / ncols,0
omega = 0.56 * beta ** 3 - 0.95 * beta ** 2 + 1.82 * beta + 1.43,0
return np.argmin(s > omega * np.median(s)),0
"Note that if tracklets are very short, some may actually be part of the same track",0
and thus incorrectly reflect separate track endpoints...,0
"Map each Tracklet to an entry and output nodes and vice versa,",0
which is convenient once the tracklets are stitched.,0
TODO Avoid looping over all pairs of tracklets,0
The algorithm works better with integer weights,0
Let us prune the graph by removing all source and sink edges,0
but those connecting the `n_tracks` first and last tracklets.,0
Preflow push seems to work slightly better than shortest,0
"augmentation path..., and is more computationally efficient.",0
Verify whether there are overlapping tracklets,0
"Pick the segment that minimizes ""smoothness"", computed here",0
with the coefficient of variation of the differences.,0
Cycle through the residuals and incorporate back those,0
that only fit in a single tracklet.,0
Greedily add the remaining residuals,0
Refresh temporal bounds,0
Default to the distance cost function,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
average distance and average # significant differences avg. over comparisonbodyparts,0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
###################################################,0
Dependencies,0
###################################################,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as,0
specified in the config file,0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
Prepare figure,0
Mask rather than drop unwanted bodyparts to ensure consistent coloring,0
cropping!,0
nx=X2-X1,0
!/usr/bin/env python3,0
The above already raises a ValueError if formatting is wrong,0
"Rotate, see: https://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg",0
interesting option to just update metadata.,0
"Historically DLC used: from scipy.misc import imread, imresize >> deprecated functions",0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
Read the video until a frame is successfully read,0
import platform,0
"if platform.system() == ""Darwin"":  # for OSX use WXAgg",0
fig.canvas.start_event_loop(timeout=-1),0
else:,0
#fig.canvas.stop_event_loop(),0
"Windows throws error if file path is > 260 characters, can fix with prefix.",0
See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation,0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
"Attention this order has to be consistent (for training set creation, training, inference etc.)",0
CHECKS if each bpt is connected to at least one other bpt,0
TODO: check that there is a path leading from each (multi)bpt to each other (multi)bpt!,1
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
if suffix=='_full': #save metadata!,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"-> adding (single,bpt) for uniquebodyparts",0
"-> adding (indivdual,bpt) for multianimalbodyparts",0
load defaults,0
set project specific parameters:,0
###################################################,0
Dependencies,0
###################################################,0
Pose X vs pose Y,0
Poses vs time,0
Likelihoods,0
Histograms,0
#################################################,0
Looping analysis over video,0
#################################################,0
Keep only the individuals and bodyparts that were labeled,0
Determine whether the data are single- or multi-animal without loading into memory,0
simply by checking whether 'individuals' is in the second line of the CSV.,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
Ensure same order as in config.yaml,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
Iterate over data files and stop as soon as one matching the scorer is found,0
If there was no match...,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
NEW ROW:,0
Read the infer config file,0
let the user open the file with default text editor. Also make it mac compatible,0
self.save.Enable(False),0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = io.imread(img),0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
topSplitter = wx.SplitterWindow(self),0
,0
"self.image_panel = ImagePanel(topSplitter, config,video,shuffle,Dataframe,self.gui_size)",0
self.widget_panel = WidgetPanel(topSplitter),0
,0
"topSplitter.SplitHorizontally(self.image_panel, self.widget_panel,sashPosition=self.gui_size[1]*0.83)#0.9",0
topSplitter.SetSashGravity(1),0
sizer = wx.BoxSizer(wx.VERTICAL),0
"sizer.Add(topSplitter, 1, wx.EXPAND)",0
self.SetSizer(sizer),0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
"cv2.imwrite(img_name, frame)",0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0,10), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=10)",0
Add all the options,0
line = wx.StaticLine(self),0
"self.sizer.Add(line, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=10)",0
,0
"self.multi_choice.Bind(wx.EVT_CHECKBOX,self.activate_copy_videos)",0
Hide the button as this is not the default option,0
self.vids.Enable(False),0
self.SetSizer(self.sizer),0
"self.sizer.Add(self.sizer, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=15)",0
self.ok.Enable(False),0
else:,0
self.ok.Enable(True),0
For mac compatibility,0
Remove the pages in case the user goes back to the create new project and creates/load a new project,0
Add all the other pages,0
self.yes.Enable(False),0
self.no.Enable(False),0
Here we create a panel and a notebook on the panel,0
create the page windows as children of the notebook and add the pages to the notebook with the label to show on the tab,0
Take into consideration imprecise OpenCV estimation of total number of frames,0
Automatically disable the draggable points,0
Save additional frames to the labeled-data directory,0
Store the newly-refined data,0
Uncertain keypoints are ignored,0
Merge with the already existing annotated data,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.buttonCounter = [],0
self.bodyparts = [],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
load labeledPoints and fundamental Matrix,0
Find offset terms for drawing epipolar Lines,0
Get crop params for camera being labeled,0
Get crop params for other camera,0
draw epipolar lines,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
variable initilization,0
design the panel,0
Add logo of DLC,0
self.filelist = self.filelist + self.vids,0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
load labeledPoints and fundamental Matrix,0
Find offset terms for drawing epipolar Lines,0
Get crop params for camera being labeled,0
Get crop params for other camera,0
im = cv2.imread(img),0
convert the image to RGB as you are showing the image with matplotlib,0
draw epipolar lines,0
divider = make_axes_locatable(self.axes),0
"colorIndex = np.linspace(np.min(im),np.max(im),len(bodyparts))",0
"cax = divider.append_axes(""right"", size=""5%"", pad=0.05)",0
"cbar = self.figure.colorbar(ax, cax=cax,spacing='proportional', ticks=colorIndex)",0
cbar.set_ticklabels(bodyparts[::-1]),0
self.figure.canvas.draw(),0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"MainFrame.saveDataSet(self, event)",0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
checks for unique bodyparts,0
"Reading the existing dataset,if already present",0
Handle data previously labeled on a different platform,0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Create an empty data frame,0
Cache original bodyparts,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Rearrange bodypart columns in config order,0
Test whether there are missing frames and superfluous data,0
Check whether new labels were added,0
Checking if user added a new label,0
Found new labels in either multiple bodyparts or unique bodyparts,0
check if single is slected when radio buttons are changed,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Discard data associated with bodyparts that are no longer in the config,0
Re-organize the dataframe so the CSV looks consistent with the config,0
restore the background region,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
Settting the GUI size and panels design,0
Checks if zoom/pan button is ON,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
use the default pose_cfg file for default values,0
self.display_iters.Enable(False),0
self.save_iters.Enable(False),0
self.max_iters.Enable(False),0
self.snapshots.Enable(False),0
Read the pose config file,0
"print(os.path.join(cfg['project_path'],auxiliaryfunctions.GetModelFolder(trainFraction, self.shuffles.GetValue(),cfg),'train','pose_cfg.yaml'))",0
let the user open the file with default text editor. Also make it mac compatible,0
update the variables with the edited values in the pose config file,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"if self.cfg.get(""multianimalproject"", False):",0
pass,0
else:,0
removing this as several downstream maDLC steps don't support dest_folder at this time:,0
"destfolder_text = wx.StaticBox(self, label=""Specify destination folder"")",0
"destfolderboxsizer = wx.StaticBoxSizer(destfolder_text, wx.VERTICAL)",0
self.change_workingdir = wx.CheckBox(,0
"self, label=""optional destination folder""",0
),0
self.hbox2.Add(self.change_workingdir),0
"self.change_workingdir.Bind(wx.EVT_CHECKBOX, self.activate_change_wd)",0
"self.sel_wd = wx.Button(self, label=""Browse"")",0
self.sel_wd.Enable(False),0
"self.sel_wd.Bind(wx.EVT_BUTTON, self.select_destfolder)",0
"self.hbox2.Add(self.sel_wd, 0, wx.ALL, -1)",0
Read the infer config file,0
let the user open the file with default text editor. Also make it mac compatible,0
"def video_tracklets(self,event):",0
shuffle = self.shuffle.GetValue(),0
trainingsetindex = self.trainingset.GetValue(),0
"deeplabcut.create_video_from_pickled_tracks(self.filelist, picklefile, pcutoff=0.6)",0
"self.select_destfolder.SetPath(""None"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.shuffles.SetValue(""1"")",0
"DownSampleVideo(vname,width=-1,height=200,outsuffix='downsampled',outpath=None,rotateccw=False):",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"majorDimension=0,",0
"self.videotype.SetStringSelection("".avi"")",0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
"self.choiceBox,self.visualization_rdb = self.choice_panel.addRadioButtons()",0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
print(self.Dataframe.head()),0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
fix move to corner,0
fix move to corner,0
fix move to corner,0
fix move to corner,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"if self.plot_scoremaps.GetStringSelection() == ""Yes"":",0
Read the infer config file,0
let the user open the file with default text editor. Also make it mac compatible,0
shuffle = self.shuffle.GetValue(),0
Read from edited inf. file first ...,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
self.cfg = auxiliaryfunctions.read_config(config),0
"self.statusbar.SetStatusText("""")",0
"dlg = wx.MessageDialog(None,""Are you sure?"", ""Quit!"",wx.YES_NO | wx.ICON_WARNING)",0
result = dlg.ShowModal(),0
if result == wx.ID_YES:,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0, 4), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=5)",0
variable initilization,0
design the panel,0
Add logo of DLC,0
#         design the panel,0
Add image of DLC,0
if editing this text make sure you add the '\n' to get the new line. The sizer is unable to format lines correctly.,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
actualbpts=set(Data.columns.get_level_values(0)),0
utility function to split different crops from same image into either train or test!,0
loading & linking pretrained models,0
CURRENTLY ONLY ResNet supported!,0
multianimal case:,0
"ATTENTION: order has to be multibodyparts, then uniquebodyparts (for indexing)",0
Loading the encoder (if necessary downloading from TF),0
Map back to the original indices.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
load image to get dimensions:,0
"print ""Grayscale!""",0
if socialbdpt in actualbpts:,0
if socialbdpt in actualbpts:,0
Drop missing body parts,0
Drop points lying outside the image,0
###############################################################################,0
Saving metadata and data file (Pickle file),0
###############################################################################,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Setting inference cfg file:,0
Load updated lists:,0
Find the corresponding video file,0
Avoid cropping already cropped images,0
"moving old entry to _original, dropping it from video_set and update crop parameters",0
"print(key, ""dropping?"")",0
adding important values for multianiaml project:,0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Let's check if the code is *not* run on windows (Source: #https://stackoverflow.com/questions/1325581/how-do-i-check-if-im-running-on-windows-in-python),0
but the paths are in windows format...,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(index,name.split(os.sep)[1])",0
"print(name,test_video_name)",0
Check that points lie within the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
updating variable if null/None! #backwardscompatability,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
these values are dropped as scalecrop,0
doesn't have rotation implemented,0
sort the models directories,0
get the shuffle index,0
read cfg file,0
create log file,0
Check for variable correctness,0
Moviepy:,0
select crop method,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
"sess, inputs, outputs = predict.setup_pose_prediction(dlc_cfg)",0
#################################################,0
Looping over videos,0
#################################################,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
from skimage.io import imread,0
"frame=imread(os.path.join(directory,framename),mode='RGB')",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
TODO: addd cropping as in video analysis!,1
if cropping is not None:,0
cfg['cropping']=True,0
"cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping",0
"print(""Overwriting cropping parameters:"", cropping)",0
"print(""These are used for all videos, but won't be save to the cfg file."")",0
Check which snapshots are available and sort them by # iterations,0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
"NOTE: this can be used if only a subset is relevant. I.e. [0,1] for only first and second joint!",0
TODO: adjust this for multi + unique bodyparts!,1
this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. ),0
Test whether the unique bodyparts have been assembled,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"print(""If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract any outlier frames!"")",0
imagenet mean for resnet pretraining:,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/AlexEMG/DeepLabCut/pull/324",0
for DLC 2.2. (here all set False to not use PAFs/pairwise fields),0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
"reloading defaults, as they can bleed over from a previous run otherwise",0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
dlc_cfg_train = load_config(str(path_train_config)),0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
Name for deeplabcut net (based on its parameters),0
"DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])",0
"print(""Extracting maps for "", DLCscorer, "" with # of trainingiterations:"", trainingsiterations)",0
if notanalyzed: #this only applies to ask if h5 exists...,0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
Dependencies for anaysis,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Load meta data & annotations,0
get the graph!,0
"NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*",0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
TODO: Make this code not so redundant!,1
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
"# Also storing one ""large"" table with results:",0
note: evaluationfolder.parents[0] to get common folder above all shuffle evaluations.,0
#################################################,0
Load data...,0
#################################################,0
Loading human annotatated data,0
Handle data previously annotated on a different platform,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
TODO: IMPLEMENT for different batch sizes?,1
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
print(dlc_cfg),0
Specifying state of model (snapshot / training state),0
Evaluate PAF edge lengths to calibrate `distnorm`,0
FIXME Is having an empty array vs nan really that necessary?!,1
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in,0
Compute all distance statistics,0
Calculate overall prediction error,0
"For OKS/PCK, compute the standard deviation error across all frames",0
returning to intial folder,0
wild guesses for a wide range:,0
Check which snapshots are available and sort them by # iterations,0
Pick distance threshold for (r)PCK from the statistics computed during evaluation,0
update number of individuals to retain.,0
calculating result at best best solution,0
"print(""Quantification:"", DataOptParams.head())",0
DataOptParams.to_hdf(,0
"path_inference_config.split("".yaml"")[0] + "".h5"",",0
"""df_with_missing"",",0
"format=""table"",",0
"mode=""w"",",0
),0
Store best predictions,0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
"[_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})",0
Save snapshot,0
return to original path.,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
from deeplabcut.pose_estimation_tensorflow.predict_multianimal import convert_detections2tracklet,0
initializing constants,0
"PredicteData['frame'+str(counter)]=predict.get_detectionswithcosts(frame, dlc_cfg, sess, inputs, outputs, outall=False,nms_radius=dlc_cfg.nmsradius,det_min_score=dlc_cfg.minconfidence)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
"PredicteData[batch_num*batchsize:batch_num*batchsize+batch_ind, :] = pose[:batch_ind,:]",0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
animal & bpt-wise distance!,0
"corrkps=np.sum((gt[row_indices]-ani[col_indices])**2,axis=2)<dcorr**2",0
animal & bpt-wise distance!,0
"corrkps=np.sum((gt[row_indices]-ani[col_indices])**2,axis=2)<dcorr**2",0
load params,0
Ensure type consistency,0
"stats = compute_crossval_metrics(config_path, inferencecfg, shuffle,trainingsetindex,",0
"dcorr=dcorr,leastbpts=leastbpts,modelprefix=modelprefix)",0
val = stats['rmse_test'].values[0]*(1+stats['misses_test'].values[0]*1./stats['hits_test'].values[0]),0
##################################,0
### auxiliaryfunctions,0
##################################,0
#########################################################,0
### conversion & greedy bodypart matching code,0
#########################################################,0
"d=distance(np.array(cand_a[i][:2]),np.array(cand_b[j][:2]))",0
filtering with global distance bounds,0
sort candidate connections by score,0
"if both bodyparts don't exist, create a new subset",0
filter detections according to inferencecfg parameters,0
filter connections according to inferencecfg parameters,0
assemble putative subsets,0
define constant velocity model,0
Regularize by forcing AR <= 5,0
max_ar = 5,0
if el.aspect_ratio >= max_ar:,0
if el.height > el.width:,0
el.width = el.height / max_ar,0
else:,0
el.height = el.width / max_ar,0
Orient the ellipse such that it encompasses most points,0
"n_inside = el.contains_points(np.c_[self.x, self.y]).sum()",0
el.theta += 0.5 * np.pi,0
"if el.contains_points(np.c_[self.x, self.y]).sum() < n_inside:",0
el.theta -= 0.5 * np.pi,0
"r2 = chi2.ppf(2 * norm.cdf(sd) - 1, 2)",0
"height, width = np.sqrt(E * r2)",0
The general quadratic curve has the form:,0
ax^2 + 2bxy + cy^2 + 2dx + 2fy + g = 0,0
Ellipse center coordinates,0
Semi-axes lengths,0
Angle to the horizontal,0
Reset tracker IDs,0
diff = val - cost_matrix,0
"diff[row, col] += val",0
if (,0
val < self.iou_threshold,0
or np.any(diff[row] <= 0.2),0
"or np.any(diff[:, col] <= 0.2)",0
):,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
TODO Try particle filter (since we already have the keypoints),1
Modified from scipy source code:,0
- to restrict its use to 2D,0
- to get rid of shuffling (since arrays are only (nbodyparts * 3) element long),0
TODO - factor in keypoint confidence (and weight by # of observations??),1
"mat = self.calc_pairwise_oks(poses, poses_ref)",0
Remove matched detections with low OKS,0
matches = [],0
"for row, col in zip(row_indices, col_indices):",0
"if mat[row, col] < self.oks_threshold:",0
unmatched_poses.append(row),0
unmatched_trackers.append(col),0
else:,0
"matches.append([row, col])",0
if not len(matches):,0
"matches = np.empty((0, 2), dtype=int)",0
else:,0
matches = np.stack(matches),0
filter out matched with low IOU,0
get predicted locations from existing trackers.,0
update matched trackers with assigned detections,0
create and initialise new trackers for unmatched detections,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
TODO: get cropping parameters and utilize!,1
openmp_arg = '-fopenmp',0
"if _platform == ""win32"":",0
openmp_arg = '-openmp',0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
assuming batchsize 1 here!,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
setting pairwise bodypart loss,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
defaults will be a public argument for namedtuple in Python 3.7,0
https://docs.python.org/3/library/collections.html#collections.namedtuple,0
Make sure that round down does not go down by more than 10%.,0
Builds the block accordings to arguments.,0
Expansion phase:,0
Depth-wise convolution phase:,0
Squeeze and Excitation layer.,0
Output phase:,0
only apply drop_connect if skip presents.,0
Expansion phase:,0
Output phase:,0
only apply drop_connect if skip presents.,0
Builds blocks.,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Stem part.,0
Head part.,0
Calls Stem layers,0
Calls blocks.,0
Calls final layers and returns logits.,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"(width_coefficient, depth_coefficient, resolution, dropout_rate)",0
blocks_args = [,0
"'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',",0
"'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',",0
"'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',",0
"'r1_k3_s11_e6_i192_o320_se0.25',",0
],0
The default is TPU-specific batch norm.,0
The alternative is tf.layers.BatchNormalization.,0
"batch_norm=utils.TpuBatchNormalization,  # TPU-specific requirement.",0
ValueError will be raised here if override_params has fields not included,0
in global_params.,0
print(end_points.keys()) >> to see what else is available.,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
setting pairwise bodypart loss,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
class TpuBatchNormalization(tf.layers.BatchNormalization):,0
Compute variance using: Var[X]= E[X^2] - E[X]^2.,0
Compute keep_prob,0
TODO(tanmingxing): add support for training progress.,1
Compute drop_connect tensor,0
Clear the old folder.,0
Write checkpoints.,0
Update the best objective.,0
We maintain mva for batch norm moving mean and variance as well.,0
Return the top 5 predictions (idx and prob) for each image.,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
from nets.mobilenet import conv_blocks as ops,0
from nets.mobilenet import mobilenet as lib,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
The next part of the code depends upon which tensorflow version you have.,0
assuming batchsize 1 here!,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
setting pairwise bodypart loss,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
get coordinates for bp1 and bp2,0
'm2',0
"distscalarproduct=np.zeros((len(C1),len(C2)))*np.nan",0
"distscalarproduct[c1i,c2i]=dy*v[1]+dx*v[0] #scalar product [v unit vector dx,dy in pixel coordinats from partaffinitymap]",0
Distances[l]['m2'] = distscalarproduct,0
TODO: compute all the grids etc. once for the video analysis method,1
(and then just pass on the variables),0
apply nms,0
"IMPORTANT, as C++ function expects row-major",0
These two functions are for evaluation specifically (one also calculates integral between gt points),0
get dist_grid,0
apply nms,0
"IMPORTANT, as C++ function expects row-major",0
if 'eval_scale' in cfg.keys():,0
import imgaug.augmenters as iaa,0
im = iaa.Resize(float(cfg['eval_scale']))(images=im),0
# Functions below implement are for batch sizes > 1:,0
get dist_grid,0
apply nms,0
"IMPORTANT, as C++ function expects row-major",0
"batchsize,ny,nx,num_joints = scmap.shape",0
Change the stride from 2 to 1 to get 16x downscaling instead of 32x.,0
https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142,0
The next part of the code depends upon which tensorflow version you have.,0
"return self.prediction_layers(net, end_points)",0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Augmentation functions,0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"First, initializing variables (if they don't exist)",0
what is the fraction of training samples with scaling augmentation?,0
loading defaults for rotation range!,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"cfg[""rotateratio""] = cfg.get(",0
"""rotratio"", 0.4",0
)  # what is the fraction of training samples with rotation augmentation?,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"Auto cropping is new (was not in Nature Neuroscience 2018 paper, but introduced in Nath et al. Nat. Protocols 2019)",0
"and boosts performance by 2X, particularly on challenging datasets, like the cheetah in Nath et al.",0
Parameters for augmentation with regard to cropping:,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
Pickle the 'data' dictionary using the highest protocol available.,0
item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal,0
Scale is sampled only once (per batch) to transform all of the images into same size.,0
assert len(batch_images) == self.batch_size,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
"imageio.imwrite(data_items[i].im_path.split('/')[-1],im)",0
if returndata:,0
"return batch_images,batch_joints,targetmaps",0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
"print(person_id,joint_ids,data_item.im_path.split('/')[-1])",0
if self.cfg.partaffinityfield_predict:,0
"print(""hello"",joint_id)",0
print(np.concatenate(joint_id)) #this is all joint_ids for all individuals!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
"plt.savefig(""along""+str(data_item.im_path.split('/')[-1]))",0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
"print(person_id,k, j_id, coords[0])",0
assert(0==1),0
"A[:,:,2]=partaffinityfield_map[:,:,j]*0",0
"A[:,:,2]=partaffinityfield_map[:,:,j]*0",0
"plt.savefig(""SomePafs""+str(data_item.im_path.split('/')[-1]))",0
WIP!,0
STD of gaussian is 1/4 of threshold,0
Grid of coordinates,0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Distance between the joint point and each coordinate,0
NEEDS TO BE DONE!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
parameter initialization for augmentation pipeline:,0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images of a batch into same size.,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
"from scipy.misc import imread, imresize",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
Get rid of the interpolation beyond the spline knots,0
Retrieve original individual label indices,0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
"net_type = ""efficientnet-b0""  # to -b6",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
TENSORPACK could fail in WINDOWS...,0
"NET = ""efficientnet-b0""",0
"""multi_step"": [[0.001, N_ITER]],",0
Need to add the 'likelihood' level value to simulate analyzed data,0
Ugliest hack in the history of pandas,1
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
# Create one split and make Shuffle 2 and 3 have the same split.,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
for EfficientNet,0
imgaug,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
# Create one split and make Shuffle 2 and 3 have the same split.,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Dowloading the calibration images,0
Deleting unneccesary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
example how to set pose config variables:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
Verify the aspect ratio is preserved,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
quicker variant,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
Suppress tensorflow warning messages,0
DLClight version does not support GUIs. Importing accordingly,0
"Train, evaluate & predict functions / all require TF",0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Updating config file:,0
Create the pose_config.yaml files,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path, num_shuffles=auxfun_models.Check4weights(pose_cfg['net_type'], parent_path, num_shuffles= 1)",0
Updating training and test pose_cfg:,0
Read the config file,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
common parameters:,0
Write dictionary to yaml  config file,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
Flatten the list of bodyparts to connect,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Start plotting for every frame,0
"Once all the frames are saved, then make a movie using ffmpeg.",0
Create the fig and define the axes,0
Clear plot and initialize the variables,0
Initialize arrays for appending the 3d data for actual plotting the points,0
Initialize arrays for appending the 3d data for drawing the lines,0
Initialize arrays for appending the 2d data from cam1 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam2 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam1 for drawing the lines,0
Initialize arrays for appending the 2d data from cam2 for drawing the lines,0
Get the scorer names from the dataframe,0
"Set the x,y, and z limits for the 3d view",0
"Set the frame number to read#max(0,index-trailpoints):index",0
Plot the labels for each body part,0
Connecting the bodyparts specified in the config file.3d file is created based on the likelihoods of cam1 and cam2. Using 3d file and check if the body part is nan then dont plot skeleton,0
Saving the frames,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
Extract the indices of frames where the likelihood of a bodypart for both cameras are less than pvalue,0
Extract frames where likelihood for both the views is less than the pcutoff,0
"low_likelihood_frames = np.all(likelihoods < pcutoff, axis=1)",0
For cam1 camera: Assign nans to x and y values of a bodypart where the likelihood for is less than pvalue,0
For cam2 camera: Assign nans to x and y values of a bodypart where the likelihood is less than pvalue,0
ToDo: speed up func. below by saving in numpy.array,1
"Create an empty dataframe to store x,y,z of 3d data",0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Undistorting the points from cam2 camera,0
Save the undistorted files,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = io.imread(img),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
topSplitter = wx.SplitterWindow(self),0
,0
"self.image_panel = ImagePanel(topSplitter, config,video,shuffle,Dataframe,self.gui_size)",0
self.widget_panel = WidgetPanel(topSplitter),0
,0
"topSplitter.SplitHorizontally(self.image_panel, self.widget_panel,sashPosition=self.gui_size[1]*0.83)#0.9",0
topSplitter.SetSashGravity(1),0
sizer = wx.BoxSizer(wx.VERTICAL),0
"sizer.Add(topSplitter, 1, wx.EXPAND)",0
self.SetSizer(sizer),0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
"cv2.imwrite(img_name, frame)",0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.buttonCounter = [],0
self.bodyparts = [],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
Checks if zoom/pan button is ON,0
Alternatively,0
Sort tracklets by length to prioritize greater continuity,0
Drop tracklets that are too short,0
"Where slots are available, copy the data over",0
"If about to overwrite data, keep tracklets with highest confidence",0
Squeeze some data into empty slots,0
"For the remaining data, overwrite where we are least confident",0
Find keypoints closest to the remaining data,0
Use Manhattan distance to avoid overflow,0
Only overwrite if improving confidence,0
Fill existing gaps,0
Retrieve original individual label indices,0
Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.,0
ID swaps occur when X and Y simultaneously intersect each other.,0
Get only those bodypart pairs that belong to different individuals,0
Take into consideration imprecise OpenCV estimation of total number of frames,0
Automatically disable the draggable points,0
Save additional frames to the labeled-data directory,0
Store the newly-refined data,0
Uncertain keypoints are ignored,0
Merge with the already existing annotated data,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
"See Li et al., 2012. Cross-view Activity Recognition using Hankelets.",0
"As proposed in the paper, the Hankel matrix can either be formed from",0
the tracklet's centroid or its normalized velocity.,0
"vel = np.diff(self.centroid, axis=0)",0
"vel /= np.linalg.norm(vel, axis=1, keepdims=True)",0
return self.hankelize(vel),0
TODO Add missing data imputation,1
"nrows, ncols = mat.shape",0
beta = nrows / ncols,0
omega = 0.56 * beta ** 3 - 0.95 * beta ** 2 + 1.82 * beta + 1.43,0
return np.argmin(s > omega * np.median(s)),0
"Note that if tracklets are very short, some may actually be part of the same track",0
and thus incorrectly reflect separate track endpoints...,0
"Map each Tracklet to an entry and output nodes and vice versa,",0
which is convenient once the tracklets are stitched.,0
TODO Avoid looping over all pairs of tracklets,0
The algorithm works better with integer weights,0
Let us prune the graph by removing all source and sink edges,0
but those connecting the `n_tracks` first and last tracklets.,0
Preflow push seems to work slightly better than shortest,0
"augmentation path..., and is more computationally efficient.",0
Verify whether there are overlapping tracklets,0
"Pick the segment that minimizes ""smoothness"", computed here",0
with the coefficient of variation of the differences.,0
Cycle through the residuals and incorporate back those,0
that only fit in a single tracklet.,0
Greedily add the remaining residuals,0
Refresh temporal bounds,0
Default to the distance cost function,0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
average distance and average # significant differences avg. over comparisonbodyparts,0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
"self.choiceBox,self.visualization_rdb = self.choice_panel.addRadioButtons()",0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
print(self.Dataframe.head()),0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
fix move to corner,0
fix move to corner,0
fix move to corner,0
fix move to corner,0
Checks if zoom/pan button is ON,0
"print(""DLC loaded in light mode; you cannot use the relabeling GUI!"")",0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
###################################################,0
Dependencies,0
###################################################,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as,0
specified in the config file,0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
Prepare figure,0
Mask rather than drop unwanted bodyparts to ensure consistent coloring,0
cropping!,0
nx=X2-X1,0
!/usr/bin/env python3,0
The above already raises a ValueError if formatting is wrong,0
"Rotate, see: https://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg",0
interesting option to just update metadata.,0
"Historically DLC used: from scipy.misc import imread, imresize >> deprecated functions",0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
Read the video until a frame is successfully read,0
import platform,0
"if platform.system() == ""Darwin"":  # for OSX use WXAgg",0
fig.canvas.start_event_loop(timeout=-1),0
else:,0
#fig.canvas.stop_event_loop(),0
"Windows throws error if file path is > 260 characters, can fix with prefix.",0
See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation,0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
"Attention this order has to be consistent (for training set creation, training, inference etc.)",0
CHECKS if each bpt is connected to at least one other bpt,0
TODO: check that there is a path leading from each (multi)bpt to each other (multi)bpt!,1
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
if suffix=='_full': #save metadata!,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"-> adding (single,bpt) for uniquebodyparts",0
"-> adding (indivdual,bpt) for multianimalbodyparts",0
load defaults,0
set project specific parameters:,0
###################################################,0
Dependencies,0
###################################################,0
Pose X vs pose Y,0
Poses vs time,0
Likelihoods,0
Histograms,0
#################################################,0
Looping analysis over video,0
#################################################,0
Keep only the individuals and bodyparts that were labeled,0
Determine whether the data are single- or multi-animal without loading into memory,0
simply by checking whether 'individuals' is in the second line of the CSV.,0
Is there a scorer for this?,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
Ensure same order as in config.yaml,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
Iterate over data files and stop as soon as one matching the scorer is found,0
If there was no match...,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
self.cfg = auxiliaryfunctions.read_config(config),0
"self.statusbar.SetStatusText("""")",0
"dlg = wx.MessageDialog(None,""Are you sure?"", ""Quit!"",wx.YES_NO | wx.ICON_WARNING)",0
result = dlg.ShowModal(),0
if result == wx.ID_YES:,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
NEW ROW:,0
self.save.Enable(False),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0,10), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=10)",0
Add all the options,0
line = wx.StaticLine(self),0
"self.sizer.Add(line, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=10)",0
,0
"self.multi_choice.Bind(wx.EVT_CHECKBOX,self.activate_copy_videos)",0
Hide the button as this is not the default option,0
self.vids.Enable(False),0
self.SetSizer(self.sizer),0
"self.sizer.Add(self.sizer, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=15)",0
self.ok.Enable(False),0
else:,0
self.ok.Enable(True),0
For mac compatibility,0
Remove the pages in case the user goes back to the create new project and creates/load a new project,0
Add all the other pages,0
self.yes.Enable(False),0
self.no.Enable(False),0
Here we create a panel and a notebook on the panel,0
create the page windows as children of the notebook and add the pages to the notebook with the label to show on the tab,0
variable initilization,0
design the panel,0
Add logo of DLC,0
self.filelist = self.filelist + self.vids,0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
use the default pose_cfg file for default values,0
self.display_iters.Enable(False),0
self.save_iters.Enable(False),0
self.max_iters.Enable(False),0
self.snapshots.Enable(False),0
Read the pose config file,0
"print(os.path.join(cfg['project_path'],auxiliaryfunctions.GetModelFolder(trainFraction, self.shuffles.GetValue(),cfg),'train','pose_cfg.yaml'))",0
let the user open the file with default text editor. Also make it mac compatible,0
update the variables with the edited values in the pose config file,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"if self.cfg.get(""multianimalproject"", False):",0
pass,0
else:,0
removing this as several downstream maDLC steps don't support dest_folder at this time:,0
"destfolder_text = wx.StaticBox(self, label=""Specify destination folder"")",0
"destfolderboxsizer = wx.StaticBoxSizer(destfolder_text, wx.VERTICAL)",0
self.change_workingdir = wx.CheckBox(,0
"self, label=""optional destination folder""",0
),0
self.hbox2.Add(self.change_workingdir),0
"self.change_workingdir.Bind(wx.EVT_CHECKBOX, self.activate_change_wd)",0
"self.sel_wd = wx.Button(self, label=""Browse"")",0
self.sel_wd.Enable(False),0
"self.sel_wd.Bind(wx.EVT_BUTTON, self.select_destfolder)",0
"self.hbox2.Add(self.sel_wd, 0, wx.ALL, -1)",0
Read the infer config file,0
let the user open the file with default text editor. Also make it mac compatible,0
"def video_tracklets(self,event):",0
shuffle = self.shuffle.GetValue(),0
trainingsetindex = self.trainingset.GetValue(),0
"deeplabcut.create_video_from_pickled_tracks(self.filelist, picklefile, pcutoff=0.6)",0
"self.select_destfolder.SetPath(""None"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.shuffles.SetValue(""1"")",0
"DownSampleVideo(vname,width=-1,height=200,outsuffix='downsampled',outpath=None,rotateccw=False):",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.videotype.SetStringSelection("".avi"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"if self.plot_scoremaps.GetStringSelection() == ""Yes"":",0
Read the infer config file,0
let the user open the file with default text editor. Also make it mac compatible,0
shuffle = self.shuffle.GetValue(),0
Read from edited inf. file first ...,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0, 4), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=5)",0
variable initilization,0
design the panel,0
Add logo of DLC,0
#         design the panel,0
Add image of DLC,0
if editing this text make sure you add the '\n' to get the new line. The sizer is unable to format lines correctly.,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
convert the image to RGB as you are showing the image with matplotlib,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Checks if zoom/pan button is ON,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = cv2.imread(img),0
convert the image to RGB as you are showing the image with matplotlib,0
divider = make_axes_locatable(self.axes),0
"colorIndex = np.linspace(np.min(im),np.max(im),len(bodyparts))",0
"cax = divider.append_axes(""right"", size=""5%"", pad=0.05)",0
"cbar = self.figure.colorbar(ax, cax=cax,spacing='proportional', ticks=colorIndex)",0
cbar.set_ticklabels(bodyparts[::-1]),0
self.figure.canvas.draw(),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"MainFrame.saveDataSet(self, event)",0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
checks for unique bodyparts,0
"Reading the existing dataset,if already present",0
Handle data previously labeled on a different platform,0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Create an empty data frame,0
Cache original bodyparts,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Rearrange bodypart columns in config order,0
Test whether there are missing frames and superfluous data,0
Check whether new labels were added,0
Checking if user added a new label,0
Found new labels in either multiple bodyparts or unique bodyparts,0
check if single is slected when radio buttons are changed,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Discard data associated with bodyparts that are no longer in the config,0
Re-organize the dataframe so the CSV looks consistent with the config,0
Checks if zoom/pan button is ON,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
actualbpts=set(Data.columns.get_level_values(0)),0
utility function to split different crops from same image into either train or test!,0
loading & linking pretrained models,0
CURRENTLY ONLY ResNet supported!,0
multianimal case:,0
"ATTENTION: order has to be multibodyparts, then uniquebodyparts (for indexing)",0
Loading the encoder (if necessary downloading from TF),0
Map back to the original indices.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
load image to get dimensions:,0
"print ""Grayscale!""",0
if socialbdpt in actualbpts:,0
if socialbdpt in actualbpts:,0
Drop missing body parts,0
Drop points lying outside the image,0
###############################################################################,0
Saving metadata and data file (Pickle file),0
###############################################################################,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Setting inference cfg file:,0
Load updated lists:,0
Find the corresponding video file,0
Avoid cropping already cropped images,0
"moving old entry to _original, dropping it from video_set and update crop parameters",0
"print(key, ""dropping?"")",0
adding important values for multianiaml project:,0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Let's check if the code is *not* run on windows (Source: #https://stackoverflow.com/questions/1325581/how-do-i-check-if-im-running-on-windows-in-python),0
but the paths are in windows format...,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(index,name.split(os.sep)[1])",0
"print(name,test_video_name)",0
Check that points lie within the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
updating variable if null/None! #backwardscompatability,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
these values are dropped as scalecrop,0
doesn't have rotation implemented,0
sort the models directories,0
get the shuffle index,0
read cfg file,0
create log file,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
"text = str(self.individual_names+','+self.bodyParts)",0
Check for variable correctness,0
Moviepy:,0
"print(""DLC loaded in light mode; you cannot use the labeling GUI!"")",0
"def __init__(self, point,bodyParts):",0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
text = str(self.bodyParts),0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
"sess, inputs, outputs = predict.setup_pose_prediction(dlc_cfg)",0
#################################################,0
Looping over videos,0
#################################################,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
from skimage.io import imread,0
"frame=imread(os.path.join(directory,framename),mode='RGB')",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
TODO: addd cropping as in video analysis!,1
if cropping is not None:,0
cfg['cropping']=True,0
"cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping",0
"print(""Overwriting cropping parameters:"", cropping)",0
"print(""These are used for all videos, but won't be save to the cfg file."")",0
Check which snapshots are available and sort them by # iterations,0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
"NOTE: this can be used if only a subset is relevant. I.e. [0,1] for only first and second joint!",0
TODO: adjust this for multi + unique bodyparts!,1
this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. ),0
get corresponding bounding boxes!,0
Test whether the unique bodyparts have been assembled,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"print(""If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract any outlier frames!"")",0
imagenet mean for resnet pretraining:,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/AlexEMG/DeepLabCut/pull/324",0
for DLC 2.2. (here all set False to not use PAFs/pairwise fields),0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
"reloading defaults, as they can bleed over from a previous run otherwise",0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
dlc_cfg_train = load_config(str(path_train_config)),0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
Name for deeplabcut net (based on its parameters),0
"DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])",0
"print(""Extracting maps for "", DLCscorer, "" with # of trainingiterations:"", trainingsiterations)",0
if notanalyzed: #this only applies to ask if h5 exists...,0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
Dependencies for anaysis,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Load meta data & annotations,0
get the graph!,0
"NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*",0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
TODO: Make this code not so redundant!,1
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
#################################################,0
Load data...,0
#################################################,0
Loading human annotatated data,0
Handle data previously annotated on a different platform,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
TODO: IMPLEMENT for different batch sizes?,1
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
print(dlc_cfg),0
Specifying state of model (snapshot / training state),0
Evaluate PAF edge lengths to calibrate `distnorm`,0
FIXME Is having an empty array vs nan really that necessary?!,1
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in,0
Compute all distance statistics,0
Calculate overall prediction error,0
"For OKS/PCK, compute the standard deviation error across all frames",0
returning to intial folder,0
wild guesses for a wide range:,0
Check which snapshots are available and sort them by # iterations,0
Pick distance threshold for (r)PCK from the statistics computed during evaluation,0
update number of individuals to retain.,0
calculating result at best best solution,0
"print(""Quantification:"", DataOptParams.head())",0
DataOptParams.to_hdf(,0
"path_inference_config.split("".yaml"")[0] + "".h5"",",0
"""df_with_missing"",",0
"format=""table"",",0
"mode=""w"",",0
),0
Store best predictions,0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
"[_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})",0
Save snapshot,0
return to original path.,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
from deeplabcut.pose_estimation_tensorflow.predict_multianimal import convert_detections2tracklet,0
initializing constants,0
"PredicteData['frame'+str(counter)]=predict.get_detectionswithcosts(frame, dlc_cfg, sess, inputs, outputs, outall=False,nms_radius=dlc_cfg.nmsradius,det_min_score=dlc_cfg.minconfidence)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
"PredicteData[batch_num*batchsize:batch_num*batchsize+batch_ind, :] = pose[:batch_ind,:]",0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
animal & bpt-wise distance!,0
"corrkps=np.sum((gt[row_indices]-ani[col_indices])**2,axis=2)<dcorr**2",0
animal & bpt-wise distance!,0
"corrkps=np.sum((gt[row_indices]-ani[col_indices])**2,axis=2)<dcorr**2",0
load params,0
Ensure type consistency,0
"stats = compute_crossval_metrics(config_path, inferencecfg, shuffle,trainingsetindex,",0
"dcorr=dcorr,leastbpts=leastbpts,modelprefix=modelprefix)",0
val = stats['rmse_test'].values[0]*(1+stats['misses_test'].values[0]*1./stats['hits_test'].values[0]),0
##################################,0
### auxiliaryfunctions,0
##################################,0
#########################################################,0
### conversion & greedy bodypart matching code,0
#########################################################,0
"d=distance(np.array(cand_a[i][:2]),np.array(cand_b[j][:2]))",0
filtering with global distance bounds,0
sort candidate connections by score,0
"if both bodyparts don't exist, create a new subset",0
filter detections according to inferencecfg parameters,0
filter connections according to inferencecfg parameters,0
assemble putative subsets,0
define constant velocity model,0
TODO Try particle filter (since we already have the keypoints),1
Modified from scipy source code:,0
- to restrict its use to 2D,0
- to get rid of shuffling (since arrays are only (nbodyparts * 3) element long),0
TODO - factor in keypoint confidence (and weight by # of observations??),1
"mat = self.calc_pairwise_oks(poses, poses_ref)",0
Remove matched detections with low OKS,0
matches = [],0
"for row, col in zip(row_indices, col_indices):",0
"if mat[row, col] < self.oks_threshold:",0
unmatched_poses.append(row),0
unmatched_trackers.append(col),0
else:,0
"matches.append([row, col])",0
if not len(matches):,0
"matches = np.empty((0, 2), dtype=int)",0
else:,0
matches = np.stack(matches),0
filter out matched with low IOU,0
get predicted locations from existing trackers.,0
update matched trackers with assigned detections,0
create and initialise new trackers for unmatched detections,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
openmp_arg = '-fopenmp',0
"if _platform == ""win32"":",0
openmp_arg = '-openmp',0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
assuming batchsize 1 here!,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
setting pairwise bodypart loss,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
defaults will be a public argument for namedtuple in Python 3.7,0
https://docs.python.org/3/library/collections.html#collections.namedtuple,0
Make sure that round down does not go down by more than 10%.,0
Builds the block accordings to arguments.,0
Expansion phase:,0
Depth-wise convolution phase:,0
Squeeze and Excitation layer.,0
Output phase:,0
only apply drop_connect if skip presents.,0
Expansion phase:,0
Output phase:,0
only apply drop_connect if skip presents.,0
Builds blocks.,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Stem part.,0
Head part.,0
Calls Stem layers,0
Calls blocks.,0
Calls final layers and returns logits.,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"(width_coefficient, depth_coefficient, resolution, dropout_rate)",0
blocks_args = [,0
"'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',",0
"'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',",0
"'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',",0
"'r1_k3_s11_e6_i192_o320_se0.25',",0
],0
The default is TPU-specific batch norm.,0
The alternative is tf.layers.BatchNormalization.,0
"batch_norm=utils.TpuBatchNormalization,  # TPU-specific requirement.",0
ValueError will be raised here if override_params has fields not included,0
in global_params.,0
print(end_points.keys()) >> to see what else is available.,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
setting pairwise bodypart loss,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
class TpuBatchNormalization(tf.layers.BatchNormalization):,0
Compute variance using: Var[X]= E[X^2] - E[X]^2.,0
Compute keep_prob,0
TODO(tanmingxing): add support for training progress.,1
Compute drop_connect tensor,0
Clear the old folder.,0
Write checkpoints.,0
Update the best objective.,0
We maintain mva for batch norm moving mean and variance as well.,0
Return the top 5 predictions (idx and prob) for each image.,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
from nets.mobilenet import conv_blocks as ops,0
from nets.mobilenet import mobilenet as lib,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
The next part of the code depends upon which tensorflow version you have.,0
assuming batchsize 1 here!,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
setting pairwise bodypart loss,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
get coordinates for bp1 and bp2,0
'm2',0
"distscalarproduct=np.zeros((len(C1),len(C2)))*np.nan",0
"distscalarproduct[c1i,c2i]=dy*v[1]+dx*v[0] #scalar product [v unit vector dx,dy in pixel coordinats from partaffinitymap]",0
Distances[l]['m2'] = distscalarproduct,0
TODO: compute all the grids etc. once for the video analysis method,1
(and then just pass on the variables),0
apply nms,0
"IMPORTANT, as C++ function expects row-major",0
These two functions are for evaluation specifically (one also calculates integral between gt points),0
get dist_grid,0
apply nms,0
"IMPORTANT, as C++ function expects row-major",0
if 'eval_scale' in cfg.keys():,0
import imgaug.augmenters as iaa,0
im = iaa.Resize(float(cfg['eval_scale']))(images=im),0
# Functions below implement are for batch sizes > 1:,0
get dist_grid,0
apply nms,0
"IMPORTANT, as C++ function expects row-major",0
"batchsize,ny,nx,num_joints = scmap.shape",0
Change the stride from 2 to 1 to get 16x downscaling instead of 32x.,0
https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142,0
The next part of the code depends upon which tensorflow version you have.,0
"return self.prediction_layers(net, end_points)",0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Augmentation functions,0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"First, initializing variables (if they don't exist)",0
what is the fraction of training samples with scaling augmentation?,0
loading defaults for rotation range!,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"cfg[""rotateratio""] = cfg.get(",0
"""rotratio"", 0.4",0
)  # what is the fraction of training samples with rotation augmentation?,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"Auto cropping is new (was not in Nature Neuroscience 2018 paper, but introduced in Nath et al. Nat. Protocols 2019)",0
"and boosts performance by 2X, particularly on challenging datasets, like the cheetah in Nath et al.",0
Parameters for augmentation with regard to cropping:,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
Pickle the 'data' dictionary using the highest protocol available.,0
item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal,0
Scale is sampled only once (per batch) to transform all of the images into same size.,0
assert len(batch_images) == self.batch_size,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
"imageio.imwrite(data_items[i].im_path.split('/')[-1],im)",0
if returndata:,0
"return batch_images,batch_joints,targetmaps",0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
"print(person_id,joint_ids,data_item.im_path.split('/')[-1])",0
if self.cfg.partaffinityfield_predict:,0
"print(""hello"",joint_id)",0
print(np.concatenate(joint_id)) #this is all joint_ids for all individuals!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
"plt.savefig(""along""+str(data_item.im_path.split('/')[-1]))",0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
"print(person_id,k, j_id, coords[0])",0
assert(0==1),0
"A[:,:,2]=partaffinityfield_map[:,:,j]*0",0
"A[:,:,2]=partaffinityfield_map[:,:,j]*0",0
"plt.savefig(""SomePafs""+str(data_item.im_path.split('/')[-1]))",0
WIP!,0
STD of gaussian is 1/4 of threshold,0
Grid of coordinates,0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Distance between the joint point and each coordinate,0
NEEDS TO BE DONE!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
parameter initialization for augmentation pipeline:,0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images of a batch into same size.,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
"from scipy.misc import imread, imresize",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
Get rid of the interpolation beyond the spline knots,0
Retrieve original individual label indices,0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
"net_type = ""mobilenet_v2_0.35""",0
"net_type = ""efficientnet-b0""  # to -b6",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
TENSORPACK could fail in WINDOWS...,0
"NET = ""efficientnet-b0""",0
"""multi_step"": [[0.001, N_ITER]],",0
Need to add the 'likelihood' level value to simulate analyzed data,0
Ugliest hack in the history of pandas,1
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
# Create one split and make Shuffle 2 and 3 have the same split.,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
for EfficientNet,0
imgaug,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
# Create one split and make Shuffle 2 and 3 have the same split.,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Dowloading the calibration images,0
Deleting unneccesary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
example how to set pose config variables:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
Verify the aspect ratio is preserved,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
quicker variant,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
Suppress tensorflow warning messages,0
DLClight version does not support GUIs. Importing accordingly,0
"Train, evaluate & predict functions / all require TF",0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Updating config file:,0
Create the pose_config.yaml files,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path, num_shuffles=auxfun_models.Check4weights(pose_cfg['net_type'], parent_path, num_shuffles= 1)",0
Updating training and test pose_cfg:,0
Read the config file,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
common parameters:,0
Write dictionary to yaml  config file,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
Flatten the list of bodyparts to connect,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Start plotting for every frame,0
"Once all the frames are saved, then make a movie using ffmpeg.",0
Create the fig and define the axes,0
Clear plot and initialize the variables,0
Initialize arrays for appending the 3d data for actual plotting the points,0
Initialize arrays for appending the 3d data for drawing the lines,0
Initialize arrays for appending the 2d data from cam1 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam2 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam1 for drawing the lines,0
Initialize arrays for appending the 2d data from cam2 for drawing the lines,0
Get the scorer names from the dataframe,0
"Set the x,y, and z limits for the 3d view",0
"Set the frame number to read#max(0,index-trailpoints):index",0
Plot the labels for each body part,0
Connecting the bodyparts specified in the config file.3d file is created based on the likelihoods of cam1 and cam2. Using 3d file and check if the body part is nan then dont plot skeleton,0
Saving the frames,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
Extract the indices of frames where the likelihood of a bodypart for both cameras are less than pvalue,0
Extract frames where likelihood for both the views is less than the pcutoff,0
"low_likelihood_frames = np.all(likelihoods < pcutoff, axis=1)",0
For cam1 camera: Assign nans to x and y values of a bodypart where the likelihood for is less than pvalue,0
For cam2 camera: Assign nans to x and y values of a bodypart where the likelihood is less than pvalue,0
ToDo: speed up func. below by saving in numpy.array,1
"Create an empty dataframe to store x,y,z of 3d data",0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Undistorting the points from cam2 camera,0
Save the undistorted files,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = io.imread(img),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
topSplitter = wx.SplitterWindow(self),0
,0
"self.image_panel = ImagePanel(topSplitter, config,video,shuffle,Dataframe,self.gui_size)",0
self.widget_panel = WidgetPanel(topSplitter),0
,0
"topSplitter.SplitHorizontally(self.image_panel, self.widget_panel,sashPosition=self.gui_size[1]*0.83)#0.9",0
topSplitter.SetSashGravity(1),0
sizer = wx.BoxSizer(wx.VERTICAL),0
"sizer.Add(topSplitter, 1, wx.EXPAND)",0
self.SetSizer(sizer),0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
"cv2.imwrite(img_name, frame)",0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.buttonCounter = [],0
self.bodyparts = [],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
Checks if zoom/pan button is ON,0
Alternatively,0
Sort tracklets by length to prioritize greater continuity,0
Drop tracklets that are too short,0
"Where slots are available, copy the data over",0
"If about to overwrite data, keep tracklets with highest confidence",0
Squeeze some data into empty slots,0
"For the remaining data, overwrite where we are least confident",0
Find keypoints closest to the remaining data,0
Use Manhattan distance to avoid overflow,0
Only overwrite if improving confidence,0
Fill existing gaps,0
Retrieve original individual label indices,0
Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.,0
ID swaps occur when X and Y simultaneously intersect each other.,0
Get only those bodypart pairs that belong to different individuals,0
Take into consideration imprecise OpenCV estimation of total number of frames,0
Automatically disable the draggable points,0
Save additional frames to the labeled-data directory,0
Store the newly-refined data,0
Uncertain keypoints are ignored,0
Merge with the already existing annotated data,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
average distance and average # significant differences avg. over comparisonbodyparts,0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
"self.choiceBox,self.visualization_rdb = self.choice_panel.addRadioButtons()",0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
print(self.Dataframe.head()),0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
fix move to corner,0
fix move to corner,0
fix move to corner,0
fix move to corner,0
Checks if zoom/pan button is ON,0
"print(""DLC loaded in light mode; you cannot use the relabeling GUI!"")",0
Find uncropped labeled data,0
Handle image previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
###################################################,0
Dependencies,0
###################################################,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as,0
specified in the config file,0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
Prepare figure,0
Mask rather than drop unwanted bodyparts to ensure consistent coloring,0
cropping!,0
nx=X2-X1,0
!/usr/bin/env python3,0
The above already raises a ValueError if formatting is wrong,0
"Rotate, see: https://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg",0
interesting option to just update metadata.,0
"Historically DLC used: from scipy.misc import imread, imresize >> deprecated functions",0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
Read the video until a frame is successfully read,0
import platform,0
"if platform.system() == ""Darwin"":  # for OSX use WXAgg",0
fig.canvas.start_event_loop(timeout=-1),0
else:,0
#fig.canvas.stop_event_loop(),0
"Windows throws error if file path is > 260 characters, can fix with prefix.",0
See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation,0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
"Attention this order has to be consistent (for training set creation, training, inference etc.)",0
CHECKS if each bpt is connected to at least one other bpt,0
TODO: check that there is a path leading from each (multi)bpt to each other (multi)bpt!,1
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
if suffix=='_full': #save metadata!,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"-> adding (single,bpt) for uniquebodyparts",0
"-> adding (indivdual,bpt) for multianimalbodyparts",0
load defaults,0
set project specific parameters:,0
###################################################,0
Dependencies,0
###################################################,0
Pose X vs pose Y,0
Poses vs time,0
Likelihoods,0
Histograms,0
#################################################,0
Looping analysis over video,0
#################################################,0
Keep only the individuals and bodyparts that were labeled,0
Determine whether the data are single- or multi-animal without loading into memory,0
simply by checking whether 'individuals' is in the second line of the CSV.,0
Is there a scorer for this?,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
Ensure same order as in config.yaml,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
Iterate over data files and stop as soon as one matching the scorer is found,0
If there was no match...,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
self.cfg = auxiliaryfunctions.read_config(config),0
"self.statusbar.SetStatusText("""")",0
"dlg = wx.MessageDialog(None,""Are you sure?"", ""Quit!"",wx.YES_NO | wx.ICON_WARNING)",0
result = dlg.ShowModal(),0
if result == wx.ID_YES:,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
NEW ROW:,0
self.save.Enable(False),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0,10), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=10)",0
Add all the options,0
line = wx.StaticLine(self),0
"self.sizer.Add(line, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=10)",0
,0
"self.multi_choice.Bind(wx.EVT_CHECKBOX,self.activate_copy_videos)",0
Hide the button as this is not the default option,0
self.vids.Enable(False),0
self.SetSizer(self.sizer),0
"self.sizer.Add(self.sizer, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=15)",0
self.ok.Enable(False),0
else:,0
self.ok.Enable(True),0
For mac compatibility,0
Remove the pages in case the user goes back to the create new project and creates/load a new project,0
Add all the other pages,0
self.yes.Enable(False),0
self.no.Enable(False),0
Here we create a panel and a notebook on the panel,0
create the page windows as children of the notebook and add the pages to the notebook with the label to show on the tab,0
variable initilization,0
design the panel,0
Add logo of DLC,0
self.filelist = self.filelist + self.vids,0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
use the default pose_cfg file for default values,0
self.display_iters.Enable(False),0
self.save_iters.Enable(False),0
self.max_iters.Enable(False),0
self.snapshots.Enable(False),0
Read the pose config file,0
"print(os.path.join(cfg['project_path'],auxiliaryfunctions.GetModelFolder(trainFraction, self.shuffles.GetValue(),cfg),'train','pose_cfg.yaml'))",0
let the user open the file with default text editor. Also make it mac compatible,0
update the variables with the edited values in the pose config file,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"if self.cfg.get(""multianimalproject"", False):",0
pass,0
else:,0
removing this as several downstream maDLC steps don't support dest_folder at this time:,0
"destfolder_text = wx.StaticBox(self, label=""Specify destination folder"")",0
"destfolderboxsizer = wx.StaticBoxSizer(destfolder_text, wx.VERTICAL)",0
self.change_workingdir = wx.CheckBox(,0
"self, label=""optional destination folder""",0
),0
self.hbox2.Add(self.change_workingdir),0
"self.change_workingdir.Bind(wx.EVT_CHECKBOX, self.activate_change_wd)",0
"self.sel_wd = wx.Button(self, label=""Browse"")",0
self.sel_wd.Enable(False),0
"self.sel_wd.Bind(wx.EVT_BUTTON, self.select_destfolder)",0
"self.hbox2.Add(self.sel_wd, 0, wx.ALL, -1)",0
Read the infer config file,0
let the user open the file with default text editor. Also make it mac compatible,0
"def video_tracklets(self,event):",0
shuffle = self.shuffle.GetValue(),0
trainingsetindex = self.trainingset.GetValue(),0
"deeplabcut.create_video_from_pickled_tracks(self.filelist, picklefile, pcutoff=0.6)",0
"self.select_destfolder.SetPath(""None"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.shuffles.SetValue(""1"")",0
"DownSampleVideo(vname,width=-1,height=200,outsuffix='downsampled',outpath=None,rotateccw=False):",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.videotype.SetStringSelection("".avi"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"if self.plot_scoremaps.GetStringSelection() == ""Yes"":",0
Read the infer config file,0
let the user open the file with default text editor. Also make it mac compatible,0
shuffle = self.shuffle.GetValue(),0
Read from edited inf. file first ...,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0, 4), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=5)",0
variable initilization,0
design the panel,0
Add logo of DLC,0
#         design the panel,0
Add image of DLC,0
if editing this text make sure you add the '\n' to get the new line. The sizer is unable to format lines correctly.,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
convert the image to RGB as you are showing the image with matplotlib,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Reset updated coords,0
Checks for the last image and disables the Next button,0
Backup previous save,0
Drop Nan data frames,0
Windows compatible,0
Checks if zoom/pan button is ON,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = cv2.imread(img),0
convert the image to RGB as you are showing the image with matplotlib,0
divider = make_axes_locatable(self.axes),0
"colorIndex = np.linspace(np.min(im),np.max(im),len(bodyparts))",0
"cax = divider.append_axes(""right"", size=""5%"", pad=0.05)",0
"cbar = self.figure.colorbar(ax, cax=cax,spacing='proportional', ticks=colorIndex)",0
cbar.set_ticklabels(bodyparts[::-1]),0
self.figure.canvas.draw(),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"MainFrame.saveDataSet(self, event)",0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
checks for unique bodyparts,0
"Reading the existing dataset,if already present",0
Handle data previously labeled on a different platform,0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Create an empty data frame,0
Cache original bodyparts,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Rearrange bodypart columns in config order,0
Test whether there are missing frames and superfluous data,0
Check whether new labels were added,0
Checking if user added a new label,0
Found new labels in either multiple bodyparts or unique bodyparts,0
check if single is slected when radio buttons are changed,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Reset updated coords,0
Checks for the last image and disables the Next button,0
Backup previous save,0
Windows compatible,0
Discard data associated with bodyparts that are no longer in the config,0
Re-organize the dataframe so the CSV looks consistent with the config,0
Checks if zoom/pan button is ON,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
actualbpts=set(Data.columns.get_level_values(0)),0
utility function to split different crops from same image into either train or test!,0
loading & linking pretrained models,0
CURRENTLY ONLY ResNet supported!,0
multianimal case:,0
"ATTENTION: order has to be multibodyparts, then uniquebodyparts (for indexing)",0
Loading the encoder (if necessary downloading from TF),0
Map back to the original indices.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
load image to get dimensions:,0
"print ""Grayscale!""",0
if socialbdpt in actualbpts:,0
if socialbdpt in actualbpts:,0
Drop missing body parts,0
Drop points lying outside the image,0
###############################################################################,0
Saving metadata and data file (Pickle file),0
###############################################################################,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Setting inference cfg file:,0
Load updated lists:,0
Find the corresponding video file,0
Avoid cropping already cropped images,0
"moving old entry to _original, dropping it from video_set and update crop parameters",0
"print(key, ""dropping?"")",0
adding important values for multianiaml project:,0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Let's check if the code is *not* run on windows (Source: #https://stackoverflow.com/questions/1325581/how-do-i-check-if-im-running-on-windows-in-python),0
but the paths are in windows format...,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(index,name.split(os.sep)[1])",0
"print(name,test_video_name)",0
Check that points lie within the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
updating variable if null/None! #backwardscompatability,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
these values are dropped as scalecrop,0
doesn't have rotation implemented,0
sort the models directories,0
get the shuffle index,0
read cfg file,0
create log file,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
"text = str(self.individual_names+','+self.bodyParts)",0
Check for variable correctness,0
Moviepy:,0
"print(""DLC loaded in light mode; you cannot use the labeling GUI!"")",0
"def __init__(self, point,bodyParts):",0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
text = str(self.bodyParts),0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
"sess, inputs, outputs = predict.setup_pose_prediction(dlc_cfg)",0
#################################################,0
Looping over videos,0
#################################################,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
from skimage.io import imread,0
"frame=imread(os.path.join(directory,framename),mode='RGB')",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
TODO: addd cropping as in video analysis!,1
if cropping is not None:,0
cfg['cropping']=True,0
"cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping",0
"print(""Overwriting cropping parameters:"", cropping)",0
"print(""These are used for all videos, but won't be save to the cfg file."")",0
Check which snapshots are available and sort them by # iterations,0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
"NOTE: this can be used if only a subset is relevant. I.e. [0,1] for only first and second joint!",0
TODO: adjust this for multi + unique bodyparts!,1
this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. ),0
get corresponding bounding boxes!,0
Test whether the unique bodyparts have been assembled,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"print(""If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract any outlier frames!"")",0
imagenet mean for resnet pretraining:,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/AlexEMG/DeepLabCut/pull/324",0
for DLC 2.2. (here all set False to not use PAFs/pairwise fields),0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
"reloading defaults, as they can bleed over from a previous run otherwise",0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
dlc_cfg_train = load_config(str(path_train_config)),0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
Name for deeplabcut net (based on its parameters),0
"DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])",0
"print(""Extracting maps for "", DLCscorer, "" with # of trainingiterations:"", trainingsiterations)",0
if notanalyzed: #this only applies to ask if h5 exists...,0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
Dependencies for anaysis,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Load meta data & annotations,0
get the graph!,0
"NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*",0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
TODO: Make this code not so redundant!,1
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
#################################################,0
Load data...,0
#################################################,0
Loading human annotatated data,0
Handle data previously annotated on a different platform,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
TODO: IMPLEMENT for different batch sizes?,1
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
print(dlc_cfg),0
Specifying state of model (snapshot / training state),0
Evaluate PAF edge lengths to calibrate `distnorm`,0
FIXME Is having an empty array vs nan really that necessary?!,1
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in,0
Compute all distance statistics,0
Calculate overall prediction error,0
"For OKS/PCK, compute the standard deviation error across all frames",0
returning to intial folder,0
wild guesses for a wide range:,0
Check which snapshots are available and sort them by # iterations,0
Pick distance threshold for (r)PCK from the statistics computed during evaluation,0
update number of individuals to retain.,0
calculating result at best best solution,0
"print(""Quantification:"", DataOptParams.head())",0
DataOptParams.to_hdf(,0
"path_inference_config.split("".yaml"")[0] + "".h5"",",0
"""df_with_missing"",",0
"format=""table"",",0
"mode=""w"",",0
),0
Store best predictions,0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
"[_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})",0
Save snapshot,0
return to original path.,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
from deeplabcut.pose_estimation_tensorflow.predict_multianimal import convert_detections2tracklet,0
initializing constants,0
TODO Fix the code below...,1
We can't just break the whole thing if there is one corrupted frame,0
in the middle of the video. Rather iterate over all frames and simply skip corruptions,0
"PredicteData['frame'+str(counter)]=predict.get_detectionswithcosts(frame, dlc_cfg, sess, inputs, outputs, outall=False,nms_radius=dlc_cfg.nmsradius,det_min_score=dlc_cfg.minconfidence)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"PredicteData[batch_num*batchsize:(batch_num+1)*batchsize, :] = pose",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
"PredicteData[batch_num*batchsize:batch_num*batchsize+batch_ind, :] = pose[:batch_ind,:]",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"PredicteData[batch_num*batchsize:(batch_num+1)*batchsize, :] = pose",0
print(PredicteData),0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
animal & bpt-wise distance!,0
"corrkps=np.sum((gt[row_indices]-ani[col_indices])**2,axis=2)<dcorr**2",0
animal & bpt-wise distance!,0
"corrkps=np.sum((gt[row_indices]-ani[col_indices])**2,axis=2)<dcorr**2",0
load params,0
Ensure type consistency,0
"stats = compute_crossval_metrics(config_path, inferencecfg, shuffle,trainingsetindex,",0
"dcorr=dcorr,leastbpts=leastbpts,modelprefix=modelprefix)",0
val = stats['rmse_test'].values[0]*(1+stats['misses_test'].values[0]*1./stats['hits_test'].values[0]),0
##################################,0
### auxiliaryfunctions,0
##################################,0
#########################################################,0
### conversion & greedy bodypart matching code,0
#########################################################,0
"d=distance(np.array(cand_a[i][:2]),np.array(cand_b[j][:2]))",0
filtering with global distance bounds,0
sort candidate connections by score,0
"if both bodyparts don't exist, create a new subset",0
filter detections according to inferencecfg parameters,0
filter connections according to inferencecfg parameters,0
assemble putative subsets,0
define constant velocity model,0
TODO Try particle filter (since we already have the keypoints),1
Modified from scipy source code:,0
- to restrict its use to 2D,0
- to get rid of shuffling (since arrays are only (nbodyparts * 3) element long),0
TODO - factor in keypoint confidence (and weight by # of observations??),1
"mat = self.calc_pairwise_oks(poses, poses_ref)",0
Remove matched detections with low OKS,0
matches = [],0
"for row, col in zip(row_indices, col_indices):",0
"if mat[row, col] < self.oks_threshold:",0
unmatched_poses.append(row),0
unmatched_trackers.append(col),0
else:,0
"matches.append([row, col])",0
if not len(matches):,0
"matches = np.empty((0, 2), dtype=int)",0
else:,0
matches = np.stack(matches),0
filter out matched with low IOU,0
get predicted locations from existing trackers.,0
update matched trackers with assigned detections,0
create and initialise new trackers for unmatched detections,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
openmp_arg = '-fopenmp',0
"if _platform == ""win32"":",0
openmp_arg = '-openmp',0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
assuming batchsize 1 here!,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
setting pairwise bodypart loss,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
defaults will be a public argument for namedtuple in Python 3.7,0
https://docs.python.org/3/library/collections.html#collections.namedtuple,0
Make sure that round down does not go down by more than 10%.,0
Builds the block accordings to arguments.,0
Expansion phase:,0
Depth-wise convolution phase:,0
Squeeze and Excitation layer.,0
Output phase:,0
only apply drop_connect if skip presents.,0
Expansion phase:,0
Output phase:,0
only apply drop_connect if skip presents.,0
Builds blocks.,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Stem part.,0
Head part.,0
Calls Stem layers,0
Calls blocks.,0
Calls final layers and returns logits.,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"(width_coefficient, depth_coefficient, resolution, dropout_rate)",0
blocks_args = [,0
"'r1_k3_s11_e1_i32_o16_se0.25', 'r2_k3_s22_e6_i16_o24_se0.25',",0
"'r2_k5_s22_e6_i24_o40_se0.25', 'r3_k3_s22_e6_i40_o80_se0.25',",0
"'r3_k5_s11_e6_i80_o112_se0.25', 'r4_k5_s22_e6_i112_o192_se0.25',",0
"'r1_k3_s11_e6_i192_o320_se0.25',",0
],0
The default is TPU-specific batch norm.,0
The alternative is tf.layers.BatchNormalization.,0
"batch_norm=utils.TpuBatchNormalization,  # TPU-specific requirement.",0
ValueError will be raised here if override_params has fields not included,0
in global_params.,0
print(end_points.keys()) >> to see what else is available.,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
setting pairwise bodypart loss,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2019 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
class TpuBatchNormalization(tf.layers.BatchNormalization):,0
Compute variance using: Var[X]= E[X^2] - E[X]^2.,0
Compute keep_prob,0
TODO(tanmingxing): add support for training progress.,1
Compute drop_connect tensor,0
Clear the old folder.,0
Write checkpoints.,0
Update the best objective.,0
We maintain mva for batch norm moving mean and variance as well.,0
Return the top 5 predictions (idx and prob) for each image.,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
from nets.mobilenet import conv_blocks as ops,0
from nets.mobilenet import mobilenet as lib,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
The next part of the code depends upon which tensorflow version you have.,0
assuming batchsize 1 here!,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
setting pairwise bodypart loss,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
get coordinates for bp1 and bp2,0
'm2',0
"distscalarproduct=np.zeros((len(C1),len(C2)))*np.nan",0
"distscalarproduct[c1i,c2i]=dy*v[1]+dx*v[0] #scalar product [v unit vector dx,dy in pixel coordinats from partaffinitymap]",0
Distances[l]['m2'] = distscalarproduct,0
TODO: compute all the grids etc. once for the video analysis method,1
(and then just pass on the variables),0
apply nms,0
"IMPORTANT, as C++ function expects row-major",0
These two functions are for evaluation specifically (one also calculates integral between gt points),0
get dist_grid,0
apply nms,0
"IMPORTANT, as C++ function expects row-major",0
if 'eval_scale' in cfg.keys():,0
import imgaug.augmenters as iaa,0
im = iaa.Resize(float(cfg['eval_scale']))(images=im),0
# Functions below implement are for batch sizes > 1:,0
get dist_grid,0
apply nms,0
"IMPORTANT, as C++ function expects row-major",0
"batchsize,ny,nx,num_joints = scmap.shape",0
Change the stride from 2 to 1 to get 16x downscaling instead of 32x.,0
https://towardsdatascience.com/complete-architectural-details-of-all-efficientnet-models-5fd5b736142,0
The next part of the code depends upon which tensorflow version you have.,0
"return self.prediction_layers(net, end_points)",0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Augmentation functions,0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"First, initializing variables (if they don't exist)",0
what is the fraction of training samples with scaling augmentation?,0
loading defaults for rotation range!,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"cfg[""rotateratio""] = cfg.get(",0
"""rotratio"", 0.4",0
)  # what is the fraction of training samples with rotation augmentation?,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"Auto cropping is new (was not in Nature Neuroscience 2018 paper, but introduced in Nath et al. Nat. Protocols 2019)",0
"and boosts performance by 2X, particularly on challenging datasets, like the cheetah in Nath et al.",0
Parameters for augmentation with regard to cropping:,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
Pickle the 'data' dictionary using the highest protocol available.,0
item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal,0
Scale is sampled only once (per batch) to transform all of the images into same size.,0
assert len(batch_images) == self.batch_size,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
"imageio.imwrite(data_items[i].im_path.split('/')[-1],im)",0
if returndata:,0
"return batch_images,batch_joints,targetmaps",0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
"print(person_id,joint_ids,data_item.im_path.split('/')[-1])",0
if self.cfg.partaffinityfield_predict:,0
"print(""hello"",joint_id)",0
print(np.concatenate(joint_id)) #this is all joint_ids for all individuals!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
"plt.savefig(""along""+str(data_item.im_path.split('/')[-1]))",0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
"print(person_id,k, j_id, coords[0])",0
assert(0==1),0
"A[:,:,2]=partaffinityfield_map[:,:,j]*0",0
"A[:,:,2]=partaffinityfield_map[:,:,j]*0",0
"plt.savefig(""SomePafs""+str(data_item.im_path.split('/')[-1]))",0
WIP!,0
STD of gaussian is 1/4 of threshold,0
Grid of coordinates,0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Distance between the joint point and each coordinate,0
NEEDS TO BE DONE!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
parameter initialization for augmentation pipeline:,0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images of a batch into same size.,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
"from scipy.misc import imread, imresize",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
Get rid of the interpolation beyond the spline knots,0
Retrieve original individual label indices,0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
"net_type = ""efficientnet-b0""  # to -b6",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
TENSORPACK could fail in WINDOWS...,0
Need to add the 'likelihood' level value to simulate analyzed data,0
Ugliest hack in the history of pandas,1
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
# Create one split and make Shuffle 2 and 3 have the same split.,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
for EfficientNet,0
imgaug,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
# Create one split and make Shuffle 2 and 3 have the same split.,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Dowloading the calibration images,0
Deleting unneccesary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
example how to set pose config variables:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
Verify the aspect ratio is preserved,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
Suppress tensorflow warning messages,0
DLClight version does not support GUIs. Importing accordingly,0
"Train, evaluate & predict functions / all require TF",0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Updating config file:,0
Create the pose_config.yaml files,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path, num_shuffles=auxfun_models.Check4weights(pose_cfg['net_type'], parent_path, num_shuffles= 1)",0
Updating training and test pose_cfg:,0
Read the config file,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
common parameters:,0
Write dictionary to yaml  config file,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
Flatten the list of bodyparts to connect,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Start plotting for every frame,0
"Once all the frames are saved, then make a movie using ffmpeg.",0
Create the fig and define the axes,0
Clear plot and initialize the variables,0
Initialize arrays for appending the 3d data for actual plotting the points,0
Initialize arrays for appending the 3d data for drawing the lines,0
Initialize arrays for appending the 2d data from cam1 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam2 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam1 for drawing the lines,0
Initialize arrays for appending the 2d data from cam2 for drawing the lines,0
Get the scorer names from the dataframe,0
"Set the x,y, and z limits for the 3d view",0
"Set the frame number to read#max(0,index-trailpoints):index",0
Plot the labels for each body part,0
Connecting the bodyparts specified in the config file.3d file is created based on the likelihoods of cam1 and cam2. Using 3d file and check if the body part is nan then dont plot skeleton,0
Saving the frames,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
Extract the indices of frames where the likelihood of a bodypart for both cameras are less than pvalue,0
Extract frames where likelihood for both the views is less than the pcutoff,0
"low_likelihood_frames = np.all(likelihoods < pcutoff, axis=1)",0
For cam1 camera: Assign nans to x and y values of a bodypart where the likelihood for is less than pvalue,0
For cam2 camera: Assign nans to x and y values of a bodypart where the likelihood is less than pvalue,0
ToDo: speed up func. below by saving in numpy.array,1
"Create an empty dataframe to store x,y,z of 3d data",0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Undistorting the points from cam1 camera,0
Undistorting the points from cam2 camera,0
Save the undistorted files,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = io.imread(img),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
topSplitter = wx.SplitterWindow(self),0
,0
"self.image_panel = ImagePanel(topSplitter, config,video,shuffle,Dataframe,self.gui_size)",0
self.widget_panel = WidgetPanel(topSplitter),0
,0
"topSplitter.SplitHorizontally(self.image_panel, self.widget_panel,sashPosition=self.gui_size[1]*0.83)#0.9",0
topSplitter.SetSashGravity(1),0
sizer = wx.BoxSizer(wx.VERTICAL),0
"sizer.Add(topSplitter, 1, wx.EXPAND)",0
self.SetSizer(sizer),0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
"cv2.imwrite(img_name, frame)",0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.buttonCounter = [],0
self.bodyparts = [],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
Checks if zoom/pan button is ON,0
Alternatively,0
Sort tracklets by length to prioritize greater continuity,0
Drop tracklets that are too short,0
"Where slots are available, copy the data over",0
"If about to overwrite data, keep tracklets with highest confidence",0
Squeeze some data into empty slots,0
"For the remaining data, overwrite where we are least confident",0
Find keypoints closest to the remaining data,0
Use Manhattan distance to avoid overflow,0
Only overwrite if improving confidence,0
Fill existing gaps,0
Retrieve original individual label indices,0
Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.,0
ID swaps occur when X and Y simultaneously intersect each other.,0
Get only those bodypart pairs that belong to different individuals,0
Take into consideration imprecise OpenCV estimation of total number of frames,0
Automatically disable the draggable points,0
Save additional frames to the labeled-data directory,0
Store the newly-refined data,0
Uncertain keypoints are ignored,0
Merge with the already existing annotated data,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
average distance and average # significant differences avg. over comparisonbodyparts,0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
"self.choiceBox,self.visualization_rdb = self.choice_panel.addRadioButtons()",0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
print(self.Dataframe.head()),0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
fix move to corner,0
fix move to corner,0
fix move to corner,0
fix move to corner,0
Checks if zoom/pan button is ON,0
"print(""DLC loaded in light mode; you cannot use the relabeling GUI!"")",0
Find uncropped labeled data,0
Handle data previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
###################################################,0
Dependencies,0
###################################################,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as,0
specified in the config file,0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
Prepare figure,0
Mask rather than drop unwanted bodyparts to ensure consistent coloring,0
cropping!,0
nx=X2-X1,0
!/usr/bin/env python3,0
The above already raises a ValueError if formatting is wrong,0
"Rotate, see: https://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg",0
interesting option to just update metadata.,0
"Historically DLC used: from scipy.misc import imread, imresize >> deprecated functions",0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
Read the video until a frame is successfully read,0
import platform,0
"if platform.system() == ""Darwin"":  # for OSX use WXAgg",0
fig.canvas.start_event_loop(timeout=-1),0
else:,0
#fig.canvas.stop_event_loop(),0
"Windows throws error if file path is > 260 characters, can fix with prefix.",0
See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation,0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
"Attention this order has to be consistent (for training set creation, training, inference etc.)",0
CHECKS if each bpt is connected to at least one other bpt,0
TODO: check that there is a path leading from each (multi)bpt to each other (multi)bpt!,1
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
if suffix=='_full': #save metadata!,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"-> adding (single,bpt) for uniquebodyparts",0
"-> adding (indivdual,bpt) for multianimalbodyparts",0
load defaults,0
set project specific parameters:,0
###################################################,0
Dependencies,0
###################################################,0
Pose X vs pose Y,0
Poses vs time,0
Likelihoods,0
Histograms,0
#################################################,0
Looping analysis over video,0
#################################################,0
Keep only the individuals and bodyparts that were labeled,0
Determine whether the data are single- or multi-animal without loading into memory,0
simply by checking whether 'individuals' is in the second line of the CSV.,0
Is there a scorer for this?,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
Ensure same order as in config.yaml,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
Iterate over data files and stop as soon as one matching the scorer is found,0
If there was no match...,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
self.cfg = auxiliaryfunctions.read_config(config),0
"self.statusbar.SetStatusText("""")",0
"dlg = wx.MessageDialog(None,""Are you sure?"", ""Quit!"",wx.YES_NO | wx.ICON_WARNING)",0
result = dlg.ShowModal(),0
if result == wx.ID_YES:,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
NEW ROW:,0
self.save.Enable(False),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0,10), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=10)",0
Add all the options,0
line = wx.StaticLine(self),0
"self.sizer.Add(line, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=10)",0
,0
"self.multi_choice.Bind(wx.EVT_CHECKBOX,self.activate_copy_videos)",0
Hide the button as this is not the default option,0
self.vids.Enable(False),0
self.SetSizer(self.sizer),0
"self.sizer.Add(self.sizer, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=15)",0
self.ok.Enable(False),0
else:,0
self.ok.Enable(True),0
For mac compatibility,0
Remove the pages in case the user goes back to the create new project and creates/load a new project,0
Add all the other pages,0
self.yes.Enable(False),0
self.no.Enable(False),0
Here we create a panel and a notebook on the panel,0
create the page windows as children of the notebook and add the pages to the notebook with the label to show on the tab,0
variable initilization,0
design the panel,0
Add logo of DLC,0
self.filelist = self.filelist + self.vids,0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
use the default pose_cfg file for default values,0
self.display_iters.Enable(False),0
self.save_iters.Enable(False),0
self.max_iters.Enable(False),0
self.snapshots.Enable(False),0
Read the pose config file,0
"print(os.path.join(cfg['project_path'],auxiliaryfunctions.GetModelFolder(trainFraction, self.shuffles.GetValue(),cfg),'train','pose_cfg.yaml'))",0
let the user open the file with default text editor. Also make it mac compatible,0
update the variables with the edited values in the pose config file,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"if self.cfg.get(""multianimalproject"", False):",0
pass,0
else:,0
removing this as several downstream maDLC steps don't support dest_folder at this time:,0
"destfolder_text = wx.StaticBox(self, label=""Specify destination folder"")",0
"destfolderboxsizer = wx.StaticBoxSizer(destfolder_text, wx.VERTICAL)",0
self.change_workingdir = wx.CheckBox(,0
"self, label=""optional destination folder""",0
),0
self.hbox2.Add(self.change_workingdir),0
"self.change_workingdir.Bind(wx.EVT_CHECKBOX, self.activate_change_wd)",0
"self.sel_wd = wx.Button(self, label=""Browse"")",0
self.sel_wd.Enable(False),0
"self.sel_wd.Bind(wx.EVT_BUTTON, self.select_destfolder)",0
"self.hbox2.Add(self.sel_wd, 0, wx.ALL, -1)",0
Read the infer config file,0
let the user open the file with default text editor. Also make it mac compatible,0
"def video_tracklets(self,event):",0
shuffle = self.shuffle.GetValue(),0
trainingsetindex = self.trainingset.GetValue(),0
"deeplabcut.create_video_from_pickled_tracks(self.filelist, picklefile, pcutoff=0.6)",0
"self.select_destfolder.SetPath(""None"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.shuffles.SetValue(""1"")",0
"DownSampleVideo(vname,width=-1,height=200,outsuffix='downsampled',outpath=None,rotateccw=False):",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.videotype.SetStringSelection("".avi"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"if self.plot_scoremaps.GetStringSelection() == ""Yes"":",0
Read the infer config file,0
let the user open the file with default text editor. Also make it mac compatible,0
shuffle = self.shuffle.GetValue(),0
Read from edited inf. file first ...,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0, 4), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=5)",0
variable initilization,0
design the panel,0
Add logo of DLC,0
#         design the panel,0
Add image of DLC,0
if editing this text make sure you add the '\n' to get the new line. The sizer is unable to format lines correctly.,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
convert the image to RGB as you are showing the image with matplotlib,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Reset updated coords,0
Checks for the last image and disables the Next button,0
Backup previous save,0
Drop Nan data frames,0
Windows compatible,0
Checks if zoom/pan button is ON,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = cv2.imread(img),0
convert the image to RGB as you are showing the image with matplotlib,0
divider = make_axes_locatable(self.axes),0
"colorIndex = np.linspace(np.min(im),np.max(im),len(bodyparts))",0
"cax = divider.append_axes(""right"", size=""5%"", pad=0.05)",0
"cbar = self.figure.colorbar(ax, cax=cax,spacing='proportional', ticks=colorIndex)",0
cbar.set_ticklabels(bodyparts[::-1]),0
self.figure.canvas.draw(),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"MainFrame.saveDataSet(self, event)",0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
checks for unique bodyparts,0
"Reading the existing dataset,if already present",0
Handle data previously labeled on a different platform,0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Create an empty data frame,0
Cache original bodyparts,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Rearrange bodypart columns in config order,0
Check whether new labels were added,0
Checking if user added a new label,0
Found new labels in either multiple bodyparts or unique bodyparts,0
check if single is slected when radio buttons are changed,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Reset updated coords,0
Checks for the last image and disables the Next button,0
Backup previous save,0
Drop Nan data frames,0
Windows compatible,0
Discard data associated with bodyparts that are no longer in the config,0
Re-organize the dataframe so the CSV looks consistent with the config,0
Checks if zoom/pan button is ON,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
actualbpts=set(Data.columns.get_level_values(0)),0
utility function to split different crops from same image into either train or test!,0
loading & linking pretrained models,0
CURRENTLY ONLY ResNet supported!,0
multianimal case:,0
"ATTENTION: order has to be multibodyparts, then uniquebodyparts (for indexing)",0
Loading the encoder (if necessary downloading from TF),0
Map back to the original indices.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
load image to get dimensions:,0
"print ""Grayscale!""",0
if socialbdpt in actualbpts:,0
if socialbdpt in actualbpts:,0
Drop missing body parts,0
Drop points lying outside the image,0
###############################################################################,0
Saving metadata and data file (Pickle file),0
###############################################################################,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Setting inference cfg file:,0
Load updated lists:,0
Find the corresponding video file,0
Avoid cropping already cropped images,0
"moving old entry to _original, dropping it from video_set and update crop parameters",0
"print(key, ""dropping?"")",0
adding important values for multianiaml project:,0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Let's check if the code is *not* run on windows (Source: #https://stackoverflow.com/questions/1325581/how-do-i-check-if-im-running-on-windows-in-python),0
but the paths are in windows format...,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(index,name.split(os.sep)[1])",0
"print(name,test_video_name)",0
Check that points lie within the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
updating variable if null/None! #backwardscompatability,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
these values are dropped as scalecrop,0
doesn't have rotation implemented,0
sort the models directories,0
get the shuffle index,0
read cfg file,0
create log file,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
"text = str(self.individual_names+','+self.bodyParts)",0
Check for variable correctness,0
Moviepy:,0
"print(""DLC loaded in light mode; you cannot use the labeling GUI!"")",0
"def __init__(self, point,bodyParts):",0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
text = str(self.bodyParts),0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
"sess, inputs, outputs = predict.setup_pose_prediction(dlc_cfg)",0
#################################################,0
Looping over videos,0
#################################################,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
from skimage.io import imread,0
"frame=imread(os.path.join(directory,framename),mode='RGB')",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
TODO: addd cropping as in video analysis!,1
if cropping is not None:,0
cfg['cropping']=True,0
"cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping",0
"print(""Overwriting cropping parameters:"", cropping)",0
"print(""These are used for all videos, but won't be save to the cfg file."")",0
Check which snapshots are available and sort them by # iterations,0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
"NOTE: this can be used if only a subset is relevant. I.e. [0,1] for only first and second joint!",0
TODO: adjust this for multi + unique bodyparts!,1
this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. ),0
get corresponding bounding boxes!,0
Test whether the unique bodyparts have been assembled,0
TODO Perhaps easier to check whether links were defined in the PAF graph?,1
Find an unused tracklet ID for the 'unique' bodyparts,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"print(""If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract any outlier frames!"")",0
imagenet mean for resnet pretraining:,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/AlexEMG/DeepLabCut/pull/324",0
for DLC 2.2. (here all set False to not use PAFs/pairwise fields),0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
"reloading defaults, as they can bleed over from a previous run otherwise",0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
dlc_cfg_train = load_config(str(path_train_config)),0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
Name for deeplabcut net (based on its parameters),0
"DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])",0
"print(""Extracting maps for "", DLCscorer, "" with # of trainingiterations:"", trainingsiterations)",0
if notanalyzed: #this only applies to ask if h5 exists...,0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
Dependencies for anaysis,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Load meta data & annotations,0
get the graph!,0
plt.figure(),0
"source=np.array(Data[ind,j1,'x'][jj],Data[ind,j1,'y'][jj])",0
"target=np.array(Data[ind2,j1,'x'][jj],Data[ind2,j2,'y'][jj])",0
"NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*",0
print(inferenceboundscfg),0
"plt.subplot(len(partaffinityfield_graph),1,pi+1)",0
"plt.hist(ds_within,bins=np.linspace(0,100,21),color='red')",0
"plt.hist(ds_across,bins=np.linspace(0,100,21),color='blue')",0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
TODO: Make this code not so redundant!,1
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
"df.to_hdf(output_path.replace('csv', 'h5'), 'df_with_missing', format='table', mode='w')",0
#################################################,0
Load data...,0
#################################################,0
Loading human annotatated data,0
Handle data previously annotated on a different platform,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
TODO: IMPLEMENT for different batch sizes?,1
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
print(dlc_cfg),0
Specifying state of model (snapshot / training state),0
Evaluate PAF edge lengths to calibrate `distnorm`,0
FIXME Is having an empty array vs nan really that necessary?!,1
"Pick the predictions closest to ground truth,",0
rather than the ones the model has most confident in,0
Compute all distance statistics,0
"For OKS/PCK, compute the standard deviation error across all frames",0
returning to intial folder,0
wild guesses for a wide range:,0
Check which snapshots are available and sort them by # iterations,0
Pick distance threshold for (r)PCK from the statistics computed during evaluation,0
update number of individuals to retain.,0
calculating result at best best solution,0
"print(""Quantification:"", DataOptParams.head())",0
DataOptParams.to_hdf(,0
"path_inference_config.split("".yaml"")[0] + "".h5"",",0
"""df_with_missing"",",0
"format=""table"",",0
"mode=""w"",",0
),0
Store best predictions,0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
"[_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})",0
Save snapshot,0
return to original path.,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
from deeplabcut.pose_estimation_tensorflow.predict_multianimal import convert_detections2tracklet,0
initializing constants,0
TODO Fix the code below...,1
We can't just break the whole thing if there is one corrupted frame,0
in the middle of the video. Rather iterate over all frames and simply skip corruptions,0
"PredicteData['frame'+str(counter)]=predict.get_detectionswithcosts(frame, dlc_cfg, sess, inputs, outputs, outall=False,nms_radius=dlc_cfg.nmsradius,det_min_score=dlc_cfg.minconfidence)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"PredicteData[batch_num*batchsize:(batch_num+1)*batchsize, :] = pose",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
"PredicteData[batch_num*batchsize:batch_num*batchsize+batch_ind, :] = pose[:batch_ind,:]",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"PredicteData[batch_num*batchsize:(batch_num+1)*batchsize, :] = pose",0
print(PredicteData),0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
animal & bpt-wise distance!,0
"corrkps=np.sum((gt[row_indices]-ani[col_indices])**2,axis=2)<dcorr**2",0
animal & bpt-wise distance!,0
"corrkps=np.sum((gt[row_indices]-ani[col_indices])**2,axis=2)<dcorr**2",0
load params,0
Ensure type consistency,0
"stats = compute_crossval_metrics(config_path, inferencecfg, shuffle,trainingsetindex,",0
"dcorr=dcorr,leastbpts=leastbpts,modelprefix=modelprefix)",0
val = stats['rmse_test'].values[0]*(1+stats['misses_test'].values[0]*1./stats['hits_test'].values[0]),0
##################################,0
### auxiliaryfunctions,0
##################################,0
#########################################################,0
### conversion & greedy bodypart matching code,0
#########################################################,0
"d=distance(np.array(cand_a[i][:2]),np.array(cand_b[j][:2]))",0
filtering with global distance bounds,0
sort candidate connections by score,0
"if both bodyparts don't exist, create a new subset",0
filter detections according to inferencecfg parameters,0
filter connections according to inferencecfg parameters,0
assemble putative subsets,0
define constant velocity model,0
TODO Try particle filter (since we already have the keypoints),1
Modified from scipy source code:,0
- to restrict its use to 2D,0
- to get rid of shuffling (since arrays are only (nbodyparts * 3) element long),0
TODO - factor in keypoint confidence (and weight by # of observations??),1
"mat = self.calc_pairwise_oks(poses, poses_ref)",0
Remove matched detections with low OKS,0
matches = [],0
"for row, col in zip(row_indices, col_indices):",0
"if mat[row, col] < self.oks_threshold:",0
unmatched_poses.append(row),0
unmatched_trackers.append(col),0
else:,0
"matches.append([row, col])",0
if not len(matches):,0
"matches = np.empty((0, 2), dtype=int)",0
else:,0
matches = np.stack(matches),0
filter out matched with low IOU,0
get predicted locations from existing trackers.,0
update matched trackers with assigned detections,0
create and initialise new trackers for unmatched detections,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
openmp_arg = '-fopenmp',0
"if _platform == ""win32"":",0
openmp_arg = '-openmp',0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
print(end_points.keys()) >> to see what else is available.,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
from nets.mobilenet import conv_blocks as ops,0
from nets.mobilenet import mobilenet as lib,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
The next part of the code depends upon which tensorflow version you have.,0
assuming batchsize 1 here!,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
setting pairwise bodypart loss,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
get coordinates for bp1 and bp2,0
'm2',0
"distscalarproduct=np.zeros((len(C1),len(C2)))*np.nan",0
"distscalarproduct[c1i,c2i]=dy*v[1]+dx*v[0] #scalar product [v unit vector dx,dy in pixel coordinats from partaffinitymap]",0
Distances[l]['m2'] = distscalarproduct,0
TODO: compute all the grids etc. once for the video analysis method,1
(and then just pass on the variables),0
apply nms,0
"IMPORTANT, as C++ function expects row-major",0
These two functions are for evaluation specifically (one also calculates integral between gt points),0
get dist_grid,0
apply nms,0
"IMPORTANT, as C++ function expects row-major",0
# Functions below implement are for batch sizes > 1:,0
get dist_grid,0
apply nms,0
"IMPORTANT, as C++ function expects row-major",0
"batchsize,ny,nx,num_joints = scmap.shape",0
The next part of the code depends upon which tensorflow version you have.,0
"return self.prediction_layers(net, end_points)",0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Augmentation functions,0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"First, initializing variables (if they don't exist)",0
what is the fraction of training samples with scaling augmentation?,0
loading defaults for rotation range!,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"cfg[""rotateratio""] = cfg.get(",0
"""rotratio"", 0.4",0
)  # what is the fraction of training samples with rotation augmentation?,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"Auto cropping is new (was not in Nature Neuroscience 2018 paper, but introduced in Nath et al. Nat. Protocols 2019)",0
"and boosts performance by 2X, particularly on challenging datasets, like the cheetah in Nath et al.",0
Parameters for augmentation with regard to cropping:,0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
"what is the minimal frames size for cropping plus/minus ie.. [-100,100]^2 for an arb. joint",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
Pickle the 'data' dictionary using the highest protocol available.,0
item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal,0
Scale is sampled only once (per batch) to transform all of the images into same size.,0
assert len(batch_images) == self.batch_size,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
"imageio.imwrite(data_items[i].im_path.split('/')[-1],im)",0
if returndata:,0
"return batch_images,batch_joints,targetmaps",0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
"print(person_id,joint_ids,data_item.im_path.split('/')[-1])",0
if self.cfg.partaffinityfield_predict:,0
"print(""hello"",joint_id)",0
print(np.concatenate(joint_id)) #this is all joint_ids for all individuals!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
"plt.savefig(""along""+str(data_item.im_path.split('/')[-1]))",0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
"print(person_id,k, j_id, coords[0])",0
assert(0==1),0
"A[:,:,2]=partaffinityfield_map[:,:,j]*0",0
"A[:,:,2]=partaffinityfield_map[:,:,j]*0",0
"plt.savefig(""SomePafs""+str(data_item.im_path.split('/')[-1]))",0
WIP!,0
STD of gaussian is 1/4 of threshold,0
Grid of coordinates,0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Distance between the joint point and each coordinate,0
NEEDS TO BE DONE!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
parameter initialization for augmentation pipeline:,0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images of a batch into same size.,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
"from scipy.misc import imread, imresize",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
Get rid of the interpolation beyond the spline knots,0
Retrieve original individual label indices,0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"deeplabcut.create_labeled_video(path_config_file,[newvideo], destfolder=dfolder,filtered=True)",0
TENSORPACK could fail in WINDOWS...,0
os.environ['DLClight'] = 'True',0
Need to add the 'likelihood' level value to simulate analyzed data,0
Ugliest hack in the history of pandas,1
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
# Create one split and make Shuffle 2 and 3 have the same split.,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
"# here is an ""old way"" to do this",0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Dowloading the calibration images,0
Deleting unneccesary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Importing the toolbox (takes several seconds),0
Loading example data set,0
example how to set pose config variables:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
Verify the aspect ratio is preserved,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
Suppress tensorflow warning messages,0
DLClight version does not support GUIs. Importing accordingly,0
"Train, evaluate & predict functions / all require TF",0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Updating config file:,0
Create the pose_config.yaml files,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path, num_shuffles=auxfun_models.Check4weights(pose_cfg['net_type'], parent_path, num_shuffles= 1)",0
Updating training and test pose_cfg:,0
Read the config file,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
get vcap property,0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
common parameters:,0
Write dictionary to yaml  config file,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
Flatten the list of bodyparts to connect,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Start plotting for every frame,0
"Once all the frames are saved, then make a movie using ffmpeg.",0
Create the fig and define the axes,0
Clear plot and initialize the variables,0
Initialize arrays for appending the 3d data for actual plotting the points,0
Initialize arrays for appending the 3d data for drawing the lines,0
Initialize arrays for appending the 2d data from cam1 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam2 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam1 for drawing the lines,0
Initialize arrays for appending the 2d data from cam2 for drawing the lines,0
Get the scorer names from the dataframe,0
"Set the x,y, and z limits for the 3d view",0
"Set the frame number to read#max(0,index-trailpoints):index",0
Plot the labels for each body part,0
Connecting the bodyparts specified in the config file.3d file is created based on the likelihoods of cam1 and cam2. Using 3d file and check if the body part is nan then dont plot skeleton,0
Saving the frames,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
Extract the indices of frames where the likelihood of a bodypart for both cameras are less than pvalue,0
Extract frames where likelihood for both the views is less than the pcutoff,0
"low_likelihood_frames = np.all(likelihoods < pcutoff, axis=1)",0
For cam1 camera: Assign nans to x and y values of a bodypart where the likelihood for is less than pvalue,0
For cam2 camera: Assign nans to x and y values of a bodypart where the likelihood is less than pvalue,0
ToDo: speed up func. below by saving in numpy.array,1
"Create an empty dataframe to store x,y,z of 3d data",0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Undistorting the points from cam1 camera,0
Undistorting the points from cam2 camera,0
Save the undistorted files,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = io.imread(img),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
topSplitter = wx.SplitterWindow(self),0
,0
"self.image_panel = ImagePanel(topSplitter, config,video,shuffle,Dataframe,self.gui_size)",0
self.widget_panel = WidgetPanel(topSplitter),0
,0
"topSplitter.SplitHorizontally(self.image_panel, self.widget_panel,sashPosition=self.gui_size[1]*0.83)#0.9",0
topSplitter.SetSashGravity(1),0
sizer = wx.BoxSizer(wx.VERTICAL),0
"sizer.Add(topSplitter, 1, wx.EXPAND)",0
self.SetSizer(sizer),0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
self.cropping = False,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
"cv2.imwrite(img_name, frame)",0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.buttonCounter = [],0
self.bodyparts = [],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
Checks if zoom/pan button is ON,0
Alternatively,0
"Store tracklets, such that we later manipulate long chains",0
"rather than data of individual frames, yielding greater continuity.",0
Recursively fill the data containers,0
"Where slots are available, copy the data over",0
"If about to overwrite data, keep tracklets with highest confidence",0
Squeeze some data into empty slots,0
"For the remaining data, overwrite where we are least confident",0
Find keypoints closest to the remaining data,0
Only overwrite if improving confidence,0
Fill existing gaps,0
Retrieve original individual label indices,0
Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.,0
ID swaps occur when X and Y simultaneously intersect each other.,0
Get only those bodypart pairs that belong to different individuals,0
Take into consideration imprecise OpenCV estimation of total number of frames,0
Automatically disable the draggable points,0
Save additional frames to the labeled-data directory,0
Store the newly-refined data,0
Uncertain keypoints are ignored,0
Merge with the already existing annotated data,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
average distance and average # significant differences avg. over comparisonbodyparts,0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
"self.choiceBox,self.visualization_rdb = self.choice_panel.addRadioButtons()",0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
print(self.Dataframe.head()),0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
Checks if zoom/pan button is ON,0
"print(""DLC loaded in light mode; you cannot use the relabeling GUI!"")",0
Find uncropped labeled data,0
Handle data previously annotated on a different platform,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
###################################################,0
Dependencies,0
###################################################,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as,0
specified in the config file,0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
Prepare figure,0
cropping!,0
nx=X2-X1,0
!/usr/bin/env python3,0
"Historically DLC used: from scipy.misc import imread, imresize >> deprecated functions",0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
TODO check if those times exist...,1
"subprocess.call(['ffmpeg','-i',vname,'-ss',str(start),'-to',str(stop),'-c:v','copy','-c:a', newfilename])",0
"Rotate, see: https://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg",0
interesting option to just update metadata.,0
Read the video until a frame is successfully read,0
import platform,0
"if platform.system() == ""Darwin"":  # for OSX use WXAgg",0
fig.canvas.start_event_loop(timeout=-1),0
else:,0
#fig.canvas.stop_event_loop(),0
"Windows throws error if file path is > 260 characters, can fix with prefix.",0
See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation,0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
"Attention this order has to be consistent (for training set creation, training, inference etc.)",0
CHECKS if each bpt is connected to at least one other bpt,0
TODO: check that there is a path leading from each (multi)bpt to each other (multi)bpt!,1
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
if suffix=='_full': #save metadata!,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"-> adding (single,bpt) for uniquebodyparts",0
"-> adding (indivdual,bpt) for multianimalbodyparts",0
load defaults,0
set project specific parameters:,0
###################################################,0
Dependencies,0
###################################################,0
Pose X vs pose Y,0
Poses vs time,0
Likelihoods,0
Histograms,0
#################################################,0
Looping analysis over video,0
#################################################,0
Keep only the individuals and bodyparts that were labeled,0
Determine whether the data are single- or multi-animal without loading into memory,0
simply by checking whether 'individuals' is in the second line of the CSV.,0
Is there a scorer for this?,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
Ensure same order as in config.yaml,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
Iterate over data files and stop as soon as one matching the scorer is found,0
If there was no match...,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
self.cfg = auxiliaryfunctions.read_config(config),0
"self.statusbar.SetStatusText("""")",0
"dlg = wx.MessageDialog(None,""Are you sure?"", ""Quit!"",wx.YES_NO | wx.ICON_WARNING)",0
result = dlg.ShowModal(),0
if result == wx.ID_YES:,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
NEW ROW:,0
self.save.Enable(False),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0,10), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=10)",0
Add all the options,0
line = wx.StaticLine(self),0
"self.sizer.Add(line, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=10)",0
,0
"self.multi_choice.Bind(wx.EVT_CHECKBOX,self.activate_copy_videos)",0
Hide the button as this is not the default option,0
self.vids.Enable(False),0
self.SetSizer(self.sizer),0
"self.sizer.Add(self.sizer, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=15)",0
self.ok.Enable(False),0
else:,0
self.ok.Enable(True),0
For mac compatibility,0
Remove the pages in case the user goes back to the create new project and creates/load a new project,0
Add all the other pages,0
self.yes.Enable(False),0
self.no.Enable(False),0
Here we create a panel and a notebook on the panel,0
create the page windows as children of the notebook and add the pages to the notebook with the label to show on the tab,0
variable initilization,0
design the panel,0
Add logo of DLC,0
self.filelist = self.filelist + self.vids,0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
use the default pose_cfg file for default values,0
self.display_iters.Enable(False),0
self.save_iters.Enable(False),0
self.max_iters.Enable(False),0
self.snapshots.Enable(False),0
Read the pose config file,0
"print(os.path.join(cfg['project_path'],auxiliaryfunctions.GetModelFolder(trainFraction, self.shuffles.GetValue(),cfg),'train','pose_cfg.yaml'))",0
let the user open the file with default text editor. Also make it mac compatible,0
update the variables with the edited values in the pose config file,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"if self.cfg.get(""multianimalproject"", False):",0
pass,0
else:,0
removing this as several downstream maDLC steps don't support dest_folder at this time:,0
"destfolder_text = wx.StaticBox(self, label=""Specify destination folder"")",0
"destfolderboxsizer = wx.StaticBoxSizer(destfolder_text, wx.VERTICAL)",0
self.change_workingdir = wx.CheckBox(,0
"self, label=""optional destination folder""",0
),0
self.hbox2.Add(self.change_workingdir),0
"self.change_workingdir.Bind(wx.EVT_CHECKBOX, self.activate_change_wd)",0
"self.sel_wd = wx.Button(self, label=""Browse"")",0
self.sel_wd.Enable(False),0
"self.sel_wd.Bind(wx.EVT_BUTTON, self.select_destfolder)",0
"self.hbox2.Add(self.sel_wd, 0, wx.ALL, -1)",0
"self.sizer.Add(boxsizer, pos=(4, 0), span=(1, 5),flag=wx.EXPAND|wx.TOP|wx.LEFT|wx.RIGHT , border=10)",0
"boxsizer.Add(self.hbox1,0, 5)",0
Read the infer config file,0
let the user open the file with default text editor. Also make it mac compatible,0
"def video_tracklets(self,event):",0
shuffle = self.shuffle.GetValue(),0
trainingsetindex = self.trainingset.GetValue(),0
"deeplabcut.create_video_from_pickled_tracks(self.filelist, picklefile, pcutoff=0.6)",0
"self.select_destfolder.SetPath(""None"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.shuffles.SetValue(""1"")",0
"DownSampleVideo(vname,width=-1,height=200,outsuffix='downsampled',outpath=None,rotateccw=False):",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.videotype.SetStringSelection("".avi"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"if self.plot_scoremaps.GetStringSelection() == ""Yes"":",0
Read the infer config file,0
let the user open the file with default text editor. Also make it mac compatible,0
shuffle = self.shuffle.GetValue(),0
Read from edited inf. file first ...,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0, 4), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=5)",0
variable initilization,0
design the panel,0
Add logo of DLC,0
#         design the panel,0
Add image of DLC,0
if editing this text make sure you add the '\n' to get the new line. The sizer is unable to format lines correctly.,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
convert the image to RGB as you are showing the image with matplotlib,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Checks if zoom/pan button is ON,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = cv2.imread(img),0
convert the image to RGB as you are showing the image with matplotlib,0
divider = make_axes_locatable(self.axes),0
"colorIndex = np.linspace(np.min(im),np.max(im),len(bodyparts))",0
"cax = divider.append_axes(""right"", size=""5%"", pad=0.05)",0
"cbar = self.figure.colorbar(ax, cax=cax,spacing='proportional', ticks=colorIndex)",0
cbar.set_ticklabels(bodyparts[::-1]),0
self.figure.canvas.draw(),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"MainFrame.saveDataSet(self, event)",0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
checks for unique bodyparts,0
"Reading the existing dataset,if already present",0
Handle data previously labeled on a different platform,0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Create an empty data frame,0
Cache original bodyparts,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Rearrange bodypart columns in config order,0
Check whether new labels were added,0
Checking if user added a new label,0
Found new labels in either multiple bodyparts or unique bodyparts,0
check if single is slected when radio buttons are changed,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Discard data associated with bodyparts that are no longer in the config,0
Re-organize the dataframe so the CSV looks consistent with the config,0
Checks if zoom/pan button is ON,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
actualbpts=set(Data.columns.get_level_values(0)),0
utility function to split different crops from same image into either train or test!,0
loading & linking pretrained models,0
CURRENTLY ONLY ResNet supported!,0
multianimal case:,0
"ATTENTION: order has to be multibodyparts, then uniquebodyparts (for indexing)",0
Loading the encoder (if necessary downloading from TF),0
Map back to the original indices.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
load image to get dimensions:,0
"print ""Grayscale!""",0
if socialbdpt in actualbpts:,0
if socialbdpt in actualbpts:,0
Drop missing body parts,0
Drop points lying outside the image,0
###############################################################################,0
Saving metadata and data file (Pickle file),0
###############################################################################,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Setting inference cfg file:,0
"TODO:   ""distnormalization"":  could be calculated here based on data and set",0
>> now we calculate this during evaluation (which is a good spot...),0
Load updated lists:,0
Find the corresponding video file,0
Avoid cropping already cropped images,0
"moving old entry to _original, dropping it from video_set and update crop parameters",0
adding important values for multianiaml project:,0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Let's check if the code is *not* run on windows (Source: #https://stackoverflow.com/questions/1325581/how-do-i-check-if-im-running-on-windows-in-python),0
but the paths are in windows format...,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(index,name.split(os.sep)[1])",0
"print(name,test_video_name)",0
Check that points lie within the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
updating variable if null/None! #backwardscompatability,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
sort the models directories,0
get the shuffle index,0
read cfg file,0
create log file,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
"text = str(self.individual_names+','+self.bodyParts)",0
Check for variable correctness,0
Moviepy:,0
"print(""DLC loaded in light mode; you cannot use the labeling GUI!"")",0
"def __init__(self, point,bodyParts):",0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
text = str(self.bodyParts),0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
"sess, inputs, outputs = predict.setup_pose_prediction(dlc_cfg)",0
#################################################,0
Looping over videos,0
#################################################,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
from skimage.io import imread,0
"frame=imread(os.path.join(directory,framename),mode='RGB')",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
TODO: addd cropping as in video analysis!,1
if cropping is not None:,0
cfg['cropping']=True,0
"cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping",0
"print(""Overwriting cropping parameters:"", cropping)",0
"print(""These are used for all videos, but won't be save to the cfg file."")",0
Check which snapshots are available and sort them by # iterations,0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
"NOTE: this can be used if only a subset is relevant. I.e. [0,1] for only first and second joint!",0
TODO: adjust this for multi + unique bodyparts!,1
this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. ),0
get corresponding bounding boxes!,0
Test whether the unique bodyparts have been assembled,0
TODO Perhaps easier to check whether links were defined in the PAF graph?,1
Find an unused tracklet ID for the 'unique' bodyparts,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"print(""If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract any outlier frames!"")",0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/AlexEMG/DeepLabCut/pull/324",0
for DLC 2.2.,0
Parameters for augmentation with regard to cropping,0
"Added and described in ""Using DeepLabCut for 3D markerless pose estimation across species and behaviors""",0
Source: https://www.nature.com/articles/s41596-019-0176-0,0
"limit width  [-leftwidth*u-100,100+u*rightwidth] x [-bottomwith*u-100,100+u*topwidth] where u is always a (different) random number in unit interval",0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
Name for deeplabcut net (based on its parameters),0
"DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])",0
"print(""Extracting maps for "", DLCscorer, "" with # of trainingiterations:"", trainingsiterations)",0
if notanalyzed: #this only applies to ask if h5 exists...,0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
return DATA,0
"print(np.shape(image),np.shape(scmap),np.shape(locref),np.shape(paf))",0
"dest_folder = os.path.join(cfg['project_path'], 'maps')",0
Dependencies for anaysis,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Load meta data & annotations,0
get the graph!,0
plt.figure(),0
"source=np.array(Data[ind,j1,'x'][jj],Data[ind,j1,'y'][jj])",0
"target=np.array(Data[ind2,j1,'x'][jj],Data[ind2,j2,'y'][jj])",0
"NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*",0
print(inferenceboundscfg),0
"plt.subplot(len(partaffinityfield_graph),1,pi+1)",0
"plt.hist(ds_within,bins=np.linspace(0,100,21),color='red')",0
"plt.hist(ds_across,bins=np.linspace(0,100,21),color='blue')",0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
TODO: Make this code not so redundant!,1
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
"df.to_hdf(output_path.replace('csv', 'h5'), 'df_with_missing', format='table', mode='w')",0
Dependencies for anaysis,0
#################################################,0
Load data...,0
#################################################,0
Loading human annotatated data,0
Handle data previously annotated on a different platform,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
TODO: IMPLEMENT for different batch sizes?,1
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
print(dlc_cfg),0
Specifying state of model (snapshot / training state),0
"Storing GT data as dictionary, so it can be used for calculating connection costs",0
returning to intial folder,0
wild guesses for a wide range:,0
Check which snapshots are available and sort them by # iterations,0
update number of individuals to retain.,0
calculating result at best best solution,0
"print(""Quantification:"", DataOptParams.head())",0
Store best predictions,0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
"[_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})",0
Save snapshot,0
return to original path.,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
from deeplabcut.pose_estimation_tensorflow.predict_multianimal import convert_detections2tracklet,0
initializing constants,0
"PredicteData['frame'+str(counter)]=predict.get_detectionswithcosts(frame, dlc_cfg, sess, inputs, outputs, outall=False,nms_radius=dlc_cfg.nmsradius,det_min_score=dlc_cfg.minconfidence)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"PredicteData[batch_num*batchsize:(batch_num+1)*batchsize, :] = pose",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
"PredicteData[batch_num*batchsize:batch_num*batchsize+batch_ind, :] = pose[:batch_ind,:]",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"PredicteData[batch_num*batchsize:(batch_num+1)*batchsize, :] = pose",0
print(PredicteData),0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
animal & bpt-wise distance!,0
"corrkps=np.sum((gt[row_indices]-ani[col_indices])**2,axis=2)<dcorr**2",0
animal & bpt-wise distance!,0
"corrkps=np.sum((gt[row_indices]-ani[col_indices])**2,axis=2)<dcorr**2",0
load params,0
Ensure type consistency,0
"stats = compute_crossval_metrics(config_path, inferencecfg, shuffle,trainingsetindex,",0
"dcorr=dcorr,leastbpts=leastbpts,modelprefix=modelprefix)",0
val = stats['rmse_test'].values[0]*(1+stats['misses_test'].values[0]*1./stats['hits_test'].values[0]),0
Saving log file,0
##################################,0
### auxiliaryfunctions,0
##################################,0
#########################################################,0
### conversion & greedy bodypart matching code,0
#########################################################,0
"d=distance(np.array(cand_a[i][:2]),np.array(cand_b[j][:2]))",0
filtering with global distance bounds,0
sort candidate connections by score,0
"if both bodyparts don't exist, create a new subset",0
filter detections according to inferencecfg parameters,0
filter connections according to inferencecfg parameters,0
assemble putative subsets,0
define constant velocity model,0
TODO Try particle filter (since we already have the keypoints),1
Modified from scipy source code:,0
- to restrict its use to 2D,0
- to get rid of shuffling (since arrays are only (nbodyparts * 3) element long),0
TODO - factor in keypoint confidence (and weight by # of observations??),1
"mat = self.calc_pairwise_oks(poses, poses_ref)",0
Remove matched detections with low OKS,0
matches = [],0
"for row, col in zip(row_indices, col_indices):",0
"if mat[row, col] < self.oks_threshold:",0
unmatched_poses.append(row),0
unmatched_trackers.append(col),0
else:,0
"matches.append([row, col])",0
if not len(matches):,0
"matches = np.empty((0, 2), dtype=int)",0
else:,0
matches = np.stack(matches),0
filter out matched with low IOU,0
get predicted locations from existing trackers.,0
update matched trackers with assigned detections,0
create and initialise new trackers for unmatched detections,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
openmp_arg = '-fopenmp',0
"if _platform == ""win32"":",0
openmp_arg = '-openmp',0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
print(end_points.keys()) >> to see what else is available.,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
from nets.mobilenet import conv_blocks as ops,0
from nets.mobilenet import mobilenet as lib,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
The next part of the code depends upon which tensorflow version you have.,0
assuming batchsize 1 here!,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
setting pairwise bodypart loss,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
get coordinates for bp1 and bp2,0
'm2',0
"distscalarproduct=np.zeros((len(C1),len(C2)))*np.nan",0
"distscalarproduct[c1i,c2i]=dy*v[1]+dx*v[0] #scalar product [v unit vector dx,dy in pixel coordinats from partaffinitymap]",0
Distances[l]['m2'] = distscalarproduct,0
TODO: compute all the grids etc. once for the video analysis method,1
(and then just pass on the variables),0
apply nms,0
"IMPORTANT, as C++ function expects row-major",0
These two functions are for evaluation specifically (one also calculates integral between gt points),0
get dist_grid,0
apply nms,0
"IMPORTANT, as C++ function expects row-major",0
# Functions below implement are for batch sizes > 1:,0
get dist_grid,0
apply nms,0
"IMPORTANT, as C++ function expects row-major",0
"batchsize,ny,nx,num_joints = scmap.shape",0
The next part of the code depends upon which tensorflow version you have.,0
"return self.prediction_layers(net, end_points)",0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Augmentation functions,0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
Initializing variables if they don't exist...,0
what is the fraction of training samples with scaling augmentation?,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
Pickle the 'data' dictionary using the highest protocol available.,0
item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal,0
Scale is sampled only once (per batch) to transform all of the images into same size.,0
assert len(batch_images) == self.batch_size,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
"imageio.imwrite(data_items[i].im_path.split('/')[-1],im)",0
if returndata:,0
"return batch_images,batch_joints,targetmaps",0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
"print(person_id,joint_ids,data_item.im_path.split('/')[-1])",0
if self.cfg.partaffinityfield_predict:,0
"print(""hello"",joint_id)",0
print(np.concatenate(joint_id)) #this is all joint_ids for all individuals!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
"plt.savefig(""along""+str(data_item.im_path.split('/')[-1]))",0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
"print(person_id,k, j_id, coords[0])",0
assert(0==1),0
"A[:,:,2]=partaffinityfield_map[:,:,j]*0",0
"A[:,:,2]=partaffinityfield_map[:,:,j]*0",0
"plt.savefig(""SomePafs""+str(data_item.im_path.split('/')[-1]))",0
WIP!,0
STD of gaussian is 1/4 of threshold,0
Grid of coordinates,0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Distance between the joint point and each coordinate,0
NEEDS TO BE DONE!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images into same size.,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
"from scipy.misc import imread, imresize",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
"from scipy.misc import imread, imresize",0
"from dataset.pose_dataset import Batch, data_to_input, mirror_joints_map, CropImage, DataItem",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
Get rid of the interpolation beyond the spline knots,0
Retrieve original individual label indices,0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"deeplabcut.create_labeled_video(path_config_file,[newvideo], destfolder=dfolder,filtered=True)",0
TENSORPACK could fail in WINDOWS...,0
os.environ['DLClight'] = 'True',0
Need to add the 'likelihood' level value to simulate analyzed data,0
Ugliest hack in the history of pandas,1
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
#create one split and make Shuffle 2 and 3 have the same split.,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Dowloading the calibration images,0
Deleting unneccesary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Importing the toolbox (takes several seconds),0
Loading example data set,0
example how to set pose config variables:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
Suppress tensorflow warning messages,0
DLClight version does not support GUIs. Importing accordingly,0
"Train, evaluate & predict functions / all require TF",0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Updating config file:,0
Create the pose_config.yaml files,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path, num_shuffles=auxfun_models.Check4weights(pose_cfg['net_type'], parent_path, num_shuffles= 1)",0
Updating training and test pose_cfg:,0
Read the config file,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
get vcap property,0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
common parameters:,0
Write dictionary to yaml  config file,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
Flatten the list of bodyparts to connect,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Start plotting for every frame,0
"Once all the frames are saved, then make a movie using ffmpeg.",0
Create the fig and define the axes,0
Clear plot and initialize the variables,0
Initialize arrays for appending the 3d data for actual plotting the points,0
Initialize arrays for appending the 3d data for drawing the lines,0
Initialize arrays for appending the 2d data from cam1 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam2 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam1 for drawing the lines,0
Initialize arrays for appending the 2d data from cam2 for drawing the lines,0
Get the scorer names from the dataframe,0
"Set the x,y, and z limits for the 3d view",0
"Set the frame number to read#max(0,index-trailpoints):index",0
Plot the labels for each body part,0
Connecting the bodyparts specified in the config file.3d file is created based on the likelihoods of cam1 and cam2. Using 3d file and check if the body part is nan then dont plot skeleton,0
Saving the frames,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
Extract the indices of frames where the likelihood of a bodypart for both cameras are less than pvalue,0
Extract frames where likelihood for both the views is less than the pcutoff,0
"low_likelihood_frames = np.all(likelihoods < pcutoff, axis=1)",0
For cam1 camera: Assign nans to x and y values of a bodypart where the likelihood for is less than pvalue,0
For cam2 camera: Assign nans to x and y values of a bodypart where the likelihood is less than pvalue,0
ToDo: speed up func. below by saving in numpy.array,1
"Create an empty dataframe to store x,y,z of 3d data",0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Undistorting the points from cam1 camera,0
Undistorting the points from cam2 camera,0
Save the undistorted files,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = io.imread(img),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
topSplitter = wx.SplitterWindow(self),0
,0
"self.image_panel = ImagePanel(topSplitter, config,video,shuffle,Dataframe,self.gui_size)",0
self.widget_panel = WidgetPanel(topSplitter),0
,0
"topSplitter.SplitHorizontally(self.image_panel, self.widget_panel,sashPosition=self.gui_size[1]*0.83)#0.9",0
topSplitter.SetSashGravity(1),0
sizer = wx.BoxSizer(wx.VERTICAL),0
"sizer.Add(topSplitter, 1, wx.EXPAND)",0
self.SetSizer(sizer),0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
self.cropping = False,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
"cv2.imwrite(img_name, frame)",0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.buttonCounter = [],0
self.bodyparts = [],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
Checks if zoom/pan button is ON,0
Alternatively,0
"Store tracklets, such that we later manipulate long chains",0
"rather than data of individual frames, yielding greater continuity.",0
Recursively fill the data containers,0
"Where slots are available, copy the data over",0
"If about to overwrite data, keep tracklets with highest confidence",0
Squeeze some data into empty slots,0
"For the remaining data, overwrite where we are least confident",0
Find keypoints closest to the remaining data,0
Only overwrite if improving confidence,0
Fill existing gaps and slightly smooth the tracklets,0
Retrieve original individual label indices,0
Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.,0
ID swaps occur when X and Y simultaneously intersect each other.,0
Get only those bodypart pairs that belong to different individuals,0
Take into consideration imprecise OpenCV estimation of total number of frames,0
Automatically disable the draggable points,0
Save additional frames to the labeled-data directory,0
Store the newly-refined data,0
Merge with the already existing annotated data,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
average distance and average # significant differences avg. over comparisonbodyparts,0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
"self.choiceBox,self.visualization_rdb = self.choice_panel.addRadioButtons()",0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
print(self.Dataframe.head()),0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
Checks if zoom/pan button is ON,0
"print(""DLC loaded in light mode; you cannot use the relabeling GUI!"")",0
Find uncropped labeled data,0
Draw the skeleton if already existent,0
Find the most 'complete' animal,0
###################################################,0
Dependencies,0
###################################################,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as specified in the config file,0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
Prepare figure,0
if notanalyzed:,0
"notanalyzed,outdataname,sourcedataname,DLCscorer=auxiliaryfunctions.CheckifPostProcessing(folder,vname,DLCscorer,DLCscorerlegacy,suffix='checking')",0
cropping!,0
nx=X2-X1,0
!/usr/bin/env python3,0
"Historically DLC used: from scipy.misc import imread, imresize >> deprecated functions",0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
TODO check if those times exist...,1
"subprocess.call(['ffmpeg','-i',vname,'-ss',str(start),'-to',str(stop),'-c:v','copy','-c:a', newfilename])",0
"Rotate, see: https://stackoverflow.com/questions/3937387/rotating-videos-with-ffmpeg",0
interesting option to just update metadata.,0
Read the video until a frame is successfully read,0
import platform,0
"if platform.system() == ""Darwin"":  # for OSX use WXAgg",0
fig.canvas.start_event_loop(timeout=-1),0
else:,0
#fig.canvas.stop_event_loop(),0
"Windows throws error if file path is > 260 characters, can fix with prefix.",0
See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation,0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
"Attention this order has to be consistent (for training set creation, training, inference etc.)",0
CHECKS if each bpt is connected to at least one other bpt,0
TODO: check that there is a path leading from each (multi)bpt to each other (multi)bpt!,1
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
if suffix=='_full': #save metadata!,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"-> adding (single,bpt) for uniquebodyparts",0
"-> adding (indivdual,bpt) for multianimalbodyparts",0
load defaults,0
set project specific parameters:,0
###################################################,0
Dependencies,0
###################################################,0
Pose X vs pose Y,0
Poses vs time,0
Likelihoods,0
Histograms,0
#################################################,0
Looping analysis over video,0
#################################################,0
Keep only the individuals and bodyparts that were labeled,0
Determine whether the data are single- or multi-animal without loading into memory,0
simply by checking whether 'individuals' is in the second line of the CSV.,0
Is there a scorer for this?,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
Ensure same order as in config.yaml,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
Iterate over data files and stop as soon as one matching the scorer is found,0
If there was no match...,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
self.cfg = auxiliaryfunctions.read_config(config),0
"self.statusbar.SetStatusText("""")",0
"dlg = wx.MessageDialog(None,""Are you sure?"", ""Quit!"",wx.YES_NO | wx.ICON_WARNING)",0
result = dlg.ShowModal(),0
if result == wx.ID_YES:,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
NEW ROW:,0
self.save.Enable(False),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0,10), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=10)",0
Add all the options,0
line = wx.StaticLine(self),0
"self.sizer.Add(line, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=10)",0
,0
"self.multi_choice.Bind(wx.EVT_CHECKBOX,self.activate_copy_videos)",0
Hide the button as this is not the default option,0
self.vids.Enable(False),0
self.SetSizer(self.sizer),0
"self.sizer.Add(self.sizer, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=15)",0
self.ok.Enable(False),0
else:,0
self.ok.Enable(True),0
For mac compatibility,0
Remove the pages in case the user goes back to the create new project and creates/load a new project,0
Add all the other pages,0
self.yes.Enable(False),0
self.no.Enable(False),0
Here we create a panel and a notebook on the panel,0
create the page windows as children of the notebook and add the pages to the notebook with the label to show on the tab,0
variable initilization,0
design the panel,0
Add logo of DLC,0
self.filelist = self.filelist + self.vids,0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
use the default pose_cfg file for default values,0
self.display_iters.Enable(False),0
self.save_iters.Enable(False),0
self.max_iters.Enable(False),0
self.snapshots.Enable(False),0
Read the pose config file,0
"print(os.path.join(cfg['project_path'],auxiliaryfunctions.GetModelFolder(trainFraction, self.shuffles.GetValue(),cfg),'train','pose_cfg.yaml'))",0
let the user open the file with default text editor. Also make it mac compatible,0
update the variables with the edited values in the pose config file,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"if self.cfg.get(""multianimalproject"", False):",0
pass,0
else:,0
removing this as several downstream maDLC steps don't support dest_folder at this time:,0
"destfolder_text = wx.StaticBox(self, label=""Specify destination folder"")",0
"destfolderboxsizer = wx.StaticBoxSizer(destfolder_text, wx.VERTICAL)",0
self.change_workingdir = wx.CheckBox(,0
"self, label=""optional destination folder""",0
),0
self.hbox2.Add(self.change_workingdir),0
"self.change_workingdir.Bind(wx.EVT_CHECKBOX, self.activate_change_wd)",0
"self.sel_wd = wx.Button(self, label=""Browse"")",0
self.sel_wd.Enable(False),0
"self.sel_wd.Bind(wx.EVT_BUTTON, self.select_destfolder)",0
"self.hbox2.Add(self.sel_wd, 0, wx.ALL, -1)",0
"self.sizer.Add(boxsizer, pos=(4, 0), span=(1, 5),flag=wx.EXPAND|wx.TOP|wx.LEFT|wx.RIGHT , border=10)",0
"boxsizer.Add(self.hbox1,0, 5)",0
Read the infer config file,0
let the user open the file with default text editor. Also make it mac compatible,0
"def video_tracklets(self,event):",0
shuffle = self.shuffle.GetValue(),0
trainingsetindex = self.trainingset.GetValue(),0
"deeplabcut.create_video_from_pickled_tracks(self.filelist, picklefile, pcutoff=0.6)",0
"self.select_destfolder.SetPath(""None"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.shuffles.SetValue(""1"")",0
"DownSampleVideo(vname,width=-1,height=200,outsuffix='downsampled',outpath=None,rotateccw=False):",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.videotype.SetStringSelection("".avi"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"if self.plot_scoremaps.GetStringSelection() == ""Yes"":",0
Read the infer config file,0
let the user open the file with default text editor. Also make it mac compatible,0
shuffle = self.shuffle.GetValue(),0
Read from edited inf. file first ...,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0, 4), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=5)",0
variable initilization,0
design the panel,0
Add logo of DLC,0
#         design the panel,0
Add image of DLC,0
if editing this text make sure you add the '\n' to get the new line. The sizer is unable to format lines correctly.,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
convert the image to RGB as you are showing the image with matplotlib,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Checks if zoom/pan button is ON,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = cv2.imread(img),0
convert the image to RGB as you are showing the image with matplotlib,0
divider = make_axes_locatable(self.axes),0
"colorIndex = np.linspace(np.min(im),np.max(im),len(bodyparts))",0
"cax = divider.append_axes(""right"", size=""5%"", pad=0.05)",0
"cbar = self.figure.colorbar(ax, cax=cax,spacing='proportional', ticks=colorIndex)",0
cbar.set_ticklabels(bodyparts[::-1]),0
self.figure.canvas.draw(),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"MainFrame.saveDataSet(self, event)",0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
checks for unique bodyparts,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Create an empty data frame,0
Cache original bodyparts,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Rearrange bodypart columns in config order,0
Check whether new labels were added,0
Checking if user added a new label,0
Found new labels in either multiple bodyparts or unique bodyparts,0
check if single is slected when radio buttons are changed,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Discard data associated with bodyparts that are no longer in the config,0
Re-organize the dataframe so the CSV looks consistent,0
Checks if zoom/pan button is ON,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
actualbpts=set(Data.columns.get_level_values(0)),0
utility function to split different crops from same image into either train or test!,0
loading & linking pretrained models,0
CURRENTLY ONLY ResNet supported!,0
multianimal case:,0
"ATTENTION: order has to be multibodyparts, then uniquebodyparts (for indexing)",0
Loading the encoder (if necessary downloading from TF),0
Map back to the original indices.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
load image to get dimensions:,0
"print ""Grayscale!""",0
if socialbdpt in actualbpts:,0
if socialbdpt in actualbpts:,0
Drop missing body parts,0
Drop points lying outside the image,0
###############################################################################,0
Saving metadata and data file (Pickle file),0
###############################################################################,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Setting inference cfg file:,0
"TODO:   ""distnormalization"":  could be calculated here based on data and set",0
>> now we calculate this during evaluation (which is a good spot...),0
Load updated lists:,0
Find the corresponding video file,0
Avoid cropping already cropped images,0
"moving old entry to _original, dropping it from video_set and update crop parameters",0
adding important values for multianiaml project:,0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Let's check if the code is *not* run on windows (Source: #https://stackoverflow.com/questions/1325581/how-do-i-check-if-im-running-on-windows-in-python),0
but the paths are in windows format...,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(index,name.split(os.sep)[1])",0
"print(name,test_video_name)",0
Check that points lie within the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
updating variable if null/None! #backwardscompatability,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndices,testIndices, Shuffles, augmenter_type,net_type)",0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
sort the models directories,0
get the shuffle index,0
read cfg file,0
create log file,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
"text = str(self.individual_names+','+self.bodyParts)",0
Check for variable correctness,0
Moviepy:,0
"print(""DLC loaded in light mode; you cannot use the labeling GUI!"")",0
"def __init__(self, point,bodyParts):",0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
text = str(self.bodyParts),0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
"sess, inputs, outputs = predict.setup_pose_prediction(dlc_cfg)",0
#################################################,0
Looping over videos,0
#################################################,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
from skimage.io import imread,0
"frame=imread(os.path.join(directory,framename),mode='RGB')",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
TODO: addd cropping as in video analysis!,1
if cropping is not None:,0
cfg['cropping']=True,0
"cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping",0
"print(""Overwriting cropping parameters:"", cropping)",0
"print(""These are used for all videos, but won't be save to the cfg file."")",0
Check which snapshots are available and sort them by # iterations,0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
"NOTE: this can be used if only a subset is relevant. I.e. [0,1] for only first and second joint!",0
TODO: adjust this for multi + unique bodyparts!,1
this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. ),0
get corresponding bounding boxes!,0
Test whether the unique bodyparts have been assembled,0
TODO Perhaps easier to check whether links were defined in the PAF graph?,1
Find an unused tracklet ID for the 'unique' bodyparts,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"print(""If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract any outlier frames!"")",0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/AlexEMG/DeepLabCut/pull/324",0
for DLC 2.2.,0
Parameters for augmentation with regard to cropping,0
"Added and described in ""Using DeepLabCut for 3D markerless pose estimation across species and behaviors""",0
Source: https://www.nature.com/articles/s41596-019-0176-0,0
"limit width  [-leftwidth*u-100,100+u*rightwidth] x [-bottomwith*u-100,100+u*topwidth] where u is always a (different) random number in unit interval",0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
Name for deeplabcut net (based on its parameters),0
"DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])",0
"print(""Extracting maps for "", DLCscorer, "" with # of trainingiterations:"", trainingsiterations)",0
if notanalyzed: #this only applies to ask if h5 exists...,0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
return DATA,0
"print(np.shape(image),np.shape(scmap),np.shape(locref),np.shape(paf))",0
"dest_folder = os.path.join(cfg['project_path'], 'maps')",0
Dependencies for anaysis,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Load meta data & annotations,0
get the graph!,0
plt.figure(),0
"source=np.array(Data[ind,j1,'x'][jj],Data[ind,j1,'y'][jj])",0
"target=np.array(Data[ind2,j1,'x'][jj],Data[ind2,j2,'y'][jj])",0
"NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*",0
print(inferenceboundscfg),0
"plt.subplot(len(partaffinityfield_graph),1,pi+1)",0
"plt.hist(ds_within,bins=np.linspace(0,100,21),color='red')",0
"plt.hist(ds_across,bins=np.linspace(0,100,21),color='blue')",0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
TODO: Make this code not so redundant!,1
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
"df.to_hdf(output_path.replace('csv', 'h5'), 'df_with_missing', format='table', mode='w')",0
Dependencies for anaysis,0
#################################################,0
Load data...,0
#################################################,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
TODO: IMPLEMENT for different batch sizes?,1
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
print(dlc_cfg),0
Specifying state of model (snapshot / training state),0
"Storing GT data as dictionary, so it can be used for calculating connection costs",0
returning to intial folder,0
wild guesses for a wide range:,0
Check which snapshots are available and sort them by # iterations,0
update number of individuals to retain.,0
calculating result at best best solution,0
"print(""Quantification:"", DataOptParams.head())",0
Store best predictions,0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
"[_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})",0
Save snapshot,0
return to original path.,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
from deeplabcut.pose_estimation_tensorflow.predict_multianimal import convert_detections2tracklet,0
initializing constants,0
"PredicteData['frame'+str(counter)]=predict.get_detectionswithcosts(frame, dlc_cfg, sess, inputs, outputs, outall=False,nms_radius=dlc_cfg.nmsradius,det_min_score=dlc_cfg.minconfidence)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"PredicteData[batch_num*batchsize:(batch_num+1)*batchsize, :] = pose",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
"PredicteData[batch_num*batchsize:batch_num*batchsize+batch_ind, :] = pose[:batch_ind,:]",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"PredicteData[batch_num*batchsize:(batch_num+1)*batchsize, :] = pose",0
print(PredicteData),0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
animal & bpt-wise distance!,0
"corrkps=np.sum((gt[row_indices]-ani[col_indices])**2,axis=2)<dcorr**2",0
animal & bpt-wise distance!,0
"corrkps=np.sum((gt[row_indices]-ani[col_indices])**2,axis=2)<dcorr**2",0
load params,0
Ensure type consistency,0
"stats = compute_crossval_metrics(config_path, inferencecfg, shuffle,trainingsetindex,",0
"dcorr=dcorr,leastbpts=leastbpts,modelprefix=modelprefix)",0
val = stats['rmse_test'].values[0]*(1+stats['misses_test'].values[0]*1./stats['hits_test'].values[0]),0
Saving log file,0
##################################,0
### auxiliaryfunctions,0
##################################,0
#########################################################,0
### conversion & greedy bodypart matching code,0
#########################################################,0
"d=distance(np.array(cand_a[i][:2]),np.array(cand_b[j][:2]))",0
filtering with global distance bounds,0
sort candidate connections by score,0
"if both bodyparts don't exist, create a new subset",0
filter detections according to inferencecfg parameters,0
filter connections according to inferencecfg parameters,0
assemble putative subsets,0
define constant velocity model,0
TODO Try particle filter (since we already have the keypoints),1
Modified from scipy source code:,0
- to restrict its use to 2D,0
- to get rid of shuffling (since arrays are only (nbodyparts * 3) element long),0
TODO - factor in keypoint confidence (and weight by # of observations??),1
"mat = self.calc_pairwise_oks(poses, poses_ref)",0
Remove matched detections with low OKS,0
matches = [],0
"for row, col in zip(row_indices, col_indices):",0
"if mat[row, col] < self.oks_threshold:",0
unmatched_poses.append(row),0
unmatched_trackers.append(col),0
else:,0
"matches.append([row, col])",0
if not len(matches):,0
"matches = np.empty((0, 2), dtype=int)",0
else:,0
matches = np.stack(matches),0
filter out matched with low IOU,0
get predicted locations from existing trackers.,0
update matched trackers with assigned detections,0
create and initialise new trackers for unmatched detections,0
+1 as MOT benchmark requires positive >> this is removed for DLC!,0
remove dead tracklet,0
openmp_arg = '-fopenmp',0
"if _platform == ""win32"":",0
openmp_arg = '-openmp',0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
print(end_points.keys()) >> to see what else is available.,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
from nets.mobilenet import conv_blocks as ops,0
from nets.mobilenet import mobilenet as lib,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
The next part of the code depends upon which tensorflow version you have.,0
assuming batchsize 1 here!,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
setting pairwise bodypart loss,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
get coordinates for bp1 and bp2,0
'm2',0
"distscalarproduct=np.zeros((len(C1),len(C2)))*np.nan",0
"distscalarproduct[c1i,c2i]=dy*v[1]+dx*v[0] #scalar product [v unit vector dx,dy in pixel coordinats from partaffinitymap]",0
Distances[l]['m2'] = distscalarproduct,0
TODO: compute all the grids etc. once for the video analysis method,1
(and then just pass on the variables),0
apply nms,0
"IMPORTANT, as C++ function expects row-major",0
These two functions are for evaluation specifically (one also calculates integral between gt points),0
get dist_grid,0
apply nms,0
"IMPORTANT, as C++ function expects row-major",0
# Functions below implement are for batch sizes > 1:,0
get dist_grid,0
apply nms,0
"IMPORTANT, as C++ function expects row-major",0
"batchsize,ny,nx,num_joints = scmap.shape",0
The next part of the code depends upon which tensorflow version you have.,0
"return self.prediction_layers(net, end_points)",0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Augmentation functions,0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
Initializing variables if they don't exist...,0
what is the fraction of training samples with scaling augmentation?,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
Pickle the 'data' dictionary using the highest protocol available.,0
item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal,0
Scale is sampled only once (per batch) to transform all of the images into same size.,0
assert len(batch_images) == self.batch_size,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
"imageio.imwrite(data_items[i].im_path.split('/')[-1],im)",0
if returndata:,0
"return batch_images,batch_joints,targetmaps",0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
"print(person_id,joint_ids,data_item.im_path.split('/')[-1])",0
if self.cfg.partaffinityfield_predict:,0
"print(""hello"",joint_id)",0
print(np.concatenate(joint_id)) #this is all joint_ids for all individuals!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
"plt.savefig(""along""+str(data_item.im_path.split('/')[-1]))",0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
"print(person_id,k, j_id, coords[0])",0
assert(0==1),0
"A[:,:,2]=partaffinityfield_map[:,:,j]*0",0
"A[:,:,2]=partaffinityfield_map[:,:,j]*0",0
"plt.savefig(""SomePafs""+str(data_item.im_path.split('/')[-1]))",0
WIP!,0
STD of gaussian is 1/4 of threshold,0
Grid of coordinates,0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Distance between the joint point and each coordinate,0
NEEDS TO BE DONE!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images into same size.,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
"from scipy.misc import imread, imresize",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
"from scipy.misc import imread, imresize",0
"from dataset.pose_dataset import Batch, data_to_input, mirror_joints_map, CropImage, DataItem",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
Get rid of the interpolation beyond the spline knots,0
Retrieve original individual label indices,0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
Process skeleton,0
save,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"deeplabcut.create_labeled_video(path_config_file,[newvideo], destfolder=dfolder,filtered=True)",0
TENSORPACK could fail in WINDOWS...,0
os.environ['DLClight'] = 'True',0
Need to add the 'likelihood' level value to simulate analyzed data,0
Ugliest hack in the history of pandas,1
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
#create one split and make Shuffle 2 and 3 have the same split.,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Dowloading the calibration images,0
Deleting unneccesary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Importing the toolbox (takes several seconds),0
Loading example data set,0
example how to set pose config variables:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
Supress tensorflow warning messages,0
DLClight version does not support GUIs. Importing accordingly,0
"Train, evaluate & predict functions / require TF",0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Updating config file:,0
Create the pose_config.yaml files,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path, num_shuffles=auxfun_models.Check4weights(pose_cfg['net_type'], parent_path, num_shuffles= 1)",0
Updating training and test pose_cfg:,0
Read the config file,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
get vcap property,0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
cfg_file['resnet']=50,0
Write dictionary to yaml  config file,0
import yaml,0
import cv2,0
import shutil,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
Flatten the list of bodyparts to connect,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Start plotting for every frame,0
"Once all the frames are saved, then make a movie using ffmpeg.",0
Create the fig and define the axes,0
Clear plot and initialize the variables,0
Initialize arrays for appending the 3d data for actual plotting the points,0
Initialize arrays for appending the 3d data for drawing the lines,0
Initialize arrays for appending the 2d data from cam1 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam2 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam1 for drawing the lines,0
Initialize arrays for appending the 2d data from cam2 for drawing the lines,0
Get the scorer names from the dataframe,0
"Set the x,y, and z limits for the 3d view",0
"Set the frame number to read#max(0,index-trailpoints):index",0
Plot the labels for each body part,0
Connecting the bodyparts specified in the config file.3d file is created based on the likelihoods of cam1 and cam2. Using 3d file and check if the body part is nan then dont plot skeleton,0
Saving the frames,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
Extract the indices of frames where the likelihood of a bodypart for both cameras are less than pvalue,0
Extract frames where likelihood for both the views is less than the pcutoff,0
"low_likelihood_frames = np.all(likelihoods < pcutoff, axis=1)",0
For cam1 camera: Assign nans to x and y values of a bodypart where the likelihood for is less than pvalue,0
For cam2 camera: Assign nans to x and y values of a bodypart where the likelihood is less than pvalue,0
ToDo: speed up func. below by saving in numpy.array,1
"Create an empty dataframe to store x,y,z of 3d data",0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Undistorting the points from cam1 camera,0
Undistorting the points from cam2 camera,0
Save the undistorted files,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = io.imread(img),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
self.cropping = False,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
"cv2.imwrite(img_name, frame)",0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.buttonCounter = [],0
self.bodyparts = [],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
Checks if zoom/pan button is ON,0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
extract min and max index based on start stop interval.,0
figure out body part list:,0
all indices between start and stop with jump larger than epsilon (leading up to this point!),0
deviation_dataname = str(Path(videofolder)/Path(dataname)),0
Calculate deviatons for video,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
data.to_csv(dataname.split('.h5')[0]+'filtered.csv'),0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
"Dataframe = pd.read_hdf(os.path.join(videofolder,dataname+'.h5'))",0
drop duplicate labels:,0
updates iteration by 1,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
"self.choiceBox,self.visualization_rdb = self.choice_panel.addRadioButtons()",0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
print(self.Dataframe.head()),0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
Checks if zoom/pan button is ON,0
"print(""DLC loaded in light mode; you cannot use the relabeling GUI!"")",0
###################################################,0
Dependencies,0
###################################################,0
import sys,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as specified in the config file,0
"rr, cc,val = line_aa(int(df_y[pair[0],index]),int(df_x[pair[0],index]),int(df_y[pair[1],index]), int(df_x[pair[1],index]))",0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Adds skeleton to the video,0
###################,0
less transparent present.,0
# One can change the parameters of the video creation script below:,0
See ffmpeg user guide: http://ffmpeg.org/ffmpeg.html#Video-and-Audio-file-format-conversion,0
,0
if notanalyzed:,0
"notanalyzed,outdataname,sourcedataname,DLCscorer=auxiliaryfunctions.CheckifPostProcessing(folder,vname,DLCscorer,DLCscorerlegacy,suffix='checking')",0
Loading cropping data used during analysis,0
!/usr/bin/env python3,0
"Historically DLC used: from scipy.misc import imread, imresize >> deprecated functions",0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
TODO check if those times exist...,1
"subprocess.call(['ffmpeg','-i',vname,'-ss',str(start),'-to',str(stop),'-c:v','copy','-c:a', newfilename])",0
Read the video until a frame is successfully read,0
"windows throws error if file path is > 260 characters, can fix with prefix. see https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation",0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
TODO: update how DLC path is obtained,1
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
###################################################,0
Dependencies,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
"fig.colorbar(im, ax=ax)",0
#################################################,0
Looping analysis over video,0
#################################################,0
LoadData,0
"nlines,numcolumns=data.shape",0
assert(len(orderofbpincsv)==len(cfg['bodyparts'])),0
TODO: test len of images vs. len of imagenames for another sanity check,1
Is there a scorer for this?,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
update path to current location of config.yaml (if different),0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
"with open(projconfigfile, 'w') as cf:",0
"ruamelFile_3d.dump(cfg_file_3d, cf)",0
Read the pickle file,0
Write the pickle file,0
checks if input is a directory,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
Ensure same order as in config.yaml,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
self.cfg = auxiliaryfunctions.read_config(config),0
"self.statusbar.SetStatusText("""")",0
"dlg = wx.MessageDialog(None,""Are you sure?"", ""Quit!"",wx.YES_NO | wx.ICON_WARNING)",0
result = dlg.ShowModal(),0
if result == wx.ID_YES:,0
from deeplabcut.gui.refine_tracklets import Refine_tracklets,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0,10), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=10)",0
Add all the options,0
line = wx.StaticLine(self),0
"self.sizer.Add(line, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=10)",0
,0
"self.multi_choice = wx.CheckBox(self, label=""Is it a multi-animal project?"")",0
"self.multi_choice.Bind(wx.EVT_CHECKBOX,self.activate_copy_videos)",0
hbox4.Add(self.multi_choice),0
Hide the button as this is not the default option,0
self.vids.Enable(False),0
self.multi_choice.Enable(False),0
self.SetSizer(self.sizer),0
"self.sizer.Add(self.sizer, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=15)",0
self.multi_choice.Enable(True),0
self.ok.Enable(False),0
else:,0
self.ok.Enable(True),0
For mac compatibility,0
Remove the pages in case the user goes back to the create new project and creates/load a new project,0
Add all the other pages,0
self.multi_choice.Enable(True),0
self.yes.Enable(False),0
self.no.Enable(False),0
"wx.Frame.__init__(self, None, title=""DeepLabCut"")",0
"wx.Frame.__init__( self, None, id = wx.ID_ANY, title = 'DeepLabCut',size = wx.Size(self.gui_size), pos = wx.DefaultPosition, style = wx.RESIZE_BORDER|wx.DEFAULT_FRAME_STYLE|wx.TAB_TRAVERSAL )",0
Here we create a panel and a notebook on the panel,0
create the page windows as children of the notebook and add the pages to the notebook with the label to show on the tab,0
variable initilization,0
design the panel,0
Add logo of DLC,0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
use the default pose_cfg file for default values,0
self.display_iters.Enable(False),0
self.save_iters.Enable(False),0
self.max_iters.Enable(False),0
self.snapshots.Enable(False),0
Read the pose config file,0
"print(os.path.join(cfg['project_path'],auxiliaryfunctions.GetModelFolder(trainFraction, self.shuffles.GetValue(),cfg),'train','pose_cfg.yaml'))",0
let the user open the file with default text editor. Also make it mac compatible,0
update the variables with the edited values in the pose config file,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.shuffles = wx.SpinCtrl(self, value='1',min=1,max=100)",0
"shuffles_text_boxsizer.Add(self.shuffles,1, wx.EXPAND|wx.TOP|wx.BOTTOM, 10)",0
if self.draw_skeleton.IsShow():,0
"if self.draw_skeleton.GetStringSelection() == ""Yes"":",0
self.draw = True,0
else:,0
self.draw = True,0
"print(self.config,self.filelist,self.videotype.GetValue(),shuffle,trainingsetindex,gputouse=None,save_as_csv=save_as_csv,destfolder=self.destfolder,cropping=cropping)",0
self.gpu.SetValue(-1),0
import wx.lib.scrolledpanel as SP,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"DownSampleVideo(vname,width=-1,height=200,outsuffix='downsampled',outpath=None,rotateccw=False):",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.videotype.SetStringSelection("".avi"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
self.hbox2 = wx.BoxSizer(wx.HORIZONTAL),0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
"boxsizer.Add(self.hbox2,5, wx.EXPAND|wx.TOP|wx.BOTTOM, 10)",0
shuffle = self.shuffle.GetValue(),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.sel_config.SetPath(""Please select the config file"")",0
else:,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
#         design the panel,0
Add image of DLC,0
if editing this text make sure you add the '\n' to get the new line. The sizer is unable to format lines correctly.,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
convert the image to RGB as you are showing the image with matplotlib,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Checks if zoom/pan button is ON,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = cv2.imread(img),0
convert the image to RGB as you are showing the image with matplotlib,0
divider = make_axes_locatable(self.axes),0
"colorIndex = np.linspace(np.min(im),np.max(im),len(bodyparts))",0
"cax = divider.append_axes(""right"", size=""5%"", pad=0.05)",0
"cbar = self.figure.colorbar(ax, cax=cax,spacing='proportional', ticks=colorIndex)",0
cbar.set_ticklabels(bodyparts[::-1]),0
self.figure.canvas.draw(),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"MainFrame.saveDataSet(self, event)",0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Create an empty data frame,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of old labels,0
Extracting the list of old unique labels,0
Checking if new labels are added or not,0
Checking if user added a new label,0
Found new labels in either multiple bodyparts or unique bodyparts,0
check if single is slected when radio buttons are changed,0
Checking if user added a new label,0
if self.new_Multibodyparts==[] and self.new_UniqueBodyparts==[]: # i.e. no new labels,0
"self.figure,self.axes,self.canvas,self.toolbar,self.image_axis = self.image_panel.drawplot(self.img,img_name,self.iter,self.index,self.multibodyparts,self.colormap,keep_view=self.view_locked)",0
"self.axes.callbacks.connect('xlim_changed', self.onZoom)",0
"self.axes.callbacks.connect('ylim_changed', self.onZoom)",0
"self.image_panel.addcolorbar(self.img,self.image_axis,self.iter,self.multibodyparts,self.colormap)",0
# Only show the bodyparts corresponding to single,0
if self.individual_names[0] == 'single':,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.uniquebodyparts,self.individual_names,self.file,self.markerSize)",0
else:,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.multibodyparts,self.individual_names,self.file,self.markerSize)",0
,0
"self.individualrdb.Bind(wx.EVT_RADIOBOX,self.select_individual)",0
,0
"# Get the color index, depending on if single is selcted or not.",0
"if self.rdb.GetStringSelection==""single"":",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.uniquebodyparts)",0
else:,0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
"self.cidClick = self.canvas.mpl_connect('button_press_event', self.onClick)",0
else:,0
# Found new labels in either multiple bodyparts or unique bodyparts,0
"dlg = wx.MessageDialog(None,""New label found in the config file. Do you want to see all the other labels?"", ""New label found"",wx.YES_NO | wx.ICON_WARNING)",0
result = dlg.ShowModal(),0
if result == wx.ID_NO:,0
if self.new_Multibodyparts!=[]:,0
self.multibodyparts = self.new_Multibodyparts,0
if self.new_UniqueBodyparts!=[]:,0
self.uniquebodyparts = self.new_UniqueBodyparts,0
,0
"self.dataFrame = MainFrame.create_dataframe(self,self.dataFrame,self.relativeimagenames,self.individual_names,self.new_UniqueBodyparts,self.new_Multibodyparts)",0
"self.figure,self.axes,self.canvas,self.toolbar,self.image_axis = self.image_panel.drawplot(self.img,img_name,self.iter,self.index,self.multibodyparts,self.colormap,keep_view=self.view_locked)",0
"self.axes.callbacks.connect('xlim_changed', self.onZoom)",0
"self.axes.callbacks.connect('ylim_changed', self.onZoom)",0
"self.image_panel.addcolorbar(self.img,self.image_axis,self.iter,self.multibodyparts,self.colormap)",0
,0
if self.individual_names[0] == 'single':,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.uniquebodyparts,self.individual_names,self.file,self.markerSize)",0
else:,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.multibodyparts,self.individual_names,self.file,self.markerSize)",0
"self.individualrdb.Bind(wx.EVT_RADIOBOX,self.select_individual)",0
"if self.rdb.GetStringSelection==""single"":",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.uniquebodyparts)",0
else:,0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
"self.cidClick = self.canvas.mpl_connect('button_press_event', self.onClick)",0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
"def plot(self,img):",0
self.drs= [],0
self.updatedCoords = [],0
for ind in self.individual_names:,0
if self.are_unique_bodyparts_present == False: #no unique bodyparts present,0
#            for ind in self.individual_names:,0
image_points = [],0
"for c, bp in enumerate(self.multibodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
"self.points = [self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter]]",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
color = self.colormap(self.norm(self.colorIndex[c])),0
"circle = patches.Circle((self.points[0], self.points[1]), radius=self.markerSize, fc = color, alpha=self.alpha)",0
#                    print(circle),0
print(self.axes),0
self.axes.add_patch(circle),0
"self.dr = auxfun_drag_label_multiple_individuals.DraggablePoint(circle,ind,self.multibodyparts[c])",0
self.dr.connect(),0
self.dr.coords = image_points,0
self.drs.append(self.dr),0
self.updatedCoords.append(self.dr.coords),0
if np.isnan(self.points)[0] == False:,0
self.buttonCounter[ind].append(self.multibodyparts[c]),0
else:,0
if ind == 'single':,0
"for c, bp in enumerate(self.uniquebodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
"self.points = [self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter]]",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.uniquebodyparts)",0
color = self.colormap(self.norm(self.colorIndex[c])),0
"circle = patches.Circle((self.points[0], self.points[1]), radius=self.markerSize, fc = color, alpha=self.alpha)",0
#                    print(circle),0
print(self.axes),0
self.axes.add_patch(circle),0
"self.dr = auxfun_drag_label_multiple_individuals.DraggablePoint(circle,ind,self.uniquebodyparts[c])",0
self.dr.connect(),0
self.dr.coords = image_points,0
self.drs.append(self.dr),0
self.updatedCoords.append(self.dr.coords),0
if np.isnan(self.points)[0] == False:,0
self.buttonCounter[ind].append(self.uniquebodyparts[c]),0
else:,0
"for c, bp in enumerate(self.multibodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
"self.points = [self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter]]",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
color = self.colormap(self.norm(self.colorIndex[c])),0
"circle = patches.Circle((self.points[0], self.points[1]), radius=self.markerSize, fc = color, alpha=self.alpha)",0
#                    print(circle),0
print(self.axes),0
self.axes.add_patch(circle),0
"self.dr = auxfun_drag_label_multiple_individuals.DraggablePoint(circle,ind,self.multibodyparts[c])",0
self.dr.connect(),0
self.dr.coords = image_points,0
self.drs.append(self.dr),0
self.updatedCoords.append(self.dr.coords),0
if np.isnan(self.points)[0] == False:,0
self.buttonCounter[ind].append(self.multibodyparts[c]),0
,0
MainFrame.saveEachImage(self),0
self.figure.canvas.draw(),0
return(self.buttonCounter),0
print(self.dataFrame),0
Windows compatible,0
Checks if zoom/pan button is ON,0
matplotlib.use('Agg'),0
Load updated lists:,0
cfg['video_sets'][vn],0
"labeling_toolbox.show(config,Screens,scale_w,scale_h, winHack, img_scale)",0
plotting parameters:,0
folders = [Path(config).parent / 'labeled-data' /Path(i) for i in video_names],0
plt.savefig(str(Path(tmpfolder)/imagename.split(os.sep)[-1])),0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Let's check if the code is *not* run on windows (Source: #https://stackoverflow.com/questions/1325581/how-do-i-check-if-im-running-on-windows-in-python),0
but the paths are in windows format...,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(index,name.split(os.sep)[1])",0
"print(name,test_video_name)",0
Check that points lie within the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
"print(trainIndexes,testIndexes, Shuffles, augmenter_type,net_type)",0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
sort the models directories,0
get the shuffle index,0
read cfg file,0
create log file,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
"text = str(self.individual_names+','+self.bodyParts)",0
Check for variable correctness,0
Moviepy:,0
"print(""DLC loaded in light mode; you cannot use the labeling GUI!"")",0
"def __init__(self, point,bodyParts):",0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
text = str(self.bodyParts),0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
"sess, inputs, outputs = predict.setup_pose_prediction(dlc_cfg)",0
#################################################,0
Datafolder,0
#################################################,0
looping over videos,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
from skimage.io import imread,0
"frame=imread(os.path.join(directory,framename),mode='RGB')",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/AlexEMG/DeepLabCut/pull/324",0
Parameters for augmentation with regard to cropping,0
"Added and described in ""Using DeepLabCut for 3D markerless pose estimation across species and behaviors""",0
Source: https://www.nature.com/articles/s41596-019-0176-0,0
"limit width  [-leftwidth*u-100,100+u*rightwidth] x [-bottomwith*u-100,100+u*topwidth] where u is always a (different) random number in unit interval",0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
Dependencies for anaysis,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"DLCscorer = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
import skimage.color,0
from skimage.io import imread,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
image = skimage.color.gray2rgb(image),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
"df.to_hdf(output_path.replace('csv', 'h5'), 'df_with_missing', format='table', mode='w')",0
from scipy.misc import imresize,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
Read file path for pose_config file. >> pass it on,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
print(end_points.keys()) >> to see what else is available.,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
from nets.mobilenet import conv_blocks as ops,0
from nets.mobilenet import mobilenet as lib,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
The next part of the code depends upon which tensorflow version you have.,0
if cfg.location_refinement:,0
assuming batchsize 1 here!,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
from scipy.misc import imresize,0
Augmentation functions,0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
Initializing variables if they don't exist...,0
what is the fraction of training samples with scaling augmentation?,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images into same size.,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
"from scipy.misc import imread, imresize",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
"from scipy.misc import imread, imresize",0
"from dataset.pose_dataset import Batch, data_to_input, mirror_joints_map, CropImage, DataItem",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
Process skeleton,0
save,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"deeplabcut.create_labeled_video(path_config_file,[newvideo], destfolder=dfolder,filtered=True)",0
TENSORPACK could fail in WINDOWS...,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
#create one split and make Shuffle 2 and 3 have the same split.,0
##Note that the new function in DLC 2.1 does that much easier...,0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Dowloading the calibration images,0
Deleting unneccesary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Importing the toolbox (takes several seconds),0
Loading example data set,0
"print(""Analyze Video"")",0
"videofile_path = os.path.join(os.getcwd(),'openfield-Pranav-2018-10-30','videos','m3v1mp4.mp4')",0
"deeplabcut.analyze_videos(path_config_file,[videofile_path], shuffle=shuffle)",0
"print(""Create Labeled Video"")",0
"deeplabcut.create_labeled_video(path_config_file,[videofile_path],save_frames=False, shuffle=shuffle)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
Suppress tensorflow warning messages,0
DLClight version does not support GUIs. Importing accordingly,0
"Train, evaluate & predict functions / all require TF",0
"with open(saveasfile, ""w"") as f:",0
"yaml.dump(dict_test, f)",0
TBD: 'partaffinityfield_graph' >> use to set skeleton!,0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Updating config file:,0
Create the pose_config.yaml files,0
downloading base encoder / not required unless on re-trains (but when a training set is created this happens anyway),0
"model_path, num_shuffles=auxfun_models.Check4weights(pose_cfg['net_type'], parent_path, num_shuffles= 1)",0
Updating training and test pose_cfg:,0
Read the config file,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
get vcap property,0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
common parameters:,0
Write dictionary to yaml  config file,0
import yaml,0
import cv2,0
import shutil,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
Flatten the list of bodyparts to connect,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Start plotting for every frame,0
"Once all the frames are saved, then make a movie using ffmpeg.",0
Create the fig and define the axes,0
Clear plot and initialize the variables,0
Initialize arrays for appending the 3d data for actual plotting the points,0
Initialize arrays for appending the 3d data for drawing the lines,0
Initialize arrays for appending the 2d data from cam1 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam2 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam1 for drawing the lines,0
Initialize arrays for appending the 2d data from cam2 for drawing the lines,0
Get the scorer names from the dataframe,0
"Set the x,y, and z limits for the 3d view",0
"Set the frame number to read#max(0,index-trailpoints):index",0
Plot the labels for each body part,0
Connecting the bodyparts specified in the config file.3d file is created based on the likelihoods of cam1 and cam2. Using 3d file and check if the body part is nan then dont plot skeleton,0
Saving the frames,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
Extract the indices of frames where the likelihood of a bodypart for both cameras are less than pvalue,0
Extract frames where likelihood for both the views is less than the pcutoff,0
"low_likelihood_frames = np.all(likelihoods < pcutoff, axis=1)",0
For cam1 camera: Assign nans to x and y values of a bodypart where the likelihood for is less than pvalue,0
For cam2 camera: Assign nans to x and y values of a bodypart where the likelihood is less than pvalue,0
ToDo: speed up func. below by saving in numpy.array,1
"Create an empty dataframe to store x,y,z of 3d data",0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Undistorting the points from cam1 camera,0
Undistorting the points from cam2 camera,0
Save the undistorted files,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = io.imread(img),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
topSplitter = wx.SplitterWindow(self),0
,0
"self.image_panel = ImagePanel(topSplitter, config,video,shuffle,Dataframe,self.gui_size)",0
self.widget_panel = WidgetPanel(topSplitter),0
,0
"topSplitter.SplitHorizontally(self.image_panel, self.widget_panel,sashPosition=self.gui_size[1]*0.83)#0.9",0
topSplitter.SetSashGravity(1),0
sizer = wx.BoxSizer(wx.VERTICAL),0
"sizer.Add(topSplitter, 1, wx.EXPAND)",0
self.SetSizer(sizer),0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
self.cropping = False,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
"cv2.imwrite(img_name, frame)",0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.buttonCounter = [],0
self.bodyparts = [],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
Checks if zoom/pan button is ON,0
Alternatively,0
"Store tracklets, such that we later manipulate long chains",0
"rather than data of individual frames, yielding greater continuity.",0
Recursively fill the data containers,0
"Where slots are available, copy the data over",0
"If about to overwrite data, keep tracklets with highest confidence",0
Squeeze some data into empty slots,0
"For the remaining data, overwrite where we are least confident",0
Find keypoints closest to the remaining data,0
Only overwrite if improving confidence,0
Fill existing gaps and slightly smooth the tracklets,0
Retrieve original individual label indices,0
Map a tracklet # to the animal ID it belongs to or the bodypart # it corresponds to.,0
ID swaps occur when X and Y simultaneously intersect each other.,0
Get only those bodypart pairs that belong to different individuals,0
Take into consideration imprecise OpenCV estimation of total number of frames,0
Automatically disable the draggable points,0
Save additional frames to the labeled-data directory,0
Store the newly-refined data,0
Merge with the already existing annotated data,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
average distance and average # significant differences avg. over comparisonbodyparts,0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
"self.choiceBox,self.visualization_rdb = self.choice_panel.addRadioButtons()",0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
print(self.Dataframe.head()),0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
Checks if zoom/pan button is ON,0
"print(""DLC loaded in light mode; you cannot use the relabeling GUI!"")",0
Find uncropped labeled data,0
###################################################,0
Dependencies,0
###################################################,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as specified in the config file,0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
Prepare figure,0
if notanalyzed:,0
"notanalyzed,outdataname,sourcedataname,DLCscorer=auxiliaryfunctions.CheckifPostProcessing(folder,vname,DLCscorer,DLCscorerlegacy,suffix='checking')",0
cropping!,0
nx=X2-X1,0
!/usr/bin/env python3,0
"Historically DLC used: from scipy.misc import imread, imresize >> deprecated functions",0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
TODO check if those times exist...,1
"subprocess.call(['ffmpeg','-i',vname,'-ss',str(start),'-to',str(stop),'-c:v','copy','-c:a', newfilename])",0
Read the video until a frame is successfully read,0
"Windows throws error if file path is > 260 characters, can fix with prefix.",0
See https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation,0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
"Attention this order has to be consistent (for training set creation, training, inference etc.)",0
print(lookupdict),0
"print(link,lookupdict[link[0]])",0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
if suffix=='_full': #save metadata!,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
load defaults,0
set project specific parameters:,0
###################################################,0
Dependencies,0
###################################################,0
Pose X vs pose Y,0
Poses vs time,0
Likelihoods,0
Histograms,0
#################################################,0
Looping analysis over video,0
#################################################,0
Keep only the individuals and bodyparts that were labeled,0
Determine whether the data are single- or multi-animal without loading into memory,0
simply by checking whether 'individuals' is in the second line of the CSV.,0
Is there a scorer for this?,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
update path to current location of config.yaml (if different),0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
Ensure same order as in config.yaml,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
Iterate over data files and stop as soon as one matching the scorer is found,0
If there was no match...,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
self.cfg = auxiliaryfunctions.read_config(config),0
"self.statusbar.SetStatusText("""")",0
"dlg = wx.MessageDialog(None,""Are you sure?"", ""Quit!"",wx.YES_NO | wx.ICON_WARNING)",0
result = dlg.ShowModal(),0
if result == wx.ID_YES:,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
NEW ROW:,0
self.save.Enable(False),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0,10), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=10)",0
Add all the options,0
line = wx.StaticLine(self),0
"self.sizer.Add(line, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=10)",0
,0
"self.multi_choice.Bind(wx.EVT_CHECKBOX,self.activate_copy_videos)",0
Hide the button as this is not the default option,0
self.vids.Enable(False),0
self.SetSizer(self.sizer),0
"self.sizer.Add(self.sizer, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=15)",0
self.ok.Enable(False),0
else:,0
self.ok.Enable(True),0
For mac compatibility,0
Remove the pages in case the user goes back to the create new project and creates/load a new project,0
Add all the other pages,0
self.yes.Enable(False),0
self.no.Enable(False),0
"wx.Frame.__init__(self, None, title=""DeepLabCut"")",0
"wx.Frame.__init__( self, None, id = wx.ID_ANY, title = 'DeepLabCut',size = wx.Size(self.gui_size), pos = wx.DefaultPosition, style = wx.RESIZE_BORDER|wx.DEFAULT_FRAME_STYLE|wx.TAB_TRAVERSAL )",0
Here we create a panel and a notebook on the panel,0
create the page windows as children of the notebook and add the pages to the notebook with the label to show on the tab,0
variable initilization,0
design the panel,0
Add logo of DLC,0
self.filelist = self.filelist + self.vids,0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
use the default pose_cfg file for default values,0
self.display_iters.Enable(False),0
self.save_iters.Enable(False),0
self.max_iters.Enable(False),0
self.snapshots.Enable(False),0
Read the pose config file,0
"print(os.path.join(cfg['project_path'],auxiliaryfunctions.GetModelFolder(trainFraction, self.shuffles.GetValue(),cfg),'train','pose_cfg.yaml'))",0
let the user open the file with default text editor. Also make it mac compatible,0
update the variables with the edited values in the pose config file,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"if self.cfg.get(""multianimalproject"", False):",0
pass,0
else:,0
removing this as several downstream maDLC steps don't support dest_folder at this time:,0
"destfolder_text = wx.StaticBox(self, label=""Specify destination folder"")",0
"destfolderboxsizer = wx.StaticBoxSizer(destfolder_text, wx.VERTICAL)",0
self.change_workingdir = wx.CheckBox(,0
"self, label=""optional destination folder""",0
),0
self.hbox2.Add(self.change_workingdir),0
"self.change_workingdir.Bind(wx.EVT_CHECKBOX, self.activate_change_wd)",0
"self.sel_wd = wx.Button(self, label=""Browse"")",0
self.sel_wd.Enable(False),0
"self.sel_wd.Bind(wx.EVT_BUTTON, self.select_destfolder)",0
"self.hbox2.Add(self.sel_wd, 0, wx.ALL, -1)",0
"self.sizer.Add(boxsizer, pos=(4, 0), span=(1, 5),flag=wx.EXPAND|wx.TOP|wx.LEFT|wx.RIGHT , border=10)",0
"boxsizer.Add(self.hbox1,0, 5)",0
Read the infer config file,0
let the user open the file with default text editor. Also make it mac compatible,0
"def video_tracklets(self,event):",0
shuffle = self.shuffle.GetValue(),0
trainingsetindex = self.trainingset.GetValue(),0
"deeplabcut.create_video_from_pickled_tracks(self.filelist, picklefile, pcutoff=0.6)",0
"self.select_destfolder.SetPath(""None"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.shuffles.SetValue(""1"")",0
"DownSampleVideo(vname,width=-1,height=200,outsuffix='downsampled',outpath=None,rotateccw=False):",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.videotype.SetStringSelection("".avi"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"if self.plot_scoremaps.GetStringSelection() == ""Yes"":",0
Read the infer config file,0
let the user open the file with default text editor. Also make it mac compatible,0
shuffle = self.shuffle.GetValue(),0
Read from edited inf. file first ...,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"icon = wx.StaticBitmap(self, bitmap=wx.Bitmap(logo))",0
"self.sizer.Add(icon, pos=(0, 4), flag=wx.TOP|wx.RIGHT|wx.ALIGN_RIGHT,border=5)",0
variable initilization,0
design the panel,0
Add logo of DLC,0
#         design the panel,0
Add image of DLC,0
if editing this text make sure you add the '\n' to get the new line. The sizer is unable to format lines correctly.,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
convert the image to RGB as you are showing the image with matplotlib,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Checks if zoom/pan button is ON,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = cv2.imread(img),0
convert the image to RGB as you are showing the image with matplotlib,0
divider = make_axes_locatable(self.axes),0
"colorIndex = np.linspace(np.min(im),np.max(im),len(bodyparts))",0
"cax = divider.append_axes(""right"", size=""5%"", pad=0.05)",0
"cbar = self.figure.colorbar(ax, cax=cax,spacing='proportional', ticks=colorIndex)",0
cbar.set_ticklabels(bodyparts[::-1]),0
self.figure.canvas.draw(),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"MainFrame.saveDataSet(self, event)",0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Create an empty data frame,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of old labels,0
Extracting the list of old unique labels,0
Checking if new labels are added or not,0
Checking if user added a new label,0
Found new labels in either multiple bodyparts or unique bodyparts,0
check if single is slected when radio buttons are changed,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
print(self.dataFrame),0
Windows compatible,0
Checks if zoom/pan button is ON,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
actualbpts=set(Data.columns.get_level_values(0)),0
utility function to split different crops from same image into either train or test!,0
loading & linking pretrained models,0
CURRENTLY ONLY ResNet supported!,0
multianimal case:,0
"ATTENTION: order has to be multibodyparts, then uniquebodyparts (for indexing)",0
Loading the encoder (if necessary downloading from TF),0
Map back to the original indices.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
load image to get dimensions:,0
"print ""Grayscale!""",0
if socialbdpt in actualbpts:,0
if socialbdpt in actualbpts:,0
Drop missing body parts,0
Drop points lying outside the image,0
###############################################################################,0
Saving metadata and data file (Pickle file),0
###############################################################################,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Setting inference cfg file:,0
"TODO:   ""distnormalization"":  could be calculated here based on data and set",0
>> now we calculate this during evaluation (which is a good spot...),0
Load updated lists:,0
Find the corresponding video file,0
folders = [Path(config).parent / 'labeled-data' /Path(i[1]) for i in video_names],0
Avoid cropping already cropped images,0
"moving old entry to _original, dropping it from video_set and update crop parameters",0
adding important values for multianiaml project:,0
"When concatenating DataFrames with misaligned column labels,",0
all sorts of reordering may happen (mainly depending on 'sort' and 'join'),0
Ensure the 'bodyparts' level agrees with the order in the config file.,0
Let's check if the code is *not* run on windows (Source: #https://stackoverflow.com/questions/1325581/how-do-i-check-if-im-running-on-windows-in-python),0
but the paths are in windows format...,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(index,name.split(os.sep)[1])",0
"print(name,test_video_name)",0
Check that points lie within the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
updating variable if null/None! #backwardscompatability,0
Loading the encoder (if necessary downloading from TF),0
"print(trainIndexes,testIndexes, Shuffles, augmenter_type,net_type)",0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
sort the models directories,0
get the shuffle index,0
read cfg file,0
create log file,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
"text = str(self.individual_names+','+self.bodyParts)",0
Check for variable correctness,0
Moviepy:,0
"print(""DLC loaded in light mode; you cannot use the labeling GUI!"")",0
"def __init__(self, point,bodyParts):",0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
text = str(self.bodyParts),0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
"sess, inputs, outputs = predict.setup_pose_prediction(dlc_cfg)",0
#################################################,0
Looping over videos,0
#################################################,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
from skimage.io import imread,0
"frame=imread(os.path.join(directory,framename),mode='RGB')",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
TODO: addd cropping as in video analysis!,1
if cropping is not None:,0
cfg['cropping']=True,0
"cfg['x1'],cfg['x2'],cfg['y1'],cfg['y2']=cropping",0
"print(""Overwriting cropping parameters:"", cropping)",0
"print(""These are used for all videos, but won't be save to the cfg file."")",0
Check which snapshots are available and sort them by # iterations,0
Name for scorer:,0
#################################################,0
Looping over videos,0
#################################################,0
"NOTE: this can be used if only a subset is relevant. I.e. [0,1] for only first and second joint!",0
TODO: adjust this for multi + unique bodyparts!,1
this is only for multianimal parts and uniquebodyparts as one (not one uniquebodyparts guy tracked etc. ),0
get corresponding bounding boxes!,0
Test whether the unique bodyparts have been assembled,0
TODO Perhaps easier to check whether links were defined in the PAF graph?,1
Find an unused tracklet ID for the 'unique' bodyparts,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"print(""If the tracking is not satisfactory for some videos, consider expanding the training set. You can use the function 'extract_outlier_frames' to extract any outlier frames!"")",0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/AlexEMG/DeepLabCut/pull/324",0
for DLC 2.2.,0
Parameters for augmentation with regard to cropping,0
"Added and described in ""Using DeepLabCut for 3D markerless pose estimation across species and behaviors""",0
Source: https://www.nature.com/articles/s41596-019-0176-0,0
"limit width  [-leftwidth*u-100,100+u*rightwidth] x [-bottomwith*u-100,100+u*topwidth] where u is always a (different) random number in unit interval",0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
Name for deeplabcut net (based on its parameters),0
"DLCscorer,DLCscorerlegacy = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"notanalyzed, resultsfilename, DLCscorer=auxiliaryfunctions.CheckifNotEvaluated(str(evaluationfolder),DLCscorer,DLCscorerlegacy,Snapshots[snapindex])",0
"print(""Extracting maps for "", DLCscorer, "" with # of trainingiterations:"", trainingsiterations)",0
if notanalyzed: #this only applies to ask if h5 exists...,0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
return DATA,0
"print(np.shape(image),np.shape(scmap),np.shape(locref),np.shape(paf))",0
"dest_folder = os.path.join(cfg['project_path'], 'maps')",0
Dependencies for anaysis,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Load meta data & annotations,0
get the graph!,0
plt.figure(),0
"source=np.array(Data[ind,j1,'x'][jj],Data[ind,j1,'y'][jj])",0
"target=np.array(Data[ind2,j1,'x'][jj],Data[ind2,j2,'y'][jj])",0
"NOTE: the inter-animal distances are currently not used, but are interesting to compare to intra_*",0
"plt.subplot(len(partaffinityfield_graph),1,pi+1)",0
"plt.hist(ds_within,bins=np.linspace(0,100,21),color='red')",0
"plt.hist(ds_across,bins=np.linspace(0,100,21),color='blue')",0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
TODO: Make this code not so redundant!,1
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
"df.to_hdf(output_path.replace('csv', 'h5'), 'df_with_missing', format='table', mode='w')",0
Dependencies for anaysis,0
#################################################,0
Load data...,0
#################################################,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
TODO: IMPLEMENT for different batch sizes?,1
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
print(dlc_cfg),0
Specifying state of model (snapshot / training state),0
"Storing GT data as dictionary, so it can be used for calculating connection costs",0
returning to intial folder,0
wild guesses for a wide range:,0
Check which snapshots are available and sort them by # iterations,0
update number of individuals to retain.,0
calculating result at best best solution,0
"print(""Quantification:"", DataOptParams.head())",0
Store best predictions,0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
"[_, loss_val, summary] = sess.run([train_op, total_loss, merged_summaries],feed_dict={learning_rate: current_lr})",0
Save snapshot,0
return to original path.,0
from scipy.misc import imresize,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
from deeplabcut.pose_estimation_tensorflow.predict_multianimal import convert_detections2tracklet,0
initializing constants,0
"PredicteData['frame'+str(counter)]=predict.get_detectionswithcosts(frame, dlc_cfg, sess, inputs, outputs, outall=False,nms_radius=dlc_cfg.nmsradius,det_min_score=dlc_cfg.minconfidence)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"PredicteData[batch_num*batchsize:(batch_num+1)*batchsize, :] = pose",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
"PredicteData[batch_num*batchsize:batch_num*batchsize+batch_ind, :] = pose[:batch_ind,:]",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"PredicteData[batch_num*batchsize:(batch_num+1)*batchsize, :] = pose",0
print(PredicteData),0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
print(end_points.keys()) >> to see what else is available.,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
from nets.mobilenet import conv_blocks as ops,0
from nets.mobilenet import mobilenet as lib,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
The next part of the code depends upon which tensorflow version you have.,0
assuming batchsize 1 here!,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
setting pairwise bodypart loss,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
get coordinates for bp1 and bp2,0
'm2',0
"distscalarproduct=np.zeros((len(C1),len(C2)))*np.nan",0
"distscalarproduct[c1i,c2i]=dy*v[1]+dx*v[0] #scalar product [v unit vector dx,dy in pixel coordinats from partaffinitymap]",0
Distances[l]['m2'] = distscalarproduct,0
TODO: compute all the grids etc. once for the video analysis method,1
(and then just pass on the variables),0
apply nms,0
"IMPORTANT, as C++ function expects row-major",0
These two functions are for evaluation specifically (one also calculates integral between gt points),0
get dist_grid,0
apply nms,0
"IMPORTANT, as C++ function expects row-major",0
# Functions below implement are for batch sizes > 1:,0
get dist_grid,0
apply nms,0
"IMPORTANT, as C++ function expects row-major",0
"batchsize,ny,nx,num_joints = scmap.shape",0
The next part of the code depends upon which tensorflow version you have.,0
"return self.prediction_layers(net, end_points)",0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Augmentation functions,0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
Initializing variables if they don't exist...,0
what is the fraction of training samples with scaling augmentation?,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
Pickle the 'data' dictionary using the highest protocol available.,0
item.numanimals=len(item.joints)-1 #as there are also the parts that are not per animal,0
Scale is sampled only once (per batch) to transform all of the images into same size.,0
assert len(batch_images) == self.batch_size,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
"imageio.imwrite(data_items[i].im_path.split('/')[-1],im)",0
if returndata:,0
"return batch_images,batch_joints,targetmaps",0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
"print(person_id,joint_ids,data_item.im_path.split('/')[-1])",0
if self.cfg.partaffinityfield_predict:,0
"print(""hello"",joint_id)",0
print(np.concatenate(joint_id)) #this is all joint_ids for all individuals!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
"plt.savefig(""along""+str(data_item.im_path.split('/')[-1]))",0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
"print(person_id,k, j_id, coords[0])",0
assert(0==1),0
"A[:,:,2]=partaffinityfield_map[:,:,j]*0",0
"A[:,:,2]=partaffinityfield_map[:,:,j]*0",0
"plt.savefig(""SomePafs""+str(data_item.im_path.split('/')[-1]))",0
WIP!,0
STD of gaussian is 1/4 of threshold,0
Grid of coordinates,0
the animal id plays no role for scoremap + locref!,0
so let's just loop over all bpts.,0
Distance between the joint point and each coordinate,0
NEEDS TO BE DONE!,0
"for k, joint_ids in enumerate(joint_id[person_id]):",0
CONSIDER SMARTER SEARCHES here... (i.e. calculate the bpts beforehand?),0
mask3 = ((x >= 0) & (x <= width-1)),0
mask4 = ((y >= 0) & (y <= height-1)),0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images into same size.,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
"from scipy.misc import imread, imresize",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
"from scipy.misc import imread, imresize",0
"from dataset.pose_dataset import Batch, data_to_input, mirror_joints_map, CropImage, DataItem",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
Get rid of the interpolation beyond the spline knots,0
Retrieve original individual label indices,0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
Process skeleton,0
save,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"posefile=os.path.join(cfg['project_path'],'dlc-models/iteration-'+str(cfg['iteration'])+'/'+ cfg['Task'] + cfg['date'] + '-trainset' + str(int(cfg['TrainingFraction'][0] * 100)) + 'shuffle' + str(1),'train/pose_cfg.yaml')",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"deeplabcut.create_labeled_video(path_config_file,[newvideo], destfolder=dfolder,filtered=True)",0
TENSORPACK could fail in WINDOWS...,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
#create one split and make Shuffle 2 and 3 have the same split.,0
##Note that the new function in DLC 2.1 simplifies network/augmentation comparisons greatly:,0
legacy mode:,0
new way:,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Dowloading the calibration images,0
Deleting unneccesary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Importing the toolbox (takes several seconds),0
Loading example data set,0
example how to set pose config variables:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
Supress tensorflow warning messages,0
DLClight version does not support GUIs. Importing accordingly,0
"Train, evaluate & predict functions / require TF",0
docs[0]['init_weights'] = '../../pretrained/resnet_v1_101.ckpt',0
dict_test['init_weights'] = 'models/mpii/snapshot-1030000',0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Create the pose_config.yaml files,0
Analyze the videos,0
Read the config file,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
get vcap property,0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Silently sweep the files that were already written.,0
Set values to config file:,0
cfg_file['resnet']=50,0
Write dictionary to yaml  config file,0
import yaml,0
import cv2,0
import shutil,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
Flatten the list of bodyparts to connect,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Start plotting for every frame,0
"Once all the frames are saved, then make a movie using ffmpeg.",0
Create the fig and define the axes,0
Clear plot and initialize the variables,0
Initialize arrays for appending the 3d data for actual plotting the points,0
Initialize arrays for appending the 3d data for drawing the lines,0
Initialize arrays for appending the 2d data from cam1 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam2 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam1 for drawing the lines,0
Initialize arrays for appending the 2d data from cam2 for drawing the lines,0
Get the scorer names from the dataframe,0
"Set the x,y, and z limits for the 3d view",0
"Set the frame number to read#max(0,index-trailpoints):index",0
Plot the labels for each body part,0
Connecting the bodyparts specified in the config file.3d file is created based on the likelihoods of cam1 and cam2. Using 3d file and check if the body part is nan then dont plot skeleton,0
Saving the frames,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
Extract the indices of frames where the likelihood of a bodypart for both cameras are less than pvalue,0
Extract frames where likelihood for both the views is less than the pcutoff,0
"low_likelihood_frames = np.all(likelihoods < pcutoff, axis=1)",0
For cam1 camera: Assign nans to x and y values of a bodypart where the likelihood for is less than pvalue,0
For cam2 camera: Assign nans to x and y values of a bodypart where the likelihood is less than pvalue,0
ToDo: speed up func. below by saving in numpy.array,1
"Create an empty dataframe to store x,y,z of 3d data",0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Undistorting the points from cam1 camera,0
Undistorting the points from cam2 camera,0
Save the undistorted files,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = io.imread(img),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
self.cropping = False,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
"cv2.imwrite(img_name, frame)",0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.buttonCounter = [],0
self.bodyparts = [],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
Checks if zoom/pan button is ON,0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
extract min and max index based on start stop interval.,0
figure out body part list:,0
all indices between start and stop with jump larger than epsilon (leading up to this point!),0
deviation_dataname = str(Path(videofolder)/Path(dataname)),0
Calculate deviatons for video,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
data.to_csv(dataname.split('.h5')[0]+'filtered.csv'),0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
"Dataframe = pd.read_hdf(os.path.join(videofolder,dataname+'.h5'))",0
drop duplicate labels:,0
updates iteration by 1,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
"self.choiceBox,self.visualization_rdb = self.choice_panel.addRadioButtons()",0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
print(self.Dataframe.head()),0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
Checks if zoom/pan button is ON,0
"print(""DLC loaded in light mode; you cannot use the relabeling GUI!"")",0
###################################################,0
Dependencies,0
###################################################,0
import sys,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as specified in the config file,0
"rr, cc,val = line_aa(int(df_y[pair[0],index]),int(df_x[pair[0],index]),int(df_y[pair[1],index]), int(df_x[pair[1],index]))",0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Adds skeleton to the video,0
###################,0
less transparent present.,0
# One can change the parameters of the video creation script below:,0
See ffmpeg user guide: http://ffmpeg.org/ffmpeg.html#Video-and-Audio-file-format-conversion,0
,0
if notanalyzed:,0
"notanalyzed,outdataname,sourcedataname,DLCscorer=auxiliaryfunctions.CheckifPostProcessing(folder,vname,DLCscorer,DLCscorerlegacy,suffix='checking')",0
Loading cropping data used during analysis,0
!/usr/bin/env python3,0
"Historically DLC used: from scipy.misc import imread, imresize >> deprecated functions",0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
TODO check if those times exist...,1
"windows throws error if file path is > 260 characters, can fix with prefix. see https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation",0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
###################################################,0
Dependencies,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
"fig.colorbar(im, ax=ax)",0
#################################################,0
Looping analysis over video,0
#################################################,0
LoadData,0
"nlines,numcolumns=data.shape",0
assert(len(orderofbpincsv)==len(cfg['bodyparts'])),0
TODO: test len of images vs. len of imagenames for another sanity check,1
Is there a scorer for this?,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
"with open(projconfigfile, 'w') as cf:",0
"ruamelFile_3d.dump(cfg_file_3d, cf)",0
Read the pickle file,0
Write the pickle file,0
checks if input is a directory,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
variable initilization,0
design the panel,0
Add logo of DLC,0
Add all the options,0
,0
Hide the button as this is not the default option,0
self.SetSizer(self.sizer),0
"self.sizer.Add(self.sizer, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=15)",0
self.ok.Enable(False),0
else:,0
self.ok.Enable(True),0
For mac compatibility,0
Remove the pages in case the user goes back to the create new project and creates/load a new project,0
Add all the other pages,0
"wx.Frame.__init__(self, None, title=""DeepLabCut"")",0
"wx.Frame.__init__( self, None, id = wx.ID_ANY, title = 'DeepLabCut',size = wx.Size(self.gui_size), pos = wx.DefaultPosition, style = wx.RESIZE_BORDER|wx.DEFAULT_FRAME_STYLE|wx.TAB_TRAVERSAL )",0
Here we create a panel and a notebook on the panel,0
create the page windows as children of the notebook and add the pages to the notebook with the label to show on the tab,0
variable initilization,0
design the panel,0
Add logo of DLC,0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
use the default pose_cfg file for default values,0
self.display_iters.Enable(False),0
self.save_iters.Enable(False),0
self.max_iters.Enable(False),0
self.snapshots.Enable(False),0
Read the pose config file,0
"print(os.path.join(cfg['project_path'],auxiliaryfunctions.GetModelFolder(trainFraction, self.shuffles.GetValue(),cfg),'train','pose_cfg.yaml'))",0
let the user open the file with default text editor. Also make it mac compatible,0
update the variables with the edited values in the pose config file,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.shuffles = wx.SpinCtrl(self, value='1',min=1,max=100)",0
"shuffles_text_boxsizer.Add(self.shuffles,1, wx.EXPAND|wx.TOP|wx.BOTTOM, 10)",0
if self.draw_skeleton.IsShow():,0
"if self.draw_skeleton.GetStringSelection() == ""Yes"":",0
self.draw = True,0
else:,0
self.draw = True,0
"print(self.config,self.filelist,self.videotype.GetValue(),shuffle,trainingsetindex,gputouse=None,save_as_csv=save_as_csv,destfolder=self.destfolder,cropping=cropping)",0
self.gpu.SetValue(-1),0
import wx.lib.scrolledpanel as SP,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
self.hbox2 = wx.BoxSizer(wx.HORIZONTAL),0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
"boxsizer.Add(self.hbox2,5, wx.EXPAND|wx.TOP|wx.BOTTOM, 10)",0
shuffle = self.shuffle.GetValue(),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.sel_config.SetPath(""Please select the config file"")",0
else:,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
#         design the panel,0
Add image of DLC,0
if editing this text make sure you add the '\n' to get the new line. The sizer is unable to format lines correctly.,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
convert the image to RGB as you are showing the image with matplotlib,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Checks if zoom/pan button is ON,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = cv2.imread(img),0
convert the image to RGB as you are showing the image with matplotlib,0
divider = make_axes_locatable(self.axes),0
"colorIndex = np.linspace(np.min(im),np.max(im),len(bodyparts))",0
"cax = divider.append_axes(""right"", size=""5%"", pad=0.05)",0
"cbar = self.figure.colorbar(ax, cax=cax,spacing='proportional', ticks=colorIndex)",0
cbar.set_ticklabels(bodyparts[::-1]),0
self.figure.canvas.draw(),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"MainFrame.saveDataSet(self, event)",0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Create an empty data frame,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of old labels,0
Extracting the list of old unique labels,0
Checking if new labels are added or not,0
Checking if user added a new label,0
Found new labels in either multiple bodyparts or unique bodyparts,0
check if single is slected when radio buttons are changed,0
Checking if user added a new label,0
if self.new_Multibodyparts==[] and self.new_UniqueBodyparts==[]: # i.e. no new labels,0
"self.figure,self.axes,self.canvas,self.toolbar,self.image_axis = self.image_panel.drawplot(self.img,img_name,self.iter,self.index,self.multibodyparts,self.colormap,keep_view=self.view_locked)",0
"self.axes.callbacks.connect('xlim_changed', self.onZoom)",0
"self.axes.callbacks.connect('ylim_changed', self.onZoom)",0
"self.image_panel.addcolorbar(self.img,self.image_axis,self.iter,self.multibodyparts,self.colormap)",0
# Only show the bodyparts corresponding to single,0
if self.individual_names[0] == 'single':,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.uniquebodyparts,self.individual_names,self.file,self.markerSize)",0
else:,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.multibodyparts,self.individual_names,self.file,self.markerSize)",0
,0
"self.individualrdb.Bind(wx.EVT_RADIOBOX,self.select_individual)",0
,0
"# Get the color index, depending on if single is selcted or not.",0
"if self.rdb.GetStringSelection==""single"":",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.uniquebodyparts)",0
else:,0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
"self.cidClick = self.canvas.mpl_connect('button_press_event', self.onClick)",0
else:,0
# Found new labels in either multiple bodyparts or unique bodyparts,0
"dlg = wx.MessageDialog(None,""New label found in the config file. Do you want to see all the other labels?"", ""New label found"",wx.YES_NO | wx.ICON_WARNING)",0
result = dlg.ShowModal(),0
if result == wx.ID_NO:,0
if self.new_Multibodyparts!=[]:,0
self.multibodyparts = self.new_Multibodyparts,0
if self.new_UniqueBodyparts!=[]:,0
self.uniquebodyparts = self.new_UniqueBodyparts,0
,0
"self.dataFrame = MainFrame.create_dataframe(self,self.dataFrame,self.relativeimagenames,self.individual_names,self.new_UniqueBodyparts,self.new_Multibodyparts)",0
"self.figure,self.axes,self.canvas,self.toolbar,self.image_axis = self.image_panel.drawplot(self.img,img_name,self.iter,self.index,self.multibodyparts,self.colormap,keep_view=self.view_locked)",0
"self.axes.callbacks.connect('xlim_changed', self.onZoom)",0
"self.axes.callbacks.connect('ylim_changed', self.onZoom)",0
"self.image_panel.addcolorbar(self.img,self.image_axis,self.iter,self.multibodyparts,self.colormap)",0
,0
if self.individual_names[0] == 'single':,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.uniquebodyparts,self.individual_names,self.file,self.markerSize)",0
else:,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.multibodyparts,self.individual_names,self.file,self.markerSize)",0
"self.individualrdb.Bind(wx.EVT_RADIOBOX,self.select_individual)",0
"if self.rdb.GetStringSelection==""single"":",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.uniquebodyparts)",0
else:,0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
"self.cidClick = self.canvas.mpl_connect('button_press_event', self.onClick)",0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
"def plot(self,img):",0
self.drs= [],0
self.updatedCoords = [],0
for ind in self.individual_names:,0
if self.are_unique_bodyparts_present == False: #no unique bodyparts present,0
#            for ind in self.individual_names:,0
image_points = [],0
"for c, bp in enumerate(self.multibodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
"self.points = [self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter]]",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
color = self.colormap(self.norm(self.colorIndex[c])),0
"circle = patches.Circle((self.points[0], self.points[1]), radius=self.markerSize, fc = color, alpha=self.alpha)",0
#                    print(circle),0
print(self.axes),0
self.axes.add_patch(circle),0
"self.dr = auxfun_drag_label_multiple_individuals.DraggablePoint(circle,ind,self.multibodyparts[c])",0
self.dr.connect(),0
self.dr.coords = image_points,0
self.drs.append(self.dr),0
self.updatedCoords.append(self.dr.coords),0
if np.isnan(self.points)[0] == False:,0
self.buttonCounter[ind].append(self.multibodyparts[c]),0
else:,0
if ind == 'single':,0
"for c, bp in enumerate(self.uniquebodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
"self.points = [self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter]]",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.uniquebodyparts)",0
color = self.colormap(self.norm(self.colorIndex[c])),0
"circle = patches.Circle((self.points[0], self.points[1]), radius=self.markerSize, fc = color, alpha=self.alpha)",0
#                    print(circle),0
print(self.axes),0
self.axes.add_patch(circle),0
"self.dr = auxfun_drag_label_multiple_individuals.DraggablePoint(circle,ind,self.uniquebodyparts[c])",0
self.dr.connect(),0
self.dr.coords = image_points,0
self.drs.append(self.dr),0
self.updatedCoords.append(self.dr.coords),0
if np.isnan(self.points)[0] == False:,0
self.buttonCounter[ind].append(self.uniquebodyparts[c]),0
else:,0
"for c, bp in enumerate(self.multibodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
"self.points = [self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter]]",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
color = self.colormap(self.norm(self.colorIndex[c])),0
"circle = patches.Circle((self.points[0], self.points[1]), radius=self.markerSize, fc = color, alpha=self.alpha)",0
#                    print(circle),0
print(self.axes),0
self.axes.add_patch(circle),0
"self.dr = auxfun_drag_label_multiple_individuals.DraggablePoint(circle,ind,self.multibodyparts[c])",0
self.dr.connect(),0
self.dr.coords = image_points,0
self.drs.append(self.dr),0
self.updatedCoords.append(self.dr.coords),0
if np.isnan(self.points)[0] == False:,0
self.buttonCounter[ind].append(self.multibodyparts[c]),0
,0
MainFrame.saveEachImage(self),0
self.figure.canvas.draw(),0
return(self.buttonCounter),0
print(self.dataFrame),0
Windows compatible,0
Checks if zoom/pan button is ON,0
matplotlib.use('Agg'),0
Load updated lists:,0
cfg['video_sets'][vn],0
"labeling_toolbox.show(config,Screens,scale_w,scale_h, winHack, img_scale)",0
plotting parameters:,0
folders = [Path(config).parent / 'labeled-data' /Path(i) for i in video_names],0
plt.savefig(str(Path(tmpfolder)/imagename.split(os.sep)[-1])),0
Let's check if the code is *not* run on windows (Source: #https://stackoverflow.com/questions/1325581/how-do-i-check-if-im-running-on-windows-in-python),0
but the paths are in windows format...,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(index,name.split(os.sep)[1])",0
"print(name,test_video_name)",0
Check that points lie within the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
"print(trainIndexes,testIndexes, Shuffles, augmenter_type,net_type)",0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
sort the models directories,0
get the shuffle index,0
read cfg file,0
create log file,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
"text = str(self.individual_names+','+self.bodyParts)",0
Check for variable correctness,0
Moviepy:,0
"print(""DLC loaded in light mode; you cannot use the labeling GUI!"")",0
"def __init__(self, point,bodyParts):",0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
text = str(self.bodyParts),0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
"sess, inputs, outputs = predict.setup_pose_prediction(dlc_cfg)",0
#################################################,0
Datafolder,0
#################################################,0
looping over videos,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
from skimage.io import imread,0
"frame=imread(os.path.join(directory,framename),mode='RGB')",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/AlexEMG/DeepLabCut/pull/324",0
Parameters for augmentation with regard to cropping,0
"Added and described in ""Using DeepLabCut for 3D markerless pose estimation across species and behaviors""",0
Source: https://www.nature.com/articles/s41596-019-0176-0,0
"limit width  [-leftwidth*u-100,100+u*rightwidth] x [-bottomwith*u-100,100+u*topwidth] where u is always a (different) random number in unit interval",0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
Deploy config.yaml - info about project origin:,0
Project path,0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
#######################,0
## find snapshot to use,0
#######################,0
Check which snapshots are available and sort them by # iterations,0
###################################,0
## Load and setup CNN part detector,0
###################################,0
Check if data already was generated:,0
load network,0
save graph to pbtxt file,0
create frozen graph from pbtxt file,0
## read config file,0
## load model,0
## set up export directory,0
## write pose config file,0
"sort dlc_cfg keys alphabetically, then save to pose_cfg.yaml in export directory",0
## copy checkpoint to export directory,0
## create pbtxt and pb files for checkpoint in export directory,0
## tar export directory,0
Dependencies for anaysis,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"DLCscorer = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
import skimage.color,0
from skimage.io import imread,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
image = skimage.color.gray2rgb(image),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
"df.to_hdf(output_path.replace('csv', 'h5'), 'df_with_missing', format='table', mode='w')",0
from scipy.misc import imresize,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
Read file path for pose_config file. >> pass it on,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
print(end_points.keys()) >> to see what else is available.,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
from nets.mobilenet import conv_blocks as ops,0
from nets.mobilenet import mobilenet as lib,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
The next part of the code depends upon which tensorflow version you have.,0
if cfg.location_refinement:,0
assuming batchsize 1 here!,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
from scipy.misc import imresize,0
Augmentation functions,0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
Initializing variables if they don't exist...,0
what is the fraction of training samples with scaling augmentation?,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images into same size.,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
"from scipy.misc import imread, imresize",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
"from scipy.misc import imread, imresize",0
"from dataset.pose_dataset import Batch, data_to_input, mirror_joints_map, CropImage, DataItem",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
Process skeleton,0
save,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"deeplabcut.create_labeled_video(path_config_file,[newvideo], destfolder=dfolder,filtered=True)",0
TENSORPACK could fail in WINDOWS...,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
#create one split and make Shuffle 2 and 3 have the same split.,0
##Note that the new function in DLC 2.1 does that much easier...,0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Dowloading the calibration images,0
Deleting unneccesary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Importing the toolbox (takes several seconds),0
Loading example data set,0
"print(""Analyze Video"")",0
"videofile_path = os.path.join(os.getcwd(),'openfield-Pranav-2018-10-30','videos','m3v1mp4.mp4')",0
"deeplabcut.analyze_videos(path_config_file,[videofile_path], shuffle=shuffle)",0
"print(""Create Labeled Video"")",0
"deeplabcut.create_labeled_video(path_config_file,[videofile_path],save_frames=False, shuffle=shuffle)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
Supress tensorflow warning messages,0
DLClight version does not support GUIs. Importing accordingly,0
"Train, evaluate & predict functions / require TF",0
docs[0]['init_weights'] = '../../pretrained/resnet_v1_101.ckpt',0
dict_test['init_weights'] = 'models/mpii/snapshot-1030000',0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Create the pose_config.yaml files,0
Analyze the videos,0
Read the config file,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
get vcap property,0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
"Import all videos in a folder or if just one video withouth [] passed, then make it a list.",0
"if isinstance(videos,str):",0
#there are two cases:,0
if os.path.isdir(videos): # it is a path!,0
path=videos,0
"videos=[os.path.join(path,vp) for vp in os.listdir(path) if videotype in vp]",0
if len(videos)==0:,0
"print(""No videos found in"",path,os.listdir(path))",0
"print(""Perhaps change the videotype, which is currently set to:"", videotype)",0
else:,0
"print(""Directory entered, "" , len(videos),"" videos were found."")",0
else:,0
if os.path.isfile(videos):,0
videos=[videos],0
https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
try:,0
"#shutil.copy(src,dst)",0
except OSError or TypeError: #https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
"shutil.copy(os.fspath(src),os.fspath(dst))",0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Set values to config file:,0
cfg_file['resnet']=50,0
Write dictionary to yaml  config file,0
import yaml,0
import cv2,0
import shutil,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
Flatten the list of bodyparts to connect,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Start plotting for every frame,0
"Once all the frames are saved, then make a movie using ffmpeg.",0
Create the fig and define the axes,0
Clear plot and initialize the variables,0
Initialize arrays for appending the 3d data for actual plotting the points,0
Initialize arrays for appending the 3d data for drawing the lines,0
Initialize arrays for appending the 2d data from cam1 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam2 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam1 for drawing the lines,0
Initialize arrays for appending the 2d data from cam2 for drawing the lines,0
Get the scorer names from the dataframe,0
"Set the x,y, and z limits for the 3d view",0
"Set the frame number to read#max(0,index-trailpoints):index",0
Plot the labels for each body part,0
Connecting the bodyparts specified in the config file.3d file is created based on the likelihoods of cam1 and cam2. Using 3d file and check if the body part is nan then dont plot skeleton,0
Saving the frames,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
Extract the indices of frames where the likelihood of a bodypart for both cameras are less than pvalue,0
Extract frames where likelihood for both the views is less than the pcutoff,0
"low_likelihood_frames = np.all(likelihoods < pcutoff, axis=1)",0
For cam1 camera: Assign nans to x and y values of a bodypart where the likelihood for is less than pvalue,0
For cam2 camera: Assign nans to x and y values of a bodypart where the likelihood is less than pvalue,0
ToDo: speed up func. below by saving in numpy.array,1
"Create an empty dataframe to store x,y,z of 3d data",0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Undistorting the points from cam1 camera,0
Undistorting the points from cam2 camera,0
Save the undistorted files,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = io.imread(img),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
self.cropping = False,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
"cv2.imwrite(img_name, frame)",0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.slider.Enable(False),0
self.checkBox.Enable(False),0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
Checks if zoom/pan button is ON,0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
extract min and max index based on start stop interval.,0
figure out body part list:,0
all indices between start and stop with jump larger than epsilon (leading up to this point!),0
deviation_dataname = str(Path(videofolder)/Path(dataname)),0
Calculate deviatons for video,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"The process is not stationary, but the default SARIMAX model tries to solve for such a distribution...",0
Relaxing those constraints should do the job.,0
data.to_csv(dataname.split('.h5')[0]+'filtered.csv'),0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
"Dataframe = pd.read_hdf(os.path.join(videofolder,dataname+'.h5'))",0
drop duplicate labels:,0
updates iteration by 1,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
"self.choiceBox,self.visualization_rdb = self.choice_panel.addRadioButtons()",0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
print(self.Dataframe.head()),0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
Checks if zoom/pan button is ON,0
"print(""DLC loaded in light mode; you cannot use the relabeling GUI!"")",0
###################################################,0
Dependencies,0
###################################################,0
import sys,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as specified in the config file,0
"rr, cc,val = line_aa(int(df_y[pair[0],index]),int(df_x[pair[0],index]),int(df_y[pair[1],index]), int(df_x[pair[1],index]))",0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Adds skeleton to the video,0
###################,0
less transparent present.,0
# One can change the parameters of the video creation script below:,0
See ffmpeg user guide: http://ffmpeg.org/ffmpeg.html#Video-and-Audio-file-format-conversion,0
,0
if notanalyzed:,0
"notanalyzed,outdataname,sourcedataname,DLCscorer=auxiliaryfunctions.CheckifPostProcessing(folder,vname,DLCscorer,DLCscorerlegacy,suffix='checking')",0
Loading cropping data used during analysis,0
!/usr/bin/env python3,0
"Historically DLC used: from scipy.misc import imread, imresize >> deprecated functions",0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
TODO check if those times exist...,1
"windows throws error if file path is > 260 characters, can fix with prefix. see https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation",0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
###################################################,0
Dependencies,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
"fig.colorbar(im, ax=ax)",0
#################################################,0
Looping analysis over video,0
#################################################,0
LoadData,0
"nlines,numcolumns=data.shape",0
assert(len(orderofbpincsv)==len(cfg['bodyparts'])),0
TODO: test len of images vs. len of imagenames for another sanity check,1
Is there a scorer for this?,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
"with open(projconfigfile, 'w') as cf:",0
"ruamelFile_3d.dump(cfg_file_3d, cf)",0
Read the pickle file,0
Write the pickle file,0
checks if input is a directory,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
variable initilization,0
design the panel,0
Add logo of DLC,0
Add all the options,0
,0
Hide the button as this is not the default option,0
self.SetSizer(self.sizer),0
"self.sizer.Add(self.sizer, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=15)",0
self.ok.Enable(False),0
else:,0
self.ok.Enable(True),0
For mac compatibility,0
Remove the pages in case the user goes back to the create new project and creates/load a new project,0
Add all the other pages,0
"wx.Frame.__init__(self, None, title=""DeepLabCut"")",0
"wx.Frame.__init__( self, None, id = wx.ID_ANY, title = 'DeepLabCut',size = wx.Size(self.gui_size), pos = wx.DefaultPosition, style = wx.RESIZE_BORDER|wx.DEFAULT_FRAME_STYLE|wx.TAB_TRAVERSAL )",0
Here we create a panel and a notebook on the panel,0
create the page windows as children of the notebook and add the pages to the notebook with the label to show on the tab,0
variable initilization,0
design the panel,0
Add logo of DLC,0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
use the default pose_cfg file for default values,0
self.display_iters.Enable(False),0
self.save_iters.Enable(False),0
self.max_iters.Enable(False),0
self.snapshots.Enable(False),0
Read the pose config file,0
"print(os.path.join(cfg['project_path'],auxiliaryfunctions.GetModelFolder(trainFraction, self.shuffles.GetValue(),cfg),'train','pose_cfg.yaml'))",0
let the user open the file with default text editor. Also make it mac compatible,0
update the variables with the edited values in the pose config file,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.shuffles = wx.SpinCtrl(self, value='1',min=1,max=100)",0
"shuffles_text_boxsizer.Add(self.shuffles,1, wx.EXPAND|wx.TOP|wx.BOTTOM, 10)",0
if self.draw_skeleton.IsShow():,0
"if self.draw_skeleton.GetStringSelection() == ""Yes"":",0
self.draw = True,0
else:,0
self.draw = True,0
"print(self.config,self.filelist,self.videotype.GetValue(),shuffle,trainingsetindex,gputouse=None,save_as_csv=save_as_csv,destfolder=self.destfolder,cropping=cropping)",0
self.gpu.SetValue(-1),0
import wx.lib.scrolledpanel as SP,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
self.hbox2 = wx.BoxSizer(wx.HORIZONTAL),0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
"boxsizer.Add(self.hbox2,5, wx.EXPAND|wx.TOP|wx.BOTTOM, 10)",0
shuffle = self.shuffle.GetValue(),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.sel_config.SetPath(""Please select the config file"")",0
else:,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
#         design the panel,0
Add image of DLC,0
if editing this text make sure you add the '\n' to get the new line. The sizer is unable to format lines correctly.,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
convert the image to RGB as you are showing the image with matplotlib,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Checks if zoom/pan button is ON,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = cv2.imread(img),0
convert the image to RGB as you are showing the image with matplotlib,0
divider = make_axes_locatable(self.axes),0
"colorIndex = np.linspace(np.min(im),np.max(im),len(bodyparts))",0
"cax = divider.append_axes(""right"", size=""5%"", pad=0.05)",0
"cbar = self.figure.colorbar(ax, cax=cax,spacing='proportional', ticks=colorIndex)",0
cbar.set_ticklabels(bodyparts[::-1]),0
self.figure.canvas.draw(),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"MainFrame.saveDataSet(self, event)",0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Create an empty data frame,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of old labels,0
Extracting the list of old unique labels,0
Checking if new labels are added or not,0
Checking if user added a new label,0
Found new labels in either multiple bodyparts or unique bodyparts,0
check if single is slected when radio buttons are changed,0
Checking if user added a new label,0
if self.new_Multibodyparts==[] and self.new_UniqueBodyparts==[]: # i.e. no new labels,0
"self.figure,self.axes,self.canvas,self.toolbar,self.image_axis = self.image_panel.drawplot(self.img,img_name,self.iter,self.index,self.multibodyparts,self.colormap,keep_view=self.view_locked)",0
"self.axes.callbacks.connect('xlim_changed', self.onZoom)",0
"self.axes.callbacks.connect('ylim_changed', self.onZoom)",0
"self.image_panel.addcolorbar(self.img,self.image_axis,self.iter,self.multibodyparts,self.colormap)",0
# Only show the bodyparts corresponding to single,0
if self.individual_names[0] == 'single':,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.uniquebodyparts,self.individual_names,self.file,self.markerSize)",0
else:,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.multibodyparts,self.individual_names,self.file,self.markerSize)",0
,0
"self.individualrdb.Bind(wx.EVT_RADIOBOX,self.select_individual)",0
,0
"# Get the color index, depending on if single is selcted or not.",0
"if self.rdb.GetStringSelection==""single"":",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.uniquebodyparts)",0
else:,0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
"self.cidClick = self.canvas.mpl_connect('button_press_event', self.onClick)",0
else:,0
# Found new labels in either multiple bodyparts or unique bodyparts,0
"dlg = wx.MessageDialog(None,""New label found in the config file. Do you want to see all the other labels?"", ""New label found"",wx.YES_NO | wx.ICON_WARNING)",0
result = dlg.ShowModal(),0
if result == wx.ID_NO:,0
if self.new_Multibodyparts!=[]:,0
self.multibodyparts = self.new_Multibodyparts,0
if self.new_UniqueBodyparts!=[]:,0
self.uniquebodyparts = self.new_UniqueBodyparts,0
,0
"self.dataFrame = MainFrame.create_dataframe(self,self.dataFrame,self.relativeimagenames,self.individual_names,self.new_UniqueBodyparts,self.new_Multibodyparts)",0
"self.figure,self.axes,self.canvas,self.toolbar,self.image_axis = self.image_panel.drawplot(self.img,img_name,self.iter,self.index,self.multibodyparts,self.colormap,keep_view=self.view_locked)",0
"self.axes.callbacks.connect('xlim_changed', self.onZoom)",0
"self.axes.callbacks.connect('ylim_changed', self.onZoom)",0
"self.image_panel.addcolorbar(self.img,self.image_axis,self.iter,self.multibodyparts,self.colormap)",0
,0
if self.individual_names[0] == 'single':,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.uniquebodyparts,self.individual_names,self.file,self.markerSize)",0
else:,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.multibodyparts,self.individual_names,self.file,self.markerSize)",0
"self.individualrdb.Bind(wx.EVT_RADIOBOX,self.select_individual)",0
"if self.rdb.GetStringSelection==""single"":",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.uniquebodyparts)",0
else:,0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
"self.cidClick = self.canvas.mpl_connect('button_press_event', self.onClick)",0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
"def plot(self,img):",0
self.drs= [],0
self.updatedCoords = [],0
for ind in self.individual_names:,0
if self.are_unique_bodyparts_present == False: #no unique bodyparts present,0
#            for ind in self.individual_names:,0
image_points = [],0
"for c, bp in enumerate(self.multibodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
"self.points = [self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter]]",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
color = self.colormap(self.norm(self.colorIndex[c])),0
"circle = patches.Circle((self.points[0], self.points[1]), radius=self.markerSize, fc = color, alpha=self.alpha)",0
#                    print(circle),0
print(self.axes),0
self.axes.add_patch(circle),0
"self.dr = auxfun_drag_label_multiple_individuals.DraggablePoint(circle,ind,self.multibodyparts[c])",0
self.dr.connect(),0
self.dr.coords = image_points,0
self.drs.append(self.dr),0
self.updatedCoords.append(self.dr.coords),0
if np.isnan(self.points)[0] == False:,0
self.buttonCounter[ind].append(self.multibodyparts[c]),0
else:,0
if ind == 'single':,0
"for c, bp in enumerate(self.uniquebodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
"self.points = [self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter]]",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.uniquebodyparts)",0
color = self.colormap(self.norm(self.colorIndex[c])),0
"circle = patches.Circle((self.points[0], self.points[1]), radius=self.markerSize, fc = color, alpha=self.alpha)",0
#                    print(circle),0
print(self.axes),0
self.axes.add_patch(circle),0
"self.dr = auxfun_drag_label_multiple_individuals.DraggablePoint(circle,ind,self.uniquebodyparts[c])",0
self.dr.connect(),0
self.dr.coords = image_points,0
self.drs.append(self.dr),0
self.updatedCoords.append(self.dr.coords),0
if np.isnan(self.points)[0] == False:,0
self.buttonCounter[ind].append(self.uniquebodyparts[c]),0
else:,0
"for c, bp in enumerate(self.multibodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
"self.points = [self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter]]",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
color = self.colormap(self.norm(self.colorIndex[c])),0
"circle = patches.Circle((self.points[0], self.points[1]), radius=self.markerSize, fc = color, alpha=self.alpha)",0
#                    print(circle),0
print(self.axes),0
self.axes.add_patch(circle),0
"self.dr = auxfun_drag_label_multiple_individuals.DraggablePoint(circle,ind,self.multibodyparts[c])",0
self.dr.connect(),0
self.dr.coords = image_points,0
self.drs.append(self.dr),0
self.updatedCoords.append(self.dr.coords),0
if np.isnan(self.points)[0] == False:,0
self.buttonCounter[ind].append(self.multibodyparts[c]),0
,0
MainFrame.saveEachImage(self),0
self.figure.canvas.draw(),0
return(self.buttonCounter),0
print(self.dataFrame),0
Windows compatible,0
Checks if zoom/pan button is ON,0
matplotlib.use('Agg'),0
Load updated lists:,0
cfg['video_sets'][vn],0
"labeling_toolbox.show(config,Screens,scale_w,scale_h, winHack, img_scale)",0
plotting parameters:,0
folders = [Path(config).parent / 'labeled-data' /Path(i) for i in video_names],0
plt.savefig(str(Path(tmpfolder)/imagename.split(os.sep)[-1])),0
Let's check if the code is *not* run on windows (Source: #https://stackoverflow.com/questions/1325581/how-do-i-check-if-im-running-on-windows-in-python),0
but the paths are in windows format...,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(index,name.split(os.sep)[1])",0
"print(name,test_video_name)",0
Check that points lie within the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
"print(trainIndexes,testIndexes, Shuffles, augmenter_type,net_type)",0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
sort the models directories,0
get the shuffle index,0
read cfg file,0
create log file,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
"text = str(self.individual_names+','+self.bodyParts)",0
Check for variable correctness,0
Moviepy:,0
close video.,0
"print(""DLC loaded in light mode; you cannot use the labeling GUI!"")",0
"def __init__(self, point,bodyParts):",0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
text = str(self.bodyParts),0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
"sess, inputs, outputs = predict.setup_pose_prediction(dlc_cfg)",0
#################################################,0
Datafolder,0
#################################################,0
looping over videos,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
from skimage.io import imread,0
"frame=imread(os.path.join(directory,framename),mode='RGB')",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/AlexEMG/DeepLabCut/pull/324",0
Parameters for augmentation with regard to cropping,0
"Added and described in ""Using DeepLabCut for 3D markerless pose estimation across species and behaviors""",0
Source: https://www.nature.com/articles/s41596-019-0176-0,0
"limit width  [-leftwidth*u-100,100+u*rightwidth] x [-bottomwith*u-100,100+u*topwidth] where u is always a (different) random number in unit interval",0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
Dependencies for anaysis,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"DLCscorer = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
import skimage.color,0
from skimage.io import imread,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
image = skimage.color.gray2rgb(image),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
"df.to_hdf(output_path.replace('csv', 'h5'), 'df_with_missing', format='table', mode='w')",0
from scipy.misc import imresize,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
Read file path for pose_config file. >> pass it on,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
print(end_points.keys()) >> to see what else is available.,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
from nets.mobilenet import conv_blocks as ops,0
from nets.mobilenet import mobilenet as lib,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
The next part of the code depends upon which tensorflow version you have.,0
if cfg.location_refinement:,0
assuming batchsize 1 here!,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
from scipy.misc import imresize,0
Augmentation functions,0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
Initializing variables if they don't exist...,0
what is the fraction of training samples with scaling augmentation?,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images into same size.,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
"from scipy.misc import imread, imresize",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
"from scipy.misc import imread, imresize",0
"from dataset.pose_dataset import Batch, data_to_input, mirror_joints_map, CropImage, DataItem",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
Process skeleton,0
save,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"deeplabcut.create_labeled_video(path_config_file,[newvideo], destfolder=dfolder,filtered=True)",0
TENSORPACK could fail in WINDOWS...,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
#create one split and make Shuffle 2 and 3 have the same split.,0
##Note that the new function in DLC 2.1 does that much easier...,0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Dowloading the calibration images,0
Deleting unneccesary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Importing the toolbox (takes several seconds),0
Loading example data set,0
deeplabcut.load_demo_data(path_config_file),0
shuffle=11 #>> imageio functions!,0
"print(""Analyze Video"")",0
"videofile_path = os.path.join(os.getcwd(),'openfield-Pranav-2018-10-30','videos','m3v1mp4.mp4')",0
"deeplabcut.analyze_videos(path_config_file,[videofile_path], shuffle=shuffle)",0
"print(""Create Labeled Video"")",0
"deeplabcut.create_labeled_video(path_config_file,[videofile_path],save_frames=False, shuffle=shuffle)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
Supress tensorflow warning messages,0
DLClight version does not support GUIs. Importing accordingly,0
"Train, evaluate & predict functions / require TF",0
docs[0]['init_weights'] = '../../pretrained/resnet_v1_101.ckpt',0
dict_test['init_weights'] = 'models/mpii/snapshot-1030000',0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Create the pose_config.yaml files,0
Analyze the videos,0
Read the config file,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
get vcap property,0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
"Import all videos in a folder or if just one video withouth [] passed, then make it a list.",0
"if isinstance(videos,str):",0
#there are two cases:,0
if os.path.isdir(videos): # it is a path!,0
path=videos,0
"videos=[os.path.join(path,vp) for vp in os.listdir(path) if videotype in vp]",0
if len(videos)==0:,0
"print(""No videos found in"",path,os.listdir(path))",0
"print(""Perhaps change the videotype, which is currently set to:"", videotype)",0
else:,0
"print(""Directory entered, "" , len(videos),"" videos were found."")",0
else:,0
if os.path.isfile(videos):,0
videos=[videos],0
https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
try:,0
"#shutil.copy(src,dst)",0
except OSError or TypeError: #https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
"shutil.copy(os.fspath(src),os.fspath(dst))",0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Set values to config file:,0
cfg_file['resnet']=50,0
Write dictionary to yaml  config file,0
import yaml,0
import cv2,0
import shutil,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
Flatten the list of bodyparts to connect,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Start plotting for every frame,0
"Once all the frames are saved, then make a movie using ffmpeg.",0
Create the fig and define the axes,0
Clear plot and initialize the variables,0
Initialize arrays for appending the 3d data for actual plotting the points,0
Initialize arrays for appending the 3d data for drawing the lines,0
Initialize arrays for appending the 2d data from cam1 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam2 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam1 for drawing the lines,0
Initialize arrays for appending the 2d data from cam2 for drawing the lines,0
Get the scorer names from the dataframe,0
"Set the x,y, and z limits for the 3d view",0
"Set the frame number to read#max(0,index-trailpoints):index",0
Plot the labels for each body part,0
Connecting the bodyparts specified in the config file.3d file is created based on the likelihoods of cam1 and cam2. Using 3d file and check if the body part is nan then dont plot skeleton,0
Saving the frames,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
Extract the indices of frames where the likelihood of a bodypart for both cameras are less than pvalue,0
Extract frames where likelihood for both the views is less than the pcutoff,0
"low_likelihood_frames = np.all(likelihoods < pcutoff, axis=1)",0
For cam1 camera: Assign nans to x and y values of a bodypart where the likelihood for is less than pvalue,0
For cam2 camera: Assign nans to x and y values of a bodypart where the likelihood is less than pvalue,0
ToDo: speed up func. below by saving in numpy.array,1
"Create an empty dataframe to store x,y,z of 3d data",0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Undistorting the points from cam1 camera,0
Undistorting the points from cam2 camera,0
Save the undistorted files,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = io.imread(img),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
self.cropping = False,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
"cv2.imwrite(img_name, frame)",0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.slider.Enable(False),0
self.checkBox.Enable(False),0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
Checks if zoom/pan button is ON,0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
extract min and max index based on start stop interval.,0
figure out body part list:,0
all indices between start and stop with jump larger than epsilon (leading up to this point!),0
deviation_dataname = str(Path(videofolder)/Path(dataname)),0
Calculate deviatons for video,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
data.to_csv(dataname.split('.h5')[0]+'filtered.csv'),0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
"Dataframe = pd.read_hdf(os.path.join(videofolder,dataname+'.h5'))",0
drop duplicate labels:,0
updates iteration by 1,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
"self.choiceBox,self.visualization_rdb = self.choice_panel.addRadioButtons()",0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
print(self.Dataframe.head()),0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
Checks if zoom/pan button is ON,0
"print(""DLC loaded in light mode; you cannot use the relabeling GUI!"")",0
###################################################,0
Dependencies,0
###################################################,0
import sys,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as specified in the config file,0
"rr, cc,val = line_aa(int(df_y[pair[0],index]),int(df_x[pair[0],index]),int(df_y[pair[1],index]), int(df_x[pair[1],index]))",0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Adds skeleton to the video,0
###################,0
less transparent present.,0
# One can change the parameters of the video creation script below:,0
See ffmpeg user guide: http://ffmpeg.org/ffmpeg.html#Video-and-Audio-file-format-conversion,0
,0
if notanalyzed:,0
"notanalyzed,outdataname,sourcedataname,DLCscorer=auxiliaryfunctions.CheckifPostProcessing(folder,vname,DLCscorer,DLCscorerlegacy,suffix='checking')",0
Loading cropping data used during analysis,0
!/usr/bin/env python3,0
"Historically DLC used: from scipy.misc import imread, imresize >> deprecated functions",0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
TODO check if those times exist...,1
"windows throws error if file path is > 260 characters, can fix with prefix. see https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation",0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
###################################################,0
Dependencies,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
"fig.colorbar(im, ax=ax)",0
#################################################,0
Looping analysis over video,0
#################################################,0
LoadData,0
"nlines,numcolumns=data.shape",0
assert(len(orderofbpincsv)==len(cfg['bodyparts'])),0
TODO: test len of images vs. len of imagenames for another sanity check,1
Is there a scorer for this?,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
"with open(projconfigfile, 'w') as cf:",0
"ruamelFile_3d.dump(cfg_file_3d, cf)",0
Read the pickle file,0
Write the pickle file,0
checks if input is a directory,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
variable initilization,0
design the panel,0
Add logo of DLC,0
Add all the options,0
,0
Hide the button as this is not the default option,0
self.SetSizer(self.sizer),0
"self.sizer.Add(self.sizer, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=15)",0
self.ok.Enable(False),0
else:,0
self.ok.Enable(True),0
For mac compatibility,0
Remove the pages in case the user goes back to the create new project and creates/load a new project,0
Add all the other pages,0
"wx.Frame.__init__(self, None, title=""DeepLabCut"")",0
"wx.Frame.__init__( self, None, id = wx.ID_ANY, title = 'DeepLabCut',size = wx.Size(self.gui_size), pos = wx.DefaultPosition, style = wx.RESIZE_BORDER|wx.DEFAULT_FRAME_STYLE|wx.TAB_TRAVERSAL )",0
Here we create a panel and a notebook on the panel,0
create the page windows as children of the notebook and add the pages to the notebook with the label to show on the tab,0
variable initilization,0
design the panel,0
Add logo of DLC,0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.crop_choice = wx.RadioBox(self, label='Want to crop the frames?', choices=['No', 'Yes'],majorDimension=1, style=wx.RA_SPECIFY_COLS)",0
"hbox1.Add(self.crop_choice,5, wx.EXPAND|wx.TOP|wx.BOTTOM, 5)",0
self.crop_choice.Enable(False),0
self.crop_choice.Enable(True),0
if self.crop_choice.GetStringSelection() == 'Yes':,0
crop = True,0
else:,0
crop = False,0
self.crop_choice.Enable(True),0
"self.crop_choice.SetStringSelection(""No"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
use the default pose_cfg file for default values,0
self.display_iters.Enable(False),0
self.save_iters.Enable(False),0
self.max_iters.Enable(False),0
self.snapshots.Enable(False),0
Read the pose config file,0
"print(os.path.join(cfg['project_path'],auxiliaryfunctions.GetModelFolder(trainFraction, self.shuffles.GetValue(),cfg),'train','pose_cfg.yaml'))",0
let the user open the file with default text editor. Also make it mac compatible,0
update the variables with the edited values in the pose config file,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.shuffles = wx.SpinCtrl(self, value='1',min=1,max=100)",0
"shuffles_text_boxsizer.Add(self.shuffles,1, wx.EXPAND|wx.TOP|wx.BOTTOM, 10)",0
if self.draw_skeleton.IsShow():,0
"if self.draw_skeleton.GetStringSelection() == ""Yes"":",0
self.draw = True,0
else:,0
self.draw = True,0
"print(self.config,self.filelist,self.videotype.GetValue(),shuffle,trainingsetindex,gputouse=None,save_as_csv=save_as_csv,destfolder=self.destfolder,cropping=cropping)",0
self.gpu.SetValue(-1),0
import wx.lib.scrolledpanel as SP,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
self.hbox2 = wx.BoxSizer(wx.HORIZONTAL),0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
"boxsizer.Add(self.hbox2,5, wx.EXPAND|wx.TOP|wx.BOTTOM, 10)",0
shuffle = self.shuffle.GetValue(),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.sel_config.SetPath(""Please select the config file"")",0
else:,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
#         design the panel,0
Add image of DLC,0
if editing this text make sure you add the '\n' to get the new line. The sizer is unable to format lines correctly.,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = cv2.imread(img),0
convert the image to RGB as you are showing the image with matplotlib,0
divider = make_axes_locatable(self.axes),0
"colorIndex = np.linspace(np.min(im),np.max(im),len(bodyparts))",0
"cax = divider.append_axes(""right"", size=""5%"", pad=0.05)",0
"cbar = self.figure.colorbar(ax, cax=cax,spacing='proportional', ticks=colorIndex)",0
cbar.set_ticklabels(bodyparts[::-1]),0
self.figure.canvas.draw(),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"MainFrame.saveDataSet(self, event)",0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"if self.uniquebodyparts == """":",0
self.uniquebodyparts = self.multibodyparts,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Create an empty data frame,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of old labels,0
Extracting the list of old unique labels,0
Checking if new labels are added or not,0
Checking if user added a new label,0
Found new labels in either multiple bodyparts or unique bodyparts,0
check if single is slected when radio buttons are changed,0
Checking if user added a new label,0
if self.new_Multibodyparts==[] and self.new_UniqueBodyparts==[]: # i.e. no new labels,0
"self.figure,self.axes,self.canvas,self.toolbar,self.image_axis = self.image_panel.drawplot(self.img,img_name,self.iter,self.index,self.multibodyparts,self.colormap,keep_view=self.view_locked)",0
"self.axes.callbacks.connect('xlim_changed', self.onZoom)",0
"self.axes.callbacks.connect('ylim_changed', self.onZoom)",0
"self.image_panel.addcolorbar(self.img,self.image_axis,self.iter,self.multibodyparts,self.colormap)",0
# Only show the bodyparts corresponding to single,0
if self.individual_names[0] == 'single':,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.uniquebodyparts,self.individual_names,self.file,self.markerSize)",0
else:,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.multibodyparts,self.individual_names,self.file,self.markerSize)",0
,0
"self.individualrdb.Bind(wx.EVT_RADIOBOX,self.select_individual)",0
,0
"# Get the color index, depending on if single is selcted or not.",0
"if self.rdb.GetStringSelection==""single"":",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.uniquebodyparts)",0
else:,0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
"self.cidClick = self.canvas.mpl_connect('button_press_event', self.onClick)",0
else:,0
# Found new labels in either multiple bodyparts or unique bodyparts,0
"dlg = wx.MessageDialog(None,""New label found in the config file. Do you want to see all the other labels?"", ""New label found"",wx.YES_NO | wx.ICON_WARNING)",0
result = dlg.ShowModal(),0
if result == wx.ID_NO:,0
if self.new_Multibodyparts!=[]:,0
self.multibodyparts = self.new_Multibodyparts,0
if self.new_UniqueBodyparts!=[]:,0
self.uniquebodyparts = self.new_UniqueBodyparts,0
,0
"self.dataFrame = MainFrame.create_dataframe(self,self.dataFrame,self.relativeimagenames,self.individual_names,self.new_UniqueBodyparts,self.new_Multibodyparts)",0
"self.figure,self.axes,self.canvas,self.toolbar,self.image_axis = self.image_panel.drawplot(self.img,img_name,self.iter,self.index,self.multibodyparts,self.colormap,keep_view=self.view_locked)",0
"self.axes.callbacks.connect('xlim_changed', self.onZoom)",0
"self.axes.callbacks.connect('ylim_changed', self.onZoom)",0
"self.image_panel.addcolorbar(self.img,self.image_axis,self.iter,self.multibodyparts,self.colormap)",0
,0
if self.individual_names[0] == 'single':,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.uniquebodyparts,self.individual_names,self.file,self.markerSize)",0
else:,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.multibodyparts,self.individual_names,self.file,self.markerSize)",0
"self.individualrdb.Bind(wx.EVT_RADIOBOX,self.select_individual)",0
"if self.rdb.GetStringSelection==""single"":",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.uniquebodyparts)",0
else:,0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
"self.cidClick = self.canvas.mpl_connect('button_press_event', self.onClick)",0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
"def plot(self,img):",0
self.drs= [],0
self.updatedCoords = [],0
for ind in self.individual_names:,0
if self.are_unique_bodyparts_present == False: #no unique bodyparts present,0
#            for ind in self.individual_names:,0
image_points = [],0
"for c, bp in enumerate(self.multibodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
"self.points = [self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter]]",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
color = self.colormap(self.norm(self.colorIndex[c])),0
"circle = patches.Circle((self.points[0], self.points[1]), radius=self.markerSize, fc = color, alpha=self.alpha)",0
#                    print(circle),0
print(self.axes),0
self.axes.add_patch(circle),0
"self.dr = auxfun_drag_label_multiple_individuals.DraggablePoint(circle,ind,self.multibodyparts[c])",0
self.dr.connect(),0
self.dr.coords = image_points,0
self.drs.append(self.dr),0
self.updatedCoords.append(self.dr.coords),0
if np.isnan(self.points)[0] == False:,0
self.buttonCounter[ind].append(self.multibodyparts[c]),0
else:,0
if ind == 'single':,0
"for c, bp in enumerate(self.uniquebodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
"self.points = [self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter]]",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.uniquebodyparts)",0
color = self.colormap(self.norm(self.colorIndex[c])),0
"circle = patches.Circle((self.points[0], self.points[1]), radius=self.markerSize, fc = color, alpha=self.alpha)",0
#                    print(circle),0
print(self.axes),0
self.axes.add_patch(circle),0
"self.dr = auxfun_drag_label_multiple_individuals.DraggablePoint(circle,ind,self.uniquebodyparts[c])",0
self.dr.connect(),0
self.dr.coords = image_points,0
self.drs.append(self.dr),0
self.updatedCoords.append(self.dr.coords),0
if np.isnan(self.points)[0] == False:,0
self.buttonCounter[ind].append(self.uniquebodyparts[c]),0
else:,0
"for c, bp in enumerate(self.multibodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
"self.points = [self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter]]",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
color = self.colormap(self.norm(self.colorIndex[c])),0
"circle = patches.Circle((self.points[0], self.points[1]), radius=self.markerSize, fc = color, alpha=self.alpha)",0
#                    print(circle),0
print(self.axes),0
self.axes.add_patch(circle),0
"self.dr = auxfun_drag_label_multiple_individuals.DraggablePoint(circle,ind,self.multibodyparts[c])",0
self.dr.connect(),0
self.dr.coords = image_points,0
self.drs.append(self.dr),0
self.updatedCoords.append(self.dr.coords),0
if np.isnan(self.points)[0] == False:,0
self.buttonCounter[ind].append(self.multibodyparts[c]),0
,0
MainFrame.saveEachImage(self),0
self.figure.canvas.draw(),0
return(self.buttonCounter),0
print(self.dataFrame),0
Windows compatible,0
Checks if zoom/pan button is ON,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
convert the image to RGB as you are showing the image with matplotlib,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Checks if zoom/pan button is ON,0
matplotlib.use('Agg'),0
Load updated lists:,0
cfg['video_sets'][vn],0
"labeling_toolbox.show(config,Screens,scale_w,scale_h, winHack, img_scale)",0
plotting parameters:,0
folders = [Path(config).parent / 'labeled-data' /Path(i) for i in video_names],0
plt.savefig(str(Path(tmpfolder)/imagename.split(os.sep)[-1])),0
Let's check if the code is *not* run on windows (Source: #https://stackoverflow.com/questions/1325581/how-do-i-check-if-im-running-on-windows-in-python),0
but the paths are in windows format...,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(index,name.split(os.sep)[1])",0
"print(name,test_video_name)",0
Check that points lie within the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
"print(trainIndexes,testIndexes, Shuffles, augmenter_type,net_type)",0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
sort the models directories,0
get the shuffle index,0
read cfg file,0
create log file,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
"text = str(self.individual_names+','+self.bodyParts)",0
Check for variable correctness,0
"plt.close(""all"")",0
Moviepy:,0
Call the GUI to select the cropping parameters,0
Update the config.yaml file with current cropping parameters,0
store full frame from random location (good for augmentation),0
"clip=clip.crop(y1 = int(coords[2]),y2 = int(coords[3]),x1 = int(coords[0]), x2 = int(coords[1]))",0
close video.,0
"print(""DLC loaded in light mode; you cannot use the labeling GUI!"")",0
"def __init__(self, point,bodyParts):",0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
text = str(self.bodyParts),0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
"sess, inputs, outputs = predict.setup_pose_prediction(dlc_cfg)",0
#################################################,0
Datafolder,0
#################################################,0
looping over videos,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
from skimage.io import imread,0
"frame=imread(os.path.join(directory,framename),mode='RGB')",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/AlexEMG/DeepLabCut/pull/324",0
Parameters for augmentation with regard to cropping,0
"Added and described in ""Using DeepLabCut for 3D markerless pose estimation across species and behaviors""",0
Source: https://www.nature.com/articles/s41596-019-0176-0,0
"limit width  [-leftwidth*u-100,100+u*rightwidth] x [-bottomwith*u-100,100+u*topwidth] where u is always a (different) random number in unit interval",0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
Dependencies for anaysis,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"DLCscorer = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
import skimage.color,0
from skimage.io import imread,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
image = skimage.color.gray2rgb(image),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
"df.to_hdf(output_path.replace('csv', 'h5'), 'df_with_missing', format='table', mode='w')",0
from scipy.misc import imresize,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
Read file path for pose_config file. >> pass it on,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
print(end_points.keys()) >> to see what else is available.,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
from nets.mobilenet import conv_blocks as ops,0
from nets.mobilenet import mobilenet as lib,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
The next part of the code depends upon which tensorflow version you have.,0
if cfg.location_refinement:,0
assuming batchsize 1 here!,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
from scipy.misc import imresize,0
Augmentation functions,0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
Initializing variables if they don't exist...,0
what is the fraction of training samples with scaling augmentation?,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images into same size.,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
"from scipy.misc import imread, imresize",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
"from scipy.misc import imread, imresize",0
"from dataset.pose_dataset import Batch, data_to_input, mirror_joints_map, CropImage, DataItem",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
Process skeleton,0
save,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"deeplabcut.create_labeled_video(path_config_file,[newvideo], destfolder=dfolder,filtered=True)",0
TENSORPACK could fail in WINDOWS...,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
#create one split and make Shuffle 2 and 3 have the same split.,0
##Note that the new function in DLC 2.1 does that much easier...,0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Dowloading the calibration images,0
Deleting unneccesary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Importing the toolbox (takes several seconds),0
Loading example data set,0
deeplabcut.load_demo_data(path_config_file),0
shuffle=11 #>> imageio functions!,0
"print(""Analyze Video"")",0
"videofile_path = os.path.join(os.getcwd(),'openfield-Pranav-2018-10-30','videos','m3v1mp4.mp4')",0
"deeplabcut.analyze_videos(path_config_file,[videofile_path], shuffle=shuffle)",0
"print(""Create Labeled Video"")",0
"deeplabcut.create_labeled_video(path_config_file,[videofile_path],save_frames=False, shuffle=shuffle)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
Supress tensorflow warning messages,0
DLClight version does not support GUIs. Importing accordingly,0
"Train, evaluate & predict functions / require TF",0
docs[0]['init_weights'] = '../../pretrained/resnet_v1_101.ckpt',0
dict_test['init_weights'] = 'models/mpii/snapshot-1030000',0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Create the pose_config.yaml files,0
Analyze the videos,0
Read the config file,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
get vcap property,0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
"Import all videos in a folder or if just one video withouth [] passed, then make it a list.",0
"if isinstance(videos,str):",0
#there are two cases:,0
if os.path.isdir(videos): # it is a path!,0
path=videos,0
"videos=[os.path.join(path,vp) for vp in os.listdir(path) if videotype in vp]",0
if len(videos)==0:,0
"print(""No videos found in"",path,os.listdir(path))",0
"print(""Perhaps change the videotype, which is currently set to:"", videotype)",0
else:,0
"print(""Directory entered, "" , len(videos),"" videos were found."")",0
else:,0
if os.path.isfile(videos):,0
videos=[videos],0
https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
try:,0
"#shutil.copy(src,dst)",0
except OSError or TypeError: #https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
"shutil.copy(os.fspath(src),os.fspath(dst))",0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Set values to config file:,0
cfg_file['resnet']=50,0
Write dictionary to yaml  config file,0
import yaml,0
import cv2,0
import shutil,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
Flatten the list of bodyparts to connect,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Start plotting for every frame,0
"Once all the frames are saved, then make a movie using ffmpeg.",0
Create the fig and define the axes,0
Clear plot and initialize the variables,0
Initialize arrays for appending the 3d data for actual plotting the points,0
Initialize arrays for appending the 3d data for drawing the lines,0
Initialize arrays for appending the 2d data from cam1 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam2 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam1 for drawing the lines,0
Initialize arrays for appending the 2d data from cam2 for drawing the lines,0
Get the scorer names from the dataframe,0
"Set the x,y, and z limits for the 3d view",0
"Set the frame number to read#max(0,index-trailpoints):index",0
Plot the labels for each body part,0
Connecting the bodyparts specified in the config file.3d file is created based on the likelihoods of cam1 and cam2. Using 3d file and check if the body part is nan then dont plot skeleton,0
Saving the frames,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
Extract the indices of frames where the likelihood of a bodypart for both cameras are less than pvalue,0
Extract frames where likelihood for both the views is less than the pcutoff,0
"low_likelihood_frames = np.all(likelihoods < pcutoff, axis=1)",0
For cam1 camera: Assign nans to x and y values of a bodypart where the likelihood for is less than pvalue,0
For cam2 camera: Assign nans to x and y values of a bodypart where the likelihood is less than pvalue,0
ToDo: speed up func. below by saving in numpy.array,1
"Create an empty dataframe to store x,y,z of 3d data",0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Undistorting the points from cam1 camera,0
Undistorting the points from cam2 camera,0
Save the undistorted files,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = io.imread(img),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
self.cropping = False,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
"cv2.imwrite(img_name, frame)",0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.slider.Enable(False),0
self.checkBox.Enable(False),0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
Checks if zoom/pan button is ON,0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
extract min and max index based on start stop interval.,0
figure out body part list:,0
all indices between start and stop with jump larger than epsilon (leading up to this point!),0
deviation_dataname = str(Path(videofolder)/Path(dataname)),0
Calculate deviatons for video,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
data.to_csv(dataname.split('.h5')[0]+'filtered.csv'),0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
"Dataframe = pd.read_hdf(os.path.join(videofolder,dataname+'.h5'))",0
drop duplicate labels:,0
updates iteration by 1,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
"self.choiceBox,self.visualization_rdb = self.choice_panel.addRadioButtons()",0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
print(self.Dataframe.head()),0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
Checks if zoom/pan button is ON,0
"print(""DLC loaded in light mode; you cannot use the relabeling GUI!"")",0
###################################################,0
Dependencies,0
###################################################,0
import sys,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as specified in the config file,0
"rr, cc,val = line_aa(int(df_y[pair[0],index]),int(df_x[pair[0],index]),int(df_y[pair[1],index]), int(df_x[pair[1],index]))",0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Adds skeleton to the video,0
###################,0
less transparent present.,0
# One can change the parameters of the video creation script below:,0
See ffmpeg user guide: http://ffmpeg.org/ffmpeg.html#Video-and-Audio-file-format-conversion,0
,0
if notanalyzed:,0
"notanalyzed,outdataname,sourcedataname,DLCscorer=auxiliaryfunctions.CheckifPostProcessing(folder,vname,DLCscorer,DLCscorerlegacy,suffix='checking')",0
Loading cropping data used during analysis,0
!/usr/bin/env python3,0
"Historically DLC used: from scipy.misc import imread, imresize >> deprecated functions",0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
TODO check if those times exist...,1
"windows throws error if file path is > 260 characters, can fix with prefix. see https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation",0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
###################################################,0
Dependencies,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
"fig.colorbar(im, ax=ax)",0
#################################################,0
Looping analysis over video,0
#################################################,0
LoadData,0
"nlines,numcolumns=data.shape",0
assert(len(orderofbpincsv)==len(cfg['bodyparts'])),0
TODO: test len of images vs. len of imagenames for another sanity check,1
Is there a scorer for this?,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
"with open(projconfigfile, 'w') as cf:",0
"ruamelFile_3d.dump(cfg_file_3d, cf)",0
Read the pickle file,0
Write the pickle file,0
checks if input is a directory,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
variable initilization,0
design the panel,0
Add logo of DLC,0
Add all the options,0
,0
Hide the button as this is not the default option,0
self.SetSizer(self.sizer),0
"self.sizer.Add(self.sizer, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=15)",0
self.ok.Enable(False),0
else:,0
self.ok.Enable(True),0
For mac compatibility,0
Remove the pages in case the user goes back to the create new project and creates/load a new project,0
Add all the other pages,0
"wx.Frame.__init__(self, None, title=""DeepLabCut"")",0
"wx.Frame.__init__( self, None, id = wx.ID_ANY, title = 'DeepLabCut',size = wx.Size(self.gui_size), pos = wx.DefaultPosition, style = wx.RESIZE_BORDER|wx.DEFAULT_FRAME_STYLE|wx.TAB_TRAVERSAL )",0
Here we create a panel and a notebook on the panel,0
create the page windows as children of the notebook and add the pages to the notebook with the label to show on the tab,0
variable initilization,0
design the panel,0
Add logo of DLC,0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.crop_choice = wx.RadioBox(self, label='Want to crop the frames?', choices=['No', 'Yes'],majorDimension=1, style=wx.RA_SPECIFY_COLS)",0
"hbox1.Add(self.crop_choice,5, wx.EXPAND|wx.TOP|wx.BOTTOM, 5)",0
self.crop_choice.Enable(False),0
self.crop_choice.Enable(True),0
if self.crop_choice.GetStringSelection() == 'Yes':,0
crop = True,0
else:,0
crop = False,0
self.crop_choice.Enable(True),0
"self.crop_choice.SetStringSelection(""No"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
use the default pose_cfg file for default values,0
self.display_iters.Enable(False),0
self.save_iters.Enable(False),0
self.max_iters.Enable(False),0
self.snapshots.Enable(False),0
Read the pose config file,0
"print(os.path.join(cfg['project_path'],auxiliaryfunctions.GetModelFolder(trainFraction, self.shuffles.GetValue(),cfg),'train','pose_cfg.yaml'))",0
let the user open the file with default text editor. Also make it mac compatible,0
update the variables with the edited values in the pose config file,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.shuffles = wx.SpinCtrl(self, value='1',min=1,max=100)",0
"shuffles_text_boxsizer.Add(self.shuffles,1, wx.EXPAND|wx.TOP|wx.BOTTOM, 10)",0
if self.draw_skeleton.IsShow():,0
"if self.draw_skeleton.GetStringSelection() == ""Yes"":",0
self.draw = True,0
else:,0
self.draw = True,0
"print(self.config,self.filelist,self.videotype.GetValue(),shuffle,trainingsetindex,gputouse=None,save_as_csv=save_as_csv,destfolder=self.destfolder,cropping=cropping)",0
self.gpu.SetValue(-1),0
import wx.lib.scrolledpanel as SP,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
self.hbox2 = wx.BoxSizer(wx.HORIZONTAL),0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
"boxsizer.Add(self.hbox2,5, wx.EXPAND|wx.TOP|wx.BOTTOM, 10)",0
shuffle = self.shuffle.GetValue(),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.sel_config.SetPath(""Please select the config file"")",0
else:,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
#         design the panel,0
Add image of DLC,0
if editing this text make sure you add the '\n' to get the new line. The sizer is unable to format lines correctly.,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = cv2.imread(img),0
convert the image to RGB as you are showing the image with matplotlib,0
divider = make_axes_locatable(self.axes),0
"colorIndex = np.linspace(np.min(im),np.max(im),len(bodyparts))",0
"cax = divider.append_axes(""right"", size=""5%"", pad=0.05)",0
"cbar = self.figure.colorbar(ax, cax=cax,spacing='proportional', ticks=colorIndex)",0
cbar.set_ticklabels(bodyparts[::-1]),0
self.figure.canvas.draw(),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"MainFrame.saveDataSet(self, event)",0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"if self.uniquebodyparts == """":",0
self.uniquebodyparts = self.multibodyparts,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Create an empty data frame,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of old labels,0
Extracting the list of old unique labels,0
Checking if new labels are added or not,0
Checking if user added a new label,0
Found new labels in either multiple bodyparts or unique bodyparts,0
check if single is slected when radio buttons are changed,0
Checking if user added a new label,0
if self.new_Multibodyparts==[] and self.new_UniqueBodyparts==[]: # i.e. no new labels,0
"self.figure,self.axes,self.canvas,self.toolbar,self.image_axis = self.image_panel.drawplot(self.img,img_name,self.iter,self.index,self.multibodyparts,self.colormap,keep_view=self.view_locked)",0
"self.axes.callbacks.connect('xlim_changed', self.onZoom)",0
"self.axes.callbacks.connect('ylim_changed', self.onZoom)",0
"self.image_panel.addcolorbar(self.img,self.image_axis,self.iter,self.multibodyparts,self.colormap)",0
# Only show the bodyparts corresponding to single,0
if self.individual_names[0] == 'single':,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.uniquebodyparts,self.individual_names,self.file,self.markerSize)",0
else:,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.multibodyparts,self.individual_names,self.file,self.markerSize)",0
,0
"self.individualrdb.Bind(wx.EVT_RADIOBOX,self.select_individual)",0
,0
"# Get the color index, depending on if single is selcted or not.",0
"if self.rdb.GetStringSelection==""single"":",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.uniquebodyparts)",0
else:,0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
"self.cidClick = self.canvas.mpl_connect('button_press_event', self.onClick)",0
else:,0
# Found new labels in either multiple bodyparts or unique bodyparts,0
"dlg = wx.MessageDialog(None,""New label found in the config file. Do you want to see all the other labels?"", ""New label found"",wx.YES_NO | wx.ICON_WARNING)",0
result = dlg.ShowModal(),0
if result == wx.ID_NO:,0
if self.new_Multibodyparts!=[]:,0
self.multibodyparts = self.new_Multibodyparts,0
if self.new_UniqueBodyparts!=[]:,0
self.uniquebodyparts = self.new_UniqueBodyparts,0
,0
"self.dataFrame = MainFrame.create_dataframe(self,self.dataFrame,self.relativeimagenames,self.individual_names,self.new_UniqueBodyparts,self.new_Multibodyparts)",0
"self.figure,self.axes,self.canvas,self.toolbar,self.image_axis = self.image_panel.drawplot(self.img,img_name,self.iter,self.index,self.multibodyparts,self.colormap,keep_view=self.view_locked)",0
"self.axes.callbacks.connect('xlim_changed', self.onZoom)",0
"self.axes.callbacks.connect('ylim_changed', self.onZoom)",0
"self.image_panel.addcolorbar(self.img,self.image_axis,self.iter,self.multibodyparts,self.colormap)",0
,0
if self.individual_names[0] == 'single':,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.uniquebodyparts,self.individual_names,self.file,self.markerSize)",0
else:,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.multibodyparts,self.individual_names,self.file,self.markerSize)",0
"self.individualrdb.Bind(wx.EVT_RADIOBOX,self.select_individual)",0
"if self.rdb.GetStringSelection==""single"":",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.uniquebodyparts)",0
else:,0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
"self.cidClick = self.canvas.mpl_connect('button_press_event', self.onClick)",0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
"def plot(self,img):",0
self.drs= [],0
self.updatedCoords = [],0
for ind in self.individual_names:,0
if self.are_unique_bodyparts_present == False: #no unique bodyparts present,0
#            for ind in self.individual_names:,0
image_points = [],0
"for c, bp in enumerate(self.multibodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
"self.points = [self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter]]",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
color = self.colormap(self.norm(self.colorIndex[c])),0
"circle = patches.Circle((self.points[0], self.points[1]), radius=self.markerSize, fc = color, alpha=self.alpha)",0
#                    print(circle),0
print(self.axes),0
self.axes.add_patch(circle),0
"self.dr = auxfun_drag_label_multiple_individuals.DraggablePoint(circle,ind,self.multibodyparts[c])",0
self.dr.connect(),0
self.dr.coords = image_points,0
self.drs.append(self.dr),0
self.updatedCoords.append(self.dr.coords),0
if np.isnan(self.points)[0] == False:,0
self.buttonCounter[ind].append(self.multibodyparts[c]),0
else:,0
if ind == 'single':,0
"for c, bp in enumerate(self.uniquebodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
"self.points = [self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter]]",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.uniquebodyparts)",0
color = self.colormap(self.norm(self.colorIndex[c])),0
"circle = patches.Circle((self.points[0], self.points[1]), radius=self.markerSize, fc = color, alpha=self.alpha)",0
#                    print(circle),0
print(self.axes),0
self.axes.add_patch(circle),0
"self.dr = auxfun_drag_label_multiple_individuals.DraggablePoint(circle,ind,self.uniquebodyparts[c])",0
self.dr.connect(),0
self.dr.coords = image_points,0
self.drs.append(self.dr),0
self.updatedCoords.append(self.dr.coords),0
if np.isnan(self.points)[0] == False:,0
self.buttonCounter[ind].append(self.uniquebodyparts[c]),0
else:,0
"for c, bp in enumerate(self.multibodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
"self.points = [self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter]]",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
color = self.colormap(self.norm(self.colorIndex[c])),0
"circle = patches.Circle((self.points[0], self.points[1]), radius=self.markerSize, fc = color, alpha=self.alpha)",0
#                    print(circle),0
print(self.axes),0
self.axes.add_patch(circle),0
"self.dr = auxfun_drag_label_multiple_individuals.DraggablePoint(circle,ind,self.multibodyparts[c])",0
self.dr.connect(),0
self.dr.coords = image_points,0
self.drs.append(self.dr),0
self.updatedCoords.append(self.dr.coords),0
if np.isnan(self.points)[0] == False:,0
self.buttonCounter[ind].append(self.multibodyparts[c]),0
,0
MainFrame.saveEachImage(self),0
self.figure.canvas.draw(),0
return(self.buttonCounter),0
print(self.dataFrame),0
Windows compatible,0
Checks if zoom/pan button is ON,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
convert the image to RGB as you are showing the image with matplotlib,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Checks if zoom/pan button is ON,0
matplotlib.use('Agg'),0
Load updated lists:,0
cfg['video_sets'][vn],0
"labeling_toolbox.show(config,Screens,scale_w,scale_h, winHack, img_scale)",0
plotting parameters:,0
folders = [Path(config).parent / 'labeled-data' /Path(i) for i in video_names],0
plt.savefig(str(Path(tmpfolder)/imagename.split(os.sep)[-1])),0
Let's check if the code is *not* run on windows (Source: #https://stackoverflow.com/questions/1325581/how-do-i-check-if-im-running-on-windows-in-python),0
but the paths are in windows format...,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(index,name.split(os.sep)[1])",0
"print(name,test_video_name)",0
Check that points lie within the image,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
sort the models directories,0
get the shuffle index,0
read cfg file,0
create log file,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
"text = str(self.individual_names+','+self.bodyParts)",0
Check for variable correctness,0
"plt.close(""all"")",0
Moviepy:,0
Call the GUI to select the cropping parameters,0
Update the config.yaml file with current cropping parameters,0
store full frame from random location (good for augmentation),0
"clip=clip.crop(y1 = int(coords[2]),y2 = int(coords[3]),x1 = int(coords[0]), x2 = int(coords[1]))",0
close video.,0
"print(""DLC loaded in light mode; you cannot use the labeling GUI!"")",0
"def __init__(self, point,bodyParts):",0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
text = str(self.bodyParts),0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
"sess, inputs, outputs = predict.setup_pose_prediction(dlc_cfg)",0
#################################################,0
Datafolder,0
#################################################,0
looping over videos,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
from skimage.io import imread,0
"frame=imread(os.path.join(directory,framename),mode='RGB')",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/AlexEMG/DeepLabCut/pull/324",0
Parameters for augmentation with regard to cropping,0
"Added and described in ""Using DeepLabCut for 3D markerless pose estimation across species and behaviors""",0
Source: https://www.nature.com/articles/s41596-019-0176-0,0
"limit width  [-leftwidth*u-100,100+u*rightwidth] x [-bottomwith*u-100,100+u*topwidth] where u is always a (different) random number in unit interval",0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
Dependencies for anaysis,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"DLCscorer = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
import skimage.color,0
from skimage.io import imread,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
image = skimage.color.gray2rgb(image),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
from scipy.misc import imresize,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
Read file path for pose_config file. >> pass it on,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
print(end_points.keys()) >> to see what else is available.,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
from nets.mobilenet import conv_blocks as ops,0
from nets.mobilenet import mobilenet as lib,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
The next part of the code depends upon which tensorflow version you have.,0
if cfg.location_refinement:,0
assuming batchsize 1 here!,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
from scipy.misc import imresize,0
Augmentation functions,0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
Initializing variables if they don't exist...,0
what is the fraction of training samples with scaling augmentation?,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images into same size.,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
"from scipy.misc import imread, imresize",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
"from scipy.misc import imread, imresize",0
"from dataset.pose_dataset import Batch, data_to_input, mirror_joints_map, CropImage, DataItem",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
Process skeleton,0
save,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"deeplabcut.create_labeled_video(path_config_file,[newvideo], destfolder=dfolder,filtered=True)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
#create one split and make Shuffle 2 and 3 have the same split.,0
##Note that the new function in DLC 2.1 does that much easier...,0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Dowloading the calibration images,0
Deleting unneccesary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Importing the toolbox (takes several seconds),0
Loading example data set,0
deeplabcut.load_demo_data(path_config_file),0
shuffle=11 #>> imageio functions!,0
"print(""Analyze Video"")",0
"videofile_path = os.path.join(os.getcwd(),'openfield-Pranav-2018-10-30','videos','m3v1mp4.mp4')",0
"deeplabcut.analyze_videos(path_config_file,[videofile_path], shuffle=shuffle)",0
"print(""Create Labeled Video"")",0
"deeplabcut.create_labeled_video(path_config_file,[videofile_path],save_frames=False, shuffle=shuffle)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
if module is executed directly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
Supress tensorflow warning messages,0
DLClight version does not support GUIs. Importing accordingly,0
"Train, evaluate & predict functions / require TF",0
docs[0]['init_weights'] = '../../pretrained/resnet_v1_101.ckpt',0
dict_test['init_weights'] = 'models/mpii/snapshot-1030000',0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Create the pose_config.yaml files,0
Analyze the videos,0
Read the config file,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
get vcap property,0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
"Import all videos in a folder or if just one video withouth [] passed, then make it a list.",0
"if isinstance(videos,str):",0
#there are two cases:,0
if os.path.isdir(videos): # it is a path!,0
path=videos,0
"videos=[os.path.join(path,vp) for vp in os.listdir(path) if videotype in vp]",0
if len(videos)==0:,0
"print(""No videos found in"",path,os.listdir(path))",0
"print(""Perhaps change the videotype, which is currently set to:"", videotype)",0
else:,0
"print(""Directory entered, "" , len(videos),"" videos were found."")",0
else:,0
if os.path.isfile(videos):,0
videos=[videos],0
https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
try:,0
"#shutil.copy(src,dst)",0
except OSError or TypeError: #https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
"shutil.copy(os.fspath(src),os.fspath(dst))",0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Set values to config file:,0
cfg_file['resnet']=50,0
Write dictionary to yaml  config file,0
import yaml,0
import cv2,0
import shutil,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
Flatten the list of bodyparts to connect,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
"print(""Found filtered predictions, will be use these for triangulation."")",0
Start plotting for every frame,0
"Once all the frames are saved, then make a movie using ffmpeg.",0
Create the fig and define the axes,0
Clear plot and initialize the variables,0
Initialize arrays for appending the 3d data for actual plotting the points,0
Initialize arrays for appending the 3d data for drawing the lines,0
Initialize arrays for appending the 2d data from cam1 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam2 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam1 for drawing the lines,0
Initialize arrays for appending the 2d data from cam2 for drawing the lines,0
Get the scorer names from the dataframe,0
"Set the x,y, and z limits for the 3d view",0
"Set the frame number to read#max(0,index-trailpoints):index",0
Plot the labels for each body part,0
Connecting the bodyparts specified in the config file.3d file is created based on the likelihoods of cam1 and cam2. Using 3d file and check if the body part is nan then dont plot skeleton,0
Saving the frames,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
Extract the indices of frames where the likelihood of a bodypart for both cameras are less than pvalue,0
Extract frames where likelihood for both the views is less than the pcutoff,0
"low_likelihood_frames = np.all(likelihoods < pcutoff, axis=1)",0
For cam1 camera: Assign nans to x and y values of a bodypart where the likelihood for is less than pvalue,0
For cam2 camera: Assign nans to x and y values of a bodypart where the likelihood is less than pvalue,0
ToDo: speed up func. below by saving in numpy.array,1
"Create an empty dataframe to store x,y,z of 3d data",0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Undistorting the points from cam1 camera,0
Undistorting the points from cam2 camera,0
Save the undistorted files,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = io.imread(img),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
self.cropping = False,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
"cv2.imwrite(img_name, frame)",0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.slider.Enable(False),0
self.checkBox.Enable(False),0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
Checks if zoom/pan button is ON,0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
extract min and max index based on start stop interval.,0
figure out body part list:,0
all indices between start and stop with jump larger than epsilon (leading up to this point!),0
deviation_dataname = str(Path(videofolder)/Path(dataname)),0
Calculate deviatons for video,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
data.to_csv(dataname.split('.h5')[0]+'filtered.csv'),0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
"Dataframe = pd.read_hdf(os.path.join(videofolder,dataname+'.h5'))",0
drop duplicate labels:,0
updates iteration by 1,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
"self.choiceBox,self.visualization_rdb = self.choice_panel.addRadioButtons()",0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
print(self.Dataframe.head()),0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
Checks if zoom/pan button is ON,0
"print(""DLC loaded in light mode; you cannot use the relabeling GUI!"")",0
###################################################,0
Dependencies,0
###################################################,0
import sys,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as specified in the config file,0
"rr, cc,val = line_aa(int(df_y[pair[0],index]),int(df_x[pair[0],index]),int(df_y[pair[1],index]), int(df_x[pair[1],index]))",0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Adds skeleton to the video,0
###################,0
less transparent present.,0
# One can change the parameters of the video creation script below:,0
See ffmpeg user guide: http://ffmpeg.org/ffmpeg.html#Video-and-Audio-file-format-conversion,0
,0
if notanalyzed:,0
"notanalyzed,outdataname,sourcedataname,DLCscorer=auxiliaryfunctions.CheckifPostProcessing(folder,vname,DLCscorer,DLCscorerlegacy,suffix='checking')",0
Loading cropping data used during analysis,0
!/usr/bin/env python3,0
"Historically DLC used: from scipy.misc import imread, imresize >> deprecated functions",0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
TODO check if those times exist...,1
"windows throws error if file path is > 260 characters, can fix with prefix. see https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation",0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
###################################################,0
Dependencies,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
"fig.colorbar(im, ax=ax)",0
#################################################,0
Looping analysis over video,0
#################################################,0
LoadData,0
"nlines,numcolumns=data.shape",0
assert(len(orderofbpincsv)==len(cfg['bodyparts'])),0
TODO: test len of images vs. len of imagenames for another sanity check,1
Is there a scorer for this?,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
"with open(projconfigfile, 'w') as cf:",0
"ruamelFile_3d.dump(cfg_file_3d, cf)",0
Read the pickle file,0
Write the pickle file,0
checks if input is a directory,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
variable initilization,0
design the panel,0
Add logo of DLC,0
Add all the options,0
,0
Hide the button as this is not the default option,0
self.SetSizer(self.sizer),0
"self.sizer.Add(self.sizer, pos=(3, 0), span=(1, 8),flag=wx.EXPAND|wx.BOTTOM, border=15)",0
self.ok.Enable(False),0
else:,0
self.ok.Enable(True),0
For mac compatibility,0
Remove the pages in case the user goes back to the create new project and creates/load a new project,0
Add all the other pages,0
"wx.Frame.__init__(self, None, title=""DeepLabCut"")",0
"wx.Frame.__init__( self, None, id = wx.ID_ANY, title = 'DeepLabCut',size = wx.Size(self.gui_size), pos = wx.DefaultPosition, style = wx.RESIZE_BORDER|wx.DEFAULT_FRAME_STYLE|wx.TAB_TRAVERSAL )",0
Here we create a panel and a notebook on the panel,0
create the page windows as children of the notebook and add the pages to the notebook with the label to show on the tab,0
variable initilization,0
design the panel,0
Add logo of DLC,0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.crop_choice = wx.RadioBox(self, label='Want to crop the frames?', choices=['No', 'Yes'],majorDimension=1, style=wx.RA_SPECIFY_COLS)",0
"hbox1.Add(self.crop_choice,5, wx.EXPAND|wx.TOP|wx.BOTTOM, 5)",0
self.crop_choice.Enable(False),0
self.crop_choice.Enable(True),0
if self.crop_choice.GetStringSelection() == 'Yes':,0
crop = True,0
else:,0
crop = False,0
self.crop_choice.Enable(True),0
"self.crop_choice.SetStringSelection(""No"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
use the default pose_cfg file for default values,0
self.display_iters.Enable(False),0
self.save_iters.Enable(False),0
self.max_iters.Enable(False),0
self.snapshots.Enable(False),0
Read the pose config file,0
"print(os.path.join(cfg['project_path'],auxiliaryfunctions.GetModelFolder(trainFraction, self.shuffles.GetValue(),cfg),'train','pose_cfg.yaml'))",0
let the user open the file with default text editor. Also make it mac compatible,0
update the variables with the edited values in the pose config file,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.shuffles = wx.SpinCtrl(self, value='1',min=1,max=100)",0
"shuffles_text_boxsizer.Add(self.shuffles,1, wx.EXPAND|wx.TOP|wx.BOTTOM, 10)",0
if self.draw_skeleton.IsShow():,0
"if self.draw_skeleton.GetStringSelection() == ""Yes"":",0
self.draw = True,0
else:,0
self.draw = True,0
"print(self.config,self.filelist,self.videotype.GetValue(),shuffle,trainingsetindex,gputouse=None,save_as_csv=save_as_csv,destfolder=self.destfolder,cropping=cropping)",0
self.gpu.SetValue(-1),0
import wx.lib.scrolledpanel as SP,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
self.hbox2 = wx.BoxSizer(wx.HORIZONTAL),0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
"boxsizer.Add(self.hbox2,5, wx.EXPAND|wx.TOP|wx.BOTTOM, 10)",0
shuffle = self.shuffle.GetValue(),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.sel_config.SetPath(""Please select the config file"")",0
else:,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
#         design the panel,0
Add image of DLC,0
if editing this text make sure you add the '\n' to get the new line. The sizer is unable to format lines correctly.,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = cv2.imread(img),0
convert the image to RGB as you are showing the image with matplotlib,0
divider = make_axes_locatable(self.axes),0
"colorIndex = np.linspace(np.min(im),np.max(im),len(bodyparts))",0
"cax = divider.append_axes(""right"", size=""5%"", pad=0.05)",0
"cbar = self.figure.colorbar(ax, cax=cax,spacing='proportional', ticks=colorIndex)",0
cbar.set_ticklabels(bodyparts[::-1]),0
self.figure.canvas.draw(),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"MainFrame.saveDataSet(self, event)",0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"if self.uniquebodyparts == """":",0
self.uniquebodyparts = self.multibodyparts,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Create an empty data frame,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of old labels,0
Extracting the list of old unique labels,0
Checking if new labels are added or not,0
Checking if user added a new label,0
Found new labels in either multiple bodyparts or unique bodyparts,0
check if single is slected when radio buttons are changed,0
Checking if user added a new label,0
if self.new_Multibodyparts==[] and self.new_UniqueBodyparts==[]: # i.e. no new labels,0
"self.figure,self.axes,self.canvas,self.toolbar,self.image_axis = self.image_panel.drawplot(self.img,img_name,self.iter,self.index,self.multibodyparts,self.colormap,keep_view=self.view_locked)",0
"self.axes.callbacks.connect('xlim_changed', self.onZoom)",0
"self.axes.callbacks.connect('ylim_changed', self.onZoom)",0
"self.image_panel.addcolorbar(self.img,self.image_axis,self.iter,self.multibodyparts,self.colormap)",0
# Only show the bodyparts corresponding to single,0
if self.individual_names[0] == 'single':,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.uniquebodyparts,self.individual_names,self.file,self.markerSize)",0
else:,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.multibodyparts,self.individual_names,self.file,self.markerSize)",0
,0
"self.individualrdb.Bind(wx.EVT_RADIOBOX,self.select_individual)",0
,0
"# Get the color index, depending on if single is selcted or not.",0
"if self.rdb.GetStringSelection==""single"":",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.uniquebodyparts)",0
else:,0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
"self.cidClick = self.canvas.mpl_connect('button_press_event', self.onClick)",0
else:,0
# Found new labels in either multiple bodyparts or unique bodyparts,0
"dlg = wx.MessageDialog(None,""New label found in the config file. Do you want to see all the other labels?"", ""New label found"",wx.YES_NO | wx.ICON_WARNING)",0
result = dlg.ShowModal(),0
if result == wx.ID_NO:,0
if self.new_Multibodyparts!=[]:,0
self.multibodyparts = self.new_Multibodyparts,0
if self.new_UniqueBodyparts!=[]:,0
self.uniquebodyparts = self.new_UniqueBodyparts,0
,0
"self.dataFrame = MainFrame.create_dataframe(self,self.dataFrame,self.relativeimagenames,self.individual_names,self.new_UniqueBodyparts,self.new_Multibodyparts)",0
"self.figure,self.axes,self.canvas,self.toolbar,self.image_axis = self.image_panel.drawplot(self.img,img_name,self.iter,self.index,self.multibodyparts,self.colormap,keep_view=self.view_locked)",0
"self.axes.callbacks.connect('xlim_changed', self.onZoom)",0
"self.axes.callbacks.connect('ylim_changed', self.onZoom)",0
"self.image_panel.addcolorbar(self.img,self.image_axis,self.iter,self.multibodyparts,self.colormap)",0
,0
if self.individual_names[0] == 'single':,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.uniquebodyparts,self.individual_names,self.file,self.markerSize)",0
else:,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.multibodyparts,self.individual_names,self.file,self.markerSize)",0
"self.individualrdb.Bind(wx.EVT_RADIOBOX,self.select_individual)",0
"if self.rdb.GetStringSelection==""single"":",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.uniquebodyparts)",0
else:,0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
"self.cidClick = self.canvas.mpl_connect('button_press_event', self.onClick)",0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
"def plot(self,img):",0
self.drs= [],0
self.updatedCoords = [],0
for ind in self.individual_names:,0
if self.are_unique_bodyparts_present == False: #no unique bodyparts present,0
#            for ind in self.individual_names:,0
image_points = [],0
"for c, bp in enumerate(self.multibodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
"self.points = [self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter]]",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
color = self.colormap(self.norm(self.colorIndex[c])),0
"circle = patches.Circle((self.points[0], self.points[1]), radius=self.markerSize, fc = color, alpha=self.alpha)",0
#                    print(circle),0
print(self.axes),0
self.axes.add_patch(circle),0
"self.dr = auxfun_drag_label_multiple_individuals.DraggablePoint(circle,ind,self.multibodyparts[c])",0
self.dr.connect(),0
self.dr.coords = image_points,0
self.drs.append(self.dr),0
self.updatedCoords.append(self.dr.coords),0
if np.isnan(self.points)[0] == False:,0
self.buttonCounter[ind].append(self.multibodyparts[c]),0
else:,0
if ind == 'single':,0
"for c, bp in enumerate(self.uniquebodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
"self.points = [self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter]]",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.uniquebodyparts)",0
color = self.colormap(self.norm(self.colorIndex[c])),0
"circle = patches.Circle((self.points[0], self.points[1]), radius=self.markerSize, fc = color, alpha=self.alpha)",0
#                    print(circle),0
print(self.axes),0
self.axes.add_patch(circle),0
"self.dr = auxfun_drag_label_multiple_individuals.DraggablePoint(circle,ind,self.uniquebodyparts[c])",0
self.dr.connect(),0
self.dr.coords = image_points,0
self.drs.append(self.dr),0
self.updatedCoords.append(self.dr.coords),0
if np.isnan(self.points)[0] == False:,0
self.buttonCounter[ind].append(self.uniquebodyparts[c]),0
else:,0
"for c, bp in enumerate(self.multibodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
"self.points = [self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter]]",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
color = self.colormap(self.norm(self.colorIndex[c])),0
"circle = patches.Circle((self.points[0], self.points[1]), radius=self.markerSize, fc = color, alpha=self.alpha)",0
#                    print(circle),0
print(self.axes),0
self.axes.add_patch(circle),0
"self.dr = auxfun_drag_label_multiple_individuals.DraggablePoint(circle,ind,self.multibodyparts[c])",0
self.dr.connect(),0
self.dr.coords = image_points,0
self.drs.append(self.dr),0
self.updatedCoords.append(self.dr.coords),0
if np.isnan(self.points)[0] == False:,0
self.buttonCounter[ind].append(self.multibodyparts[c]),0
,0
MainFrame.saveEachImage(self),0
self.figure.canvas.draw(),0
return(self.buttonCounter),0
print(self.dataFrame),0
Windows compatible,0
Checks if zoom/pan button is ON,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
convert the image to RGB as you are showing the image with matplotlib,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Checks if zoom/pan button is ON,0
matplotlib.use('Agg'),0
Load updated lists:,0
cfg['video_sets'][vn],0
"labeling_toolbox.show(config,Screens,scale_w,scale_h, winHack, img_scale)",0
plotting parameters:,0
folders = [Path(config).parent / 'labeled-data' /Path(i) for i in video_names],0
plt.savefig(str(Path(tmpfolder)/imagename.split(os.sep)[-1])),0
Let's check if the code is *not* run on windows (Source: #https://stackoverflow.com/questions/1325581/how-do-i-check-if-im-running-on-windows-in-python),0
but the paths are in windows format...,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(index,name.split(os.sep)[1])",0
"print(name,test_video_name)",0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
"trainIndexes, testIndexes = SplitTrials(range(len(Data.index)), trainFraction)",0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
load image to get dimensions:,0
"print ""Grayscale!""",0
check whether the labels are positive and inside the img,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
sort the models directories,0
get the shuffle index,0
read cfg file,0
create log file,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
"text = str(self.individual_names+','+self.bodyParts)",0
Check for variable correctness,0
"plt.close(""all"")",0
Moviepy:,0
Call the GUI to select the cropping parameters,0
Update the config.yaml file with current cropping parameters,0
store full frame from random location (good for augmentation),0
"clip=clip.crop(y1 = int(coords[2]),y2 = int(coords[3]),x1 = int(coords[0]), x2 = int(coords[1]))",0
close video.,0
"print(""DLC loaded in light mode; you cannot use the labeling GUI!"")",0
"def __init__(self, point,bodyParts):",0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
text = str(self.bodyParts),0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
"sess, inputs, outputs = predict.setup_pose_prediction(dlc_cfg)",0
#################################################,0
Datafolder,0
#################################################,0
looping over videos,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
from skimage.io import imread,0
"frame=imread(os.path.join(directory,framename),mode='RGB')",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/AlexEMG/DeepLabCut/pull/324",0
Parameters for augmentation with regard to cropping,0
"Added and described in ""Using DeepLabCut for 3D markerless pose estimation across species and behaviors""",0
Source: https://www.nature.com/articles/s41596-019-0176-0,0
"limit width  [-leftwidth*u-100,100+u*rightwidth] x [-bottomwith*u-100,100+u*topwidth] where u is always a (different) random number in unit interval",0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
Dependencies for anaysis,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"DLCscorer = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
import skimage.color,0
from skimage.io import imread,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
image = skimage.color.gray2rgb(image),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
from scipy.misc import imresize,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
Read file path for pose_config file. >> pass it on,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
print(end_points.keys()) >> to see what else is available.,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
from nets.mobilenet import conv_blocks as ops,0
from nets.mobilenet import mobilenet as lib,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
The next part of the code depends upon which tensorflow version you have.,0
if cfg.location_refinement:,0
assuming batchsize 1 here!,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
from scipy.misc import imresize,0
Augmentation functions,0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
Initializing variables if they don't exist...,0
what is the fraction of training samples with scaling augmentation?,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images into same size.,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
"from scipy.misc import imread, imresize",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
"from scipy.misc import imread, imresize",0
"from dataset.pose_dataset import Batch, data_to_input, mirror_joints_map, CropImage, DataItem",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
Process skeleton,0
save,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"deeplabcut.create_labeled_video(path_config_file,[newvideo], destfolder=dfolder,filtered=True)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
#create one split and make Shuffle 2 and 3 have the same split.,0
##Note that the new function in DLC 2.1 does that much easier...,0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Dowloading the calibration images,0
Deleting unneccesary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Importing the toolbox (takes several seconds),0
Loading example data set,0
deeplabcut.load_demo_data(path_config_file),0
shuffle=11 #>> imageio functions!,0
"print(""Analyze Video"")",0
"videofile_path = os.path.join(os.getcwd(),'openfield-Pranav-2018-10-30','videos','m3v1mp4.mp4')",0
"deeplabcut.analyze_videos(path_config_file,[videofile_path], shuffle=shuffle)",0
"print(""Create Labeled Video"")",0
"deeplabcut.create_labeled_video(path_config_file,[videofile_path],save_frames=False, shuffle=shuffle)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
if module is executed direcyly (i.e. `python -m deeplabcut.__init__`) launch straight into the GUI,0
Supress tensorflow warning messages,0
DLClight version does not support GUIs. Importing accordingly,0
"Train, evaluate & predict functions / require TF",0
docs[0]['init_weights'] = '../../pretrained/resnet_v1_101.ckpt',0
dict_test['init_weights'] = 'models/mpii/snapshot-1030000',0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Create the pose_config.yaml files,0
Analyze the videos,0
Read the config file,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
get vcap property,0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
"Import all videos in a folder or if just one video withouth [] passed, then make it a list.",0
"if isinstance(videos,str):",0
#there are two cases:,0
if os.path.isdir(videos): # it is a path!,0
path=videos,0
"videos=[os.path.join(path,vp) for vp in os.listdir(path) if videotype in vp]",0
if len(videos)==0:,0
"print(""No videos found in"",path,os.listdir(path))",0
"print(""Perhaps change the videotype, which is currently set to:"", videotype)",0
else:,0
"print(""Directory entered, "" , len(videos),"" videos were found."")",0
else:,0
if os.path.isfile(videos):,0
videos=[videos],0
https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
try:,0
"#shutil.copy(src,dst)",0
except OSError or TypeError: #https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
"shutil.copy(os.fspath(src),os.fspath(dst))",0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Set values to config file:,0
cfg_file['resnet']=50,0
Write dictionary to yaml  config file,0
import yaml,0
import cv2,0
import shutil,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
Flatten the list of bodyparts to connect,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
Start plotting for every frame,0
"Once all the frames are saved, then make a movie using ffmpeg.",0
Create the fig and define the axes,0
Clear plot and initialize the variables,0
Initialize arrays for appending the 3d data for actual plotting the points,0
Initialize arrays for appending the 3d data for drawing the lines,0
Initialize arrays for appending the 2d data from cam1 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam2 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam1 for drawing the lines,0
Initialize arrays for appending the 2d data from cam2 for drawing the lines,0
Get the scorer names from the dataframe,0
"Set the x,y, and z limits for the 3d view",0
"Set the frame number to read#max(0,index-trailpoints):index",0
Plot the labels for each body part,0
Connecting the bodyparts specified in the config file.3d file is created based on the likelihoods of cam1 and cam2. Using 3d file and check if the body part is nan then dont plot skeleton,0
Saving the frames,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
Extract the indices of frames where the likelihood of a bodypart for both cameras are less than pvalue,0
Extract frames where likelihood for both the views is less than the pcutoff,0
"low_likelihood_frames = np.all(likelihoods < pcutoff, axis=1)",0
For cam1 camera: Assign nans to x and y values of a bodypart where the likelihood for is less than pvalue,0
For cam2 camera: Assign nans to x and y values of a bodypart where the likelihood is less than pvalue,0
ToDo: speed up func. below by saving in numpy.array,1
"Create an empty dataframe to store x,y,z of 3d data",0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Undistorting the points from cam1 camera,0
Undistorting the points from cam2 camera,0
Save the undistorted files,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = io.imread(img),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
self.cropping = False,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
"cv2.imwrite(img_name, frame)",0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.slider.Enable(False),0
self.checkBox.Enable(False),0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
Checks if zoom/pan button is ON,0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
extract min and max index based on start stop interval.,0
figure out body part list:,0
all indices between start and stop with jump larger than epsilon (leading up to this point!),0
deviation_dataname = str(Path(videofolder)/Path(dataname)),0
Calculate deviatons for video,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
data.to_csv(dataname.split('.h5')[0]+'filtered.csv'),0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
"Dataframe = pd.read_hdf(os.path.join(videofolder,dataname+'.h5'))",0
drop duplicate labels:,0
updates iteration by 1,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
"self.choiceBox,self.visualization_rdb = self.choice_panel.addRadioButtons()",0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
take into account of all the bodyparts for the colorscheme. Sort the bodyparts to have same order as in the config file,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
self.figure.delaxes(self.figure.axes[1]) # Removes the axes corresponding to the colorbar,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
print(self.Dataframe.head()),0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
Checks if zoom/pan button is ON,0
"print(""DLC loaded in light mode; you cannot use the relabeling GUI!"")",0
###################################################,0
Dependencies,0
###################################################,0
import sys,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as specified in the config file,0
"rr, cc,val = line_aa(int(df_y[pair[0],index]),int(df_x[pair[0],index]),int(df_y[pair[1],index]), int(df_x[pair[1],index]))",0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Adds skeleton to the video,0
###################,0
less transparent present.,0
# One can change the parameters of the video creation script below:,0
See ffmpeg user guide: http://ffmpeg.org/ffmpeg.html#Video-and-Audio-file-format-conversion,0
,0
if notanalyzed:,0
"notanalyzed,outdataname,sourcedataname,DLCscorer=auxiliaryfunctions.CheckifPostProcessing(folder,vname,DLCscorer,DLCscorerlegacy,suffix='checking')",0
Loading cropping data used during analysis,0
!/usr/bin/env python3,0
"Historically DLC used: from scipy.misc import imread, imresize >> deprecated functions",0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
TODO check if those times exist...,1
"windows throws error if file path is > 260 characters, can fix with prefix. see https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation",0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
###################################################,0
Dependencies,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
"fig.colorbar(im, ax=ax)",0
#################################################,0
Looping analysis over video,0
#################################################,0
LoadData,0
"nlines,numcolumns=data.shape",0
assert(len(orderofbpincsv)==len(cfg['bodyparts'])),0
TODO: test len of images vs. len of imagenames for another sanity check,1
Is there a scorer for this?,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
"with open(projconfigfile, 'w') as cf:",0
"ruamelFile_3d.dump(cfg_file_3d, cf)",0
Read the pickle file,0
Write the pickle file,0
checks if input is a directory,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
variable initilization,0
design the panel,0
Add logo of DLC,0
Add all the options,0
,0
Hide the button as this is not the default option,0
self.ok.Enable(False),0
else:,0
self.ok.Enable(True),0
For mac compatibility,0
Remove the pages in case the user goes back to the create new project and creates/load a new project,0
Add all the other pages,0
"wx.Frame.__init__(self, None, title=""DeepLabCut"")",0
"wx.Frame.__init__( self, None, id = wx.ID_ANY, title = 'DeepLabCut',size = wx.Size(self.gui_size), pos = wx.DefaultPosition, style = wx.RESIZE_BORDER|wx.DEFAULT_FRAME_STYLE|wx.TAB_TRAVERSAL )",0
Here we create a panel and a notebook on the panel,0
create the page windows as children of the notebook and add the pages to the notebook with the label to show on the tab,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.crop_choice = wx.RadioBox(self, label='Want to crop the frames?', choices=['No', 'Yes'],majorDimension=1, style=wx.RA_SPECIFY_COLS)",0
"hbox1.Add(self.crop_choice,5, wx.EXPAND|wx.TOP|wx.BOTTOM, 5)",0
self.crop_choice.Enable(False),0
self.crop_choice.Enable(True),0
if self.crop_choice.GetStringSelection() == 'Yes':,0
crop = True,0
else:,0
crop = False,0
self.crop_choice.Enable(True),0
"self.crop_choice.SetStringSelection(""No"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
use the default pose_cfg file for default values,0
self.display_iters.Enable(False),0
self.save_iters.Enable(False),0
self.max_iters.Enable(False),0
self.snapshots.Enable(False),0
Read the pose config file,0
"print(os.path.join(cfg['project_path'],auxiliaryfunctions.GetModelFolder(trainFraction, self.shuffles.GetValue(),cfg),'train','pose_cfg.yaml'))",0
let the user open the file with default text editor. Also make it mac compatible,0
update the variables with the edited values in the pose config file,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.shuffles = wx.SpinCtrl(self, value='1',min=1,max=100)",0
"shuffles_text_boxsizer.Add(self.shuffles,1, wx.EXPAND|wx.TOP|wx.BOTTOM, 10)",0
if self.draw_skeleton.IsShow():,0
"if self.draw_skeleton.GetStringSelection() == ""Yes"":",0
self.draw = True,0
else:,0
self.draw = True,0
"print(self.config,self.filelist,self.videotype.GetValue(),shuffle,trainingsetindex,gputouse=None,save_as_csv=save_as_csv,destfolder=self.destfolder,cropping=cropping)",0
self.gpu.SetValue(-1),0
import wx.lib.scrolledpanel as SP,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
self.hbox2 = wx.BoxSizer(wx.HORIZONTAL),0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
"boxsizer.Add(self.hbox2,5, wx.EXPAND|wx.TOP|wx.BOTTOM, 10)",0
shuffle = self.shuffle.GetValue(),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.sel_config.SetPath(""Please select the config file"")",0
else:,0
deeplabcut.label_frames(self.config),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
#         design the panel,0
Add image of DLC,0
if editing this text make sure you add the '\n' to get the new line. The sizer is unable to format lines correctly.,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = cv2.imread(img),0
convert the image to RGB as you are showing the image with matplotlib,0
divider = make_axes_locatable(self.axes),0
"colorIndex = np.linspace(np.min(im),np.max(im),len(bodyparts))",0
"cax = divider.append_axes(""right"", size=""5%"", pad=0.05)",0
"cbar = self.figure.colorbar(ax, cax=cax,spacing='proportional', ticks=colorIndex)",0
cbar.set_ticklabels(bodyparts[::-1]),0
self.figure.canvas.draw(),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"MainFrame.saveDataSet(self, event)",0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"if self.uniquebodyparts == """":",0
self.uniquebodyparts = self.multibodyparts,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Create an empty data frame,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of old labels,0
Extracting the list of old unique labels,0
Checking if new labels are added or not,0
Checking if user added a new label,0
Found new labels in either multiple bodyparts or unique bodyparts,0
check if single is slected when radio buttons are changed,0
Checking if user added a new label,0
if self.new_Multibodyparts==[] and self.new_UniqueBodyparts==[]: # i.e. no new labels,0
"self.figure,self.axes,self.canvas,self.toolbar,self.image_axis = self.image_panel.drawplot(self.img,img_name,self.iter,self.index,self.multibodyparts,self.colormap,keep_view=self.view_locked)",0
"self.axes.callbacks.connect('xlim_changed', self.onZoom)",0
"self.axes.callbacks.connect('ylim_changed', self.onZoom)",0
"self.image_panel.addcolorbar(self.img,self.image_axis,self.iter,self.multibodyparts,self.colormap)",0
# Only show the bodyparts corresponding to single,0
if self.individual_names[0] == 'single':,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.uniquebodyparts,self.individual_names,self.file,self.markerSize)",0
else:,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.multibodyparts,self.individual_names,self.file,self.markerSize)",0
,0
"self.individualrdb.Bind(wx.EVT_RADIOBOX,self.select_individual)",0
,0
"# Get the color index, depending on if single is selcted or not.",0
"if self.rdb.GetStringSelection==""single"":",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.uniquebodyparts)",0
else:,0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
"self.cidClick = self.canvas.mpl_connect('button_press_event', self.onClick)",0
else:,0
# Found new labels in either multiple bodyparts or unique bodyparts,0
"dlg = wx.MessageDialog(None,""New label found in the config file. Do you want to see all the other labels?"", ""New label found"",wx.YES_NO | wx.ICON_WARNING)",0
result = dlg.ShowModal(),0
if result == wx.ID_NO:,0
if self.new_Multibodyparts!=[]:,0
self.multibodyparts = self.new_Multibodyparts,0
if self.new_UniqueBodyparts!=[]:,0
self.uniquebodyparts = self.new_UniqueBodyparts,0
,0
"self.dataFrame = MainFrame.create_dataframe(self,self.dataFrame,self.relativeimagenames,self.individual_names,self.new_UniqueBodyparts,self.new_Multibodyparts)",0
"self.figure,self.axes,self.canvas,self.toolbar,self.image_axis = self.image_panel.drawplot(self.img,img_name,self.iter,self.index,self.multibodyparts,self.colormap,keep_view=self.view_locked)",0
"self.axes.callbacks.connect('xlim_changed', self.onZoom)",0
"self.axes.callbacks.connect('ylim_changed', self.onZoom)",0
"self.image_panel.addcolorbar(self.img,self.image_axis,self.iter,self.multibodyparts,self.colormap)",0
,0
if self.individual_names[0] == 'single':,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.uniquebodyparts,self.individual_names,self.file,self.markerSize)",0
else:,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.multibodyparts,self.individual_names,self.file,self.markerSize)",0
"self.individualrdb.Bind(wx.EVT_RADIOBOX,self.select_individual)",0
"if self.rdb.GetStringSelection==""single"":",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.uniquebodyparts)",0
else:,0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
"self.cidClick = self.canvas.mpl_connect('button_press_event', self.onClick)",0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
"def plot(self,img):",0
self.drs= [],0
self.updatedCoords = [],0
for ind in self.individual_names:,0
if self.are_unique_bodyparts_present == False: #no unique bodyparts present,0
#            for ind in self.individual_names:,0
image_points = [],0
"for c, bp in enumerate(self.multibodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
"self.points = [self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter]]",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
color = self.colormap(self.norm(self.colorIndex[c])),0
"circle = patches.Circle((self.points[0], self.points[1]), radius=self.markerSize, fc = color, alpha=self.alpha)",0
#                    print(circle),0
print(self.axes),0
self.axes.add_patch(circle),0
"self.dr = auxfun_drag_label_multiple_individuals.DraggablePoint(circle,ind,self.multibodyparts[c])",0
self.dr.connect(),0
self.dr.coords = image_points,0
self.drs.append(self.dr),0
self.updatedCoords.append(self.dr.coords),0
if np.isnan(self.points)[0] == False:,0
self.buttonCounter[ind].append(self.multibodyparts[c]),0
else:,0
if ind == 'single':,0
"for c, bp in enumerate(self.uniquebodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
"self.points = [self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter]]",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.uniquebodyparts)",0
color = self.colormap(self.norm(self.colorIndex[c])),0
"circle = patches.Circle((self.points[0], self.points[1]), radius=self.markerSize, fc = color, alpha=self.alpha)",0
#                    print(circle),0
print(self.axes),0
self.axes.add_patch(circle),0
"self.dr = auxfun_drag_label_multiple_individuals.DraggablePoint(circle,ind,self.uniquebodyparts[c])",0
self.dr.connect(),0
self.dr.coords = image_points,0
self.drs.append(self.dr),0
self.updatedCoords.append(self.dr.coords),0
if np.isnan(self.points)[0] == False:,0
self.buttonCounter[ind].append(self.uniquebodyparts[c]),0
else:,0
"for c, bp in enumerate(self.multibodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
"self.points = [self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter]]",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
color = self.colormap(self.norm(self.colorIndex[c])),0
"circle = patches.Circle((self.points[0], self.points[1]), radius=self.markerSize, fc = color, alpha=self.alpha)",0
#                    print(circle),0
print(self.axes),0
self.axes.add_patch(circle),0
"self.dr = auxfun_drag_label_multiple_individuals.DraggablePoint(circle,ind,self.multibodyparts[c])",0
self.dr.connect(),0
self.dr.coords = image_points,0
self.drs.append(self.dr),0
self.updatedCoords.append(self.dr.coords),0
if np.isnan(self.points)[0] == False:,0
self.buttonCounter[ind].append(self.multibodyparts[c]),0
,0
MainFrame.saveEachImage(self),0
self.figure.canvas.draw(),0
return(self.buttonCounter),0
print(self.dataFrame),0
Windows compatible,0
Checks if zoom/pan button is ON,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
convert the image to RGB as you are showing the image with matplotlib,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Checks if zoom/pan button is ON,0
matplotlib.use('Agg'),0
Load updated lists:,0
cfg['video_sets'][vn],0
"labeling_toolbox.show(config,Screens,scale_w,scale_h, winHack, img_scale)",0
plotting parameters:,0
folders = [Path(config).parent / 'labeled-data' /Path(i) for i in video_names],0
plt.savefig(str(Path(tmpfolder)/imagename.split(os.sep)[-1])),0
Let's check if the code is *not* run on windows (Source: #https://stackoverflow.com/questions/1325581/how-do-i-check-if-im-running-on-windows-in-python),0
but the paths are in windows format...,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(index,name.split(os.sep)[1])",0
"print(name,test_video_name)",0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
"trainIndexes, testIndexes = SplitTrials(range(len(Data.index)), trainFraction)",0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
load image to get dimensions:,0
"print ""Grayscale!""",0
check whether the labels are positive and inside the img,0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
sort the models directories,0
get the shuffle index,0
read cfg file,0
create log file,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
"text = str(self.individual_names+','+self.bodyParts)",0
Check for variable correctness,0
"plt.close(""all"")",0
Moviepy:,0
Call the GUI to select the cropping parameters,0
Update the config.yaml file with current cropping parameters,0
store full frame from random location (good for augmentation),0
"clip=clip.crop(y1 = int(coords[2]),y2 = int(coords[3]),x1 = int(coords[0]), x2 = int(coords[1]))",0
close video.,0
"print(""DLC loaded in light mode; you cannot use the labeling GUI!"")",0
"def __init__(self, point,bodyParts):",0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
text = str(self.bodyParts),0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
"sess, inputs, outputs = predict.setup_pose_prediction(dlc_cfg)",0
#################################################,0
Datafolder,0
#################################################,0
looping over videos,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
from skimage.io import imread,0
"frame=imread(os.path.join(directory,framename),mode='RGB')",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/AlexEMG/DeepLabCut/pull/324",0
Parameters for augmentation with regard to cropping,0
"Added and described in ""Using DeepLabCut for 3D markerless pose estimation across species and behaviors""",0
Source: https://www.nature.com/articles/s41596-019-0176-0,0
"limit width  [-leftwidth*u-100,100+u*rightwidth] x [-bottomwith*u-100,100+u*topwidth] where u is always a (different) random number in unit interval",0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
Dependencies for anaysis,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"DLCscorer = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
import skimage.color,0
from skimage.io import imread,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
image = skimage.color.gray2rgb(image),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
from scipy.misc import imresize,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
Read file path for pose_config file. >> pass it on,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
print(end_points.keys()) >> to see what else is available.,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
from nets.mobilenet import conv_blocks as ops,0
from nets.mobilenet import mobilenet as lib,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
The next part of the code depends upon which tensorflow version you have.,0
if cfg.location_refinement:,0
assuming batchsize 1 here!,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
from scipy.misc import imresize,0
Augmentation functions,0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
Initializing variables if they don't exist...,0
what is the fraction of training samples with scaling augmentation?,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
make sure joint ids are 0-indexed,0
Scale is sampled only once to transform all of the images into same size.,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
import imageio,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
"from scipy.misc import imread, imresize",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
"from scipy.misc import imread, imresize",0
"from dataset.pose_dataset import Batch, data_to_input, mirror_joints_map, CropImage, DataItem",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
Process skeleton,0
save,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"deeplabcut.create_labeled_video(path_config_file,[newvideo], destfolder=dfolder,filtered=True)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
#create one split and make Shuffle 2 and 3 have the same split.,0
##Note that the new function in DLC 2.1 does that much easier...,0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Dowloading the calibration images,0
Deleting unneccesary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Importing the toolbox (takes several seconds),0
Loading example data set,0
deeplabcut.load_demo_data(path_config_file),0
shuffle=11 #>> imageio functions!,0
"print(""Analyze Video"")",0
"videofile_path = os.path.join(os.getcwd(),'openfield-Pranav-2018-10-30','videos','m3v1mp4.mp4')",0
"deeplabcut.analyze_videos(path_config_file,[videofile_path], shuffle=shuffle)",0
"print(""Create Labeled Video"")",0
"deeplabcut.create_labeled_video(path_config_file,[videofile_path],save_frames=False, shuffle=shuffle)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=None,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
Supress tensorflow warning messages,0
DLClight version does not support GUIs. Importing accordingly,0
"Train, evaluate & predict functions / require TF",0
docs[0]['init_weights'] = '../../pretrained/resnet_v1_101.ckpt',0
dict_test['init_weights'] = 'models/mpii/snapshot-1030000',0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Create the pose_config.yaml files,0
Analyze the videos,0
Read the config file,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
get vcap property,0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
"Import all videos in a folder or if just one video withouth [] passed, then make it a list.",0
"if isinstance(videos,str):",0
#there are two cases:,0
if os.path.isdir(videos): # it is a path!,0
path=videos,0
"videos=[os.path.join(path,vp) for vp in os.listdir(path) if videotype in vp]",0
if len(videos)==0:,0
"print(""No videos found in"",path,os.listdir(path))",0
"print(""Perhaps change the videotype, which is currently set to:"", videotype)",0
else:,0
"print(""Directory entered, "" , len(videos),"" videos were found."")",0
else:,0
if os.path.isfile(videos):,0
videos=[videos],0
https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
try:,0
"#shutil.copy(src,dst)",0
except OSError or TypeError: #https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
"shutil.copy(os.fspath(src),os.fspath(dst))",0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Set values to config file:,0
cfg_file['resnet']=50,0
Write dictionary to yaml  config file,0
import yaml,0
import cv2,0
import shutil,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
Flatten the list of bodyparts to connect,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
Start plotting for every frame,0
"Once all the frames are saved, then make a movie using ffmpeg.",0
Create the fig and define the axes,0
Clear plot and initialize the variables,0
Initialize arrays for appending the 3d data for actual plotting the points,0
Initialize arrays for appending the 3d data for drawing the lines,0
Initialize arrays for appending the 2d data from cam1 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam2 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam1 for drawing the lines,0
Initialize arrays for appending the 2d data from cam2 for drawing the lines,0
Get the scorer names from the dataframe,0
"Set the x,y, and z limits for the 3d view",0
"Set the frame number to read#max(0,index-trailpoints):index",0
Plot the labels for each body part,0
Connecting the bodyparts specified in the config file.3d file is created based on the likelihoods of cam1 and cam2. Using 3d file and check if the body part is nan then dont plot skeleton,0
Saving the frames,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
Extract the indices of frames where the likelihood of a bodypart for both cameras are less than pvalue,0
Extract frames where likelihood for both the views is less than the pcutoff,0
"low_likelihood_frames = np.all(likelihoods < pcutoff, axis=1)",0
For cam1 camera: Assign nans to x and y values of a bodypart where the likelihood for is less than pvalue,0
For cam2 camera: Assign nans to x and y values of a bodypart where the likelihood is less than pvalue,0
ToDo: speed up func. below by saving in numpy.array,1
"Create an empty dataframe to store x,y,z of 3d data",0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Undistorting the points from cam1 camera,0
Undistorting the points from cam2 camera,0
Save the undistorted files,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = io.imread(img),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
self.cropping = False,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
"cv2.imwrite(img_name, frame)",0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.slider.Enable(False),0
self.checkBox.Enable(False),0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
Checks if zoom/pan button is ON,0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
extract min and max index based on start stop interval.,0
figure out body part list:,0
all indices between start and stop with jump larger than epsilon (leading up to this point!),0
deviation_dataname = str(Path(videofolder)/Path(dataname)),0
Calculate deviatons for video,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
data.to_csv(dataname.split('.h5')[0]+'filtered.csv'),0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
"Dataframe = pd.read_hdf(os.path.join(videofolder,dataname+'.h5'))",0
drop duplicate labels:,0
updates iteration by 1,0
###################################################,0
Dependencies,0
###################################################,0
import sys,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as specified in the config file,0
"rr, cc,val = line_aa(int(df_y[pair[0],index]),int(df_x[pair[0],index]),int(df_y[pair[1],index]), int(df_x[pair[1],index]))",0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Adds skeleton to the video,0
###################,0
less transparent present.,0
# One can change the parameters of the video creation script below:,0
See ffmpeg user guide: http://ffmpeg.org/ffmpeg.html#Video-and-Audio-file-format-conversion,0
,0
if notanalyzed:,0
"notanalyzed,outdataname,sourcedataname,DLCscorer=auxiliaryfunctions.CheckifPostProcessing(folder,vname,DLCscorer,DLCscorerlegacy,suffix='checking')",0
Loading cropping data used during analysis,0
!/usr/bin/env python3,0
import imageio,0
from skimage import io,0
from skimage.util import img_as_ubyte,0
from PIL import Image,0
"Historically DLC used: from scipy.misc import imread, imresize >> deprecated functions",0
"print(np.max(a),np.min(a),np.median(a))",0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
"height, width, depth = img.shape",0
imgScale = W/width,0
"newX,newY = oriimg.shape[1]*imgScale, oriimg.shape[0]*imgScale",0
"a=cv2.resize(img,(int(height*size),int(width*size)))",0
print(np.shape(a)),0
"print(np.max(a),np.min(a),np.median(a))",0
TODO check if those times exist...,1
"windows throws error if file path is > 260 characters, can fix with prefix. see https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation",0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
###################################################,0
Dependencies,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
"fig.colorbar(im, ax=ax)",0
#################################################,0
Looping analysis over video,0
#################################################,0
LoadData,0
"nlines,numcolumns=data.shape",0
assert(len(orderofbpincsv)==len(cfg['bodyparts'])),0
TODO: test len of images vs. len of imagenames for another sanity check,1
Is there a scorer for this?,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
"with open(projconfigfile, 'w') as cf:",0
"ruamelFile_3d.dump(cfg_file_3d, cf)",0
Read the pickle file,0
Write the pickle file,0
Creates an empty dataFrame of same shape as df_side_view.,0
"flag = number of coordinates. e.g. 4 for 3d,3 for 2d as we need to store the likelihood too.",0
df = pd.read_hdf(dataframe),0
checks if input is a directory,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
variable initilization,0
design the panel,0
Add logo of DLC,0
Add all the options,0
,0
Hide the button as this is not the default option,0
"def help_function(self,event):",0
"wx.MessageBox(""1. Choose an option to create a new project or load an existing project.\n\n2. To create a new project, enter the name of project, experimenter, and choose videos to add to the project.\n\n3. As an optional step, choose a directory where you need to create the project and specify if you need to copy the videos\n\n4. To load an existing project, use the active Browse button to select the config.yaml file of the project"",'Help',wx.OK | wx.ICON_INFORMATION)",0
self.ok.Enable(False),0
else:,0
self.ok.Enable(True),0
For mac compatibility,0
Remove the pages in case the user goes back to the create new project and creates/load a new project,0
Add all the other pages,0
"wx.Frame.__init__(self, None, title=""DeepLabCut"")",0
"wx.Frame.__init__( self, None, id = wx.ID_ANY, title = 'DeepLabCut',size = wx.Size(self.gui_size), pos = wx.DefaultPosition, style = wx.RESIZE_BORDER|wx.DEFAULT_FRAME_STYLE|wx.TAB_TRAVERSAL )",0
Here we create a panel and a notebook on the panel,0
create the page windows as children of the notebook and add the pages to the notebook with the label to show on the tab,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.crop_choice = wx.RadioBox(self, label='Want to crop the frames?', choices=['No', 'Yes'],majorDimension=1, style=wx.RA_SPECIFY_COLS)",0
"hbox1.Add(self.crop_choice,5, wx.EXPAND|wx.TOP|wx.BOTTOM, 5)",0
self.crop_choice.Enable(False),0
self.crop_choice.Enable(True),0
if self.crop_choice.GetStringSelection() == 'Yes':,0
crop = True,0
else:,0
crop = False,0
self.crop_choice.Enable(True),0
"self.crop_choice.SetStringSelection(""No"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
use the default pose_cfg file for default values,0
self.display_iters.Enable(False),0
self.save_iters.Enable(False),0
self.max_iters.Enable(False),0
self.snapshots.Enable(False),0
Read the pose config file,0
"print(os.path.join(cfg['project_path'],auxiliaryfunctions.GetModelFolder(trainFraction, self.shuffles.GetValue(),cfg),'train','pose_cfg.yaml'))",0
let the user open the file with default text editor. Also make it mac compatible,0
update the variables with the edited values in the pose config file,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.shuffles = wx.SpinCtrl(self, value='1',min=1,max=100)",0
"shuffles_text_boxsizer.Add(self.shuffles,1, wx.EXPAND|wx.TOP|wx.BOTTOM, 10)",0
if self.draw_skeleton.IsShow():,0
"if self.draw_skeleton.GetStringSelection() == ""Yes"":",0
self.draw = True,0
else:,0
self.draw = True,0
"print(self.config,self.filelist,self.videotype.GetValue(),shuffle,trainingsetindex,gputouse=None,save_as_csv=save_as_csv,destfolder=self.destfolder,cropping=cropping)",0
self.gpu.SetValue(-1),0
import wx.lib.scrolledpanel as SP,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
self.hbox2 = wx.BoxSizer(wx.HORIZONTAL),0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
"boxsizer.Add(self.hbox2,5, wx.EXPAND|wx.TOP|wx.BOTTOM, 10)",0
shuffle = self.shuffle.GetValue(),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.sel_config.SetPath(""Please select the config file"")",0
else:,0
deeplabcut.label_frames(self.config),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
#         design the panel,0
Add image of DLC,0
if editing this text make sure you add the '\n' to get the new line. The sizer is unable to format lines correctly.,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = cv2.imread(img),0
convert the image to RGB as you are showing the image with matplotlib,0
divider = make_axes_locatable(self.axes),0
"colorIndex = np.linspace(np.min(im),np.max(im),len(bodyparts))",0
"cax = divider.append_axes(""right"", size=""5%"", pad=0.05)",0
"cbar = self.figure.colorbar(ax, cax=cax,spacing='proportional', ticks=colorIndex)",0
cbar.set_ticklabels(bodyparts[::-1]),0
self.figure.canvas.draw(),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"MainFrame.saveDataSet(self, event)",0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"if self.uniquebodyparts == """":",0
self.uniquebodyparts = self.multibodyparts,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Create an empty data frame,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of old labels,0
Extracting the list of old unique labels,0
Checking if new labels are added or not,0
Checking if user added a new label,0
Found new labels in either multiple bodyparts or unique bodyparts,0
check if single is slected when radio buttons are changed,0
Checking if user added a new label,0
if self.new_Multibodyparts==[] and self.new_UniqueBodyparts==[]: # i.e. no new labels,0
"self.figure,self.axes,self.canvas,self.toolbar,self.image_axis = self.image_panel.drawplot(self.img,img_name,self.iter,self.index,self.multibodyparts,self.colormap,keep_view=self.view_locked)",0
"self.axes.callbacks.connect('xlim_changed', self.onZoom)",0
"self.axes.callbacks.connect('ylim_changed', self.onZoom)",0
"self.image_panel.addcolorbar(self.img,self.image_axis,self.iter,self.multibodyparts,self.colormap)",0
# Only show the bodyparts corresponding to single,0
if self.individual_names[0] == 'single':,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.uniquebodyparts,self.individual_names,self.file,self.markerSize)",0
else:,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.multibodyparts,self.individual_names,self.file,self.markerSize)",0
,0
"self.individualrdb.Bind(wx.EVT_RADIOBOX,self.select_individual)",0
,0
"# Get the color index, depending on if single is selcted or not.",0
"if self.rdb.GetStringSelection==""single"":",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.uniquebodyparts)",0
else:,0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
"self.cidClick = self.canvas.mpl_connect('button_press_event', self.onClick)",0
else:,0
# Found new labels in either multiple bodyparts or unique bodyparts,0
"dlg = wx.MessageDialog(None,""New label found in the config file. Do you want to see all the other labels?"", ""New label found"",wx.YES_NO | wx.ICON_WARNING)",0
result = dlg.ShowModal(),0
if result == wx.ID_NO:,0
if self.new_Multibodyparts!=[]:,0
self.multibodyparts = self.new_Multibodyparts,0
if self.new_UniqueBodyparts!=[]:,0
self.uniquebodyparts = self.new_UniqueBodyparts,0
,0
"self.dataFrame = MainFrame.create_dataframe(self,self.dataFrame,self.relativeimagenames,self.individual_names,self.new_UniqueBodyparts,self.new_Multibodyparts)",0
"self.figure,self.axes,self.canvas,self.toolbar,self.image_axis = self.image_panel.drawplot(self.img,img_name,self.iter,self.index,self.multibodyparts,self.colormap,keep_view=self.view_locked)",0
"self.axes.callbacks.connect('xlim_changed', self.onZoom)",0
"self.axes.callbacks.connect('ylim_changed', self.onZoom)",0
"self.image_panel.addcolorbar(self.img,self.image_axis,self.iter,self.multibodyparts,self.colormap)",0
,0
if self.individual_names[0] == 'single':,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.uniquebodyparts,self.individual_names,self.file,self.markerSize)",0
else:,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.multibodyparts,self.individual_names,self.file,self.markerSize)",0
"self.individualrdb.Bind(wx.EVT_RADIOBOX,self.select_individual)",0
"if self.rdb.GetStringSelection==""single"":",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.uniquebodyparts)",0
else:,0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
"self.cidClick = self.canvas.mpl_connect('button_press_event', self.onClick)",0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
"def plot(self,img):",0
self.drs= [],0
self.updatedCoords = [],0
for ind in self.individual_names:,0
if self.are_unique_bodyparts_present == False: #no unique bodyparts present,0
#            for ind in self.individual_names:,0
image_points = [],0
"for c, bp in enumerate(self.multibodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
"self.points = [self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter]]",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
color = self.colormap(self.norm(self.colorIndex[c])),0
"circle = patches.Circle((self.points[0], self.points[1]), radius=self.markerSize, fc = color, alpha=self.alpha)",0
#                    print(circle),0
print(self.axes),0
self.axes.add_patch(circle),0
"self.dr = auxfun_drag_label_multiple_individuals.DraggablePoint(circle,ind,self.multibodyparts[c])",0
self.dr.connect(),0
self.dr.coords = image_points,0
self.drs.append(self.dr),0
self.updatedCoords.append(self.dr.coords),0
if np.isnan(self.points)[0] == False:,0
self.buttonCounter[ind].append(self.multibodyparts[c]),0
else:,0
if ind == 'single':,0
"for c, bp in enumerate(self.uniquebodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
"self.points = [self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter]]",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.uniquebodyparts)",0
color = self.colormap(self.norm(self.colorIndex[c])),0
"circle = patches.Circle((self.points[0], self.points[1]), radius=self.markerSize, fc = color, alpha=self.alpha)",0
#                    print(circle),0
print(self.axes),0
self.axes.add_patch(circle),0
"self.dr = auxfun_drag_label_multiple_individuals.DraggablePoint(circle,ind,self.uniquebodyparts[c])",0
self.dr.connect(),0
self.dr.coords = image_points,0
self.drs.append(self.dr),0
self.updatedCoords.append(self.dr.coords),0
if np.isnan(self.points)[0] == False:,0
self.buttonCounter[ind].append(self.uniquebodyparts[c]),0
else:,0
"for c, bp in enumerate(self.multibodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
"self.points = [self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter]]",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
color = self.colormap(self.norm(self.colorIndex[c])),0
"circle = patches.Circle((self.points[0], self.points[1]), radius=self.markerSize, fc = color, alpha=self.alpha)",0
#                    print(circle),0
print(self.axes),0
self.axes.add_patch(circle),0
"self.dr = auxfun_drag_label_multiple_individuals.DraggablePoint(circle,ind,self.multibodyparts[c])",0
self.dr.connect(),0
self.dr.coords = image_points,0
self.drs.append(self.dr),0
self.updatedCoords.append(self.dr.coords),0
if np.isnan(self.points)[0] == False:,0
self.buttonCounter[ind].append(self.multibodyparts[c]),0
,0
MainFrame.saveEachImage(self),0
self.figure.canvas.draw(),0
return(self.buttonCounter),0
print(self.dataFrame),0
Windows compatible,0
Checks if zoom/pan button is ON,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
convert the image to RGB as you are showing the image with matplotlib,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Checks if zoom/pan button is ON,0
matplotlib.use('Agg'),0
Load updated lists:,0
cfg['video_sets'][vn],0
"labeling_toolbox.show(config,Screens,scale_w,scale_h, winHack, img_scale)",0
plotting parameters:,0
folders = [Path(config).parent / 'labeled-data' /Path(i) for i in video_names],0
plt.savefig(str(Path(tmpfolder)/imagename.split(os.sep)[-1])),0
Let's check if the code is *not* run on windows (Source: #https://stackoverflow.com/questions/1325581/how-do-i-check-if-im-running-on-windows-in-python),0
but the paths are in windows format...,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(index,name.split(os.sep)[1])",0
"print(name,test_video_name)",0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
"trainIndexes, testIndexes = SplitTrials(range(len(Data.index)), trainFraction)",0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
load image to get dimensions:,0
"print ""Grayscale!""",0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
sort the models directories,0
get the shuffle index,0
read cfg file,0
create log file,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
"text = str(self.individual_names+','+self.bodyParts)",0
Check for variable correctness,0
"plt.close(""all"")",0
Moviepy:,0
Call the GUI to select the cropping parameters,0
Update the config.yaml file with current cropping parameters,0
store full frame from random location (good for augmentation),0
"clip=clip.crop(y1 = int(coords[2]),y2 = int(coords[3]),x1 = int(coords[0]), x2 = int(coords[1]))",0
close video.,0
"def __init__(self, point,bodyParts):",0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
text = str(self.bodyParts),0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
"sess, inputs, outputs = predict.setup_pose_prediction(dlc_cfg)",0
#################################################,0
Datafolder,0
#################################################,0
looping over videos,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
from skimage.io import imread,0
"frame=imread(os.path.join(directory,framename),mode='RGB')",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/AlexEMG/DeepLabCut/pull/324",0
Parameters for augmentation with regard to cropping,0
"Added and described in ""Using DeepLabCut for 3D markerless pose estimation across species and behaviors""",0
Source: https://www.nature.com/articles/s41596-019-0176-0,0
"limit width  [-leftwidth*u-100,100+u*rightwidth] x [-bottomwith*u-100,100+u*topwidth] where u is always a (different) random number in unit interval",0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
Dependencies for anaysis,0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
"Data=pd.read_hdf(os.path.join(cfg[""project_path""],str(trainingsetfolder),'CollectedData_' + cfg[""scorer""] + '.h5'),'df_with_missing')",0
Get list of body parts to evaluate network for,0
#################################################,0
Load data...,0
#################################################,0
Load meta data,0
########################## RESCALING (to global scale),0
Check which snapshots are available and sort them by # iterations,0
name for deeplabcut net (based on its parameters),0
"DLCscorer = auxiliaryfunctions.GetScorerName(cfg,shuffle,trainFraction,trainingsiterations)",0
"resultsfilename=os.path.join(str(evaluationfolder),DLCscorer + '-' + str(Snapshots[snapindex])+  '.h5') # + '-' + str(snapshot)+  ' #'-' + Snapshots[snapindex]+  '.h5')",0
import skimage.color,0
from skimage.io import imread,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
image = skimage.color.gray2rgb(image),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
from scipy.misc import imresize,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
Read file path for pose_config file. >> pass it on,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
print(end_points.keys()) >> to see what else is available.,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
from nets.mobilenet import conv_blocks as ops,0
from nets.mobilenet import mobilenet as lib,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
The next part of the code depends upon which tensorflow version you have.,0
if cfg.location_refinement:,0
assuming batchsize 1 here!,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
from scipy.misc import imresize,0
Augmentation functions,0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
Initializing variables if they don't exist...,0
what is the fraction of training samples with scaling augmentation?,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
make sure joint ids are 0-indexed,0
Load Matlab file dataset annotation,0
mlab = sio.loadmat(file_name),0
"mlab = sio.loadmat(os.path.join(self.cfg.project_path,file_name))",0
Pickle the 'data' dictionary using the highest protocol available.,0
mlab = mlab['dataset'],0
Scale is sampled only once to transform all of the images into same size.,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
"from scipy.misc import imread, imresize",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
"from scipy.misc import imread, imresize",0
"from dataset.pose_dataset import Batch, data_to_input, mirror_joints_map, CropImage, DataItem",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
Process skeleton,0
save,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"deeplabcut.create_labeled_video(path_config_file,[newvideo], destfolder=dfolder,filtered=True)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
#create one split and make Shuffle 2 and 3 have the same split.,0
##Note that the new function in DLC 2.1 does that much easier...,0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Dowloading the calibration images,0
Deleting unneccesary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Importing the toolbox (takes several seconds),0
Loading example data set,0
deeplabcut.load_demo_data(path_config_file),0
shuffle=11 #>> imageio functions!,0
"print(""Analyze Video"")",0
"videofile_path = os.path.join(os.getcwd(),'openfield-Pranav-2018-10-30','videos','m3v1mp4.mp4')",0
"deeplabcut.analyze_videos(path_config_file,[videofile_path], shuffle=shuffle)",0
"print(""Create Labeled Video"")",0
"deeplabcut.create_labeled_video(path_config_file,[videofile_path],save_frames=False, shuffle=shuffle)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://www.python.org/dev/peps/pep-0440/#compatible-release,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
Supress tensorflow warning messages,0
DLClight version does not support GUIs. Importing accordingly,0
"Train, evaluate & predict functions / require TF",0
docs[0]['init_weights'] = '../../pretrained/resnet_v1_101.ckpt',0
dict_test['init_weights'] = 'models/mpii/snapshot-1030000',0
Create the model directory,0
Download the weights and put then in appropriate directory,0
Create the pose_config.yaml files,0
Analyze the videos,0
Read the config file,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
get vcap property,0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
"Import all videos in a folder or if just one video withouth [] passed, then make it a list.",0
"if isinstance(videos,str):",0
#there are two cases:,0
if os.path.isdir(videos): # it is a path!,0
path=videos,0
"videos=[os.path.join(path,vp) for vp in os.listdir(path) if videotype in vp]",0
if len(videos)==0:,0
"print(""No videos found in"",path,os.listdir(path))",0
"print(""Perhaps change the videotype, which is currently set to:"", videotype)",0
else:,0
"print(""Directory entered, "" , len(videos),"" videos were found."")",0
else:,0
if os.path.isfile(videos):,0
videos=[videos],0
https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
try:,0
"#shutil.copy(src,dst)",0
except OSError or TypeError: #https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
"shutil.copy(os.fspath(src),os.fspath(dst))",0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Set values to config file:,0
cfg_file['resnet']=50,0
Write dictionary to yaml  config file,0
import yaml,0
import cv2,0
import shutil,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
Flatten the list of bodyparts to connect,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
name for output video,0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
Start plotting for every frame,0
"Once all the frames are saved, then make a movie using ffmpeg.",0
Create the fig and define the axes,0
Clear plot and initialize the variables,0
Initialize arrays for appending the 3d data for actual plotting the points,0
Initialize arrays for appending the 3d data for drawing the lines,0
Initialize arrays for appending the 2d data from cam1 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam2 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam1 for drawing the lines,0
Initialize arrays for appending the 2d data from cam2 for drawing the lines,0
Get the scorer names from the dataframe,0
"Set the x,y, and z limits for the 3d view",0
"Set the frame number to read#max(0,index-trailpoints):index",0
Plot the labels for each body part,0
Connecting the bodyparts specified in the config file.3d file is created based on the likelihoods of cam1 and cam2. Using 3d file and check if the body part is nan then dont plot skeleton,0
Saving the frames,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
In case user adds save_as_csv is True after triangulating,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
Extract the indices of frames where the likelihood of a bodypart for both cameras are less than pvalue,0
Extract frames where likelihood for both the views is less than the pcutoff,0
"low_likelihood_frames = np.all(likelihoods < pcutoff, axis=1)",0
For cam1 camera: Assign nans to x and y values of a bodypart where the likelihood for is less than pvalue,0
For cam2 camera: Assign nans to x and y values of a bodypart where the likelihood is less than pvalue,0
ToDo: speed up func. below by saving in numpy.array,1
"Create an empty dataframe to store x,y,z of 3d data",0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Undistorting the points from cam1 camera,0
Undistorting the points from cam2 camera,0
Save the undistorted files,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = io.imread(img),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
self.cropping = False,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
"cv2.imwrite(img_name, frame)",0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.slider.Enable(False),0
self.checkBox.Enable(False),0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
Checks if zoom/pan button is ON,0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
extract min and max index based on start stop interval.,0
figure out body part list:,0
all indices between start and stop with jump larger than epsilon (leading up to this point!),0
deviation_dataname = str(Path(videofolder)/Path(dataname)),0
Calculate deviatons for video,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
data.to_csv(dataname.split('.h5')[0]+'filtered.csv'),0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
"Dataframe = pd.read_hdf(os.path.join(videofolder,dataname+'.h5'))",0
drop duplicate labels:,0
updates iteration by 1,0
###################################################,0
Dependencies,0
###################################################,0
import sys,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as specified in the config file,0
"rr, cc,val = line_aa(int(df_y[pair[0],index]),int(df_x[pair[0],index]),int(df_y[pair[1],index]), int(df_x[pair[1],index]))",0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Adds skeleton to the video,0
###################,0
less transparent present.,0
# One can change the parameters of the video creation script below:,0
See ffmpeg user guide: http://ffmpeg.org/ffmpeg.html#Video-and-Audio-file-format-conversion,0
,0
if notanalyzed:,0
"notanalyzed,outdataname,sourcedataname,DLCscorer=auxiliaryfunctions.CheckifPostProcessing(folder,vname,DLCscorer,DLCscorerlegacy,suffix='checking')",0
Loading cropping data used during analysis,0
!/usr/bin/env python3,0
import imageio,0
from skimage import io,0
from skimage.util import img_as_ubyte,0
from PIL import Image,0
"Historically DLC used: from scipy.misc import imread, imresize >> deprecated functions",0
"print(np.max(a),np.min(a),np.median(a))",0
https://docs.opencv.org/3.4.0/da/d54/group__imgproc__transform.html#ga5bb5a1fea74ea38e1a5445ca803ff121,0
"height, width, depth = img.shape",0
imgScale = W/width,0
"newX,newY = oriimg.shape[1]*imgScale, oriimg.shape[0]*imgScale",0
"a=cv2.resize(img,(int(height*size),int(width*size)))",0
print(np.shape(a)),0
"print(np.max(a),np.min(a),np.median(a))",0
TODO check if those times exist...,1
"windows throws error if file path is > 260 characters, can fix with prefix. see https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation",0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
###################################################,0
Dependencies,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
"fig.colorbar(im, ax=ax)",0
#################################################,0
Looping analysis over video,0
#################################################,0
LoadData,0
"nlines,numcolumns=data.shape",0
assert(len(orderofbpincsv)==len(cfg['bodyparts'])),0
TODO: test len of images vs. len of imagenames for another sanity check,1
Is there a scorer for this?,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
"with open(projconfigfile, 'w') as cf:",0
"ruamelFile_3d.dump(cfg_file_3d, cf)",0
Read the pickle file,0
Write the pickle file,0
Creates an empty dataFrame of same shape as df_side_view.,0
"flag = number of coordinates. e.g. 4 for 3d,3 for 2d as we need to store the likelihood too.",0
df = pd.read_hdf(dataframe),0
checks if input is a directory,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
legacy scorername until DLC 2.1. (cfg['resnet'] is deprecated / which is why we get the resnet_xyz name from dlc_cfg!,0
"scorer_legacy = 'DeepCut' + ""_resnet"" + str(cfg['resnet']) + ""_"" + Task + str(date) + 'shuffle' + str(shuffle) + '_' + str(trainingsiterations)",0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
variable initilization,0
design the panel,0
Add logo of DLC,0
Add all the options,0
,0
Hide the button as this is not the default option,0
"def help_function(self,event):",0
"wx.MessageBox(""1. Choose an option to create a new project or load an existing project.\n\n2. To create a new project, enter the name of project, experimenter, and choose videos to add to the project.\n\n3. As an optional step, choose a directory where you need to create the project and specify if you need to copy the videos\n\n4. To load an existing project, use the active Browse button to select the config.yaml file of the project"",'Help',wx.OK | wx.ICON_INFORMATION)",0
self.ok.Enable(False),0
else:,0
self.ok.Enable(True),0
For mac compatibility,0
Remove the pages in case the user goes back to the create new project and creates/load a new project,0
Add all the other pages,0
"wx.Frame.__init__(self, None, title=""DeepLabCut"")",0
"wx.Frame.__init__( self, None, id = wx.ID_ANY, title = 'DeepLabCut',size = wx.Size(self.gui_size), pos = wx.DefaultPosition, style = wx.RESIZE_BORDER|wx.DEFAULT_FRAME_STYLE|wx.TAB_TRAVERSAL )",0
Here we create a panel and a notebook on the panel,0
create the page windows as children of the notebook and add the pages to the notebook with the label to show on the tab,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.crop_choice = wx.RadioBox(self, label='Want to crop the frames?', choices=['No', 'Yes'],majorDimension=1, style=wx.RA_SPECIFY_COLS)",0
"hbox1.Add(self.crop_choice,5, wx.EXPAND|wx.TOP|wx.BOTTOM, 5)",0
self.crop_choice.Enable(False),0
self.crop_choice.Enable(True),0
if self.crop_choice.GetStringSelection() == 'Yes':,0
crop = True,0
else:,0
crop = False,0
self.crop_choice.Enable(True),0
"self.crop_choice.SetStringSelection(""No"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
use the default pose_cfg file for default values,0
self.display_iters.Enable(False),0
self.save_iters.Enable(False),0
self.max_iters.Enable(False),0
self.snapshots.Enable(False),0
Read the pose config file,0
"print(os.path.join(cfg['project_path'],auxiliaryfunctions.GetModelFolder(trainFraction, self.shuffles.GetValue(),cfg),'train','pose_cfg.yaml'))",0
let the user open the file with default text editor. Also make it mac compatible,0
update the variables with the edited values in the pose config file,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.shuffles = wx.SpinCtrl(self, value='1',min=1,max=100)",0
"shuffles_text_boxsizer.Add(self.shuffles,1, wx.EXPAND|wx.TOP|wx.BOTTOM, 10)",0
if self.draw_skeleton.IsShow():,0
"if self.draw_skeleton.GetStringSelection() == ""Yes"":",0
self.draw = True,0
else:,0
self.draw = True,0
"print(self.config,self.filelist,self.videotype.GetValue(),shuffle,trainingsetindex,gputouse=None,save_as_csv=save_as_csv,destfolder=self.destfolder,cropping=cropping)",0
self.gpu.SetValue(-1),0
import wx.lib.scrolledpanel as SP,0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
self.hbox2 = wx.BoxSizer(wx.HORIZONTAL),0
self.SetSizer(self.sizer),0
self.sizer.Fit(self),0
"boxsizer.Add(self.hbox2,5, wx.EXPAND|wx.TOP|wx.BOTTOM, 10)",0
shuffle = self.shuffle.GetValue(),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
"self.sel_config.SetPath(""Please select the config file"")",0
else:,0
deeplabcut.label_frames(self.config),0
variable initilization,0
design the panel,0
Add logo of DLC,0
"self.sel_config = wx.FilePickerCtrl(self, path="""",style=wx.FLP_USE_TEXTCTRL,message=""Choose the config.yaml file"", wildcard=""config.yaml"")",0
#         design the panel,0
Add image of DLC,0
if editing this text make sure you add the '\n' to get the new line. The sizer is unable to format lines correctly.,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = cv2.imread(img),0
convert the image to RGB as you are showing the image with matplotlib,0
divider = make_axes_locatable(self.axes),0
"colorIndex = np.linspace(np.min(im),np.max(im),len(bodyparts))",0
"cax = divider.append_axes(""right"", size=""5%"", pad=0.05)",0
"cbar = self.figure.colorbar(ax, cax=cax,spacing='proportional', ticks=colorIndex)",0
cbar.set_ticklabels(bodyparts[::-1]),0
self.figure.canvas.draw(),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"MainFrame.saveDataSet(self, event)",0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"if self.uniquebodyparts == """":",0
self.uniquebodyparts = self.multibodyparts,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Create an empty data frame,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of old labels,0
Extracting the list of old unique labels,0
Checking if new labels are added or not,0
Checking if user added a new label,0
Found new labels in either multiple bodyparts or unique bodyparts,0
check if single is slected when radio buttons are changed,0
Checking if user added a new label,0
if self.new_Multibodyparts==[] and self.new_UniqueBodyparts==[]: # i.e. no new labels,0
"self.figure,self.axes,self.canvas,self.toolbar,self.image_axis = self.image_panel.drawplot(self.img,img_name,self.iter,self.index,self.multibodyparts,self.colormap,keep_view=self.view_locked)",0
"self.axes.callbacks.connect('xlim_changed', self.onZoom)",0
"self.axes.callbacks.connect('ylim_changed', self.onZoom)",0
"self.image_panel.addcolorbar(self.img,self.image_axis,self.iter,self.multibodyparts,self.colormap)",0
# Only show the bodyparts corresponding to single,0
if self.individual_names[0] == 'single':,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.uniquebodyparts,self.individual_names,self.file,self.markerSize)",0
else:,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.multibodyparts,self.individual_names,self.file,self.markerSize)",0
,0
"self.individualrdb.Bind(wx.EVT_RADIOBOX,self.select_individual)",0
,0
"# Get the color index, depending on if single is selcted or not.",0
"if self.rdb.GetStringSelection==""single"":",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.uniquebodyparts)",0
else:,0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
"self.cidClick = self.canvas.mpl_connect('button_press_event', self.onClick)",0
else:,0
# Found new labels in either multiple bodyparts or unique bodyparts,0
"dlg = wx.MessageDialog(None,""New label found in the config file. Do you want to see all the other labels?"", ""New label found"",wx.YES_NO | wx.ICON_WARNING)",0
result = dlg.ShowModal(),0
if result == wx.ID_NO:,0
if self.new_Multibodyparts!=[]:,0
self.multibodyparts = self.new_Multibodyparts,0
if self.new_UniqueBodyparts!=[]:,0
self.uniquebodyparts = self.new_UniqueBodyparts,0
,0
"self.dataFrame = MainFrame.create_dataframe(self,self.dataFrame,self.relativeimagenames,self.individual_names,self.new_UniqueBodyparts,self.new_Multibodyparts)",0
"self.figure,self.axes,self.canvas,self.toolbar,self.image_axis = self.image_panel.drawplot(self.img,img_name,self.iter,self.index,self.multibodyparts,self.colormap,keep_view=self.view_locked)",0
"self.axes.callbacks.connect('xlim_changed', self.onZoom)",0
"self.axes.callbacks.connect('ylim_changed', self.onZoom)",0
"self.image_panel.addcolorbar(self.img,self.image_axis,self.iter,self.multibodyparts,self.colormap)",0
,0
if self.individual_names[0] == 'single':,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.uniquebodyparts,self.individual_names,self.file,self.markerSize)",0
else:,0
"self.choiceBox,self.individualrdb,self.rdb,self.change_marker_size,self.checkBox = self.choice_panel.addRadioButtons(self.multibodyparts,self.individual_names,self.file,self.markerSize)",0
"self.individualrdb.Bind(wx.EVT_RADIOBOX,self.select_individual)",0
"if self.rdb.GetStringSelection==""single"":",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.uniquebodyparts)",0
else:,0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
"self.cidClick = self.canvas.mpl_connect('button_press_event', self.onClick)",0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
"def plot(self,img):",0
self.drs= [],0
self.updatedCoords = [],0
for ind in self.individual_names:,0
if self.are_unique_bodyparts_present == False: #no unique bodyparts present,0
#            for ind in self.individual_names:,0
image_points = [],0
"for c, bp in enumerate(self.multibodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
"self.points = [self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter]]",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
color = self.colormap(self.norm(self.colorIndex[c])),0
"circle = patches.Circle((self.points[0], self.points[1]), radius=self.markerSize, fc = color, alpha=self.alpha)",0
#                    print(circle),0
print(self.axes),0
self.axes.add_patch(circle),0
"self.dr = auxfun_drag_label_multiple_individuals.DraggablePoint(circle,ind,self.multibodyparts[c])",0
self.dr.connect(),0
self.dr.coords = image_points,0
self.drs.append(self.dr),0
self.updatedCoords.append(self.dr.coords),0
if np.isnan(self.points)[0] == False:,0
self.buttonCounter[ind].append(self.multibodyparts[c]),0
else:,0
if ind == 'single':,0
"for c, bp in enumerate(self.uniquebodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
"self.points = [self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter]]",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.uniquebodyparts)",0
color = self.colormap(self.norm(self.colorIndex[c])),0
"circle = patches.Circle((self.points[0], self.points[1]), radius=self.markerSize, fc = color, alpha=self.alpha)",0
#                    print(circle),0
print(self.axes),0
self.axes.add_patch(circle),0
"self.dr = auxfun_drag_label_multiple_individuals.DraggablePoint(circle,ind,self.uniquebodyparts[c])",0
self.dr.connect(),0
self.dr.coords = image_points,0
self.drs.append(self.dr),0
self.updatedCoords.append(self.dr.coords),0
if np.isnan(self.points)[0] == False:,0
self.buttonCounter[ind].append(self.uniquebodyparts[c]),0
else:,0
"for c, bp in enumerate(self.multibodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
"self.points = [self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter]]",0
"self.norm,self.colorIndex = self.image_panel.getColorIndices(self.img,self.multibodyparts)",0
color = self.colormap(self.norm(self.colorIndex[c])),0
"circle = patches.Circle((self.points[0], self.points[1]), radius=self.markerSize, fc = color, alpha=self.alpha)",0
#                    print(circle),0
print(self.axes),0
self.axes.add_patch(circle),0
"self.dr = auxfun_drag_label_multiple_individuals.DraggablePoint(circle,ind,self.multibodyparts[c])",0
self.dr.connect(),0
self.dr.coords = image_points,0
self.drs.append(self.dr),0
self.updatedCoords.append(self.dr.coords),0
if np.isnan(self.points)[0] == False:,0
self.buttonCounter[ind].append(self.multibodyparts[c]),0
,0
MainFrame.saveEachImage(self),0
self.figure.canvas.draw(),0
return(self.buttonCounter),0
print(self.dataFrame),0
Windows compatible,0
Checks if zoom/pan button is ON,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
convert the image to RGB as you are showing the image with matplotlib,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Checks if zoom/pan button is ON,0
matplotlib.use('Agg'),0
Load updated lists:,0
cfg['video_sets'][vn],0
"labeling_toolbox.show(config,Screens,scale_w,scale_h, winHack, img_scale)",0
plotting parameters:,0
folders = [Path(config).parent / 'labeled-data' /Path(i) for i in video_names],0
plt.savefig(str(Path(tmpfolder)/imagename.split(os.sep)[-1])),0
Let's check if the code is *not* run on windows (Source: #https://stackoverflow.com/questions/1325581/how-do-i-check-if-im-running-on-windows-in-python),0
but the paths are in windows format...,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(index,name.split(os.sep)[1])",0
"print(name,test_video_name)",0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
"trainIndexes, testIndexes = SplitTrials(range(len(Data.index)), trainFraction)",0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
load image to get dimensions:,0
"print ""Grayscale!""",0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
sort the models directories,0
get the shuffle index,0
read cfg file,0
create log file,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
"text = str(self.individual_names+','+self.bodyParts)",0
Check for variable correctness,0
"plt.close(""all"")",0
Moviepy:,0
Call the GUI to select the cropping parameters,0
Update the config.yaml file with current cropping parameters,0
store full frame from random location (good for augmentation),0
"clip=clip.crop(y1 = int(coords[2]),y2 = int(coords[3]),x1 = int(coords[0]), x2 = int(coords[1]))",0
close video.,0
"def __init__(self, point,bodyParts):",0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
text = str(self.bodyParts),0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
Update number of output and batchsize,0
update batchsize (based on parameters in config.yaml),0
"(state,detectiontreshold,margin)=dynamic",0
Name for scorer:,0
"sess, inputs, outputs = predict.setup_pose_prediction(dlc_cfg)",0
#################################################,0
Datafolder,0
#################################################,0
looping over videos,0
"pose = predict.getpose(frame, dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames,dlc_cfg, sess, inputs, outputs)",0
"pose = predict.getposeNP(frames, dlc_cfg, sess, inputs, outputs) #process the whole batch (some frames might be from previous batch!)",0
TODO: perform detection on resized image (For speed),1
"print(counter,x1,x2,y1,y2,detected)",0
"print(""looking again, lost!"")",0
"GetPoseF_GTF(cfg,dlc_cfg, sess, inputs, outputs,cap,nframes,int(dlc_cfg[""batch_size""]))",0
"""gpu_info"": device_lib.list_local_devices()",0
from skimage.io import imread,0
"frame=imread(os.path.join(directory,framename),mode='RGB')",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/AlexEMG/DeepLabCut/pull/324",0
Parameters for augmentation with regard to cropping,0
"Added and described in ""Using DeepLabCut for 3D markerless pose estimation across species and behaviors""",0
Source: https://www.nature.com/articles/s41596-019-0176-0,0
"limit width  [-leftwidth*u-100,100+u*rightwidth] x [-bottomwith*u-100,100+u*topwidth] where u is always a (different) random number in unit interval",0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
Dependencies for anaysis,0
import skimage.color,0
from skimage.io import imread,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
########################## RESCALING (to global scale),0
#################################################,0
Compute predictions over images,0
#################################################,0
Name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
image = skimage.color.gray2rgb(image),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
from scipy.misc import imresize,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
"loading backbone from ResNet, MobileNet etc.",0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
Read file path for pose_config file. >> pass it on,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
print(end_points.keys()) >> to see what else is available.,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
pylint: disable=g-backslash-continuation,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
"Only insert rate to params if rate > 1 and kernel size is not [1, 1].",0
We will apply atrous rate in the following cases:,0
"1) When kernel_size is not in params, the operation then uses",0
default kernel size 3x3.,0
"2) When kernel_size is in params, and if the kernel_size is not",0
"equal to (1, 1) (there is no need to apply atrous convolution to",0
any 1x1 convolution).,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
## Code for TF inference on GPU,0
Restore variables from disk.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
from nets.mobilenet import conv_blocks as ops,0
from nets.mobilenet import mobilenet as lib,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
The next part of the code depends upon which tensorflow version you have.,0
if cfg.location_refinement:,0
assuming batchsize 1 here!,0
"probs = tf.squeeze(probs, axis=0)",0
locref = locref*cfg.locref_stdev,0
turn into x times y time bs * bpts,0
print(locref.get_shape().as_list()),0
print(probs.get_shape().as_list()),0
extract corresponding locref x and y as well as probability,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
from scipy.misc import imresize,0
Augmentation functions,0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
Initializing variables if they don't exist...,0
what is the fraction of training samples with scaling augmentation?,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
make sure joint ids are 0-indexed,0
Load Matlab file dataset annotation,0
mlab = sio.loadmat(file_name),0
"mlab = sio.loadmat(os.path.join(self.cfg.project_path,file_name))",0
Pickle the 'data' dictionary using the highest protocol available.,0
mlab = mlab['dataset'],0
Scale is sampled only once to transform all of the images into same size.,0
Approximating the scale,0
"If you would like to check the augmented images, script for saving",0
the images with joints on:,0
for i in range(self.batch_size):,0
joints = batch_joints[i],0
"kps = KeypointsOnImage([Keypoint(x=joint[0], y=joint[1]) for joint in joints], shape=batch_images[i].shape)",0
im = kps.draw_on_image(batch_images[i]),0
"imageio.imwrite('some_location/augmented/'+str(i)+'.png', im)",0
dist_thresh = float(self.cfg.pos_dist_thresh * scale),0
Grid of coordinates,0
Distance between the joint point and each coordinate,0
"from scipy.misc import imread, imresize",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
"from scipy.misc import imread, imresize",0
"from dataset.pose_dataset import Batch, data_to_input, mirror_joints_map, CropImage, DataItem",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
utility functions,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
Process skeleton,0
save,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
"newvideo=deeplabcut.ShortenVideo(video,start='00:00:00',stop='00:00:20',outsuffix='short')",0
"print(""CREATE VIDEO"")",0
"Checked: dynamic=(True,1.1,0)) >> only keeps large limits (full frame >> never detects)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
For testing a color video:,0
videoname='baby4hin2min',0
"video=[os.path.join('/home/alex/Desktop/Data',videoname+'.mp4')]",0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.evaluate_network(path_config_file,plotting=True,trainingsetindex=33)",0
Make super short video (so the analysis is quick!),0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"subprocess.call(['ffmpeg','-i',video[0],'-ss','00:00:00','-to','00:00:00.4','-c','copy',newvideo])",0
"deeplabcut.create_labeled_video(path_config_file,[newvideo], destfolder=dfolder,filtered=True)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
#create one split and make Shuffle 2 and 3 have the same split.,0
##Note that the new function in DLC 2.1 does that much easier...,0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Dowloading the calibration images,0
Deleting unneccesary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"deeplabcut.train_network(configfile,shuffle=1) #>> fails one body part too much!",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Importing the toolbox (takes several seconds),0
Loading example data set,0
deeplabcut.load_demo_data(path_config_file),0
shuffle=11 #>> imageio functions!,0
"print(""Analyze Video"")",0
"videofile_path = os.path.join(os.getcwd(),'openfield-Pranav-2018-10-30','videos','m3v1mp4.mp4')",0
"deeplabcut.analyze_videos(path_config_file,[videofile_path], shuffle=shuffle)",0
"print(""Create Labeled Video"")",0
"deeplabcut.create_labeled_video(path_config_file,[videofile_path],save_frames=False, shuffle=shuffle)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://stackoverflow.com/questions/39590187/in-requirements-txt-what-does-tilde-equals-mean,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
Supress tensorflow warning messages,0
DLClight version does not support GUIs. Importing accordingly,0
"Train, evaluate & predict functions / require TF",0
docs[0]['init_weights'] = '../../pretrained/resnet_v1_101.ckpt',0
dict_test['init_weights'] = 'models/mpii/snapshot-1030000',0
Create the model directory,0
Download the weights and put then in appropriate directory,0
create the pose_config.yaml files,0
Analyze the videos,0
Read the config file,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
get vcap property,0
Create project and sub-directories,0
"Add all videos in the folder. Multiple folders can be passed in a list, similar to the video files. Folders and video files can also be passed!",0
Check if it is a folder,0
"Import all videos in a folder or if just one video withouth [] passed, then make it a list.",0
"if isinstance(videos,str):",0
#there are two cases:,0
if os.path.isdir(videos): # it is a path!,0
path=videos,0
"videos=[os.path.join(path,vp) for vp in os.listdir(path) if videotype in vp]",0
if len(videos)==0:,0
"print(""No videos found in"",path,os.listdir(path))",0
"print(""Perhaps change the videotype, which is currently set to:"", videotype)",0
else:,0
"print(""Directory entered, "" , len(videos),"" videos were found."")",0
else:,0
if os.path.isfile(videos):,0
videos=[videos],0
https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
try:,0
"#shutil.copy(src,dst)",0
except OSError or TypeError: #https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
"shutil.copy(os.fspath(src),os.fspath(dst))",0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Set values to config file:,0
Write dictionary to yaml  config file,0
import yaml,0
import cv2,0
import shutil,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
Flatten the list of bodyparts to connect,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
Start plotting for every frame,0
"Once all the frames are saved, then make a movie using ffmpeg.",0
Create the fig and define the axes,0
Clear plot and initialize the variables,0
Initialize arrays for appending the 3d data for actual plotting the points,0
Initialize arrays for appending the 3d data for drawing the lines,0
Initialize arrays for appending the 2d data from cam1 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam2 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam1 for drawing the lines,0
Initialize arrays for appending the 2d data from cam2 for drawing the lines,0
Get the scorer names from the dataframe,0
"Set the x,y, and z limits for the 3d view",0
"Set the frame number to read#max(0,index-trailpoints):index",0
Plot the labels for each body part,0
Connecting the bodyparts specified in the config file.3d file is created based on the likelihoods of cam1 and cam2. Using 3d file and check if the body part is nan then dont plot skeleton,0
Saving the frames,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
Extract the indices of frames where the likelihood of a bodypart for both cameras are less than pvalue,0
Extract frames where likelihood for both the views is less than the pcutoff,0
"low_likelihood_frames = np.all(likelihoods < pcutoff, axis=1)",0
For cam1 camera: Assign nans to x and y values of a bodypart where the likelihood for is less than pvalue,0
For cam2 camera: Assign nans to x and y values of a bodypart where the likelihood is less than pvalue,0
ToDo: speed up func. below by saving in numpy.array,1
"Create an empty dataframe to store x,y,z of 3d data",0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Undistorting the points from cam1 camera,0
Undistorting the points from cam2 camera,0
Save the undistorted files,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = io.imread(img),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
self.cropping = False,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variable initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
Overwrite machine label file,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.slider.Enable(False),0
self.checkBox.Enable(False),0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
Checks if zoom/pan button is ON,0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
extract min and max index based on start stop interval.,0
figure out body part list:,0
all indices between start and stop with jump larger than epsilon (leading up to this point!),0
deviation_dataname = str(Path(videofolder)/Path(dataname)),0
Calculate deviatons for video,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
data.to_csv(dataname.split('.h5')[0]+'filtered.csv'),0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
"Dataframe = pd.read_hdf(os.path.join(videofolder,dataname+'.h5'))",0
drop duplicate labels:,0
updates iteration by 1,0
###################################################,0
Dependencies,0
###################################################,0
import sys,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as specified in the config file,0
"rr, cc,val = line_aa(int(df_y[pair[0],index]),int(df_x[pair[0],index]),int(df_y[pair[1],index]), int(df_x[pair[1],index]))",0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Adds skeleton to the video,0
###################,0
less transparent present.,0
# One can change the parameters of the video creation script below:,0
See ffmpeg user guide: http://ffmpeg.org/ffmpeg.html#Video-and-Audio-file-format-conversion,0
,0
videofolder = str(Path(video).parents[0]),0
print(metadata),0
Loading cropping data used during analysis,0
!/usr/bin/env python3,0
TODO check if those times exist...,1
"windows throws error if file path is > 260 characters, can fix with prefix. see https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation",0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
###################################################,0
Dependencies,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
"fig.colorbar(im, ax=ax)",0
#################################################,0
Looping analysis over video,0
#################################################,0
"nlines,numcolumns=data.shape",0
assert(len(orderofbpincsv)==len(cfg['bodyparts'])),0
TODO: test len of images vs. len of imagenames for another sanity check,1
Is there a scorer for this?,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
"with open(projconfigfile, 'w') as cf:",0
"ruamelFile_3d.dump(cfg_file_3d, cf)",0
Read the pickle file,0
Write the pickle file,0
Creates an empty dataFrame of same shape as df_side_view.,0
"flag = number of coordinates. e.g. 4 for 3d,3 for 2d as we need to store the likelihood too.",0
df = pd.read_hdf(dataframe),0
checks if input is a directory,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"dlc_cfg = read_config(os.path.join(modelfolder,'pose_cfg.yaml'))",0
"dlc_cfg['init_weights'] = os.path.join(modelfolder , 'train', Snapshots[snapshotindex])",0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = cv2.imread(img),0
convert the image to RGB as you are showing the image with matplotlib,0
divider = make_axes_locatable(self.axes),0
"colorIndex = np.linspace(np.min(im),np.max(im),len(bodyparts))",0
"cax = divider.append_axes(""right"", size=""5%"", pad=0.05)",0
"cbar = self.figure.colorbar(ax, cax=cax,spacing='proportional', ticks=colorIndex)",0
cbar.set_ticklabels(bodyparts[::-1]),0
self.figure.canvas.draw(),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"MainFrame.saveDataSet(self, event)",0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Create an empty data frame,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of old labels,0
Extracting the list of old unique labels,0
Checking if new labels are added or not,0
Checking if user added a new label,0
Found new labels in either multiple bodyparts or unique bodyparts,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Checks if zoom/pan button is ON,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
convert the image to RGB as you are showing the image with matplotlib,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Checks if zoom/pan button is ON,0
matplotlib.use('Agg'),0
Load updated lists:,0
cfg['video_sets'][vn],0
"labeling_toolbox.show(config,Screens,scale_w,scale_h, winHack, img_scale)",0
plotting parameters:,0
folders = [Path(config).parent / 'labeled-data' /Path(i) for i in video_names],0
plt.savefig(str(Path(tmpfolder)/imagename.split(os.sep)[-1])),0
Let's check if the code is *not* run on windows (Source: #https://stackoverflow.com/questions/1325581/how-do-i-check-if-im-running-on-windows-in-python),0
but the paths are in windows format...,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(index,name.split(os.sep)[1])",0
"print(name,test_video_name)",0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
"trainIndexes, testIndexes = SplitTrials(range(len(Data.index)), trainFraction)",0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
load image to get dimensions:,0
"print ""Grayscale!""",0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
"text = str(self.individual_names+','+self.bodyParts)",0
Check for variable correctness,0
"plt.close(""all"")",0
Moviepy:,0
Call the GUI to select the cropping parameters,0
Update the config.yaml file with current cropping parameters,0
store full frame from random location (good for augmentation),0
"clip=clip.crop(y1 = int(coords[2]),y2 = int(coords[3]),x1 = int(coords[0]), x2 = int(coords[1]))",0
close video.,0
"def __init__(self, point,bodyParts):",0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
text = str(self.bodyParts),0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
update number of outputs,0
Name for scorer:,0
#################################################,0
Datafolder,0
#################################################,0
looping over videos,0
reset cap to frame 0!,0
Attempt to load data...,0
"frame=io.imread(os.path.join(directory,framename),mode='RGB')",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
Attempt to load data...,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/AlexEMG/DeepLabCut/pull/324",0
Parameters for augmentation with regard to cropping,0
"Added and described in ""Using DeepLabCut for 3D markerless pose estimation across species and behaviors""",0
Source: https://www.nature.com/articles/s41596-019-0176-0,0
"limit width  [-leftwidth*u-100,100+u*rightwidth] x [-bottomwith*u-100,100+u*topwidth] where u is always a (different) random number in unit interval",0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
Dependencies for anaysis,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
The next part of the code depends upon which tensorflow version you have.,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Augmentation functions,0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
print(item.joints),0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
Initializing variables if they don't exist...,0
what is the fraction of training samples with scaling augmentation?,0
Randomly rotates an image with respect to the image center within the,0
range [-rotate_max_deg_abs; rotate_max_deg_abs] to augment training data,0
"Randomly adds brightness within the range [-brightness_dif, brightness_dif]",0
to augment training data,0
Randomly applies x = (x - mean) * contrast_factor + mean`` to each,0
"color channel within the range [contrast_factor_lo, contrast_factor_up]",0
to augment training data,0
"Randomly adjusts saturation within range 1 + [-saturation_max_dif, saturation_max_dif]",0
to augment training data,0
"Randomly applies gaussian noise N(0, noise_sigma^2) to an image",0
to augment training data,0
Randomly applies gaussian blur to an image with a random window size,0
"within the range [0, 2 * blur_max_window_size + 1] to augment training data",0
"Whether image is RGB  or RBG. If None, contrast augmentation uses the mean per-channel.",0
"Clips image to [0, 255] even when data type is not uint8",0
Number of processes to use per core during training,0
Number of datapoints to prefetch at a time during training,0
"don't loop over entire heatmap, but just relevant locations",0
print(la.norm(diff)),0
"logging.debug('image %s', im_file)",0
print('image: {}'.format(im_file)),0
"logging.debug('mirror %r', mirror)",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
"from dataset.pose_dataset import Batch, data_to_input, mirror_joints_map, CropImage, DataItem",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
"print(""Filtering with ARIMA model %s"",video)",0
UTILS FUNCTIONS,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
See if file was already proccessed,0
Process skeleton,0
save,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
for windows:,0
"deeplabcut.create_labeled_video(path_config_file,[newvideo], destfolder=dfolder,filtered=True)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Loading example data set,0
create one split and make Shuffle 5 and 6 have the same split.,0
otherwise default...,0
DLC_config=deeplabcut.auxiliaryfunctions.read_plainconfig(posefile),0
DLC_config['save_iters']=10,0
DLC_config['display_iters']=2,0
"DLC_config['multi_step']=[[0.005,15001]]",0
"deeplabcut.auxiliaryfunctions.write_plainconfig(posefile,DLC_config)",0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Dowloading the calibration images,0
Deleting unneccesary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Importing the toolbox (takes several seconds),0
Loading example data set,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://stackoverflow.com/questions/39590187/in-requirements-txt-what-does-tilde-equals-mean,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
Supress tensorflow warning messages,0
DLClight version does not support GUIs. Importing accordingly,0
"Train, evaluate & predict functions / require TF",0
docs[0]['init_weights'] = '../../pretrained/resnet_v1_101.ckpt',0
dict_test['init_weights'] = 'models/mpii/snapshot-1030000',0
Create the model directory,0
Download the weights and put then in appropriate directory,0
create the pose_config.yaml files,0
Analyze the videos,0
Read the config file,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
get vcap property,0
Create project and sub-directories,0
"Import all videos in a folder or if just one video withouth [] passed, then make it a list.",0
there are two cases:,0
https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
try:,0
"#shutil.copy(src,dst)",0
except OSError or TypeError: #https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
"shutil.copy(os.fspath(src),os.fspath(dst))",0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Set values to config file:,0
Write dictionary to yaml  config file,0
import yaml,0
import cv2,0
import shutil,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
Flatten the list of bodyparts to connect,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
Start plotting for every frame,0
"Once all the frames are saved, then make a movie using ffmpeg.",0
Create the fig and define the axes,0
Clear plot and initialize the variables,0
Initialize arrays for appending the 3d data for actual plotting the points,0
Initialize arrays for appending the 3d data for drawing the lines,0
Initialize arrays for appending the 2d data from cam1 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam2 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam1 for drawing the lines,0
Initialize arrays for appending the 2d data from cam2 for drawing the lines,0
Get the scorer names from the dataframe,0
"Set the x,y, and z limits for the 3d view",0
"Set the frame number to read#max(0,index-trailpoints):index",0
Plot the labels for each body part,0
Connecting the bodyparts specified in the config file.3d file is created based on the likelihoods of cam1 and cam2. Using 3d file and check if the body part is nan then dont plot skeleton,0
Saving the frames,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
flag to check if the video_path variable is a string or a list of list,0
Check for the camera matrix,0
Check for scorer names in the pickle file of 3d output,0
Analyze video if score name is different,0
if len(dataname)>0:,0
undistort points for this pair,0
"raise Exception(""The number of frames do not match in the two videos. Please make sure that your videos have same number of frames and then retry!"")",0
Extract the indices of frames where the likelihood of a bodypart for both cameras are less than pvalue,0
Extract frames where likelihood for both the views is less than the pcutoff,0
"low_likelihood_frames = np.all(likelihoods < pcutoff, axis=1)",0
For cam1 camera: Assign nans to x and y values of a bodypart where the likelihood for is less than pvalue,0
For cam2 camera: Assign nans to x and y values of a bodypart where the likelihood is less than pvalue,0
ToDo: speed up func. below by saving in numpy.array,1
"Create an empty dataframe to store x,y,z of 3d data",0
have to make the dest folder none so that it can be updated for a new pair of videos,0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Undistorting the points from cam1 camera,0
Undistorting the points from cam2 camera,0
Save the undistorted files,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = io.imread(img),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
self.cropping = False,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
"SP.ScrolledPanel.__init__(self, parent, -1,style=wx.SUNKEN_BORDER, **kwargs)",0
self.parent = parent,0
"self.SetupScrolling(scroll_x=True, scrollToTop=False)",0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
widgetsizer.AddStretchSpacer(15),0
"self.adjustLabelCheck = wx.CheckBox(self.widget_panel,id=wx.ID_ANY,  label = 'Adjust original labels?')",0
"widgetsizer.Add(self.adjustLabelCheck , 1, wx.ALL, 15)",0
"self.adjustLabelCheck.Bind(wx.EVT_CHECKBOX,self.adjustLabel)",0
##############################################################################################################################,0
Variable initialization,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
self.axes.clear(),0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.slider.Enable(False),0
self.checkBox.Enable(False),0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
print(self.points),0
Checks if zoom/pan button is ON,0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
extract min and max index based on start stop interval.,0
figure out body part list:,0
all indices between start and stop with jump larger than epsilon (leading up to this point!),0
deviation_dataname = str(Path(videofolder)/Path(dataname)),0
Calculate deviatons for video,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
data.to_csv(dataname.split('.h5')[0]+'filtered.csv'),0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
"Dataframe = pd.read_hdf(os.path.join(videofolder,dataname+'.h5'))",0
drop duplicate labels:,0
updates iteration by 1,0
###################################################,0
Dependencies,0
###################################################,0
import sys,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as specified in the config file,0
"rr, cc,val = line_aa(int(df_y[pair[0],index]),int(df_x[pair[0],index]),int(df_y[pair[1],index]), int(df_x[pair[1],index]))",0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Adds skeleton to the video,0
###################,0
less transparent present.,0
# One can change the parameters of the video creation script below:,0
See ffmpeg user guide: http://ffmpeg.org/ffmpeg.html#Video-and-Audio-file-format-conversion,0
,0
videofolder = str(Path(video).parents[0]),0
print(metadata),0
Loading cropping data used during analysis,0
"windows throws error if file path is > 260 characters, can fix with prefix. see https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation",0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
###################################################,0
Dependencies,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
"fig.colorbar(im, ax=ax)",0
#################################################,0
Looping analysis over video,0
#################################################,0
"nlines,numcolumns=data.shape",0
assert(len(orderofbpincsv)==len(cfg['bodyparts'])),0
TODO: test len of images vs. len of imagenames for another sanity check,1
Is there a scorer for this?,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
"with open(projconfigfile, 'w') as cf:",0
"ruamelFile_3d.dump(cfg_file_3d, cf)",0
Read the pickle file,0
Write the pickle file,0
Creates an empty dataFrame of same shape as df_side_view.,0
"flag = number of coordinates. e.g. 4 for 3d,3 for 2d as we need to store the likelihood too.",0
df = pd.read_hdf(dataframe),0
checks if input is a directory,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"dlc_cfg = read_config(os.path.join(modelfolder,'pose_cfg.yaml'))",0
"dlc_cfg['init_weights'] = os.path.join(modelfolder , 'train', Snapshots[snapshotindex])",0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
convert the image to RGB as you are showing the image with matplotlib,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
MainFrame.saveEachImage(self),0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
MainFrame.saveEachImage(self),0
"def getLabels(self,ind,img_index):",0
"""""""",0
Returns a list of x and y labels of the corresponding image index,0
"""""""",0
self.previous_image_points = [],0
"for bpindex, bp in enumerate(self.bodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
self.previous_image_points.append(image_points),0
return(self.previous_image_points),0
"self.dr.coords = MainFrame.getLabels(self,ind,self.iter)[bpindex]",0
Windows compatible,0
Checks if zoom/pan button is ON,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
convert the image to RGB as you are showing the image with matplotlib,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Checks if zoom/pan button is ON,0
matplotlib.use('Agg'),0
Load updated lists:,0
cfg['video_sets'][vn],0
"labeling_toolbox.show(config,Screens,scale_w,scale_h, winHack, img_scale)",0
plotting parameters:,0
folders = [Path(config).parent / 'labeled-data' /Path(i) for i in video_names],0
plt.savefig(str(Path(tmpfolder)/imagename.split(os.sep)[-1])),0
Let's check if the code is *not* run on windows (Source: #https://stackoverflow.com/questions/1325581/how-do-i-check-if-im-running-on-windows-in-python),0
but the paths are in windows format...,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(index,name.split(os.sep)[1])",0
"print(name,test_video_name)",0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
"trainIndexes, testIndexes = SplitTrials(range(len(Data.index)), trainFraction)",0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
load image to get dimensions:,0
"print ""Grayscale!""",0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
"text = str(self.individual_names+','+self.bodyParts)",0
Check for variable correctness,0
"plt.close(""all"")",0
Moviepy:,0
Call the GUI to select the cropping parameters,0
Update the config.yaml file with current cropping parameters,0
store full frame from random location (good for augmentation),0
"clip=clip.crop(y1 = int(coords[2]),y2 = int(coords[3]),x1 = int(coords[0]), x2 = int(coords[1]))",0
close video.,0
"def __init__(self, point,bodyParts):",0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
text = str(self.bodyParts),0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
update number of outputs,0
Name for scorer:,0
#################################################,0
Datafolder,0
#################################################,0
looping over videos,0
Attempt to load data...,0
"frame=io.imread(os.path.join(directory,framename),mode='RGB')",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
update number of outputs and adjust pandas indices,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
Attempt to load data...,0
"types of datasets, see factory: deeplabcut/pose_estimation_tensorflow/dataset/factory.py",0
"you can also set this to deterministic, see https://github.com/AlexEMG/DeepLabCut/pull/324",0
Parameters for augmentation with regard to cropping,0
"Added and described in ""Using DeepLabCut for 3D markerless pose estimation across species and behaviors""",0
Source: https://www.nature.com/articles/s41596-019-0176-0,0
"limit width  [-leftwidth*u-100,100+u*rightwidth] x [-bottomwith*u-100,100+u*topwidth] where u is always a (different) random number in unit interval",0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
Dependencies for anaysis,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
The next part of the code depends upon which tensorflow version you have.,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Augmentation functions,0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
"from dataset.pose_dataset import Batch, data_to_input, mirror_joints_map, CropImage, DataItem",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
"print(""Filtering with ARIMA model %s"",video)",0
UTILS FUNCTIONS,0
Check dataformats,0
Calculate distance,0
if not 0 <= ang <+ 360: raise ValueError('Ang was not computed correctly'),0
Check data format,0
Calculate,0
Process single bone,0
get bone length and orientation,0
keep the smallest of the two likelihoods,0
Create dataframe and return,0
df.index.name=name,0
MAIN FUNC,0
"Load config file, scorer and videos",0
See if file was already proccessed,0
Process skeleton,0
save,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
for windows:,0
"deeplabcut.create_labeled_video(path_config_file,[newvideo], destfolder=dfolder,filtered=True)",0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Dowloading the calibration images,0
Deleting unneccesary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Importing the toolbox (takes several seconds),0
Loading example data set,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://stackoverflow.com/questions/39590187/in-requirements-txt-what-does-tilde-equals-mean,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
Supress tensorflow warning messages,0
DLClight version does not support GUIs. Importing accordingly,0
"Train, evaluate & predict functions / require TF",0
Read the config file,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
get vcap property,0
Create project and sub-directories,0
"Import all videos in a folder or if just one video withouth [] passed, then make it a list.",0
there are two cases:,0
https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
try:,0
"#shutil.copy(src,dst)",0
except OSError or TypeError: #https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
"shutil.copy(os.fspath(src),os.fspath(dst))",0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Set values to config file:,0
Write dictionary to yaml  config file,0
import yaml,0
import cv2,0
import shutil,0
Create project and sub-directories,0
Create config file,0
"cfg_file_3d['config_files']= [str('Enter the path of the config file ')+str(i)+ ' to include' for i in range(1,3)]",0
cfg_file_3d['config_files']= ['Enter the path of the config file 1'],0
Read the config file and related variables,0
Flatten the list of bodyparts to connect,0
"triangulated file is a list which is always sorted as [triangulated.h5,camera-1.videotype,camera-2.videotype]",0
Read the video files and corresponfing h5 files,0
Look for the filtered predictions file,0
Start plotting for every frame,0
"Once all the frames are saved, then make a movie using ffmpeg.",0
Create the fig and define the axes,0
Clear plot and initialize the variables,0
Initialize arrays for appending the 3d data for actual plotting the points,0
Initialize arrays for appending the 3d data for drawing the lines,0
Initialize arrays for appending the 2d data from cam1 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam2 for actual plotting the points,0
Initialize arrays for appending the 2d data from cam1 for drawing the lines,0
Initialize arrays for appending the 2d data from cam2 for drawing the lines,0
Get the scorer names from the dataframe,0
"Set the x,y, and z limits for the 3d view",0
"Set the frame number to read#max(0,index-trailpoints):index",0
Plot the labels for each body part,0
Connecting the bodyparts specified in the config file.3d file is created based on the likelihoods of cam1 and cam2. Using 3d file and check if the body part is nan then dont plot skeleton,0
Saving the frames,0
Termination criteria,0
"Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)",0
Read the config file,0
update the variable snapshot* in config file according to the name of the cameras,0
Initialize the dictionary,0
Sort the images.,0
Find the chess board corners,0
"If found, add object points, image points (after refining them)",0
Draw the corners and store the images,0
Perform calibration for each cameras and store the matrices as a pickle file,0
Calibrating each camera,0
Save the camera calibration result for later use (we won't use rvecs / tvecs),0
Compute mean re-projection errors for individual cameras,0
Compute stereo calibration for each pair of cameras,0
Stereo Rectification,0
Read the config file,0
colormap = plt.get_cmap(cfg_3d['colormap']),0
Sort the images,0
Remapping dataFrame_camera1_undistort,0
Remapping,0
Display images in RGB,0
Plot the undistorted corner points,0
Triangulate,0
Check if the config file exists,0
undistort points for this pair,0
Extract the indices of frames where the likelihood of a bodypart for both cameras are less than pvalue,0
Extract frames where likelihood for both the views is less than the pcutoff,0
"low_likelihood_frames = np.all(likelihoods < pcutoff, axis=1)",0
For cam1 camera: Assign nans to x and y values of a bodypart where the likelihood for is less than pvalue,0
For cam2 camera: Assign nans to x and y values of a bodypart where the likelihood is less than pvalue,0
ToDo: speed up func. below by saving in numpy.array,1
"Create an empty dataframe to store x,y,z of 3d data",0
currently no interm. saving of this due to high speed.,0
check if the undistorted files are already present,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Create an empty dataFrame to store the undistorted 2d coordinates and likelihood,0
Undistorting the points from cam1 camera,0
Undistorting the points from cam2 camera,0
Save the undistorted files,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = io.imread(img),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
self.cropping = False,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
"SP.ScrolledPanel.__init__(self, parent, -1,style=wx.SUNKEN_BORDER, **kwargs)",0
self.parent = parent,0
"self.SetupScrolling(scroll_x=True, scrollToTop=False)",0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
widgetsizer.AddStretchSpacer(15),0
"self.adjustLabelCheck = wx.CheckBox(self.widget_panel,id=wx.ID_ANY,  label = 'Adjust original labels?')",0
"widgetsizer.Add(self.adjustLabelCheck , 1, wx.ALL, 15)",0
"self.adjustLabelCheck.Bind(wx.EVT_CHECKBOX,self.adjustLabel)",0
##############################################################################################################################,0
Variable initialization,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.slider.Enable(False),0
self.checkBox.Enable(False),0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
print(self.points),0
Checks if zoom/pan button is ON,0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
extract min and max index based on start stop interval.,0
figure out body part list:,0
all indices between start and stop with jump larger than epsilon (leading up to this point!),0
deviation_dataname = str(Path(videofolder)/Path(dataname)),0
Calculate deviatons for video,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
data.to_csv(dataname.split('.h5')[0]+'filtered.csv'),0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
"Dataframe = pd.read_hdf(os.path.join(videofolder,dataname+'.h5'))",0
drop duplicate labels:,0
updates iteration by 1,0
###################################################,0
Dependencies,0
###################################################,0
import sys,0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Draw the skeleton for specific bodyparts to be connected as specified in the config file,0
"rr, cc,val = line_aa(int(df_y[pair[0],index]),int(df_x[pair[0],index]),int(df_y[pair[1],index]), int(df_x[pair[1],index]))",0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
recode the bodyparts2connect into indices for df_x and df_y for speed,0
Adds skeleton to the video,0
###################,0
less transparent present.,0
# One can change the parameters of the video creation script below:,0
See ffmpeg user guide: http://ffmpeg.org/ffmpeg.html#Video-and-Audio-file-format-conversion,0
,0
videofolder = str(Path(video).parents[0]),0
print(metadata),0
Loading cropping data used during analysis,0
"windows throws error if file path is > 260 characters, can fix with prefix. see https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation",0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
Creates an empty dataFrame of same shape as df_side_view,0
flag = 2d or 3d,0
Plotting,0
Find videos only specific to the cam names,0
Exclude the labeled video files,0
"print(""here is what I found"",vid)",0
"print([os.path.join(path,pref+cam+suf+ending),putativecam2name])",0
found a pair!!!,0
Checks if filepath is a directory,0
Get the filename of the triangulated file excluing the scorer name and remove any '-' or _ from it,0
Get the suffix and prefix of the video filenames so that they can be used for matching the triangulated file names.,0
Match the suffix and prefix with the triangulated file name and return the list with triangulated file and corresponding video files.,0
###################################################,0
Dependencies,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
"fig.colorbar(im, ax=ax)",0
#################################################,0
Looping analysis over video,0
#################################################,0
"nlines,numcolumns=data.shape",0
assert(len(orderofbpincsv)==len(cfg['bodyparts'])),0
TODO: test len of images vs. len of imagenames for another sanity check,1
Is there a scorer for this?,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Project definitions (do not edit),0
Project path (change when moving around),0
Plotting configuration,0
"Number of cameras, camera names, path of the config files, shuffle index and trainingsetindex used to analyze videos:",0
Adding default value for variable skeleton and skeleton_color for backward compatibility.,0
"with open(projconfigfile, 'w') as cf:",0
"ruamelFile_3d.dump(cfg_file_3d, cf)",0
Read the pickle file,0
Write the pickle file,0
Creates an empty dataFrame of same shape as df_side_view.,0
"flag = number of coordinates. e.g. 4 for 3d,3 for 2d as we need to store the likelihood too.",0
df = pd.read_hdf(dataframe),0
checks if input is a directory,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"dlc_cfg = read_config(os.path.join(modelfolder,'pose_cfg.yaml'))",0
"dlc_cfg['init_weights'] = os.path.join(modelfolder , 'train', Snapshots[snapshotindex])",0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
convert the image to RGB as you are showing the image with matplotlib,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
MainFrame.saveEachImage(self),0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
MainFrame.saveEachImage(self),0
"def getLabels(self,ind,img_index):",0
"""""""",0
Returns a list of x and y labels of the corresponding image index,0
"""""""",0
self.previous_image_points = [],0
"for bpindex, bp in enumerate(self.bodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
self.previous_image_points.append(image_points),0
return(self.previous_image_points),0
"self.dr.coords = MainFrame.getLabels(self,ind,self.iter)[bpindex]",0
Windows compatible,0
Checks if zoom/pan button is ON,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
convert the image to RGB as you are showing the image with matplotlib,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
Workaround for MAC - xlim and ylim changed events seem to be triggered too often so need to make sure that the,0
xlim and ylim have actually changed before turning zoom off,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
Save pre-zoom xlim and ylim values,0
See if axis limits have actually changed,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Checks if zoom/pan button is ON,0
matplotlib.use('Agg'),0
Load updated lists:,0
cfg['video_sets'][vn],0
"labeling_toolbox.show(config,Screens,scale_w,scale_h, winHack, img_scale)",0
plotting parameters:,0
folders = [Path(config).parent / 'labeled-data' /Path(i) for i in video_names],0
Let's check if the code is *not* run on windows (Source: #https://stackoverflow.com/questions/1325581/how-do-i-check-if-im-running-on-windows-in-python),0
but the paths are in windows format...,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(index,name.split(os.sep)[1])",0
"print(name,test_video_name)",0
Loading metadata from config file:,0
Create path for training sets & store data there,0
loading & linking pretrained models,0
"trainIndexes, testIndexes = SplitTrials(range(len(Data.index)), trainFraction)",0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
load image to get dimensions:,0
"print ""Grayscale!""",0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
"text = str(self.individual_names+','+self.bodyParts)",0
Check for variable correctness,0
"plt.close(""all"")",0
Moviepy:,0
Call the GUI to select the cropping parameters,0
Update the config.yaml file with current cropping parameters,0
store full frame from random location (good for augmentation),0
"clip=clip.crop(y1 = int(coords[2]),y2 = int(coords[3]),x1 = int(coords[0]), x2 = int(coords[1]))",0
close video.,0
"def __init__(self, point,bodyParts):",0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
text = str(self.bodyParts),0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
#################################################,0
Datafolder,0
#################################################,0
looping over videos,0
Attempt to load data...,0
"frame=io.imread(os.path.join(directory,framename),mode='RGB')",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
Attempt to load data...,0
Parameters for augmentation with regard to cropping,0
"limit width  [-leftwidth*u-100,100+u*rightwidth] x [-bottomwith*u-100,100+u*topwidth] where u is always a (different) random number in unit interval",0
these will be updated by trainingsetmanipulation.py in the future,0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
Dependencies for anaysis,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
reload logger.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
The next part of the code depends upon which tensorflow version you have.,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"Xstart=int(np.max([0,int(Xlabel)-widthback]))",0
"Xstop=int(np.min([np.shape(im)[1]-1,int(Xlabel)+widthforward]))",0
"Ystart=int(np.max([0,int(Ylabel)-hdown]))",0
"Ystop=int(np.min([np.shape(im)[0]-1,int(Ylabel)+hup]))",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
"print(""Filtering with ARIMA model %s"",video)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Importing the toolbox (takes several seconds),0
Loading example data set,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
for windows:,0
"deeplabcut.create_labeled_video(path_config_file,[newvideo], destfolder=dfolder,filtered=True)",0
"copying demo video from reaching data set and create two ""views"":",0
"copying demo video from reaching data set and create two ""views"":",0
checking if 2d test project is available,0
Creating the name of the project,0
Dowloading the calibration images,0
Deleting unneccesary images; the ones whose corners are not detected and .mat files,0
change the file names for calibration images to match the name of cameras in config.yaml file.i.e. camera-1 and camera-2,0
Sorting images,0
Removing some of the images where the corner was not detected,0
"output_path = [os.path.join(basepath,folder)]",0
"deeplabcut.create_labeled_video_3d(path_config_file,output_path,start=5,end=10)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://stackoverflow.com/questions/39590187/in-requirements-txt-what-does-tilde-equals-mean,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
Supress tensorflow warning messages,0
Direct import for convenience,0
Direct import for convenience,0
from deeplabcut import create_project,0
Direct import for convenience,0
Read the config file,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
get vcap property,0
Create project and sub-directories,0
"Import all videos in a folder or if just one video withouth [] passed, then make it a list.",0
there are two cases:,0
https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
try:,0
"#shutil.copy(src,dst)",0
except OSError or TypeError: #https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
"shutil.copy(os.fspath(src),os.fspath(dst))",0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Set values to config file:,0
Write dictionary to yaml  config file,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = io.imread(img),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
self.cropping = False,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
"SP.ScrolledPanel.__init__(self, parent, -1,style=wx.SUNKEN_BORDER, **kwargs)",0
self.parent = parent,0
"self.SetupScrolling(scroll_x=True, scrollToTop=False)",0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
widgetsizer.AddStretchSpacer(15),0
"self.adjustLabelCheck = wx.CheckBox(self.widget_panel,id=wx.ID_ANY,  label = 'Adjust original labels?')",0
"widgetsizer.Add(self.adjustLabelCheck , 1, wx.ALL, 15)",0
"self.adjustLabelCheck.Bind(wx.EVT_CHECKBOX,self.adjustLabel)",0
##############################################################################################################################,0
Variable initialization,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.slider.Enable(False),0
self.checkBox.Enable(False),0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
print(self.points),0
Checks if zoom/pan button is ON,0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
extract min and max index based on start stop interval.,0
figure out body part list:,0
all indices between start and stop with jump larger than epsilon (leading up to this point!),0
deviation_dataname = str(Path(videofolder)/Path(dataname)),0
Calculate deviatons for video,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
data.to_csv(dataname.split('.h5')[0]+'filtered.csv'),0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
"Dataframe = pd.read_hdf(os.path.join(videofolder,dataname+'.h5'))",0
drop duplicate labels:,0
updates iteration by 1,0
###################################################,0
Dependencies,0
###################################################,0
import sys,0
"rr, cc = circle_perimeter(yc,xc,radius)",0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
vname=str(Path(tmpfolder).stem).split('-')[1],0
# One can change the parameters of the video creation script below:,0
See ffmpeg user guide: http://ffmpeg.org/ffmpeg.html#Video-and-Audio-file-format-conversion,0
,0
videofolder = str(Path(video).parents[0]),0
Loading cropping data used during analysis,0
"CreateVideoSlow(clip,Dataframe,tmpfolder,cfg[""dotsize""],cfg[""colormap""],cfg[""alphavalue""],cfg[""pcutoff""],cfg[""cropping""],cfg[""x1""],cfg[""x2""],cfg[""y1""],cfg[""y2""],delete,DLCscorer,bodyparts)",0
"windows throws error if file path is > 260 characters, can fix with prefix. see https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation",0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
###################################################,0
Dependencies,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
"fig.colorbar(im, ax=ax)",0
#################################################,0
Looping analysis over video,0
#################################################,0
"nlines,numcolumns=data.shape",0
assert(len(orderofbpincsv)==len(cfg['bodyparts'])),0
TODO: test len of images vs. len of imagenames for another sanity check,1
Is there a scorer for this?,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
checks if input is a directory,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"dlc_cfg = read_config(os.path.join(modelfolder,'pose_cfg.yaml'))",0
"dlc_cfg['init_weights'] = os.path.join(modelfolder , 'train', Snapshots[snapshotindex])",0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
convert the image to RGB as you are showing the image with matplotlib,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
MainFrame.saveEachImage(self),0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
MainFrame.saveEachImage(self),0
"def getLabels(self,ind,img_index):",0
"""""""",0
Returns a list of x and y labels of the corresponding image index,0
"""""""",0
self.previous_image_points = [],0
"for bpindex, bp in enumerate(self.bodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
self.previous_image_points.append(image_points),0
return(self.previous_image_points),0
"self.dr.coords = MainFrame.getLabels(self,ind,self.iter)[bpindex]",0
Windows compatible,0
Checks if zoom/pan button is ON,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
convert the image to RGB as you are showing the image with matplotlib,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"self.axes.callbacks.connect('xlim_changed', self.onZoom)",0
"self.axes.callbacks.connect('ylim_changed', self.onZoom)",0
"def onZoom(self, ax):",0
print(self.zoom.GetValue()),0
if self.zoom.GetValue():,0
self.updateZoomPan(),0
"self.statusbar.SetStatusText(""Zoom Off"")",0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
"self.axes.callbacks.connect('xlim_changed', self.onZoom)",0
"self.axes.callbacks.connect('ylim_changed', self.onZoom)",0
"self.axes.callbacks.connect('xlim_changed', self.onZoom)",0
"self.axes.callbacks.connect('ylim_changed', self.onZoom)",0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
"self.axes.callbacks.connect('xlim_changed', self.onZoom)",0
"self.axes.callbacks.connect('ylim_changed', self.onZoom)",0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
"self.axes.callbacks.connect('xlim_changed', self.onZoom)",0
"self.axes.callbacks.connect('ylim_changed', self.onZoom)",0
Windows compatible,0
Checks if zoom/pan button is ON,0
"if os.environ.get('DLClight', default=False) == 'True':",0
mpl.use('AGG') #anti-grain geometry engine #https://matplotlib.org/faq/usage_faq.html,0
pass,0
else:,0
mpl.use('TkAgg'),0
import matplotlib.pyplot as plt,0
matplotlib.use('Agg'),0
Load updated lists:,0
cfg['video_sets'][vn],0
"labeling_toolbox.show(config,Screens,scale_w,scale_h, winHack, img_scale)",0
plotting parameters:,0
folders = [Path(config).parent / 'labeled-data' /Path(i) for i in video_names],0
Let's check if the code is *not* run on windows (Source: #https://stackoverflow.com/questions/1325581/how-do-i-check-if-im-running-on-windows-in-python),0
but the paths are in windows format...,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(index,name.split(os.sep)[1])",0
"print(name,test_video_name)",0
Loading metadata from config file:,0
Create path for training sets & store data there,0
set model type. we will allow more in the future.,0
"trainIndexes, testIndexes = SplitTrials(range(len(Data.index)), trainFraction)",0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
load image to get dimensions:,0
"print ""Grayscale!""",0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
"text = str(self.individual_names+','+self.bodyParts)",0
Check for variable correctness,0
"plt.close(""all"")",0
indexlength = int(np.ceil(np.log10(clip.duration * clip.fps))),0
Moviepy:,0
Display the image,0
ax.imshow(image),0
Call the GUI to select the cropping parameters,0
Update the config.yaml file with current cropping parameters,0
store full frame from random location (good for augmentation),0
"clip=clip.crop(y1 = int(coords[2]),y2 = int(coords[3]),x1 = int(coords[0]), x2 = int(coords[1]))",0
close video.,0
"def __init__(self, point,bodyParts):",0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
text = str(self.bodyParts),0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
#################################################,0
Datafolder,0
#################################################,0
looping over videos,0
Attempt to load data...,0
"frame=io.imread(os.path.join(directory,framename),mode='RGB')",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
Attempt to load data...,0
Parameters for augmentation with regard to cropping,0
"limit width  [-leftwidth*u-100,100+u*rightwidth] x [-bottomwith*u-100,100+u*topwidth] where u is always a (different) random number in unit interval",0
these will be updated by trainingsetmanipulation.py in the future,0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
Dependencies for anaysis,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
The next part of the code depends upon which tensorflow version you have.,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"Xstart=int(np.max([0,int(Xlabel)-widthback]))",0
"Xstop=int(np.min([np.shape(im)[1]-1,int(Xlabel)+widthforward]))",0
"Ystart=int(np.max([0,int(Ylabel)-hdown]))",0
"Ystop=int(np.min([np.shape(im)[0]-1,int(Ylabel)+hup]))",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Importing the toolbox (takes several seconds),0
Loading example data set,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
for windows:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://stackoverflow.com/questions/39590187/in-requirements-txt-what-does-tilde-equals-mean,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
Supress tensorflow warning messages,0
Direct import for convenience,0
Direct import for convenience,0
from deeplabcut import create_project,0
Direct import for convenience,0
Read the config file,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video.,0
video_path = os.path.realpath(video),0
get vcap property,0
Create project and sub-directories,0
"Import all videos in a folder or if just one video withouth [] passed, then make it a list.",0
there are two cases:,0
https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
try:,0
"#shutil.copy(src,dst)",0
except OSError or TypeError: #https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
"shutil.copy(os.fspath(src),os.fspath(dst))",0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
For windows os.path.realpath does not work and does not link to the real video. [old: rel_video_path = os.path.realpath(video)],0
Set values to config file:,0
Write dictionary to yaml  config file,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = io.imread(img),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
"self.startFrame = wx.SpinCtrl(self.widget_panel, value='0', size=(100, -1), min=0, max=120)",0
,0
Variables initialization,0
self.cropping = False,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
Check for it output path and a machine label file exist,0
If machine label file does not exist then create one,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
"SP.ScrolledPanel.__init__(self, parent, -1,style=wx.SUNKEN_BORDER, **kwargs)",0
self.parent = parent,0
"self.SetupScrolling(scroll_x=True, scrollToTop=False)",0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
widgetsizer.AddStretchSpacer(15),0
"self.adjustLabelCheck = wx.CheckBox(self.widget_panel,id=wx.ID_ANY,  label = 'Adjust original labels?')",0
"widgetsizer.Add(self.adjustLabelCheck , 1, wx.ALL, 15)",0
"self.adjustLabelCheck.Bind(wx.EVT_CHECKBOX,self.adjustLabel)",0
##############################################################################################################################,0
Variable initialization,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.slider.Enable(False),0
self.checkBox.Enable(False),0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
print(self.points),0
Checks if zoom/pan button is ON,0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
extract min and max index based on start stop interval.,0
figure out body part list:,0
all indices between start and stop with jump larger than epsilon (leading up to this point!),0
deviation_dataname = str(Path(videofolder)/Path(dataname)),0
Calculate deviatons for video,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
data.to_csv(dataname.split('.h5')[0]+'filtered.csv'),0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
"Dataframe = pd.read_hdf(os.path.join(videofolder,dataname+'.h5'))",0
drop duplicate labels:,0
updates iteration by 1,0
###################################################,0
Dependencies,0
###################################################,0
import sys,0
"rr, cc = circle_perimeter(yc,xc,radius)",0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
vname=str(Path(tmpfolder).stem).split('-')[1],0
# One can change the parameters of the video creation script below:,0
See ffmpeg user guide: http://ffmpeg.org/ffmpeg.html#Video-and-Audio-file-format-conversion,0
,0
videofolder = str(Path(video).parents[0]),0
Loading cropping data used during analysis,0
"CreateVideoSlow(clip,Dataframe,tmpfolder,cfg[""dotsize""],cfg[""colormap""],cfg[""alphavalue""],cfg[""pcutoff""],cfg[""cropping""],cfg[""x1""],cfg[""x2""],cfg[""y1""],cfg[""y2""],delete,DLCscorer,bodyparts)",0
"windows throws error if file path is > 260 characters, can fix with prefix. see https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation",0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
###################################################,0
Dependencies,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
"fig.colorbar(im, ax=ax)",0
#################################################,0
Looping analysis over video,0
#################################################,0
"nlines,numcolumns=data.shape",0
assert(len(orderofbpincsv)==len(cfg['bodyparts'])),0
TODO: test len of images vs. len of imagenames for another sanity check,1
Is there a scorer for this?,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
checks if input is a directory,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"dlc_cfg = read_config(os.path.join(modelfolder,'pose_cfg.yaml'))",0
"dlc_cfg['init_weights'] = os.path.join(modelfolder , 'train', Snapshots[snapshotindex])",0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
Variables initialization,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
convert the image to RGB as you are showing the image with matplotlib,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
MainFrame.saveEachImage(self),0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
MainFrame.saveEachImage(self),0
"def getLabels(self,ind,img_index):",0
"""""""",0
Returns a list of x and y labels of the corresponding image index,0
"""""""",0
self.previous_image_points = [],0
"for bpindex, bp in enumerate(self.bodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
self.previous_image_points.append(image_points),0
return(self.previous_image_points),0
"self.dr.coords = MainFrame.getLabels(self,ind,self.iter)[bpindex]",0
Windows compatible,0
Checks if zoom/pan button is ON,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
convert the image to RGB as you are showing the image with matplotlib,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"self.axes.callbacks.connect('xlim_changed', self.onZoom)",0
"self.axes.callbacks.connect('ylim_changed', self.onZoom)",0
"def onZoom(self, ax):",0
print(self.zoom.GetValue()),0
if self.zoom.GetValue():,0
self.updateZoomPan(),0
"self.statusbar.SetStatusText(""Zoom Off"")",0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
"self.axes.callbacks.connect('xlim_changed', self.onZoom)",0
"self.axes.callbacks.connect('ylim_changed', self.onZoom)",0
"self.axes.callbacks.connect('xlim_changed', self.onZoom)",0
"self.axes.callbacks.connect('ylim_changed', self.onZoom)",0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
"self.axes.callbacks.connect('xlim_changed', self.onZoom)",0
"self.axes.callbacks.connect('ylim_changed', self.onZoom)",0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
"self.axes.callbacks.connect('xlim_changed', self.onZoom)",0
"self.axes.callbacks.connect('ylim_changed', self.onZoom)",0
Windows compatible,0
Checks if zoom/pan button is ON,0
"if os.environ.get('DLClight', default=False) == 'True':",0
mpl.use('AGG') #anti-grain geometry engine #https://matplotlib.org/faq/usage_faq.html,0
pass,0
else:,0
mpl.use('TkAgg'),0
import matplotlib.pyplot as plt,0
matplotlib.use('Agg'),0
Load updated lists:,0
cfg['video_sets'][vn],0
"labeling_toolbox.show(config,Screens,scale_w,scale_h, winHack, img_scale)",0
plotting parameters:,0
folders = [Path(config).parent / 'labeled-data' /Path(i) for i in video_names],0
Let's check if the code is *not* run on windows (Source: #https://stackoverflow.com/questions/1325581/how-do-i-check-if-im-running-on-windows-in-python),0
but the paths are in windows format...,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
"print(index,name.split(os.sep)[1])",0
"print(name,test_video_name)",0
Loading metadata from config file:,0
Create path for training sets & store data there,0
set model type. we will allow more in the future.,0
"trainIndexes, testIndexes = SplitTrials(range(len(Data.index)), trainFraction)",0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
load image to get dimensions:,0
"print ""Grayscale!""",0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
"text = str(self.individual_names+','+self.bodyParts)",0
Check for variable correctness,0
"plt.close(""all"")",0
indexlength = int(np.ceil(np.log10(clip.duration * clip.fps))),0
Moviepy:,0
Display the image,0
ax.imshow(image),0
Call the GUI to select the cropping parameters,0
Update the config.yaml file with current cropping parameters,0
store full frame from random location (good for augmentation),0
"clip=clip.crop(y1 = int(coords[2]),y2 = int(coords[3]),x1 = int(coords[0]), x2 = int(coords[1]))",0
close video.,0
"def __init__(self, point,bodyParts):",0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
text = str(self.bodyParts),0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
#################################################,0
Datafolder,0
#################################################,0
looping over videos,0
Attempt to load data...,0
"frame=io.imread(os.path.join(directory,framename),mode='RGB')",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
Attempt to load data...,0
Parameters for augmentation with regard to cropping,0
"limit width  [-leftwidth*u-100,100+u*rightwidth] x [-bottomwith*u-100,100+u*topwidth] where u is always a (different) random number in unit interval",0
these will be updated by trainingsetmanipulation.py in the future,0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
Dependencies for anaysis,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
The next part of the code depends upon which tensorflow version you have.,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"Xstart=int(np.max([0,int(Xlabel)-widthback]))",0
"Xstop=int(np.min([np.shape(im)[1]-1,int(Xlabel)+widthforward]))",0
"Ystart=int(np.max([0,int(Ylabel)-hdown]))",0
"Ystop=int(np.min([np.shape(im)[0]-1,int(Ylabel)+hup]))",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Importing the toolbox (takes several seconds),0
Loading example data set,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
to test destination folder:,0
dfolder=basepath,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
for windows:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://stackoverflow.com/questions/39590187/in-requirements-txt-what-does-tilde-equals-mean,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
Supress tensorflow warning messages,0
Direct import for convenience,0
Direct import for convenience,0
from deeplabcut import create_project,0
Direct import for convenience,0
Read the config file,0
adds the video list to the config.yaml file,0
get vcap property,0
Create project and sub-directories,0
"Import all videos in a folder or if just one video withouth [] passed, then make it a list.",0
there are two cases:,0
https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
try:,0
"#shutil.copy(src,dst)",0
except OSError or TypeError: #https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
"shutil.copy(os.fspath(src),os.fspath(dst))",0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
Set values to config file:,0
Write dictionary to yaml  config file,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
im = io.imread(img),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
"widgetsizer.Add(self.help , 1, wx.ALL, 15)",0
,0
Variables initialization,0
self.cropping = False,0
Read confing file,0
Read the video file,0
Set the values of slider and range of frames,0
Set the status bar,0
Adding the video file to the config file.,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
"SP.ScrolledPanel.__init__(self, parent, -1,style=wx.SUNKEN_BORDER, **kwargs)",0
self.parent = parent,0
"self.SetupScrolling(scroll_x=True, scrollToTop=False)",0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
widgetsizer.AddStretchSpacer(15),0
"self.adjustLabelCheck = wx.CheckBox(self.widget_panel,id=wx.ID_ANY,  label = 'Adjust original labels?')",0
"widgetsizer.Add(self.adjustLabelCheck , 1, wx.ALL, 15)",0
"self.adjustLabelCheck.Bind(wx.EVT_CHECKBOX,self.adjustLabel)",0
##############################################################################################################################,0
Variable initialization,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"dlg = wx.FileDialog(self, ""Choose the machinelabels file for current iteration."",cwd, """",wildcard=fname,style=wx.FD_OPEN | wx.FD_FILE_MUST_EXIST)",0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.slider.Enable(False),0
self.checkBox.Enable(False),0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
print(self.points),0
Checks if zoom/pan button is ON,0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
extract min and max index based on start stop interval.,0
figure out body part list:,0
all indices between start and stop with jump larger than epsilon (leading up to this point!),0
deviation_dataname = str(Path(videofolder)/Path(dataname)),0
Calculate deviatons for video,0
Some heuristics for extracting frames based on distance:,0
Run always except when the outlieralgorithm == manual.,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"meanx,CIx=meanx.values,CIx.values",0
"meany,CIy=meany.values,CIy.values",0
data.to_csv(dataname.split('.h5')[0]+'filtered.csv'),0
elif extractionalgorithm=='manual':,0
wd = Path(config).resolve().parents[0],0
os.chdir(str(wd)),0
from deeplabcut.refine_training_dataset import outlier_frame_extraction_toolbox,0
"outlier_frame_extraction_toolbox.show(config,video,shuffle)",0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
###################################################,0
Dependencies,0
###################################################,0
import sys,0
"rr, cc = circle_perimeter(yc,xc,radius)",0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
# One can change the parameters of the video creation script below:,0
See ffmpeg user guide: http://ffmpeg.org/ffmpeg.html#Video-and-Audio-file-format-conversion,0
,0
print(metadata),0
Loading cropping data used during analysis,0
"CreateVideoSlow(clip,Dataframe,tmpfolder,cfg[""dotsize""],cfg[""colormap""],cfg[""alphavalue""],cfg[""pcutoff""],cfg[""cropping""],cfg[""x1""],cfg[""x2""],cfg[""y1""],cfg[""y2""],delete,DLCscorer,bodyparts)",0
"windows throws error if file path is > 260 characters, can fix with prefix. see https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation",0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
###################################################,0
Dependencies,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
"fig.colorbar(im, ax=ax)",0
vname = video.split('.')[0],0
#################################################,0
Looping analysis over video,0
#################################################,0
"nlines,numcolumns=data.shape",0
assert(len(orderofbpincsv)==len(cfg['bodyparts'])),0
TODO: test len of images vs. len of imagenames for another sanity check,1
Is there a scorer for this?,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
checks if input is a directory,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"dlc_cfg = read_config(os.path.join(modelfolder,'pose_cfg.yaml'))",0
"dlc_cfg['init_weights'] = os.path.join(modelfolder , 'train', Snapshots[snapshotindex])",0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counters,0
MainFrame.saveEachImage(self),0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
MainFrame.saveEachImage(self),0
"def getLabels(self,ind,img_index):",0
"""""""",0
Returns a list of x and y labels of the corresponding image index,0
"""""""",0
self.previous_image_points = [],0
"for bpindex, bp in enumerate(self.bodyparts):",0
"image_points = [[self.dataFrame[self.scorer][ind][bp]['x'].values[self.iter],self.dataFrame[self.scorer][ind][bp]['y'].values[self.iter],ind,bp]]",0
self.previous_image_points.append(image_points),0
return(self.previous_image_points),0
"self.dr.coords = MainFrame.getLabels(self,ind,self.iter)[bpindex]",0
Windows compatible,0
Checks if zoom/pan button is ON,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"Reading the existing dataset,if already present",0
Finds the first empty row in the dataframe and sets the iteration to that index,0
Reading the image name,0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
Windows compatible,0
Checks if zoom/pan button is ON,0
matplotlib.use('Agg'),0
Load updated lists:,0
cfg['video_sets'][vn],0
"labeling_toolbox.show(config,Screens,scale_w,scale_h, winHack, img_scale)",0
plotting parameters:,0
folders = [Path(config).parent / 'labeled-data' /Path(i) for i in video_names],0
Let's check if the code is *not* run on windows (Source: #https://stackoverflow.com/questions/1325581/how-do-i-check-if-im-running-on-windows-in-python),0
but the paths are in windows format...,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
set model type. we will allow more in the future.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
load image to get dimensions:,0
"print ""Grayscale!""",0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
Hiding these widgets and show them once the video is loaded,0
Variables initialization,0
Select ROI of interest by drawing a rectangle,0
Update the config.yaml file,0
Checks if the video is corrupt.,0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
"text = str(self.individual_names+','+self.bodyParts)",0
Check for variable correctness,0
"plt.close(""all"")",0
indexlength = int(np.ceil(np.log10(clip.duration * clip.fps))),0
Moviepy:,0
Display the image,0
store full frame from random location (good for augmentation),0
"clip=clip.crop(y1 = int(coords[2]),y2 = int(coords[3]),x1 = int(coords[0]), x2 = int(coords[1]))",0
close video.,0
"def __init__(self, point,bodyParts):",0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
text = str(self.bodyParts),0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
#################################################,0
Datafolder,0
#################################################,0
looping over videos,0
Attempt to load data...,0
"frame=io.imread(os.path.join(directory,framename),mode='RGB')",0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
Attempt to load data...,0
Parameters for augmentation with regard to cropping,0
"limit width  [-leftwidth*u-100,100+u*rightwidth] x [-bottomwith*u-100,100+u*topwidth] where u is always a (different) random number in unit interval",0
these will be updated by trainingsetmanipulation.py in the future,0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
Dependencies for anaysis,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
The next part of the code depends upon which tensorflow version you have.,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"Xstart=int(np.max([0,int(Xlabel)-widthback]))",0
"Xstop=int(np.min([np.shape(im)[1]-1,int(Xlabel)+widthforward]))",0
"Ystart=int(np.max([0,int(Ylabel)-hdown]))",0
"Ystop=int(np.min([np.shape(im)[0]-1,int(Ylabel)+hup]))",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Importing the toolbox (takes several seconds),0
Loading example data set,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
for windows:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://stackoverflow.com/questions/39590187/in-requirements-txt-what-does-tilde-equals-mean,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
Supress tensorflow warning messages,0
Direct import for convenience,0
from deeplabcut import create_project,0
Direct import for convenience,0
Read the config file,0
adds the video list to the config.yaml file,0
get vcap property,0
Create project and sub-directories,0
"Import all videos in a folder or if just one video withouth [] passed, then make it a list.",0
there are two cases:,0
https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
try:,0
"#shutil.copy(src,dst)",0
except OSError or TypeError: #https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
"shutil.copy(os.fspath(src),os.fspath(dst))",0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
Set values to config file:,0
Write dictionary to yaml  config file,0
from skimage import io,0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
"SP.ScrolledPanel.__init__(self, parent, -1,style=wx.SUNKEN_BORDER, **kwargs)",0
self.parent = parent,0
"self.SetupScrolling(scroll_x=True, scrollToTop=False)",0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
widgetsizer.AddStretchSpacer(15),0
"self.adjustLabelCheck = wx.CheckBox(self.widget_panel,id=wx.ID_ANY,  label = 'Adjust original labels?')",0
"widgetsizer.Add(self.adjustLabelCheck , 1, wx.ALL, 15)",0
"self.adjustLabelCheck.Bind(wx.EVT_CHECKBOX,self.adjustLabel)",0
##############################################################################################################################,0
Variable initialization,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
bodyParts = self.Dataframe.columns.get_level_values(1),0
"_, idx = np.unique(bodyParts, return_index=True)",0
self.num_joints = len(self.bodyparts),0
self.bodyparts =  bodyParts[np.sort(idx)],0
Reading images,0
Adding Slider and Checkbox,0
Show image,0
Setting axis title:dont want to show the threshold as it is not selected yet.,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks for the first image and disables the Previous button,0
Reading Image,0
Plotting,0
Checks if zoom/pan button is ON,0
Checks if zoom/pan button is ON,0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
self.slider.Enable(False),0
self.checkBox.Enable(False),0
###########################################################################,0
Other functions,0
###########################################################################,0
small hack in case there are any 0 intensity images!,1
print(self.points),0
Checks if zoom/pan button is ON,0
self.adjust_original_labels = adjust_original_labels,0
restore the background region,0
if self.adjust_original_labels == True:,0
text = str(self.bodyParts),0
else:,0
extract min and max index based on start stop interval.,0
figure out body part list:,0
all indices between start and stop with jump larger than epsilon (leading up to this point!),0
deviation_dataname = str(Path(videofolder)/Path(dataname)),0
Calculate deviatons for video,0
Some heuristics for extracting frames based on distance:,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"meanx,CIx=meanx.values,CIx.values",0
"meany,CIy=meany.values,CIy.values",0
data.to_csv(dataname.split('.h5')[0]+'filtered.csv'),0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
###################################################,0
Dependencies,0
###################################################,0
import sys,0
"rr, cc = circle_perimeter(yc,xc,radius)",0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
# One can change the parameters of the video creation script below:,0
See ffmpeg user guide: http://ffmpeg.org/ffmpeg.html#Video-and-Audio-file-format-conversion,0
,0
print(metadata),0
Loading cropping data used during analysis,0
"CreateVideoSlow(clip,Dataframe,tmpfolder,cfg[""dotsize""],cfg[""colormap""],cfg[""alphavalue""],cfg[""pcutoff""],cfg[""cropping""],cfg[""x1""],cfg[""x2""],cfg[""y1""],cfg[""y2""],delete,DLCscorer,bodyparts)",0
"windows throws error if file path is > 260 characters, can fix with prefix. see https://docs.microsoft.com/en-us/windows/desktop/fileio/naming-a-file#maximum-path-length-limitation",0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
###################################################,0
Dependencies,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
"fig.colorbar(im, ax=ax)",0
vname = video.split('.')[0],0
#################################################,0
Looping analysis over video,0
#################################################,0
"nlines,numcolumns=data.shape",0
TODO: test len of images vs. len of imagenames for another sanity check,1
Is there a scorer for this?,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
checks if input is a directory,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"dlc_cfg = read_config(os.path.join(modelfolder,'pose_cfg.yaml'))",0
"dlc_cfg['init_weights'] = os.path.join(modelfolder , 'train', Snapshots[snapshotindex])",0
import matplotlib as mpl,0
mpl.use('WxAgg'),0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
"def __init__(self, parent, **kwargs):",0
"SP.ScrolledPanel.__init__(self, parent, -1,style=wx.SUNKEN_BORDER, **kwargs)",0
self.parent = parent,0
"self.SetupScrolling(scroll_x=True, scrollToTop=False)",0
self.Layout(),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
if self.pan.GetValue() == True:,0
self.toolbar.pan(),0
if self.zoom.GetValue() == True:,0
self.toolbar.zoom(),0
if self.flag == len(self.bodyparts):,0
"wx.MessageBox('All body parts are annotated! Click \'Save\' to save the changes. \n Click OK to continue.', 'Done!', wx.OK | wx.ICON_INFORMATION)",0
self.canvas.mpl_disconnect(self.onClick),0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"np.sort(glob.glob(os.path.join(self.dir,'*.png')))",0
"self.labeled_imgs = np.sort(glob.glob(os.path.join(self.dir,'*_labeled.png')))",0
self.index = np.sort(list(set(self.index) - set(self.labeled_imgs))),0
Reading the image name,0
"Reading the existing dataset,if already present",0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
self.dr.coords = self.updatedCoords,0
self.updatedCoords = [],0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
print(self.updatedCoords),0
"self.dataFrame.loc[self.index[self.iter]][self.scorer, bp[0][-2],'x' ] = bp[-1][0]",0
"self.dataFrame.loc[self.index[self.iter]][self.scorer, bp[0][-2],'y' ] = bp[-1][1]",0
"self.dataFrame.loc[self.relativeimagenames[self.iter]][self.scorer, bp[0][-2],'x' ] = bp[-1][0]",0
"self.dataFrame.loc[self.relativeimagenames[self.iter]][self.scorer, bp[0][-2],'y' ] = bp[-1][1]",0
Windows compatible,0
Checks if zoom/pan button is ON,0
matplotlib.use('Agg'),0
Load updated lists:,0
cfg['video_sets'][vn],0
"labeling_toolbox.show(config,Screens,scale_w,scale_h, winHack, img_scale)",0
plotting parameters:,0
folders = [Path(config).parent / 'labeled-data' /Path(i) for i in video_names],0
Let's check if the code is *not* run on windows (Source: #https://stackoverflow.com/questions/1325581/how-do-i-check-if-im-running-on-windows-in-python),0
but the paths are in windows format...,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
set model type. we will allow more in the future.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
load image to get dimensions:,0
"print ""Grayscale!""",0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
matplotlib.use('Agg'),0
Load updated lists:,0
cfg['video_sets'][vn],0
"labeling_toolbox.show(config,Screens,scale_w,scale_h, winHack, img_scale)",0
plotting parameters:,0
folders = [Path(config).parent / 'labeled-data' /Path(i) for i in video_names],0
Let's check if the code is *not* run on windows (Source: #https://stackoverflow.com/questions/1325581/how-do-i-check-if-im-running-on-windows-in-python),0
but the paths are in windows format...,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
set model type. we will allow more in the future.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make training file!,0
load image to get dimensions:,0
"print ""Grayscale!""",0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
matplotlib.use('TkAgg') #already set..,0
from skimage.util import img_as_ubyte,0
import math,0
"print(""Scaled GUI width"", self.gui_width, ""and height"", self.gui_height)",0
Select ROI of interest by adjusting values in myconfig.py,0
time = start,0
set image size:,0
"self.ax1f1.set_title(str(str(self.currFrame)+""/""+str(self.numberFrames) +"" ""+ self.filename))",0
self.canvas.Destroy(),0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
Check for variable correctness,0
"plt.close(""all"")",0
indexlength = int(np.ceil(np.log10(clip.duration * clip.fps))),0
Moviepy:,0
Display the image,0
Create a Rectangle patch,0
Add the patch to the Axes,0
"saveimg = str(Path(config).parents[0] / Path('labeled-data','IsCroppingOK_'+fname.stem +"".png""))",0
"io.imsave(saveimg, image)",0
store full frame from random location (good for augmentation),0
"clip=clip.crop(y1 = int(coords[2]),y2 = int(coords[3]),x1 = int(coords[0]), x2 = int(coords[1]))",0
close video.,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
#################################################,0
Datafolder,0
#################################################,0
looping over videos,0
Attempt to load data...,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
Attempt to load data...,0
Parameters for augmentation with regard to cropping,0
"limit width  [-leftwidth*u-100,100+u*rightwidth] x [-bottomwith*u-100,100+u*topwidth] where u is always a (different) random number in unit interval",0
these will be updated by trainingsetmanipulation.py in the future,0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
Dependencies for anaysis,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
The next part of the code depends upon which tensorflow version you have.,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"Xstart=int(np.max([0,int(Xlabel)-widthback]))",0
"Xstop=int(np.min([np.shape(im)[1]-1,int(Xlabel)+widthforward]))",0
"Ystart=int(np.max([0,int(Ylabel)-hdown]))",0
"Ystop=int(np.min([np.shape(im)[0]-1,int(Ylabel)+hup]))",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Importing the toolbox (takes several seconds),0
Loading example data set,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
for windows:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://stackoverflow.com/questions/39590187/in-requirements-txt-what-does-tilde-equals-mean,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
Supress tensorflow warning messages,0
Direct import for convenience,0
from deeplabcut import create_project,0
Direct import for convenience,0
Read the config file,0
adds the video list to the config.yaml file,0
get vcap property,0
Create project and sub-directories,0
"Import all videos in a folder or if just one video withouth [] passed, then make it a list.",0
there are two cases:,0
https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
try:,0
"#shutil.copy(src,dst)",0
except OSError or TypeError: #https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
"shutil.copy(os.fspath(src),os.fspath(dst))",0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
Set values to config file:,0
Write dictionary to yaml  config file,0
from skimage import io,0
import matplotlib as mpl,0
mpl.use('TkAgg'),0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
minic small screen:,0
"displaysize = (400, 400)",0
"Note, if the variable Screens = 2, it assumes two screens in landscape next to eachother! If you use a different configuration, consider changing displaysize to your known display size. see troubleshooting for more information https://github.com/AlexEMG/DeepLabCut/wiki/Troubleshooting-Tips.",0
"On Windows, there can be a issue with the sizing on start, so you can scale it down then resize on your screen. Namely, set winHack=.5 and this solves this issue. Thanks to Federico Claudi for troubleshooting this with us!",1
"print(""Scaled GUI width"", self.gui_width, ""and height"", self.gui_height)",0
Add SplitterWindow panels top for figure and bottom for buttons,0
"self.top_split = wx.Panel(self.split_win, style=wx.SUNKEN_BORDER)",0
Add Buttons to the bottom_split window and bind them to plot functions,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
Reading images,0
Plotting,0
"imagename1 = os.path.join(self.dir,self.index[self.iter])",0
Checks for the last image and disables the Next button,0
read the image,0
"imagename1 = os.path.join(self.dir,self.index[self.iter])",0
Plotting,0
"imagename1 = os.path.join(self.dir,self.index[self.iter])",0
im=io.imread(imagename1),0
Plotting,0
self.cb.SetValue(False),0
Checks for the first image and disables the Previous button,0
Reading Image,0
"imagename1 = os.path.join(self.dir,""file%04d.png"" % self.index[self.iter])",0
"imagename1 = os.path.join(self.dir,self.index[self.iter])",0
im=io.imread(imagename1),0
Plotting,0
"plt.tight_layout(rect=[0, 0.1, 1, 0.95])",0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
"self.canvas = FigureCanvas(self, -1, self.fig1)",0
small hack in case there are any 0 intensity images!,1
"cbar.set_ticks(range(12,np.max(im),8))",0
Calling auxfun_drag class for moving points around,0
###########################################################################,0
Class for MatPlotLib Panel,0
###########################################################################,0
restore the background region,0
extract min and max index based on start stop interval.,0
figure out body part list:,0
all indices between start and stop with jump larger than epsilon (leading up to this point!),0
deviation_dataname = str(Path(videofolder)/Path(dataname)),0
Calculate deviatons for video,0
Some heuristics for extracting frames based on distance:,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"meanx,CIx=meanx.values,CIx.values",0
"meany,CIy=meany.values,CIy.values",0
data.to_csv(dataname.split('.h5')[0]+'filtered.csv'),0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
###################################################,0
Dependencies,0
###################################################,0
import sys,0
"rr, cc = circle_perimeter(yc,xc,radius)",0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
print(metadata),0
Loading cropping data used during analysis,0
"CreateVideoSlow(clip,Dataframe,tmpfolder,cfg[""dotsize""],cfg[""colormap""],cfg[""alphavalue""],cfg[""pcutoff""],cfg[""cropping""],cfg[""x1""],cfg[""x2""],cfg[""y1""],cfg[""y2""],delete,DLCscorer,bodyparts)",0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
###################################################,0
Dependencies,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
"fig.colorbar(im, ax=ax)",0
vname = video.split('.')[0],0
#################################################,0
Looping analysis over video,0
#################################################,0
checks if input is a directory,0
"nlines,numcolumns=data.shape",0
TODO: test len of images vs. len of imagenames for another sanity check,1
Is there a scorer for this?,0
Project definitions (do not edit),0
Project path (change when moving around),0
Annotation data set configuration (and individual video cropping parameters),0
Plotting configuration,0
"Training,Evaluation and Analysis configuration",0
Cropping Parameters (for analysis and outlier frame detection),0
"if cropping is true for analysis, then set the values here:",0
Refinement configuration (parameters from annotation dataset configuration also relevant in this stage),0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"dlc_cfg = read_config(os.path.join(modelfolder,'pose_cfg.yaml'))",0
"dlc_cfg['init_weights'] = os.path.join(modelfolder , 'train', Snapshots[snapshotindex])",0
import matplotlib as mpl,0
mpl.use('WxAgg'),0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
"def __init__(self, parent, **kwargs):",0
"SP.ScrolledPanel.__init__(self, parent, -1,style=wx.SUNKEN_BORDER, **kwargs)",0
self.parent = parent,0
"self.SetupScrolling(scroll_x=True, scrollToTop=False)",0
self.Layout(),0
Settting the GUI size and panels design,0
##################################################################################################################################################,0
Spliting the frame into top and bottom panels. Bottom panels contains the widgets. The top panel is for showing images and plotting!,0
"self.choice_panel.SetupScrolling(scroll_x=True, scroll_y=True, scrollToTop=False)",0
"self.choice_panel.SetupScrolling(scroll_x=True, scrollToTop=False)",0
##################################################################################################################################################,0
Add Buttons to the WidgetPanel and bind them to their respective functions.,0
,0
##############################################################################################################################,0
Variables initialization,0
##############################################################################################################################,0
BUTTONS FUNCTIONS FOR HOTKEYS,0
if self.pan.GetValue() == True:,0
self.toolbar.pan(),0
if self.zoom.GetValue() == True:,0
self.toolbar.zoom(),0
if self.flag == len(self.bodyparts):,0
"wx.MessageBox('All body parts are annotated! Click \'Save\' to save the changes. \n Click OK to continue.', 'Done!', wx.OK | wx.ICON_INFORMATION)",0
self.canvas.mpl_disconnect(self.onClick),0
"Enabling the zoom, pan and home buttons",0
Reading config file and its variables,0
"np.sort(glob.glob(os.path.join(self.dir,'*.png')))",0
"self.labeled_imgs = np.sort(glob.glob(os.path.join(self.dir,'*_labeled.png')))",0
self.index = np.sort(list(set(self.index) - set(self.labeled_imgs))),0
Reading the image name,0
"Reading the existing dataset,if already present",0
Checking for new frames and adding them to the existing dataframe,0
Create an empty dataframe with all the new images and then merge this to the existing dataframe.,0
Sort it by the index values,0
checks for unique bodyparts,0
Extracting the list of new labels,0
Checking if user added a new label,0
Checks for the last image and disables the Next button,0
Checks if zoom/pan button is ON,0
Refreshing the button counter,0
self.dr.coords = self.updatedCoords,0
self.updatedCoords = [],0
Checks for the first image and disables the Previous button,0
Checks if zoom/pan button is ON,0
print(self.updatedCoords),0
"self.dataFrame.loc[self.index[self.iter]][self.scorer, bp[0][-2],'x' ] = bp[-1][0]",0
"self.dataFrame.loc[self.index[self.iter]][self.scorer, bp[0][-2],'y' ] = bp[-1][1]",0
"self.dataFrame.loc[self.relativeimagenames[self.iter]][self.scorer, bp[0][-2],'x' ] = bp[-1][0]",0
"self.dataFrame.loc[self.relativeimagenames[self.iter]][self.scorer, bp[0][-2],'y' ] = bp[-1][1]",0
Windows compatible,0
Checks if zoom/pan button is ON,0
matplotlib.use('Agg'),0
Load updated lists:,0
cfg['video_sets'][vn],0
"labeling_toolbox.show(config,Screens,scale_w,scale_h, winHack, img_scale)",0
plotting parameters:,0
folders = [Path(config).parent / 'labeled-data' /Path(i) for i in video_names],0
Let's check if the code is *not* run on windows (Source: #https://stackoverflow.com/questions/1325581/how-do-i-check-if-im-running-on-windows-in-python),0
but the paths are in windows format...,0
Loading metadata from config file:,0
Create path for training sets & store data there,0
set model type. we will allow more in the future.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make matlab train file!,0
load image to get dimensions:,0
"print ""Grayscale!""",0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
matplotlib.use('TkAgg') #already set..,0
from skimage.util import img_as_ubyte,0
import math,0
"print(""Scaled GUI width"", self.gui_width, ""and height"", self.gui_height)",0
Select ROI of interest by adjusting values in myconfig.py,0
time = start,0
set image size:,0
"self.ax1f1.set_title(str(str(self.currFrame)+""/""+str(self.numberFrames) +"" ""+ self.filename))",0
self.canvas.Destroy(),0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
restore the background region,0
Check for variable correctness,0
"plt.close(""all"")",0
indexlength = int(np.ceil(np.log10(clip.duration * clip.fps))),0
Moviepy:,0
Display the image,0
Create a Rectangle patch,0
Add the patch to the Axes,0
"saveimg = str(Path(config).parents[0] / Path('labeled-data','IsCroppingOK_'+fname.stem +"".png""))",0
"io.imsave(saveimg, image)",0
store full frame from random location (good for augmentation),0
"clip=clip.crop(y1 = int(coords[2]),y2 = int(coords[3]),x1 = int(coords[0]), x2 = int(coords[1]))",0
close video.,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
#################################################,0
Datafolder,0
#################################################,0
checks if input is a directory,0
looping over videos,0
Attempt to load data...,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
Attempt to load data...,0
Parameters for augmentation with regard to cropping,0
"limit width  [-leftwidth*u-100,100+u*rightwidth] x [-bottomwith*u-100,100+u*topwidth] where u is always a (different) random number in unit interval",0
these will be updated by trainingsetmanipulation.py in the future,0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
Dependencies for anaysis,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
The next part of the code depends upon which tensorflow version you have.,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"Xstart=int(np.max([0,int(Xlabel)-widthback]))",0
"Xstop=int(np.min([np.shape(im)[1]-1,int(Xlabel)+widthforward]))",0
"Ystart=int(np.max([0,int(Ylabel)-hdown]))",0
"Ystop=int(np.min([np.shape(im)[0]-1,int(Ylabel)+hup]))",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Importing the toolbox (takes several seconds),0
Loading example data set,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
for windows:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://stackoverflow.com/questions/39590187/in-requirements-txt-what-does-tilde-equals-mean,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
Supress tensorflow warning messages,0
Direct import for convenience,0
from deeplabcut import create_project,0
Direct import for convenience,0
Read the config file,0
get vcap property,0
Create project and sub-directories,0
"Import all videos in a folder or if just one video withouth [] passed, then make it a list.",0
there are two cases:,0
https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
try:,0
"#shutil.copy(src,dst)",0
except OSError or TypeError: #https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
"shutil.copy(os.fspath(src),os.fspath(dst))",0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
Configuration file templates,0
Write dictionary to yaml  config file,0
from skimage import io,0
import matplotlib as mpl,0
mpl.use('TkAgg'),0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
minic small screen:,0
"displaysize = (400, 400)",0
"Note, if the variable Screens = 2, it assumes two screens in landscape next to eachother! If you use a different configuration, consider changing displaysize to your known display size. see troubleshooting for more information https://github.com/AlexEMG/DeepLabCut/wiki/Troubleshooting-Tips.",0
"On Windows, there can be a issue with the sizing on start, so you can scale it down then resize on your screen. Namely, set winHack=.5 and this solves this issue. Thanks to Federico Claudi for troubleshooting this with us!",1
"print(""Scaled GUI width"", self.gui_width, ""and height"", self.gui_height)",0
Add SplitterWindow panels top for figure and bottom for buttons,0
"self.top_split = wx.Panel(self.split_win, style=wx.SUNKEN_BORDER)",0
Add Buttons to the bottom_split window and bind them to plot functions,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
Reading images,0
Plotting,0
"imagename1 = os.path.join(self.dir,self.index[self.iter])",0
Checks for the last image and disables the Next button,0
read the image,0
"imagename1 = os.path.join(self.dir,self.index[self.iter])",0
Plotting,0
"imagename1 = os.path.join(self.dir,self.index[self.iter])",0
im=io.imread(imagename1),0
Plotting,0
self.cb.SetValue(False),0
Checks for the first image and disables the Previous button,0
Reading Image,0
"imagename1 = os.path.join(self.dir,""file%04d.png"" % self.index[self.iter])",0
"imagename1 = os.path.join(self.dir,self.index[self.iter])",0
im=io.imread(imagename1),0
Plotting,0
"plt.tight_layout(rect=[0, 0.1, 1, 0.95])",0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
"self.canvas = FigureCanvas(self, -1, self.fig1)",0
small hack in case there are any 0 intensity images!,1
"cbar.set_ticks(range(12,np.max(im),8))",0
Calling auxfun_drag class for moving points around,0
###########################################################################,0
Class for MatPlotLib Panel,0
###########################################################################,0
restore the background region,0
extract min and max index based on start stop interval.,0
figure out body part list:,0
all indices between start and stop with jump larger than epsilon (leading up to this point!),0
deviation_dataname = str(Path(videofolder)/Path(dataname)),0
Calculate deviatons for video,0
Some heuristics for extracting frames based on distance:,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"meanx,CIx=meanx.values,CIx.values",0
"meany,CIy=meany.values,CIy.values",0
data.to_csv(dataname.split('.h5')[0]+'filtered.csv'),0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
close videos,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
###################################################,0
Dependencies,0
###################################################,0
import sys,0
"rr, cc = circle_perimeter(yc,xc,radius)",0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
print(metadata),0
Loading cropping data used during analysis,0
"CreateVideoSlow(clip,Dataframe,tmpfolder,cfg[""dotsize""],cfg[""colormap""],cfg[""alphavalue""],cfg[""pcutoff""],cfg[""cropping""],cfg[""x1""],cfg[""x2""],cfg[""y1""],cfg[""y2""],delete,DLCscorer,bodyparts)",0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
###################################################,0
Dependencies,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
"fig.colorbar(im, ax=ax)",0
vname = video.split('.')[0],0
#################################################,0
Looping analysis over video,0
#################################################,0
checks if input is a directory,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"dlc_cfg = read_config(os.path.join(modelfolder,'pose_cfg.yaml'))",0
"dlc_cfg['init_weights'] = os.path.join(modelfolder , 'train', Snapshots[snapshotindex])",0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
minic small screen:,0
"displaysize = (400, 400)",0
"Note, if the variable Screens = 2, it assumes two screens in landscape next to eachother! If you use a different configuration, consider changing displaysize to your known display size. see troubleshooting for more information https://github.com/AlexEMG/DeepLabCut/wiki/Troubleshooting-Tips.",0
"On Windows, there can be a issue with the sizing on start, so you can scale it down then resize on your screen. Namely, set winHack=.5 and this solves this issue. Thanks to Federico Claudi for troubleshooting this with us!",1
"print(""Scaled GUI width"", self.gui_width, ""and height"", self.gui_height)",0
add buttons for  zoom,0
"radio buttons position: (1250, 65)",0
for btn in buttons_list:,0
"btn.SetBackgroundColour((160, 160, 160))",0
Define variables,0
BUTTONS FUNCTIONS,0
fallback: warn user,0
self.relativeimagenames=self.index ##[n.split(self.project_path+'/')[1] for n in self.index],0
self.relativeimagenames=[n.split(self.project_path+'/')[1] for n in self.index],0
checks for unique bodyparts,0
"frame = pd.DataFrame(a, columns = index, index = self.index)",0
Checks for the last image and disables the Next button + diesbt load the next if RIGHT arrow key pressed,0
Refreshing the button counter,0
read the image,0
Plotting,0
Recreate toolbar for zooming,0
Windows compatible,0
color = self.colormap(normalize(col)),0
"self.toolbar.set_active([0,1])",0
This way of adding to sizer allows resizing,0
Best to allow the toolbar to resize!,0
"sizer.Add(self.toolbar, 0, wx.GROW)",0
You will need to override GetToolBar if you are using an,0
unmanaged toolbar in your frame,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
uses openCV,0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
"image=img_as_ubyte(cv2.resize(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB),None,fx=ratio,fy=ratio))",0
cap.release() >> still used in frame_extraction!,0
matplotlib.use('Agg'),0
Load updated lists:,0
cfg['video_sets'][vn],0
plotting parameters:,0
folders = [Path(config).parent / 'labeled-data' /Path(i) for i in video_names],0
Loading metadata from config file:,0
Create path for training sets & store data there,0
set model type. we will allow more in the future.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make matlab train file!,0
load image to get dimensions:,0
"print ""Grayscale!""",0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
matplotlib.use('TkAgg') #already set..,0
from skimage.util import img_as_ubyte,0
import math,0
"print(""Scaled GUI width"", self.gui_width, ""and height"", self.gui_height)",0
Select ROI of interest by adjusting values in myconfig.py,0
time = start,0
set image size:,0
"self.ax1f1.set_title(str(str(self.currFrame)+""/""+str(self.numberFrames) +"" ""+ self.filename))",0
self.canvas.Destroy(),0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
self.Button1.Enable(True),0
self.Button2.Enable(False),0
self.Button4.Enable(False),0
else:,0
self.Close(True),0
self.press = None,0
DraggablePoint.lock = None,0
self.point.set_animated(False),0
self.background = None,0
"self.final_point = (np.nan,np.nan)",0
self.coords.append(self.final_point),0
self.point.figure.canvas.draw(),0
"self.point.center = (np.nan,np.nan)",0
restore the background region,0
Check for variable correctness,0
"plt.close(""all"")",0
indexlength = int(np.ceil(np.log10(clip.duration * clip.fps))),0
Moviepy:,0
Display the image,0
Create a Rectangle patch,0
Add the patch to the Axes,0
"saveimg = str(Path(config).parents[0] / Path('labeled-data','IsCroppingOK_'+fname.stem +"".png""))",0
"io.imsave(saveimg, image)",0
store full frame from random location (good for augmentation),0
"clip=clip.crop(y1 = int(coords[2]),y2 = int(coords[3]),x1 = int(coords[0]), x2 = int(coords[1]))",0
close video.,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
#################################################,0
Datafolder,0
#################################################,0
checks if input is a directory,0
looping over videos,0
Attempt to load data...,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
#################################################,0
Loading the images,0
#################################################,0
checks if input is a directory,0
Attempt to load data...,0
Parameters for augmentation with regard to cropping,0
"limit width  [-leftwidth*u-100,100+u*rightwidth] x [-bottomwith*u-100,100+u*topwidth] where u is always a (different) random number in unit interval",0
these will be updated by trainingsetmanipulation.py in the future,0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
Dependencies for anaysis,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
The next part of the code depends upon which tensorflow version you have.,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"Xstart=int(np.max([0,int(Xlabel)-widthback]))",0
"Xstop=int(np.min([np.shape(im)[1]-1,int(Xlabel)+widthforward]))",0
"Ystart=int(np.max([0,int(Ylabel)-hdown]))",0
"Ystop=int(np.min([np.shape(im)[0]-1,int(Ylabel)+hup]))",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Importing the toolbox (takes several seconds),0
Loading example data set,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"this is much easier now: deeplabcut.train_network(path_config_file,gputouse=0,max_snapshots_to_keep=None,saveiters=1)",0
Make super short video (so the analysis is quick!),0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
for windows:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://stackoverflow.com/questions/39590187/in-requirements-txt-what-does-tilde-equals-mean,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
Supress tensorflow warning messages,0
Direct import for convenience,0
from deeplabcut import create_project,0
Direct import for convenience,0
Read the config file,0
get vcap property,0
Create project and sub-directories,0
"Import all videos in a folder or if just one video withouth [] passed, then make it a list.",0
there are two cases:,0
https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
try:,0
"#shutil.copy(src,dst)",0
except OSError or TypeError: #https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
"shutil.copy(os.fspath(src),os.fspath(dst))",0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
Configuration file templates,0
Write dictionary to yaml  config file,0
from skimage import io,0
import matplotlib as mpl,0
mpl.use('TkAgg'),0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
minic small screen:,0
"displaysize = (400, 400)",0
"Note, if the variable Screens = 2, it assumes two screens in landscape next to eachother! If you use a different configuration, consider changing displaysize to your known display size. see troubleshooting for more information https://github.com/AlexEMG/DeepLabCut/wiki/Troubleshooting-Tips.",0
"On Windows, there can be a issue with the sizing on start, so you can scale it down then resize on your screen. Namely, set winHack=.5 and this solves this issue. Thanks to Federico Claudi for troubleshooting this with us!",1
"print(""Scaled GUI width"", self.gui_width, ""and height"", self.gui_height)",0
Add SplitterWindow panels top for figure and bottom for buttons,0
"self.top_split = wx.Panel(self.split_win, style=wx.SUNKEN_BORDER)",0
Add Buttons to the bottom_split window and bind them to plot functions,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
Reading images,0
Plotting,0
"imagename1 = os.path.join(self.dir,self.index[self.iter])",0
Checks for the last image and disables the Next button,0
read the image,0
"imagename1 = os.path.join(self.dir,self.index[self.iter])",0
Plotting,0
"imagename1 = os.path.join(self.dir,self.index[self.iter])",0
im=io.imread(imagename1),0
Plotting,0
self.cb.SetValue(False),0
Checks for the first image and disables the Previous button,0
Reading Image,0
"imagename1 = os.path.join(self.dir,""file%04d.png"" % self.index[self.iter])",0
"imagename1 = os.path.join(self.dir,self.index[self.iter])",0
im=io.imread(imagename1),0
Plotting,0
"plt.tight_layout(rect=[0, 0.1, 1, 0.95])",0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
"self.canvas = FigureCanvas(self, -1, self.fig1)",0
small hack in case there are any 0 intensity images!,1
"cbar.set_ticks(range(12,np.max(im),8))",0
Calling auxfun_drag class for moving points around,0
###########################################################################,0
Class for MatPlotLib Panel,0
###########################################################################,0
restore the background region,0
extract min and max index based on start stop interval.,0
figure out body part list:,0
all indices between start and stop with jump larger than epsilon (leading up to this point!),0
deviation_dataname = str(Path(videofolder)/Path(dataname)),0
Calculate deviatons for video,0
Some heuristics for extracting frames based on distance:,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"meanx,CIx=meanx.values,CIx.values",0
"meany,CIy=meany.values,CIy.values",0
data.to_csv(dataname.split('.h5')[0]+'filtered.csv'),0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
###################################################,0
Dependencies,0
###################################################,0
import sys,0
"rr, cc = circle_perimeter(yc,xc,radius)",0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
print(metadata),0
Loading cropping data used during analysis,0
"CreateVideoSlow(clip,Dataframe,tmpfolder,cfg[""dotsize""],cfg[""colormap""],cfg[""alphavalue""],cfg[""pcutoff""],cfg[""cropping""],cfg[""x1""],cfg[""x2""],cfg[""y1""],cfg[""y2""],delete,DLCscorer,bodyparts)",0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
###################################################,0
Dependencies,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
"fig.colorbar(im, ax=ax)",0
vname = video.split('.')[0],0
#################################################,0
Looping analysis over video,0
#################################################,0
checks if input is a directory,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"dlc_cfg = read_config(os.path.join(modelfolder,'pose_cfg.yaml'))",0
"dlc_cfg['init_weights'] = os.path.join(modelfolder , 'train', Snapshots[snapshotindex])",0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
minic small screen:,0
"displaysize = (400, 400)",0
"Note, if the variable Screens = 2, it assumes two screens in landscape next to eachother! If you use a different configuration, consider changing displaysize to your known display size. see troubleshooting for more information https://github.com/AlexEMG/DeepLabCut/wiki/Troubleshooting-Tips.",0
"On Windows, there can be a issue with the sizing on start, so you can scale it down then resize on your screen. Namely, set winHack=.5 and this solves this issue. Thanks to Federico Claudi for troubleshooting this with us!",1
"print(""Scaled GUI width"", self.gui_width, ""and height"", self.gui_height)",0
add buttons for  zoom,0
"radio buttons position: (1250, 65)",0
for btn in buttons_list:,0
"btn.SetBackgroundColour((160, 160, 160))",0
Define variables,0
BUTTONS FUNCTIONS,0
fallback: warn user,0
self.relativeimagenames=self.index ##[n.split(self.project_path+'/')[1] for n in self.index],0
self.relativeimagenames=[n.split(self.project_path+'/')[1] for n in self.index],0
checks for unique bodyparts,0
"frame = pd.DataFrame(a, columns = index, index = self.index)",0
Checks for the last image and disables the Next button + diesbt load the next if RIGHT arrow key pressed,0
Refreshing the button counter,0
read the image,0
Plotting,0
Recreate toolbar for zooming,0
Windows compatible,0
color = self.colormap(normalize(col)),0
"self.toolbar.set_active([0,1])",0
This way of adding to sizer allows resizing,0
Best to allow the toolbar to resize!,0
"sizer.Add(self.toolbar, 0, wx.GROW)",0
You will need to override GetToolBar if you are using an,0
unmanaged toolbar in your frame,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
matplotlib.use('Agg'),0
Load updated lists:,0
cfg['video_sets'][vn],0
plotting parameters:,0
folders = [Path(config).parent / 'labeled-data' /Path(i) for i in video_names],0
Loading metadata from config file:,0
Create path for training sets & store data there,0
set model type. we will allow more in the future.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make matlab train file!,0
load image to get dimensions:,0
"print ""Grayscale!""",0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
matplotlib.use('TkAgg') #already set..,0
from skimage.util import img_as_ubyte,0
import math,0
"print(""Scaled GUI width"", self.gui_width, ""and height"", self.gui_height)",0
Select ROI of interest by adjusting values in myconfig.py,0
time = start,0
set image size:,0
"self.ax1f1.set_title(str(str(self.currFrame)+""/""+str(self.numberFrames) +"" ""+ self.filename))",0
self.canvas.Destroy(),0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
self.Button1.Enable(True),0
self.Button2.Enable(False),0
self.Button4.Enable(False),0
else:,0
self.Close(True),0
self.press = None,0
DraggablePoint.lock = None,0
self.point.set_animated(False),0
self.background = None,0
"self.final_point = (np.nan,np.nan)",0
self.coords.append(self.final_point),0
self.point.figure.canvas.draw(),0
"self.point.center = (np.nan,np.nan)",0
restore the background region,0
Check for variable correctness,0
update to openCV,0
Display the image,0
Create a Rectangle patch,0
Add the patch to the Axes,0
"saveimg = str(Path(config).parents[0] / Path('labeled-data','IsCroppingOK_'+fname.stem +"".png""))",0
"io.imsave(saveimg, image)",0
store full frame (good for augmentation),0
crop and move on with extraction of frames:,0
crop and move on with extraction of frames:,0
close video.,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
#################################################,0
Datafolder,0
#################################################,0
checks if input is a directory,0
looping over videos,0
from moviepy.editor import VideoFileClip,0
videotype = Path(video).suffix,0
Attempt to load data...,0
Parameters for augmentation with regard to cropping,0
"limit width  [-leftwidth*u-100,100+u*rightwidth] x [-bottomwith*u-100,100+u*topwidth] where u is always a (different) random number in unit interval",0
these will be updated by trainingsetmanipulation.py in the future,0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
Dependencies for anaysis,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
The next part of the code depends upon which tensorflow version you have.,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"Xstart=int(np.max([0,int(Xlabel)-widthback]))",0
"Xstop=int(np.min([np.shape(im)[1]-1,int(Xlabel)+widthforward]))",0
"Ystart=int(np.max([0,int(Ylabel)-hdown]))",0
"Ystop=int(np.min([np.shape(im)[0]-1,int(Ylabel)+hup]))",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Importing the toolbox (takes several seconds),0
Loading example data set,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"this is much easier now: deeplabcut.train_network(path_config_file,gputouse=0,max_snapshots_to_keep=None,saveiters=1)",0
Make super short video (so the analysis is quick!),0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
for windows:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://stackoverflow.com/questions/39590187/in-requirements-txt-what-does-tilde-equals-mean,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
Supress tensorflow warning messages,0
Direct import for convenience,0
from deeplabcut import create_project,0
Direct import for convenience,0
Read the config file,0
get vcap property,0
Create project and sub-directories,0
"Import all videos in a folder or if just one video withouth [] passed, then make it a list.",0
there are two cases:,0
https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
try:,0
"#shutil.copy(src,dst)",0
except OSError or TypeError: #https://github.com/AlexEMG/DeepLabCut/issues/105 (for windows),0
"shutil.copy(os.fspath(src),os.fspath(dst))",0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
Configuration file templates,0
Write dictionary to yaml  config file,0
from skimage import io,0
import matplotlib as mpl,0
mpl.use('TkAgg'),0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Add SplitterWindow panels top for figure and bottom for buttons,0
"self.top_split = wx.Panel(self.split_win, style=wx.SUNKEN_BORDER)",0
Add Buttons to the bottom_split window and bind them to plot functions,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
Reading images,0
Plotting,0
Checks for the last image and disables the Next button,0
read the image,0
"imagename1 = os.path.join(self.dir,self.index[self.iter])",0
Plotting,0
"imagename1 = os.path.join(self.dir,self.index[self.iter])",0
im=io.imread(imagename1),0
Plotting,0
self.cb.SetValue(False),0
Checks for the first image and disables the Previous button,0
Reading Image,0
"imagename1 = os.path.join(self.dir,""file%04d.png"" % self.index[self.iter])",0
"imagename1 = os.path.join(self.dir,self.index[self.iter])",0
im=io.imread(imagename1),0
Plotting,0
"plt.tight_layout(rect=[0, 0.1, 1, 0.95])",0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
"self.canvas = FigureCanvas(self, -1, self.fig1)",0
small hack in case there are any 0 intensity images!,1
"cbar.set_ticks(range(12,np.max(im),8))",0
Calling auxfun_drag class for moving points around,0
###########################################################################,0
Class for MatPlotLib Panel,0
###########################################################################,0
restore the background region,0
extract min and max index based on start stop interval.,0
figure out body part list:,0
all indices between start and stop with jump larger than epsilon (leading up to this point!),0
deviation_dataname = str(Path(videofolder)/Path(dataname)),0
Calculate deviatons for video,0
Some heuristics for extracting frames based on distance:,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"meanx,CIx=meanx.values,CIx.values",0
"meany,CIy=meany.values,CIy.values",0
data.to_csv(dataname.split('.h5')[0]+'filtered.csv'),0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
###################################################,0
Dependencies,0
###################################################,0
import sys,0
"rr, cc = circle_perimeter(yc,xc,radius)",0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
print(metadata),0
Loading cropping data used during analysis,0
"CreateVideoSlow(clip,Dataframe,tmpfolder,cfg[""dotsize""],cfg[""colormap""],cfg[""alphavalue""],cfg[""pcutoff""],cfg[""cropping""],cfg[""x1""],cfg[""x2""],cfg[""y1""],cfg[""y2""],delete,DLCscorer,bodyparts)",0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
###################################################,0
Dependencies,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
"fig.colorbar(im, ax=ax)",0
vname = video.split('.')[0],0
#################################################,0
Looping analysis over video,0
#################################################,0
checks if input is a directory,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"dlc_cfg = read_config(os.path.join(modelfolder,'pose_cfg.yaml'))",0
"dlc_cfg['init_weights'] = os.path.join(modelfolder , 'train', Snapshots[snapshotindex])",0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
minic small screen:,0
"displaysize = (400, 400)",0
"Note, if the variable Screens = 2, it assumes two screens in landscape next to eachother! If you use a different configuration, consider changing displaysize to your known display size. see troubleshooting for more information https://github.com/AlexEMG/DeepLabCut/wiki/Troubleshooting-Tips.",0
"On Windows, there can be a issue with the sizing on start, so you can scale it down then resize on your screen. Namely, set winHack=.5 and this solves this issue. Thanks to Federico Claudi for troubleshooting this with us!",1
"print(""Scaled GUI width"", self.gui_width, ""and height"", self.gui_height)",0
add buttons for  zoom,0
"radio buttons position: (1250, 65)",0
for btn in buttons_list:,0
"btn.SetBackgroundColour((160, 160, 160))",0
"btn.SetForegroundColour((0, 0, 0))",0
Define variables,0
BUTTONS FUNCTIONS,0
fallback: warn user,0
checks for unique bodyparts,0
"frame = pd.DataFrame(a, columns = index, index = self.index)",0
Checks for the last image and disables the Next button + diesbt load the next if RIGHT arrow key pressed,0
Refreshing the button counter,0
read the image,0
Plotting,0
Recreate toolbar for zooming,0
Windows compatible,0
color = self.colormap(normalize(col)),0
"self.toolbar.set_active([0,1])",0
This way of adding to sizer allows resizing,0
Best to allow the toolbar to resize!,0
"sizer.Add(self.toolbar, 0, wx.GROW)",0
You will need to override GetToolBar if you are using an,0
unmanaged toolbar in your frame,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
matplotlib.use('Agg'),0
plotting parameters:,0
folders = [Path(config).parent / 'labeled-data' /Path(i) for i in video_names],0
Loading metadata from config file:,0
Create path for training sets & store data there,0
set model type. we will allow more in the future.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make matlab train file!,0
load image to get dimensions:,0
"print ""Grayscale!""",0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
matplotlib.use('TkAgg') #already set..,0
from skimage.util import img_as_ubyte,0
import math,0
"print(""Scaled GUI width"", self.gui_width, ""and height"", self.gui_height)",0
Select ROI of interest by adjusting values in myconfig.py,0
time = start,0
set image size:,0
"self.ax1f1.set_title(str(str(self.currFrame)+""/""+str(self.numberFrames) +"" ""+ self.filename))",0
self.canvas.Destroy(),0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
self.Button1.Enable(True),0
self.Button2.Enable(False),0
self.Button4.Enable(False),0
else:,0
self.Close(True),0
self.press = None,0
DraggablePoint.lock = None,0
self.point.set_animated(False),0
self.background = None,0
"self.final_point = (np.nan,np.nan)",0
self.coords.append(self.final_point),0
self.point.figure.canvas.draw(),0
"self.point.center = (np.nan,np.nan)",0
restore the background region,0
Check for variable correctness,0
update to openCV,0
Display the image,0
Create a Rectangle patch,0
Add the patch to the Axes,0
"saveimg = str(Path(config).parents[0] / Path('labeled-data','IsCroppingOK_'+fname.stem +"".png""))",0
"io.imsave(saveimg, image)",0
store full frame (good for augmentation),0
crop and move on with extraction of frames:,0
crop and move on with extraction of frames:,0
close video.,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
#################################################,0
Datafolder,0
#################################################,0
checks if input is a directory,0
looping over videos,0
from moviepy.editor import VideoFileClip,0
videotype = Path(video).suffix,0
Attempt to load data...,0
Parameters for augmentation with regard to cropping,0
"limit width  [-leftwidth*u-100,100+u*rightwidth] x [-bottomwith*u-100,100+u*topwidth] where u is always a (different) random number in unit interval",0
these will be updated by trainingsetmanipulation.py in the future,0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
Dependencies for anaysis,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
Restore variables from disk.,0
"display_iters = max(1,int(displayiters))",0
Save snapshot,0
return to original path.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
The next part of the code depends upon which tensorflow version you have.,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"Xstart=int(np.max([0,int(Xlabel)-widthback]))",0
"Xstop=int(np.min([np.shape(im)[1]-1,int(Xlabel)+widthforward]))",0
"Ystart=int(np.max([0,int(Ylabel)-hdown]))",0
"Ystop=int(np.min([np.shape(im)[0]-1,int(Ylabel)+hup]))",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Importing the toolbox (takes several seconds),0
Loading example data set,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"this is much easier now: deeplabcut.train_network(path_config_file,gputouse=0,max_snapshots_to_keep=None,saveiters=1)",0
Make super short video (so the analysis is quick!),0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
for windows:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
https://stackoverflow.com/questions/39590187/in-requirements-txt-what-does-tilde-equals-mean,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
Supress tensorflow warning messages,0
Direct import for convenience,0
from deeplabcut import create_project,0
Direct import for convenience,0
Read the config file,0
get vcap property,0
Create project and sub-directories,0
"Import all videos in a folder or if just one video withouth [] passed, then make it a list.",0
there are two cases:,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
Configuration file templates,0
Write dictionary to yaml  config file,0
from skimage import io,0
import matplotlib as mpl,0
mpl.use('TkAgg'),0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Add SplitterWindow panels top for figure and bottom for buttons,0
"self.top_split = wx.Panel(self.split_win, style=wx.SUNKEN_BORDER)",0
Add Buttons to the bottom_split window and bind them to plot functions,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
Reading images,0
Plotting,0
Checks for the last image and disables the Next button,0
read the image,0
"imagename1 = os.path.join(self.dir,self.index[self.iter])",0
Plotting,0
"imagename1 = os.path.join(self.dir,self.index[self.iter])",0
im=io.imread(imagename1),0
Plotting,0
self.cb.SetValue(False),0
Checks for the first image and disables the Previous button,0
Reading Image,0
"imagename1 = os.path.join(self.dir,""file%04d.png"" % self.index[self.iter])",0
"imagename1 = os.path.join(self.dir,self.index[self.iter])",0
im=io.imread(imagename1),0
Plotting,0
"plt.tight_layout(rect=[0, 0.1, 1, 0.95])",0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
"self.canvas = FigureCanvas(self, -1, self.fig1)",0
small hack in case there are any 0 intensity images!,1
"cbar.set_ticks(range(12,np.max(im),8))",0
Calling auxfun_drag class for moving points around,0
###########################################################################,0
Class for MatPlotLib Panel,0
###########################################################################,0
restore the background region,0
extract min and max index based on start stop interval.,0
figure out body part list:,0
all indices between start and stop with jump larger than epsilon (leading up to this point!),0
deviation_dataname = str(Path(videofolder)/Path(dataname)),0
Calculate deviatons for video,0
Some heuristics for extracting frames based on distance:,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"meanx,CIx=meanx.values,CIx.values",0
"meany,CIy=meany.values,CIy.values",0
data.to_csv(dataname.split('.h5')[0]+'filtered.csv'),0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
###################################################,0
Dependencies,0
###################################################,0
import sys,0
"rr, cc = circle_perimeter(yc,xc,radius)",0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
print(metadata),0
Loading cropping data used during analysis,0
"CreateVideoSlow(clip,Dataframe,tmpfolder,cfg[""dotsize""],cfg[""colormap""],cfg[""alphavalue""],cfg[""pcutoff""],cfg[""cropping""],cfg[""x1""],cfg[""x2""],cfg[""y1""],cfg[""y2""],delete,DLCscorer,bodyparts)",0
"return cv2.cvtColor(np.flip(self.vid.read()[1],2), cv2.COLOR_BGR2RGB)",0
###################################################,0
Dependencies,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
"fig.colorbar(im, ax=ax)",0
vname = video.split('.')[0],0
#################################################,0
Looping analysis over video,0
#################################################,0
checks if input is a directory,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"dlc_cfg = read_config(os.path.join(modelfolder,'pose_cfg.yaml'))",0
"dlc_cfg['init_weights'] = os.path.join(modelfolder , 'train', Snapshots[snapshotindex])",0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Add SplitterWindow panels top for figure and bottom for buttons,0
"self.top_split = wx.Panel(self.split_win, style=wx.SUNKEN_BORDER)",0
"self.top_split.SetBackgroundColour((100, 100, 100))",0
"self.bottom_split.SetBackgroundColour((80, 80, 80))",0
Add Buttons to the bottom_split window and bind them to plot functions,0
add buttons for  zoom,0
"radio buttons position: (1250, 65)",0
for btn in buttons_list:,0
"btn.SetBackgroundColour((160, 160, 160))",0
"btn.SetForegroundColour((0, 0, 0))",0
Define variables,0
BUTTONS FUNCTIONS,0
fallback: warn user,0
checks for unique bodyparts,0
"frame = pd.DataFrame(a, columns = index, index = self.index)",0
Checks for the last image and disables the Next button + diesbt load the next if RIGHT arrow key pressed,0
Refreshing the button counter,0
read the image,0
Plotting,0
Recreate toolbar for zooming,0
"self.dataFrame.loc[self.index[self.iter]][self.scorer, bp[0][-2],'x' ] = bp[-1][0]",0
"self.dataFrame.loc[self.index[self.iter]][self.scorer, bp[0][-2],'y' ] = bp[-1][1]",0
Windows compatible,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
matplotlib.use('Agg'),0
plotting parameters:,0
folders = [Path(config).parent / 'labeled-data' /Path(i) for i in video_names],0
Loading metadata from config file:,0
Create path for training sets & store data there,0
set model type. we will allow more in the future.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make matlab train file!,0
load image to get dimensions:,0
"print ""Grayscale!""",0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
matplotlib.use('TkAgg') #already set..,0
from skimage.util import img_as_ubyte,0
import math,0
,0
Select ROI of interest by adjusting values in myconfig.py,0
time = start,0
"self.ax1f1.set_title(str(str(self.currFrame)+""/""+str(self.numberFrames) +"" ""+ self.filename))",0
self.canvas.Destroy(),0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
self.Button1.Enable(True),0
self.Button2.Enable(False),0
self.Button4.Enable(False),0
else:,0
self.Close(True),0
self.press = None,0
DraggablePoint.lock = None,0
self.point.set_animated(False),0
self.background = None,0
"self.final_point = (np.nan,np.nan)",0
self.coords.append(self.final_point),0
self.point.figure.canvas.draw(),0
"self.point.center = (np.nan,np.nan)",0
restore the background region,0
Check for variable correctness,0
update to openCV,0
Display the image,0
Create a Rectangle patch,0
Add the patch to the Axes,0
"saveimg = str(Path(config).parents[0] / Path('labeled-data','IsCroppingOK_'+fname.stem +"".png""))",0
"io.imsave(saveimg, image)",0
store full frame (good for augmentation),0
crop and move on with extraction of frames:,0
crop and move on with extraction of frames:,0
close video.,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
#################################################,0
Datafolder,0
#################################################,0
checks if input is a directory,0
looping over videos,0
from moviepy.editor import VideoFileClip,0
videotype = Path(video).suffix,0
Attempt to load data...,0
Parameters for augmentation with regard to cropping,0
"limit width  [-leftwidth*u-100,100+u*rightwidth] x [-bottomwith*u-100,100+u*topwidth] where u is always a (different) random number in unit interval",0
these will be updated by trainingsetmanipulation.py in the future,0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
Dependencies for anaysis,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
"change batch size, if it was edited during analysis!",0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
Restore variables from disk.,0
Save snapshot,0
return to original path.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
The next part of the code depends upon which tensorflow version you have.,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"Xstart=int(np.max([0,int(Xlabel)-widthback]))",0
"Xstop=int(np.min([np.shape(im)[1]-1,int(Xlabel)+widthforward]))",0
"Ystart=int(np.max([0,int(Ylabel)-hdown]))",0
"Ystop=int(np.min([np.shape(im)[0]-1,int(Ylabel)+hup]))",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Importing the toolbox (takes several seconds),0
Loading example data set,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"this is much easier now: deeplabcut.train_network(path_config_file,gputouse=0,max_snapshots_to_keep=None,saveiters=1)",0
Make super short video (so the analysis is quick!),0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
for windows:,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
@click.version_option(),0
##########################################################################################################################,0
"type=click.Path(exists=True, file_okay=False, resolve_path=True), default=Path.cwd(),",0
help='Directory to create project in. Default is cwd().'),0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
for video in videos:,0
"predict.predict_video(config, video,**kwargs)",0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
##########################################################################################################################,0
Supress tensorflow warning messages,0
Direct import for convenience,0
from deeplabcut import create_project,0
Direct import for convenience,0
Read the config file,0
get vcap property,0
Create project and sub-directories,0
creates the symlinks of the video and puts it in the videos directory.,0
adds the video list to the config.yaml file,0
Configuration file templates,0
Write dictionary to yaml  config file,0
from skimage import io,0
import matplotlib as mpl,0
mpl.use('TkAgg'),0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Add SplitterWindow panels top for figure and bottom for buttons,0
"self.top_split = wx.Panel(self.split_win, style=wx.SUNKEN_BORDER)",0
Add Buttons to the bottom_split window and bind them to plot functions,0
###########################################################################,0
functions for button responses,0
###########################################################################,0
Reading images,0
Plotting,0
Checks for the last image and disables the Next button,0
read the image,0
"imagename1 = os.path.join(self.dir,self.index[self.iter])",0
Plotting,0
"imagename1 = os.path.join(self.dir,self.index[self.iter])",0
im=io.imread(imagename1),0
Plotting,0
self.cb.SetValue(False),0
Checks for the first image and disables the Previous button,0
Reading Image,0
"imagename1 = os.path.join(self.dir,""file%04d.png"" % self.index[self.iter])",0
"imagename1 = os.path.join(self.dir,self.index[self.iter])",0
im=io.imread(imagename1),0
Plotting,0
"plt.tight_layout(rect=[0, 0.1, 1, 0.95])",0
combine datasets Original Col. + corrected machinefiles:,0
Now drop redundant ones keeping the first one [this will make sure that the refined machine file gets preference],0
###########################################################################,0
Other functions,0
###########################################################################,0
"self.canvas = FigureCanvas(self, -1, self.fig1)",0
small hack in case there are any 0 intensity images!,1
"cbar.set_ticks(range(12,np.max(im),8))",0
Calling auxfun_drag class for moving points around,0
###########################################################################,0
Class for MatPlotLib Panel,0
###########################################################################,0
restore the background region,0
extract min and max index based on start stop interval.,0
figure out body part list:,0
all indices between start and stop with jump larger than epsilon (leading up to this point!),0
deviation_dataname = str(Path(videofolder)/Path(dataname)),0
Calculate deviatons for video,0
Some heuristics for extracting frames based on distance:,0
Now extract from those Indices!,0
Seasonal Autoregressive Integrated Moving-Average with eXogenous regressors (SARIMAX),0
see http://www.statsmodels.org/stable/statespace.html#seasonal-autoregressive-integrated-moving-average-with-exogenous-regressors-sarimax,0
SARIMAX implemetnation has better prediction models than simple ARIMAX (however we do not use the seasonal etc. parameters!),0
"Autoregressive Moving Average ARMA(p,q) Model",0
"mod = sm.tsa.ARIMA(Y, order=(ARdegree,0,MAdegree)) #order=(ARdegree,0,MAdegree)",0
"meanx,CIx=meanx.values,CIx.values",0
"meany,CIy=meany.values,CIy.values",0
data.to_csv(dataname.split('.h5')[0]+'filtered.csv'),0
Extract frames + frames with plotted labels and store them in folder (with name derived from video name) nder labeled-data,0
Extract annotations based on DeepLabCut and store in the folder (with name derived from video name) under labeled-data,0
drop duplicate labels:,0
updates iteration by 1,0
###################################################,0
Dependencies,0
###################################################,0
import sys,0
"rr, cc = circle_perimeter(yc,xc,radius)",0
scorer=np.unique(Dataframe.columns.get_level_values(0))[0],0
bodyparts2plot = list(np.unique(Dataframe.columns.get_level_values(1))),0
print(metadata),0
Loading cropping data used during analysis,0
"CreateVideoSlow(clip,Dataframe,tmpfolder,cfg[""dotsize""],cfg[""colormap""],cfg[""alphavalue""],cfg[""pcutoff""],cfg[""cropping""],cfg[""x1""],cfg[""x2""],cfg[""y1""],cfg[""y2""],delete,DLCscorer,bodyparts)",0
###################################################,0
Dependencies,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
"fig.colorbar(im, ax=ax)",0
vname = video.split('.')[0],0
#################################################,0
Looping analysis over video,0
#################################################,0
checks if input is a directory,0
Pickle the 'data' dictionary using the highest protocol available.,0
Pickle the 'labeled-data' dictionary using the highest protocol available.,0
"# Various functions to get filenames, foldernames etc. based on configuration parameters.",0
Filename for metadata and data relative to project path for corresponding parameters,0
"dlc_cfg = read_config(os.path.join(modelfolder,'pose_cfg.yaml'))",0
"dlc_cfg['init_weights'] = os.path.join(modelfolder , 'train', Snapshots[snapshotindex])",0
###########################################################################,0
Class for GUI MainFrame,0
###########################################################################,0
Add SplitterWindow panels top for figure and bottom for buttons,0
"self.top_split = wx.Panel(self.split_win, style=wx.SUNKEN_BORDER)",0
Add Buttons to the bottom_split window and bind them to plot functions,0
,0
checks for unique bodyparts,0
"frame = pd.DataFrame(a, columns = index, index = self.index)",0
Refreshing the button counter,0
Checks for the last image and disables the Next button,0
read the image,0
Plotting,0
"self.dataFrame.loc[self.index[self.iter]][self.scorer, bp[0][-2],'x' ] = bp[-1][0]",0
"self.dataFrame.loc[self.index[self.iter]][self.scorer, bp[0][-2],'y' ] = bp[-1][1]",0
Windows compatible,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
matplotlib.use('Agg'),0
plotting parameters:,0
folders = [Path(config).parent / 'labeled-data' /Path(i) for i in video_names],0
Loading metadata from config file:,0
Create path for training sets & store data there,0
set model type. we will allow more in the future.,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make matlab train file!,0
load image to get dimensions:,0
"print ""Grayscale!""",0
###############################################################################,0
Saving metadata (Pickle file),0
###############################################################################,0
###############################################################################,0
Saving data file (convert to training file for deeper cut (*.mat)),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
str(cfg['proj_path']+'/'+Path(modelfoldername) / 'test'  /  'pose_cfg.yaml'),0
matplotlib.use('TkAgg') #already set..,0
from skimage.util import img_as_ubyte,0
import math,0
,0
Select ROI of interest by adjusting values in myconfig.py,0
time = start,0
"self.ax1f1.set_title(str(str(self.currFrame)+""/""+str(self.numberFrames) +"" ""+ self.filename))",0
self.canvas.Destroy(),0
self.likelihood = likelihood,0
"self.coords = [0,0]",0
self.Button1.Enable(True),0
self.Button2.Enable(False),0
self.Button4.Enable(False),0
else:,0
self.Close(True),0
self.press = None,0
DraggablePoint.lock = None,0
self.point.set_animated(False),0
self.background = None,0
"self.final_point = (np.nan,np.nan)",0
self.coords.append(self.final_point),0
self.point.figure.canvas.draw(),0
"self.point.center = (np.nan,np.nan)",0
restore the background region,0
Check for variable correctness,0
update to openCV,0
Display the image,0
Create a Rectangle patch,0
Add the patch to the Axes,0
"saveimg = str(Path(config).parents[0] / Path('labeled-data','IsCroppingOK_'+fname.stem +"".png""))",0
"io.imsave(saveimg, image)",0
store full frame (good for augmentation),0
crop and move on with extraction of frames:,0
close video.,0
###################################################,0
Dependencies,0
###################################################,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
import imageio,0
imageio.plugins.ffmpeg.download(),0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check if data already was generated:,0
update batchsize (based on parameters in config.yaml),0
Name for scorer:,0
#################################################,0
Datafolder,0
#################################################,0
checks if input is a directory,0
looping over videos,0
from moviepy.editor import VideoFileClip,0
videotype = Path(video).suffix,0
Attempt to load data...,0
Parameters for augmentation with regard to cropping,0
"limit width  [-leftwidth*u-100,100+u*rightwidth] x [-bottomwith*u-100,100+u*topwidth] where u is always a (different) random number in unit interval",0
these will be updated by trainingsetmanipulation.py in the future,0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Update the snapshot path to the corresponding path!,0
"the default is: ""./snapshot""",0
Dependencies for anaysis,0
tf.logging.set_verbosity(tf.logging.WARN),0
Read file path for pose_config file. >> pass it on,0
Loading human annotatated data,0
Get list of body parts to evaluate network for,0
Make folder for evaluation,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Load meta data,0
Create folder structure to store results.,0
path_train_config = modelfolder / 'train' / 'pose_cfg.yaml',0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
name for deeplabcut net (based on its parameters),0
Specifying state of model (snapshot / training state),0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results,0
print(final_result),0
returning to intial folder,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
Restore variables from disk.,0
Save snapshot,0
return to original path.,0
Read file path for pose_config file. >> pass it on,0
Set environment variables,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
The next part of the code depends upon which tensorflow version you have.,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"Xstart=int(np.max([0,int(Xlabel)-widthback]))",0
"Xstop=int(np.min([np.shape(im)[1]-1,int(Xlabel)+widthforward]))",0
"Ystart=int(np.max([0,int(Ylabel)-hdown]))",0
"Ystop=int(np.min([np.shape(im)[0]-1,int(Ylabel)+hup]))",0
Load Matlab file dataset annotation,0
print('Dataset has {} images'.format(num_images)),0
print(sample),0
make sure joint ids are 0-indexed,0
if cfg.crop:,0
crop = sample[3][0] - 1,0
"item.crop = extend_crop(crop, cfg.crop_pad, item.im_size)",0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"print(im_file, os.getcwd())",0
print(self.cfg.project_path),0
1. get center of joints,0
draw random crop dimensions & subtract joint points,0
"print(joints,j,'ahah')",0
if self.has_gt:,0
"joints[0,:, 1] -= x0",0
"joints[0,:, 2] -= y0",0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
Importing the toolbox (takes several seconds),0
Loading example data set,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
"this is much easier now: deeplabcut.train_network(path_config_file,gputouse=0,max_snapshots_to_keep=None,saveiters=1)",0
Make super short video (so the analysis is quick!),0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
"As this next step is manual, we update the labels by putting them on the diagonal (fixed for all frames)",0
Make super short video (so the analysis is quick!),0
for windows:,0
coding: utf-8,0
########################################################################################,0
This configuration file sets various parameters for running a trained model on videos!,0
,0
"First, make sure that the network performed well on the train/test data set.",0
,0
The pose output is saved as hdf file with a name containing the video name as well as the network name,0
You can also save the output as csv (see below) or in many other formats see https://github.com/AlexEMG/DeepLabCut/issues/17,0
,0
########################################################################################,0
Filename and path to behavioral video for analysis,0
"Note: under the hood there is moviepy, which can handle many types of videos:",0
See: https://zulko.github.io/moviepy/_modules/moviepy/video/io/VideoFileClip.html,0
"If you have stacks of tiffs (instead of videos) you can use ""AnalyzeABunchofPictures.py""",0
ROI dimensions / bounding box (only used if cropping == True),0
"x1,y1 indicates the top left corner and",0
"x2,y2 is the lower right corner of the cropped region.",0
########################################################################################,0
Analysis Network parameters,0
########################################################################################,0
These variables should be changed so that the right networks is loaded for analysis,0
(Typicaly just copy them over from myconfig.py),0
inferencemethod = >> set inside individual script!,0
batch based processing,0
"the ideal value strongly depends on frame sizes, see for considerations: https://www.biorxiv.org/content/early/2018/10/30/457242.article-info",0
Note the data is always saved in hdf - format which is an efficient format,0
that easily allows to load the full pandas multiarray at a later stage,0
########################################################################################,0
# For plotting (MakingLabeledVideo.py / MakingLabeledVideo_fast.py),0
########################################################################################,0
based on the labels for this network state.,0
delete individual (labeled) frames after making video? (note there could be many...),0
coding: utf-8,0
####################################################################################,0
This configuration file sets various parameters for training and evaluating DeepLabCut,0
####################################################################################,0
myconfig.py:,0
#######################################,0
Step 1: Selecting Frames from videos,0
#######################################,0
Filename and path to behavioral video:,0
Put name of video: / or 'all' to extract frames from all videos in folder.,0
filename = 'all',0
ROI dimensions / bounding box (only used if cropping == True),0
"x1,y1 indicates the top left corner and",0
"x2,y2 is the lower right corner of the cropped region.",0
"Portion of the video to sample from in step 1 (In relative terms of video length, i.e. [0,1] is the full video)",0
Number of frames to pick,0
Method to pick frames: uniform or kmeans,0
"the algorithm 'uniform' temporally uniformly sampes frames in interval (start,stop). Visual information within video is irrelevant for this method. This code is very fast and sufficient (to extract distinct frames) when behavioral videos naturally covers many states.",0
Alternatively consider using: 'kmeans',0
"This code downsamples the video. The video is extracted as a numpy array, which is then",0
clustered by kmeans whereby each frames are treated as a vector. Frames from different clusters are then selected for labeling. This,0
"procedure makes sure that the frames ""look different"", i.e. different postures etc.",0
On large videos this code is slow. Consider not extracting the frames from the whole video but rather set start and stop to a period around interesting behavior that you want DeepLabCut to resolve/analyze (i.e. a reach).,0
#######################################,0
Step 2: Converting frames to pandas array,0
#######################################,0
annotator in *.csv file (for multibodypartsfile). For single files order is irrelevant,0
"Set this true if the data was sequentially labeled and if there is one file per folder (you can set the name of this file below, i.e. multibodypartsfilename)",0
"Otherwise there should be individual files per bodypart, i.e. in our demo case hand.csv, Finger1.csv etc.",0
If true then those files will be generated from Results.txt,0
When importing the images and the labels in the csv/xls files should be in the same order!,0
During labeling in Fiji one can thus (for occluded body parts) click in the origin of the image,0
"(i.e. top left corner (close to 0,0)), these ""false"" labels will then be removed. To do so set the following variable:",0
set this to 0 if no labels should be removed!,0
If you started from already extracted frames in a different format then change the format here (for step2 to 4).,0
#######################################,0
Step 3: Check labels / makes plots,0
#######################################,0
#######################################,0
Step 4: Generate Training Files,0
#######################################,0
Userparameters for training set. Other parameters can be set in pose_cfg.yaml,0
"Which resnet to use, 101 for deeper!",0
then change net_type and init_weights in Generating_a_Training_Set/pose_cfg.yaml,0
Afterwards train your network!,0
#######################################################################################################################,0
For Evaluation your network,0
#######################################################################################################################,0
identifier of evaluation network:,0
To evaluate the last snapshot (i.e. the network that was trained longest) : -1,0
"To evaluate all models (training stages) set this to: ""all""  (as string!)",0
This cutoff will also be used in plots.,0
Note that this will be plotted for all snapshots as indicated by snapshotindex,0
Select ROI of interest by adjusting values in myconfig.py,0
###################################################,0
Creating folder with name of experiment and extract random frames,0
###################################################,0
Extract the first frame (not cropped!) - useful for data augmentation,0
Select ROI of interest by adjusting values in myconfig.py,0
####################################################################,0
First load the image and crop (if necessary)/ set checkcropping = True (in myconfig to do so),0
####################################################################,0
###################################################,0
Creating folder with name of experiment and extract random frames,0
###################################################,0
###################################################,0
Loading dependencies,0
###################################################,0
##################################################,0
Code if each bodypart has its own label file!,0
##################################################,0
############################################,0
Make sure you update the train.yaml file!,0
############################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
"Data frame to hold data of all data sets for different scorers, bodyparts and images",0
Make list of different video data sets:,0
"videos=np.sort([fn for fn in os.listdir(os.curdir) if (""avi"" in fn)])",0
sort image file names according to how they were stacked (when labeled in Fiji),0
This is important when using data combined / which runs consecutively!,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
##################################################,0
Code if all bodyparts (per folder are shared in one file),0
This code below converts it into multiple csv files per body part & folder,0
Based on an idea by @sneakers-the-rat,0
##################################################,0
"load csv, iterate over nth value in a grouping by frame, save to bodyparts files",0
##################################################,0
Code if each bodypart has its own label file!,0
##################################################,0
"Data frame to hold data of all data sets for different scorers,",0
bodyparts and images,0
Make list of different video data sets / each one has its own folder,0
"print(""found"",len(folders),len(numdistinctfolders))",0
"print(""Loading folder "", folder)",0
sort image file names according to how they were stacked,0
files=np.sort([fn for fn in os.listdir(os.curdir),0
"if (""img"" in fn and "".png"" in fn and ""_labelled"" not in fn)])",0
"Note: If your csv file is not correctly loaded, then a common error is:",0
"""AttributeError: 'DataFrame' object has no attribute 'X'"" or the corresponding error with Slice",0
Try to make sure you specify the seperator of the csv file correctly. See https://github.com/AlexEMG/DeepLabCut/issues/10 for details.,0
get rid of values that are invisible >> thus user scored in left corner!,0
"frame=pd.DataFrame(np.vstack([dframe.X,dframe.Y]).T, columns=index,index=imageaddress)",0
print(frame.head()),0
"print(""Done with folder "", folder)",0
Save data by this scorer,0
###################################################,0
Loading dependencies,0
###################################################,0
###################################################,0
"Definitions (Folders, data source and labels)",0
###################################################,0
Loading scorer's data:,0
Make that folder and put in the collecteddata (see below),0
This relative path is required due way DeeperCut is structured,0
copy images and folder structure in the folder containing,0
training data comparison,0
Filename for pickle file:,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make matlab train file!,0
load image to get dimensions:,0
"print ""Grayscale!""",0
Pickle the 'data' dictionary using the highest protocol available.,0
###############################################################################,0
Convert to idosyncratic training file for deeper cut (*.mat),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Pickle the 'data' dictionary using the highest protocol available.,0
https://stackoverflow.com/questions/800197/how-to-get-all-of-the-immediate-subdirectories-in-python,0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
"sys.path.append(subfolder + ""/pose-tensorflow/"")",0
###################################################,0
Auxiliary functions,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
###################################################,0
Loading dependencies,0
###################################################,0
loading meta data / i.e. training & test files,0
###################################################,0
Models vs. benchmark for varying training state,0
###################################################,0
only specific parts can also be compared (not all!) (in that case change which bodyparts by providing a list below),0
extract training iterations:,0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Deep-cut dependencies,0
Dependencies for anaysis,0
loading meta data / i.e. training & test files & labels,0
######################################################################,0
Load and setup CNN part detector as well as its configuration,0
######################################################################,0
Specifying state of model (snapshot / training state),0
#################################################,0
Compute predictions over images,0
#################################################,0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results:,0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Deep-cut dependencies,0
###################################################,0
Loading data and evaluating network on data,0
###################################################,0
###############################################################################,0
Check which snapshots exist for given network (with training data split).,0
###############################################################################,0
Check which snap shots are available and sort them by # iterations,0
"if not analyzed, then call auxiliary script to analyze the network:",0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
Restore variables from disk.,0
Save snapshot,0
Adapted from original predict.py by Eldar Insafutdinov's implementation of [DeeperCut](https://github.com/eldar/pose-tensorflow),0
To do faster inference on videos. See https://www.biorxiv.org/content/early/2018/10/30/457242,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
The next part of the code depends upon which tensorflow version you have.,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Load Matlab file dataset annotation,0
make sure joint ids are 0-indexed,0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Dependencies for video:,0
###################################################,0
Loading descriptors of model,0
###################################################,0
loading meta data / i.e. training & test files,0
"basefolder = os.path.join('..','pose-tensorflow','models')",0
"datafolder = os.path.join(basefolder , ""UnaugmentedDataSet_"" + Task + date)",0
"Data = pd.read_hdf(os.path.join(datafolder , 'data-' + Task , 'CollectedData_' + humanscorer + '.h5'),'df_with_missing')",0
Name for scorer based on passed on parameters from myconfig_analysis. Make sure they refer to the network of interest.,0
###################################################,0
Auxiliary function,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
one might want to adjust,0
clip.reader.skip_frames(1),0
image = img_as_ubyte(clip.get_frame(index * 1. / clip.fps)),0
image = img_as_ubyte(clip.reader.read_frame()),0
#################################################,0
Datafolder,0
#################################################,0
videofolder='../videos/' #where your folder with videos is.,0
return to start path.,0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Deep-cut dependencies,0
Dependencies for video:,0
import matplotlib.pyplot as plt,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
Check if data already was generated:,0
Name for scorer:,0
Name for scorer:,0
#################################################,0
Datafolder,0
#################################################,0
Auxiliary functions:,0
Intitalize net & change batch size,0
Change to video folder.,0
videofolder='../videos/' #where your folder with videos is.,0
Attempt to load data.,0
"clip = clip.crop(y1=y1, y2=y2, x1=x1, x2=x2)  # one might want to adjust",0
Delete clip + reader object https://github.com/Zulko/moviepy/issues/57,0
https://github.com/Zulko/moviepy/issues/518 (can slow down on Windows ...),0
reader.close(),0
del clip.reader,0
return to start path.,0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Deep-cut dependencies,0
Dependencies for video:,0
import matplotlib.pyplot as plt,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check which snap shots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
Check if data already was generated:,0
Name for scorer:,0
Name for scorer:,0
#################################################,0
Datafolder,0
#################################################,0
Folder where your tiffstacks are:,0
Attempt to load data...,0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Deep-cut dependencies,0
Dependencies for video:,0
import matplotlib.pyplot as plt,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
Check if data already was generated:,0
Name for scorer:,0
Name for scorer:,0
#################################################,0
Datafolder,0
#################################################,0
videofolder='../videos/' #where your folder with videos is.,0
Attempt to load data...,0
nframes = np.sum(1 for j in clip.iter_frames()) #this is slow (but accurate),0
this will overestimage number of frames (see https://github.com/AlexEMG/DeepLabCut/issues/9) This is especially a problem,0
for high frame rates and long durations due to rounding errors (as Rich Warren found). Later we crop the result (line 187),0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Dependencies for video:,0
###################################################,0
Loading descriptors of model,0
###################################################,0
loading meta data / i.e. training & test files,0
"basefolder = os.path.join('..','pose-tensorflow','models')",0
"datafolder = os.path.join(basefolder , ""UnaugmentedDataSet_"" + Task + date)",0
"Data = pd.read_hdf(os.path.join(datafolder , 'data-' + Task , 'CollectedData_' + humanscorer + '.h5'),'df_with_missing')",0
Name for scorer based on passed on parameters from myconfig_analysis. Make sure they refer to the network of interest.,0
###################################################,0
Auxiliary function,0
###################################################,0
"rr, cc = circle_perimeter(yc,xc,radius)",0
#################################################,0
Datafolder,0
#################################################,0
return to start path.,0
coding: utf-8,0
########################################################################################,0
This configuration file sets various parameters for running a trained model on videos!,0
,0
"First, make sure that the network performed well on the train/test data set.",0
,0
The pose output is saved as hdf file with a name containing the video name as well as the network name,0
You can also save the output as csv (see below) or in many other formats see https://github.com/AlexEMG/DeepLabCut/issues/17,0
,0
########################################################################################,0
Filename and path to behavioral video for analysis,0
"Note: under the hood there is moviepy, which can handle many types of videos:",0
See: https://zulko.github.io/moviepy/_modules/moviepy/video/io/VideoFileClip.html,0
"If you have stacks of tiffs (instead of videos) you can use ""AnalyzeABunchofPictures.py""",0
ROI dimensions / bounding box (only used if cropping == True),0
"x1,y1 indicates the top left corner and",0
"x2,y2 is the lower right corner of the cropped region.",0
########################################################################################,0
Analysis Network parameters,0
########################################################################################,0
These variables should be changed so that the right networks is loaded for analysis,0
(Typicaly just copy them over from myconfig.py),0
inferencemethod = >> set inside individual script!,0
batch based processing,0
Note the data is always saved in hdf - format which is an efficient format,0
that easily allows to load the full pandas multiarray at a later stage,0
########################################################################################,0
# For plotting (MakingLabeledVideo.py / MakingLabeledVideo_fast.py),0
########################################################################################,0
based on the labels for this network state.,0
delete individual (labeled) frames after making video? (note there could be many...),0
coding: utf-8,0
####################################################################################,0
This configuration file sets various parameters for training and evaluating DeepLabCut,0
####################################################################################,0
myconfig.py:,0
#######################################,0
Step 1: Selecting Frames from videos,0
#######################################,0
Filename and path to behavioral video:,0
Put name of video: / or 'all' to extract frames from all videos in folder.,0
filename = 'all',0
ROI dimensions / bounding box (only used if cropping == True),0
"x1,y1 indicates the top left corner and",0
"x2,y2 is the lower right corner of the cropped region.",0
"Portion of the video to sample from in step 1 (In relative terms of video length, i.e. [0,1] is the full video)",0
Number of frames to pick,0
Method to pick frames: uniform or kmeans,0
"the algorithm 'uniform' temporally uniformly sampes frames in interval (start,stop). Visual information within video is irrelevant for this method. This code is very fast and sufficient (to extract distinct frames) when behavioral videos naturally covers many states.",0
Alternatively consider using: 'kmeans',0
"This code downsamples the video. The video is extracted as a numpy array, which is then",0
clustered by kmeans whereby each frames are treated as a vector. Frames from different clusters are then selected for labeling. This,0
"procedure makes sure that the frames ""look different"", i.e. different postures etc.",0
On large videos this code is slow. Consider not extracting the frames from the whole video but rather set start and stop to a period around interesting behavior that you want DeepLabCut to resolve/analyze (i.e. a reach).,0
#######################################,0
Step 2: Converting frames to pandas array,0
#######################################,0
annotator in *.csv file (for multibodypartsfile). For single files order is irrelevant,0
"Set this true if the data was sequentially labeled and if there is one file per folder (you can set the name of this file below, i.e. multibodypartsfilename)",0
"Otherwise there should be individual files per bodypart, i.e. in our demo case hand.csv, Finger1.csv etc.",0
If true then those files will be generated from Results.txt,0
When importing the images and the labels in the csv/xls files should be in the same order!,0
During labeling in Fiji one can thus (for occluded body parts) click in the origin of the image,0
"(i.e. top left corner (close to 0,0)), these ""false"" labels will then be removed. To do so set the following variable:",0
set this to 0 if no labels should be removed!,0
If you started from already extracted frames in a different format then change the format here (for step2 to 4).,0
#######################################,0
Step 3: Check labels / makes plots,0
#######################################,0
#######################################,0
Step 4: Generate Training Files,0
#######################################,0
Userparameters for training set. Other parameters can be set in pose_cfg.yaml,0
"Which resnet to use, 101 for deeper!",0
then change net_type and init_weights in Generating_a_Training_Set/pose_cfg.yaml,0
Afterwards train your network!,0
#######################################################################################################################,0
For Evaluation your network,0
#######################################################################################################################,0
identifier of evaluation network:,0
To evaluate the last snapshot (i.e. the network that was trained longest) : -1,0
"To evaluate all models (training stages) set this to: ""all""  (as string!)",0
This cutoff will also be used in plots.,0
Note that this will be plotted for all snapshots as indicated by snapshotindex,0
Select ROI of interest by adjusting values in myconfig.py,0
###################################################,0
Creating folder with name of experiment and extract random frames,0
###################################################,0
Extract the first frame (not cropped!) - useful for data augmentation,0
Select ROI of interest by adjusting values in myconfig.py,0
####################################################################,0
First load the image and crop (if necessary)/ set checkcropping = True (in myconfig to do so),0
####################################################################,0
###################################################,0
Creating folder with name of experiment and extract random frames,0
###################################################,0
###################################################,0
Loading dependencies,0
###################################################,0
##################################################,0
Code if each bodypart has its own label file!,0
##################################################,0
############################################,0
Make sure you update the train.yaml file!,0
############################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
"Data frame to hold data of all data sets for different scorers, bodyparts and images",0
Make list of different video data sets:,0
"videos=np.sort([fn for fn in os.listdir(os.curdir) if (""avi"" in fn)])",0
sort image file names according to how they were stacked (when labeled in Fiji),0
This is important when using data combined / which runs consecutively!,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
##################################################,0
Code if all bodyparts (per folder are shared in one file),0
This code below converts it into multiple csv files per body part & folder,0
Based on an idea by @sneakers-the-rat,0
##################################################,0
"load csv, iterate over nth value in a grouping by frame, save to bodyparts files",0
##################################################,0
Code if each bodypart has its own label file!,0
##################################################,0
"Data frame to hold data of all data sets for different scorers,",0
bodyparts and images,0
Make list of different video data sets / each one has its own folder,0
"print(""found"",len(folders),len(numdistinctfolders))",0
"print(""Loading folder "", folder)",0
sort image file names according to how they were stacked,0
files=np.sort([fn for fn in os.listdir(os.curdir),0
"if (""img"" in fn and "".png"" in fn and ""_labelled"" not in fn)])",0
"Note: If your csv file is not correctly loaded, then a common error is:",0
"""AttributeError: 'DataFrame' object has no attribute 'X'"" or the corresponding error with Slice",0
Try to make sure you specify the seperator of the csv file correctly. See https://github.com/AlexEMG/DeepLabCut/issues/10 for details.,0
get rid of values that are invisible >> thus user scored in left corner!,0
"frame=pd.DataFrame(np.vstack([dframe.X,dframe.Y]).T, columns=index,index=imageaddress)",0
print(frame.head()),0
"print(""Done with folder "", folder)",0
Save data by this scorer,0
###################################################,0
Loading dependencies,0
###################################################,0
###################################################,0
"Definitions (Folders, data source and labels)",0
###################################################,0
Loading scorer's data:,0
Make that folder and put in the collecteddata (see below),0
This relative path is required due way DeeperCut is structured,0
copy images and folder structure in the folder containing,0
training data comparison,0
Filename for pickle file:,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make matlab train file!,0
load image to get dimensions:,0
"print ""Grayscale!""",0
Pickle the 'data' dictionary using the highest protocol available.,0
###############################################################################,0
Convert to idosyncratic training file for deeper cut (*.mat),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Pickle the 'data' dictionary using the highest protocol available.,0
https://stackoverflow.com/questions/800197/how-to-get-all-of-the-immediate-subdirectories-in-python,0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
"sys.path.append(subfolder + ""/pose-tensorflow/"")",0
###################################################,0
Auxiliary functions,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
###################################################,0
Loading dependencies,0
###################################################,0
loading meta data / i.e. training & test files,0
###################################################,0
Models vs. benchmark for varying training state,0
###################################################,0
only specific parts can also be compared (not all!) (in that case change which bodyparts by providing a list below),0
extract training iterations:,0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Deep-cut dependencies,0
Dependencies for anaysis,0
loading meta data / i.e. training & test files & labels,0
######################################################################,0
Load and setup CNN part detector as well as its configuration,0
######################################################################,0
Specifying state of model (snapshot / training state),0
#################################################,0
Compute predictions over images,0
#################################################,0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results:,0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Deep-cut dependencies,0
###################################################,0
Loading data and evaluating network on data,0
###################################################,0
###############################################################################,0
Check which snapshots exist for given network (with training data split).,0
###############################################################################,0
Check which snap shots are available and sort them by # iterations,0
"if not analyzed, then call auxiliary script to analyze the network:",0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
Restore variables from disk.,0
Save snapshot,0
Adapted from original predict.py by Eldar Insafutdinov's implementation of [DeeperCut](https://github.com/eldar/pose-tensorflow),0
To do faster inference on videos. See https://www.biorxiv.org/content/early/2018/10/30/457242,0
Restore variables from disk.,0
# Functions below implement are for batch sizes > 1:,0
Combine scoremat and offsets to the final pose.,0
The next part of the code depends upon which tensorflow version you have.,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Load Matlab file dataset annotation,0
make sure joint ids are 0-indexed,0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Dependencies for video:,0
###################################################,0
Loading descriptors of model,0
###################################################,0
loading meta data / i.e. training & test files,0
"basefolder = os.path.join('..','pose-tensorflow','models')",0
"datafolder = os.path.join(basefolder , ""UnaugmentedDataSet_"" + Task + date)",0
"Data = pd.read_hdf(os.path.join(datafolder , 'data-' + Task , 'CollectedData_' + humanscorer + '.h5'),'df_with_missing')",0
Name for scorer based on passed on parameters from myconfig_analysis. Make sure they refer to the network of interest.,0
###################################################,0
Auxiliary function,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
one might want to adjust,0
image = img_as_ubyte(clip.get_frame(index * 1. / clip.fps)),0
#################################################,0
Datafolder,0
#################################################,0
videofolder='../videos/' #where your folder with videos is.,0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Deep-cut dependencies,0
Dependencies for video:,0
import matplotlib.pyplot as plt,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
Check if data already was generated:,0
Name for scorer:,0
Name for scorer:,0
#################################################,0
Datafolder,0
#################################################,0
Auxiliary functions:,0
videofolder='../videos/' #where your folder with videos is.,0
Attempt to load data.,0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Deep-cut dependencies,0
Dependencies for video:,0
import matplotlib.pyplot as plt,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check which snap shots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
Check if data already was generated:,0
Name for scorer:,0
Name for scorer:,0
#################################################,0
Datafolder,0
#################################################,0
Folder where your tiffstacks are:,0
Attempt to load data...,0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Deep-cut dependencies,0
Dependencies for video:,0
import matplotlib.pyplot as plt,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
Check if data already was generated:,0
Name for scorer:,0
Name for scorer:,0
#################################################,0
Datafolder,0
#################################################,0
videofolder='../videos/' #where your folder with videos is.,0
Attempt to load data...,0
nframes = np.sum(1 for j in clip.iter_frames()) #this is slow (but accurate),0
this will overestimage number of frames (see https://github.com/AlexEMG/DeepLabCut/issues/9) This is especially a problem,0
for high frame rates and long durations due to rounding errors (as Rich Warren found). Later we crop the result (line 187),0
image = img_as_ubyte(clip.get_frame(index * 1. / fps)),0
Thanks to Rick Warren for the  following snipplet:,0
"if close to end of video, start checking whether two adjacent frames are identical",0
this should only happen when moviepy has reached the final frame,0
"if two adjacent frames are identical, terminate the loop",0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Dependencies for video:,0
###################################################,0
Loading descriptors of model,0
###################################################,0
loading meta data / i.e. training & test files,0
"basefolder = os.path.join('..','pose-tensorflow','models')",0
"datafolder = os.path.join(basefolder , ""UnaugmentedDataSet_"" + Task + date)",0
"Data = pd.read_hdf(os.path.join(datafolder , 'data-' + Task , 'CollectedData_' + humanscorer + '.h5'),'df_with_missing')",0
Name for scorer based on passed on parameters from myconfig_analysis. Make sure they refer to the network of interest.,0
###################################################,0
Auxiliary function,0
###################################################,0
"rr, cc = circle_perimeter(yc,xc,radius)",0
#################################################,0
Datafolder,0
#################################################,0
coding: utf-8,0
########################################################################################,0
This configuration file sets various parameters for running a trained model on videos!,0
,0
"First, make sure that the network performed well on the train/test data set.",0
,0
The pose output is saved as hdf file with a name containing the video name as well as the network name,0
You can also save the output as csv (see below) or in many other formats see https://github.com/AlexEMG/DeepLabCut/issues/17,0
,0
########################################################################################,0
Filename and path to behavioral video for analysis,0
"Note: under the hood there is moviepy, which can handle many types of videos:",0
See: https://zulko.github.io/moviepy/_modules/moviepy/video/io/VideoFileClip.html,0
"If you have stacks of tiffs (instead of videos) you can use ""AnalyzeABunchofPictures.py""",0
ROI dimensions / bounding box (only used if cropping == True),0
"x1,y1 indicates the top left corner and",0
"x2,y2 is the lower right corner of the cropped region.",0
########################################################################################,0
Analysis Network parameters,0
########################################################################################,0
These variables should be changed so that the right networks is loaded for analysis,0
(Typicaly just copy them over from myconfig.py),0
Note the data is always saved in hdf - format which is an efficient format,0
that easily allows to load the full pandas multiarray at a later stage,0
########################################################################################,0
# For plotting (MakingLabeledVideo.py / MakingLabeledVideo_fast.py),0
########################################################################################,0
based on the labels for this network state.,0
delete individual (labeled) frames after making video? (note there could be many...),0
coding: utf-8,0
####################################################################################,0
This configuration file sets various parameters for training and evaluating DeepLabCut,0
####################################################################################,0
myconfig.py:,0
#######################################,0
Step 1: Selecting Frames from videos,0
#######################################,0
Filename and path to behavioral video:,0
Put name of video: / or 'all' to extract frames from all videos in folder.,0
filename = 'all',0
ROI dimensions / bounding box (only used if cropping == True),0
"x1,y1 indicates the top left corner and",0
"x2,y2 is the lower right corner of the cropped region.",0
"Portion of the video to sample from in step 1 (In relative terms of video length, i.e. [0,1] is the full video)",0
Number of frames to pick,0
Method to pick frames: uniform or kmeans,0
"the algorithm 'uniform' temporally uniformly sampes frames in interval (start,stop). Visual information within video is irrelevant for this method. This code is very fast and sufficient (to extract distinct frames) when behavioral videos naturally covers many states.",0
Alternatively consider using: 'kmeans',0
"This code downsamples the video. The video is extracted as a numpy array, which is then",0
clustered by kmeans whereby each frames are treated as a vector. Frames from different clusters are then selected for labeling. This,0
"procedure makes sure that the frames ""look different"", i.e. different postures etc.",0
On large videos this code is slow. Consider not extracting the frames from the whole video but rather set start and stop to a period around interesting behavior that you want DeepLabCut to resolve/analyze (i.e. a reach).,0
#######################################,0
Step 2: Converting frames to pandas array,0
#######################################,0
annotator in *.csv file (for multibodypartsfile). For single files order is irrelevant,0
"Set this true if the data was sequentially labeled and if there is one file per folder (you can set the name of this file below, i.e. multibodypartsfilename)",0
"Otherwise there should be individual files per bodypart, i.e. in our demo case hand.csv, Finger1.csv etc.",0
If true then those files will be generated from Results.txt,0
When importing the images and the labels in the csv/xls files should be in the same order!,0
During labeling in Fiji one can thus (for occluded body parts) click in the origin of the image,0
"(i.e. top left corner (close to 0,0)), these ""false"" labels will then be removed. To do so set the following variable:",0
set this to 0 if no labels should be removed!,0
If you started from already extracted frames in a different format then change the format here (for step2 to 4).,0
#######################################,0
Step 3: Check labels / makes plots,0
#######################################,0
#######################################,0
Step 4: Generate Training Files,0
#######################################,0
Userparameters for training set. Other parameters can be set in pose_cfg.yaml,0
"Which resnet to use, 101 for deeper!",0
then change net_type and init_weights in Generating_a_Training_Set/pose_cfg.yaml,0
Afterwards train your network!,0
#######################################################################################################################,0
For Evaluation your network,0
#######################################################################################################################,0
identifier of evaluation network:,0
To evaluate the last snapshot (i.e. the network that was trained longest) : -1,0
"To evaluate all models (training stages) set this to: ""all""  (as string!)",0
This cutoff will also be used in plots.,0
Note that this will be plotted for all snapshots as indicated by snapshotindex,0
Select ROI of interest by adjusting values in myconfig.py,0
###################################################,0
Creating folder with name of experiment and extract random frames,0
###################################################,0
Extract the first frame (not cropped!) - useful for data augmentation,0
Select ROI of interest by adjusting values in myconfig.py,0
####################################################################,0
First load the image and crop (if necessary)/ set checkcropping = True (in myconfig to do so),0
####################################################################,0
###################################################,0
Creating folder with name of experiment and extract random frames,0
###################################################,0
###################################################,0
Loading dependencies,0
###################################################,0
##################################################,0
Code if each bodypart has its own label file!,0
##################################################,0
############################################,0
Make sure you update the train.yaml file!,0
############################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
"Data frame to hold data of all data sets for different scorers, bodyparts and images",0
Make list of different video data sets:,0
"videos=np.sort([fn for fn in os.listdir(os.curdir) if (""avi"" in fn)])",0
sort image file names according to how they were stacked (when labeled in Fiji),0
This is important when using data combined / which runs consecutively!,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
##################################################,0
Code if all bodyparts (per folder are shared in one file),0
This code below converts it into multiple csv files per body part & folder,0
Based on an idea by @sneakers-the-rat,0
##################################################,0
"load csv, iterate over nth value in a grouping by frame, save to bodyparts files",0
##################################################,0
Code if each bodypart has its own label file!,0
##################################################,0
"Data frame to hold data of all data sets for different scorers,",0
bodyparts and images,0
Make list of different video data sets / each one has its own folder,0
"print(""found"",len(folders),len(numdistinctfolders))",0
"print(""Loading folder "", folder)",0
sort image file names according to how they were stacked,0
files=np.sort([fn for fn in os.listdir(os.curdir),0
"if (""img"" in fn and "".png"" in fn and ""_labelled"" not in fn)])",0
"Note: If your csv file is not correctly loaded, then a common error is:",0
"""AttributeError: 'DataFrame' object has no attribute 'X'"" or the corresponding error with Slice",0
Try to make sure you specify the seperator of the csv file correctly. See https://github.com/AlexEMG/DeepLabCut/issues/10 for details.,0
get rid of values that are invisible >> thus user scored in left corner!,0
"frame=pd.DataFrame(np.vstack([dframe.X,dframe.Y]).T, columns=index,index=imageaddress)",0
print(frame.head()),0
"print(""Done with folder "", folder)",0
Save data by this scorer,0
###################################################,0
Loading dependencies,0
###################################################,0
###################################################,0
"Definitions (Folders, data source and labels)",0
###################################################,0
Loading scorer's data:,0
Make that folder and put in the collecteddata (see below),0
This relative path is required due way DeeperCut is structured,0
copy images and folder structure in the folder containing,0
training data comparison,0
Filename for pickle file:,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make matlab train file!,0
load image to get dimensions:,0
"print ""Grayscale!""",0
Pickle the 'data' dictionary using the highest protocol available.,0
###############################################################################,0
Convert to idosyncratic training file for deeper cut (*.mat),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Pickle the 'data' dictionary using the highest protocol available.,0
https://stackoverflow.com/questions/800197/how-to-get-all-of-the-immediate-subdirectories-in-python,0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
"sys.path.append(subfolder + ""/pose-tensorflow/"")",0
###################################################,0
Auxiliary functions,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
###################################################,0
Loading dependencies,0
###################################################,0
loading meta data / i.e. training & test files,0
###################################################,0
Models vs. benchmark for varying training state,0
###################################################,0
only specific parts can also be compared (not all!) (in that case change which bodyparts by providing a list below),0
extract training iterations:,0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Deep-cut dependencies,0
Dependencies for anaysis,0
loading meta data / i.e. training & test files & labels,0
######################################################################,0
Load and setup CNN part detector as well as its configuration,0
######################################################################,0
Specifying state of model (snapshot / training state),0
#################################################,0
Compute predictions over images,0
#################################################,0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results:,0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Deep-cut dependencies,0
###################################################,0
Loading data and evaluating network on data,0
###################################################,0
###############################################################################,0
Check which snapshots exist for given network (with training data split).,0
###############################################################################,0
Check which snap shots are available and sort them by # iterations,0
"if not analyzed, then call auxiliary script to analyze the network:",0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Restore variables from disk.,0
The next part of the code depends upon which tensorflow version you have.,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
Restore variables from disk.,0
Save snapshot,0
Restore variables from disk.,0
The next part of the code depends upon which tensorflow version you have.,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Load Matlab file dataset annotation,0
make sure joint ids are 0-indexed,0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Dependencies for video:,0
###################################################,0
Loading descriptors of model,0
###################################################,0
loading meta data / i.e. training & test files,0
"basefolder = os.path.join('..','pose-tensorflow','models')",0
"datafolder = os.path.join(basefolder , ""UnaugmentedDataSet_"" + Task + date)",0
"Data = pd.read_hdf(os.path.join(datafolder , 'data-' + Task , 'CollectedData_' + humanscorer + '.h5'),'df_with_missing')",0
Name for scorer based on passed on parameters from myconfig_analysis. Make sure they refer to the network of interest.,0
###################################################,0
Auxiliary function,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
one might want to adjust,0
image = img_as_ubyte(clip.get_frame(index * 1. / clip.fps)),0
#################################################,0
Datafolder,0
#################################################,0
videofolder='../videos/' #where your folder with videos is.,0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Deep-cut dependencies,0
Dependencies for video:,0
import matplotlib.pyplot as plt,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
Check if data already was generated:,0
Name for scorer:,0
Name for scorer:,0
#################################################,0
Datafolder,0
#################################################,0
videofolder='../videos/' #where your folder with videos is.,0
Attempt to load data...,0
nframes = np.sum(1 for j in clip.iter_frames()) #this is slow (but accurate),0
this will overestimage number of frames (see https://github.com/AlexEMG/DeepLabCut/issues/9) This is especially a problem,0
for high frame rates and long durations due to rounding errors (as Rich Warren found). Later we crop the result (line 187),0
image = img_as_ubyte(clip.get_frame(index * 1. / fps)),0
Thanks to Rick Warren for the  following snipplet:,0
"if close to end of video, start checking whether two adjacent frames are identical",0
this should only happen when moviepy has reached the final frame,0
"if two adjacent frames are identical, terminate the loop",0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Deep-cut dependencies,0
Dependencies for video:,0
import matplotlib.pyplot as plt,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check which snap shots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
Check if data already was generated:,0
Name for scorer:,0
Name for scorer:,0
#################################################,0
Datafolder,0
#################################################,0
Folder where your tiffstacks are:,0
Attempt to load data...,0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Dependencies for video:,0
###################################################,0
Loading descriptors of model,0
###################################################,0
loading meta data / i.e. training & test files,0
"basefolder = os.path.join('..','pose-tensorflow','models')",0
"datafolder = os.path.join(basefolder , ""UnaugmentedDataSet_"" + Task + date)",0
"Data = pd.read_hdf(os.path.join(datafolder , 'data-' + Task , 'CollectedData_' + humanscorer + '.h5'),'df_with_missing')",0
Name for scorer based on passed on parameters from myconfig_analysis. Make sure they refer to the network of interest.,0
###################################################,0
Auxiliary function,0
###################################################,0
"rr, cc = circle_perimeter(yc,xc,radius)",0
#################################################,0
Datafolder,0
#################################################,0
coding: utf-8,0
########################################################################################,0
This configuration file sets various parameters for running a trained model on videos!,0
,0
"First, make sure that the network performed well on the train/test data set.",0
,0
The pose output is saved as hdf file with a name containing the video name as well as the network name,0
You can also save the output as csv (see below) or in many other formats see https://github.com/AlexEMG/DeepLabCut/issues/17,0
,0
########################################################################################,0
Filename and path to behavioral video for analysis,0
"Note: under the hood there is moviepy, which can handle many types of videos:",0
See: https://zulko.github.io/moviepy/_modules/moviepy/video/io/VideoFileClip.html,0
"If you have stacks of tiffs (instead of videos) you can use ""AnalyzeABunchofPictures.py""",0
ROI dimensions / bounding box (only used if cropping == True),0
"x1,y1 indicates the top left corner and",0
"x2,y2 is the lower right corner of the cropped region.",0
########################################################################################,0
Analysis Network parameters,0
########################################################################################,0
These variables should be changed so that the right networks is loaded for analysis,0
(Typicaly just copy them over from myconfig.py),0
Note the data is always saved in hdf - format which is an efficient format,0
that easily allows to load the full pandas multiarray at a later stage,0
########################################################################################,0
# For plotting (MakingLabeledVideo.py / MakingLabeledVideo_fast.py),0
########################################################################################,0
based on the labels for this network state.,0
delete individual (labeled) frames after making video? (note there could be many...),0
coding: utf-8,0
####################################################################################,0
This configuration file sets various parameters for training and evaluating DeepLabCut,0
####################################################################################,0
myconfig.py:,0
#######################################,0
Step 1: Selecting Frames from videos,0
#######################################,0
Filename and path to behavioral video:,0
Put name of video: / or 'all' to extract frames from all videos in folder.,0
filename = 'all',0
ROI dimensions / bounding box (only used if cropping == True),0
"x1,y1 indicates the top left corner and",0
"x2,y2 is the lower right corner of the cropped region.",0
"Portion of the video to sample from in step 1 (In relative terms of video length, i.e. [0,1] is the full video)",0
Number of frames to pick,0
Method to pick frames: uniform or kmeans,0
"the algorithm 'uniform' temporally uniformly sampes frames in interval (start,stop). Visual information within video is irrelevant for this method. This code is very fast and sufficient (to extract distinct frames) when behavioral videos naturally covers many states.",0
Alternatively consider using: 'kmeans',0
"This code downsamples the video. The video is extracted as a numpy array, which is then",0
clustered by kmeans whereby each frames are treated as a vector. Frames from different clusters are then selected for labeling. This,0
"procedure makes sure that the frames ""look different"", i.e. different postures etc.",0
On large videos this code is slow. Consider not extracting the frames from the whole video but rather set start and stop to a period around interesting behavior that you want DeepLabCut to resolve/analyze (i.e. a reach).,0
#######################################,0
Step 2: Converting frames to pandas array,0
#######################################,0
annotator in *.csv file (for multibodypartsfile). For single files order is irrelevant,0
"Set this true if the data was sequentially labeled and if there is one file per folder (you can set the name of this file below, i.e. multibodypartsfilename)",0
"Otherwise there should be individual files per bodypart, i.e. in our demo case hand.csv, Finger1.csv etc.",0
If true then those files will be generated from Results.txt,0
When importing the images and the labels in the csv/xls files should be in the same order!,0
During labeling in Fiji one can thus (for occluded body parts) click in the origin of the image,0
"(i.e. top left corner (close to 0,0)), these ""false"" labels will then be removed. To do so set the following variable:",0
set this to 0 if no labels should be removed!,0
If you started from already extracted frames in a different format then change the format here (for step2 to 4).,0
#######################################,0
Step 3: Check labels / makes plots,0
#######################################,0
#######################################,0
Step 4: Generate Training Files,0
#######################################,0
Userparameters for training set. Other parameters can be set in pose_cfg.yaml,0
"Which resnet to use, 101 for deeper!",0
then change net_type and init_weights in Generating_a_Training_Set/pose_cfg.yaml,0
Afterwards train your network!,0
#######################################################################################################################,0
For Evaluation your network,0
#######################################################################################################################,0
identifier of evaluation network:,0
To evaluate the last snapshot (i.e. the network that was trained longest) : -1,0
"To evaluate all models (training stages) set this to: ""all""  (as string!)",0
This cutoff will also be used in plots.,0
Note that this will be plotted for all snapshots as indicated by snapshotindex,0
Select ROI of interest by adjusting values in myconfig.py,0
###################################################,0
Creating folder with name of experiment and extract random frames,0
###################################################,0
Extract the first frame (not cropped!) - useful for data augmentation,0
Select ROI of interest by adjusting values in myconfig.py,0
####################################################################,0
First load the image and crop (if necessary)/ set checkcropping = True (in myconfig to do so),0
####################################################################,0
###################################################,0
Creating folder with name of experiment and extract random frames,0
###################################################,0
###################################################,0
Loading dependencies,0
###################################################,0
##################################################,0
Code if each bodypart has its own label file!,0
##################################################,0
############################################,0
Make sure you update the train.yaml file!,0
############################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
"Data frame to hold data of all data sets for different scorers, bodyparts and images",0
Make list of different video data sets:,0
"videos=np.sort([fn for fn in os.listdir(os.curdir) if (""avi"" in fn)])",0
sort image file names according to how they were stacked (when labeled in Fiji),0
This is important when using data combined / which runs consecutively!,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
##################################################,0
Code if all bodyparts (per folder are shared in one file),0
This code below converts it into multiple csv files per body part & folder,0
Based on an idea by @sneakers-the-rat,0
##################################################,0
"load csv, iterate over nth value in a grouping by frame, save to bodyparts files",0
##################################################,0
Code if each bodypart has its own label file!,0
##################################################,0
"Data frame to hold data of all data sets for different scorers,",0
bodyparts and images,0
Make list of different video data sets / each one has its own folder,0
"print(""found"",len(folders),len(numdistinctfolders))",0
"print(""Loading folder "", folder)",0
sort image file names according to how they were stacked,0
files=np.sort([fn for fn in os.listdir(os.curdir),0
"if (""img"" in fn and "".png"" in fn and ""_labelled"" not in fn)])",0
"Note: If your csv file is not correctly loaded, then a common error is:",0
"""AttributeError: 'DataFrame' object has no attribute 'X'"" or the corresponding error with Slice",0
Try to make sure you specify the seperator of the csv file correctly. See https://github.com/AlexEMG/DeepLabCut/issues/10 for details.,0
get rid of values that are invisible >> thus user scored in left corner!,0
"frame=pd.DataFrame(np.vstack([dframe.X,dframe.Y]).T, columns=index,index=imageaddress)",0
print(frame.head()),0
"print(""Done with folder "", folder)",0
Save data by this scorer,0
###################################################,0
Loading dependencies,0
###################################################,0
###################################################,0
"Definitions (Folders, data source and labels)",0
###################################################,0
Loading scorer's data:,0
Make that folder and put in the collecteddata (see below),0
This relative path is required due way DeeperCut is structured,0
copy images and folder structure in the folder containing,0
training data comparison,0
Filename for pickle file:,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make matlab train file!,0
load image to get dimensions:,0
"print ""Grayscale!""",0
Pickle the 'data' dictionary using the highest protocol available.,0
###############################################################################,0
Convert to idosyncratic training file for deeper cut (*.mat),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Pickle the 'data' dictionary using the highest protocol available.,0
https://stackoverflow.com/questions/800197/how-to-get-all-of-the-immediate-subdirectories-in-python,0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
"sys.path.append(subfolder + ""/pose-tensorflow/"")",0
###################################################,0
Auxiliary functions,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
###################################################,0
Loading dependencies,0
###################################################,0
loading meta data / i.e. training & test files,0
###################################################,0
Models vs. benchmark for varying training state,0
###################################################,0
only specific parts can also be compared (not all!) (in that case change which bodyparts by providing a list below),0
extract training iterations:,0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Deep-cut dependencies,0
Dependencies for anaysis,0
loading meta data / i.e. training & test files & labels,0
######################################################################,0
Load and setup CNN part detector as well as its configuration,0
######################################################################,0
Specifying state of model (snapshot / training state),0
#################################################,0
Compute predictions over images,0
#################################################,0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results:,0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Deep-cut dependencies,0
###################################################,0
Loading data and evaluating network on data,0
###################################################,0
###############################################################################,0
Check which snapshots exist for given network (with training data split).,0
###############################################################################,0
Check which snap shots are available and sort them by # iterations,0
"if not analyzed, then call auxiliary script to analyze the network:",0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Restore variables from disk.,0
The next part of the code depends upon which tensorflow version you have.,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
Restore variables from disk.,0
Save snapshot,0
Restore variables from disk.,0
The next part of the code depends upon which tensorflow version you have.,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Load Matlab file dataset annotation,0
make sure joint ids are 0-indexed,0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Dependencies for video:,0
###################################################,0
Loading descriptors of model,0
###################################################,0
loading meta data / i.e. training & test files,0
"basefolder = os.path.join('..','pose-tensorflow','models')",0
"datafolder = os.path.join(basefolder , ""UnaugmentedDataSet_"" + Task + date)",0
"Data = pd.read_hdf(os.path.join(datafolder , 'data-' + Task , 'CollectedData_' + humanscorer + '.h5'),'df_with_missing')",0
Name for scorer based on passed on parameters from myconfig_analysis. Make sure they refer to the network of interest.,0
###################################################,0
Auxiliary function,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
one might want to adjust,0
image = img_as_ubyte(clip.get_frame(index * 1. / clip.fps)),0
#################################################,0
Datafolder,0
#################################################,0
videofolder='../videos/' #where your folder with videos is.,0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Deep-cut dependencies,0
Dependencies for video:,0
import matplotlib.pyplot as plt,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
Check if data already was generated:,0
Name for scorer:,0
Name for scorer:,0
#################################################,0
Datafolder,0
#################################################,0
videofolder='../videos/' #where your folder with videos is.,0
Attempt to load data...,0
nframes = np.sum(1 for j in clip.iter_frames()) #this is slow (but accurate),0
this will overestimage number of frames (see https://github.com/AlexEMG/DeepLabCut/issues/9) This is especially a problem,0
for high frame rates and long durations due to rounding errors (as Rich Warren found). Later we crop the result (line 187),0
image = img_as_ubyte(clip.get_frame(index * 1. / fps)),0
Thanks to Rick Warren for the  following snipplet:,0
"if close to end of video, start checking whether two adjacent frames are identical",0
this should only happen when moviepy has reached the final frame,0
"if two adjacent frames are identical, terminate the loop",0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Deep-cut dependencies,0
Dependencies for video:,0
import matplotlib.pyplot as plt,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check which snap shots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
Check if data already was generated:,0
Name for scorer:,0
Name for scorer:,0
#################################################,0
Datafolder,0
#################################################,0
Folder where your tiffstacks are:,0
Attempt to load data...,0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Dependencies for video:,0
###################################################,0
Loading descriptors of model,0
###################################################,0
loading meta data / i.e. training & test files,0
"basefolder = os.path.join('..','pose-tensorflow','models')",0
"datafolder = os.path.join(basefolder , ""UnaugmentedDataSet_"" + Task + date)",0
"Data = pd.read_hdf(os.path.join(datafolder , 'data-' + Task , 'CollectedData_' + humanscorer + '.h5'),'df_with_missing')",0
Name for scorer based on passed on parameters from myconfig_analysis. Make sure they refer to the network of interest.,0
###################################################,0
Auxiliary function,0
###################################################,0
"rr, cc = circle_perimeter(yc,xc,radius)",0
#################################################,0
Datafolder,0
#################################################,0
coding: utf-8,0
########################################################################################,0
This configuration file sets various parameters for running a trained model on videos!,0
,0
"First, make sure that the network performed well on the train/test data set.",0
,0
The pose output is saved as hdf file with a name containing the video name as well as the network name,0
You can also save the output as csv (see below) or in many other formats see https://github.com/AlexEMG/DeepLabCut/issues/17,0
,0
########################################################################################,0
Filename and path to behavioral video for analysis,0
"Note: under the hood there is moviepy, which can handle many types of videos:",0
See: https://zulko.github.io/moviepy/_modules/moviepy/video/io/VideoFileClip.html,0
"If you have stacks of tiffs (instead of videos) you can use ""AnalyzeABunchofPictures.py""",0
ROI dimensions / bounding box (only used if cropping == True),0
"x1,y1 indicates the top left corner and",0
"x2,y2 is the lower right corner of the cropped region.",0
########################################################################################,0
Analysis Network parameters,0
########################################################################################,0
These variables should be changed so that the right networks is loaded for analysis,0
(Typicaly just copy them over from myconfig.py),0
Note the data is always saved in hdf - format which is an efficient format,0
that easily allows to load the full pandas multiarray at a later stage,0
########################################################################################,0
# For plotting (MakingLabeledVideo.py / MakingLabeledVideo_fast.py),0
########################################################################################,0
based on the labels for this network state.,0
delete individual (labeled) frames after making video? (note there could be many...),0
coding: utf-8,0
####################################################################################,0
This configuration file sets various parameters for training and evaluating DeepLabCut,0
####################################################################################,0
myconfig.py:,0
#######################################,0
Step 1: Selecting Frames from videos,0
#######################################,0
Filename and path to behavioral video:,0
Put name of video: / or 'all' to extract frames from all videos in folder.,0
filename = 'all',0
ROI dimensions / bounding box (only used if cropping == True),0
"x1,y1 indicates the top left corner and",0
"x2,y2 is the lower right corner of the cropped region.",0
"Portion of the video to sample from in step 1 (In relative terms of video length, i.e. [0,1] is the full video)",0
Number of frames to pick,0
Method to pick frames:,0
"the algorithm 'uniform' temporally uniformly sampes frames in interval (start,stop). Visual information within video is irrelevant for this method. This code is very fast and sufficient (to extract distinct frames) when behavioral videos naturally covers many states.",0
Alternatively consider using: 'kmeans',0
"This code downsamples the video. The video is extracted as a numpy array, which is then",0
clustered by kmeans whereby each frames are treated as a vector. Frames from different clusters are then selected for labeling. This,0
"procedure makes sure that the frames ""look different"", i.e. different postures etc.",0
On large videos this code is slow. Consider not extracting the frames from the whole video but rather set start and stop to a period around interesting behavior that you want DeepLabCut to resolve/analyze (i.e. a reach).,0
#######################################,0
Step 2: Converting frames to pandas array,0
#######################################,0
annotator in *.csv file (for multibodypartsfile). For single files order is irrelevant,0
"Set this true if the data was sequentially labeled and if there is one file per folder (you can set the name of this file below, i.e. multibodypartsfilename)",0
"Otherwise there should be individual files per bodypart, i.e. in our demo case hand.csv, Finger1.csv etc.",0
If true then those files will be generated from Results.txt,0
When importing the images and the labels in the csv/xls files should be in the same order!,0
During labeling in Fiji one can thus (for occluded body parts) click in the origin of the image,0
"(i.e. top left corner (close to 0,0)), these ""false"" labels will then be removed. To do so set the following variable:",0
set this to 0 if no labels should be removed!,0
If you started from already extracted frames in a different format then change the format here (for step2 to 4).,0
#######################################,0
Step 3: Check labels / makes plots,0
#######################################,0
#######################################,0
Step 4: Generate Training Files,0
#######################################,0
Userparameters for training set. Other parameters can be set in pose_cfg.yaml,0
"Which resnet to use, 101 for deeper!",0
then change net_type and init_weights in Generating_a_Training_Set/pose_cfg.yaml,0
Afterwards train your network!,0
#######################################################################################################################,0
For Evaluation your network,0
#######################################################################################################################,0
identifier of evaluation network:,0
To evaluate the last snapshot (i.e. the network that was trained longest) : -1,0
"To evaluate all models (training stages) set this to: ""all""  (as string!)",0
This cutoff will also be used in plots.,0
Note that this will be plotted for all snapshots as indicated by snapshotindex,0
Select ROI of interest by adjusting values in myconfig.py,0
###################################################,0
Creating folder with name of experiment and extract random frames,0
###################################################,0
Extract the first frame (not cropped!) - useful for data augmentation,0
Select ROI of interest by adjusting values in myconfig.py,0
####################################################################,0
First load the image and crop (if necessary)/ set checkcropping = True (in myconfig to do so),0
####################################################################,0
###################################################,0
Creating folder with name of experiment and extract random frames,0
###################################################,0
###################################################,0
Loading dependencies,0
###################################################,0
##################################################,0
Code if each bodypart has its own label file!,0
##################################################,0
############################################,0
Make sure you update the train.yaml file!,0
############################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
"Data frame to hold data of all data sets for different scorers, bodyparts and images",0
Make list of different video data sets:,0
"videos=np.sort([fn for fn in os.listdir(os.curdir) if (""avi"" in fn)])",0
sort image file names according to how they were stacked (when labeled in Fiji),0
This is important when using data combined / which runs consecutively!,0
!/usr/bin/env python3,0
-*- coding: utf-8 -*-,0
##################################################,0
Code if all bodyparts (per folder are shared in one file),0
This code below converts it into multiple csv files per body part & folder,0
Based on an idea by @sneakers-the-rat,0
##################################################,0
"load csv, iterate over nth value in a grouping by frame, save to bodyparts files",0
##################################################,0
Code if each bodypart has its own label file!,0
##################################################,0
"Data frame to hold data of all data sets for different scorers,",0
bodyparts and images,0
Make list of different video data sets / each one has its own folder,0
"print(""found"",len(folders),len(numdistinctfolders))",0
"print(""Loading folder "", folder)",0
sort image file names according to how they were stacked,0
files=np.sort([fn for fn in os.listdir(os.curdir),0
"if (""img"" in fn and "".png"" in fn and ""_labelled"" not in fn)])",0
"Note: If your csv file is not correctly loaded, then a common error is:",0
"""AttributeError: 'DataFrame' object has no attribute 'X'"" or the corresponding error with Slice",0
Try to make sure you specify the seperator of the csv file correctly. See https://github.com/AlexEMG/DeepLabCut/issues/10 for details.,0
get rid of values that are invisible >> thus user scored in left corner!,0
"frame=pd.DataFrame(np.vstack([dframe.X,dframe.Y]).T, columns=index,index=imageaddress)",0
print(frame.head()),0
"print(""Done with folder "", folder)",0
Save data by this scorer,0
###################################################,0
Loading dependencies,0
###################################################,0
###################################################,0
"Definitions (Folders, data source and labels)",0
###################################################,0
Loading scorer's data:,0
Make that folder and put in the collecteddata (see below),0
This relative path is required due way DeeperCut is structured,0
copy images and folder structure in the folder containing,0
training data comparison,0
Filename for pickle file:,0
###################################################,0
Generating data structure with labeled information & frame metadata (for deep cut),0
###################################################,0
Make matlab train file!,0
load image to get dimensions:,0
"print ""Grayscale!""",0
Pickle the 'data' dictionary using the highest protocol available.,0
###############################################################################,0
Convert to idosyncratic training file for deeper cut (*.mat),0
###############################################################################,0
###############################################################################,0
Creating file structure for training &,0
Test files as well as pose_yaml files (containing training and testing information),0
################################################################################,0
Pickle the 'data' dictionary using the highest protocol available.,0
https://stackoverflow.com/questions/800197/how-to-get-all-of-the-immediate-subdirectories-in-python,0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
"sys.path.append(subfolder + ""/pose-tensorflow/"")",0
###################################################,0
Auxiliary functions,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
###################################################,0
Loading dependencies,0
###################################################,0
loading meta data / i.e. training & test files,0
###################################################,0
Models vs. benchmark for varying training state,0
###################################################,0
only specific parts can also be compared (not all!) (in that case change which bodyparts by providing a list below),0
extract training iterations:,0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Deep-cut dependencies,0
Dependencies for anaysis,0
loading meta data / i.e. training & test files & labels,0
######################################################################,0
Load and setup CNN part detector as well as its configuration,0
######################################################################,0
Specifying state of model (snapshot / training state),0
#################################################,0
Compute predictions over images,0
#################################################,0
Compute prediction with the CNN,0
"Extract maximum scoring location from the heatmap, assume 1 person",0
Saving results:,0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Deep-cut dependencies,0
###################################################,0
Loading data and evaluating network on data,0
###################################################,0
###############################################################################,0
Check which snapshots exist for given network (with training data split).,0
###############################################################################,0
Check which snap shots are available and sort them by # iterations,0
"if not analyzed, then call auxiliary script to analyze the network:",0
a must specify keys that are in b,0
if k not in b:,0
raise KeyError('{} is not a valid config key'.format(k)),0
recursively merge dicts,0
Restore variables from disk.,0
The next part of the code depends upon which tensorflow version you have.,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
scmask = batch[Batch.part_score_weights],0
if scmask.size > 1:,0
scmask = np.squeeze(scmask).astype('uint8'),0
else:,0
scmask = np.zeros(img.shape),0
figure(0),0
"plt.imshow(np.sum(scmap, axis=2))",0
plt.figure(100),0
plt.imshow(img),0
plt.figure(2),0
plt.imshow(scmask),0
Restore variables from disk.,0
Save snapshot,0
Restore variables from disk.,0
The next part of the code depends upon which tensorflow version you have.,0
loss['total_loss'] = slim.losses.get_total_loss(add_regularization_losses=params.regularize),0
Load Matlab file dataset annotation,0
make sure joint ids are 0-indexed,0
"horizontally flip the x-coordinate, keep y unchanged",0
joint ids are 0 indexed,0
swap the joint_id for a symmetric one,0
"don't loop over entire heatmap, but just relevant locations",0
"pt = arr([i*stride+half_stride, j*stride+half_stride])",0
diff = joint_pt - pt,0
The code above is too slow in python,0
print(la.norm(diff)),0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Dependencies for video:,0
###################################################,0
Loading descriptors of model,0
###################################################,0
loading meta data / i.e. training & test files,0
"basefolder = os.path.join('..','pose-tensorflow','models')",0
"datafolder = os.path.join(basefolder , ""UnaugmentedDataSet_"" + Task + date)",0
"Data = pd.read_hdf(os.path.join(datafolder , 'data-' + Task , 'CollectedData_' + humanscorer + '.h5'),'df_with_missing')",0
Name for scorer based on passed on parameters from myconfig_analysis. Make sure they refer to the network of interest.,0
###################################################,0
Auxiliary function,0
###################################################,0
https://stackoverflow.com/questions/14720331/how-to-generate-random-colors-in-matplotlib,0
one might want to adjust,0
image = img_as_ubyte(clip.get_frame(index * 1. / clip.fps)),0
#################################################,0
Datafolder,0
#################################################,0
videofolder='../videos/' #where your folder with videos is.,0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Deep-cut dependencies,0
Dependencies for video:,0
import matplotlib.pyplot as plt,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check which snapshots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
Check if data already was generated:,0
Name for scorer:,0
Name for scorer:,0
#################################################,0
Datafolder,0
#################################################,0
videofolder='../videos/' #where your folder with videos is.,0
Attempt to load data...,0
nframes = np.sum(1 for j in clip.iter_frames()) #this is slow (but accurate),0
this will overestimage number of frames (see https://github.com/AlexEMG/DeepLabCut/issues/9) This is especially a problem,0
for high frame rates and long durations due to rounding errors (as Rich Warren found). Later we crop the result (line 187),0
image = img_as_ubyte(clip.get_frame(index * 1. / fps)),0
Thanks to Rick Warren for the  following snipplet:,0
"if close to end of video, start checking whether two adjacent frames are identical",0
this should only happen when moviepy has reached the final frame,0
"if two adjacent frames are identical, terminate the loop",0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Deep-cut dependencies,0
Dependencies for video:,0
import matplotlib.pyplot as plt,0
###################################################,0
"Loading data, and defining model folder",0
###################################################,0
#################################################,0
Load and setup CNN part detector,0
#################################################,0
Check which snap shots are available and sort them by # iterations,0
#################################################,0
Compute predictions over images,0
#################################################,0
Check if data already was generated:,0
Name for scorer:,0
Name for scorer:,0
#################################################,0
Datafolder,0
#################################################,0
Folder where your tiffstacks are:,0
Attempt to load data...,0
###################################################,0
Dependencies,0
###################################################,0
add parent directory: (where nnet & config are!),0
Dependencies for video:,0
###################################################,0
Loading descriptors of model,0
###################################################,0
loading meta data / i.e. training & test files,0
"basefolder = os.path.join('..','pose-tensorflow','models')",0
"datafolder = os.path.join(basefolder , ""UnaugmentedDataSet_"" + Task + date)",0
"Data = pd.read_hdf(os.path.join(datafolder , 'data-' + Task , 'CollectedData_' + humanscorer + '.h5'),'df_with_missing')",0
Name for scorer based on passed on parameters from myconfig_analysis. Make sure they refer to the network of interest.,0
###################################################,0
Auxiliary function,0
###################################################,0
"rr, cc = circle_perimeter(yc,xc,radius)",0
#################################################,0
Datafolder,0
#################################################,0
