Commit Message,predict
Get the long description from the README file,0
Versions should comply with PEP440.  For a discussion on single-sourcing,0
"the version across setup.py and the project code, see",0
https://packaging.python.org/en/latest/single_source_version.html,0
The project's main homepage.,0
Author details,0
Choose your license,0
See https://pypi.python.org/pypi?%3Aaction=list_classifiers,0
How mature is this project? Common values are,0
3 - Alpha,0
4 - Beta,0
5 - Production/Stable,0
Indicate who your project is intended for,0
"Pick your license as you wish (should match ""license"" above)",0
"Specify the Python versions you support here. In particular, ensure",0
"that you indicate whether you support Python 2, Python 3 or both.",0
What does your project relate to?,0
You can just specify the packages manually here if your project is,0
simple. Or you can use find_packages().,0
"Alternatively, if you want to distribute just a my_module.py, uncomment",0
this:,0
"py_modules=[""my_module""],",0
List run-time dependencies here.  These will be installed by pip when,0
"your project is installed. For an analysis of ""install_requires"" vs pip's",0
requirements files see:,0
https://packaging.python.org/en/latest/requirements.html,0
"To provide executable scripts, use entry points in preference to the",0
"""scripts"" keyword. Entry points provide cross-platform support and allow",0
pip to create the appropriate form of executable for the target platform.,0
Communication to TensorFlow server via gRPC,0
TensorFlow serving stuff to send messages,0
Command line arguments,0
Send request,0
request.inputs['input'].CopyFrom(),0
"TestModels.onnx_emit,",0
"TestModels.caffe_emit,",0
"TestModels.cntk_emit,",0
"TestModels.coreml_emit,",0
"TestModels.mxnet_emit,",0
"TestModels.pytorch_emit,",0
TestModels.tensorflow_emit,0
"TestModels.caffe_emit,",0
"TestModels.coreml_emit,",0
"TestModels.keras_emit,",0
"TestModels.mxnet_emit,",0
Run too slow on Travis.,0
cntk_emit OOM on Travis,0
"TestModels.cntk_emit,",0
"TestModels.cntk_emit,",0
Cannot run on Travis since it seems to consume too much memory.,0
"TestModels.onnx_emit,",0
"TestModels.caffe_emit,",0
"TestModels.cntk_emit,",0
"TestModels.coreml_emit,",0
"TestModels.keras_emit,",0
"TestModels.keras_emit,",0
"TestModels.caffe_emit,",0
"TestModels.cntk_emit,",0
"TestModels.pytorch_emit,",0
"TestModels.caffe_emit,",0
"TestModels.cntk_emit,",0
"TestModels.pytorch_emit,",0
"TestModels.caffe_emit,",0
"TestModels.cntk_emit,",0
"TestModels.pytorch_emit,",0
"TestModels.caffe_emit,",0
"TestModels.cntk_emit,",0
"TestModels.onnx_emit,",0
"TestModels.caffe_emit,",0
"TestModels.cntk_emit,",0
"TestModels.caffe_emit,",0
"TestModels.cntk_emit,",0
"TestModels.coreml_emit,",0
"TestModels.keras_emit,",0
"TestModels.mxnet_emit,",0
"TestModels.pytorch_emit,",0
TestModels.tensorflow_emit,0
omit tensorflow lead to crash,0
"TestModels.cntk_emit,",0
"TestModels.cntk_emit,",0
"TestModels.caffe_emit,",0
"TestModels.cntk_emit,",0
"TestModels.keras_emit,",0
"TestModels.mxnet_emit,",0
"TestModels.pytorch_emit,",0
TestModels.tensorflow_emit,0
"TestModels.cntk_emit,",0
"Function self.assertEquals has deprecated, change to assertEqual",0
"self.assertEqual(original_predict.shape, converted_predict.shape)",0
"TestModels.caffe_emit,",0
"TestModels.cntk_emit,",0
"TestModels.mxnet_emit,",0
"TestModels.pytorch_emit,",0
"TestModels.onnx_emit,",0
"TestModels.caffe_emit,",0
"TestModels.cntk_emit,",0
"TestModels.mxnet_emit,",0
"TestModels.pytorch_emit,",0
"TestModels.onnx_emit,",0
"TestModels.cntk_emit,",0
"TestModels.mxnet_emit,",0
"TestModels.onnx_emit,",0
"TestModels.cntk_emit,",0
get original model prediction result,0
original to IR,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
print(original_predict),0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
get shape,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
IR to code,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
The list is in RGB oder,0
IR to Model,0
"converted_file = original_framework + '_coreml_' + architecture_name + ""_converted""",0
"converted_file = converted_file.replace('.', '_')",0
load model,0
save model,0
"coremltools.utils.save_spec(model.get_spec(), converted_file)",0
inference,0
IR to code,0
import converted model,0
"In case of odd number add the extra padding at the end for SAME_UPPER(eg. pads:[0, 2, 2, 0, 0, 3, 3, 0]) and at the beginning for SAME_LOWER(eg. pads:[0, 3, 3, 0, 0, 2, 2, 0])",0
"'xception'     : [onnx_emit],",0
"'nasnet'       : [onnx_emit],",0
"Temporarily disable 'yolo2'        : [onnx_emit],",0
"Temporarily disable 'inception_v4'  : [onnx_emit],",0
"'voc-fcn8s'     : [onnx_emit], # TODO: ConvTranspose, Crop",0
"'voc-fcn16s'    : [onnx_emit], # TODO: ConvTranspose, Crop",0
"'voc-fcn32s'    : [onnx_emit], # TODO: ConvTranspose, Crop",0
"Temporarily disable 'xception'      : [onnx_emit],",0
"Temporarily disable 'facenet'               : [onnx_emit],",0
"'resnet_v1_50'          : [onnx_emit], # POOL: strides > window_shape not supported due to inconsistency between CPU and GPU implementations",0
"'resnet_v1_152'         : [onnx_emit], # POOL: strides > window_shape not supported due to inconsistency between CPU and GPU implementations",0
"'resnet_v2_50'          : [onnx_emit], # POOL: strides > window_shape not supported due to inconsistency between CPU and GPU implementations",0
"'resnet_v2_152'         : [onnx_emit], # POOL: strides > window_shape not supported due to inconsistency between CPU and GPU implementations",0
"'nasnet-a_large'        : [onnx_emit], # POOL: strides > window_shape not supported due to inconsistency between CPU and GPU implementations",0
"Temporarily disable 'facenet'           : [onnx_emit],",0
TODO: coredump,1
"'alexnet'       : [cntk_emit, keras_emit, tensorflow_emit],",0
"'nasnet'       : [tensorflow_emit, keras_emit, coreml_emit],",0
"Temporarily disable 'yolo2'        : [keras_emit],",0
"'facenet'      : [tensorflow_emit, coreml_emit,mxnet_emit,keras_emit]  # TODO",0
"Temporarily disable 'inception_v4'  : [cntk_emit, coreml_emit, keras_emit, pytorch_emit, tensorflow_emit], # TODO mxnet_emit(Small error), caffe_emit(Crash for shape)",1
"Temporarily disable 'xception'      : [coreml_emit, cntk_emit, mxnet_emit, pytorch_emit, tensorflow_emit], #  TODO: Caffe(Crash) keras_emit(too slow)",1
"Temporarily disable 'facenet'               : [mxnet_emit, tensorflow_emit, keras_emit, pytorch_emit, caffe_emit], # TODO: coreml_emit",0
"Temporarily disable 'rnn_lstm_gru_stacked'  : [tensorflow_emit, keras_emit, pytorch_emit, mxnet_emit] #TODO cntk_emit",0
"Temporarily disable 'facenet'           : [mxnet_emit, tensorflow_emit, keras_emit, caffe_emit] # TODO: coreml_emit",0
"'tinyyolo'     : [coreml_emit, keras_emit, mxnet_emit, pytorch_emit, tensorflow_emit],",0
"'vgg16': [tensorflow_emit],",0
'alexnet': [tensorflow_emit],0
get test input path,0
get original model prediction result,0
def test_caffe(self):,0
try:,0
import caffe,0
"self._test_function('caffe', self.caffe_parse)",0
except ImportError:,0
"print('Please install caffe! Or caffe is not supported in your platform.', file=sys.stderr)",0
def test_cntk(self):,0
try:,0
import cntk,0
"self._test_function('cntk', self.cntk_parse)",0
except ImportError:,0
"print('Please install cntk! Or cntk is not supported in your platform.', file=sys.stderr)",0
def test_coreml(self):,0
from coremltools.models.utils import macos_version,0
"if macos_version() < (10, 13):",0
"print('Coreml is not supported in your platform.', file=sys.stderr)",0
else:,0
"self._test_function('coreml', self.coreml_parse)",0
def test_keras(self):,0
"self._test_function('keras', self.keras_parse)",0
def test_mxnet(self):,0
"self._test_function('mxnet', self.mxnet_parse)",0
def test_darknet(self):,0
"self._test_function('darknet', self.darknet_parse)",0
def test_paddle(self):,0
# omit tensorflow lead to crash,0
import tensorflow as tf,0
try:,0
import paddle.v2 as paddle,0
"self._test_function('paddle', self.paddle_parse)",0
except ImportError:,0
"print('Please install Paddlepaddle! Or Paddlepaddle is not supported in your platform.', file=sys.stderr)",0
def test_pytorch(self):,0
"self._test_function('pytorch', self.pytorch_parse)",0
def test_tensorflow(self):,0
"self._test_function('tensorflow', self.tensorflow_parse)",0
def test_tensorflow_frozen(self):,0
"self._test_function('tensorflow_frozen', self.tensorflow_frozen_parse)",0
"TestModels.onnx_emit,",0
"TestModels.cntk_emit,",0
"TestModels.onnx_emit,",0
"TestModels.cntk_emit,",0
"TestModels.coreml_emit,",0
"TestModels.onnx_emit,",0
"TestModels.cntk_emit,",0
"Temporarily disable 'xception'      : [TestModels.coreml_emit, TestModels.cntk_emit, TestModels.tensorflow_emit],",0
"TestModels.onnx_emit,",0
"TestModels.cntk_emit,",0
Cannot run on Travis since it seems to consume too much memory.,0
"Temporarily disable 'xception'      : [TestModels.mxnet_emit, TestModels.pytorch_emit],",0
"Temporarily disable 'inception_v4'  : [TestModels.cntk_emit, TestModels.coreml_emit, TestModels.keras_emit, TestModels.pytorch_emit, TestModels.tensorflow_emit],",0
"TestModels.onnx_emit,",0
"TestModels.caffe_emit,",0
"TestModels.cntk_emit,",0
"TestModels.coreml_emit,",0
"TestModels.mxnet_emit,",0
"TestModels.pytorch_emit,",0
TestModels.tensorflow_emit,0
"TestModels.onnx_emit,",0
"TestModels.caffe_emit,",0
"TestModels.cntk_emit,",0
"TestModels.coreml_emit,",0
"TestModels.mxnet_emit,",0
"TestModels.pytorch_emit,",0
TestModels.tensorflow_emit,0
"TestModels.onnx_emit,",0
"TestModels.cntk_emit,",0
"TestModels.onnx_emit,",0
"TestModels.cntk_emit,",0
!/usr/bin/python,0
major python major_python_versions as python2 and python3,0
operating system,0
do not deal,0
I don't want to deal with auto_pad,0
Don't support auto_pad current!,0
2018-02-28,0
if group is None:,0
group = 1,0
group is not support yet too.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
node,0
input,0
edge(node a <-> node b),0
,0
"key is edge name, value is src/dst node name",0
"key is initializer name, value is TensorProto",0
print(name),0
print(layer.op_type),0
n is input data,0
n is input edge,0
omit node of some type,0
self.nodes.append(IR_node.variable_name + '_bias'),0
self.nodes.append(IR_node.variable_name + '_weight'),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"if node len(in_edges), generate additional DataInput node",0
print,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
TODO,1
"'max_pool2d': convert_maxpool,",0
"'onnx::Mul': convert_elementwise_mul,",0
"'onnx::Sub': convert_elementwise_sub,",0
"'onnx::ConvTranspose': convert_convtranspose,",0
"'onnx::LeakyRelu': convert_lrelu,",0
"'onnx::Sigmoid': convert_sigmoid,",0
"'onnx::Softmax': convert_softmax,",0
"'onnx::Selu': convert_selu,",0
"'onnx::Transpose': convert_transpose,",0
"'onnx::Reshape': convert_reshape,",0
"'onnx::MatMul': convert_matmul,",0
"'onnx::Gather': convert_gather,",0
"'onnx::ReduceSum': convert_reduce_sum,",0
"'onnx::Constant': convert_constant,",0
"'onnx::Upsample': convert_upsample,",0
"'onnx::Pad': convert_padding,",0
###########,0
property #,0
###########,0
###################,0
Public Functions #,0
###################,0
test,0
cpu: https://github.com/pytorch/pytorch/issues/5286,0
Build network graph,0
"(batch, C, H, W)  & NHWC",0
#########,0
Layers #,0
#########,0
dilation,0
handle bias,0
TODO,1
output_shape,0
epsilon,0
mean,0
var,0
Kit weight tranpose,0
weight: N x M -> C x H x W x M -> H x W x C x M -> N x M,0
,0
weights,0
use_bias,0
units,0
handle weight,0
handle bias,0
###################,0
Helper Functions #,0
###################,0
print (dir(data)),0
print (i),0
"print (""    {} with shape {}"".format(j, load_weight[i][j].shape))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Scopes created in a nested scope may have initial characters,0
that are illegal as the initial character of an op name,0
"(viz. '-', '\', '/', and '_').",0
sanity check.,0
run dce first to eliminate dead parts of the graph that might have been,0
left behind by things like symbolic_override,0
construct graph,0
build each layer,0
make connection,0
nodes,0
connect name and id in nodes with weights,0
print(node.__str__()),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Base Functions,0
"padding,",0
"exstr = "", value=float('-Inf')""",0
"exstr = """"",0
Change to padding defuse,0
"input_node = self._defuse_padding(IR_node, exstr)",0
Change to padding defuse,0
Ignore it in Pytorch,0
for Keras,0
"self.add_init(2, ""self.{:<15} = torch.from_numpy(_weights_dict['{}']['value'])"".format(",0
"IR_node.variable_name,",0
IR_node.name)),0
param_code does not need parameter slice.,0
from torch.nn.parameter import Parameter,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self._type = layer.__class__.__name__.replace('Backward', '')",0
"self._name = ""{}_{}"".format(self.type, id)",0
TODO,1
"""""""",0
build graph for pytorch 0.2.0,0
"""""""",0
dummy_input = torch.autograd.Variable(torch.randn(shape)),0
output_node = self.model(dummy_input),0
search_queue = [output_node.grad_fn],0
"tmp_node = PyTorchGraphNode(output_node.grad_fn, 0)",0
self.layer_map[tmp_node.name] = tmp_node,0
visited = {output_node.grad_fn : self.layer_map[tmp_node.name]},0
idx = 0,0
node_count = 1,0
while (idx < len(search_queue)):,0
current_node = search_queue[idx],0
current_type = visited[current_node].type,0
"if hasattr(current_node, 'next_functions'):",0
"for parent, _ in current_node.next_functions:",0
"parent_type = parent.__class__.__name__.replace('Backward', '')",0
if parent_type != 'AccumulateGrad' and \,0
(parent_type != 'Transpose' or current_type != 'Addmm'):,0
if not parent in visited:,0
"tmp_node = PyTorchGraphNode(parent, node_count)",0
self.layer_map[tmp_node.name] = tmp_node,0
node_count += 1,0
visited[parent] = tmp_node,0
search_queue.append(parent),0
"self._make_connection(visited[parent].name, visited[current_node].name)",0
idx += 1,0
try:,0
return TorchGraph._forward_torch_random_input(,0
"torch_model,",0
"input_shapes,",0
is_batch=False,0
),0
except:,0
# try batch mode,0
# return TorchGraph._forward_torch_random_input(,0
"#     torch_model,",0
"#     input_shapes,",0
#     is_batch=True,0
# ),0
pass,0
tensor = torch.rand(*shape).float(),0
multi output,0
single output,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
###########,0
property #,0
###########,0
###################,0
Public Functions #,0
###################,0
Build network graph,0
#########,0
Layers #,0
#########,0
handle weight,0
handle bias,0
handle weight,0
handle bias,0
###################,0
Helper Functions #,0
###################,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
load model file into Coreml Graph,0
model.encode() convert to str --- python2 may crash due to type 'unicode',0
Build Network Graph,0
self.data_format ? TODO,1
load the model network,0
convert to Model_pb2.Model,0
determine the type of the current_node,0
staticmethods,0
"(seq, batch, C, H, W)  & NHWC",0
TODO dtype_map,1
##### convert methods,0
convolution,0
input edge,0
important!,0
reshape the weight!,0
"[2, 3, 0, 1]",0
pads,0
use_bias,0
isDeconvolution,0
"name, op",0
kwargs['kernel_shape'] = weights.shape,0
strides,0
"[1, sd, sh, sw, 1]",0
activation,0
TODO,1
padding in conv,0
"pad in IR is [x1_b, x2_b, ..., x1_e, x2_e, ...]",0
padding,0
compute padding for 'same',0
https://discuss.mxnet.io/t/pooling-and-convolution-with-same-mode/528/3,0
https://www.jianshu.com/p/05c4f1621c7e,0
padding in pooling,0
padding,0
https://discuss.mxnet.io/t/pooling-and-convolution-with-same-mode/528/3,0
TODO,1
symmetric padding,0
"name, op",0
input edge,0
For concat axis,0
"NO axis in coreml, so set the last axis",0
"The first -1 means in coreml there is one-more axis,",0
The second -1 means the last axis,0
"name, op",0
input edge,0
padding,0
Future Module TODO,1
#### rename methods,0
"name, op",0
activation type,0
else:,0
assert False,0
input edge,0
Merge layers,0
only for training,0
"name, op",0
input edge,0
shape,0
NHWC channel last,0
"in fact, here is NWHC",0
"name, op",0
input edge,0
axis TODO,0
"channels_first, then axis = 1",0
scale,0
bias,0
epsilon,0
compute adjusted parameters,0
Reference: parameter transformation https://github.com/apple/coremltools/issues/153,0
convert type because of tensorflow,0
mean,0
var,0
"name, op",0
input edge,0
bias,0
"name, op",0
input edge,0
"MAX = 0, AVERAGE = 1, L2 = 2",0
padding,0
strides,0
"[1, sd, sh, sw, 1]",0
window_shape,0
"[1, pd, ph, pw, 1]",0
"name, op",0
input edge,0
"name, op",0
input edge,0
"name, op",0
input edge,0
units,0
use_bias,0
weights,0
change to single because of the tf matmul,0
in features,0
"name, op",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
the end of the tensorflow_resnet_v2_50's squeeze shape is [unknown_rank: true] with len 0,0
1001 means the 1001 classes for tensorflow_resnet_v2_50,0
!Alert! TODO,1
Future implement can be changed to the last two layer,0
TODO - remove style transfer 1D hack,1
"Input is 1D but it goes to the width dimension: (1,1,W)",0
"assume (Batch, Channels) - Batch dimension should be dropped",0
"assume (Batch, Sequence-Length, channels)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Add classifier classes (if applicable),0
Set pre-processing paramsters,0
"image_input_names,",0
Return the protobuf spec,0
model = _MLModel(self.builder.spec),0
if IR_node.name in self.weights_dict and 'weights' in self.weights_dict[IR_node.name]:,0
pass,0
"self._emit_merge(IR_node,'DOT')",0
Get input and output names,0
Dimensions and weights,0
"W should have shape (height, width, kernel_channels, output_channels), where kernel_channel = input_channels / groups",0
Dilations,0
see protobuf,0
assert False,0
Get the weights.,0
Dilations,0
see protobuf,0
depth-wise convolution,0
Dilations,0
see protobuf,0
Get input and output names,0
Pooling layer type,0
"if it's global, set the global flag",0
TODO  global pooling modification,0
Padding,0
see protobuf,0
Padding,0
see protobuf,0
Get input and output names,0
"type: ""list(int). A 1-D values of (leftBorder, topBorder, rightBorder, bottomBorder).""",0
This is central crop,0
Now add the layer,0
"Allowed values: 'CHW', 'HW', 'C', 'H', 'W'",0
Get input and output names,0
Get input and output names,0
Get the weights from keras,0
Get input and output names,0
blob_order == 0 if the input blob needs not be rearranged,0
blob_order == 1 if the input blob needs to be rearranged,0
"using keras_layer.input.shape have a ""?"" (Dimension[None] at the front),",0
making a 3D tensor with unknown batch size 4D,0
Get input and output names,0
activation emit,0
Get input and output names,0
negate it,0
apply threshold,0
negate it back,0
for Keras,0
Get input and output names,0
Set parameters,0
"Parameter arrangement in Keras: gamma, beta, mean, variance",0
compute adjusted parameters,0
Reference: parameter transformation https://github.com/apple/coremltools/issues/153,0
padding type TODO,0
"Type of the padding. Can be one of 'constant', 'reflection' or 'replication",0
Now add the layer,0
self.emit_Flatten(IR_node),0
Get the weights,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
input layer,0
build each layer,0
"if A.output == B.input, then make the connection: A -> B",0
"print('{0:20}->     {1:20}'.format(layerA.name, layerB.name))",0
"if A.name == B.input, then make the connection: A -> B, here A is the input",0
build each layer,0
"if A.output == B.input, then make the connection: A -> B",0
"print('{0:20}->     {1:20}'.format(layerA.name, layerB.name))",0
"if A.name == B.input, then make the connection: A -> B, here A is the input",0
build each layer,0
"if A.output == B.input, then make the connection: A -> B",0
"print('{0:20}->     {1:20}'.format(layerA.name, layerB.name))",0
"if A.name == B.input, then make the connection: A -> B, here A is the input",0
The information of the layer,0
Generated by the protocol buffer compiler.  DO NOT EDIT!,0
source: caffe.proto,0
@@protoc_insertion_point(imports),0
@@protoc_insertion_point(class_scope:caffe.BlobShape),0
@@protoc_insertion_point(class_scope:caffe.BlobProto),0
@@protoc_insertion_point(class_scope:caffe.BlobProtoVector),0
@@protoc_insertion_point(class_scope:caffe.Datum),0
@@protoc_insertion_point(class_scope:caffe.FillerParameter),0
@@protoc_insertion_point(class_scope:caffe.NetParameter),0
@@protoc_insertion_point(class_scope:caffe.SolverParameter),0
@@protoc_insertion_point(class_scope:caffe.SolverState),0
@@protoc_insertion_point(class_scope:caffe.NetState),0
@@protoc_insertion_point(class_scope:caffe.NetStateRule),0
@@protoc_insertion_point(class_scope:caffe.ParamSpec),0
@@protoc_insertion_point(class_scope:caffe.LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.TransformationParameter),0
@@protoc_insertion_point(class_scope:caffe.LossParameter),0
@@protoc_insertion_point(class_scope:caffe.AccuracyParameter),0
@@protoc_insertion_point(class_scope:caffe.ArgMaxParameter),0
@@protoc_insertion_point(class_scope:caffe.ConcatParameter),0
@@protoc_insertion_point(class_scope:caffe.BatchNormParameter),0
@@protoc_insertion_point(class_scope:caffe.BiasParameter),0
@@protoc_insertion_point(class_scope:caffe.ContrastiveLossParameter),0
@@protoc_insertion_point(class_scope:caffe.ConvolutionParameter),0
@@protoc_insertion_point(class_scope:caffe.CropParameter),0
@@protoc_insertion_point(class_scope:caffe.DataParameter),0
@@protoc_insertion_point(class_scope:caffe.DropoutParameter),0
@@protoc_insertion_point(class_scope:caffe.DummyDataParameter),0
@@protoc_insertion_point(class_scope:caffe.EltwiseParameter),0
@@protoc_insertion_point(class_scope:caffe.ELUParameter),0
@@protoc_insertion_point(class_scope:caffe.EmbedParameter),0
@@protoc_insertion_point(class_scope:caffe.ExpParameter),0
@@protoc_insertion_point(class_scope:caffe.FlattenParameter),0
@@protoc_insertion_point(class_scope:caffe.HDF5DataParameter),0
@@protoc_insertion_point(class_scope:caffe.HDF5OutputParameter),0
@@protoc_insertion_point(class_scope:caffe.HingeLossParameter),0
@@protoc_insertion_point(class_scope:caffe.ImageDataParameter),0
@@protoc_insertion_point(class_scope:caffe.InfogainLossParameter),0
@@protoc_insertion_point(class_scope:caffe.InnerProductParameter),0
@@protoc_insertion_point(class_scope:caffe.InputParameter),0
@@protoc_insertion_point(class_scope:caffe.LogParameter),0
@@protoc_insertion_point(class_scope:caffe.LRNParameter),0
@@protoc_insertion_point(class_scope:caffe.MemoryDataParameter),0
@@protoc_insertion_point(class_scope:caffe.MVNParameter),0
@@protoc_insertion_point(class_scope:caffe.ParameterParameter),0
@@protoc_insertion_point(class_scope:caffe.PoolingParameter),0
@@protoc_insertion_point(class_scope:caffe.PowerParameter),0
@@protoc_insertion_point(class_scope:caffe.PythonParameter),0
@@protoc_insertion_point(class_scope:caffe.RecurrentParameter),0
@@protoc_insertion_point(class_scope:caffe.ReductionParameter),0
@@protoc_insertion_point(class_scope:caffe.ReLUParameter),0
@@protoc_insertion_point(class_scope:caffe.ReshapeParameter),0
@@protoc_insertion_point(class_scope:caffe.ScaleParameter),0
@@protoc_insertion_point(class_scope:caffe.SigmoidParameter),0
@@protoc_insertion_point(class_scope:caffe.SliceParameter),0
@@protoc_insertion_point(class_scope:caffe.SoftmaxParameter),0
@@protoc_insertion_point(class_scope:caffe.TanHParameter),0
@@protoc_insertion_point(class_scope:caffe.TileParameter),0
@@protoc_insertion_point(class_scope:caffe.ThresholdParameter),0
@@protoc_insertion_point(class_scope:caffe.WindowDataParameter),0
@@protoc_insertion_point(class_scope:caffe.SPPParameter),0
@@protoc_insertion_point(class_scope:caffe.V1LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.V0LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.PReLUParameter),0
@@protoc_insertion_point(module_scope),0
"TODO: We need to identify whether this is 4D image data, otherwise we shouldn't change the dimension order",1
print(node.parameters),0
assert False,0
"Stochastic pooling, for instance.",0
TODO: Axis,0
TODO: Unbiased,1
check if need the Flatten layer,0
if parent.output_shape.height > 1 or parent.output_shape.width > 1:,0
TODO: The gamma parameter has to be set (in node.data?) and this should work.,1
"Also, mean should be set to 0, and var to 1, just to be safe.",0
skip the split node,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"For some reason argparser gives us unicode, so we need to conver to str first",0
for test,0
"with open(""graph.txt"", 'w') as f:",0
for layer in self.IR_graph.topological_sort:,0
current_node = self.IR_graph.get_node(layer),0
"print(""========current_node=========\n{}"".format(current_node.layer), file=f)",0
test end,0
"print(""========current_node={}"".format(current_node.layer))",0
check if have pad layer,0
Pad_h < kernel_h (vgg19 caffe2caffe),0
implement asymmetric paddings by applying symmetric padding then cropping,0
num_output = IR_node.get_attr('kernel_shape')[-2],0
keys = [],0
for key in self.weights_dict[IR_node.name].keys():,0
keys.append(key),0
"print(""=======Layer: {}, keys: {}"".format(IR_node.name, keys))",0
"if asymmetric padding, set offset to 1",0
Change the layer name,0
check if need crop output shape,0
check if need crop output shape,0
"change the key ""name"" to ""variable_name"", in case of the layer name has invalid characters",0
"Convert NHWC (IR) to NCHW (Caffe): [0,1,2,3]->[0,3,1,2]",0
"def emit_Square(self, IR_node):",0
"input_layers = ', '.join(('n.' + self.IR_graph.get_node(edge).real_variable_name) for edge in IR_node.in_edges)",0
"self.add_body(1, ""n.{:<15} = L.Square({}, ntop=1)"".format(",0
"IR_node.variable_name,",0
input_layers)),0
Caffe Types,0
Same value applies to all spatial dimensions,0
Extract the value for the given spatial dimension,0
consider rewrite this function to Network.py,0
Dropout layers appear in a fair number of Caffe,0
test-time networks. These are just ignored. We'll,0
filter them out here.,0
TODO: raise error,1
TODO: raise error,1
Automatically set a name if not provided.,0
Figure out the layer inputs.,0
"print('op: %s   shape: %s' % (op, layer_output._keras_shape))",0
"print('op: %s   shape: %s' % (op, layer_output.get_shape().as_list()))",0
Add to layer LUT.,0
Old-style input specification,0
New-style input specification,0
"We most likely have a data layer on our hands. The problem is,",0
Caffe infers the dimensions of the data from the source (eg: LMDB).,0
We want to avoid reading datasets here. Fail for now.,0
This can be temporarily fixed by transforming the data layer to,0
"Caffe's ""input"" layer (as is usually used in the ""deploy"" version).",0
TODO: Find a better solution for this.,1
The .prototxt file defining the graph,0
The .caffemodel file containing the learned parameters,0
Set to true if the fallback protocol-buffer based backend was used,0
"A list containing (layer name, parameters) tuples",0
Load the parameters,0
"When using the protobuf-backend, each parameter initially has four dimensions.",0
"In certain cases (like FC layers), we want to eliminate the singleton dimensions.",0
"This implementation takes care of the common cases. However, it does leave the",0
potential for future issues.,0
The Caffe-backend does not suffer from this problem.,0
A dictionary mapping NodeKind to the transposed order.,0
The node kinds eligible for reshaping,0
"If true, the reshaped data will replace the old one.",0
"Otherwise, it's set to the reshaped_data attribute.",0
Check for 2+ dimensional data,0
The FC layer connected to the spatial layer needs to be,0
re-wired to match the new spatial ordering.,0
node.reshaped_data = weights.transpose(transpose_order),0
Set the weights,0
We're only fusing nodes with single parents,0
We can only fuse a node if its parent's,0
value isn't used by any other node.,0
Rewrite the fused node's children to its parent.,0
Disconnect the fused node from the graph.,0
Let the sub-class merge the fused node in any arbitrary way.,0
Fuse ReLUs when the parent node is one of the given types.,0
"If None, all node types are eligible.",0
Prescale the stats,0
Replace with the updated values,0
Include the scale and bias terms,0
self.graph = NodeRenamer()(graph),0
"return Graph(name, [self.map_node(node) for node in self.graph.nodes])",0
assert mapped_node is not None,0
skip when mapped_node is None,0
FIXME:,0
output = node.output,0
Decompose DAG into chains,0
Generate Python code line by line,0
Fall back to the protobuf implementation,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
from tensorflow.core.framework.node_def_pb2 import NodeDef,0
from tensorflow.core.framework import attr_value_pb2,0
pass,0
"print(""\n"")",0
print(i),0
print(block),0
continue,0
NHWC,0
assert False,0
print(shortcut_layer['input'] ),0
spacetodepth,0
print(block),0
print(region_layer),0
assert False,0
print line,0
set default value,0
"yolo3(608) start at 1, yolo2(608) start at 0. yolo2(416) start at 1, yolo3(416) start at 0",0
load weight by original order,0
print(node_type),0
print(IR_node),0
assert False,0
strides,0
assert False,0
padding,0
only load weight conv,0
"print(""----------------"",self.start)",0
print(kernel.shape),0
print(k_bias.shape),0
"buf, start, scale_layer['name'], bn_layer['name'], conv_layer['name']",0
"print(""=============="",self.start)",0
print(bias.shape),0
print(scale.shape),0
print(mean.shape),0
print(var.shape),0
print(kernel.shape),0
print(W),0
assert False,0
no use,0
print(source_node.layer),0
for image classification(resnet) AVG pooling,0
print(source_node.layer),0
print(source_node.layer),0
kwargs['ignore_thresh'] = source_node.get_attr('ignore_thresh'),0
print(source_node.get_attr('anchors')),0
"kwargs['anchors'] = ['0.52','0.22']",0
kwargs['mask'] = source_node.get_attr('mask'),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
tensorflow dump tag,0
Reference: 1) Keras image preprocess: https://github.com/keras-team/keras/blob/master/keras/applications/imagenet_utils.py,0
2) tensorflow github issue: https://github.com/tensorflow/models/issues/517,0
"R-G-B for Imagenet === [123.68, 116.78, 103.94]",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.input, self.model, self.testop = self.MainModel.KitModel(self.args.w)",0
testop = tf.get_default_graph().get_operation_by_name(layer_name),0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result(None, False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
layer_name = 'block2_pool',0
"intermediate_layer_model = keras.Model(inputs=model.input,",0
outputs=model.get_layer(layer_name).output),0
intermediate_output = intermediate_layer_model.predict(img),0
print (intermediate_output),0
print (intermediate_output.shape),0
"print (""%.30f"" % np.sum(intermediate_output))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Load a real image and do default tf imageNet preprocessing,0
"[Batch, Height, Width, Channels] --> [Batch, Channels, Height, Width]",0
We only deal with non-recurrent networks for now,0
"(H,W,C) --> (C,H,W)",0
Load TensorFlow model,0
Prepare inputs,0
Run TF session,0
Evaluate coreml model,0
evaluate the TF model,0
evaluate CoreML,0
Test the default CoreML evaluation,0
Download model,0
Convert to coreml,0
Test predictions on an image,0
Download model,0
Convert to coreml,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
"@unittest.skip(""Failing GPU backend: related to https://github.com/tf-coreml/tf-coreml/issues/26"")",0
? style transfer image size and style number?,0
Test predictions on an image,0
evaluate the TF model,0
evaluate CoreML,0
Test the default CoreML evaluation,0
"compare_tf_coreml_outputs(tf_out_flatten, coreml_out_flatten)",0
#Download model,0
url = 'https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz',0
tf_model_dir = _download_file(url = url),0
"tf_model_path = os.path.join(TMP_MODEL_DIR, 'inception_v3_2016_08_28_frozen.pb')",0
#Convert to coreml,0
"mlmodel_path = os.path.join(TMP_MODEL_DIR, 'inception_v3_2016_08_28.mlmodel')",0
mlmodel = tf_converter.convert(,0
"tf_model_path = tf_model_path,",0
"mlmodel_path = mlmodel_path,",0
"output_feature_names = ['InceptionV3/Predictions/Softmax:0'],",0
"input_name_shape_dict = {'input:0':[1,299,299,3]},",0
"image_input_names = ['input:0'],",0
"red_bias = -1,",0
"green_bias = -1,",0
"blue_bias = -1,",0
image_scale = 2.0/255.0),0
#Test predictions on an image,0
_test_coreml_model_image_input(,0
"tf_model_path = tf_model_path,",0
"coreml_model = mlmodel,",0
"input_tensor_name = 'input:0',",0
"output_tensor_name = 'InceptionV3/Predictions/Softmax:0',",0
img_size = 299),0
Download model,0
Convert to coreml,0
Test predictions on an image,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
self.test_truth(),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
from collections import namedtuple,0
"Batch = namedtuple('Batch', ['data'])",0
TODO,1
Apple has published some of their own models. They can be downloaded from https://developer.apple.com/machine-learning/.,0
"Those published models are: SqueezeNet, Places205-GoogLeNet, ResNet50, Inception v3, VGG16",0
TODO,1
load model,0
TODO: Multiple inputs,1
TODO: Multiple outputs,1
inference,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
caffe uses NCHW,0
"self.print_intermediate_result('pooling0', False)",0
delete tmp model files,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
pylint: disable=line-too-long,0
pylint: enable=line-too-long,0
"net = caffe.Net(arch_fn, weight_fn, caffe.TEST)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
# Image Classification,0
# Semantic Segmentation,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
print(r),0
d = darknet_extractor(),0
model_filename = d.download('yolov3'),0
print(model_filename),0
"image_path = ""./mmdnn/conversion/examples/data/dog.jpg""",0
"model_path = ""./""",0
d = darknet_extractor(),0
"result = d.inference('yolov3', model_filename, model_path, image_path = image_path)",0
print(result),0
print(i),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result('pooling0', False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
need to be updated,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.input, self.model, self.testop = self.MainModel.KitModel(self.args.w)",0
testop = tf.get_default_graph().get_operation_by_name(layer_name),0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"'mobilenet_v1': mobilenet_v1.mobilenet_v1_arg_scope,",0
"'mobilenet_v1' : mobilenet_v1.mobilenet_v1,",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
raise NotImplementedError(),0
Load file,0
Run TensorBoard,0
print(cmd),0
Using cuDNN since vanilla RNN,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Notes for training NASNet Cifar Model,0
-------------------------------------,0
batch_size: 32,0
learning rate: 0.025,0
cosine (single period) learning rate decay,0
auxiliary head loss weighting: 0.4,0
clip global norm of all gradients by 5,0
600 epochs with a batch size of 32,0
This is used for the drop path probabilities since it needs to increase,0
the drop out probability over the course of training.,0
Notes for training large NASNet model on ImageNet,0
-------------------------------------,0
batch size (per replica): 16,0
learning rate: 0.015 * 100,0
learning rate decay factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 100 replicas,0
auxiliary head loss weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Notes for training the mobile NASNet ImageNet model,0
-------------------------------------,0
batch size (per replica): 32,0
learning rate: 0.04 * 50,0
learning rate scaling factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 50 replicas,0
auxiliary head weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Shape of feature map before the final layer.,0
149 x 149 x 32,0
Run the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Find where to place the reduction cells or stride normal cells,0
Setup for building in the auxiliary head.,0
Run the cells,0
true_cell_num accounts for the stem cells,0
Final softmax layer,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80,0
71 x 71 x 192,0
35 x 35 x 192,0
35 x 35 x 320,0
TODO(alemi): Register intermediate endpoints,1
"17 x 17 x 1088 if output_stride == 8,",0
33 x 33 x 1088 if output_stride == 16,0
TODO(alemi): register intermediate endpoints,1
TODO(gpapan): Properly support output_stride for the rest of the net.,1
8 x 8 x 2080,0
TODO(alemi): register intermediate endpoints,1
8 x 8 x 1536,0
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which",1
can be set to False to disable pooling here (as in resnet_*()).,0
Set weight_decay for weights in conv2d and fully_connected layers.,0
Set activation_fn and parameters for batch_norm.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
=============================================================================,0
Tensorflow mandates these.,0
Conv and DepthSepConv namedtuple define layers of the MobileNet architecture,0
Conv defines 3x3 convolution layers,0
DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.,0
stride is the stride of the convolution,0
depth is the number of channels or filters in a layer,0
_CONV_DEFS specifies the MobileNet body,0
Used to find thinned depths for each layer.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
By passing filters=None,0
separable_conv2d produces only a depthwise convolution layer,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 1024,0
Set weight_decay for weights in Conv and DepthSepConv layers.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Skip path 1,0
Skip path 2,0
"First pad with 0's on the right and bottom, then shift the filter to",0
include those 0's that were added.,0
Concat and apply BN,0
Set the prev layer to the current layer if it is none,0
Check to be sure prev layer stuff is setup correctly,0
Apply conv operations,0
Combine hidden states using 'add'.,0
Add hiddenstate to the list of hiddenstates we can choose from,0
Dont stride if this is not one of the original hiddenstates,0
"Check if a stride is needed, then use a strided 1x1 here",0
Determine if a reduction should be applied to make the number of,0
filters match.,0
Return the concat of all the states,0
Scale keep prob by layer number,0
The added 2 is for the reduction cells,0
Decrease the keep probability over time,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Inception-Resnet-A,0
Inception-Resnet-B,0
Inception-Resnet-C,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
force in-place updates of mean and variance estimates,0
Moving averages ends up in the trainable variables collection,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80,0
71 x 71 x 192,0
35 x 35 x 256,0
5 x Inception-resnet-A,0
Reduction-A,0
10 x Inception-Resnet-B,0
Reduction-B,0
5 x Inception-Resnet-C,0
pylint: disable=no-member,0
"net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None,",0
"scope='Bottleneck', reuse=False)",0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
Only insert rate to params if rate > 1.,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
import self.model,0
self.model,0
how the model can not load from `***.bin`,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
pylint: disable=line-too-long,0
pylint: enable=line-too-long,0
refer to https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/v2/tests/test_rnn_layer.py#L35,0
refer to https://github.com/PaddlePaddle/Paddle/issues/7403,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
refer to https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/v2/tests/test_rnn_layer.py#L35,0
refer to https://github.com/PaddlePaddle/Paddle/issues/7403,0
the channel of input feature is 3,0
"depth should be one of 20, 32, 44, 56, 110, 1202",0
Copyright (c) Microsoft. All rights reserved.,0
Licensed under the MIT license. See LICENSE.md file in the project root,0
for full license information.,0
==============================================================================,0
"self.model, self.testop = self.MainModel.KitModel(self.args.w)",0
"self.print_intermediate_result(None, False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
pylint: disable=line-too-long,0
pylint: enable=line-too-long,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.anchors = np.array([[10,13], [16,30],[33,23],[30,61],[62,45], [59,119],[116,90],[156,198],[373,326]])",0
print(output),0
"print(out_boxes, out_scores, out_classes)",0
get random colors,0
My kingdom for a good redistributable image drawing library.,0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
yolov3 80 classes,0
"Reshape to batch, height, width, num_anchors, box_params.",0
Adjust preditions to each spatial grid point and anchor size.,0
Note: YOLO iterates over height index before width index.,0
"TODO: It works with +1, don't know why.",1
Scale boxes back to original image shape.,0
"print(""feats,anchors, num_classes, input_shape"", feats, anchors, num_classes, input_shape)",0
"print(box_xy, box_wh, box_confidence, box_class_probs)",0
"yolo_outputs order 13,26,52",0
TODO: use keras backend instead of tf.,1
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
save network structure as JSON,0
layer_name = 'block2_pool',0
"intermediate_layer_model = keras.Model(inputs=model.input,",0
outputs=model.get_layer(layer_name).output),0
intermediate_output = intermediate_layer_model.predict(img),0
print (intermediate_output),0
print (intermediate_output.shape),0
"print (""%.30f"" % np.sum(intermediate_output))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Convert network architecture only,0
MXNet,0
Caffe,0
For CoreML,0
Caffe,0
Darknet,0
"exts = ['.pb', '.npy', '.py']",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check non-sense always input module.Module,0
if not (type(model) == mx.module.Module,0
or type(model) == mx.module.SequentialModule,0
or type(model) == mx.model),0
"raise TypeError(""MXNet layer of type %s is not supported."" % type(model))",0
"if layer[""op""] == ""null"":",0
continue,0
"raise NotImplementedError(""Cannot support multi-input"")",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"mxnet-cpu only support channel first, default convert the model and weight as channel first",0
We use the real_name for specifying the input layer in data_names,0
since MXNet API wants the actual name of the layer. On the other,0
"hand, the module API wants the last symbol in the symbol chain, so",0
for the output node we need to use the actual python variable name,0
of the last layer (real_variable_name).,0
download and show the image,0
"convert into format (batch, RGB, width, height)",0
"to show the image, change the argument show into True",0
compute the predict probabilities,0
print the top-5,0
# call function predict,0
"with open('synset.txt', 'r') as f:",0
labels = [l.rstrip() for l in f],0
"predict(model, labels, 'http://writm.com/wp-content/uploads/2016/08/Cat-hd-wallpapers.jpg')",0
"print(""Warning: MXNet Convolution Layer pad does not match IR Convolution Layer pad"")",0
"layout = IR_node.IR_layer.attr[""data_format""].s",0
if layout not in MXNetEmitter.channels_last:,0
Add LeakyReLU Elu(slope not support),0
"axis = IR_node.IR_layer.attr[""axis""].i",0
"self.output_weights[IR_node.name + ""_gamma""] = np.multiply(weight_dict['scale'], weight_dict_scale['scale'])",0
"self.output_weights[IR_node.name + ""_beta""] = np.multiply(weight_dict['bias'], weight_dict_scale['scale']) + weight_dict_scale['bias']",0
not supported yet,0
"axis = IR_node.IR_layer.attr[""axis""].i",0
not supported yet,0
"axis = IR_node.IR_layer.attr[""axis""].i",0
not supported yet,0
"print(""Warning: MXNet Pooling Layer pad does not match IR Pooling Layer pad"")",0
"def emit_ConvTranspose(self, IR_node):",0
if self.weight_loaded:,0
weight_dict = self.weights[IR_node.name],0
weights = weight_dict['weights'],0
"dim = len(IR_node.IR_layer.attr[""kernel_shape""].list.i) - 2",0
kernel = list(),0
"for idx in range(0, dim):",0
"kernel.append(IR_node.IR_layer.attr[""kernel_shape""].list.i[idx])",0
stride = list(),0
"for e in IR_node.IR_layer.attr[""strides""].list.i[1:-1]:",0
stride.append(e),0
dilate = list(),0
"for e in IR_node.IR_layer.attr[""dilations""].list.i[1:-1]:",0
dilate.append(e),0
"dilate = ', '.join('%s' % i for i in dilate)",0
defuse_pad = False,0
pad = list(),0
"if ""pads"" in IR_node.IR_layer.attr:",0
output_shape = list(),0
"for e in IR_node.IR_layer.attr[""_output_shapes""].list.shape[0].dim:",0
output_shape.append(e.size),0
"# print(""Warning: MXNet Deconvolution Layer pad does not match IR Deconvolution Layer pad"")",0
"defuse_pad, pad = MXNetEmitter.transfer_pad(IR_node.IR_layer.attr[""pads""].list.i)",0
"pad = ', '.join('%s' % i for i in pad)",0
"kernel = ', '.join('%s' % i for i in kernel)",0
"stride = ', '.join('%s' % i for i in stride)",0
"num_filter = IR_node.IR_layer.attr[""kernel_shape""].list.i[-2]",0
"no_bias = not IR_node.IR_layer.attr[""use_bias""].b",0
if not no_bias and self.weight_loaded:,0
"self.output_weights[IR_node.replace_scope(IR_node.name) + ""_bias""] = weight_dict['bias']",0
"# layout = IR_node.IR_layer.attr[""data_format""].s",0
if dim == 1:,0
layout = 'NCW',0
elif dim == 2:,0
layout = 'NCHW',0
elif dim == 3:,0
layout = 'NCDHW',0
if self.weight_loaded:,0
# if layout not in MXNetEmitter.channels_last:,0
"weights = MXNetEmitter.transpose(weights, dim)",0
"self.output_weights[IR_node.replace_scope(IR_node.name) + ""_weight""] = weights",0
"code = """"",0
if not defuse_pad:,0
"code = ""{:<15} = mx.sym.Deconvolution(data = {}, kernel = ({}), stride = ({}), dilate = ({}), pad = ({}), num_filter = {}, no_bias = {}, layout = '{}', name = '{}')"".format(",0
"IR_node.replace_scope(IR_node.name),",0
"IR_node.replace_scope(IR_node.in_edges[0]),",0
"kernel,",0
"stride,",0
"dilate,",0
"pad,",0
"num_filter,",0
"no_bias,",0
"layout,",0
IR_node.replace_scope(IR_node.name)),0
else:,0
"code = self.set_pad(IR_node, code, pad)",0
"code += ""\n    {:<15} = mx.sym.Deconvolution(data = {}, kernel = ({}), stride = ({}), dilate = ({}), num_filter = {}, no_bias = {}, layout = '{}', name = '{}')"".format(",0
"IR_node.replace_scope(IR_node.name), IR_node.replace_scope(IR_node.name) + ""_pad"", kernel, stride, dilate, num_filter, no_bias, layout, IR_node.replace_scope(IR_node.name))",0
return code,0
reverse cannot support yet,0
"code = ""{:<15} = mx.sym.transpose(data = {}, axes = (0, 2, 3, 1))\n"".format(""trans"", self.parent_variable_name(IR_node))",0
save the constant into weight dict,0
param_code does not need parameter slice.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Not support yet,0
"""softrelu""  : ""SoftReLU""",0
Load the model network and weights,0
"digraph = mx.viz.plot_network(sym, save_format='jpg') # For debugging",0
digraph.render(),0
model.bind(data_shapes = data_shapes),0
model.init_params(),0
"mod.load(model_path, epoch_num)",0
return mod.get_params(),0
load the model network,0
adjust the data format,0
load model files into MXNet graph,0
data_shape arguments added to calculate infer_shape(required),0
"if isinstance(input_arg, basestring):",0
Build network graph,0
transpose to channel last,0
"name, op",0
input edge,0
output shape,0
input edge,0
attr,0
"print(""Warning: MXNet symbol pad does not support channel last"")",0
"IR_node.attr[""pads""].list.i.extend([0, 0])",0
for e in pad:,0
"IR_node.attr[""pads""].list.i.extend([e, e])",0
"IR_node.attr[""pads""].list.i.extend([0, 0])",0
"name, op",0
input edge,0
units,0
use bias (no_bias default = False),0
weights,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [NM]"".format(source_node.name, self.data_format))",0
input edge,0
output shape,0
kernel_shape,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [{}]"".format(source_node.name, self.data_format, layout))",0
groups,0
in_channel = in_channel // group,0
use_bias (no_bias default = False),0
strides,0
dilations,0
data_format,0
padding,0
weights,0
axis,0
scale,0
epsilon,0
momentum,0
weights,0
gamma,0
beta,0
mean,0
var,0
"name, op",0
input edge,0
pooling type (sum not allowed yet),0
strides,0
kernel_shape,0
padding,0
output shape,0
"name, op",0
input edge,0
output shape,0
dim,0
"def rename_log_softmax(self, source_node):",0
"raise NotImplementedError(""not support yet"")",0
"def rename_Correlation(self, source_node):",0
"raise NotImplementedError(""not support yet"")",0
"name, op",0
input edge,0
padding,0
output shape,0
kernel_shape,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [{}]"".format(source_node.name, self.data_format, layout))",0
use_bias (no_bias default = False),0
strides,0
dilations,0
data_format,0
groups,0
weights,0
"def rename_RNN(self, source_node):",0
"raise NotImplementedError(""RNN not support yet"")",0
"name, op",0
input edge,0
input_dim,0
output_dim,0
dtype,0
output shape,0
"IR only support elu and prelu from {'elu', 'leaky', 'prelu', 'rrelu'}",0
return,0
"name, op",0
gamma,0
input edge,0
"alpha [exp(x) - alpha], but mxnet attr slope [slope*(exp(x) - 1)] when x < 0",0
"IR_node.attr[""alpha""].f = float()",0
output shape,0
"raise NotImplementedError(""slope cannot convert to alpha"")",0
"def rename_InstanceNorm(self, source_node):",0
raise NotImplementedError,0
"def rename_L2Normalization(self, source_node):",0
raise NotImplementedError,0
keep_prob,0
mode,0
reverse cannot support yet,0
old API target_shape not support yet,0
output shape,0
"raise NotImplementedError(""adjust output shape"")",0
dim,0
dtype,0
output shape,0
"name, op",0
input edge,0
output shape,0
axis,0
"raise NotImplementedError(""No matching IR api"")",0
"left strip the ""_"" at the beginning of the name",0
"Issue #85, #135",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
keep dims,0
axes,0
"name, op",0
epsilon,0
moving variance (var),0
print(moving_variance.name),0
gamma (scale),0
mean,0
bias,0
input node,0
output node,0
load model files into TensorFlow graph,0
extract subgraph using in_nodes and dest_nodes,0
Build network graph,0
extract subgraph using dest_nodes,0
Get input node name,0
Graph Transform,0
TODO: Fused conv and pool with padding is different from defused operators,0
strides,0
window_shape,0
pool type,0
padding,0
shape,0
strides,0
input[1] : W,0
filter,0
padding,0
output[0] : B,0
converted [dropout],0
convert [tf.contrib.layers.batch_norm],0
normal Add,0
units,0
Weights,0
FullyConnected Layer,0
"name, op",0
get Bias,0
Matmul Layer,0
Deal Dropout,0
keep prob,0
Remove nodes,0
Mul,0
Floor,0
print (source_node),0
print (source_node.layer),0
assert False,0
"def rename_RandomShuffleQueueV2(self, source_node):",0
# print(source_node.layer),0
"IR_node = self._convert_identity_operation(source_node, in_edge_count = 0, new_op = 'DataInput')",0
# IR_node.attr['shape'].shape.MergeFromString(source_node.layer.attr['_output_shapes'].list.shape[0].SerializeToString()),0
# IR_node.attr['shape'].shape.dim[0].size = -1,0
IR_node.attr['dtype'].type = self.dtype_map[source_node.layer.attr['component_types'].list.type[0]],0
paddings,0
gamma (scale),0
bias,0
Mean,0
Var,0
TODO: Current it is only for slice,1
TODO:  axis,0
strides,0
input[1] : W,0
filter,0
padding,0
output[0] : B,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
check whether flatten operator should be added,0
flatten is needed,0
"self._emit_unary_operation(IR_node, ""contrib.layers.flatten"")",0
param_code does not need parameter slice.,0
skip_prefix = [,0
"""^"",",0
"""train_op"",",0
"""save"",",0
"""gradients"",",0
"""init"",",0
"""global_step"",",0
"""distort_image"",",0
"""Adagrad"",",0
],0
"""Switch""",0
"""RequantizationRange"",",0
"""Requantize"",",0
"""Identity"",",0
"""Mean"",",0
"""Cast""",0
load model files into TensorFlow graph,0
Save it to an output file,0
keep dims,0
axes,0
ssd model is transformed,0
Ax - (Au - b),0
A,0
b,0
print(sub_content),0
epsilon,0
moving variance (var) /read,0
gamma (scale),0
print(Rsqrt.out_edges),0
beta  (bias),0
moving mean (mean),0
input node,0
epsilon,0
beta,0
gamma (scale),0
"output_node = self.get_son(source_node.name, [0, 0, 0, 0], True)",0
assert False,0
Quantized model type,0
TODO: Fused conv and pool with padding is different from defused operators,0
assert variable.get_attr('_output_shapes')[0].dim[0].size == IR_node.attr['kernel_shape'].list.i[-1],0
strides,0
window_shape,0
pool type,0
padding,0
"In facenet or other newtwork using slim.batch_norm,",0
"There are two BN(train, test) skip switch and merge.",0
epsilon,0
moving variance (var) /read,0
gamma (scale),0
beta  (bias),0
moving mean (mean),0
ReduceMean,0
keep dims,0
axes,0
Skip the node as merge,0
weights,0
Skip BiasAdd,0
weights,0
"input_node_perm = self.check_const(self.get_parent(source_node.name, [1], True))",0
paddings,0
for attr.shape >= 2,0
"For models built by slim.batch_norm, remove duplicate BN (eg.facenet)",0
TODO:  only for 1D,1
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Build network graph,0
"PaddleParser._set_output_shape(source_node, IR_node)",0
"name, op",0
input edge,0
For concat axis,0
only for training,0
"name, op",0
input edge,0
input edge,0
layer and spec,0
width <=> x or height <=> y,0
output shape,0
"name, op",0
it should be in the shape of height x width x inputchannel x outputchannel,0
use_bias: TODO,1
pad_dim,0
fail report because of auto_pad,0
if dilation_x == 1 and dilation_y == 1:,0
if output_x * stride_x == input_x and output_y * stride_y == input_y:,0
"auto_pad = ""SAME""",0
kwargs['auto_pad'] = auto_pad,0
elif output_x * stride_x == input_x - width + 1 and output_y * stride_y == input_y - height + 1:,0
"auto_pad = ""VALID""",0
kwargs['auto_pad'] = auto_pad,0
defuse the activation layer,0
"name, op",0
input edge,0
layer and spec,0
output shape,0
"channels_first, then axis = 1",0
epsilon,0
compute adjusted parameters,0
Reference: parameter transformation https://github.com/apple/coremltools/issues/153,0
convert type because of tensorflow,0
flatten,0
mean,0
var,0
defuse the activation layer,0
"name, op",0
input edge,0
layer and spec,0
assert False,0
output shape,0
pad_dim,0
padding mode,0
"If padding == ""SAME"": output_spatial_shape[i] = ceil(input_spatial_shape[i] / strides[i])",0
"If padding == ""VALID"": output_spatial_shape[i] = ceil((input_spatial_shape[i] - (spatial_filter_shape[i]-1) * dilation_rate[i]) / strides[i]).",0
"name, op",0
input edge,0
layer and spec,0
units,0
output shape,0
use_bias,0
Kit weight tranpose,0
weight: N x M -> C x H x W x M -> H x W x C x M -> N x M,0
,0
weights,0
need the shape TODO,1
only for training,0
"name, op",0
input edge,0
shape,0
"name, op",0
input edge,0
layer and spec,0
output shape,0
alpha,0
beta,0
nsize,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Build network graph,0
strides,0
window_shape,0
pool type,0
padding,0
TODO: Handle square,1
self._print_layer(source_node),0
print (source_node.name),0
"print (self.src_graph.get_parent(source_node.name, [0]).real_name)",0
assert False,0
Convolution,0
Kernel,0
Attributes,0
Bias,0
Activation,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
BlockFunction node,0
redirect the composite's inputs to the true inputs,0
"BlockFunctions are short-circuited, and not added to accum[]",0
Function node,0
OutputVariable node,0
def build(self):,0
"_traverse_graph(self, self.model.root_function)",0
"super(CntkGraph, self).build()",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"IR_node.get_attr('output_dim'),",0
alpha = alpha / size,0
??,0
print(IR_node.layer),0
assert False,0
"def emit_Unstack(self, IR_node):",0
"num_str = ""{}.shape[{}]"".format(self.parent_variable_name(IR_node), IR_node.get_attr('axis'))",0
axis = IR_node.get_attr('axis'),0
"parent_variable_shape = ""list({}.shape)"".format(self.parent_variable_name(IR_node)",0
"if self.IR_graph.get_parent(IR_node.name, [0]).type != 'Embedding'",0
else self.parent_variable_name(IR_node)+'.E'),0
if axis==1:,0
"shape_str = ""tuple([{}[0]*{}[{}], 1].extend({}[{}+1:]))"".format(",0
"parent_variable_shape,",0
"parent_variable_shape,",0
"str(axis),",0
"parent_variable_shape,",0
str(axis)),0
else:,0
"shape_str = ""tuple([{}[0]*{}[{}]].extend({}[1:{}]).append(1).extend({}[{}+1:]))"".format(",0
"parent_variable_shape,",0
"parent_variable_shape,",0
"str(axis),",0
"parent_variable_shape,",0
"str(axis),",0
"parent_variable_shape,",0
str(axis)),0
"code = ""{:<15} = cntk.reshape({}, {}, name='{}')"".format(",0
"IR_node.variable_name,",0
"self.parent_variable_name(IR_node),",0
"shape_str,",0
IR_node.variable_name),0
"code = ""{: <15} = cntk.reshape({}, {}.shape, name='{}')"".format(",0
"IR_node.variable_name,",0
"self.parent_variable_name(IR_node),",0
"self.parent_variable_name(IR_node),",0
IR_node.name,0
),0
return code,0
"def emit_Fill(self, IR_node):",0
"code = ""{:<15} = cntk.Constant({}, {}, name='{}')"".format(",0
"IR_node.variable_name,",0
"IR_node.get_attr('value'),",0
"self.parent_variable_name(IR_node),",0
IR_node.name),0
return code,0
param_code does not need parameter slice.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
For padding,0
print (input_shape),0
print (kernel_shape),0
print (strides),0
Not tested,0
print (input_shape),0
print (kernel_shape),0
print (strides),0
kernel_shape[idx] = (kernel_shape[idx] - 1) * dilation_rate + 1,0
print ([0] + up_list + [0] + down_list if data_format.startswith('NC') else up_list + [0] + down_list + [0]),0
print ('-----------------------------------------------------'),0
network library,0
not support multi-threads download,0
key: layer_name    value: keras layer,0
private functions,0
next_node_info.left_in_edges -= 1,0
"print ('{} --> {}'.format(src, dst))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
share functions,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
name --> (weight_name --> ndarray),0
Generated by the protocol buffer compiler.  DO NOT EDIT!,0
source: graph.proto,0
@@protoc_insertion_point(imports),0
@@protoc_insertion_point(class_scope:GraphDef),0
@@protoc_insertion_point(class_scope:NodeDef.AttrEntry),0
@@protoc_insertion_point(class_scope:NodeDef),0
@@protoc_insertion_point(class_scope:AttrValue.ListValue),0
@@protoc_insertion_point(class_scope:AttrValue),0
@@protoc_insertion_point(class_scope:TensorShape.Dim),0
@@protoc_insertion_point(class_scope:TensorShape),0
@@protoc_insertion_point(class_scope:LiteralTensor),0
@@protoc_insertion_point(module_scope),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
First try to read it as a binary file.,0
Next try to read it as a text file.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"If pattern.inputs is empty, skips the rest and accepts all the inputs.",0
"If order doesn't matter for the inputs, then make sure we match at least",0
one permutation of the inputs.,0
"def get_tensor(self, pattern_or_name):",0
op_tensor = self._get_op_tensor(pattern_or_name),0
return op_tensor[1] if op_tensor else None,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
"Python 3.3.2+ implements `yield from`, but for now:",0
"check the same pattern scope node whether have same inputs, outputs and weights.",0
"For those don't have, rename their scope names.",0
"clear out scope node, typically input constant node.",0
get sub_scopes,0
decline the suffix number,0
Obtain nodes where the scope name that satisfies top_level is top_scope and sub_level is sub_scope,0
Obtain nodes where the scope name that satisfies in sub_level is sub_scope,0
Obtain nodes where the scope name that satisfies in top_level is top_scope,0
Obtain all nodes grouped by sub_level sub_scope,0
cover the node,0
"store idx, node into a dict and sort it later to keep its topology sort.",0
in_node's out edges replace n_name with scope node name.,0
no out nodes means the last node in scope nodes should be returned,0
"if out_node is scope node, replace the scope node's inner topology list node.",0
the input parameter shoule be sliced when call func.,0
modify the in_edges in scope inner nodes. decline the :idx.,0
1. initilize scope node,0
2. get scope nodes' topology list.,0
3. rebuild the edges connection after folding these scope nodes into one node and,0
get this scope node's return variables.,0
4. rebuild graph.,0
RNN-related attrs.,0
get input params,0
self.store_const_to_top(result),0
"self.set_top_node_prop(result, pattern_name)",0
Do not include input op.,0
"TODO: pytorch, mxnet, keras, cntk",1
-*- coding: utf-8 -*-,0
from keras.layers.core import Layer,0
Arguments,0
Input shape,0
Output shape,0
Prepare broadcasting shape.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Load the model network,0
Load the model weights,0
load model files into Keras graph,0
Keras 2.1.6,0
Keras. 2.2.2,0
"_keras.utils.plot_model(model, ""model.png"", show_shapes = True)",0
Build network graph,0
TODO: Fused conv and pool with padding is different from defused operators,0
TODO: More activation functions,1
for ELU,0
input edge,0
"name, op",0
weights,0
pads,0
filter,0
"[kd, kh, kw, channel_size, filter number]",0
use_bias,0
strides,0
"[1, sd, sh, sw, 1]",0
dilations,0
"[1, dd, dh, dw, 1]",0
activation,0
"name, op",0
input edge,0
padding,0
strides,0
"[1, sd, sh, sw, 1]",0
window_shape,0
"[1, pd, ph, pw, 1]",0
"name, op",0
input edge,0
For concat axis,0
"name, op",0
input edge,0
padding,0
only for training,0
"name, op",0
input edge,0
"name, op",0
input edge,0
Merge Layers,0
only for training,0
"name, op",0
input edge,0
shape,0
only for training,0
"name, op",0
input edge,0
Core Layers,0
"name, op",0
input edge,0
units,0
use_bias,0
weights,0
activation,0
"name, op",0
input edge,0
"name, op",0
input edge,0
size,0
"name, op",0
input edge,0
input_dim,0
output_dim,0
mask_zero,0
weights,0
"name, op",0
input edge,0
units,0
use_bias,0
"for Keras, drop_out and recurrent_dropout",0
activation,0
"name, op",0
input edge,0
units,0
activation,0
weights,0
Kit TODO : need to search the tf,1
"name, op",0
input edge,0
for target shape,0
"name, op",0
input edge,0
axis,0
"Parameter arrangement in Keras: gamma, beta, mean, variance",0
scale,0
beta,0
mean,0
var,0
"name, op",0
input edge,0
for target shape,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
rot,0
"print(""Converting layer {}({})"".format(current_node.name, node_type))",0
TODO,1
"return input_node, 'same'",0
"assert IR_node.get_attr('group', 1) == 1",0
change dw from filters to 1,0
reset the default dilation,0
############,0
Operators #,0
############,0
"code = self._emit_merge(IR_node, ""subtract"")",0
TODO,1
"when converting from coreml model, reshape is needed after the global pooling",0
for Keras,0
It arouses some problems:,0
it can be implemented by Lambda Layer,0
https://github.com/keras-team/keras/issues/890,0
Keras == 2.1.6,0
Keras == 2.2.2,0
TODO: arguments won't be saved in keras export model,1
param_code does not need parameter slice.,0
Prepare broadcasting shape.,0
Prepare broadcasting shape.,0
"output = Lambda(lambda x: tf.fill(x, value))(input)",0
return output,0
def _layer_Constant(self):,0
"self.add_body(0, '''",0
class my_constant(keras.layers.Layer):,0
"def __init__(self, value, **kwargs):",0
"super(my_constant, self).__init__(**kwargs)",0
self._value = value,0
"# the input is dummy, just for creating keras graph.",0
"def call(self, dummy):",0
res = K.constant(self._value),0
self.output_shapes = K.int_shape(res),0
return res,0
"def compute_output_shape(self, input_shape):",0
return self.output_shapes,0
'''),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
Kit: TODO,1
Duplicate models for weight sharing,0
Expand the sub-models,0
"with open('modelmapbydataset.json', 'w') as outfile:",0
"json.dump(new_data, outfile)",0
generate makedown script,0
add Image Classification,0
add Object Detection,0
MMdnn introduction,0
steps for model conversion,0
Generate model converter description,0
Generate models list,0
generate makedown script,0
MMdnn introduction,0
steps for model conversion,0
Generate model converter description,0
Generate models list,0
console.info(JSON.parse(e.target.result)),0
"default, draw the model.json file",0
window.onload = () => {,0
"d3.json(filePath, (error, json) => {",0
if (error) throw error;,0
// console.info(json),0
draw(json),0
}),0
},0
generate dag,0
IR model or keras model,0
path generator,0
let arrowL = `M ${points[len - 1].x} ${points[len - 1].y} l -7 -7`,0
let arrowR = `M ${points[len - 1].x} ${points[len - 1].y} l  7 -7`,0
select a layer,0
remove list,0
draw,0
mode;,0
"let scale = Math.min(window.innerWidth*0.85 / width, (window.innerHeight-85)/height)",0
"var filter = def.append(""filter"")",0
".attr(""id"", ""dropshadow"")",0
"filter.append(""feGaussianBlur"")",0
".attr(""in"", ""SourceAlpha"")",0
".attr(""stdDeviation"", 4)",0
".attr(""result"", ""blur"");",0
"// filter.append(""feOffset"")",0
"//     .attr(""in"", ""blur"")",0
"//     .attr(""dx"", 2)",0
"//     .attr(""dy"", 2)",0
"//     .attr(""result"", ""offsetBlur"");",0
"var feMerge = filter.append(""feMerge"");",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""offsetBlur"")",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""SourceGraphic"");",0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
"svg.on('keydown', ()=>{console.info('ddd')})",0
".on('mouseover', ()=>{console.info('mouse over')})",0
k = parseFloat(k)*parseFloat(k_),0
"limit x, y",0
// a trick to make text svg transform in MS Edge,0
"d3.selectAll("".labels"").classed(""tempclass"", true);",0
"setTimeout(function () { d3.selectAll("".labels"").classed(""tempclass"", false); }, 40);",0
when shift is down,0
ulti function,0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
rm the '_',0
Base64 encode: https://developers.google.com/protocol-buffers/docs/proto3,0
Search the node,0
select by id: https://stackoverflow.com/questions/37270787/uncaught-syntaxerror-failed-to-execute-queryselector-on-document,0
Scroll,0
console.info(JSON.parse(e.target.result)),0
"default, draw the model.json file",0
window.onload = () => {,0
"d3.json(filePath, (error, json) => {",0
if (error) throw error;,0
// console.info(json),0
draw(json),0
}),0
},0
generate dag,0
IR model or keras model,0
path generator,0
let arrowL = `M ${points[len - 1].x} ${points[len - 1].y} l -7 -7`,0
let arrowR = `M ${points[len - 1].x} ${points[len - 1].y} l  7 -7`,0
select a layer,0
draw,0
"let scale = Math.min(window.innerWidth*0.85 / width, (window.innerHeight-85)/height)",0
"var filter = def.append(""filter"")",0
".attr(""id"", ""dropshadow"")",0
"filter.append(""feGaussianBlur"")",0
".attr(""in"", ""SourceAlpha"")",0
".attr(""stdDeviation"", 4)",0
".attr(""result"", ""blur"");",0
"// filter.append(""feOffset"")",0
"//     .attr(""in"", ""blur"")",0
"//     .attr(""dx"", 2)",0
"//     .attr(""dy"", 2)",0
"//     .attr(""result"", ""offsetBlur"");",0
"var feMerge = filter.append(""feMerge"");",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""offsetBlur"")",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""SourceGraphic"");",0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
"svg.on('keydown', ()=>{console.info('ddd')})",0
".on('mouseover', ()=>{console.info('mouse over')})",0
k = parseFloat(k)*parseFloat(k_),0
"limit x, y",0
// a trick to make text svg transform in MS Edge,0
"d3.selectAll("".labels"").classed(""tempclass"", true);",0
"setTimeout(function () { d3.selectAll("".labels"").classed(""tempclass"", false); }, 40);",0
when shift is down,0
ulti function,0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
space = '',0
Get the long description from the README file,0
Versions should comply with PEP440.  For a discussion on single-sourcing,0
"the version across setup.py and the project code, see",0
https://packaging.python.org/en/latest/single_source_version.html,0
The project's main homepage.,0
Author details,0
Choose your license,0
See https://pypi.python.org/pypi?%3Aaction=list_classifiers,0
How mature is this project? Common values are,0
3 - Alpha,0
4 - Beta,0
5 - Production/Stable,0
Indicate who your project is intended for,0
"Pick your license as you wish (should match ""license"" above)",0
"Specify the Python versions you support here. In particular, ensure",0
"that you indicate whether you support Python 2, Python 3 or both.",0
What does your project relate to?,0
You can just specify the packages manually here if your project is,0
simple. Or you can use find_packages().,0
"Alternatively, if you want to distribute just a my_module.py, uncomment",0
this:,0
"py_modules=[""my_module""],",0
List run-time dependencies here.  These will be installed by pip when,0
"your project is installed. For an analysis of ""install_requires"" vs pip's",0
requirements files see:,0
https://packaging.python.org/en/latest/requirements.html,0
"To provide executable scripts, use entry points in preference to the",0
"""scripts"" keyword. Entry points provide cross-platform support and allow",0
pip to create the appropriate form of executable for the target platform.,0
Communication to TensorFlow server via gRPC,0
TensorFlow serving stuff to send messages,0
Command line arguments,0
Send request,0
request.inputs['input'].CopyFrom(),0
"TestModels.onnx_emit,",0
"TestModels.caffe_emit,",0
"TestModels.cntk_emit,",0
"TestModels.coreml_emit,",0
"TestModels.mxnet_emit,",0
"TestModels.pytorch_emit,",0
TestModels.tensorflow_emit,0
"TestModels.caffe_emit,",0
"TestModels.coreml_emit,",0
"TestModels.keras_emit,",0
"TestModels.mxnet_emit,",0
Run too slow on Travis.,0
cntk_emit OOM on Travis,0
"TestModels.cntk_emit,",0
"TestModels.cntk_emit,",0
Cannot run on Travis since it seems to consume too much memory.,0
"TestModels.onnx_emit,",0
"TestModels.caffe_emit,",0
"TestModels.cntk_emit,",0
"TestModels.coreml_emit,",0
"TestModels.keras_emit,",0
"TestModels.keras_emit,",0
"TestModels.caffe_emit,",0
"TestModels.cntk_emit,",0
"TestModels.pytorch_emit,",0
"TestModels.caffe_emit,",0
"TestModels.cntk_emit,",0
"TestModels.pytorch_emit,",0
"TestModels.caffe_emit,",0
"TestModels.cntk_emit,",0
"TestModels.pytorch_emit,",0
"TestModels.caffe_emit,",0
"TestModels.cntk_emit,",0
"TestModels.onnx_emit,",0
"TestModels.caffe_emit,",0
"TestModels.cntk_emit,",0
"TestModels.caffe_emit,",0
"TestModels.cntk_emit,",0
"TestModels.coreml_emit,",0
"TestModels.keras_emit,",0
"TestModels.mxnet_emit,",0
"TestModels.pytorch_emit,",0
TestModels.tensorflow_emit,0
omit tensorflow lead to crash,0
"TestModels.cntk_emit,",0
"TestModels.cntk_emit,",0
"TestModels.caffe_emit,",0
"TestModels.cntk_emit,",0
"TestModels.keras_emit,",0
"TestModels.mxnet_emit,",0
"TestModels.pytorch_emit,",0
TestModels.tensorflow_emit,0
"TestModels.cntk_emit,",0
"Function self.assertEquals has deprecated, change to assertEqual",0
"self.assertEqual(original_predict.shape, converted_predict.shape)",0
"TestModels.caffe_emit,",0
"TestModels.cntk_emit,",0
"TestModels.mxnet_emit,",0
"TestModels.pytorch_emit,",0
"TestModels.onnx_emit,",0
"TestModels.caffe_emit,",0
"TestModels.cntk_emit,",0
"TestModels.mxnet_emit,",0
"TestModels.pytorch_emit,",0
"TestModels.onnx_emit,",0
"TestModels.cntk_emit,",0
"TestModels.mxnet_emit,",0
"TestModels.onnx_emit,",0
"TestModels.cntk_emit,",0
get original model prediction result,0
original to IR,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
print(original_predict),0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
get shape,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
IR to code,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
The list is in RGB oder,0
IR to Model,0
"converted_file = original_framework + '_coreml_' + architecture_name + ""_converted""",0
"converted_file = converted_file.replace('.', '_')",0
load model,0
save model,0
"coremltools.utils.save_spec(model.get_spec(), converted_file)",0
inference,0
IR to code,0
import converted model,0
"In case of odd number add the extra padding at the end for SAME_UPPER(eg. pads:[0, 2, 2, 0, 0, 3, 3, 0]) and at the beginning for SAME_LOWER(eg. pads:[0, 3, 3, 0, 0, 2, 2, 0])",0
"'xception'     : [onnx_emit],",0
"'nasnet'       : [onnx_emit],",0
"Temporarily disable 'yolo2'        : [onnx_emit],",0
"Temporarily disable 'inception_v4'  : [onnx_emit],",0
"'voc-fcn8s'     : [onnx_emit], # TODO: ConvTranspose, Crop",0
"'voc-fcn16s'    : [onnx_emit], # TODO: ConvTranspose, Crop",0
"'voc-fcn32s'    : [onnx_emit], # TODO: ConvTranspose, Crop",0
"Temporarily disable 'xception'      : [onnx_emit],",0
"Temporarily disable 'facenet'               : [onnx_emit],",0
"'resnet_v1_50'          : [onnx_emit], # POOL: strides > window_shape not supported due to inconsistency between CPU and GPU implementations",0
"'resnet_v1_152'         : [onnx_emit], # POOL: strides > window_shape not supported due to inconsistency between CPU and GPU implementations",0
"'resnet_v2_50'          : [onnx_emit], # POOL: strides > window_shape not supported due to inconsistency between CPU and GPU implementations",0
"'resnet_v2_152'         : [onnx_emit], # POOL: strides > window_shape not supported due to inconsistency between CPU and GPU implementations",0
"'nasnet-a_large'        : [onnx_emit], # POOL: strides > window_shape not supported due to inconsistency between CPU and GPU implementations",0
"Temporarily disable 'facenet'           : [onnx_emit],",0
TODO: coredump,1
"'alexnet'       : [cntk_emit, keras_emit, tensorflow_emit],",0
"'nasnet'       : [tensorflow_emit, keras_emit, coreml_emit],",0
"Temporarily disable 'yolo2'        : [keras_emit],",0
"'facenet'      : [tensorflow_emit, coreml_emit,mxnet_emit,keras_emit]  # TODO",0
"Temporarily disable 'inception_v4'  : [cntk_emit, coreml_emit, keras_emit, pytorch_emit, tensorflow_emit], # TODO mxnet_emit(Small error), caffe_emit(Crash for shape)",1
"Temporarily disable 'xception'      : [coreml_emit, cntk_emit, mxnet_emit, pytorch_emit, tensorflow_emit], #  TODO: Caffe(Crash) keras_emit(too slow)",1
"Temporarily disable 'facenet'               : [mxnet_emit, tensorflow_emit, keras_emit, pytorch_emit, caffe_emit], # TODO: coreml_emit",0
"Temporarily disable 'rnn_lstm_gru_stacked'  : [tensorflow_emit, keras_emit, pytorch_emit, mxnet_emit] #TODO cntk_emit",0
"Temporarily disable 'facenet'           : [mxnet_emit, tensorflow_emit, keras_emit, caffe_emit] # TODO: coreml_emit",0
"'tinyyolo'     : [coreml_emit, keras_emit, mxnet_emit, pytorch_emit, tensorflow_emit],",0
"'vgg16': [tensorflow_emit],",0
'alexnet': [tensorflow_emit],0
get test input path,0
get original model prediction result,0
def test_caffe(self):,0
try:,0
import caffe,0
"self._test_function('caffe', self.caffe_parse)",0
except ImportError:,0
"print('Please install caffe! Or caffe is not supported in your platform.', file=sys.stderr)",0
def test_cntk(self):,0
try:,0
import cntk,0
"self._test_function('cntk', self.cntk_parse)",0
except ImportError:,0
"print('Please install cntk! Or cntk is not supported in your platform.', file=sys.stderr)",0
def test_coreml(self):,0
from coremltools.models.utils import macos_version,0
"if macos_version() < (10, 13):",0
"print('Coreml is not supported in your platform.', file=sys.stderr)",0
else:,0
"self._test_function('coreml', self.coreml_parse)",0
def test_keras(self):,0
"self._test_function('keras', self.keras_parse)",0
def test_mxnet(self):,0
"self._test_function('mxnet', self.mxnet_parse)",0
def test_darknet(self):,0
"self._test_function('darknet', self.darknet_parse)",0
def test_paddle(self):,0
# omit tensorflow lead to crash,0
import tensorflow as tf,0
try:,0
import paddle.v2 as paddle,0
"self._test_function('paddle', self.paddle_parse)",0
except ImportError:,0
"print('Please install Paddlepaddle! Or Paddlepaddle is not supported in your platform.', file=sys.stderr)",0
def test_pytorch(self):,0
"self._test_function('pytorch', self.pytorch_parse)",0
def test_tensorflow(self):,0
"self._test_function('tensorflow', self.tensorflow_parse)",0
def test_tensorflow_frozen(self):,0
"self._test_function('tensorflow_frozen', self.tensorflow_frozen_parse)",0
"TestModels.onnx_emit,",0
"TestModels.cntk_emit,",0
"TestModels.onnx_emit,",0
"TestModels.cntk_emit,",0
"TestModels.coreml_emit,",0
"TestModels.onnx_emit,",0
"TestModels.cntk_emit,",0
"Temporarily disable 'xception'      : [TestModels.coreml_emit, TestModels.cntk_emit, TestModels.tensorflow_emit],",0
"TestModels.onnx_emit,",0
"TestModels.cntk_emit,",0
Cannot run on Travis since it seems to consume too much memory.,0
"Temporarily disable 'xception'      : [TestModels.mxnet_emit, TestModels.pytorch_emit],",0
"Temporarily disable 'inception_v4'  : [TestModels.cntk_emit, TestModels.coreml_emit, TestModels.keras_emit, TestModels.pytorch_emit, TestModels.tensorflow_emit],",0
"TestModels.onnx_emit,",0
"TestModels.caffe_emit,",0
"TestModels.cntk_emit,",0
"TestModels.coreml_emit,",0
"TestModels.mxnet_emit,",0
"TestModels.pytorch_emit,",0
TestModels.tensorflow_emit,0
"TestModels.onnx_emit,",0
"TestModels.caffe_emit,",0
"TestModels.cntk_emit,",0
"TestModels.coreml_emit,",0
"TestModels.mxnet_emit,",0
"TestModels.pytorch_emit,",0
TestModels.tensorflow_emit,0
"TestModels.onnx_emit,",0
"TestModels.cntk_emit,",0
"TestModels.onnx_emit,",0
"TestModels.cntk_emit,",0
!/usr/bin/python,0
major python major_python_versions as python2 and python3,0
operating system,0
do not deal,0
I don't want to deal with auto_pad,0
Don't support auto_pad current!,0
2018-02-28,0
if group is None:,0
group = 1,0
group is not support yet too.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
node,0
input,0
edge(node a <-> node b),0
,0
"key is edge name, value is src/dst node name",0
"key is initializer name, value is TensorProto",0
print(name),0
print(layer.op_type),0
n is input data,0
n is input edge,0
omit node of some type,0
self.nodes.append(IR_node.variable_name + '_bias'),0
self.nodes.append(IR_node.variable_name + '_weight'),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"if node len(in_edges), generate additional DataInput node",0
print,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
TODO,1
"'max_pool2d': convert_maxpool,",0
"'onnx::Mul': convert_elementwise_mul,",0
"'onnx::Sub': convert_elementwise_sub,",0
"'onnx::ConvTranspose': convert_convtranspose,",0
"'onnx::LeakyRelu': convert_lrelu,",0
"'onnx::Sigmoid': convert_sigmoid,",0
"'onnx::Softmax': convert_softmax,",0
"'onnx::Selu': convert_selu,",0
"'onnx::Transpose': convert_transpose,",0
"'onnx::Reshape': convert_reshape,",0
"'onnx::MatMul': convert_matmul,",0
"'onnx::Gather': convert_gather,",0
"'onnx::ReduceSum': convert_reduce_sum,",0
"'onnx::Constant': convert_constant,",0
"'onnx::Upsample': convert_upsample,",0
"'onnx::Pad': convert_padding,",0
###########,0
property #,0
###########,0
###################,0
Public Functions #,0
###################,0
test,0
cpu: https://github.com/pytorch/pytorch/issues/5286,0
Build network graph,0
"(batch, C, H, W)  & NHWC",0
#########,0
Layers #,0
#########,0
dilation,0
handle bias,0
TODO,1
output_shape,0
epsilon,0
mean,0
var,0
Kit weight tranpose,0
weight: N x M -> C x H x W x M -> H x W x C x M -> N x M,0
,0
weights,0
use_bias,0
units,0
handle weight,0
handle bias,0
###################,0
Helper Functions #,0
###################,0
print (dir(data)),0
print (i),0
"print (""    {} with shape {}"".format(j, load_weight[i][j].shape))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Scopes created in a nested scope may have initial characters,0
that are illegal as the initial character of an op name,0
"(viz. '-', '\', '/', and '_').",0
sanity check.,0
run dce first to eliminate dead parts of the graph that might have been,0
left behind by things like symbolic_override,0
construct graph,0
nodes,0
input layer,0
TODO,1
build each layer,0
input,0
"print(node_input_name ,'->', node_name)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Base Functions,0
"padding,",0
"exstr = "", value=float('-Inf')""",0
"exstr = """"",0
Change to padding defuse,0
"input_node = self._defuse_padding(IR_node, exstr)",0
Change to padding defuse,0
Ignore it in Pytorch,0
for Keras,0
"self.add_init(2, ""self.{:<15} = torch.from_numpy(__weights_dict['{}']['value'])"".format(",0
"IR_node.variable_name,",0
IR_node.name)),0
param_code does not need parameter slice.,0
from torch.nn.parameter import Parameter,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self._type = layer.__class__.__name__.replace('Backward', '')",0
"self._name = ""{}_{}"".format(self.type, id)",0
TODO,1
"""""""",0
build graph for pytorch 0.2.0,0
"""""""",0
dummy_input = torch.autograd.Variable(torch.randn(shape)),0
output_node = self.model(dummy_input),0
search_queue = [output_node.grad_fn],0
"tmp_node = PyTorchGraphNode(output_node.grad_fn, 0)",0
self.layer_map[tmp_node.name] = tmp_node,0
visited = {output_node.grad_fn : self.layer_map[tmp_node.name]},0
idx = 0,0
node_count = 1,0
while (idx < len(search_queue)):,0
current_node = search_queue[idx],0
current_type = visited[current_node].type,0
"if hasattr(current_node, 'next_functions'):",0
"for parent, _ in current_node.next_functions:",0
"parent_type = parent.__class__.__name__.replace('Backward', '')",0
if parent_type != 'AccumulateGrad' and \,0
(parent_type != 'Transpose' or current_type != 'Addmm'):,0
if not parent in visited:,0
"tmp_node = PyTorchGraphNode(parent, node_count)",0
self.layer_map[tmp_node.name] = tmp_node,0
node_count += 1,0
visited[parent] = tmp_node,0
search_queue.append(parent),0
"self._make_connection(visited[parent].name, visited[current_node].name)",0
idx += 1,0
try:,0
return TorchGraph._forward_torch_random_input(,0
"torch_model,",0
"input_shapes,",0
is_batch=False,0
),0
except:,0
# try batch mode,0
# return TorchGraph._forward_torch_random_input(,0
"#     torch_model,",0
"#     input_shapes,",0
#     is_batch=True,0
# ),0
pass,0
tensor = torch.rand(*shape).float(),0
multi output,0
single output,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
###########,0
property #,0
###########,0
###################,0
Public Functions #,0
###################,0
Build network graph,0
#########,0
Layers #,0
#########,0
handle weight,0
handle bias,0
handle weight,0
handle bias,0
###################,0
Helper Functions #,0
###################,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
load model file into Coreml Graph,0
model.encode() convert to str --- python2 may crash due to type 'unicode',0
Build Network Graph,0
self.data_format ? TODO,1
load the model network,0
convert to Model_pb2.Model,0
determine the type of the current_node,0
staticmethods,0
"(seq, batch, C, H, W)  & NHWC",0
TODO dtype_map,1
##### convert methods,0
convolution,0
input edge,0
important!,0
reshape the weight!,0
"[2, 3, 0, 1]",0
pads,0
use_bias,0
isDeconvolution,0
"name, op",0
kwargs['kernel_shape'] = weights.shape,0
strides,0
"[1, sd, sh, sw, 1]",0
activation,0
TODO,1
padding in conv,0
"pad in IR is [x1_b, x2_b, ..., x1_e, x2_e, ...]",0
padding,0
compute padding for 'same',0
https://discuss.mxnet.io/t/pooling-and-convolution-with-same-mode/528/3,0
https://www.jianshu.com/p/05c4f1621c7e,0
padding in pooling,0
padding,0
https://discuss.mxnet.io/t/pooling-and-convolution-with-same-mode/528/3,0
TODO,1
symmetric padding,0
"name, op",0
input edge,0
For concat axis,0
"NO axis in coreml, so set the last axis",0
"The first -1 means in coreml there is one-more axis,",0
The second -1 means the last axis,0
"name, op",0
input edge,0
padding,0
Future Module TODO,1
#### rename methods,0
"name, op",0
activation type,0
else:,0
assert False,0
input edge,0
Merge layers,0
only for training,0
"name, op",0
input edge,0
shape,0
NHWC channel last,0
"in fact, here is NWHC",0
"name, op",0
input edge,0
axis TODO,0
"channels_first, then axis = 1",0
scale,0
bias,0
epsilon,0
compute adjusted parameters,0
Reference: parameter transformation https://github.com/apple/coremltools/issues/153,0
convert type because of tensorflow,0
mean,0
var,0
"name, op",0
input edge,0
bias,0
"name, op",0
input edge,0
"MAX = 0, AVERAGE = 1, L2 = 2",0
padding,0
strides,0
"[1, sd, sh, sw, 1]",0
window_shape,0
"[1, pd, ph, pw, 1]",0
"name, op",0
input edge,0
"name, op",0
input edge,0
"name, op",0
input edge,0
units,0
use_bias,0
weights,0
change to single because of the tf matmul,0
in features,0
"name, op",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
the end of the tensorflow_resnet_v2_50's squeeze shape is [unknown_rank: true] with len 0,0
1001 means the 1001 classes for tensorflow_resnet_v2_50,0
!Alert! TODO,1
Future implement can be changed to the last two layer,0
TODO - remove style transfer 1D hack,1
"Input is 1D but it goes to the width dimension: (1,1,W)",0
"assume (Batch, Channels) - Batch dimension should be dropped",0
"assume (Batch, Sequence-Length, channels)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Add classifier classes (if applicable),0
Set pre-processing paramsters,0
"image_input_names,",0
Return the protobuf spec,0
model = _MLModel(self.builder.spec),0
if IR_node.name in self.weights_dict and 'weights' in self.weights_dict[IR_node.name]:,0
pass,0
"self._emit_merge(IR_node,'DOT')",0
Get input and output names,0
Dimensions and weights,0
"W should have shape (height, width, kernel_channels, output_channels), where kernel_channel = input_channels / groups",0
Dilations,0
see protobuf,0
assert False,0
Get the weights.,0
Dilations,0
see protobuf,0
depth-wise convolution,0
Dilations,0
see protobuf,0
Get input and output names,0
Pooling layer type,0
"if it's global, set the global flag",0
TODO  global pooling modification,0
Padding,0
see protobuf,0
Padding,0
see protobuf,0
Get input and output names,0
"type: ""list(int). A 1-D values of (leftBorder, topBorder, rightBorder, bottomBorder).""",0
This is central crop,0
Now add the layer,0
"Allowed values: 'CHW', 'HW', 'C', 'H', 'W'",0
Get input and output names,0
Get input and output names,0
Get the weights from keras,0
Get input and output names,0
blob_order == 0 if the input blob needs not be rearranged,0
blob_order == 1 if the input blob needs to be rearranged,0
"using keras_layer.input.shape have a ""?"" (Dimension[None] at the front),",0
making a 3D tensor with unknown batch size 4D,0
Get input and output names,0
activation emit,0
Get input and output names,0
negate it,0
apply threshold,0
negate it back,0
for Keras,0
Get input and output names,0
Set parameters,0
"Parameter arrangement in Keras: gamma, beta, mean, variance",0
compute adjusted parameters,0
Reference: parameter transformation https://github.com/apple/coremltools/issues/153,0
padding type TODO,0
"Type of the padding. Can be one of 'constant', 'reflection' or 'replication",0
Now add the layer,0
self.emit_Flatten(IR_node),0
Get the weights,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
input layer,0
build each layer,0
"if A.output == B.input, then make the connection: A -> B",0
"print('{0:20}->     {1:20}'.format(layerA.name, layerB.name))",0
"if A.name == B.input, then make the connection: A -> B, here A is the input",0
build each layer,0
"if A.output == B.input, then make the connection: A -> B",0
"print('{0:20}->     {1:20}'.format(layerA.name, layerB.name))",0
"if A.name == B.input, then make the connection: A -> B, here A is the input",0
build each layer,0
"if A.output == B.input, then make the connection: A -> B",0
"print('{0:20}->     {1:20}'.format(layerA.name, layerB.name))",0
"if A.name == B.input, then make the connection: A -> B, here A is the input",0
The information of the layer,0
Generated by the protocol buffer compiler.  DO NOT EDIT!,0
source: caffe.proto,0
@@protoc_insertion_point(imports),0
@@protoc_insertion_point(class_scope:caffe.BlobShape),0
@@protoc_insertion_point(class_scope:caffe.BlobProto),0
@@protoc_insertion_point(class_scope:caffe.BlobProtoVector),0
@@protoc_insertion_point(class_scope:caffe.Datum),0
@@protoc_insertion_point(class_scope:caffe.FillerParameter),0
@@protoc_insertion_point(class_scope:caffe.NetParameter),0
@@protoc_insertion_point(class_scope:caffe.SolverParameter),0
@@protoc_insertion_point(class_scope:caffe.SolverState),0
@@protoc_insertion_point(class_scope:caffe.NetState),0
@@protoc_insertion_point(class_scope:caffe.NetStateRule),0
@@protoc_insertion_point(class_scope:caffe.ParamSpec),0
@@protoc_insertion_point(class_scope:caffe.LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.TransformationParameter),0
@@protoc_insertion_point(class_scope:caffe.LossParameter),0
@@protoc_insertion_point(class_scope:caffe.AccuracyParameter),0
@@protoc_insertion_point(class_scope:caffe.ArgMaxParameter),0
@@protoc_insertion_point(class_scope:caffe.ConcatParameter),0
@@protoc_insertion_point(class_scope:caffe.BatchNormParameter),0
@@protoc_insertion_point(class_scope:caffe.BiasParameter),0
@@protoc_insertion_point(class_scope:caffe.ContrastiveLossParameter),0
@@protoc_insertion_point(class_scope:caffe.ConvolutionParameter),0
@@protoc_insertion_point(class_scope:caffe.CropParameter),0
@@protoc_insertion_point(class_scope:caffe.DataParameter),0
@@protoc_insertion_point(class_scope:caffe.DropoutParameter),0
@@protoc_insertion_point(class_scope:caffe.DummyDataParameter),0
@@protoc_insertion_point(class_scope:caffe.EltwiseParameter),0
@@protoc_insertion_point(class_scope:caffe.ELUParameter),0
@@protoc_insertion_point(class_scope:caffe.EmbedParameter),0
@@protoc_insertion_point(class_scope:caffe.ExpParameter),0
@@protoc_insertion_point(class_scope:caffe.FlattenParameter),0
@@protoc_insertion_point(class_scope:caffe.HDF5DataParameter),0
@@protoc_insertion_point(class_scope:caffe.HDF5OutputParameter),0
@@protoc_insertion_point(class_scope:caffe.HingeLossParameter),0
@@protoc_insertion_point(class_scope:caffe.ImageDataParameter),0
@@protoc_insertion_point(class_scope:caffe.InfogainLossParameter),0
@@protoc_insertion_point(class_scope:caffe.InnerProductParameter),0
@@protoc_insertion_point(class_scope:caffe.InputParameter),0
@@protoc_insertion_point(class_scope:caffe.LogParameter),0
@@protoc_insertion_point(class_scope:caffe.LRNParameter),0
@@protoc_insertion_point(class_scope:caffe.MemoryDataParameter),0
@@protoc_insertion_point(class_scope:caffe.MVNParameter),0
@@protoc_insertion_point(class_scope:caffe.ParameterParameter),0
@@protoc_insertion_point(class_scope:caffe.PoolingParameter),0
@@protoc_insertion_point(class_scope:caffe.PowerParameter),0
@@protoc_insertion_point(class_scope:caffe.PythonParameter),0
@@protoc_insertion_point(class_scope:caffe.RecurrentParameter),0
@@protoc_insertion_point(class_scope:caffe.ReductionParameter),0
@@protoc_insertion_point(class_scope:caffe.ReLUParameter),0
@@protoc_insertion_point(class_scope:caffe.ReshapeParameter),0
@@protoc_insertion_point(class_scope:caffe.ScaleParameter),0
@@protoc_insertion_point(class_scope:caffe.SigmoidParameter),0
@@protoc_insertion_point(class_scope:caffe.SliceParameter),0
@@protoc_insertion_point(class_scope:caffe.SoftmaxParameter),0
@@protoc_insertion_point(class_scope:caffe.TanHParameter),0
@@protoc_insertion_point(class_scope:caffe.TileParameter),0
@@protoc_insertion_point(class_scope:caffe.ThresholdParameter),0
@@protoc_insertion_point(class_scope:caffe.WindowDataParameter),0
@@protoc_insertion_point(class_scope:caffe.SPPParameter),0
@@protoc_insertion_point(class_scope:caffe.V1LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.V0LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.PReLUParameter),0
@@protoc_insertion_point(module_scope),0
"TODO: We need to identify whether this is 4D image data, otherwise we shouldn't change the dimension order",1
print(node.parameters),0
assert False,0
"Stochastic pooling, for instance.",0
TODO: Axis,0
TODO: Unbiased,1
check if need the Flatten layer,0
if parent.output_shape.height > 1 or parent.output_shape.width > 1:,0
TODO: The gamma parameter has to be set (in node.data?) and this should work.,1
"Also, mean should be set to 0, and var to 1, just to be safe.",0
skip the split node,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"For some reason argparser gives us unicode, so we need to conver to str first",0
for test,0
"with open(""graph.txt"", 'w') as f:",0
for layer in self.IR_graph.topological_sort:,0
current_node = self.IR_graph.get_node(layer),0
"print(""========current_node=========\n{}"".format(current_node.layer), file=f)",0
test end,0
"print(""========current_node={}"".format(current_node.layer))",0
check if have pad layer,0
Pad_h < kernel_h (vgg19 caffe2caffe),0
implement asymmetric paddings by applying symmetric padding then cropping,0
num_output = IR_node.get_attr('kernel_shape')[-2],0
keys = [],0
for key in self.weights_dict[IR_node.name].keys():,0
keys.append(key),0
"print(""=======Layer: {}, keys: {}"".format(IR_node.name, keys))",0
"if asymmetric padding, set offset to 1",0
Change the layer name,0
check if need crop output shape,0
check if need crop output shape,0
"change the key ""name"" to ""variable_name"", in case of the layer name has invalid characters",0
"Convert NHWC (IR) to NCHW (Caffe): [0,1,2,3]->[0,3,1,2]",0
"def emit_Square(self, IR_node):",0
"input_layers = ', '.join(('n.' + self.IR_graph.get_node(edge).real_variable_name) for edge in IR_node.in_edges)",0
"self.add_body(1, ""n.{:<15} = L.Square({}, ntop=1)"".format(",0
"IR_node.variable_name,",0
input_layers)),0
Caffe Types,0
Same value applies to all spatial dimensions,0
Extract the value for the given spatial dimension,0
consider rewrite this function to Network.py,0
Dropout layers appear in a fair number of Caffe,0
test-time networks. These are just ignored. We'll,0
filter them out here.,0
TODO: raise error,1
TODO: raise error,1
Automatically set a name if not provided.,0
Figure out the layer inputs.,0
"print('op: %s   shape: %s' % (op, layer_output._keras_shape))",0
"print('op: %s   shape: %s' % (op, layer_output.get_shape().as_list()))",0
Add to layer LUT.,0
Old-style input specification,0
New-style input specification,0
"We most likely have a data layer on our hands. The problem is,",0
Caffe infers the dimensions of the data from the source (eg: LMDB).,0
We want to avoid reading datasets here. Fail for now.,0
This can be temporarily fixed by transforming the data layer to,0
"Caffe's ""input"" layer (as is usually used in the ""deploy"" version).",0
TODO: Find a better solution for this.,1
The .prototxt file defining the graph,0
The .caffemodel file containing the learned parameters,0
Set to true if the fallback protocol-buffer based backend was used,0
"A list containing (layer name, parameters) tuples",0
Load the parameters,0
"When using the protobuf-backend, each parameter initially has four dimensions.",0
"In certain cases (like FC layers), we want to eliminate the singleton dimensions.",0
"This implementation takes care of the common cases. However, it does leave the",0
potential for future issues.,0
The Caffe-backend does not suffer from this problem.,0
A dictionary mapping NodeKind to the transposed order.,0
The node kinds eligible for reshaping,0
"If true, the reshaped data will replace the old one.",0
"Otherwise, it's set to the reshaped_data attribute.",0
Check for 2+ dimensional data,0
The FC layer connected to the spatial layer needs to be,0
re-wired to match the new spatial ordering.,0
node.reshaped_data = weights.transpose(transpose_order),0
Set the weights,0
We're only fusing nodes with single parents,0
We can only fuse a node if its parent's,0
value isn't used by any other node.,0
Rewrite the fused node's children to its parent.,0
Disconnect the fused node from the graph.,0
Let the sub-class merge the fused node in any arbitrary way.,0
Fuse ReLUs when the parent node is one of the given types.,0
"If None, all node types are eligible.",0
Prescale the stats,0
Replace with the updated values,0
Include the scale and bias terms,0
self.graph = NodeRenamer()(graph),0
"return Graph(name, [self.map_node(node) for node in self.graph.nodes])",0
assert mapped_node is not None,0
skip when mapped_node is None,0
FIXME:,0
output = node.output,0
Decompose DAG into chains,0
Generate Python code line by line,0
Fall back to the protobuf implementation,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
from tensorflow.core.framework.node_def_pb2 import NodeDef,0
from tensorflow.core.framework import attr_value_pb2,0
pass,0
"print(""\n"")",0
print(i),0
print(block),0
continue,0
NHWC,0
assert False,0
print(shortcut_layer['input'] ),0
spacetodepth,0
print(block),0
print(region_layer),0
assert False,0
print line,0
set default value,0
"yolo3(608) start at 1, yolo2(608) start at 0. yolo2(416) start at 1, yolo3(416) start at 0",0
load weight by original order,0
print(node_type),0
print(IR_node),0
assert False,0
strides,0
assert False,0
padding,0
only load weight conv,0
"print(""----------------"",self.start)",0
print(kernel.shape),0
print(k_bias.shape),0
"buf, start, scale_layer['name'], bn_layer['name'], conv_layer['name']",0
"print(""=============="",self.start)",0
print(bias.shape),0
print(scale.shape),0
print(mean.shape),0
print(var.shape),0
print(kernel.shape),0
print(W),0
assert False,0
no use,0
print(source_node.layer),0
for image classification(resnet) AVG pooling,0
print(source_node.layer),0
print(source_node.layer),0
kwargs['ignore_thresh'] = source_node.get_attr('ignore_thresh'),0
print(source_node.get_attr('anchors')),0
"kwargs['anchors'] = ['0.52','0.22']",0
kwargs['mask'] = source_node.get_attr('mask'),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
tensorflow dump tag,0
Reference: 1) Keras image preprocess: https://github.com/keras-team/keras/blob/master/keras/applications/imagenet_utils.py,0
2) tensorflow github issue: https://github.com/tensorflow/models/issues/517,0
"R-G-B for Imagenet === [123.68, 116.78, 103.94]",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.input, self.model, self.testop = self.MainModel.KitModel(self.args.w)",0
testop = tf.get_default_graph().get_operation_by_name(layer_name),0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result(None, False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
layer_name = 'block2_pool',0
"intermediate_layer_model = keras.Model(inputs=model.input,",0
outputs=model.get_layer(layer_name).output),0
intermediate_output = intermediate_layer_model.predict(img),0
print (intermediate_output),0
print (intermediate_output.shape),0
"print (""%.30f"" % np.sum(intermediate_output))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Load a real image and do default tf imageNet preprocessing,0
"[Batch, Height, Width, Channels] --> [Batch, Channels, Height, Width]",0
We only deal with non-recurrent networks for now,0
"(H,W,C) --> (C,H,W)",0
Load TensorFlow model,0
Prepare inputs,0
Run TF session,0
Evaluate coreml model,0
evaluate the TF model,0
evaluate CoreML,0
Test the default CoreML evaluation,0
Download model,0
Convert to coreml,0
Test predictions on an image,0
Download model,0
Convert to coreml,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
"@unittest.skip(""Failing GPU backend: related to https://github.com/tf-coreml/tf-coreml/issues/26"")",0
? style transfer image size and style number?,0
Test predictions on an image,0
evaluate the TF model,0
evaluate CoreML,0
Test the default CoreML evaluation,0
"compare_tf_coreml_outputs(tf_out_flatten, coreml_out_flatten)",0
#Download model,0
url = 'https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz',0
tf_model_dir = _download_file(url = url),0
"tf_model_path = os.path.join(TMP_MODEL_DIR, 'inception_v3_2016_08_28_frozen.pb')",0
#Convert to coreml,0
"mlmodel_path = os.path.join(TMP_MODEL_DIR, 'inception_v3_2016_08_28.mlmodel')",0
mlmodel = tf_converter.convert(,0
"tf_model_path = tf_model_path,",0
"mlmodel_path = mlmodel_path,",0
"output_feature_names = ['InceptionV3/Predictions/Softmax:0'],",0
"input_name_shape_dict = {'input:0':[1,299,299,3]},",0
"image_input_names = ['input:0'],",0
"red_bias = -1,",0
"green_bias = -1,",0
"blue_bias = -1,",0
image_scale = 2.0/255.0),0
#Test predictions on an image,0
_test_coreml_model_image_input(,0
"tf_model_path = tf_model_path,",0
"coreml_model = mlmodel,",0
"input_tensor_name = 'input:0',",0
"output_tensor_name = 'InceptionV3/Predictions/Softmax:0',",0
img_size = 299),0
Download model,0
Convert to coreml,0
Test predictions on an image,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
self.test_truth(),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
from collections import namedtuple,0
"Batch = namedtuple('Batch', ['data'])",0
TODO,1
Apple has published some of their own models. They can be downloaded from https://developer.apple.com/machine-learning/.,0
"Those published models are: SqueezeNet, Places205-GoogLeNet, ResNet50, Inception v3, VGG16",0
TODO,1
load model,0
TODO: Multiple inputs,1
TODO: Multiple outputs,1
inference,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
caffe uses NCHW,0
"self.print_intermediate_result('pooling0', False)",0
delete tmp model files,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
pylint: disable=line-too-long,0
pylint: enable=line-too-long,0
"net = caffe.Net(arch_fn, weight_fn, caffe.TEST)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
# Image Classification,0
# Semantic Segmentation,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
print(r),0
d = darknet_extractor(),0
model_filename = d.download('yolov3'),0
print(model_filename),0
"image_path = ""./mmdnn/conversion/examples/data/dog.jpg""",0
"model_path = ""./""",0
d = darknet_extractor(),0
"result = d.inference('yolov3', model_filename, model_path, image_path = image_path)",0
print(result),0
print(i),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result('pooling0', False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
need to be updated,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.input, self.model, self.testop = self.MainModel.KitModel(self.args.w)",0
testop = tf.get_default_graph().get_operation_by_name(layer_name),0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"'mobilenet_v1': mobilenet_v1.mobilenet_v1_arg_scope,",0
"'mobilenet_v1' : mobilenet_v1.mobilenet_v1,",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
raise NotImplementedError(),0
Load file,0
Run TensorBoard,0
print(cmd),0
Using cuDNN since vanilla RNN,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Notes for training NASNet Cifar Model,0
-------------------------------------,0
batch_size: 32,0
learning rate: 0.025,0
cosine (single period) learning rate decay,0
auxiliary head loss weighting: 0.4,0
clip global norm of all gradients by 5,0
600 epochs with a batch size of 32,0
This is used for the drop path probabilities since it needs to increase,0
the drop out probability over the course of training.,0
Notes for training large NASNet model on ImageNet,0
-------------------------------------,0
batch size (per replica): 16,0
learning rate: 0.015 * 100,0
learning rate decay factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 100 replicas,0
auxiliary head loss weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Notes for training the mobile NASNet ImageNet model,0
-------------------------------------,0
batch size (per replica): 32,0
learning rate: 0.04 * 50,0
learning rate scaling factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 50 replicas,0
auxiliary head weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Shape of feature map before the final layer.,0
149 x 149 x 32,0
Run the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Find where to place the reduction cells or stride normal cells,0
Setup for building in the auxiliary head.,0
Run the cells,0
true_cell_num accounts for the stem cells,0
Final softmax layer,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80,0
71 x 71 x 192,0
35 x 35 x 192,0
35 x 35 x 320,0
TODO(alemi): Register intermediate endpoints,1
"17 x 17 x 1088 if output_stride == 8,",0
33 x 33 x 1088 if output_stride == 16,0
TODO(alemi): register intermediate endpoints,1
TODO(gpapan): Properly support output_stride for the rest of the net.,1
8 x 8 x 2080,0
TODO(alemi): register intermediate endpoints,1
8 x 8 x 1536,0
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which",1
can be set to False to disable pooling here (as in resnet_*()).,0
Set weight_decay for weights in conv2d and fully_connected layers.,0
Set activation_fn and parameters for batch_norm.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
=============================================================================,0
Tensorflow mandates these.,0
Conv and DepthSepConv namedtuple define layers of the MobileNet architecture,0
Conv defines 3x3 convolution layers,0
DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.,0
stride is the stride of the convolution,0
depth is the number of channels or filters in a layer,0
_CONV_DEFS specifies the MobileNet body,0
Used to find thinned depths for each layer.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
By passing filters=None,0
separable_conv2d produces only a depthwise convolution layer,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 1024,0
Set weight_decay for weights in Conv and DepthSepConv layers.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Skip path 1,0
Skip path 2,0
"First pad with 0's on the right and bottom, then shift the filter to",0
include those 0's that were added.,0
Concat and apply BN,0
Set the prev layer to the current layer if it is none,0
Check to be sure prev layer stuff is setup correctly,0
Apply conv operations,0
Combine hidden states using 'add'.,0
Add hiddenstate to the list of hiddenstates we can choose from,0
Dont stride if this is not one of the original hiddenstates,0
"Check if a stride is needed, then use a strided 1x1 here",0
Determine if a reduction should be applied to make the number of,0
filters match.,0
Return the concat of all the states,0
Scale keep prob by layer number,0
The added 2 is for the reduction cells,0
Decrease the keep probability over time,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Inception-Resnet-A,0
Inception-Resnet-B,0
Inception-Resnet-C,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
force in-place updates of mean and variance estimates,0
Moving averages ends up in the trainable variables collection,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80,0
71 x 71 x 192,0
35 x 35 x 256,0
5 x Inception-resnet-A,0
Reduction-A,0
10 x Inception-Resnet-B,0
Reduction-B,0
5 x Inception-Resnet-C,0
pylint: disable=no-member,0
"net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None,",0
"scope='Bottleneck', reuse=False)",0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
Only insert rate to params if rate > 1.,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
import self.model,0
self.model,0
how the model can not load from `***.bin`,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
pylint: disable=line-too-long,0
pylint: enable=line-too-long,0
refer to https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/v2/tests/test_rnn_layer.py#L35,0
refer to https://github.com/PaddlePaddle/Paddle/issues/7403,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
refer to https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/v2/tests/test_rnn_layer.py#L35,0
refer to https://github.com/PaddlePaddle/Paddle/issues/7403,0
the channel of input feature is 3,0
"depth should be one of 20, 32, 44, 56, 110, 1202",0
Copyright (c) Microsoft. All rights reserved.,0
Licensed under the MIT license. See LICENSE.md file in the project root,0
for full license information.,0
==============================================================================,0
"self.model, self.testop = self.MainModel.KitModel(self.args.w)",0
"self.print_intermediate_result(None, False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
pylint: disable=line-too-long,0
pylint: enable=line-too-long,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.anchors = np.array([[10,13], [16,30],[33,23],[30,61],[62,45], [59,119],[116,90],[156,198],[373,326]])",0
print(output),0
"print(out_boxes, out_scores, out_classes)",0
get random colors,0
My kingdom for a good redistributable image drawing library.,0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
yolov3 80 classes,0
"Reshape to batch, height, width, num_anchors, box_params.",0
Adjust preditions to each spatial grid point and anchor size.,0
Note: YOLO iterates over height index before width index.,0
"TODO: It works with +1, don't know why.",1
Scale boxes back to original image shape.,0
"print(""feats,anchors, num_classes, input_shape"", feats, anchors, num_classes, input_shape)",0
"print(box_xy, box_wh, box_confidence, box_class_probs)",0
"yolo_outputs order 13,26,52",0
TODO: use keras backend instead of tf.,1
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
save network structure as JSON,0
layer_name = 'block2_pool',0
"intermediate_layer_model = keras.Model(inputs=model.input,",0
outputs=model.get_layer(layer_name).output),0
intermediate_output = intermediate_layer_model.predict(img),0
print (intermediate_output),0
print (intermediate_output.shape),0
"print (""%.30f"" % np.sum(intermediate_output))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Convert network architecture only,0
MXNet,0
Caffe,0
For CoreML,0
Caffe,0
Darknet,0
"exts = ['.pb', '.npy', '.py']",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check non-sense always input module.Module,0
if not (type(model) == mx.module.Module,0
or type(model) == mx.module.SequentialModule,0
or type(model) == mx.model),0
"raise TypeError(""MXNet layer of type %s is not supported."" % type(model))",0
"if layer[""op""] == ""null"":",0
continue,0
"raise NotImplementedError(""Cannot support multi-input"")",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"mxnet-cpu only support channel first, default convert the model and weight as channel first",0
We use the real_name for specifying the input layer in data_names,0
since MXNet API wants the actual name of the layer. On the other,0
"hand, the module API wants the last symbol in the symbol chain, so",0
for the output node we need to use the actual python variable name,0
of the last layer (real_variable_name).,0
download and show the image,0
"convert into format (batch, RGB, width, height)",0
"to show the image, change the argument show into True",0
compute the predict probabilities,0
print the top-5,0
# call function predict,0
"with open('synset.txt', 'r') as f:",0
labels = [l.rstrip() for l in f],0
"predict(model, labels, 'http://writm.com/wp-content/uploads/2016/08/Cat-hd-wallpapers.jpg')",0
"print(""Warning: MXNet Convolution Layer pad does not match IR Convolution Layer pad"")",0
"layout = IR_node.IR_layer.attr[""data_format""].s",0
if layout not in MXNetEmitter.channels_last:,0
Add LeakyReLU Elu(slope not support),0
"axis = IR_node.IR_layer.attr[""axis""].i",0
"self.output_weights[IR_node.name + ""_gamma""] = np.multiply(weight_dict['scale'], weight_dict_scale['scale'])",0
"self.output_weights[IR_node.name + ""_beta""] = np.multiply(weight_dict['bias'], weight_dict_scale['scale']) + weight_dict_scale['bias']",0
not supported yet,0
"axis = IR_node.IR_layer.attr[""axis""].i",0
not supported yet,0
"axis = IR_node.IR_layer.attr[""axis""].i",0
not supported yet,0
"print(""Warning: MXNet Pooling Layer pad does not match IR Pooling Layer pad"")",0
"def emit_ConvTranspose(self, IR_node):",0
if self.weight_loaded:,0
weight_dict = self.weights[IR_node.name],0
weights = weight_dict['weights'],0
"dim = len(IR_node.IR_layer.attr[""kernel_shape""].list.i) - 2",0
kernel = list(),0
"for idx in range(0, dim):",0
"kernel.append(IR_node.IR_layer.attr[""kernel_shape""].list.i[idx])",0
stride = list(),0
"for e in IR_node.IR_layer.attr[""strides""].list.i[1:-1]:",0
stride.append(e),0
dilate = list(),0
"for e in IR_node.IR_layer.attr[""dilations""].list.i[1:-1]:",0
dilate.append(e),0
"dilate = ', '.join('%s' % i for i in dilate)",0
defuse_pad = False,0
pad = list(),0
"if ""pads"" in IR_node.IR_layer.attr:",0
output_shape = list(),0
"for e in IR_node.IR_layer.attr[""_output_shapes""].list.shape[0].dim:",0
output_shape.append(e.size),0
"# print(""Warning: MXNet Deconvolution Layer pad does not match IR Deconvolution Layer pad"")",0
"defuse_pad, pad = MXNetEmitter.transfer_pad(IR_node.IR_layer.attr[""pads""].list.i)",0
"pad = ', '.join('%s' % i for i in pad)",0
"kernel = ', '.join('%s' % i for i in kernel)",0
"stride = ', '.join('%s' % i for i in stride)",0
"num_filter = IR_node.IR_layer.attr[""kernel_shape""].list.i[-2]",0
"no_bias = not IR_node.IR_layer.attr[""use_bias""].b",0
if not no_bias and self.weight_loaded:,0
"self.output_weights[IR_node.replace_scope(IR_node.name) + ""_bias""] = weight_dict['bias']",0
"# layout = IR_node.IR_layer.attr[""data_format""].s",0
if dim == 1:,0
layout = 'NCW',0
elif dim == 2:,0
layout = 'NCHW',0
elif dim == 3:,0
layout = 'NCDHW',0
if self.weight_loaded:,0
# if layout not in MXNetEmitter.channels_last:,0
"weights = MXNetEmitter.transpose(weights, dim)",0
"self.output_weights[IR_node.replace_scope(IR_node.name) + ""_weight""] = weights",0
"code = """"",0
if not defuse_pad:,0
"code = ""{:<15} = mx.sym.Deconvolution(data = {}, kernel = ({}), stride = ({}), dilate = ({}), pad = ({}), num_filter = {}, no_bias = {}, layout = '{}', name = '{}')"".format(",0
"IR_node.replace_scope(IR_node.name),",0
"IR_node.replace_scope(IR_node.in_edges[0]),",0
"kernel,",0
"stride,",0
"dilate,",0
"pad,",0
"num_filter,",0
"no_bias,",0
"layout,",0
IR_node.replace_scope(IR_node.name)),0
else:,0
"code = self.set_pad(IR_node, code, pad)",0
"code += ""\n    {:<15} = mx.sym.Deconvolution(data = {}, kernel = ({}), stride = ({}), dilate = ({}), num_filter = {}, no_bias = {}, layout = '{}', name = '{}')"".format(",0
"IR_node.replace_scope(IR_node.name), IR_node.replace_scope(IR_node.name) + ""_pad"", kernel, stride, dilate, num_filter, no_bias, layout, IR_node.replace_scope(IR_node.name))",0
return code,0
reverse cannot support yet,0
"code = ""{:<15} = mx.sym.transpose(data = {}, axes = (0, 2, 3, 1))\n"".format(""trans"", self.parent_variable_name(IR_node))",0
save the constant into weight dict,0
param_code does not need parameter slice.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Not support yet,0
"""softrelu""  : ""SoftReLU""",0
Load the model network and weights,0
"digraph = mx.viz.plot_network(sym, save_format='jpg') # For debugging",0
digraph.render(),0
model.bind(data_shapes = data_shapes),0
model.init_params(),0
"mod.load(model_path, epoch_num)",0
return mod.get_params(),0
load the model network,0
adjust the data format,0
load model files into MXNet graph,0
data_shape arguments added to calculate infer_shape(required),0
"if isinstance(input_arg, basestring):",0
Build network graph,0
transpose to channel last,0
"name, op",0
input edge,0
output shape,0
input edge,0
attr,0
"print(""Warning: MXNet symbol pad does not support channel last"")",0
"IR_node.attr[""pads""].list.i.extend([0, 0])",0
for e in pad:,0
"IR_node.attr[""pads""].list.i.extend([e, e])",0
"IR_node.attr[""pads""].list.i.extend([0, 0])",0
"name, op",0
input edge,0
units,0
use bias (no_bias default = False),0
weights,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [NM]"".format(source_node.name, self.data_format))",0
input edge,0
output shape,0
kernel_shape,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [{}]"".format(source_node.name, self.data_format, layout))",0
groups,0
in_channel = in_channel // group,0
use_bias (no_bias default = False),0
strides,0
dilations,0
data_format,0
padding,0
weights,0
axis,0
scale,0
epsilon,0
momentum,0
weights,0
gamma,0
beta,0
mean,0
var,0
"name, op",0
input edge,0
pooling type (sum not allowed yet),0
strides,0
kernel_shape,0
padding,0
output shape,0
"name, op",0
input edge,0
output shape,0
dim,0
"def rename_log_softmax(self, source_node):",0
"raise NotImplementedError(""not support yet"")",0
"def rename_Correlation(self, source_node):",0
"raise NotImplementedError(""not support yet"")",0
"name, op",0
input edge,0
padding,0
output shape,0
kernel_shape,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [{}]"".format(source_node.name, self.data_format, layout))",0
use_bias (no_bias default = False),0
strides,0
dilations,0
data_format,0
groups,0
weights,0
"def rename_RNN(self, source_node):",0
"raise NotImplementedError(""RNN not support yet"")",0
"name, op",0
input edge,0
input_dim,0
output_dim,0
dtype,0
output shape,0
"IR only support elu and prelu from {'elu', 'leaky', 'prelu', 'rrelu'}",0
return,0
"name, op",0
gamma,0
input edge,0
"alpha [exp(x) - alpha], but mxnet attr slope [slope*(exp(x) - 1)] when x < 0",0
"IR_node.attr[""alpha""].f = float()",0
output shape,0
"raise NotImplementedError(""slope cannot convert to alpha"")",0
"def rename_InstanceNorm(self, source_node):",0
raise NotImplementedError,0
"def rename_L2Normalization(self, source_node):",0
raise NotImplementedError,0
keep_prob,0
mode,0
reverse cannot support yet,0
old API target_shape not support yet,0
output shape,0
"raise NotImplementedError(""adjust output shape"")",0
dim,0
dtype,0
output shape,0
"name, op",0
input edge,0
output shape,0
axis,0
"raise NotImplementedError(""No matching IR api"")",0
"left strip the ""_"" at the beginning of the name",0
"Issue #85, #135",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
keep dims,0
axes,0
"name, op",0
epsilon,0
moving variance (var),0
print(moving_variance.name),0
gamma (scale),0
mean,0
bias,0
input node,0
output node,0
load model files into TensorFlow graph,0
extract subgraph using in_nodes and dest_nodes,0
Build network graph,0
extract subgraph using dest_nodes,0
Get input node name,0
Graph Transform,0
TODO: Fused conv and pool with padding is different from defused operators,0
strides,0
window_shape,0
pool type,0
padding,0
shape,0
strides,0
input[1] : W,0
filter,0
padding,0
output[0] : B,0
converted [dropout],0
convert [tf.contrib.layers.batch_norm],0
normal Add,0
units,0
Weights,0
FullyConnected Layer,0
"name, op",0
get Bias,0
Matmul Layer,0
Deal Dropout,0
keep prob,0
Remove nodes,0
Mul,0
Floor,0
print (source_node),0
print (source_node.layer),0
assert False,0
"def rename_RandomShuffleQueueV2(self, source_node):",0
# print(source_node.layer),0
"IR_node = self._convert_identity_operation(source_node, in_edge_count = 0, new_op = 'DataInput')",0
# IR_node.attr['shape'].shape.MergeFromString(source_node.layer.attr['_output_shapes'].list.shape[0].SerializeToString()),0
# IR_node.attr['shape'].shape.dim[0].size = -1,0
IR_node.attr['dtype'].type = self.dtype_map[source_node.layer.attr['component_types'].list.type[0]],0
paddings,0
gamma (scale),0
bias,0
Mean,0
Var,0
TODO: Current it is only for slice,1
TODO:  axis,0
strides,0
input[1] : W,0
filter,0
padding,0
output[0] : B,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
check whether flatten operator should be added,0
flatten is needed,0
"self._emit_unary_operation(IR_node, ""contrib.layers.flatten"")",0
param_code does not need parameter slice.,0
skip_prefix = [,0
"""^"",",0
"""train_op"",",0
"""save"",",0
"""gradients"",",0
"""init"",",0
"""global_step"",",0
"""distort_image"",",0
"""Adagrad"",",0
],0
"""Switch""",0
"""RequantizationRange"",",0
"""Requantize"",",0
"""Identity"",",0
"""Mean"",",0
"""Cast""",0
load model files into TensorFlow graph,0
Save it to an output file,0
keep dims,0
axes,0
ssd model is transformed,0
Ax - (Au - b),0
A,0
b,0
print(sub_content),0
epsilon,0
moving variance (var) /read,0
gamma (scale),0
print(Rsqrt.out_edges),0
beta  (bias),0
moving mean (mean),0
input node,0
epsilon,0
beta,0
gamma (scale),0
"output_node = self.get_son(source_node.name, [0, 0, 0, 0], True)",0
assert False,0
Quantized model type,0
TODO: Fused conv and pool with padding is different from defused operators,0
assert variable.get_attr('_output_shapes')[0].dim[0].size == IR_node.attr['kernel_shape'].list.i[-1],0
strides,0
window_shape,0
pool type,0
padding,0
"In facenet or other newtwork using slim.batch_norm,",0
"There are two BN(train, test) skip switch and merge.",0
epsilon,0
moving variance (var) /read,0
gamma (scale),0
beta  (bias),0
moving mean (mean),0
ReduceMean,0
keep dims,0
axes,0
Skip the node as merge,0
weights,0
Skip BiasAdd,0
weights,0
"input_node_perm = self.check_const(self.get_parent(source_node.name, [1], True))",0
paddings,0
for attr.shape >= 2,0
"For models built by slim.batch_norm, remove duplicate BN (eg.facenet)",0
TODO:  only for 1D,1
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Build network graph,0
"PaddleParser._set_output_shape(source_node, IR_node)",0
"name, op",0
input edge,0
For concat axis,0
only for training,0
"name, op",0
input edge,0
input edge,0
layer and spec,0
width <=> x or height <=> y,0
output shape,0
"name, op",0
it should be in the shape of height x width x inputchannel x outputchannel,0
use_bias: TODO,1
pad_dim,0
fail report because of auto_pad,0
if dilation_x == 1 and dilation_y == 1:,0
if output_x * stride_x == input_x and output_y * stride_y == input_y:,0
"auto_pad = ""SAME""",0
kwargs['auto_pad'] = auto_pad,0
elif output_x * stride_x == input_x - width + 1 and output_y * stride_y == input_y - height + 1:,0
"auto_pad = ""VALID""",0
kwargs['auto_pad'] = auto_pad,0
defuse the activation layer,0
"name, op",0
input edge,0
layer and spec,0
output shape,0
"channels_first, then axis = 1",0
epsilon,0
compute adjusted parameters,0
Reference: parameter transformation https://github.com/apple/coremltools/issues/153,0
convert type because of tensorflow,0
flatten,0
mean,0
var,0
defuse the activation layer,0
"name, op",0
input edge,0
layer and spec,0
assert False,0
output shape,0
pad_dim,0
padding mode,0
"If padding == ""SAME"": output_spatial_shape[i] = ceil(input_spatial_shape[i] / strides[i])",0
"If padding == ""VALID"": output_spatial_shape[i] = ceil((input_spatial_shape[i] - (spatial_filter_shape[i]-1) * dilation_rate[i]) / strides[i]).",0
"name, op",0
input edge,0
layer and spec,0
units,0
output shape,0
use_bias,0
Kit weight tranpose,0
weight: N x M -> C x H x W x M -> H x W x C x M -> N x M,0
,0
weights,0
need the shape TODO,1
only for training,0
"name, op",0
input edge,0
shape,0
"name, op",0
input edge,0
layer and spec,0
output shape,0
alpha,0
beta,0
nsize,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Build network graph,0
strides,0
window_shape,0
pool type,0
padding,0
TODO: Handle square,1
self._print_layer(source_node),0
print (source_node.name),0
"print (self.src_graph.get_parent(source_node.name, [0]).real_name)",0
assert False,0
Convolution,0
Kernel,0
Attributes,0
Bias,0
Activation,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
BlockFunction node,0
redirect the composite's inputs to the true inputs,0
"BlockFunctions are short-circuited, and not added to accum[]",0
Function node,0
OutputVariable node,0
def build(self):,0
"_traverse_graph(self, self.model.root_function)",0
"super(CntkGraph, self).build()",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"IR_node.get_attr('output_dim'),",0
alpha = alpha / size,0
??,0
print(IR_node.layer),0
assert False,0
"def emit_Unstack(self, IR_node):",0
"num_str = ""{}.shape[{}]"".format(self.parent_variable_name(IR_node), IR_node.get_attr('axis'))",0
axis = IR_node.get_attr('axis'),0
"parent_variable_shape = ""list({}.shape)"".format(self.parent_variable_name(IR_node)",0
"if self.IR_graph.get_parent(IR_node.name, [0]).type != 'Embedding'",0
else self.parent_variable_name(IR_node)+'.E'),0
if axis==1:,0
"shape_str = ""tuple([{}[0]*{}[{}], 1].extend({}[{}+1:]))"".format(",0
"parent_variable_shape,",0
"parent_variable_shape,",0
"str(axis),",0
"parent_variable_shape,",0
str(axis)),0
else:,0
"shape_str = ""tuple([{}[0]*{}[{}]].extend({}[1:{}]).append(1).extend({}[{}+1:]))"".format(",0
"parent_variable_shape,",0
"parent_variable_shape,",0
"str(axis),",0
"parent_variable_shape,",0
"str(axis),",0
"parent_variable_shape,",0
str(axis)),0
"code = ""{:<15} = cntk.reshape({}, {}, name='{}')"".format(",0
"IR_node.variable_name,",0
"self.parent_variable_name(IR_node),",0
"shape_str,",0
IR_node.variable_name),0
"code = ""{: <15} = cntk.reshape({}, {}.shape, name='{}')"".format(",0
"IR_node.variable_name,",0
"self.parent_variable_name(IR_node),",0
"self.parent_variable_name(IR_node),",0
IR_node.name,0
),0
return code,0
"def emit_Fill(self, IR_node):",0
"code = ""{:<15} = cntk.Constant({}, {}, name='{}')"".format(",0
"IR_node.variable_name,",0
"IR_node.get_attr('value'),",0
"self.parent_variable_name(IR_node),",0
IR_node.name),0
return code,0
param_code does not need parameter slice.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
For padding,0
print (input_shape),0
print (kernel_shape),0
print (strides),0
Not tested,0
print (input_shape),0
print (kernel_shape),0
print (strides),0
kernel_shape[idx] = (kernel_shape[idx] - 1) * dilation_rate + 1,0
print ([0] + up_list + [0] + down_list if data_format.startswith('NC') else up_list + [0] + down_list + [0]),0
print ('-----------------------------------------------------'),0
network library,0
not support multi-threads download,0
key: layer_name    value: keras layer,0
private functions,0
next_node_info.left_in_edges -= 1,0
"print ('{} --> {}'.format(src, dst))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
share functions,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
name --> (weight_name --> ndarray),0
Generated by the protocol buffer compiler.  DO NOT EDIT!,0
source: graph.proto,0
@@protoc_insertion_point(imports),0
@@protoc_insertion_point(class_scope:GraphDef),0
@@protoc_insertion_point(class_scope:NodeDef.AttrEntry),0
@@protoc_insertion_point(class_scope:NodeDef),0
@@protoc_insertion_point(class_scope:AttrValue.ListValue),0
@@protoc_insertion_point(class_scope:AttrValue),0
@@protoc_insertion_point(class_scope:TensorShape.Dim),0
@@protoc_insertion_point(class_scope:TensorShape),0
@@protoc_insertion_point(class_scope:LiteralTensor),0
@@protoc_insertion_point(module_scope),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
First try to read it as a binary file.,0
Next try to read it as a text file.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"If pattern.inputs is empty, skips the rest and accepts all the inputs.",0
"If order doesn't matter for the inputs, then make sure we match at least",0
one permutation of the inputs.,0
"def get_tensor(self, pattern_or_name):",0
op_tensor = self._get_op_tensor(pattern_or_name),0
return op_tensor[1] if op_tensor else None,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
"Python 3.3.2+ implements `yield from`, but for now:",0
"check the same pattern scope node whether have same inputs, outputs and weights.",0
"For those don't have, rename their scope names.",0
"clear out scope node, typically input constant node.",0
get sub_scopes,0
decline the suffix number,0
Obtain nodes where the scope name that satisfies top_level is top_scope and sub_level is sub_scope,0
Obtain nodes where the scope name that satisfies in sub_level is sub_scope,0
Obtain nodes where the scope name that satisfies in top_level is top_scope,0
Obtain all nodes grouped by sub_level sub_scope,0
cover the node,0
"store idx, node into a dict and sort it later to keep its topology sort.",0
in_node's out edges replace n_name with scope node name.,0
no out nodes means the last node in scope nodes should be returned,0
"if out_node is scope node, replace the scope node's inner topology list node.",0
the input parameter shoule be sliced when call func.,0
modify the in_edges in scope inner nodes. decline the :idx.,0
1. initilize scope node,0
2. get scope nodes' topology list.,0
3. rebuild the edges connection after folding these scope nodes into one node and,0
get this scope node's return variables.,0
4. rebuild graph.,0
RNN-related attrs.,0
get input params,0
self.store_const_to_top(result),0
"self.set_top_node_prop(result, pattern_name)",0
Do not include input op.,0
"TODO: pytorch, mxnet, keras, cntk",1
-*- coding: utf-8 -*-,0
from keras.layers.core import Layer,0
Arguments,0
Input shape,0
Output shape,0
Prepare broadcasting shape.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Load the model network,0
Load the model weights,0
load model files into Keras graph,0
Keras 2.1.6,0
Keras. 2.2.2,0
"_keras.utils.plot_model(model, ""model.png"", show_shapes = True)",0
Build network graph,0
TODO: Fused conv and pool with padding is different from defused operators,0
TODO: More activation functions,1
for ELU,0
input edge,0
"name, op",0
weights,0
pads,0
filter,0
"[kd, kh, kw, channel_size, filter number]",0
use_bias,0
strides,0
"[1, sd, sh, sw, 1]",0
dilations,0
"[1, dd, dh, dw, 1]",0
activation,0
"name, op",0
input edge,0
padding,0
strides,0
"[1, sd, sh, sw, 1]",0
window_shape,0
"[1, pd, ph, pw, 1]",0
"name, op",0
input edge,0
For concat axis,0
"name, op",0
input edge,0
padding,0
only for training,0
"name, op",0
input edge,0
"name, op",0
input edge,0
Merge Layers,0
only for training,0
"name, op",0
input edge,0
shape,0
only for training,0
"name, op",0
input edge,0
Core Layers,0
"name, op",0
input edge,0
units,0
use_bias,0
weights,0
activation,0
"name, op",0
input edge,0
"name, op",0
input edge,0
size,0
"name, op",0
input edge,0
input_dim,0
output_dim,0
mask_zero,0
weights,0
"name, op",0
input edge,0
units,0
use_bias,0
"for Keras, drop_out and recurrent_dropout",0
activation,0
"name, op",0
input edge,0
units,0
activation,0
weights,0
Kit TODO : need to search the tf,1
"name, op",0
input edge,0
for target shape,0
"name, op",0
input edge,0
axis,0
"Parameter arrangement in Keras: gamma, beta, mean, variance",0
scale,0
beta,0
mean,0
var,0
"name, op",0
input edge,0
for target shape,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
rot,0
"print(""Converting layer {}({})"".format(current_node.name, node_type))",0
TODO,1
"return input_node, 'same'",0
"assert IR_node.get_attr('group', 1) == 1",0
change dw from filters to 1,0
reset the default dilation,0
############,0
Operators #,0
############,0
"code = self._emit_merge(IR_node, ""subtract"")",0
TODO,1
"when converting from coreml model, reshape is needed after the global pooling",0
for Keras,0
It arouses some problems:,0
it can be implemented by Lambda Layer,0
https://github.com/keras-team/keras/issues/890,0
Keras == 2.1.6,0
Keras == 2.2.2,0
TODO: arguments won't be saved in keras export model,1
param_code does not need parameter slice.,0
Prepare broadcasting shape.,0
Prepare broadcasting shape.,0
"output = Lambda(lambda x: tf.fill(x, value))(input)",0
return output,0
def _layer_Constant(self):,0
"self.add_body(0, '''",0
class my_constant(keras.layers.Layer):,0
"def __init__(self, value, **kwargs):",0
"super(my_constant, self).__init__(**kwargs)",0
self._value = value,0
"# the input is dummy, just for creating keras graph.",0
"def call(self, dummy):",0
res = K.constant(self._value),0
self.output_shapes = K.int_shape(res),0
return res,0
"def compute_output_shape(self, input_shape):",0
return self.output_shapes,0
'''),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
Kit: TODO,1
Duplicate models for weight sharing,0
Expand the sub-models,0
"with open('modelmapbydataset.json', 'w') as outfile:",0
"json.dump(new_data, outfile)",0
generate makedown script,0
add Image Classification,0
add Object Detection,0
MMdnn introduction,0
steps for model conversion,0
Generate model converter description,0
Generate models list,0
generate makedown script,0
MMdnn introduction,0
steps for model conversion,0
Generate model converter description,0
Generate models list,0
console.info(JSON.parse(e.target.result)),0
"default, draw the model.json file",0
window.onload = () => {,0
"d3.json(filePath, (error, json) => {",0
if (error) throw error;,0
// console.info(json),0
draw(json),0
}),0
},0
generate dag,0
IR model or keras model,0
path generator,0
let arrowL = `M ${points[len - 1].x} ${points[len - 1].y} l -7 -7`,0
let arrowR = `M ${points[len - 1].x} ${points[len - 1].y} l  7 -7`,0
select a layer,0
remove list,0
draw,0
mode;,0
"let scale = Math.min(window.innerWidth*0.85 / width, (window.innerHeight-85)/height)",0
"var filter = def.append(""filter"")",0
".attr(""id"", ""dropshadow"")",0
"filter.append(""feGaussianBlur"")",0
".attr(""in"", ""SourceAlpha"")",0
".attr(""stdDeviation"", 4)",0
".attr(""result"", ""blur"");",0
"// filter.append(""feOffset"")",0
"//     .attr(""in"", ""blur"")",0
"//     .attr(""dx"", 2)",0
"//     .attr(""dy"", 2)",0
"//     .attr(""result"", ""offsetBlur"");",0
"var feMerge = filter.append(""feMerge"");",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""offsetBlur"")",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""SourceGraphic"");",0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
"svg.on('keydown', ()=>{console.info('ddd')})",0
".on('mouseover', ()=>{console.info('mouse over')})",0
k = parseFloat(k)*parseFloat(k_),0
"limit x, y",0
// a trick to make text svg transform in MS Edge,0
"d3.selectAll("".labels"").classed(""tempclass"", true);",0
"setTimeout(function () { d3.selectAll("".labels"").classed(""tempclass"", false); }, 40);",0
when shift is down,0
ulti function,0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
rm the '_',0
Base64 encode: https://developers.google.com/protocol-buffers/docs/proto3,0
Search the node,0
select by id: https://stackoverflow.com/questions/37270787/uncaught-syntaxerror-failed-to-execute-queryselector-on-document,0
Scroll,0
console.info(JSON.parse(e.target.result)),0
"default, draw the model.json file",0
window.onload = () => {,0
"d3.json(filePath, (error, json) => {",0
if (error) throw error;,0
// console.info(json),0
draw(json),0
}),0
},0
generate dag,0
IR model or keras model,0
path generator,0
let arrowL = `M ${points[len - 1].x} ${points[len - 1].y} l -7 -7`,0
let arrowR = `M ${points[len - 1].x} ${points[len - 1].y} l  7 -7`,0
select a layer,0
draw,0
"let scale = Math.min(window.innerWidth*0.85 / width, (window.innerHeight-85)/height)",0
"var filter = def.append(""filter"")",0
".attr(""id"", ""dropshadow"")",0
"filter.append(""feGaussianBlur"")",0
".attr(""in"", ""SourceAlpha"")",0
".attr(""stdDeviation"", 4)",0
".attr(""result"", ""blur"");",0
"// filter.append(""feOffset"")",0
"//     .attr(""in"", ""blur"")",0
"//     .attr(""dx"", 2)",0
"//     .attr(""dy"", 2)",0
"//     .attr(""result"", ""offsetBlur"");",0
"var feMerge = filter.append(""feMerge"");",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""offsetBlur"")",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""SourceGraphic"");",0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
"svg.on('keydown', ()=>{console.info('ddd')})",0
".on('mouseover', ()=>{console.info('mouse over')})",0
k = parseFloat(k)*parseFloat(k_),0
"limit x, y",0
// a trick to make text svg transform in MS Edge,0
"d3.selectAll("".labels"").classed(""tempclass"", true);",0
"setTimeout(function () { d3.selectAll("".labels"").classed(""tempclass"", false); }, 40);",0
when shift is down,0
ulti function,0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
space = '',0
Get the long description from the README file,0
Versions should comply with PEP440.  For a discussion on single-sourcing,0
"the version across setup.py and the project code, see",0
https://packaging.python.org/en/latest/single_source_version.html,0
The project's main homepage.,0
Author details,0
Choose your license,0
See https://pypi.python.org/pypi?%3Aaction=list_classifiers,0
How mature is this project? Common values are,0
3 - Alpha,0
4 - Beta,0
5 - Production/Stable,0
Indicate who your project is intended for,0
"Pick your license as you wish (should match ""license"" above)",0
"Specify the Python versions you support here. In particular, ensure",0
"that you indicate whether you support Python 2, Python 3 or both.",0
What does your project relate to?,0
You can just specify the packages manually here if your project is,0
simple. Or you can use find_packages().,0
"Alternatively, if you want to distribute just a my_module.py, uncomment",0
this:,0
"py_modules=[""my_module""],",0
List run-time dependencies here.  These will be installed by pip when,0
"your project is installed. For an analysis of ""install_requires"" vs pip's",0
requirements files see:,0
https://packaging.python.org/en/latest/requirements.html,0
"To provide executable scripts, use entry points in preference to the",0
"""scripts"" keyword. Entry points provide cross-platform support and allow",0
pip to create the appropriate form of executable for the target platform.,0
Communication to TensorFlow server via gRPC,0
TensorFlow serving stuff to send messages,0
Command line arguments,0
Send request,0
request.inputs['input'].CopyFrom(),0
"'xception'     : [TestModels.onnx_emit],",0
"'nasnet'       : [TestModels.onnx_emit],",0
omit tensorflow lead to crash,0
"'nasnet-a_large'       : [TestModels.mxnet_emit, TestModels.pytorch_emit, TestModels.tensorflow_emit],",0
"'inception_resnet_v2'  : [TestModels.caffe_emit, TestModels.keras_emit, TestModels.mxnet_emit, TestModels.pytorch_emit, TestModels.tensorflow_emit],",0
"Function self.assertEquals has deprecated, change to assertEqual",0
"self.assertEqual(original_predict.shape, converted_predict.shape)",0
get original model prediction result,0
original to IR,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
print(original_predict),0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
get shape,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
IR to code,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
The list is in RGB oder,0
IR to Model,0
"converted_file = original_framework + '_coreml_' + architecture_name + ""_converted""",0
"converted_file = converted_file.replace('.', '_')",0
load model,0
save model,0
"coremltools.utils.save_spec(model.get_spec(), converted_file)",0
inference,0
IR to code,0
import converted model,0
"In case of odd number add the extra padding at the end for SAME_UPPER(eg. pads:[0, 2, 2, 0, 0, 3, 3, 0]) and at the beginning for SAME_LOWER(eg. pads:[0, 3, 3, 0, 0, 2, 2, 0])",0
"'xception'     : [onnx_emit],",0
"'nasnet'       : [onnx_emit],",0
"'voc-fcn8s'     : [onnx_emit], # TODO: ConvTranspose, Crop",0
"'voc-fcn16s'    : [onnx_emit], # TODO: ConvTranspose, Crop",0
"'voc-fcn32s'    : [onnx_emit], # TODO: ConvTranspose, Crop",0
"'resnet_v1_50'          : [onnx_emit], # POOL: strides > window_shape not supported due to inconsistency between CPU and GPU implementations",0
"'resnet_v1_152'         : [onnx_emit], # POOL: strides > window_shape not supported due to inconsistency between CPU and GPU implementations",0
"'resnet_v2_50'          : [onnx_emit], # POOL: strides > window_shape not supported due to inconsistency between CPU and GPU implementations",0
"'resnet_v2_152'         : [onnx_emit], # POOL: strides > window_shape not supported due to inconsistency between CPU and GPU implementations",0
"'nasnet-a_large'        : [onnx_emit], # POOL: strides > window_shape not supported due to inconsistency between CPU and GPU implementations",0
TODO: coredump,1
"'alexnet'       : [cntk_emit, keras_emit, tensorflow_emit],",0
"'nasnet'       : [tensorflow_emit, keras_emit, coreml_emit],",0
"'facenet'      : [tensorflow_emit, coreml_emit,mxnet_emit,keras_emit]  # TODO",0
"'tinyyolo'     : [coreml_emit, keras_emit, mxnet_emit, pytorch_emit, tensorflow_emit],",0
"'vgg16': [tensorflow_emit],",0
'alexnet': [tensorflow_emit],0
get test input path,0
get original model prediction result,0
def test_caffe(self):,0
try:,0
import caffe,0
"self._test_function('caffe', self.caffe_parse)",0
except ImportError:,0
"print('Please install caffe! Or caffe is not supported in your platform.', file=sys.stderr)",0
def test_cntk(self):,0
try:,0
import cntk,0
"self._test_function('cntk', self.cntk_parse)",0
except ImportError:,0
"print('Please install cntk! Or cntk is not supported in your platform.', file=sys.stderr)",0
def test_coreml(self):,0
from coremltools.models.utils import macos_version,0
"if macos_version() < (10, 13):",0
"print('Coreml is not supported in your platform.', file=sys.stderr)",0
else:,0
"self._test_function('coreml', self.coreml_parse)",0
def test_keras(self):,0
"self._test_function('keras', self.keras_parse)",0
def test_mxnet(self):,0
"self._test_function('mxnet', self.mxnet_parse)",0
def test_darknet(self):,0
"self._test_function('darknet', self.darknet_parse)",0
def test_paddle(self):,0
# omit tensorflow lead to crash,0
import tensorflow as tf,0
try:,0
import paddle.v2 as paddle,0
"self._test_function('paddle', self.paddle_parse)",0
except ImportError:,0
"print('Please install Paddlepaddle! Or Paddlepaddle is not supported in your platform.', file=sys.stderr)",0
def test_pytorch(self):,0
"self._test_function('pytorch', self.pytorch_parse)",0
def test_tensorflow(self):,0
"self._test_function('tensorflow', self.tensorflow_parse)",0
def test_tensorflow_frozen(self):,0
"self._test_function('tensorflow_frozen', self.tensorflow_frozen_parse)",0
"'vgg19'         : [TestModels.onnx_emit],",0
"'voc-fcn8s'    : [TestModels.cntk_emit, TestModels.coreml_emit, TestModels.tensorflow_emit],",0
"'vgg19'         : [TestModels.caffe_emit, TestModels.cntk_emit, TestModels.coreml_emit, TestModels.keras_emit, TestModels.mxnet_emit, TestModels.pytorch_emit, TestModels.tensorflow_emit],",0
"'voc-fcn8s'     : [TestModels.cntk_emit, TestModels.coreml_emit, TestModels.tensorflow_emit],",0
"'voc-fcn16s'    : [TestModels.cntk_emit, TestModels.coreml_emit, TestModels.tensorflow_emit],",0
"'voc-fcn32s'    : [TestModels.cntk_emit, TestModels.coreml_emit, TestModels.tensorflow_emit],",0
"'nasnet'       : [TestModels.onnx_emit],",0
"'vgg19'         : [TestModels.onnx_emit],",0
"'alexnet'       : [TestModels.caffe_emit, TestModels.cntk_emit, TestModels.coreml_emit, TestModels.mxnet_emit, TestModels.pytorch_emit, TestModels.tensorflow_emit],",0
"'nasnet-a_large'       : [TestModels.mxnet_emit, TestModels.pytorch_emit, TestModels.tensorflow_emit],",0
"'inception_resnet_v2'  : [TestModels.caffe_emit, TestModels.keras_emit, TestModels.mxnet_emit, TestModels.pytorch_emit, TestModels.tensorflow_emit],",0
"'xception'     : [TestModels.onnx_emit],",0
"'nasnet'       : [TestModels.onnx_emit],",0
!/usr/bin/python,0
major python major_python_versions as python2 and python3,0
operating system,0
do not deal,0
I don't want to deal with auto_pad,0
Don't support auto_pad current!,0
2018-02-28,0
if group is None:,0
group = 1,0
group is not support yet too.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
node,0
input,0
edge(node a <-> node b),0
,0
"key is edge name, value is src/dst node name",0
"key is initializer name, value is TensorProto",0
print(name),0
print(layer.op_type),0
n is input data,0
n is input edge,0
omit node of some type,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"if node len(in_edges), generate additional DataInput node",0
print,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
TODO,1
"'max_pool2d': convert_maxpool,",0
"'onnx::Mul': convert_elementwise_mul,",0
"'onnx::Sub': convert_elementwise_sub,",0
"'onnx::ConvTranspose': convert_convtranspose,",0
"'onnx::LeakyRelu': convert_lrelu,",0
"'onnx::Sigmoid': convert_sigmoid,",0
"'onnx::Softmax': convert_softmax,",0
"'onnx::Selu': convert_selu,",0
"'onnx::Transpose': convert_transpose,",0
"'onnx::Reshape': convert_reshape,",0
"'onnx::MatMul': convert_matmul,",0
"'onnx::Gather': convert_gather,",0
"'onnx::ReduceSum': convert_reduce_sum,",0
"'onnx::Constant': convert_constant,",0
"'onnx::Upsample': convert_upsample,",0
"'onnx::Pad': convert_padding,",0
###########,0
property #,0
###########,0
###################,0
Public Functions #,0
###################,0
test,0
cpu: https://github.com/pytorch/pytorch/issues/5286,0
Build network graph,0
"(batch, C, H, W)  & NHWC",0
#########,0
Layers #,0
#########,0
dilation,0
handle bias,0
TODO,1
output_shape,0
epsilon,0
mean,0
var,0
Kit weight tranpose,0
weight: N x M -> C x H x W x M -> H x W x C x M -> N x M,0
,0
weights,0
use_bias,0
units,0
handle weight,0
handle bias,0
###################,0
Helper Functions #,0
###################,0
print (dir(data)),0
print (i),0
"print (""    {} with shape {}"".format(j, load_weight[i][j].shape))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Scopes created in a nested scope may have initial characters,0
that are illegal as the initial character of an op name,0
"(viz. '-', '\', '/', and '_').",0
sanity check.,0
run dce first to eliminate dead parts of the graph that might have been,0
left behind by things like symbolic_override,0
construct graph,0
nodes,0
input layer,0
TODO,1
build each layer,0
input,0
"print(node_input_name ,'->', node_name)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Base Functions,0
"padding,",0
"exstr = "", value=float('-Inf')""",0
"exstr = """"",0
Change to padding defuse,0
"input_node = self._defuse_padding(IR_node, exstr)",0
Ignore it in Pytorch,0
for Keras,0
"self.add_init(2, ""self.{:<15} = torch.from_numpy(__weights_dict['{}']['value'])"".format(",0
"IR_node.variable_name,",0
IR_node.name)),0
param_code does not need parameter slice.,0
from torch.nn.parameter import Parameter,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self._type = layer.__class__.__name__.replace('Backward', '')",0
"self._name = ""{}_{}"".format(self.type, id)",0
TODO,1
"""""""",0
build graph for pytorch 0.2.0,0
"""""""",0
dummy_input = torch.autograd.Variable(torch.randn(shape)),0
output_node = self.model(dummy_input),0
search_queue = [output_node.grad_fn],0
"tmp_node = PyTorchGraphNode(output_node.grad_fn, 0)",0
self.layer_map[tmp_node.name] = tmp_node,0
visited = {output_node.grad_fn : self.layer_map[tmp_node.name]},0
idx = 0,0
node_count = 1,0
while (idx < len(search_queue)):,0
current_node = search_queue[idx],0
current_type = visited[current_node].type,0
"if hasattr(current_node, 'next_functions'):",0
"for parent, _ in current_node.next_functions:",0
"parent_type = parent.__class__.__name__.replace('Backward', '')",0
if parent_type != 'AccumulateGrad' and \,0
(parent_type != 'Transpose' or current_type != 'Addmm'):,0
if not parent in visited:,0
"tmp_node = PyTorchGraphNode(parent, node_count)",0
self.layer_map[tmp_node.name] = tmp_node,0
node_count += 1,0
visited[parent] = tmp_node,0
search_queue.append(parent),0
"self._make_connection(visited[parent].name, visited[current_node].name)",0
idx += 1,0
try:,0
return TorchGraph._forward_torch_random_input(,0
"torch_model,",0
"input_shapes,",0
is_batch=False,0
),0
except:,0
# try batch mode,0
# return TorchGraph._forward_torch_random_input(,0
"#     torch_model,",0
"#     input_shapes,",0
#     is_batch=True,0
# ),0
pass,0
tensor = torch.rand(*shape).float(),0
multi output,0
single output,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
###########,0
property #,0
###########,0
###################,0
Public Functions #,0
###################,0
Build network graph,0
#########,0
Layers #,0
#########,0
handle weight,0
handle bias,0
handle weight,0
handle bias,0
###################,0
Helper Functions #,0
###################,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
load model file into Coreml Graph,0
model.encode() convert to str --- python2 may crash due to type 'unicode',0
Build Network Graph,0
self.data_format ? TODO,1
load the model network,0
convert to Model_pb2.Model,0
determine the type of the current_node,0
staticmethods,0
"(seq, batch, C, H, W)  & NHWC",0
TODO dtype_map,1
##### convert methods,0
convolution,0
input edge,0
important!,0
reshape the weight!,0
"[2, 3, 0, 1]",0
pads,0
use_bias,0
isDeconvolution,0
"name, op",0
kwargs['kernel_shape'] = weights.shape,0
strides,0
"[1, sd, sh, sw, 1]",0
activation,0
TODO,1
padding in conv,0
"pad in IR is [x1_b, x2_b, ..., x1_e, x2_e, ...]",0
padding,0
compute padding for 'same',0
https://discuss.mxnet.io/t/pooling-and-convolution-with-same-mode/528/3,0
https://www.jianshu.com/p/05c4f1621c7e,0
padding in pooling,0
padding,0
https://discuss.mxnet.io/t/pooling-and-convolution-with-same-mode/528/3,0
TODO,1
symmetric padding,0
"name, op",0
input edge,0
For concat axis,0
"NO axis in coreml, so set the last axis",0
"The first -1 means in coreml there is one-more axis,",0
The second -1 means the last axis,0
"name, op",0
input edge,0
padding,0
Future Module TODO,1
#### rename methods,0
"name, op",0
activation type,0
else:,0
assert False,0
input edge,0
Merge layers,0
only for training,0
"name, op",0
input edge,0
shape,0
NHWC channel last,0
"in fact, here is NWHC",0
"name, op",0
input edge,0
axis TODO,0
"channels_first, then axis = 1",0
scale,0
bias,0
epsilon,0
compute adjusted parameters,0
Reference: parameter transformation https://github.com/apple/coremltools/issues/153,0
convert type because of tensorflow,0
mean,0
var,0
"name, op",0
input edge,0
bias,0
"name, op",0
input edge,0
"MAX = 0, AVERAGE = 1, L2 = 2",0
padding,0
strides,0
"[1, sd, sh, sw, 1]",0
window_shape,0
"[1, pd, ph, pw, 1]",0
"name, op",0
input edge,0
"name, op",0
input edge,0
"name, op",0
input edge,0
units,0
use_bias,0
weights,0
change to single because of the tf matmul,0
in features,0
"name, op",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
the end of the tensorflow_resnet_v2_50's squeeze shape is [unknown_rank: true] with len 0,0
1001 means the 1001 classes for tensorflow_resnet_v2_50,0
!Alert! TODO,1
Future implement can be changed to the last two layer,0
TODO - remove style transfer 1D hack,1
"Input is 1D but it goes to the width dimension: (1,1,W)",0
"assume (Batch, Channels) - Batch dimension should be dropped",0
"assume (Batch, Sequence-Length, channels)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Add classifier classes (if applicable),0
Set pre-processing paramsters,0
"image_input_names,",0
Return the protobuf spec,0
model = _MLModel(self.builder.spec),0
if IR_node.name in self.weights_dict and 'weights' in self.weights_dict[IR_node.name]:,0
pass,0
"self._emit_merge(IR_node,'DOT')",0
Get input and output names,0
Dimensions and weights,0
"W should have shape (height, width, kernel_channels, output_channels), where kernel_channel = input_channels / groups",0
Dilations,0
see protobuf,0
assert False,0
Get the weights.,0
Dilations,0
see protobuf,0
depth-wise convolution,0
Dilations,0
see protobuf,0
Get input and output names,0
Pooling layer type,0
"if it's global, set the global flag",0
TODO  global pooling modification,0
Padding,0
see protobuf,0
Padding,0
see protobuf,0
Get input and output names,0
"type: ""list(int). A 1-D values of (leftBorder, topBorder, rightBorder, bottomBorder).""",0
This is central crop,0
Now add the layer,0
"Allowed values: 'CHW', 'HW', 'C', 'H', 'W'",0
Get input and output names,0
Get input and output names,0
Get the weights from keras,0
Get input and output names,0
blob_order == 0 if the input blob needs not be rearranged,0
blob_order == 1 if the input blob needs to be rearranged,0
"using keras_layer.input.shape have a ""?"" (Dimension[None] at the front),",0
making a 3D tensor with unknown batch size 4D,0
Get input and output names,0
activation emit,0
Get input and output names,0
negate it,0
apply threshold,0
negate it back,0
for Keras,0
Get input and output names,0
Set parameters,0
"Parameter arrangement in Keras: gamma, beta, mean, variance",0
compute adjusted parameters,0
Reference: parameter transformation https://github.com/apple/coremltools/issues/153,0
padding type TODO,0
"Type of the padding. Can be one of 'constant', 'reflection' or 'replication",0
Now add the layer,0
self.emit_Flatten(IR_node),0
"depth_radius: Half-width of the 1-D normalization window.""",0
Get the weights,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
input layer,0
build each layer,0
"if A.output == B.input, then make the connection: A -> B",0
"print('{0:20}->     {1:20}'.format(layerA.name, layerB.name))",0
"if A.name == B.input, then make the connection: A -> B, here A is the input",0
build each layer,0
"if A.output == B.input, then make the connection: A -> B",0
"print('{0:20}->     {1:20}'.format(layerA.name, layerB.name))",0
"if A.name == B.input, then make the connection: A -> B, here A is the input",0
build each layer,0
"if A.output == B.input, then make the connection: A -> B",0
"print('{0:20}->     {1:20}'.format(layerA.name, layerB.name))",0
"if A.name == B.input, then make the connection: A -> B, here A is the input",0
The information of the layer,0
Generated by the protocol buffer compiler.  DO NOT EDIT!,0
source: caffe.proto,0
@@protoc_insertion_point(imports),0
@@protoc_insertion_point(class_scope:caffe.BlobShape),0
@@protoc_insertion_point(class_scope:caffe.BlobProto),0
@@protoc_insertion_point(class_scope:caffe.BlobProtoVector),0
@@protoc_insertion_point(class_scope:caffe.Datum),0
@@protoc_insertion_point(class_scope:caffe.FillerParameter),0
@@protoc_insertion_point(class_scope:caffe.NetParameter),0
@@protoc_insertion_point(class_scope:caffe.SolverParameter),0
@@protoc_insertion_point(class_scope:caffe.SolverState),0
@@protoc_insertion_point(class_scope:caffe.NetState),0
@@protoc_insertion_point(class_scope:caffe.NetStateRule),0
@@protoc_insertion_point(class_scope:caffe.ParamSpec),0
@@protoc_insertion_point(class_scope:caffe.LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.TransformationParameter),0
@@protoc_insertion_point(class_scope:caffe.LossParameter),0
@@protoc_insertion_point(class_scope:caffe.AccuracyParameter),0
@@protoc_insertion_point(class_scope:caffe.ArgMaxParameter),0
@@protoc_insertion_point(class_scope:caffe.ConcatParameter),0
@@protoc_insertion_point(class_scope:caffe.BatchNormParameter),0
@@protoc_insertion_point(class_scope:caffe.BiasParameter),0
@@protoc_insertion_point(class_scope:caffe.ContrastiveLossParameter),0
@@protoc_insertion_point(class_scope:caffe.ConvolutionParameter),0
@@protoc_insertion_point(class_scope:caffe.CropParameter),0
@@protoc_insertion_point(class_scope:caffe.DataParameter),0
@@protoc_insertion_point(class_scope:caffe.DropoutParameter),0
@@protoc_insertion_point(class_scope:caffe.DummyDataParameter),0
@@protoc_insertion_point(class_scope:caffe.EltwiseParameter),0
@@protoc_insertion_point(class_scope:caffe.ELUParameter),0
@@protoc_insertion_point(class_scope:caffe.EmbedParameter),0
@@protoc_insertion_point(class_scope:caffe.ExpParameter),0
@@protoc_insertion_point(class_scope:caffe.FlattenParameter),0
@@protoc_insertion_point(class_scope:caffe.HDF5DataParameter),0
@@protoc_insertion_point(class_scope:caffe.HDF5OutputParameter),0
@@protoc_insertion_point(class_scope:caffe.HingeLossParameter),0
@@protoc_insertion_point(class_scope:caffe.ImageDataParameter),0
@@protoc_insertion_point(class_scope:caffe.InfogainLossParameter),0
@@protoc_insertion_point(class_scope:caffe.InnerProductParameter),0
@@protoc_insertion_point(class_scope:caffe.InputParameter),0
@@protoc_insertion_point(class_scope:caffe.LogParameter),0
@@protoc_insertion_point(class_scope:caffe.LRNParameter),0
@@protoc_insertion_point(class_scope:caffe.MemoryDataParameter),0
@@protoc_insertion_point(class_scope:caffe.MVNParameter),0
@@protoc_insertion_point(class_scope:caffe.ParameterParameter),0
@@protoc_insertion_point(class_scope:caffe.PoolingParameter),0
@@protoc_insertion_point(class_scope:caffe.PowerParameter),0
@@protoc_insertion_point(class_scope:caffe.PythonParameter),0
@@protoc_insertion_point(class_scope:caffe.RecurrentParameter),0
@@protoc_insertion_point(class_scope:caffe.ReductionParameter),0
@@protoc_insertion_point(class_scope:caffe.ReLUParameter),0
@@protoc_insertion_point(class_scope:caffe.ReshapeParameter),0
@@protoc_insertion_point(class_scope:caffe.ScaleParameter),0
@@protoc_insertion_point(class_scope:caffe.SigmoidParameter),0
@@protoc_insertion_point(class_scope:caffe.SliceParameter),0
@@protoc_insertion_point(class_scope:caffe.SoftmaxParameter),0
@@protoc_insertion_point(class_scope:caffe.TanHParameter),0
@@protoc_insertion_point(class_scope:caffe.TileParameter),0
@@protoc_insertion_point(class_scope:caffe.ThresholdParameter),0
@@protoc_insertion_point(class_scope:caffe.WindowDataParameter),0
@@protoc_insertion_point(class_scope:caffe.SPPParameter),0
@@protoc_insertion_point(class_scope:caffe.V1LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.V0LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.PReLUParameter),0
@@protoc_insertion_point(module_scope),0
"TODO: We need to identify whether this is 4D image data, otherwise we shouldn't change the dimension order",1
print(node.parameters),0
assert False,0
"Stochastic pooling, for instance.",0
TODO: Axis,0
TODO: Unbiased,1
check if need the Flatten layer,0
if parent.output_shape.height > 1 or parent.output_shape.width > 1:,0
TODO: The gamma parameter has to be set (in node.data?) and this should work.,1
"Also, mean should be set to 0, and var to 1, just to be safe.",0
skip the split node,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"For some reason argparser gives us unicode, so we need to conver to str first",0
for test,0
"with open(""graph.txt"", 'w') as f:",0
for layer in self.IR_graph.topological_sort:,0
current_node = self.IR_graph.get_node(layer),0
"print(""========current_node=========\n{}"".format(current_node.layer), file=f)",0
test end,0
"print(""========current_node={}"".format(current_node.layer))",0
check if have pad layer,0
Pad_h < kernel_h (vgg19 caffe2caffe),0
implement asymmetric paddings by applying symmetric padding then cropping,0
keys = [],0
for key in self.weights_dict[IR_node.name].keys():,0
keys.append(key),0
"print(""=======Layer: {}, keys: {}"".format(IR_node.name, keys))",0
"if asymmetric padding, set offset to 1",0
Change the layer name,0
check if need crop output shape,0
"change the key ""name"" to ""variable_name"", in case of the layer name has invalid characters",0
"Convert NHWC (IR) to NCHW (Caffe): [0,1,2,3]->[0,3,1,2]",0
"def emit_Square(self, IR_node):",0
"input_layers = ', '.join(('n.' + self.IR_graph.get_node(edge).real_variable_name) for edge in IR_node.in_edges)",0
"self.add_body(1, ""n.{:<15} = L.Square({}, ntop=1)"".format(",0
"IR_node.variable_name,",0
input_layers)),0
Caffe Types,0
Same value applies to all spatial dimensions,0
Extract the value for the given spatial dimension,0
consider rewrite this function to Network.py,0
Dropout layers appear in a fair number of Caffe,0
test-time networks. These are just ignored. We'll,0
filter them out here.,0
TODO: raise error,1
TODO: raise error,1
Automatically set a name if not provided.,0
Figure out the layer inputs.,0
"print('op: %s   shape: %s' % (op, layer_output._keras_shape))",0
"print('op: %s   shape: %s' % (op, layer_output.get_shape().as_list()))",0
Add to layer LUT.,0
Old-style input specification,0
New-style input specification,0
"We most likely have a data layer on our hands. The problem is,",0
Caffe infers the dimensions of the data from the source (eg: LMDB).,0
We want to avoid reading datasets here. Fail for now.,0
This can be temporarily fixed by transforming the data layer to,0
"Caffe's ""input"" layer (as is usually used in the ""deploy"" version).",0
TODO: Find a better solution for this.,1
The .prototxt file defining the graph,0
The .caffemodel file containing the learned parameters,0
Set to true if the fallback protocol-buffer based backend was used,0
"A list containing (layer name, parameters) tuples",0
Load the parameters,0
"When using the protobuf-backend, each parameter initially has four dimensions.",0
"In certain cases (like FC layers), we want to eliminate the singleton dimensions.",0
"This implementation takes care of the common cases. However, it does leave the",0
potential for future issues.,0
The Caffe-backend does not suffer from this problem.,0
A dictionary mapping NodeKind to the transposed order.,0
The node kinds eligible for reshaping,0
"If true, the reshaped data will replace the old one.",0
"Otherwise, it's set to the reshaped_data attribute.",0
Check for 2+ dimensional data,0
The FC layer connected to the spatial layer needs to be,0
re-wired to match the new spatial ordering.,0
node.reshaped_data = weights.transpose(transpose_order),0
Set the weights,0
We're only fusing nodes with single parents,0
We can only fuse a node if its parent's,0
value isn't used by any other node.,0
Rewrite the fused node's children to its parent.,0
Disconnect the fused node from the graph.,0
Let the sub-class merge the fused node in any arbitrary way.,0
Fuse ReLUs when the parent node is one of the given types.,0
"If None, all node types are eligible.",0
Prescale the stats,0
Replace with the updated values,0
Include the scale and bias terms,0
self.graph = NodeRenamer()(graph),0
"return Graph(name, [self.map_node(node) for node in self.graph.nodes])",0
assert mapped_node is not None,0
skip when mapped_node is None,0
FIXME:,0
output = node.output,0
Decompose DAG into chains,0
Generate Python code line by line,0
try:,0
except ImportError:,0
# # Fall back to the protobuf implementation,0
# from mmdnn.conversion.caffe import caffe_pb2,0
# self.caffepb = caffe_pb2,0
# show_fallback_warning(),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
from tensorflow.core.framework.node_def_pb2 import NodeDef,0
from tensorflow.core.framework import attr_value_pb2,0
pass,0
"print(""\n"")",0
print(i),0
print(block),0
continue,0
NHWC,0
assert False,0
print(shortcut_layer['input'] ),0
spacetodepth,0
print(block),0
print(region_layer),0
assert False,0
print line,0
set default value,0
"yolo3(608) start at 1, yolo2(608) start at 0. yolo2(416) start at 1, yolo3(416) start at 0",0
load weight by original order,0
print(node_type),0
print(IR_node),0
assert False,0
strides,0
assert False,0
padding,0
only load weight conv,0
"print(""----------------"",self.start)",0
print(kernel.shape),0
print(k_bias.shape),0
"buf, start, scale_layer['name'], bn_layer['name'], conv_layer['name']",0
"print(""=============="",self.start)",0
print(bias.shape),0
print(scale.shape),0
print(mean.shape),0
print(var.shape),0
print(kernel.shape),0
print(W),0
assert False,0
no use,0
print(source_node.layer),0
for image classification(resnet) AVG pooling,0
print(source_node.layer),0
print(source_node.layer),0
kwargs['ignore_thresh'] = source_node.get_attr('ignore_thresh'),0
print(source_node.get_attr('anchors')),0
"kwargs['anchors'] = ['0.52','0.22']",0
kwargs['mask'] = source_node.get_attr('mask'),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
tensorflow dump tag,0
Reference: 1) Keras image preprocess: https://github.com/keras-team/keras/blob/master/keras/applications/imagenet_utils.py,0
2) tensorflow github issue: https://github.com/tensorflow/models/issues/517,0
"R-G-B for Imagenet === [123.68, 116.78, 103.94]",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.input, self.model, self.testop = self.MainModel.KitModel(self.args.w)",0
testop = tf.get_default_graph().get_operation_by_name(layer_name),0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result(None, False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
layer_name = 'block2_pool',0
"intermediate_layer_model = keras.Model(inputs=model.input,",0
outputs=model.get_layer(layer_name).output),0
intermediate_output = intermediate_layer_model.predict(img),0
print (intermediate_output),0
print (intermediate_output.shape),0
"print (""%.30f"" % np.sum(intermediate_output))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Load a real image and do default tf imageNet preprocessing,0
"[Batch, Height, Width, Channels] --> [Batch, Channels, Height, Width]",0
We only deal with non-recurrent networks for now,0
"(H,W,C) --> (C,H,W)",0
Load TensorFlow model,0
Prepare inputs,0
Run TF session,0
Evaluate coreml model,0
evaluate the TF model,0
evaluate CoreML,0
Test the default CoreML evaluation,0
Download model,0
Convert to coreml,0
Test predictions on an image,0
Download model,0
Convert to coreml,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
"@unittest.skip(""Failing GPU backend: related to https://github.com/tf-coreml/tf-coreml/issues/26"")",0
? style transfer image size and style number?,0
Test predictions on an image,0
evaluate the TF model,0
evaluate CoreML,0
Test the default CoreML evaluation,0
"compare_tf_coreml_outputs(tf_out_flatten, coreml_out_flatten)",0
#Download model,0
url = 'https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz',0
tf_model_dir = _download_file(url = url),0
"tf_model_path = os.path.join(TMP_MODEL_DIR, 'inception_v3_2016_08_28_frozen.pb')",0
#Convert to coreml,0
"mlmodel_path = os.path.join(TMP_MODEL_DIR, 'inception_v3_2016_08_28.mlmodel')",0
mlmodel = tf_converter.convert(,0
"tf_model_path = tf_model_path,",0
"mlmodel_path = mlmodel_path,",0
"output_feature_names = ['InceptionV3/Predictions/Softmax:0'],",0
"input_name_shape_dict = {'input:0':[1,299,299,3]},",0
"image_input_names = ['input:0'],",0
"red_bias = -1,",0
"green_bias = -1,",0
"blue_bias = -1,",0
image_scale = 2.0/255.0),0
#Test predictions on an image,0
_test_coreml_model_image_input(,0
"tf_model_path = tf_model_path,",0
"coreml_model = mlmodel,",0
"input_tensor_name = 'input:0',",0
"output_tensor_name = 'InceptionV3/Predictions/Softmax:0',",0
img_size = 299),0
Download model,0
Convert to coreml,0
Test predictions on an image,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
self.test_truth(),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
from collections import namedtuple,0
"Batch = namedtuple('Batch', ['data'])",0
TODO,1
Apple has published some of their own models. They can be downloaded from https://developer.apple.com/machine-learning/.,0
"Those published models are: SqueezeNet, Places205-GoogLeNet, ResNet50, Inception v3, VGG16",0
TODO,1
load model,0
TODO: Multiple inputs,1
TODO: Multiple outputs,1
inference,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
caffe uses NCHW,0
"self.print_intermediate_result('pooling0', False)",0
delete tmp model files,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
pylint: disable=line-too-long,0
pylint: enable=line-too-long,0
"net = caffe.Net(arch_fn, weight_fn, caffe.TEST)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
# Image Classification,0
# Semantic Segmentation,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
print(r),0
d = darknet_extractor(),0
model_filename = d.download('yolov3'),0
print(model_filename),0
"image_path = ""./mmdnn/conversion/examples/data/dog.jpg""",0
"model_path = ""./""",0
d = darknet_extractor(),0
"result = d.inference('yolov3', model_filename, model_path, image_path = image_path)",0
print(result),0
print(i),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result('pooling0', False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
need to be updated,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.input, self.model, self.testop = self.MainModel.KitModel(self.args.w)",0
testop = tf.get_default_graph().get_operation_by_name(layer_name),0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"'mobilenet_v1': mobilenet_v1.mobilenet_v1_arg_scope,",0
"'mobilenet_v1' : mobilenet_v1.mobilenet_v1,",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
raise NotImplementedError(),0
load file,0
Using cuDNN since vanilla RNN,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Notes for training NASNet Cifar Model,0
-------------------------------------,0
batch_size: 32,0
learning rate: 0.025,0
cosine (single period) learning rate decay,0
auxiliary head loss weighting: 0.4,0
clip global norm of all gradients by 5,0
600 epochs with a batch size of 32,0
This is used for the drop path probabilities since it needs to increase,0
the drop out probability over the course of training.,0
Notes for training large NASNet model on ImageNet,0
-------------------------------------,0
batch size (per replica): 16,0
learning rate: 0.015 * 100,0
learning rate decay factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 100 replicas,0
auxiliary head loss weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Notes for training the mobile NASNet ImageNet model,0
-------------------------------------,0
batch size (per replica): 32,0
learning rate: 0.04 * 50,0
learning rate scaling factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 50 replicas,0
auxiliary head weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Shape of feature map before the final layer.,0
149 x 149 x 32,0
Run the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Find where to place the reduction cells or stride normal cells,0
Setup for building in the auxiliary head.,0
Run the cells,0
true_cell_num accounts for the stem cells,0
Final softmax layer,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80,0
71 x 71 x 192,0
35 x 35 x 192,0
35 x 35 x 320,0
TODO(alemi): Register intermediate endpoints,1
"17 x 17 x 1088 if output_stride == 8,",0
33 x 33 x 1088 if output_stride == 16,0
TODO(alemi): register intermediate endpoints,1
TODO(gpapan): Properly support output_stride for the rest of the net.,1
8 x 8 x 2080,0
TODO(alemi): register intermediate endpoints,1
8 x 8 x 1536,0
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which",1
can be set to False to disable pooling here (as in resnet_*()).,0
Set weight_decay for weights in conv2d and fully_connected layers.,0
Set activation_fn and parameters for batch_norm.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
=============================================================================,0
Tensorflow mandates these.,0
Conv and DepthSepConv namedtuple define layers of the MobileNet architecture,0
Conv defines 3x3 convolution layers,0
DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.,0
stride is the stride of the convolution,0
depth is the number of channels or filters in a layer,0
_CONV_DEFS specifies the MobileNet body,0
Used to find thinned depths for each layer.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
By passing filters=None,0
separable_conv2d produces only a depthwise convolution layer,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 1024,0
Set weight_decay for weights in Conv and DepthSepConv layers.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Skip path 1,0
Skip path 2,0
"First pad with 0's on the right and bottom, then shift the filter to",0
include those 0's that were added.,0
Concat and apply BN,0
Set the prev layer to the current layer if it is none,0
Check to be sure prev layer stuff is setup correctly,0
Apply conv operations,0
Combine hidden states using 'add'.,0
Add hiddenstate to the list of hiddenstates we can choose from,0
Dont stride if this is not one of the original hiddenstates,0
"Check if a stride is needed, then use a strided 1x1 here",0
Determine if a reduction should be applied to make the number of,0
filters match.,0
Return the concat of all the states,0
Scale keep prob by layer number,0
The added 2 is for the reduction cells,0
Decrease the keep probability over time,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Inception-Resnet-A,0
Inception-Resnet-B,0
Inception-Resnet-C,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
force in-place updates of mean and variance estimates,0
Moving averages ends up in the trainable variables collection,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80,0
71 x 71 x 192,0
35 x 35 x 256,0
5 x Inception-resnet-A,0
Reduction-A,0
10 x Inception-Resnet-B,0
Reduction-B,0
5 x Inception-Resnet-C,0
pylint: disable=no-member,0
"net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None,",0
"scope='Bottleneck', reuse=False)",0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
Only insert rate to params if rate > 1.,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
import self.model,0
self.model,0
how the model can not load from `***.bin`,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
pylint: disable=line-too-long,0
pylint: enable=line-too-long,0
refer to https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/v2/tests/test_rnn_layer.py#L35,0
refer to https://github.com/PaddlePaddle/Paddle/issues/7403,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
refer to https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/v2/tests/test_rnn_layer.py#L35,0
refer to https://github.com/PaddlePaddle/Paddle/issues/7403,0
the channel of input feature is 3,0
"depth should be one of 20, 32, 44, 56, 110, 1202",0
Copyright (c) Microsoft. All rights reserved.,0
Licensed under the MIT license. See LICENSE.md file in the project root,0
for full license information.,0
==============================================================================,0
"self.model, self.testop = self.MainModel.KitModel(self.args.w)",0
"self.print_intermediate_result(None, False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
pylint: disable=line-too-long,0
pylint: enable=line-too-long,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.anchors = np.array([[10,13], [16,30],[33,23],[30,61],[62,45], [59,119],[116,90],[156,198],[373,326]])",0
print(output),0
"print(out_boxes, out_scores, out_classes)",0
get random colors,0
My kingdom for a good redistributable image drawing library.,0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
yolov3 80 classes,0
"Reshape to batch, height, width, num_anchors, box_params.",0
Adjust preditions to each spatial grid point and anchor size.,0
Note: YOLO iterates over height index before width index.,0
"TODO: It works with +1, don't know why.",1
Scale boxes back to original image shape.,0
"print(""feats,anchors, num_classes, input_shape"", feats, anchors, num_classes, input_shape)",0
"print(box_xy, box_wh, box_confidence, box_class_probs)",0
"yolo_outputs order 13,26,52",0
TODO: use keras backend instead of tf.,1
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
save network structure as JSON,0
layer_name = 'block2_pool',0
"intermediate_layer_model = keras.Model(inputs=model.input,",0
outputs=model.get_layer(layer_name).output),0
intermediate_output = intermediate_layer_model.predict(img),0
print (intermediate_output),0
print (intermediate_output.shape),0
"print (""%.30f"" % np.sum(intermediate_output))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Convert network architecture only,0
MXNet,0
Caffe,0
For CoreML,0
Caffe,0
Darknet,0
"exts = ['.pb', '.npy', '.py']",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check non-sense always input module.Module,0
if not (type(model) == mx.module.Module,0
or type(model) == mx.module.SequentialModule,0
or type(model) == mx.model),0
"raise TypeError(""MXNet layer of type %s is not supported."" % type(model))",0
"if layer[""op""] == ""null"":",0
continue,0
"raise NotImplementedError(""Cannot support multi-input"")",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"mxnet-cpu only support channel first, default convert the model and weight as channel first",0
We use the real_name for specifying the input layer in data_names,0
since MXNet API wants the actual name of the layer. On the other,0
"hand, the module API wants the last symbol in the symbol chain, so",0
for the output node we need to use the actual python variable name,0
of the last layer (real_variable_name).,0
download and show the image,0
"convert into format (batch, RGB, width, height)",0
"to show the image, change the argument show into True",0
compute the predict probabilities,0
print the top-5,0
# call function predict,0
"with open('synset.txt', 'r') as f:",0
labels = [l.rstrip() for l in f],0
"predict(model, labels, 'http://writm.com/wp-content/uploads/2016/08/Cat-hd-wallpapers.jpg')",0
"print(""Warning: MXNet Convolution Layer pad does not match IR Convolution Layer pad"")",0
"layout = IR_node.IR_layer.attr[""data_format""].s",0
if layout not in MXNetEmitter.channels_last:,0
Add LeakyReLU Elu(slope not support),0
"axis = IR_node.IR_layer.attr[""axis""].i",0
"self.output_weights[IR_node.name + ""_gamma""] = np.multiply(weight_dict['scale'], weight_dict_scale['scale'])",0
"self.output_weights[IR_node.name + ""_beta""] = np.multiply(weight_dict['bias'], weight_dict_scale['scale']) + weight_dict_scale['bias']",0
not supported yet,0
"axis = IR_node.IR_layer.attr[""axis""].i",0
not supported yet,0
"axis = IR_node.IR_layer.attr[""axis""].i",0
not supported yet,0
"print(""Warning: MXNet Pooling Layer pad does not match IR Pooling Layer pad"")",0
"def emit_ConvTranspose(self, IR_node):",0
if self.weight_loaded:,0
weight_dict = self.weights[IR_node.name],0
weights = weight_dict['weights'],0
"dim = len(IR_node.IR_layer.attr[""kernel_shape""].list.i) - 2",0
kernel = list(),0
"for idx in range(0, dim):",0
"kernel.append(IR_node.IR_layer.attr[""kernel_shape""].list.i[idx])",0
stride = list(),0
"for e in IR_node.IR_layer.attr[""strides""].list.i[1:-1]:",0
stride.append(e),0
dilate = list(),0
"for e in IR_node.IR_layer.attr[""dilations""].list.i[1:-1]:",0
dilate.append(e),0
"dilate = ', '.join('%s' % i for i in dilate)",0
defuse_pad = False,0
pad = list(),0
"if ""pads"" in IR_node.IR_layer.attr:",0
output_shape = list(),0
"for e in IR_node.IR_layer.attr[""_output_shapes""].list.shape[0].dim:",0
output_shape.append(e.size),0
"# print(""Warning: MXNet Deconvolution Layer pad does not match IR Deconvolution Layer pad"")",0
"defuse_pad, pad = MXNetEmitter.transfer_pad(IR_node.IR_layer.attr[""pads""].list.i)",0
"pad = ', '.join('%s' % i for i in pad)",0
"kernel = ', '.join('%s' % i for i in kernel)",0
"stride = ', '.join('%s' % i for i in stride)",0
"num_filter = IR_node.IR_layer.attr[""kernel_shape""].list.i[-2]",0
"no_bias = not IR_node.IR_layer.attr[""use_bias""].b",0
if not no_bias and self.weight_loaded:,0
"self.output_weights[IR_node.replace_scope(IR_node.name) + ""_bias""] = weight_dict['bias']",0
"# layout = IR_node.IR_layer.attr[""data_format""].s",0
if dim == 1:,0
layout = 'NCW',0
elif dim == 2:,0
layout = 'NCHW',0
elif dim == 3:,0
layout = 'NCDHW',0
if self.weight_loaded:,0
# if layout not in MXNetEmitter.channels_last:,0
"weights = MXNetEmitter.transpose(weights, dim)",0
"self.output_weights[IR_node.replace_scope(IR_node.name) + ""_weight""] = weights",0
"code = """"",0
if not defuse_pad:,0
"code = ""{:<15} = mx.sym.Deconvolution(data = {}, kernel = ({}), stride = ({}), dilate = ({}), pad = ({}), num_filter = {}, no_bias = {}, layout = '{}', name = '{}')"".format(",0
"IR_node.replace_scope(IR_node.name),",0
"IR_node.replace_scope(IR_node.in_edges[0]),",0
"kernel,",0
"stride,",0
"dilate,",0
"pad,",0
"num_filter,",0
"no_bias,",0
"layout,",0
IR_node.replace_scope(IR_node.name)),0
else:,0
"code = self.set_pad(IR_node, code, pad)",0
"code += ""\n    {:<15} = mx.sym.Deconvolution(data = {}, kernel = ({}), stride = ({}), dilate = ({}), num_filter = {}, no_bias = {}, layout = '{}', name = '{}')"".format(",0
"IR_node.replace_scope(IR_node.name), IR_node.replace_scope(IR_node.name) + ""_pad"", kernel, stride, dilate, num_filter, no_bias, layout, IR_node.replace_scope(IR_node.name))",0
return code,0
reverse cannot support yet,0
"code = ""{:<15} = mx.sym.transpose(data = {}, axes = (0, 2, 3, 1))\n"".format(""trans"", self.parent_variable_name(IR_node))",0
save the constant into weight dict,0
param_code does not need parameter slice.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Not support yet,0
"""softrelu""  : ""SoftReLU""",0
Load the model network and weights,0
"digraph = mx.viz.plot_network(sym, save_format='jpg') # For debugging",0
digraph.render(),0
model.bind(data_shapes = data_shapes),0
model.init_params(),0
"mod.load(model_path, epoch_num)",0
return mod.get_params(),0
load the model network,0
adjust the data format,0
load model files into MXNet graph,0
data_shape arguments added to calculate infer_shape(required),0
"if isinstance(input_arg, basestring):",0
Build network graph,0
transpose to channel last,0
"name, op",0
input edge,0
output shape,0
input edge,0
attr,0
"print(""Warning: MXNet symbol pad does not support channel last"")",0
"IR_node.attr[""pads""].list.i.extend([0, 0])",0
for e in pad:,0
"IR_node.attr[""pads""].list.i.extend([e, e])",0
"IR_node.attr[""pads""].list.i.extend([0, 0])",0
"name, op",0
input edge,0
units,0
use bias (no_bias default = False),0
weights,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [NM]"".format(source_node.name, self.data_format))",0
input edge,0
output shape,0
kernel_shape,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [{}]"".format(source_node.name, self.data_format, layout))",0
groups,0
in_channel = in_channel // group,0
use_bias (no_bias default = False),0
strides,0
dilations,0
data_format,0
padding,0
weights,0
axis,0
scale,0
epsilon,0
momentum,0
weights,0
gamma,0
beta,0
mean,0
var,0
"name, op",0
input edge,0
pooling type (sum not allowed yet),0
strides,0
kernel_shape,0
padding,0
output shape,0
"name, op",0
input edge,0
output shape,0
dim,0
"def rename_log_softmax(self, source_node):",0
"raise NotImplementedError(""not support yet"")",0
"def rename_Correlation(self, source_node):",0
"raise NotImplementedError(""not support yet"")",0
"name, op",0
input edge,0
padding,0
output shape,0
kernel_shape,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [{}]"".format(source_node.name, self.data_format, layout))",0
use_bias (no_bias default = False),0
strides,0
dilations,0
data_format,0
groups,0
weights,0
"def rename_RNN(self, source_node):",0
"raise NotImplementedError(""RNN not support yet"")",0
"name, op",0
input edge,0
input_dim,0
output_dim,0
dtype,0
output shape,0
"IR only support elu and prelu from {'elu', 'leaky', 'prelu', 'rrelu'}",0
return,0
"name, op",0
gamma,0
input edge,0
"alpha [exp(x) - alpha], but mxnet attr slope [slope*(exp(x) - 1)] when x < 0",0
"IR_node.attr[""alpha""].f = float()",0
output shape,0
"raise NotImplementedError(""slope cannot convert to alpha"")",0
"def rename_InstanceNorm(self, source_node):",0
raise NotImplementedError,0
"def rename_L2Normalization(self, source_node):",0
raise NotImplementedError,0
alpha,0
beta,0
knorm,0
nsize,0
keep_prob,0
mode,0
reverse cannot support yet,0
old API target_shape not support yet,0
output shape,0
"raise NotImplementedError(""adjust output shape"")",0
dim,0
dtype,0
output shape,0
"name, op",0
input edge,0
output shape,0
axis,0
"raise NotImplementedError(""No matching IR api"")",0
"left strip the ""_"" at the beginning of the name",0
"Issue #85, #135",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
keep dims,0
axes,0
"name, op",0
epsilon,0
moving variance (var),0
print(moving_variance.name),0
gamma (scale),0
mean,0
bias,0
input node,0
output node,0
load model files into TensorFlow graph,0
extract subgraph using in_nodes and dest_nodes,0
Build network graph,0
extract subgraph using dest_nodes,0
Graph Transform,0
Get input node name,0
TODO: Fused conv and pool with padding is different from defused operators,0
strides,0
window_shape,0
pool type,0
padding,0
shape,0
strides,0
input[1] : W,0
filter,0
padding,0
output[0] : B,0
converted [dropout],0
convert [tf.contrib.layers.batch_norm],0
normal Add,0
units,0
Weights,0
FullyConnected Layer,0
"name, op",0
get Bias,0
Matmul Layer,0
Deal Dropout,0
keep prob,0
Remove nodes,0
Mul,0
Floor,0
print (source_node),0
print (source_node.layer),0
assert False,0
"def rename_RandomShuffleQueueV2(self, source_node):",0
# print(source_node.layer),0
"IR_node = self._convert_identity_operation(source_node, in_edge_count = 0, new_op = 'DataInput')",0
# IR_node.attr['shape'].shape.MergeFromString(source_node.layer.attr['_output_shapes'].list.shape[0].SerializeToString()),0
# IR_node.attr['shape'].shape.dim[0].size = -1,0
IR_node.attr['dtype'].type = self.dtype_map[source_node.layer.attr['component_types'].list.type[0]],0
paddings,0
gamma (scale),0
bias,0
Mean,0
Var,0
TODO: Current it is only for slice,1
TODO:  only for 1D,1
strides,0
input[1] : W,0
filter,0
padding,0
output[0] : B,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
check whether flatten operator should be added,0
flatten is needed,0
"self._emit_unary_operation(IR_node, ""contrib.layers.flatten"")",0
param_code does not need parameter slice.,0
skip_prefix = [,0
"""^"",",0
"""train_op"",",0
"""save"",",0
"""gradients"",",0
"""init"",",0
"""global_step"",",0
"""distort_image"",",0
"""Adagrad"",",0
],0
"""Switch""",0
"""RequantizationRange"",",0
"""Requantize"",",0
"""Identity"",",0
"""Mean"",",0
"""Cast""",0
load model files into TensorFlow graph,0
Save it to an output file,0
keep dims,0
axes,0
ssd model is transformed,0
Ax - (Au - b),0
A,0
b,0
print(sub_content),0
epsilon,0
moving variance (var) /read,0
gamma (scale),0
print(Rsqrt.out_edges),0
beta  (bias),0
moving mean (mean),0
input node,0
epsilon,0
beta,0
gamma (scale),0
"output_node = self.get_son(source_node.name, [0, 0, 0, 0], True)",0
assert False,0
Quantized model type,0
TODO: Fused conv and pool with padding is different from defused operators,0
assert variable.get_attr('_output_shapes')[0].dim[0].size == IR_node.attr['kernel_shape'].list.i[-1],0
strides,0
window_shape,0
pool type,0
padding,0
"In facenet or other newtwork using slim.batch_norm,",0
"There are two BN(train, test) skip switch and merge.",0
epsilon,0
moving variance (var) /read,0
gamma (scale),0
beta  (bias),0
moving mean (mean),0
ReduceMean,0
keep dims,0
axes,0
Skip the node as merge,0
weights,0
alpha,0
beta,0
Skip BiasAdd,0
weights,0
"input_node_perm = self.check_const(self.get_parent(source_node.name, [1], True))",0
paddings,0
for attr.shape >= 2,0
"For models built by slim.batch_norm, remove duplicate BN (eg.facenet)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Build network graph,0
"PaddleParser._set_output_shape(source_node, IR_node)",0
"name, op",0
input edge,0
For concat axis,0
only for training,0
"name, op",0
input edge,0
input edge,0
layer and spec,0
width <=> x or height <=> y,0
output shape,0
"name, op",0
it should be in the shape of height x width x inputchannel x outputchannel,0
use_bias: TODO,1
pad_dim,0
fail report because of auto_pad,0
if dilation_x == 1 and dilation_y == 1:,0
if output_x * stride_x == input_x and output_y * stride_y == input_y:,0
"auto_pad = ""SAME""",0
kwargs['auto_pad'] = auto_pad,0
elif output_x * stride_x == input_x - width + 1 and output_y * stride_y == input_y - height + 1:,0
"auto_pad = ""VALID""",0
kwargs['auto_pad'] = auto_pad,0
defuse the activation layer,0
"name, op",0
input edge,0
layer and spec,0
output shape,0
"channels_first, then axis = 1",0
epsilon,0
compute adjusted parameters,0
Reference: parameter transformation https://github.com/apple/coremltools/issues/153,0
convert type because of tensorflow,0
flatten,0
mean,0
var,0
defuse the activation layer,0
"name, op",0
input edge,0
layer and spec,0
assert False,0
output shape,0
pad_dim,0
padding mode,0
"If padding == ""SAME"": output_spatial_shape[i] = ceil(input_spatial_shape[i] / strides[i])",0
"If padding == ""VALID"": output_spatial_shape[i] = ceil((input_spatial_shape[i] - (spatial_filter_shape[i]-1) * dilation_rate[i]) / strides[i]).",0
"name, op",0
input edge,0
layer and spec,0
units,0
output shape,0
use_bias,0
Kit weight tranpose,0
weight: N x M -> C x H x W x M -> H x W x C x M -> N x M,0
,0
weights,0
need the shape TODO,1
only for training,0
"name, op",0
input edge,0
shape,0
"name, op",0
input edge,0
layer and spec,0
output shape,0
alpha,0
beta,0
nsize,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Build network graph,0
strides,0
window_shape,0
pool type,0
padding,0
TODO: Handle square,1
self._print_layer(source_node),0
print (source_node.name),0
"print (self.src_graph.get_parent(source_node.name, [0]).real_name)",0
assert False,0
Convolution,0
Kernel,0
Attributes,0
Bias,0
Activation,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
BlockFunction node,0
redirect the composite's inputs to the true inputs,0
"BlockFunctions are short-circuited, and not added to accum[]",0
Function node,0
OutputVariable node,0
def build(self):,0
"_traverse_graph(self, self.model.root_function)",0
"super(CntkGraph, self).build()",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"IR_node.get_attr('output_dim'),",0
??,0
print(IR_node.layer),0
assert False,0
"def emit_Unstack(self, IR_node):",0
"num_str = ""{}.shape[{}]"".format(self.parent_variable_name(IR_node), IR_node.get_attr('axis'))",0
axis = IR_node.get_attr('axis'),0
"parent_variable_shape = ""list({}.shape)"".format(self.parent_variable_name(IR_node)",0
"if self.IR_graph.get_parent(IR_node.name, [0]).type != 'Embedding'",0
else self.parent_variable_name(IR_node)+'.E'),0
if axis==1:,0
"shape_str = ""tuple([{}[0]*{}[{}], 1].extend({}[{}+1:]))"".format(",0
"parent_variable_shape,",0
"parent_variable_shape,",0
"str(axis),",0
"parent_variable_shape,",0
str(axis)),0
else:,0
"shape_str = ""tuple([{}[0]*{}[{}]].extend({}[1:{}]).append(1).extend({}[{}+1:]))"".format(",0
"parent_variable_shape,",0
"parent_variable_shape,",0
"str(axis),",0
"parent_variable_shape,",0
"str(axis),",0
"parent_variable_shape,",0
str(axis)),0
"code = ""{:<15} = cntk.reshape({}, {}, name='{}')"".format(",0
"IR_node.variable_name,",0
"self.parent_variable_name(IR_node),",0
"shape_str,",0
IR_node.variable_name),0
"code = ""{: <15} = cntk.reshape({}, {}.shape, name='{}')"".format(",0
"IR_node.variable_name,",0
"self.parent_variable_name(IR_node),",0
"self.parent_variable_name(IR_node),",0
IR_node.name,0
),0
return code,0
"def emit_Fill(self, IR_node):",0
"code = ""{:<15} = cntk.Constant({}, {}, name='{}')"".format(",0
"IR_node.variable_name,",0
"IR_node.get_attr('value'),",0
"self.parent_variable_name(IR_node),",0
IR_node.name),0
return code,0
param_code does not need parameter slice.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
For padding,0
print (input_shape),0
print (kernel_shape),0
print (strides),0
Not tested,0
print (input_shape),0
print (kernel_shape),0
print (strides),0
kernel_shape[idx] = (kernel_shape[idx] - 1) * dilation_rate + 1,0
print ([0] + up_list + [0] + down_list if data_format.startswith('NC') else up_list + [0] + down_list + [0]),0
print ('-----------------------------------------------------'),0
network library,0
not support multi-threads download,0
key: layer_name    value: keras layer,0
private functions,0
next_node_info.left_in_edges -= 1,0
"print ('{} --> {}'.format(src, dst))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
share functions,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
name --> (weight_name --> ndarray),0
Generated by the protocol buffer compiler.  DO NOT EDIT!,0
source: graph.proto,0
@@protoc_insertion_point(imports),0
@@protoc_insertion_point(class_scope:GraphDef),0
@@protoc_insertion_point(class_scope:NodeDef.AttrEntry),0
@@protoc_insertion_point(class_scope:NodeDef),0
@@protoc_insertion_point(class_scope:AttrValue.ListValue),0
@@protoc_insertion_point(class_scope:AttrValue),0
@@protoc_insertion_point(class_scope:TensorShape.Dim),0
@@protoc_insertion_point(class_scope:TensorShape),0
@@protoc_insertion_point(class_scope:LiteralTensor),0
@@protoc_insertion_point(module_scope),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
First try to read it as a binary file.,0
Next try to read it as a text file.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"If pattern.inputs is empty, skips the rest and accepts all the inputs.",0
"If order doesn't matter for the inputs, then make sure we match at least",0
one permutation of the inputs.,0
"def get_tensor(self, pattern_or_name):",0
op_tensor = self._get_op_tensor(pattern_or_name),0
return op_tensor[1] if op_tensor else None,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
"Python 3.3.2+ implements `yield from`, but for now:",0
"check the same pattern scope node whether have same inputs, outputs and weights.",0
"For those don't have, rename their scope names.",0
"clear out scope node, typically input constant node.",0
get sub_scopes,0
decline the suffix number,0
Obtain nodes where the scope name that satisfies top_level is top_scope and sub_level is sub_scope,0
Obtain nodes where the scope name that satisfies in sub_level is sub_scope,0
Obtain nodes where the scope name that satisfies in top_level is top_scope,0
Obtain all nodes grouped by sub_level sub_scope,0
cover the node,0
"store idx, node into a dict and sort it later to keep its topology sort.",0
in_node's out edges replace n_name with scope node name.,0
no out nodes means the last node in scope nodes should be returned,0
"if out_node is scope node, replace the scope node's inner topology list node.",0
the input parameter shoule be sliced when call func.,0
modify the in_edges in scope inner nodes. decline the :idx.,0
1. initilize scope node,0
2. get scope nodes' topology list.,0
3. rebuild the edges connection after folding these scope nodes into one node and,0
get this scope node's return variables.,0
4. rebuild graph.,0
RNN-related attrs.,0
get input params,0
self.store_const_to_top(result),0
"self.set_top_node_prop(result, pattern_name)",0
Do not include input op.,0
"TODO: pytorch, mxnet, keras, cntk",1
-*- coding: utf-8 -*-,0
from keras.layers.core import Layer,0
Arguments,0
Input shape,0
Output shape,0
Prepare broadcasting shape.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Load the model network,0
Load the model weights,0
load model files into Keras graph,0
Keras 2.1.6,0
Keras. 2.2.2,0
"_keras.utils.plot_model(model, ""model.png"", show_shapes = True)",0
Build network graph,0
TODO: Fused conv and pool with padding is different from defused operators,0
TODO: More activation functions,1
for ELU,0
input edge,0
"name, op",0
weights,0
pads,0
filter,0
"[kd, kh, kw, channel_size, filter number]",0
use_bias,0
strides,0
"[1, sd, sh, sw, 1]",0
dilations,0
"[1, dd, dh, dw, 1]",0
activation,0
"name, op",0
input edge,0
padding,0
strides,0
"[1, sd, sh, sw, 1]",0
window_shape,0
"[1, pd, ph, pw, 1]",0
"name, op",0
input edge,0
For concat axis,0
"name, op",0
input edge,0
padding,0
only for training,0
"name, op",0
input edge,0
"name, op",0
input edge,0
Merge Layers,0
only for training,0
"name, op",0
input edge,0
shape,0
only for training,0
"name, op",0
input edge,0
Core Layers,0
"name, op",0
input edge,0
units,0
use_bias,0
weights,0
activation,0
"name, op",0
input edge,0
"name, op",0
input edge,0
size,0
"name, op",0
input edge,0
input_dim,0
output_dim,0
mask_zero,0
weights,0
"name, op",0
input edge,0
units,0
use_bias,0
"for Keras, drop_out and recurrent_dropout",0
activation,0
"name, op",0
input edge,0
units,0
activation,0
weights,0
Kit TODO : need to search the tf,1
"name, op",0
input edge,0
for target shape,0
"name, op",0
input edge,0
axis,0
"Parameter arrangement in Keras: gamma, beta, mean, variance",0
scale,0
beta,0
mean,0
var,0
"name, op",0
input edge,0
for target shape,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
rot weights,0
"print(""Converting layer {}({})"".format(current_node.name, node_type))",0
TODO,1
"return input_node, 'same'",0
"assert IR_node.get_attr('group', 1) == 1",0
change dw from filters to 1,0
reset the default dilation,0
############,0
Operators #,0
############,0
"code = self._emit_merge(IR_node, ""subtract"")",0
TODO,1
"when converting from coreml model, reshape is needed after the global pooling",0
for Keras,0
It arouses some problems:,0
it can be implemented by Lambda Layer,0
https://github.com/keras-team/keras/issues/890,0
Keras == 2.1.6,0
Keras == 2.2.2,0
TODO: arguments won't be saved in keras export model,1
param_code does not need parameter slice.,0
Prepare broadcasting shape.,0
"output = Lambda(lambda x: tf.fill(x, value))(input)",0
return output,0
def _layer_Constant(self):,0
"self.add_body(0, '''",0
class my_constant(keras.layers.Layer):,0
"def __init__(self, value, **kwargs):",0
"super(my_constant, self).__init__(**kwargs)",0
self._value = value,0
"# the input is dummy, just for creating keras graph.",0
"def call(self, dummy):",0
res = K.constant(self._value),0
self.output_shapes = K.int_shape(res),0
return res,0
"def compute_output_shape(self, input_shape):",0
return self.output_shapes,0
'''),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
Kit: TODO,1
Duplicate models for weight sharing,0
Expand the sub-models,0
"with open('modelmapbydataset.json', 'w') as outfile:",0
"json.dump(new_data, outfile)",0
generate makedown script,0
add Image Classification,0
add Object Detection,0
MMdnn introduction,0
steps for model conversion,0
Generate model converter description,0
Generate models list,0
generate makedown script,0
MMdnn introduction,0
steps for model conversion,0
Generate model converter description,0
Generate models list,0
console.info(JSON.parse(e.target.result)),0
"default, draw the model.json file",0
window.onload = () => {,0
"d3.json(filePath, (error, json) => {",0
if (error) throw error;,0
// console.info(json),0
draw(json),0
}),0
},0
generate dag,0
IR model or keras model,0
path generator,0
let arrowL = `M ${points[len - 1].x} ${points[len - 1].y} l -7 -7`,0
let arrowR = `M ${points[len - 1].x} ${points[len - 1].y} l  7 -7`,0
select a layer,0
remove list,0
draw,0
mode;,0
"let scale = Math.min(window.innerWidth*0.85 / width, (window.innerHeight-85)/height)",0
"var filter = def.append(""filter"")",0
".attr(""id"", ""dropshadow"")",0
"filter.append(""feGaussianBlur"")",0
".attr(""in"", ""SourceAlpha"")",0
".attr(""stdDeviation"", 4)",0
".attr(""result"", ""blur"");",0
"// filter.append(""feOffset"")",0
"//     .attr(""in"", ""blur"")",0
"//     .attr(""dx"", 2)",0
"//     .attr(""dy"", 2)",0
"//     .attr(""result"", ""offsetBlur"");",0
"var feMerge = filter.append(""feMerge"");",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""offsetBlur"")",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""SourceGraphic"");",0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
"svg.on('keydown', ()=>{console.info('ddd')})",0
".on('mouseover', ()=>{console.info('mouse over')})",0
k = parseFloat(k)*parseFloat(k_),0
"limit x, y",0
// a trick to make text svg transform in MS Edge,0
"d3.selectAll("".labels"").classed(""tempclass"", true);",0
"setTimeout(function () { d3.selectAll("".labels"").classed(""tempclass"", false); }, 40);",0
when shift is down,0
ulti function,0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
rm the '_',0
Base64 encode: https://developers.google.com/protocol-buffers/docs/proto3,0
Search the node,0
select by id: https://stackoverflow.com/questions/37270787/uncaught-syntaxerror-failed-to-execute-queryselector-on-document,0
Scroll,0
console.info(JSON.parse(e.target.result)),0
"default, draw the model.json file",0
window.onload = () => {,0
"d3.json(filePath, (error, json) => {",0
if (error) throw error;,0
// console.info(json),0
draw(json),0
}),0
},0
generate dag,0
IR model or keras model,0
path generator,0
let arrowL = `M ${points[len - 1].x} ${points[len - 1].y} l -7 -7`,0
let arrowR = `M ${points[len - 1].x} ${points[len - 1].y} l  7 -7`,0
select a layer,0
draw,0
"let scale = Math.min(window.innerWidth*0.85 / width, (window.innerHeight-85)/height)",0
"var filter = def.append(""filter"")",0
".attr(""id"", ""dropshadow"")",0
"filter.append(""feGaussianBlur"")",0
".attr(""in"", ""SourceAlpha"")",0
".attr(""stdDeviation"", 4)",0
".attr(""result"", ""blur"");",0
"// filter.append(""feOffset"")",0
"//     .attr(""in"", ""blur"")",0
"//     .attr(""dx"", 2)",0
"//     .attr(""dy"", 2)",0
"//     .attr(""result"", ""offsetBlur"");",0
"var feMerge = filter.append(""feMerge"");",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""offsetBlur"")",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""SourceGraphic"");",0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
"svg.on('keydown', ()=>{console.info('ddd')})",0
".on('mouseover', ()=>{console.info('mouse over')})",0
k = parseFloat(k)*parseFloat(k_),0
"limit x, y",0
// a trick to make text svg transform in MS Edge,0
"d3.selectAll("".labels"").classed(""tempclass"", true);",0
"setTimeout(function () { d3.selectAll("".labels"").classed(""tempclass"", false); }, 40);",0
when shift is down,0
ulti function,0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
space = '',0
Get the long description from the README file,0
Versions should comply with PEP440.  For a discussion on single-sourcing,0
"the version across setup.py and the project code, see",0
https://packaging.python.org/en/latest/single_source_version.html,0
The project's main homepage.,0
Author details,0
Choose your license,0
See https://pypi.python.org/pypi?%3Aaction=list_classifiers,0
How mature is this project? Common values are,0
3 - Alpha,0
4 - Beta,0
5 - Production/Stable,0
Indicate who your project is intended for,0
"Pick your license as you wish (should match ""license"" above)",0
"Specify the Python versions you support here. In particular, ensure",0
"that you indicate whether you support Python 2, Python 3 or both.",0
What does your project relate to?,0
You can just specify the packages manually here if your project is,0
simple. Or you can use find_packages().,0
"Alternatively, if you want to distribute just a my_module.py, uncomment",0
this:,0
"py_modules=[""my_module""],",0
List run-time dependencies here.  These will be installed by pip when,0
"your project is installed. For an analysis of ""install_requires"" vs pip's",0
requirements files see:,0
https://packaging.python.org/en/latest/requirements.html,0
"To provide executable scripts, use entry points in preference to the",0
"""scripts"" keyword. Entry points provide cross-platform support and allow",0
pip to create the appropriate form of executable for the target platform.,0
Communication to TensorFlow server via gRPC,0
TensorFlow serving stuff to send messages,0
Command line arguments,0
Send request,0
request.inputs['input'].CopyFrom(),0
"'xception'     : [TestModels.onnx_emit],",0
"'nasnet'       : [TestModels.onnx_emit],",0
omit tensorflow lead to crash,0
"'nasnet-a_large'       : [TestModels.mxnet_emit, TestModels.pytorch_emit, TestModels.tensorflow_emit],",0
"'inception_resnet_v2'  : [TestModels.caffe_emit, TestModels.keras_emit, TestModels.mxnet_emit, TestModels.pytorch_emit, TestModels.tensorflow_emit],",0
"Function self.assertEquals has deprecated, change to assertEqual",0
"self.assertEqual(original_predict.shape, converted_predict.shape)",0
get original model prediction result,0
original to IR,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
print(original_predict),0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
get shape,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
IR to code,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
The list is in RGB oder,0
IR to Model,0
"converted_file = original_framework + '_coreml_' + architecture_name + ""_converted""",0
"converted_file = converted_file.replace('.', '_')",0
load model,0
save model,0
"coremltools.utils.save_spec(model.get_spec(), converted_file)",0
inference,0
IR to code,0
import converted model,0
"In case of odd number add the extra padding at the end for SAME_UPPER(eg. pads:[0, 2, 2, 0, 0, 3, 3, 0]) and at the beginning for SAME_LOWER(eg. pads:[0, 3, 3, 0, 0, 2, 2, 0])",0
"'xception'     : [onnx_emit],",0
"'nasnet'       : [onnx_emit],",0
"'voc-fcn8s'     : [onnx_emit], # TODO: ConvTranspose, Crop",0
"'voc-fcn16s'    : [onnx_emit], # TODO: ConvTranspose, Crop",0
"'voc-fcn32s'    : [onnx_emit], # TODO: ConvTranspose, Crop",0
"'resnet_v1_50'          : [onnx_emit], # POOL: strides > window_shape not supported due to inconsistency between CPU and GPU implementations",0
"'resnet_v1_152'         : [onnx_emit], # POOL: strides > window_shape not supported due to inconsistency between CPU and GPU implementations",0
"'resnet_v2_50'          : [onnx_emit], # POOL: strides > window_shape not supported due to inconsistency between CPU and GPU implementations",0
"'resnet_v2_152'         : [onnx_emit], # POOL: strides > window_shape not supported due to inconsistency between CPU and GPU implementations",0
"'nasnet-a_large'        : [onnx_emit], # POOL: strides > window_shape not supported due to inconsistency between CPU and GPU implementations",0
TODO: coredump,1
"'alexnet'       : [cntk_emit, keras_emit, tensorflow_emit],",0
"'nasnet'       : [tensorflow_emit, keras_emit, coreml_emit],",0
"'facenet'      : [tensorflow_emit, coreml_emit,mxnet_emit,keras_emit]  # TODO",0
"'vgg16': [tensorflow_emit],",0
'alexnet': [tensorflow_emit],0
get original model prediction result,0
def test_caffe(self):,0
try:,0
import caffe,0
"self._test_function('caffe', self.caffe_parse)",0
except ImportError:,0
"print('Please install caffe! Or caffe is not supported in your platform.', file=sys.stderr)",0
def test_cntk(self):,0
try:,0
import cntk,0
"self._test_function('cntk', self.cntk_parse)",0
except ImportError:,0
"print('Please install cntk! Or cntk is not supported in your platform.', file=sys.stderr)",0
def test_coreml(self):,0
from coremltools.models.utils import macos_version,0
"if macos_version() < (10, 13):",0
"print('Coreml is not supported in your platform.', file=sys.stderr)",0
else:,0
"self._test_function('coreml', self.coreml_parse)",0
def test_keras(self):,0
"self._test_function('keras', self.keras_parse)",0
def test_mxnet(self):,0
"self._test_function('mxnet', self.mxnet_parse)",0
def test_darknet(self):,0
"self._test_function('darknet', self.darknet_parse)",0
def test_paddle(self):,0
# omit tensorflow lead to crash,0
import tensorflow as tf,0
try:,0
import paddle.v2 as paddle,0
"self._test_function('paddle', self.paddle_parse)",0
except ImportError:,0
"print('Please install Paddlepaddle! Or Paddlepaddle is not supported in your platform.', file=sys.stderr)",0
def test_pytorch(self):,0
"self._test_function('pytorch', self.pytorch_parse)",0
def test_tensorflow(self):,0
"self._test_function('tensorflow', self.tensorflow_parse)",0
def test_tensorflow_frozen(self):,0
"self._test_function('tensorflow_frozen', self.tensorflow_frozen_parse)",0
"'vgg19'         : [TestModels.onnx_emit],",0
"'voc-fcn8s'    : [TestModels.cntk_emit, TestModels.coreml_emit, TestModels.tensorflow_emit],",0
"'vgg19'         : [TestModels.caffe_emit, TestModels.cntk_emit, TestModels.coreml_emit, TestModels.keras_emit, TestModels.mxnet_emit, TestModels.pytorch_emit, TestModels.tensorflow_emit],",0
"'voc-fcn8s'     : [TestModels.cntk_emit, TestModels.coreml_emit, TestModels.tensorflow_emit],",0
"'voc-fcn16s'    : [TestModels.cntk_emit, TestModels.coreml_emit, TestModels.tensorflow_emit],",0
"'voc-fcn32s'    : [TestModels.cntk_emit, TestModels.coreml_emit, TestModels.tensorflow_emit],",0
"'nasnet'       : [TestModels.onnx_emit],",0
"'vgg19'         : [TestModels.onnx_emit],",0
"'alexnet'       : [TestModels.caffe_emit, TestModels.cntk_emit, TestModels.coreml_emit, TestModels.mxnet_emit, TestModels.pytorch_emit, TestModels.tensorflow_emit],",0
"'nasnet-a_large'       : [TestModels.mxnet_emit, TestModels.pytorch_emit, TestModels.tensorflow_emit],",0
"'inception_resnet_v2'  : [TestModels.caffe_emit, TestModels.keras_emit, TestModels.mxnet_emit, TestModels.pytorch_emit, TestModels.tensorflow_emit],",0
"'xception'     : [TestModels.onnx_emit],",0
"'nasnet'       : [TestModels.onnx_emit],",0
!/usr/bin/python,0
major python major_python_versions as python2 and python3,0
operating system,0
do not deal,0
I don't want to deal with auto_pad,0
Don't support auto_pad current!,0
2018-02-28,0
if group is None:,0
group = 1,0
group is not support yet too.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
node,0
input,0
edge(node a <-> node b),0
,0
"key is edge name, value is src/dst node name",0
"key is initializer name, value is TensorProto",0
print(name),0
print(layer.op_type),0
n is input data,0
n is input edge,0
omit node of some type,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"if node len(in_edges), generate additional DataInput node",0
print,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
TODO,1
"'max_pool2d': convert_maxpool,",0
"'onnx::Mul': convert_elementwise_mul,",0
"'onnx::Sub': convert_elementwise_sub,",0
"'onnx::ConvTranspose': convert_convtranspose,",0
"'onnx::LeakyRelu': convert_lrelu,",0
"'onnx::Sigmoid': convert_sigmoid,",0
"'onnx::Softmax': convert_softmax,",0
"'onnx::Selu': convert_selu,",0
"'onnx::Transpose': convert_transpose,",0
"'onnx::Reshape': convert_reshape,",0
"'onnx::MatMul': convert_matmul,",0
"'onnx::Gather': convert_gather,",0
"'onnx::ReduceSum': convert_reduce_sum,",0
"'onnx::Constant': convert_constant,",0
"'onnx::Upsample': convert_upsample,",0
"'onnx::Pad': convert_padding,",0
###########,0
property #,0
###########,0
###################,0
Public Functions #,0
###################,0
test,0
cpu: https://github.com/pytorch/pytorch/issues/5286,0
Build network graph,0
"(batch, C, H, W)  & NHWC",0
#########,0
Layers #,0
#########,0
dilation,0
handle bias,0
TODO,1
output_shape,0
epsilon,0
mean,0
var,0
Kit weight tranpose,0
weight: N x M -> C x H x W x M -> H x W x C x M -> N x M,0
,0
weights,0
use_bias,0
units,0
handle weight,0
handle bias,0
###################,0
Helper Functions #,0
###################,0
print (dir(data)),0
print (i),0
"print (""    {} with shape {}"".format(j, load_weight[i][j].shape))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Scopes created in a nested scope may have initial characters,0
that are illegal as the initial character of an op name,0
"(viz. '-', '\', '/', and '_').",0
sanity check.,0
run dce first to eliminate dead parts of the graph that might have been,0
left behind by things like symbolic_override,0
construct graph,0
nodes,0
input layer,0
TODO,1
build each layer,0
input,0
"print(node_input_name ,'->', node_name)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Base Functions,0
"padding,",0
"exstr = "", value=float('-Inf')""",0
"exstr = """"",0
Change to padding defuse,0
"input_node = self._defuse_padding(IR_node, exstr)",0
Ignore it in Pytorch,0
for Keras,0
"self.add_init(2, ""self.{:<15} = torch.from_numpy(__weights_dict['{}']['value'])"".format(",0
"IR_node.variable_name,",0
IR_node.name)),0
param_code does not need parameter slice.,0
from torch.nn.parameter import Parameter,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self._type = layer.__class__.__name__.replace('Backward', '')",0
"self._name = ""{}_{}"".format(self.type, id)",0
TODO,1
"""""""",0
build graph for pytorch 0.2.0,0
"""""""",0
dummy_input = torch.autograd.Variable(torch.randn(shape)),0
output_node = self.model(dummy_input),0
search_queue = [output_node.grad_fn],0
"tmp_node = PyTorchGraphNode(output_node.grad_fn, 0)",0
self.layer_map[tmp_node.name] = tmp_node,0
visited = {output_node.grad_fn : self.layer_map[tmp_node.name]},0
idx = 0,0
node_count = 1,0
while (idx < len(search_queue)):,0
current_node = search_queue[idx],0
current_type = visited[current_node].type,0
"if hasattr(current_node, 'next_functions'):",0
"for parent, _ in current_node.next_functions:",0
"parent_type = parent.__class__.__name__.replace('Backward', '')",0
if parent_type != 'AccumulateGrad' and \,0
(parent_type != 'Transpose' or current_type != 'Addmm'):,0
if not parent in visited:,0
"tmp_node = PyTorchGraphNode(parent, node_count)",0
self.layer_map[tmp_node.name] = tmp_node,0
node_count += 1,0
visited[parent] = tmp_node,0
search_queue.append(parent),0
"self._make_connection(visited[parent].name, visited[current_node].name)",0
idx += 1,0
try:,0
return TorchGraph._forward_torch_random_input(,0
"torch_model,",0
"input_shapes,",0
is_batch=False,0
),0
except:,0
# try batch mode,0
# return TorchGraph._forward_torch_random_input(,0
"#     torch_model,",0
"#     input_shapes,",0
#     is_batch=True,0
# ),0
pass,0
tensor = torch.rand(*shape).float(),0
multi output,0
single output,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
###########,0
property #,0
###########,0
###################,0
Public Functions #,0
###################,0
Build network graph,0
#########,0
Layers #,0
#########,0
handle weight,0
handle bias,0
handle weight,0
handle bias,0
###################,0
Helper Functions #,0
###################,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
load model file into Coreml Graph,0
model.encode() convert to str --- python2 may crash due to type 'unicode',0
Build Network Graph,0
self.data_format ? TODO,1
load the model network,0
convert to Model_pb2.Model,0
determine the type of the current_node,0
staticmethods,0
"(seq, batch, C, H, W)  & NHWC",0
TODO dtype_map,1
##### convert methods,0
convolution,0
input edge,0
important!,0
reshape the weight!,0
"[2, 3, 0, 1]",0
pads,0
use_bias,0
isDeconvolution,0
"name, op",0
kwargs['kernel_shape'] = weights.shape,0
strides,0
"[1, sd, sh, sw, 1]",0
activation,0
TODO,1
padding in conv,0
"pad in IR is [x1_b, x2_b, ..., x1_e, x2_e, ...]",0
padding,0
compute padding for 'same',0
https://discuss.mxnet.io/t/pooling-and-convolution-with-same-mode/528/3,0
https://www.jianshu.com/p/05c4f1621c7e,0
padding in pooling,0
padding,0
https://discuss.mxnet.io/t/pooling-and-convolution-with-same-mode/528/3,0
TODO,1
symmetric padding,0
"name, op",0
input edge,0
For concat axis,0
"NO axis in coreml, so set the last axis",0
"The first -1 means in coreml there is one-more axis,",0
The second -1 means the last axis,0
"name, op",0
input edge,0
padding,0
Future Module TODO,1
#### rename methods,0
"name, op",0
activation type,0
else:,0
assert False,0
input edge,0
Merge layers,0
only for training,0
"name, op",0
input edge,0
shape,0
NHWC channel last,0
"in fact, here is NWHC",0
"name, op",0
input edge,0
axis TODO,0
"channels_first, then axis = 1",0
scale,0
bias,0
epsilon,0
compute adjusted parameters,0
Reference: parameter transformation https://github.com/apple/coremltools/issues/153,0
convert type because of tensorflow,0
mean,0
var,0
"name, op",0
input edge,0
bias,0
"name, op",0
input edge,0
"MAX = 0, AVERAGE = 1, L2 = 2",0
padding,0
strides,0
"[1, sd, sh, sw, 1]",0
window_shape,0
"[1, pd, ph, pw, 1]",0
"name, op",0
input edge,0
"name, op",0
input edge,0
"name, op",0
input edge,0
units,0
use_bias,0
weights,0
change to single because of the tf matmul,0
"name, op",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
the end of the tensorflow_resnet_v2_50's squeeze shape is [unknown_rank: true] with len 0,0
1001 means the 1001 classes for tensorflow_resnet_v2_50,0
!Alert! TODO,1
Future implement can be changed to the last two layer,0
TODO - remove style transfer 1D hack,1
"Input is 1D but it goes to the width dimension: (1,1,W)",0
"assume (Batch, Channels) - Batch dimension should be dropped",0
"assume (Batch, Sequence-Length, channels)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Add classifier classes (if applicable),0
Set pre-processing paramsters,0
"image_input_names,",0
Return the protobuf spec,0
model = _MLModel(self.builder.spec),0
if IR_node.name in self.weights_dict and 'weights' in self.weights_dict[IR_node.name]:,0
pass,0
"self._emit_merge(IR_node,'DOT')",0
Get input and output names,0
Dimensions and weights,0
"W should have shape (height, width, kernel_channels, output_channels), where kernel_channel = input_channels / groups",0
Dilations,0
see protobuf,0
assert False,0
Get the weights.,0
Dilations,0
see protobuf,0
depth-wise convolution,0
Dilations,0
see protobuf,0
Get input and output names,0
Pooling layer type,0
"if it's global, set the global flag",0
TODO  global pooling modification,0
Padding,0
see protobuf,0
Padding,0
see protobuf,0
Get input and output names,0
"type: ""list(int). A 1-D values of (leftBorder, topBorder, rightBorder, bottomBorder).""",0
This is central crop,0
Now add the layer,0
"Allowed values: 'CHW', 'HW', 'C', 'H', 'W'",0
Get input and output names,0
Get input and output names,0
Get the weights from keras,0
Get input and output names,0
blob_order == 0 if the input blob needs not be rearranged,0
blob_order == 1 if the input blob needs to be rearranged,0
"using keras_layer.input.shape have a ""?"" (Dimension[None] at the front),",0
making a 3D tensor with unknown batch size 4D,0
Get input and output names,0
activation emit,0
Get input and output names,0
negate it,0
apply threshold,0
negate it back,0
for Keras,0
Get input and output names,0
Set parameters,0
"Parameter arrangement in Keras: gamma, beta, mean, variance",0
compute adjusted parameters,0
Reference: parameter transformation https://github.com/apple/coremltools/issues/153,0
padding type TODO,0
"Type of the padding. Can be one of 'constant', 'reflection' or 'replication",0
Now add the layer,0
self.emit_Flatten(IR_node),0
"depth_radius: Half-width of the 1-D normalization window.""",0
Get the weights,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
input layer,0
build each layer,0
"if A.output == B.input, then make the connection: A -> B",0
"print('{0:20}->     {1:20}'.format(layerA.name, layerB.name))",0
"if A.name == B.input, then make the connection: A -> B, here A is the input",0
build each layer,0
"if A.output == B.input, then make the connection: A -> B",0
"print('{0:20}->     {1:20}'.format(layerA.name, layerB.name))",0
"if A.name == B.input, then make the connection: A -> B, here A is the input",0
build each layer,0
"if A.output == B.input, then make the connection: A -> B",0
"print('{0:20}->     {1:20}'.format(layerA.name, layerB.name))",0
"if A.name == B.input, then make the connection: A -> B, here A is the input",0
The information of the layer,0
Generated by the protocol buffer compiler.  DO NOT EDIT!,0
source: caffe.proto,0
@@protoc_insertion_point(imports),0
@@protoc_insertion_point(class_scope:caffe.BlobShape),0
@@protoc_insertion_point(class_scope:caffe.BlobProto),0
@@protoc_insertion_point(class_scope:caffe.BlobProtoVector),0
@@protoc_insertion_point(class_scope:caffe.Datum),0
@@protoc_insertion_point(class_scope:caffe.FillerParameter),0
@@protoc_insertion_point(class_scope:caffe.NetParameter),0
@@protoc_insertion_point(class_scope:caffe.SolverParameter),0
@@protoc_insertion_point(class_scope:caffe.SolverState),0
@@protoc_insertion_point(class_scope:caffe.NetState),0
@@protoc_insertion_point(class_scope:caffe.NetStateRule),0
@@protoc_insertion_point(class_scope:caffe.ParamSpec),0
@@protoc_insertion_point(class_scope:caffe.LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.TransformationParameter),0
@@protoc_insertion_point(class_scope:caffe.LossParameter),0
@@protoc_insertion_point(class_scope:caffe.AccuracyParameter),0
@@protoc_insertion_point(class_scope:caffe.ArgMaxParameter),0
@@protoc_insertion_point(class_scope:caffe.ConcatParameter),0
@@protoc_insertion_point(class_scope:caffe.BatchNormParameter),0
@@protoc_insertion_point(class_scope:caffe.BiasParameter),0
@@protoc_insertion_point(class_scope:caffe.ContrastiveLossParameter),0
@@protoc_insertion_point(class_scope:caffe.ConvolutionParameter),0
@@protoc_insertion_point(class_scope:caffe.CropParameter),0
@@protoc_insertion_point(class_scope:caffe.DataParameter),0
@@protoc_insertion_point(class_scope:caffe.DropoutParameter),0
@@protoc_insertion_point(class_scope:caffe.DummyDataParameter),0
@@protoc_insertion_point(class_scope:caffe.EltwiseParameter),0
@@protoc_insertion_point(class_scope:caffe.ELUParameter),0
@@protoc_insertion_point(class_scope:caffe.EmbedParameter),0
@@protoc_insertion_point(class_scope:caffe.ExpParameter),0
@@protoc_insertion_point(class_scope:caffe.FlattenParameter),0
@@protoc_insertion_point(class_scope:caffe.HDF5DataParameter),0
@@protoc_insertion_point(class_scope:caffe.HDF5OutputParameter),0
@@protoc_insertion_point(class_scope:caffe.HingeLossParameter),0
@@protoc_insertion_point(class_scope:caffe.ImageDataParameter),0
@@protoc_insertion_point(class_scope:caffe.InfogainLossParameter),0
@@protoc_insertion_point(class_scope:caffe.InnerProductParameter),0
@@protoc_insertion_point(class_scope:caffe.InputParameter),0
@@protoc_insertion_point(class_scope:caffe.LogParameter),0
@@protoc_insertion_point(class_scope:caffe.LRNParameter),0
@@protoc_insertion_point(class_scope:caffe.MemoryDataParameter),0
@@protoc_insertion_point(class_scope:caffe.MVNParameter),0
@@protoc_insertion_point(class_scope:caffe.ParameterParameter),0
@@protoc_insertion_point(class_scope:caffe.PoolingParameter),0
@@protoc_insertion_point(class_scope:caffe.PowerParameter),0
@@protoc_insertion_point(class_scope:caffe.PythonParameter),0
@@protoc_insertion_point(class_scope:caffe.RecurrentParameter),0
@@protoc_insertion_point(class_scope:caffe.ReductionParameter),0
@@protoc_insertion_point(class_scope:caffe.ReLUParameter),0
@@protoc_insertion_point(class_scope:caffe.ReshapeParameter),0
@@protoc_insertion_point(class_scope:caffe.ScaleParameter),0
@@protoc_insertion_point(class_scope:caffe.SigmoidParameter),0
@@protoc_insertion_point(class_scope:caffe.SliceParameter),0
@@protoc_insertion_point(class_scope:caffe.SoftmaxParameter),0
@@protoc_insertion_point(class_scope:caffe.TanHParameter),0
@@protoc_insertion_point(class_scope:caffe.TileParameter),0
@@protoc_insertion_point(class_scope:caffe.ThresholdParameter),0
@@protoc_insertion_point(class_scope:caffe.WindowDataParameter),0
@@protoc_insertion_point(class_scope:caffe.SPPParameter),0
@@protoc_insertion_point(class_scope:caffe.V1LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.V0LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.PReLUParameter),0
@@protoc_insertion_point(module_scope),0
"TODO: We need to identify whether this is 4D image data, otherwise we shouldn't change the dimension order",1
print(node.parameters),0
assert False,0
"Stochastic pooling, for instance.",0
TODO: Axis,0
TODO: Unbiased,1
check if need the Flatten layer,0
if parent.output_shape.height > 1 or parent.output_shape.width > 1:,0
TODO: The gamma parameter has to be set (in node.data?) and this should work.,1
"Also, mean should be set to 0, and var to 1, just to be safe.",0
skip the split node,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"For some reason argparser gives us unicode, so we need to conver to str first",0
for test,0
"with open(""graph.txt"", 'w') as f:",0
for layer in self.IR_graph.topological_sort:,0
current_node = self.IR_graph.get_node(layer),0
"print(""========current_node=========\n{}"".format(current_node.layer), file=f)",0
test end,0
"print(""========current_node={}"".format(current_node.layer))",0
check if have pad layer,0
Pad_h < kernel_h (vgg19 caffe2caffe),0
implement asymmetric paddings by applying symmetric padding then cropping,0
keys = [],0
for key in self.weights_dict[IR_node.name].keys():,0
keys.append(key),0
"print(""=======Layer: {}, keys: {}"".format(IR_node.name, keys))",0
"if asymmetric padding, set offset to 1",0
Change the layer name,0
check if need crop output shape,0
"change the key ""name"" to ""variable_name"", in case of the layer name has invalid characters",0
"Convert NHWC (IR) to NCHW (Caffe): [0,1,2,3]->[0,3,1,2]",0
"def emit_Square(self, IR_node):",0
"input_layers = ', '.join(('n.' + self.IR_graph.get_node(edge).real_variable_name) for edge in IR_node.in_edges)",0
"self.add_body(1, ""n.{:<15} = L.Square({}, ntop=1)"".format(",0
"IR_node.variable_name,",0
input_layers)),0
Caffe Types,0
Same value applies to all spatial dimensions,0
Extract the value for the given spatial dimension,0
consider rewrite this function to Network.py,0
Dropout layers appear in a fair number of Caffe,0
test-time networks. These are just ignored. We'll,0
filter them out here.,0
TODO: raise error,1
TODO: raise error,1
Automatically set a name if not provided.,0
Figure out the layer inputs.,0
"print('op: %s   shape: %s' % (op, layer_output._keras_shape))",0
"print('op: %s   shape: %s' % (op, layer_output.get_shape().as_list()))",0
Add to layer LUT.,0
Old-style input specification,0
New-style input specification,0
"We most likely have a data layer on our hands. The problem is,",0
Caffe infers the dimensions of the data from the source (eg: LMDB).,0
We want to avoid reading datasets here. Fail for now.,0
This can be temporarily fixed by transforming the data layer to,0
"Caffe's ""input"" layer (as is usually used in the ""deploy"" version).",0
TODO: Find a better solution for this.,1
The .prototxt file defining the graph,0
The .caffemodel file containing the learned parameters,0
Set to true if the fallback protocol-buffer based backend was used,0
"A list containing (layer name, parameters) tuples",0
Load the parameters,0
"When using the protobuf-backend, each parameter initially has four dimensions.",0
"In certain cases (like FC layers), we want to eliminate the singleton dimensions.",0
"This implementation takes care of the common cases. However, it does leave the",0
potential for future issues.,0
The Caffe-backend does not suffer from this problem.,0
A dictionary mapping NodeKind to the transposed order.,0
The node kinds eligible for reshaping,0
"If true, the reshaped data will replace the old one.",0
"Otherwise, it's set to the reshaped_data attribute.",0
Check for 2+ dimensional data,0
The FC layer connected to the spatial layer needs to be,0
re-wired to match the new spatial ordering.,0
node.reshaped_data = weights.transpose(transpose_order),0
Set the weights,0
We're only fusing nodes with single parents,0
We can only fuse a node if its parent's,0
value isn't used by any other node.,0
Rewrite the fused node's children to its parent.,0
Disconnect the fused node from the graph.,0
Let the sub-class merge the fused node in any arbitrary way.,0
Fuse ReLUs when the parent node is one of the given types.,0
"If None, all node types are eligible.",0
Prescale the stats,0
Replace with the updated values,0
Include the scale and bias terms,0
self.graph = NodeRenamer()(graph),0
"return Graph(name, [self.map_node(node) for node in self.graph.nodes])",0
assert mapped_node is not None,0
skip when mapped_node is None,0
FIXME:,0
output = node.output,0
Decompose DAG into chains,0
Generate Python code line by line,0
try:,0
except ImportError:,0
# # Fall back to the protobuf implementation,0
# from mmdnn.conversion.caffe import caffe_pb2,0
# self.caffepb = caffe_pb2,0
# show_fallback_warning(),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
from tensorflow.core.framework.node_def_pb2 import NodeDef,0
from tensorflow.core.framework import attr_value_pb2,0
pass,0
"print(""\n"")",0
print(i),0
print(block),0
continue,0
NHWC,0
assert False,0
print(shortcut_layer['input'] ),0
spacetodepth,0
print(block),0
print(region_layer),0
assert False,0
print line,0
set default value,0
"yolo3(608) start at 1, yolo2(608) start at 0. yolo2(416) start at 1, yolo3(416) start at 0",0
load weight by original order,0
print(node_type),0
print(IR_node),0
assert False,0
strides,0
assert False,0
padding,0
only load weight conv,0
"print(""----------------"",self.start)",0
print(kernel.shape),0
print(k_bias.shape),0
"buf, start, scale_layer['name'], bn_layer['name'], conv_layer['name']",0
"print(""=============="",self.start)",0
print(bias.shape),0
print(scale.shape),0
print(mean.shape),0
print(var.shape),0
print(kernel.shape),0
print(W),0
assert False,0
no use,0
print(source_node.layer),0
for image classification(resnet) AVG pooling,0
print(source_node.layer),0
print(source_node.layer),0
kwargs['ignore_thresh'] = source_node.get_attr('ignore_thresh'),0
print(source_node.get_attr('anchors')),0
"kwargs['anchors'] = ['0.52','0.22']",0
kwargs['mask'] = source_node.get_attr('mask'),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
tensorflow dump tag,0
Reference: 1) Keras image preprocess: https://github.com/keras-team/keras/blob/master/keras/applications/imagenet_utils.py,0
2) tensorflow github issue: https://github.com/tensorflow/models/issues/517,0
"R-G-B for Imagenet === [123.68, 116.78, 103.94]",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.input, self.model, self.testop = self.MainModel.KitModel(self.args.w)",0
testop = tf.get_default_graph().get_operation_by_name(layer_name),0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result(None, False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
layer_name = 'block2_pool',0
"intermediate_layer_model = keras.Model(inputs=model.input,",0
outputs=model.get_layer(layer_name).output),0
intermediate_output = intermediate_layer_model.predict(img),0
print (intermediate_output),0
print (intermediate_output.shape),0
"print (""%.30f"" % np.sum(intermediate_output))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Load a real image and do default tf imageNet preprocessing,0
"[Batch, Height, Width, Channels] --> [Batch, Channels, Height, Width]",0
We only deal with non-recurrent networks for now,0
"(H,W,C) --> (C,H,W)",0
Load TensorFlow model,0
Prepare inputs,0
Run TF session,0
Evaluate coreml model,0
evaluate the TF model,0
evaluate CoreML,0
Test the default CoreML evaluation,0
Download model,0
Convert to coreml,0
Test predictions on an image,0
Download model,0
Convert to coreml,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
"@unittest.skip(""Failing GPU backend: related to https://github.com/tf-coreml/tf-coreml/issues/26"")",0
? style transfer image size and style number?,0
Test predictions on an image,0
evaluate the TF model,0
evaluate CoreML,0
Test the default CoreML evaluation,0
"compare_tf_coreml_outputs(tf_out_flatten, coreml_out_flatten)",0
#Download model,0
url = 'https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz',0
tf_model_dir = _download_file(url = url),0
"tf_model_path = os.path.join(TMP_MODEL_DIR, 'inception_v3_2016_08_28_frozen.pb')",0
#Convert to coreml,0
"mlmodel_path = os.path.join(TMP_MODEL_DIR, 'inception_v3_2016_08_28.mlmodel')",0
mlmodel = tf_converter.convert(,0
"tf_model_path = tf_model_path,",0
"mlmodel_path = mlmodel_path,",0
"output_feature_names = ['InceptionV3/Predictions/Softmax:0'],",0
"input_name_shape_dict = {'input:0':[1,299,299,3]},",0
"image_input_names = ['input:0'],",0
"red_bias = -1,",0
"green_bias = -1,",0
"blue_bias = -1,",0
image_scale = 2.0/255.0),0
#Test predictions on an image,0
_test_coreml_model_image_input(,0
"tf_model_path = tf_model_path,",0
"coreml_model = mlmodel,",0
"input_tensor_name = 'input:0',",0
"output_tensor_name = 'InceptionV3/Predictions/Softmax:0',",0
img_size = 299),0
Download model,0
Convert to coreml,0
Test predictions on an image,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
self.test_truth(),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
from collections import namedtuple,0
"Batch = namedtuple('Batch', ['data'])",0
TODO,1
Apple has published some of their own models. They can be downloaded from https://developer.apple.com/machine-learning/.,0
"Those published models are: SqueezeNet, Places205-GoogLeNet, ResNet50, Inception v3, VGG16",0
TODO,1
load model,0
TODO: Multiple inputs,1
TODO: Multiple outputs,1
inference,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
caffe uses NCHW,0
"self.print_intermediate_result('pooling0', False)",0
delete tmp model files,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
pylint: disable=line-too-long,0
pylint: enable=line-too-long,0
"net = caffe.Net(arch_fn, weight_fn, caffe.TEST)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
# Image Classification,0
# Semantic Segmentation,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
print(r),0
d = darknet_extractor(),0
model_filename = d.download('yolov3'),0
print(model_filename),0
"image_path = ""./mmdnn/conversion/examples/data/dog.jpg""",0
"model_path = ""./""",0
d = darknet_extractor(),0
"result = d.inference('yolov3', model_filename, model_path, image_path = image_path)",0
print(result),0
print(i),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result('pooling0', False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
need to be updated,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.input, self.model, self.testop = self.MainModel.KitModel(self.args.w)",0
testop = tf.get_default_graph().get_operation_by_name(layer_name),0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"'mobilenet_v1': mobilenet_v1.mobilenet_v1_arg_scope,",0
"'mobilenet_v1' : mobilenet_v1.mobilenet_v1,",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
raise NotImplementedError(),0
load file,0
Using cuDNN since vanilla RNN,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Notes for training NASNet Cifar Model,0
-------------------------------------,0
batch_size: 32,0
learning rate: 0.025,0
cosine (single period) learning rate decay,0
auxiliary head loss weighting: 0.4,0
clip global norm of all gradients by 5,0
600 epochs with a batch size of 32,0
This is used for the drop path probabilities since it needs to increase,0
the drop out probability over the course of training.,0
Notes for training large NASNet model on ImageNet,0
-------------------------------------,0
batch size (per replica): 16,0
learning rate: 0.015 * 100,0
learning rate decay factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 100 replicas,0
auxiliary head loss weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Notes for training the mobile NASNet ImageNet model,0
-------------------------------------,0
batch size (per replica): 32,0
learning rate: 0.04 * 50,0
learning rate scaling factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 50 replicas,0
auxiliary head weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Shape of feature map before the final layer.,0
149 x 149 x 32,0
Run the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Find where to place the reduction cells or stride normal cells,0
Setup for building in the auxiliary head.,0
Run the cells,0
true_cell_num accounts for the stem cells,0
Final softmax layer,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80,0
71 x 71 x 192,0
35 x 35 x 192,0
35 x 35 x 320,0
TODO(alemi): Register intermediate endpoints,1
"17 x 17 x 1088 if output_stride == 8,",0
33 x 33 x 1088 if output_stride == 16,0
TODO(alemi): register intermediate endpoints,1
TODO(gpapan): Properly support output_stride for the rest of the net.,1
8 x 8 x 2080,0
TODO(alemi): register intermediate endpoints,1
8 x 8 x 1536,0
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which",1
can be set to False to disable pooling here (as in resnet_*()).,0
Set weight_decay for weights in conv2d and fully_connected layers.,0
Set activation_fn and parameters for batch_norm.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
=============================================================================,0
Tensorflow mandates these.,0
Conv and DepthSepConv namedtuple define layers of the MobileNet architecture,0
Conv defines 3x3 convolution layers,0
DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.,0
stride is the stride of the convolution,0
depth is the number of channels or filters in a layer,0
_CONV_DEFS specifies the MobileNet body,0
Used to find thinned depths for each layer.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
By passing filters=None,0
separable_conv2d produces only a depthwise convolution layer,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 1024,0
Set weight_decay for weights in Conv and DepthSepConv layers.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Skip path 1,0
Skip path 2,0
"First pad with 0's on the right and bottom, then shift the filter to",0
include those 0's that were added.,0
Concat and apply BN,0
Set the prev layer to the current layer if it is none,0
Check to be sure prev layer stuff is setup correctly,0
Apply conv operations,0
Combine hidden states using 'add'.,0
Add hiddenstate to the list of hiddenstates we can choose from,0
Dont stride if this is not one of the original hiddenstates,0
"Check if a stride is needed, then use a strided 1x1 here",0
Determine if a reduction should be applied to make the number of,0
filters match.,0
Return the concat of all the states,0
Scale keep prob by layer number,0
The added 2 is for the reduction cells,0
Decrease the keep probability over time,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Inception-Resnet-A,0
Inception-Resnet-B,0
Inception-Resnet-C,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
force in-place updates of mean and variance estimates,0
Moving averages ends up in the trainable variables collection,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80,0
71 x 71 x 192,0
35 x 35 x 256,0
5 x Inception-resnet-A,0
Reduction-A,0
10 x Inception-Resnet-B,0
Reduction-B,0
5 x Inception-Resnet-C,0
pylint: disable=no-member,0
"net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None,",0
"scope='Bottleneck', reuse=False)",0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
Only insert rate to params if rate > 1.,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
import self.model,0
self.model,0
how the model can not load from `***.bin`,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
pylint: disable=line-too-long,0
pylint: enable=line-too-long,0
refer to https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/v2/tests/test_rnn_layer.py#L35,0
refer to https://github.com/PaddlePaddle/Paddle/issues/7403,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
refer to https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/v2/tests/test_rnn_layer.py#L35,0
refer to https://github.com/PaddlePaddle/Paddle/issues/7403,0
the channel of input feature is 3,0
"depth should be one of 20, 32, 44, 56, 110, 1202",0
Copyright (c) Microsoft. All rights reserved.,0
Licensed under the MIT license. See LICENSE.md file in the project root,0
for full license information.,0
==============================================================================,0
"self.model, self.testop = self.MainModel.KitModel(self.args.w)",0
"self.print_intermediate_result(None, False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
pylint: disable=line-too-long,0
pylint: enable=line-too-long,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.anchors = np.array([[10,13], [16,30],[33,23],[30,61],[62,45], [59,119],[116,90],[156,198],[373,326]])",0
print(output),0
"print(out_boxes, out_scores, out_classes)",0
get random colors,0
My kingdom for a good redistributable image drawing library.,0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
yolov3 80 classes,0
"Reshape to batch, height, width, num_anchors, box_params.",0
Adjust preditions to each spatial grid point and anchor size.,0
Note: YOLO iterates over height index before width index.,0
"TODO: It works with +1, don't know why.",1
Scale boxes back to original image shape.,0
"print(""feats,anchors, num_classes, input_shape"", feats, anchors, num_classes, input_shape)",0
"print(box_xy, box_wh, box_confidence, box_class_probs)",0
"yolo_outputs order 13,26,52",0
TODO: use keras backend instead of tf.,1
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
save network structure as JSON,0
layer_name = 'block2_pool',0
"intermediate_layer_model = keras.Model(inputs=model.input,",0
outputs=model.get_layer(layer_name).output),0
intermediate_output = intermediate_layer_model.predict(img),0
print (intermediate_output),0
print (intermediate_output.shape),0
"print (""%.30f"" % np.sum(intermediate_output))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Convert network architecture only,0
MXNet,0
Caffe,0
For CoreML,0
Caffe,0
Darknet,0
"exts = ['.pb', '.npy', '.py']",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check non-sense always input module.Module,0
if not (type(model) == mx.module.Module,0
or type(model) == mx.module.SequentialModule,0
or type(model) == mx.model),0
"raise TypeError(""MXNet layer of type %s is not supported."" % type(model))",0
"if layer[""op""] == ""null"":",0
continue,0
"raise NotImplementedError(""Cannot support multi-input"")",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"mxnet-cpu only support channel first, default convert the model and weight as channel first",0
We use the real_name for specifying the input layer in data_names,0
since MXNet API wants the actual name of the layer. On the other,0
"hand, the module API wants the last symbol in the symbol chain, so",0
for the output node we need to use the actual python variable name,0
of the last layer (real_variable_name).,0
download and show the image,0
"convert into format (batch, RGB, width, height)",0
"to show the image, change the argument show into True",0
compute the predict probabilities,0
print the top-5,0
# call function predict,0
"with open('synset.txt', 'r') as f:",0
labels = [l.rstrip() for l in f],0
"predict(model, labels, 'http://writm.com/wp-content/uploads/2016/08/Cat-hd-wallpapers.jpg')",0
"print(""Warning: MXNet Convolution Layer pad does not match IR Convolution Layer pad"")",0
"layout = IR_node.IR_layer.attr[""data_format""].s",0
if layout not in MXNetEmitter.channels_last:,0
Add LeakyReLU Elu(slope not support),0
"axis = IR_node.IR_layer.attr[""axis""].i",0
not supported yet,0
"axis = IR_node.IR_layer.attr[""axis""].i",0
not supported yet,0
"axis = IR_node.IR_layer.attr[""axis""].i",0
not supported yet,0
"print(""Warning: MXNet Pooling Layer pad does not match IR Pooling Layer pad"")",0
"def emit_ConvTranspose(self, IR_node):",0
if self.weight_loaded:,0
weight_dict = self.weights[IR_node.name],0
weights = weight_dict['weights'],0
"dim = len(IR_node.IR_layer.attr[""kernel_shape""].list.i) - 2",0
kernel = list(),0
"for idx in range(0, dim):",0
"kernel.append(IR_node.IR_layer.attr[""kernel_shape""].list.i[idx])",0
stride = list(),0
"for e in IR_node.IR_layer.attr[""strides""].list.i[1:-1]:",0
stride.append(e),0
dilate = list(),0
"for e in IR_node.IR_layer.attr[""dilations""].list.i[1:-1]:",0
dilate.append(e),0
"dilate = ', '.join('%s' % i for i in dilate)",0
defuse_pad = False,0
pad = list(),0
"if ""pads"" in IR_node.IR_layer.attr:",0
output_shape = list(),0
"for e in IR_node.IR_layer.attr[""_output_shapes""].list.shape[0].dim:",0
output_shape.append(e.size),0
"# print(""Warning: MXNet Deconvolution Layer pad does not match IR Deconvolution Layer pad"")",0
"defuse_pad, pad = MXNetEmitter.transfer_pad(IR_node.IR_layer.attr[""pads""].list.i)",0
"pad = ', '.join('%s' % i for i in pad)",0
"kernel = ', '.join('%s' % i for i in kernel)",0
"stride = ', '.join('%s' % i for i in stride)",0
"num_filter = IR_node.IR_layer.attr[""kernel_shape""].list.i[-2]",0
"no_bias = not IR_node.IR_layer.attr[""use_bias""].b",0
if not no_bias and self.weight_loaded:,0
"self.output_weights[IR_node.replace_scope(IR_node.name) + ""_bias""] = weight_dict['bias']",0
"# layout = IR_node.IR_layer.attr[""data_format""].s",0
if dim == 1:,0
layout = 'NCW',0
elif dim == 2:,0
layout = 'NCHW',0
elif dim == 3:,0
layout = 'NCDHW',0
if self.weight_loaded:,0
# if layout not in MXNetEmitter.channels_last:,0
"weights = MXNetEmitter.transpose(weights, dim)",0
"self.output_weights[IR_node.replace_scope(IR_node.name) + ""_weight""] = weights",0
"code = """"",0
if not defuse_pad:,0
"code = ""{:<15} = mx.sym.Deconvolution(data = {}, kernel = ({}), stride = ({}), dilate = ({}), pad = ({}), num_filter = {}, no_bias = {}, layout = '{}', name = '{}')"".format(",0
"IR_node.replace_scope(IR_node.name),",0
"IR_node.replace_scope(IR_node.in_edges[0]),",0
"kernel,",0
"stride,",0
"dilate,",0
"pad,",0
"num_filter,",0
"no_bias,",0
"layout,",0
IR_node.replace_scope(IR_node.name)),0
else:,0
"code = self.set_pad(IR_node, code, pad)",0
"code += ""\n    {:<15} = mx.sym.Deconvolution(data = {}, kernel = ({}), stride = ({}), dilate = ({}), num_filter = {}, no_bias = {}, layout = '{}', name = '{}')"".format(",0
"IR_node.replace_scope(IR_node.name), IR_node.replace_scope(IR_node.name) + ""_pad"", kernel, stride, dilate, num_filter, no_bias, layout, IR_node.replace_scope(IR_node.name))",0
return code,0
reverse cannot support yet,0
"code = ""{:<15} = mx.sym.transpose(data = {}, axes = (0, 2, 3, 1))\n"".format(""trans"", self.parent_variable_name(IR_node))",0
save the constant into weight dict,0
param_code does not need parameter slice.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Not support yet,0
"""softrelu""  : ""SoftReLU""",0
Load the model network and weights,0
"digraph = mx.viz.plot_network(sym, save_format='jpg') # For debugging",0
digraph.render(),0
model.bind(data_shapes = data_shapes),0
model.init_params(),0
"mod.load(model_path, epoch_num)",0
return mod.get_params(),0
load the model network,0
adjust the data format,0
load model files into MXNet graph,0
data_shape arguments added to calculate infer_shape(required),0
"if isinstance(input_arg, basestring):",0
Build network graph,0
transpose to channel last,0
"name, op",0
input edge,0
output shape,0
input edge,0
attr,0
"print(""Warning: MXNet symbol pad does not support channel last"")",0
"IR_node.attr[""pads""].list.i.extend([0, 0])",0
for e in pad:,0
"IR_node.attr[""pads""].list.i.extend([e, e])",0
"IR_node.attr[""pads""].list.i.extend([0, 0])",0
"name, op",0
input edge,0
units,0
use bias (no_bias default = False),0
weights,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [NM]"".format(source_node.name, self.data_format))",0
input edge,0
output shape,0
kernel_shape,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [{}]"".format(source_node.name, self.data_format, layout))",0
groups,0
in_channel = in_channel // group,0
use_bias (no_bias default = False),0
strides,0
dilations,0
data_format,0
padding,0
weights,0
axis,0
scale,0
epsilon,0
momentum,0
weights,0
gamma,0
beta,0
mean,0
var,0
"name, op",0
input edge,0
pooling type (sum not allowed yet),0
strides,0
kernel_shape,0
padding,0
output shape,0
"name, op",0
input edge,0
output shape,0
dim,0
"def rename_log_softmax(self, source_node):",0
"raise NotImplementedError(""not support yet"")",0
"def rename_Correlation(self, source_node):",0
"raise NotImplementedError(""not support yet"")",0
"name, op",0
input edge,0
padding,0
output shape,0
kernel_shape,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [{}]"".format(source_node.name, self.data_format, layout))",0
use_bias (no_bias default = False),0
strides,0
dilations,0
data_format,0
groups,0
weights,0
"def rename_RNN(self, source_node):",0
"raise NotImplementedError(""RNN not support yet"")",0
"name, op",0
input edge,0
input_dim,0
output_dim,0
dtype,0
output shape,0
"IR only support elu and prelu from {'elu', 'leaky', 'prelu', 'rrelu'}",0
return,0
"name, op",0
gamma,0
input edge,0
"alpha [exp(x) - alpha], but mxnet attr slope [slope*(exp(x) - 1)] when x < 0",0
"IR_node.attr[""alpha""].f = float()",0
output shape,0
"raise NotImplementedError(""slope cannot convert to alpha"")",0
"def rename_InstanceNorm(self, source_node):",0
raise NotImplementedError,0
"def rename_L2Normalization(self, source_node):",0
raise NotImplementedError,0
alpha,0
beta,0
knorm,0
nsize,0
keep_prob,0
mode,0
reverse cannot support yet,0
old API target_shape not support yet,0
output shape,0
"raise NotImplementedError(""adjust output shape"")",0
dim,0
dtype,0
output shape,0
"name, op",0
input edge,0
output shape,0
axis,0
"raise NotImplementedError(""No matching IR api"")",0
"left strip the ""_"" at the beginning of the name",0
"Issue #85, #135",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
keep dims,0
axes,0
"name, op",0
epsilon,0
moving variance (var),0
print(moving_variance.name),0
gamma (scale),0
mean,0
bias,0
input node,0
output node,0
load model files into TensorFlow graph,0
extract subgraph using in_nodes and dest_nodes,0
Build network graph,0
extract subgraph using dest_nodes,0
Graph Transform,0
Get input node name,0
TODO: Fused conv and pool with padding is different from defused operators,0
strides,0
window_shape,0
pool type,0
padding,0
shape,0
strides,0
input[1] : W,0
filter,0
padding,0
output[0] : B,0
converted [dropout],0
convert [tf.contrib.layers.batch_norm],0
normal Add,0
units,0
Weights,0
FullyConnected Layer,0
"name, op",0
get Bias,0
Matmul Layer,0
Deal Dropout,0
keep prob,0
Remove nodes,0
Mul,0
Floor,0
print (source_node),0
print (source_node.layer),0
assert False,0
"def rename_RandomShuffleQueueV2(self, source_node):",0
# print(source_node.layer),0
"IR_node = self._convert_identity_operation(source_node, in_edge_count = 0, new_op = 'DataInput')",0
# IR_node.attr['shape'].shape.MergeFromString(source_node.layer.attr['_output_shapes'].list.shape[0].SerializeToString()),0
# IR_node.attr['shape'].shape.dim[0].size = -1,0
IR_node.attr['dtype'].type = self.dtype_map[source_node.layer.attr['component_types'].list.type[0]],0
paddings,0
gamma (scale),0
bias,0
Mean,0
Var,0
TODO: Current it is only for slice,1
TODO:  only for 1D,1
strides,0
input[1] : W,0
filter,0
padding,0
output[0] : B,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
check whether flatten operator should be added,0
flatten is needed,0
"self._emit_unary_operation(IR_node, ""contrib.layers.flatten"")",0
param_code does not need parameter slice.,0
skip_prefix = [,0
"""^"",",0
"""train_op"",",0
"""save"",",0
"""gradients"",",0
"""init"",",0
"""global_step"",",0
"""distort_image"",",0
"""Adagrad"",",0
],0
"""Switch""",0
"""RequantizationRange"",",0
"""Requantize"",",0
"""Identity"",",0
"""Mean"",",0
"""Cast""",0
load model files into TensorFlow graph,0
Save it to an output file,0
keep dims,0
axes,0
ssd model is transformed,0
Ax - (Au - b),0
A,0
b,0
print(sub_content),0
epsilon,0
moving variance (var) /read,0
gamma (scale),0
print(Rsqrt.out_edges),0
beta  (bias),0
moving mean (mean),0
input node,0
epsilon,0
beta,0
gamma (scale),0
"output_node = self.get_son(source_node.name, [0, 0, 0, 0], True)",0
assert False,0
Quantized model type,0
TODO: Fused conv and pool with padding is different from defused operators,0
assert variable.get_attr('_output_shapes')[0].dim[0].size == IR_node.attr['kernel_shape'].list.i[-1],0
strides,0
window_shape,0
pool type,0
padding,0
"In facenet or other newtwork using slim.batch_norm,",0
"There are two BN(train, test) skip switch and merge.",0
epsilon,0
moving variance (var) /read,0
gamma (scale),0
beta  (bias),0
moving mean (mean),0
ReduceMean,0
keep dims,0
axes,0
Skip the node as merge,0
weights,0
alpha,0
beta,0
Skip BiasAdd,0
weights,0
"input_node_perm = self.check_const(self.get_parent(source_node.name, [1], True))",0
paddings,0
for attr.shape >= 2,0
"For models built by slim.batch_norm, remove duplicate BN (eg.facenet)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Build network graph,0
"PaddleParser._set_output_shape(source_node, IR_node)",0
"name, op",0
input edge,0
For concat axis,0
only for training,0
"name, op",0
input edge,0
input edge,0
layer and spec,0
width <=> x or height <=> y,0
output shape,0
"name, op",0
it should be in the shape of height x width x inputchannel x outputchannel,0
use_bias: TODO,1
pad_dim,0
fail report because of auto_pad,0
if dilation_x == 1 and dilation_y == 1:,0
if output_x * stride_x == input_x and output_y * stride_y == input_y:,0
"auto_pad = ""SAME""",0
kwargs['auto_pad'] = auto_pad,0
elif output_x * stride_x == input_x - width + 1 and output_y * stride_y == input_y - height + 1:,0
"auto_pad = ""VALID""",0
kwargs['auto_pad'] = auto_pad,0
defuse the activation layer,0
"name, op",0
input edge,0
layer and spec,0
output shape,0
"channels_first, then axis = 1",0
epsilon,0
compute adjusted parameters,0
Reference: parameter transformation https://github.com/apple/coremltools/issues/153,0
convert type because of tensorflow,0
flatten,0
mean,0
var,0
defuse the activation layer,0
"name, op",0
input edge,0
layer and spec,0
assert False,0
output shape,0
pad_dim,0
padding mode,0
"If padding == ""SAME"": output_spatial_shape[i] = ceil(input_spatial_shape[i] / strides[i])",0
"If padding == ""VALID"": output_spatial_shape[i] = ceil((input_spatial_shape[i] - (spatial_filter_shape[i]-1) * dilation_rate[i]) / strides[i]).",0
"name, op",0
input edge,0
layer and spec,0
units,0
output shape,0
use_bias,0
Kit weight tranpose,0
weight: N x M -> C x H x W x M -> H x W x C x M -> N x M,0
,0
weights,0
need the shape TODO,1
only for training,0
"name, op",0
input edge,0
shape,0
"name, op",0
input edge,0
layer and spec,0
output shape,0
alpha,0
beta,0
nsize,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Build network graph,0
strides,0
window_shape,0
pool type,0
padding,0
TODO: Handle square,1
self._print_layer(source_node),0
print (source_node.name),0
"print (self.src_graph.get_parent(source_node.name, [0]).real_name)",0
assert False,0
Convolution,0
Kernel,0
Attributes,0
Bias,0
Activation,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
BlockFunction node,0
redirect the composite's inputs to the true inputs,0
"BlockFunctions are short-circuited, and not added to accum[]",0
Function node,0
OutputVariable node,0
def build(self):,0
"_traverse_graph(self, self.model.root_function)",0
"super(CntkGraph, self).build()",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"IR_node.get_attr('output_dim'),",0
??,0
print(IR_node.layer),0
assert False,0
"def emit_Unstack(self, IR_node):",0
"num_str = ""{}.shape[{}]"".format(self.parent_variable_name(IR_node), IR_node.get_attr('axis'))",0
axis = IR_node.get_attr('axis'),0
"parent_variable_shape = ""list({}.shape)"".format(self.parent_variable_name(IR_node)",0
"if self.IR_graph.get_parent(IR_node.name, [0]).type != 'Embedding'",0
else self.parent_variable_name(IR_node)+'.E'),0
if axis==1:,0
"shape_str = ""tuple([{}[0]*{}[{}], 1].extend({}[{}+1:]))"".format(",0
"parent_variable_shape,",0
"parent_variable_shape,",0
"str(axis),",0
"parent_variable_shape,",0
str(axis)),0
else:,0
"shape_str = ""tuple([{}[0]*{}[{}]].extend({}[1:{}]).append(1).extend({}[{}+1:]))"".format(",0
"parent_variable_shape,",0
"parent_variable_shape,",0
"str(axis),",0
"parent_variable_shape,",0
"str(axis),",0
"parent_variable_shape,",0
str(axis)),0
"code = ""{:<15} = cntk.reshape({}, {}, name='{}')"".format(",0
"IR_node.variable_name,",0
"self.parent_variable_name(IR_node),",0
"shape_str,",0
IR_node.variable_name),0
"code = ""{: <15} = cntk.reshape({}, {}.shape, name='{}')"".format(",0
"IR_node.variable_name,",0
"self.parent_variable_name(IR_node),",0
"self.parent_variable_name(IR_node),",0
IR_node.name,0
),0
return code,0
"def emit_Fill(self, IR_node):",0
"code = ""{:<15} = cntk.Constant({}, {}, name='{}')"".format(",0
"IR_node.variable_name,",0
"IR_node.get_attr('value'),",0
"self.parent_variable_name(IR_node),",0
IR_node.name),0
return code,0
param_code does not need parameter slice.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
For padding,0
print (input_shape),0
print (kernel_shape),0
print (strides),0
Not tested,0
print (input_shape),0
print (kernel_shape),0
print (strides),0
kernel_shape[idx] = (kernel_shape[idx] - 1) * dilation_rate + 1,0
print ([0] + up_list + [0] + down_list if data_format.startswith('NC') else up_list + [0] + down_list + [0]),0
print ('-----------------------------------------------------'),0
network library,0
not support multi-threads download,0
key: layer_name    value: keras layer,0
private functions,0
next_node_info.left_in_edges -= 1,0
"print ('{} --> {}'.format(src, dst))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
share functions,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
name --> (weight_name --> ndarray),0
Generated by the protocol buffer compiler.  DO NOT EDIT!,0
source: graph.proto,0
@@protoc_insertion_point(imports),0
@@protoc_insertion_point(class_scope:GraphDef),0
@@protoc_insertion_point(class_scope:NodeDef.AttrEntry),0
@@protoc_insertion_point(class_scope:NodeDef),0
@@protoc_insertion_point(class_scope:AttrValue.ListValue),0
@@protoc_insertion_point(class_scope:AttrValue),0
@@protoc_insertion_point(class_scope:TensorShape.Dim),0
@@protoc_insertion_point(class_scope:TensorShape),0
@@protoc_insertion_point(class_scope:LiteralTensor),0
@@protoc_insertion_point(module_scope),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
First try to read it as a binary file.,0
Next try to read it as a text file.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"If pattern.inputs is empty, skips the rest and accepts all the inputs.",0
"If order doesn't matter for the inputs, then make sure we match at least",0
one permutation of the inputs.,0
"def get_tensor(self, pattern_or_name):",0
op_tensor = self._get_op_tensor(pattern_or_name),0
return op_tensor[1] if op_tensor else None,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
"Python 3.3.2+ implements `yield from`, but for now:",0
"check the same pattern scope node whether have same inputs, outputs and weights.",0
"For those don't have, rename their scope names.",0
"clear out scope node, typically input constant node.",0
get sub_scopes,0
decline the suffix number,0
Obtain nodes where the scope name that satisfies top_level is top_scope and sub_level is sub_scope,0
Obtain nodes where the scope name that satisfies in sub_level is sub_scope,0
Obtain nodes where the scope name that satisfies in top_level is top_scope,0
Obtain all nodes grouped by sub_level sub_scope,0
cover the node,0
"store idx, node into a dict and sort it later to keep its topology sort.",0
in_node's out edges replace n_name with scope node name.,0
no out nodes means the last node in scope nodes should be returned,0
"if out_node is scope node, replace the scope node's inner topology list node.",0
the input parameter shoule be sliced when call func.,0
modify the in_edges in scope inner nodes. decline the :idx.,0
1. initilize scope node,0
2. get scope nodes' topology list.,0
3. rebuild the edges connection after folding these scope nodes into one node and,0
get this scope node's return variables.,0
4. rebuild graph.,0
RNN-related attrs.,0
get input params,0
self.store_const_to_top(result),0
"self.set_top_node_prop(result, pattern_name)",0
Do not include input op.,0
"TODO: pytorch, mxnet, keras, cntk",1
-*- coding: utf-8 -*-,0
from keras.layers.core import Layer,0
Arguments,0
Input shape,0
Output shape,0
Prepare broadcasting shape.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Load the model network,0
Load the model weights,0
load model files into Keras graph,0
Keras 2.1.6,0
Keras. 2.2.2,0
"_keras.utils.plot_model(model, ""model.png"", show_shapes = True)",0
Build network graph,0
TODO: Fused conv and pool with padding is different from defused operators,0
TODO: More activation functions,1
for ELU,0
input edge,0
"name, op",0
weights,0
pads,0
filter,0
"[kd, kh, kw, channel_size, filter number]",0
use_bias,0
strides,0
"[1, sd, sh, sw, 1]",0
dilations,0
"[1, dd, dh, dw, 1]",0
activation,0
"name, op",0
input edge,0
padding,0
strides,0
"[1, sd, sh, sw, 1]",0
window_shape,0
"[1, pd, ph, pw, 1]",0
"name, op",0
input edge,0
For concat axis,0
"name, op",0
input edge,0
padding,0
only for training,0
"name, op",0
input edge,0
"name, op",0
input edge,0
Merge Layers,0
only for training,0
"name, op",0
input edge,0
shape,0
only for training,0
"name, op",0
input edge,0
Core Layers,0
"name, op",0
input edge,0
units,0
use_bias,0
weights,0
activation,0
"name, op",0
input edge,0
"name, op",0
input edge,0
size,0
"name, op",0
input edge,0
input_dim,0
output_dim,0
mask_zero,0
weights,0
"name, op",0
input edge,0
units,0
use_bias,0
"for Keras, drop_out and recurrent_dropout",0
activation,0
"name, op",0
input edge,0
units,0
activation,0
weights,0
Kit TODO : need to search the tf,1
"name, op",0
input edge,0
for target shape,0
"name, op",0
input edge,0
axis,0
"Parameter arrangement in Keras: gamma, beta, mean, variance",0
scale,0
beta,0
mean,0
var,0
"name, op",0
input edge,0
for target shape,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
rot weights,0
"print(""Converting layer {}({})"".format(current_node.name, node_type))",0
TODO,1
"return input_node, 'same'",0
"assert IR_node.get_attr('group', 1) == 1",0
change dw from filters to 1,0
reset the default dilation,0
############,0
Operators #,0
############,0
"code = self._emit_merge(IR_node, ""subtract"")",0
TODO,1
"when converting from coreml model, reshape is needed after the global pooling",0
for Keras,0
It arouses some problems:,0
it can be implemented by Lambda Layer,0
https://github.com/keras-team/keras/issues/890,0
Keras == 2.1.6,0
Keras == 2.2.2,0
TODO: arguments won't be saved in keras export model,1
param_code does not need parameter slice.,0
Prepare broadcasting shape.,0
"output = Lambda(lambda x: tf.fill(x, value))(input)",0
return output,0
def _layer_Constant(self):,0
"self.add_body(0, '''",0
class my_constant(keras.layers.Layer):,0
"def __init__(self, value, **kwargs):",0
"super(my_constant, self).__init__(**kwargs)",0
self._value = value,0
"# the input is dummy, just for creating keras graph.",0
"def call(self, dummy):",0
res = K.constant(self._value),0
self.output_shapes = K.int_shape(res),0
return res,0
"def compute_output_shape(self, input_shape):",0
return self.output_shapes,0
'''),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
Kit: TODO,1
Duplicate models for weight sharing,0
Expand the sub-models,0
"with open('modelmapbydataset.json', 'w') as outfile:",0
"json.dump(new_data, outfile)",0
generate makedown script,0
add Image Classification,0
add Object Detection,0
MMdnn introduction,0
steps for model conversion,0
Generate model converter description,0
Generate models list,0
generate makedown script,0
MMdnn introduction,0
steps for model conversion,0
Generate model converter description,0
Generate models list,0
console.info(JSON.parse(e.target.result)),0
"default, draw the model.json file",0
window.onload = () => {,0
"d3.json(filePath, (error, json) => {",0
if (error) throw error;,0
// console.info(json),0
draw(json),0
}),0
},0
generate dag,0
IR model or keras model,0
path generator,0
let arrowL = `M ${points[len - 1].x} ${points[len - 1].y} l -7 -7`,0
let arrowR = `M ${points[len - 1].x} ${points[len - 1].y} l  7 -7`,0
select a layer,0
remove list,0
draw,0
mode;,0
"let scale = Math.min(window.innerWidth*0.85 / width, (window.innerHeight-85)/height)",0
"var filter = def.append(""filter"")",0
".attr(""id"", ""dropshadow"")",0
"filter.append(""feGaussianBlur"")",0
".attr(""in"", ""SourceAlpha"")",0
".attr(""stdDeviation"", 4)",0
".attr(""result"", ""blur"");",0
"// filter.append(""feOffset"")",0
"//     .attr(""in"", ""blur"")",0
"//     .attr(""dx"", 2)",0
"//     .attr(""dy"", 2)",0
"//     .attr(""result"", ""offsetBlur"");",0
"var feMerge = filter.append(""feMerge"");",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""offsetBlur"")",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""SourceGraphic"");",0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
"svg.on('keydown', ()=>{console.info('ddd')})",0
".on('mouseover', ()=>{console.info('mouse over')})",0
k = parseFloat(k)*parseFloat(k_),0
"limit x, y",0
// a trick to make text svg transform in MS Edge,0
"d3.selectAll("".labels"").classed(""tempclass"", true);",0
"setTimeout(function () { d3.selectAll("".labels"").classed(""tempclass"", false); }, 40);",0
when shift is down,0
ulti function,0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
rm the '_',0
Base64 encode: https://developers.google.com/protocol-buffers/docs/proto3,0
Search the node,0
select by id: https://stackoverflow.com/questions/37270787/uncaught-syntaxerror-failed-to-execute-queryselector-on-document,0
Scroll,0
console.info(JSON.parse(e.target.result)),0
"default, draw the model.json file",0
window.onload = () => {,0
"d3.json(filePath, (error, json) => {",0
if (error) throw error;,0
// console.info(json),0
draw(json),0
}),0
},0
generate dag,0
IR model or keras model,0
path generator,0
let arrowL = `M ${points[len - 1].x} ${points[len - 1].y} l -7 -7`,0
let arrowR = `M ${points[len - 1].x} ${points[len - 1].y} l  7 -7`,0
select a layer,0
draw,0
"let scale = Math.min(window.innerWidth*0.85 / width, (window.innerHeight-85)/height)",0
"var filter = def.append(""filter"")",0
".attr(""id"", ""dropshadow"")",0
"filter.append(""feGaussianBlur"")",0
".attr(""in"", ""SourceAlpha"")",0
".attr(""stdDeviation"", 4)",0
".attr(""result"", ""blur"");",0
"// filter.append(""feOffset"")",0
"//     .attr(""in"", ""blur"")",0
"//     .attr(""dx"", 2)",0
"//     .attr(""dy"", 2)",0
"//     .attr(""result"", ""offsetBlur"");",0
"var feMerge = filter.append(""feMerge"");",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""offsetBlur"")",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""SourceGraphic"");",0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
"svg.on('keydown', ()=>{console.info('ddd')})",0
".on('mouseover', ()=>{console.info('mouse over')})",0
k = parseFloat(k)*parseFloat(k_),0
"limit x, y",0
// a trick to make text svg transform in MS Edge,0
"d3.selectAll("".labels"").classed(""tempclass"", true);",0
"setTimeout(function () { d3.selectAll("".labels"").classed(""tempclass"", false); }, 40);",0
when shift is down,0
ulti function,0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
space = '',0
Get the long description from the README file,0
Versions should comply with PEP440.  For a discussion on single-sourcing,0
"the version across setup.py and the project code, see",0
https://packaging.python.org/en/latest/single_source_version.html,0
The project's main homepage.,0
Author details,0
Choose your license,0
See https://pypi.python.org/pypi?%3Aaction=list_classifiers,0
How mature is this project? Common values are,0
3 - Alpha,0
4 - Beta,0
5 - Production/Stable,0
Indicate who your project is intended for,0
"Pick your license as you wish (should match ""license"" above)",0
"Specify the Python versions you support here. In particular, ensure",0
"that you indicate whether you support Python 2, Python 3 or both.",0
What does your project relate to?,0
You can just specify the packages manually here if your project is,0
simple. Or you can use find_packages().,0
"Alternatively, if you want to distribute just a my_module.py, uncomment",0
this:,0
"py_modules=[""my_module""],",0
List run-time dependencies here.  These will be installed by pip when,0
"your project is installed. For an analysis of ""install_requires"" vs pip's",0
requirements files see:,0
https://packaging.python.org/en/latest/requirements.html,0
"To provide executable scripts, use entry points in preference to the",0
"""scripts"" keyword. Entry points provide cross-platform support and allow",0
pip to create the appropriate form of executable for the target platform.,0
Communication to TensorFlow server via gRPC,0
TensorFlow serving stuff to send messages,0
Command line arguments,0
Send request,0
request.inputs['input'].CopyFrom(),0
"'xception'     : [TestModels.onnx_emit],",0
"'nasnet'       : [TestModels.onnx_emit],",0
omit tensorflow lead to crash,0
"'nasnet-a_large'       : [TestModels.mxnet_emit, TestModels.pytorch_emit, TestModels.tensorflow_emit],",0
"'inception_resnet_v2'  : [TestModels.caffe_emit, TestModels.keras_emit, TestModels.mxnet_emit, TestModels.pytorch_emit, TestModels.tensorflow_emit],",0
"Function self.assertEquals has deprecated, change to assertEqual",0
"self.assertEqual(original_predict.shape, converted_predict.shape)",0
get original model prediction result,0
original to IR,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
print(original_predict),0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
get shape,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
IR to code,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
The list is in RGB oder,0
IR to Model,0
"converted_file = original_framework + '_coreml_' + architecture_name + ""_converted""",0
"converted_file = converted_file.replace('.', '_')",0
load model,0
save model,0
"coremltools.utils.save_spec(model.get_spec(), converted_file)",0
inference,0
IR to code,0
import converted model,0
"In case of odd number add the extra padding at the end for SAME_UPPER(eg. pads:[0, 2, 2, 0, 0, 3, 3, 0]) and at the beginning for SAME_LOWER(eg. pads:[0, 3, 3, 0, 0, 2, 2, 0])",0
"'xception'     : [onnx_emit],",0
"'nasnet'       : [onnx_emit],",0
"'voc-fcn8s'     : [onnx_emit], # TODO: ConvTranspose, Crop",0
"'voc-fcn16s'    : [onnx_emit], # TODO: ConvTranspose, Crop",0
"'voc-fcn32s'    : [onnx_emit], # TODO: ConvTranspose, Crop",0
"'resnet_v1_50'          : [onnx_emit], # POOL: strides > window_shape not supported due to inconsistency between CPU and GPU implementations",0
"'resnet_v1_152'         : [onnx_emit], # POOL: strides > window_shape not supported due to inconsistency between CPU and GPU implementations",0
"'resnet_v2_50'          : [onnx_emit], # POOL: strides > window_shape not supported due to inconsistency between CPU and GPU implementations",0
"'resnet_v2_152'         : [onnx_emit], # POOL: strides > window_shape not supported due to inconsistency between CPU and GPU implementations",0
"'nasnet-a_large'        : [onnx_emit], # POOL: strides > window_shape not supported due to inconsistency between CPU and GPU implementations",0
TODO: coredump,1
"'alexnet'       : [cntk_emit, keras_emit, tensorflow_emit],",0
"'nasnet'       : [tensorflow_emit, keras_emit, coreml_emit],",0
"'facenet'      : [tensorflow_emit, coreml_emit,mxnet_emit,keras_emit]  # TODO",0
"'vgg16': [tensorflow_emit],",0
'alexnet': [tensorflow_emit],0
get original model prediction result,0
def test_caffe(self):,0
try:,0
import caffe,0
"self._test_function('caffe', self.caffe_parse)",0
except ImportError:,0
"print('Please install caffe! Or caffe is not supported in your platform.', file=sys.stderr)",0
def test_cntk(self):,0
try:,0
import cntk,0
"self._test_function('cntk', self.cntk_parse)",0
except ImportError:,0
"print('Please install cntk! Or cntk is not supported in your platform.', file=sys.stderr)",0
def test_coreml(self):,0
from coremltools.models.utils import macos_version,0
"if macos_version() < (10, 13):",0
"print('Coreml is not supported in your platform.', file=sys.stderr)",0
else:,0
"self._test_function('coreml', self.coreml_parse)",0
def test_keras(self):,0
"self._test_function('keras', self.keras_parse)",0
def test_mxnet(self):,0
"self._test_function('mxnet', self.mxnet_parse)",0
def test_darknet(self):,0
"self._test_function('darknet', self.darknet_parse)",0
def test_paddle(self):,0
# omit tensorflow lead to crash,0
import tensorflow as tf,0
try:,0
import paddle.v2 as paddle,0
"self._test_function('paddle', self.paddle_parse)",0
except ImportError:,0
"print('Please install Paddlepaddle! Or Paddlepaddle is not supported in your platform.', file=sys.stderr)",0
def test_pytorch(self):,0
"self._test_function('pytorch', self.pytorch_parse)",0
def test_tensorflow(self):,0
"self._test_function('tensorflow', self.tensorflow_parse)",0
def test_tensorflow_frozen(self):,0
"self._test_function('tensorflow_frozen', self.tensorflow_frozen_parse)",0
"'vgg19'         : [TestModels.onnx_emit],",0
"'voc-fcn8s'    : [TestModels.cntk_emit, TestModels.coreml_emit, TestModels.tensorflow_emit],",0
"'vgg19'         : [TestModels.caffe_emit, TestModels.cntk_emit, TestModels.coreml_emit, TestModels.keras_emit, TestModels.mxnet_emit, TestModels.pytorch_emit, TestModels.tensorflow_emit],",0
"'voc-fcn8s'     : [TestModels.cntk_emit, TestModels.coreml_emit, TestModels.tensorflow_emit],",0
"'voc-fcn16s'    : [TestModels.cntk_emit, TestModels.coreml_emit, TestModels.tensorflow_emit],",0
"'voc-fcn32s'    : [TestModels.cntk_emit, TestModels.coreml_emit, TestModels.tensorflow_emit],",0
"'nasnet'       : [TestModels.onnx_emit],",0
"'vgg19'         : [TestModels.onnx_emit],",0
"'alexnet'       : [TestModels.caffe_emit, TestModels.cntk_emit, TestModels.coreml_emit, TestModels.mxnet_emit, TestModels.pytorch_emit, TestModels.tensorflow_emit],",0
"'nasnet-a_large'       : [TestModels.mxnet_emit, TestModels.pytorch_emit, TestModels.tensorflow_emit],",0
"'inception_resnet_v2'  : [TestModels.caffe_emit, TestModels.keras_emit, TestModels.mxnet_emit, TestModels.pytorch_emit, TestModels.tensorflow_emit],",0
"'xception'     : [TestModels.onnx_emit],",0
"'nasnet'       : [TestModels.onnx_emit],",0
!/usr/bin/python,0
major python major_python_versions as python2 and python3,0
operating system,0
do not deal,0
I don't want to deal with auto_pad,0
Don't support auto_pad current!,0
2018-02-28,0
if group is None:,0
group = 1,0
group is not support yet too.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
node,0
input,0
edge(node a <-> node b),0
,0
"key is edge name, value is src/dst node name",0
"key is initializer name, value is TensorProto",0
print(name),0
print(layer.op_type),0
n is input data,0
n is input edge,0
omit node of some type,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"if node len(in_edges), generate additional DataInput node",0
print,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
TODO,1
"'max_pool2d': convert_maxpool,",0
"'onnx::Mul': convert_elementwise_mul,",0
"'onnx::Sub': convert_elementwise_sub,",0
"'onnx::ConvTranspose': convert_convtranspose,",0
"'onnx::LeakyRelu': convert_lrelu,",0
"'onnx::Sigmoid': convert_sigmoid,",0
"'onnx::Softmax': convert_softmax,",0
"'onnx::Selu': convert_selu,",0
"'onnx::Transpose': convert_transpose,",0
"'onnx::Reshape': convert_reshape,",0
"'onnx::MatMul': convert_matmul,",0
"'onnx::Gather': convert_gather,",0
"'onnx::ReduceSum': convert_reduce_sum,",0
"'onnx::Constant': convert_constant,",0
"'onnx::Upsample': convert_upsample,",0
"'onnx::Pad': convert_padding,",0
###########,0
property #,0
###########,0
###################,0
Public Functions #,0
###################,0
test,0
cpu: https://github.com/pytorch/pytorch/issues/5286,0
Build network graph,0
"(batch, C, H, W)  & NHWC",0
#########,0
Layers #,0
#########,0
dilation,0
handle bias,0
TODO,1
output_shape,0
epsilon,0
mean,0
var,0
Kit weight tranpose,0
weight: N x M -> C x H x W x M -> H x W x C x M -> N x M,0
,0
weights,0
use_bias,0
units,0
handle weight,0
handle bias,0
###################,0
Helper Functions #,0
###################,0
print (dir(data)),0
print (i),0
"print (""    {} with shape {}"".format(j, load_weight[i][j].shape))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Scopes created in a nested scope may have initial characters,0
that are illegal as the initial character of an op name,0
"(viz. '-', '\', '/', and '_').",0
sanity check.,0
run dce first to eliminate dead parts of the graph that might have been,0
left behind by things like symbolic_override,0
construct graph,0
nodes,0
input layer,0
TODO,1
build each layer,0
input,0
"print(node_input_name ,'->', node_name)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Base Functions,0
"padding,",0
"exstr = "", value=float('-Inf')""",0
"exstr = """"",0
Change to padding defuse,0
"input_node = self._defuse_padding(IR_node, exstr)",0
Ignore it in Pytorch,0
for Keras,0
"self.add_init(2, ""self.{:<15} = torch.from_numpy(__weights_dict['{}']['value'])"".format(",0
"IR_node.variable_name,",0
IR_node.name)),0
param_code does not need parameter slice.,0
from torch.nn.parameter import Parameter,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self._type = layer.__class__.__name__.replace('Backward', '')",0
"self._name = ""{}_{}"".format(self.type, id)",0
TODO,1
"""""""",0
build graph for pytorch 0.2.0,0
"""""""",0
dummy_input = torch.autograd.Variable(torch.randn(shape)),0
output_node = self.model(dummy_input),0
search_queue = [output_node.grad_fn],0
"tmp_node = PyTorchGraphNode(output_node.grad_fn, 0)",0
self.layer_map[tmp_node.name] = tmp_node,0
visited = {output_node.grad_fn : self.layer_map[tmp_node.name]},0
idx = 0,0
node_count = 1,0
while (idx < len(search_queue)):,0
current_node = search_queue[idx],0
current_type = visited[current_node].type,0
"if hasattr(current_node, 'next_functions'):",0
"for parent, _ in current_node.next_functions:",0
"parent_type = parent.__class__.__name__.replace('Backward', '')",0
if parent_type != 'AccumulateGrad' and \,0
(parent_type != 'Transpose' or current_type != 'Addmm'):,0
if not parent in visited:,0
"tmp_node = PyTorchGraphNode(parent, node_count)",0
self.layer_map[tmp_node.name] = tmp_node,0
node_count += 1,0
visited[parent] = tmp_node,0
search_queue.append(parent),0
"self._make_connection(visited[parent].name, visited[current_node].name)",0
idx += 1,0
try:,0
return TorchGraph._forward_torch_random_input(,0
"torch_model,",0
"input_shapes,",0
is_batch=False,0
),0
except:,0
# try batch mode,0
# return TorchGraph._forward_torch_random_input(,0
"#     torch_model,",0
"#     input_shapes,",0
#     is_batch=True,0
# ),0
pass,0
tensor = torch.rand(*shape).float(),0
multi output,0
single output,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
###########,0
property #,0
###########,0
###################,0
Public Functions #,0
###################,0
Build network graph,0
#########,0
Layers #,0
#########,0
handle weight,0
handle bias,0
handle weight,0
handle bias,0
###################,0
Helper Functions #,0
###################,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
load model file into Coreml Graph,0
model.encode() convert to str --- python2 may crash due to type 'unicode',0
Build Network Graph,0
self.data_format ? TODO,1
load the model network,0
convert to Model_pb2.Model,0
determine the type of the current_node,0
staticmethods,0
"(seq, batch, C, H, W)  & NHWC",0
TODO dtype_map,1
##### convert methods,0
convolution,0
input edge,0
important!,0
reshape the weight!,0
"[2, 3, 0, 1]",0
pads,0
use_bias,0
isDeconvolution,0
"name, op",0
kwargs['kernel_shape'] = weights.shape,0
strides,0
"[1, sd, sh, sw, 1]",0
activation,0
TODO,1
padding in conv,0
"pad in IR is [x1_b, x2_b, ..., x1_e, x2_e, ...]",0
padding,0
compute padding for 'same',0
https://discuss.mxnet.io/t/pooling-and-convolution-with-same-mode/528/3,0
https://www.jianshu.com/p/05c4f1621c7e,0
padding in pooling,0
padding,0
https://discuss.mxnet.io/t/pooling-and-convolution-with-same-mode/528/3,0
TODO,1
symmetric padding,0
"name, op",0
input edge,0
For concat axis,0
"NO axis in coreml, so set the last axis",0
"The first -1 means in coreml there is one-more axis,",0
The second -1 means the last axis,0
"name, op",0
input edge,0
padding,0
Future Module TODO,1
#### rename methods,0
"name, op",0
activation type,0
else:,0
assert False,0
input edge,0
Merge layers,0
only for training,0
"name, op",0
input edge,0
shape,0
NHWC channel last,0
"in fact, here is NWHC",0
"name, op",0
input edge,0
axis TODO,0
"channels_first, then axis = 1",0
scale,0
bias,0
epsilon,0
compute adjusted parameters,0
Reference: parameter transformation https://github.com/apple/coremltools/issues/153,0
convert type because of tensorflow,0
mean,0
var,0
"name, op",0
input edge,0
bias,0
"name, op",0
input edge,0
"MAX = 0, AVERAGE = 1, L2 = 2",0
padding,0
strides,0
"[1, sd, sh, sw, 1]",0
window_shape,0
"[1, pd, ph, pw, 1]",0
"name, op",0
input edge,0
"name, op",0
input edge,0
"name, op",0
input edge,0
units,0
use_bias,0
weights,0
change to single because of the tf matmul,0
"name, op",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
the end of the tensorflow_resnet_v2_50's squeeze shape is [unknown_rank: true] with len 0,0
1001 means the 1001 classes for tensorflow_resnet_v2_50,0
!Alert! TODO,1
Future implement can be changed to the last two layer,0
TODO - remove style transfer 1D hack,1
"Input is 1D but it goes to the width dimension: (1,1,W)",0
"assume (Batch, Channels) - Batch dimension should be dropped",0
"assume (Batch, Sequence-Length, channels)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Add classifier classes (if applicable),0
Set pre-processing paramsters,0
"image_input_names,",0
Return the protobuf spec,0
model = _MLModel(self.builder.spec),0
if IR_node.name in self.weights_dict and 'weights' in self.weights_dict[IR_node.name]:,0
pass,0
"self._emit_merge(IR_node,'DOT')",0
Get input and output names,0
Dimensions and weights,0
"W should have shape (height, width, kernel_channels, output_channels), where kernel_channel = input_channels / groups",0
Dilations,0
see protobuf,0
assert False,0
Get the weights.,0
Dilations,0
see protobuf,0
depth-wise convolution,0
Dilations,0
see protobuf,0
Get input and output names,0
Pooling layer type,0
"if it's global, set the global flag",0
TODO  global pooling modification,0
Padding,0
see protobuf,0
Padding,0
see protobuf,0
Get input and output names,0
"type: ""list(int). A 1-D values of (leftBorder, topBorder, rightBorder, bottomBorder).""",0
This is central crop,0
Now add the layer,0
"Allowed values: 'CHW', 'HW', 'C', 'H', 'W'",0
Get input and output names,0
Get input and output names,0
Get the weights from keras,0
Get input and output names,0
blob_order == 0 if the input blob needs not be rearranged,0
blob_order == 1 if the input blob needs to be rearranged,0
"using keras_layer.input.shape have a ""?"" (Dimension[None] at the front),",0
making a 3D tensor with unknown batch size 4D,0
Get input and output names,0
activation emit,0
Get input and output names,0
negate it,0
apply threshold,0
negate it back,0
for Keras,0
Get input and output names,0
Set parameters,0
"Parameter arrangement in Keras: gamma, beta, mean, variance",0
compute adjusted parameters,0
Reference: parameter transformation https://github.com/apple/coremltools/issues/153,0
padding type TODO,0
"Type of the padding. Can be one of 'constant', 'reflection' or 'replication",0
Now add the layer,0
self.emit_Flatten(IR_node),0
"depth_radius: Half-width of the 1-D normalization window.""",0
Get the weights,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
input layer,0
build each layer,0
"if A.output == B.input, then make the connection: A -> B",0
"print('{0:20}->     {1:20}'.format(layerA.name, layerB.name))",0
"if A.name == B.input, then make the connection: A -> B, here A is the input",0
build each layer,0
"if A.output == B.input, then make the connection: A -> B",0
"print('{0:20}->     {1:20}'.format(layerA.name, layerB.name))",0
"if A.name == B.input, then make the connection: A -> B, here A is the input",0
build each layer,0
"if A.output == B.input, then make the connection: A -> B",0
"print('{0:20}->     {1:20}'.format(layerA.name, layerB.name))",0
"if A.name == B.input, then make the connection: A -> B, here A is the input",0
The information of the layer,0
Generated by the protocol buffer compiler.  DO NOT EDIT!,0
source: caffe.proto,0
@@protoc_insertion_point(imports),0
@@protoc_insertion_point(class_scope:caffe.BlobShape),0
@@protoc_insertion_point(class_scope:caffe.BlobProto),0
@@protoc_insertion_point(class_scope:caffe.BlobProtoVector),0
@@protoc_insertion_point(class_scope:caffe.Datum),0
@@protoc_insertion_point(class_scope:caffe.FillerParameter),0
@@protoc_insertion_point(class_scope:caffe.NetParameter),0
@@protoc_insertion_point(class_scope:caffe.SolverParameter),0
@@protoc_insertion_point(class_scope:caffe.SolverState),0
@@protoc_insertion_point(class_scope:caffe.NetState),0
@@protoc_insertion_point(class_scope:caffe.NetStateRule),0
@@protoc_insertion_point(class_scope:caffe.ParamSpec),0
@@protoc_insertion_point(class_scope:caffe.LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.TransformationParameter),0
@@protoc_insertion_point(class_scope:caffe.LossParameter),0
@@protoc_insertion_point(class_scope:caffe.AccuracyParameter),0
@@protoc_insertion_point(class_scope:caffe.ArgMaxParameter),0
@@protoc_insertion_point(class_scope:caffe.ConcatParameter),0
@@protoc_insertion_point(class_scope:caffe.BatchNormParameter),0
@@protoc_insertion_point(class_scope:caffe.BiasParameter),0
@@protoc_insertion_point(class_scope:caffe.ContrastiveLossParameter),0
@@protoc_insertion_point(class_scope:caffe.ConvolutionParameter),0
@@protoc_insertion_point(class_scope:caffe.CropParameter),0
@@protoc_insertion_point(class_scope:caffe.DataParameter),0
@@protoc_insertion_point(class_scope:caffe.DropoutParameter),0
@@protoc_insertion_point(class_scope:caffe.DummyDataParameter),0
@@protoc_insertion_point(class_scope:caffe.EltwiseParameter),0
@@protoc_insertion_point(class_scope:caffe.ELUParameter),0
@@protoc_insertion_point(class_scope:caffe.EmbedParameter),0
@@protoc_insertion_point(class_scope:caffe.ExpParameter),0
@@protoc_insertion_point(class_scope:caffe.FlattenParameter),0
@@protoc_insertion_point(class_scope:caffe.HDF5DataParameter),0
@@protoc_insertion_point(class_scope:caffe.HDF5OutputParameter),0
@@protoc_insertion_point(class_scope:caffe.HingeLossParameter),0
@@protoc_insertion_point(class_scope:caffe.ImageDataParameter),0
@@protoc_insertion_point(class_scope:caffe.InfogainLossParameter),0
@@protoc_insertion_point(class_scope:caffe.InnerProductParameter),0
@@protoc_insertion_point(class_scope:caffe.InputParameter),0
@@protoc_insertion_point(class_scope:caffe.LogParameter),0
@@protoc_insertion_point(class_scope:caffe.LRNParameter),0
@@protoc_insertion_point(class_scope:caffe.MemoryDataParameter),0
@@protoc_insertion_point(class_scope:caffe.MVNParameter),0
@@protoc_insertion_point(class_scope:caffe.ParameterParameter),0
@@protoc_insertion_point(class_scope:caffe.PoolingParameter),0
@@protoc_insertion_point(class_scope:caffe.PowerParameter),0
@@protoc_insertion_point(class_scope:caffe.PythonParameter),0
@@protoc_insertion_point(class_scope:caffe.RecurrentParameter),0
@@protoc_insertion_point(class_scope:caffe.ReductionParameter),0
@@protoc_insertion_point(class_scope:caffe.ReLUParameter),0
@@protoc_insertion_point(class_scope:caffe.ReshapeParameter),0
@@protoc_insertion_point(class_scope:caffe.ScaleParameter),0
@@protoc_insertion_point(class_scope:caffe.SigmoidParameter),0
@@protoc_insertion_point(class_scope:caffe.SliceParameter),0
@@protoc_insertion_point(class_scope:caffe.SoftmaxParameter),0
@@protoc_insertion_point(class_scope:caffe.TanHParameter),0
@@protoc_insertion_point(class_scope:caffe.TileParameter),0
@@protoc_insertion_point(class_scope:caffe.ThresholdParameter),0
@@protoc_insertion_point(class_scope:caffe.WindowDataParameter),0
@@protoc_insertion_point(class_scope:caffe.SPPParameter),0
@@protoc_insertion_point(class_scope:caffe.V1LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.V0LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.PReLUParameter),0
@@protoc_insertion_point(module_scope),0
"TODO: We need to identify whether this is 4D image data, otherwise we shouldn't change the dimension order",1
print(node.parameters),0
assert False,0
"Stochastic pooling, for instance.",0
TODO: Axis,0
TODO: Unbiased,1
check if need the Flatten layer,0
if parent.output_shape.height > 1 or parent.output_shape.width > 1:,0
TODO: The gamma parameter has to be set (in node.data?) and this should work.,1
"Also, mean should be set to 0, and var to 1, just to be safe.",0
skip the split node,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"For some reason argparser gives us unicode, so we need to conver to str first",0
for test,0
"with open(""graph.txt"", 'w') as f:",0
for layer in self.IR_graph.topological_sort:,0
current_node = self.IR_graph.get_node(layer),0
"print(""========current_node=========\n{}"".format(current_node.layer), file=f)",0
test end,0
"print(""========current_node={}"".format(current_node.layer))",0
check if have pad layer,0
Pad_h < kernel_h (vgg19 caffe2caffe),0
implement asymmetric paddings by applying symmetric padding then cropping,0
keys = [],0
for key in self.weights_dict[IR_node.name].keys():,0
keys.append(key),0
"print(""=======Layer: {}, keys: {}"".format(IR_node.name, keys))",0
"if asymmetric padding, set offset to 1",0
Change the layer name,0
check if need crop output shape,0
"change the key ""name"" to ""variable_name"", in case of the layer name has invalid characters",0
"Convert NHWC (IR) to NCHW (Caffe): [0,1,2,3]->[0,3,1,2]",0
"def emit_Square(self, IR_node):",0
"input_layers = ', '.join(('n.' + self.IR_graph.get_node(edge).real_variable_name) for edge in IR_node.in_edges)",0
"self.add_body(1, ""n.{:<15} = L.Square({}, ntop=1)"".format(",0
"IR_node.variable_name,",0
input_layers)),0
Caffe Types,0
Same value applies to all spatial dimensions,0
Extract the value for the given spatial dimension,0
consider rewrite this function to Network.py,0
Dropout layers appear in a fair number of Caffe,0
test-time networks. These are just ignored. We'll,0
filter them out here.,0
TODO: raise error,1
TODO: raise error,1
Automatically set a name if not provided.,0
Figure out the layer inputs.,0
"print('op: %s   shape: %s' % (op, layer_output._keras_shape))",0
"print('op: %s   shape: %s' % (op, layer_output.get_shape().as_list()))",0
Add to layer LUT.,0
Old-style input specification,0
New-style input specification,0
"We most likely have a data layer on our hands. The problem is,",0
Caffe infers the dimensions of the data from the source (eg: LMDB).,0
We want to avoid reading datasets here. Fail for now.,0
This can be temporarily fixed by transforming the data layer to,0
"Caffe's ""input"" layer (as is usually used in the ""deploy"" version).",0
TODO: Find a better solution for this.,1
The .prototxt file defining the graph,0
The .caffemodel file containing the learned parameters,0
Set to true if the fallback protocol-buffer based backend was used,0
"A list containing (layer name, parameters) tuples",0
Load the parameters,0
"When using the protobuf-backend, each parameter initially has four dimensions.",0
"In certain cases (like FC layers), we want to eliminate the singleton dimensions.",0
"This implementation takes care of the common cases. However, it does leave the",0
potential for future issues.,0
The Caffe-backend does not suffer from this problem.,0
A dictionary mapping NodeKind to the transposed order.,0
The node kinds eligible for reshaping,0
"If true, the reshaped data will replace the old one.",0
"Otherwise, it's set to the reshaped_data attribute.",0
Check for 2+ dimensional data,0
The FC layer connected to the spatial layer needs to be,0
re-wired to match the new spatial ordering.,0
node.reshaped_data = weights.transpose(transpose_order),0
Set the weights,0
We're only fusing nodes with single parents,0
We can only fuse a node if its parent's,0
value isn't used by any other node.,0
Rewrite the fused node's children to its parent.,0
Disconnect the fused node from the graph.,0
Let the sub-class merge the fused node in any arbitrary way.,0
Fuse ReLUs when the parent node is one of the given types.,0
"If None, all node types are eligible.",0
Prescale the stats,0
Replace with the updated values,0
Include the scale and bias terms,0
self.graph = NodeRenamer()(graph),0
"return Graph(name, [self.map_node(node) for node in self.graph.nodes])",0
assert mapped_node is not None,0
skip when mapped_node is None,0
FIXME:,0
output = node.output,0
Decompose DAG into chains,0
Generate Python code line by line,0
try:,0
except ImportError:,0
# # Fall back to the protobuf implementation,0
# from mmdnn.conversion.caffe import caffe_pb2,0
# self.caffepb = caffe_pb2,0
# show_fallback_warning(),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
from tensorflow.core.framework.node_def_pb2 import NodeDef,0
from tensorflow.core.framework import attr_value_pb2,0
pass,0
"print(""\n"")",0
print(i),0
print(block),0
continue,0
NHWC,0
assert False,0
print(shortcut_layer['input'] ),0
spacetodepth,0
print(block),0
print(region_layer),0
assert False,0
print line,0
set default value,0
"yolo3(608) start at 1, yolo2(608) start at 0. yolo2(416) start at 1, yolo3(416) start at 0",0
load weight by original order,0
print(node_type),0
print(IR_node),0
assert False,0
strides,0
assert False,0
padding,0
only load weight conv,0
"print(""----------------"",self.start)",0
print(kernel.shape),0
print(k_bias.shape),0
"buf, start, scale_layer['name'], bn_layer['name'], conv_layer['name']",0
"print(""=============="",self.start)",0
print(bias.shape),0
print(scale.shape),0
print(mean.shape),0
print(var.shape),0
print(kernel.shape),0
print(W),0
assert False,0
no use,0
print(source_node.layer),0
for image classification(resnet) AVG pooling,0
print(source_node.layer),0
print(source_node.layer),0
kwargs['ignore_thresh'] = source_node.get_attr('ignore_thresh'),0
print(source_node.get_attr('anchors')),0
"kwargs['anchors'] = ['0.52','0.22']",0
kwargs['mask'] = source_node.get_attr('mask'),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
tensorflow dump tag,0
Reference: 1) Keras image preprocess: https://github.com/keras-team/keras/blob/master/keras/applications/imagenet_utils.py,0
2) tensorflow github issue: https://github.com/tensorflow/models/issues/517,0
"R-G-B for Imagenet === [123.68, 116.78, 103.94]",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.input, self.model, self.testop = self.MainModel.KitModel(self.args.w)",0
testop = tf.get_default_graph().get_operation_by_name(layer_name),0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result(None, False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
layer_name = 'block2_pool',0
"intermediate_layer_model = keras.Model(inputs=model.input,",0
outputs=model.get_layer(layer_name).output),0
intermediate_output = intermediate_layer_model.predict(img),0
print (intermediate_output),0
print (intermediate_output.shape),0
"print (""%.30f"" % np.sum(intermediate_output))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Load a real image and do default tf imageNet preprocessing,0
"[Batch, Height, Width, Channels] --> [Batch, Channels, Height, Width]",0
We only deal with non-recurrent networks for now,0
"(H,W,C) --> (C,H,W)",0
Load TensorFlow model,0
Prepare inputs,0
Run TF session,0
Evaluate coreml model,0
evaluate the TF model,0
evaluate CoreML,0
Test the default CoreML evaluation,0
Download model,0
Convert to coreml,0
Test predictions on an image,0
Download model,0
Convert to coreml,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
"@unittest.skip(""Failing GPU backend: related to https://github.com/tf-coreml/tf-coreml/issues/26"")",0
? style transfer image size and style number?,0
Test predictions on an image,0
evaluate the TF model,0
evaluate CoreML,0
Test the default CoreML evaluation,0
"compare_tf_coreml_outputs(tf_out_flatten, coreml_out_flatten)",0
#Download model,0
url = 'https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz',0
tf_model_dir = _download_file(url = url),0
"tf_model_path = os.path.join(TMP_MODEL_DIR, 'inception_v3_2016_08_28_frozen.pb')",0
#Convert to coreml,0
"mlmodel_path = os.path.join(TMP_MODEL_DIR, 'inception_v3_2016_08_28.mlmodel')",0
mlmodel = tf_converter.convert(,0
"tf_model_path = tf_model_path,",0
"mlmodel_path = mlmodel_path,",0
"output_feature_names = ['InceptionV3/Predictions/Softmax:0'],",0
"input_name_shape_dict = {'input:0':[1,299,299,3]},",0
"image_input_names = ['input:0'],",0
"red_bias = -1,",0
"green_bias = -1,",0
"blue_bias = -1,",0
image_scale = 2.0/255.0),0
#Test predictions on an image,0
_test_coreml_model_image_input(,0
"tf_model_path = tf_model_path,",0
"coreml_model = mlmodel,",0
"input_tensor_name = 'input:0',",0
"output_tensor_name = 'InceptionV3/Predictions/Softmax:0',",0
img_size = 299),0
Download model,0
Convert to coreml,0
Test predictions on an image,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
self.test_truth(),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
from collections import namedtuple,0
"Batch = namedtuple('Batch', ['data'])",0
TODO,1
Apple has published some of their own models. They can be downloaded from https://developer.apple.com/machine-learning/.,0
"Those published models are: SqueezeNet, Places205-GoogLeNet, ResNet50, Inception v3, VGG16",0
TODO,1
load model,0
TODO: Multiple inputs,1
TODO: Multiple outputs,1
inference,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
caffe uses NCHW,0
"self.print_intermediate_result('pooling0', False)",0
delete tmp model files,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
pylint: disable=line-too-long,0
pylint: enable=line-too-long,0
"net = caffe.Net(arch_fn, weight_fn, caffe.TEST)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
# Image Classification,0
# Semantic Segmentation,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
print(r),0
d = darknet_extractor(),0
model_filename = d.download('yolov3'),0
print(model_filename),0
"image_path = ""./mmdnn/conversion/examples/data/dog.jpg""",0
"model_path = ""./""",0
d = darknet_extractor(),0
"result = d.inference('yolov3', model_filename, model_path, image_path = image_path)",0
print(result),0
print(i),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result('pooling0', False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
need to be updated,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.input, self.model, self.testop = self.MainModel.KitModel(self.args.w)",0
testop = tf.get_default_graph().get_operation_by_name(layer_name),0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"'mobilenet_v1': mobilenet_v1.mobilenet_v1_arg_scope,",0
"'mobilenet_v1' : mobilenet_v1.mobilenet_v1,",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
raise NotImplementedError(),0
load file,0
Using cuDNN since vanilla RNN,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Notes for training NASNet Cifar Model,0
-------------------------------------,0
batch_size: 32,0
learning rate: 0.025,0
cosine (single period) learning rate decay,0
auxiliary head loss weighting: 0.4,0
clip global norm of all gradients by 5,0
600 epochs with a batch size of 32,0
This is used for the drop path probabilities since it needs to increase,0
the drop out probability over the course of training.,0
Notes for training large NASNet model on ImageNet,0
-------------------------------------,0
batch size (per replica): 16,0
learning rate: 0.015 * 100,0
learning rate decay factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 100 replicas,0
auxiliary head loss weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Notes for training the mobile NASNet ImageNet model,0
-------------------------------------,0
batch size (per replica): 32,0
learning rate: 0.04 * 50,0
learning rate scaling factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 50 replicas,0
auxiliary head weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Shape of feature map before the final layer.,0
149 x 149 x 32,0
Run the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Find where to place the reduction cells or stride normal cells,0
Setup for building in the auxiliary head.,0
Run the cells,0
true_cell_num accounts for the stem cells,0
Final softmax layer,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80,0
71 x 71 x 192,0
35 x 35 x 192,0
35 x 35 x 320,0
TODO(alemi): Register intermediate endpoints,1
"17 x 17 x 1088 if output_stride == 8,",0
33 x 33 x 1088 if output_stride == 16,0
TODO(alemi): register intermediate endpoints,1
TODO(gpapan): Properly support output_stride for the rest of the net.,1
8 x 8 x 2080,0
TODO(alemi): register intermediate endpoints,1
8 x 8 x 1536,0
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which",1
can be set to False to disable pooling here (as in resnet_*()).,0
Set weight_decay for weights in conv2d and fully_connected layers.,0
Set activation_fn and parameters for batch_norm.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
=============================================================================,0
Tensorflow mandates these.,0
Conv and DepthSepConv namedtuple define layers of the MobileNet architecture,0
Conv defines 3x3 convolution layers,0
DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.,0
stride is the stride of the convolution,0
depth is the number of channels or filters in a layer,0
_CONV_DEFS specifies the MobileNet body,0
Used to find thinned depths for each layer.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
By passing filters=None,0
separable_conv2d produces only a depthwise convolution layer,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 1024,0
Set weight_decay for weights in Conv and DepthSepConv layers.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Skip path 1,0
Skip path 2,0
"First pad with 0's on the right and bottom, then shift the filter to",0
include those 0's that were added.,0
Concat and apply BN,0
Set the prev layer to the current layer if it is none,0
Check to be sure prev layer stuff is setup correctly,0
Apply conv operations,0
Combine hidden states using 'add'.,0
Add hiddenstate to the list of hiddenstates we can choose from,0
Dont stride if this is not one of the original hiddenstates,0
"Check if a stride is needed, then use a strided 1x1 here",0
Determine if a reduction should be applied to make the number of,0
filters match.,0
Return the concat of all the states,0
Scale keep prob by layer number,0
The added 2 is for the reduction cells,0
Decrease the keep probability over time,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Inception-Resnet-A,0
Inception-Resnet-B,0
Inception-Resnet-C,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
force in-place updates of mean and variance estimates,0
Moving averages ends up in the trainable variables collection,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80,0
71 x 71 x 192,0
35 x 35 x 256,0
5 x Inception-resnet-A,0
Reduction-A,0
10 x Inception-Resnet-B,0
Reduction-B,0
5 x Inception-Resnet-C,0
pylint: disable=no-member,0
"net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None,",0
"scope='Bottleneck', reuse=False)",0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
Only insert rate to params if rate > 1.,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
import self.model,0
self.model,0
how the model can not load from `***.bin`,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
pylint: disable=line-too-long,0
pylint: enable=line-too-long,0
refer to https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/v2/tests/test_rnn_layer.py#L35,0
refer to https://github.com/PaddlePaddle/Paddle/issues/7403,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
refer to https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/v2/tests/test_rnn_layer.py#L35,0
refer to https://github.com/PaddlePaddle/Paddle/issues/7403,0
the channel of input feature is 3,0
"depth should be one of 20, 32, 44, 56, 110, 1202",0
Copyright (c) Microsoft. All rights reserved.,0
Licensed under the MIT license. See LICENSE.md file in the project root,0
for full license information.,0
==============================================================================,0
"self.model, self.testop = self.MainModel.KitModel(self.args.w)",0
"self.print_intermediate_result(None, False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
pylint: disable=line-too-long,0
pylint: enable=line-too-long,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.anchors = np.array([[10,13], [16,30],[33,23],[30,61],[62,45], [59,119],[116,90],[156,198],[373,326]])",0
print(output),0
"print(out_boxes, out_scores, out_classes)",0
get random colors,0
My kingdom for a good redistributable image drawing library.,0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
yolov3 80 classes,0
"Reshape to batch, height, width, num_anchors, box_params.",0
Adjust preditions to each spatial grid point and anchor size.,0
Note: YOLO iterates over height index before width index.,0
"TODO: It works with +1, don't know why.",1
Scale boxes back to original image shape.,0
"print(""feats,anchors, num_classes, input_shape"", feats, anchors, num_classes, input_shape)",0
"print(box_xy, box_wh, box_confidence, box_class_probs)",0
"yolo_outputs order 13,26,52",0
TODO: use keras backend instead of tf.,1
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
save network structure as JSON,0
layer_name = 'block2_pool',0
"intermediate_layer_model = keras.Model(inputs=model.input,",0
outputs=model.get_layer(layer_name).output),0
intermediate_output = intermediate_layer_model.predict(img),0
print (intermediate_output),0
print (intermediate_output.shape),0
"print (""%.30f"" % np.sum(intermediate_output))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Convert network architecture only,0
MXNet,0
Caffe,0
For CoreML,0
Caffe,0
Darknet,0
"exts = ['.pb', '.npy', '.py']",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check non-sense always input module.Module,0
if not (type(model) == mx.module.Module,0
or type(model) == mx.module.SequentialModule,0
or type(model) == mx.model),0
"raise TypeError(""MXNet layer of type %s is not supported."" % type(model))",0
"if layer[""op""] == ""null"":",0
continue,0
"raise NotImplementedError(""Cannot support multi-input"")",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"mxnet-cpu only support channel first, default convert the model and weight as channel first",0
We use the real_name for specifying the input layer in data_names,0
since MXNet API wants the actual name of the layer. On the other,0
"hand, the module API wants the last symbol in the symbol chain, so",0
for the output node we need to use the actual python variable name,0
of the last layer (real_variable_name).,0
download and show the image,0
"convert into format (batch, RGB, width, height)",0
"to show the image, change the argument show into True",0
compute the predict probabilities,0
print the top-5,0
# call function predict,0
"with open('synset.txt', 'r') as f:",0
labels = [l.rstrip() for l in f],0
"predict(model, labels, 'http://writm.com/wp-content/uploads/2016/08/Cat-hd-wallpapers.jpg')",0
"print(""Warning: MXNet Convolution Layer pad does not match IR Convolution Layer pad"")",0
"layout = IR_node.IR_layer.attr[""data_format""].s",0
if layout not in MXNetEmitter.channels_last:,0
Add LeakyReLU Elu(slope not support),0
"axis = IR_node.IR_layer.attr[""axis""].i",0
not supported yet,0
"axis = IR_node.IR_layer.attr[""axis""].i",0
not supported yet,0
"axis = IR_node.IR_layer.attr[""axis""].i",0
not supported yet,0
"print(""Warning: MXNet Pooling Layer pad does not match IR Pooling Layer pad"")",0
"def emit_ConvTranspose(self, IR_node):",0
if self.weight_loaded:,0
weight_dict = self.weights[IR_node.name],0
weights = weight_dict['weights'],0
"dim = len(IR_node.IR_layer.attr[""kernel_shape""].list.i) - 2",0
kernel = list(),0
"for idx in range(0, dim):",0
"kernel.append(IR_node.IR_layer.attr[""kernel_shape""].list.i[idx])",0
stride = list(),0
"for e in IR_node.IR_layer.attr[""strides""].list.i[1:-1]:",0
stride.append(e),0
dilate = list(),0
"for e in IR_node.IR_layer.attr[""dilations""].list.i[1:-1]:",0
dilate.append(e),0
"dilate = ', '.join('%s' % i for i in dilate)",0
defuse_pad = False,0
pad = list(),0
"if ""pads"" in IR_node.IR_layer.attr:",0
output_shape = list(),0
"for e in IR_node.IR_layer.attr[""_output_shapes""].list.shape[0].dim:",0
output_shape.append(e.size),0
"# print(""Warning: MXNet Deconvolution Layer pad does not match IR Deconvolution Layer pad"")",0
"defuse_pad, pad = MXNetEmitter.transfer_pad(IR_node.IR_layer.attr[""pads""].list.i)",0
"pad = ', '.join('%s' % i for i in pad)",0
"kernel = ', '.join('%s' % i for i in kernel)",0
"stride = ', '.join('%s' % i for i in stride)",0
"num_filter = IR_node.IR_layer.attr[""kernel_shape""].list.i[-2]",0
"no_bias = not IR_node.IR_layer.attr[""use_bias""].b",0
if not no_bias and self.weight_loaded:,0
"self.output_weights[IR_node.replace_scope(IR_node.name) + ""_bias""] = weight_dict['bias']",0
"# layout = IR_node.IR_layer.attr[""data_format""].s",0
if dim == 1:,0
layout = 'NCW',0
elif dim == 2:,0
layout = 'NCHW',0
elif dim == 3:,0
layout = 'NCDHW',0
if self.weight_loaded:,0
# if layout not in MXNetEmitter.channels_last:,0
"weights = MXNetEmitter.transpose(weights, dim)",0
"self.output_weights[IR_node.replace_scope(IR_node.name) + ""_weight""] = weights",0
"code = """"",0
if not defuse_pad:,0
"code = ""{:<15} = mx.sym.Deconvolution(data = {}, kernel = ({}), stride = ({}), dilate = ({}), pad = ({}), num_filter = {}, no_bias = {}, layout = '{}', name = '{}')"".format(",0
"IR_node.replace_scope(IR_node.name),",0
"IR_node.replace_scope(IR_node.in_edges[0]),",0
"kernel,",0
"stride,",0
"dilate,",0
"pad,",0
"num_filter,",0
"no_bias,",0
"layout,",0
IR_node.replace_scope(IR_node.name)),0
else:,0
"code = self.set_pad(IR_node, code, pad)",0
"code += ""\n    {:<15} = mx.sym.Deconvolution(data = {}, kernel = ({}), stride = ({}), dilate = ({}), num_filter = {}, no_bias = {}, layout = '{}', name = '{}')"".format(",0
"IR_node.replace_scope(IR_node.name), IR_node.replace_scope(IR_node.name) + ""_pad"", kernel, stride, dilate, num_filter, no_bias, layout, IR_node.replace_scope(IR_node.name))",0
return code,0
reverse cannot support yet,0
"code = ""{:<15} = mx.sym.transpose(data = {}, axes = (0, 2, 3, 1))\n"".format(""trans"", self.parent_variable_name(IR_node))",0
save the constant into weight dict,0
param_code does not need parameter slice.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Not support yet,0
"""softrelu""  : ""SoftReLU""",0
Load the model network and weights,0
"digraph = mx.viz.plot_network(sym, save_format='jpg') # For debugging",0
digraph.render(),0
model.bind(data_shapes = data_shapes),0
model.init_params(),0
"mod.load(model_path, epoch_num)",0
return mod.get_params(),0
load the model network,0
adjust the data format,0
load model files into MXNet graph,0
data_shape arguments added to calculate infer_shape(required),0
"if isinstance(input_arg, basestring):",0
Build network graph,0
transpose to channel last,0
"name, op",0
input edge,0
output shape,0
input edge,0
attr,0
"print(""Warning: MXNet symbol pad does not support channel last"")",0
"IR_node.attr[""pads""].list.i.extend([0, 0])",0
for e in pad:,0
"IR_node.attr[""pads""].list.i.extend([e, e])",0
"IR_node.attr[""pads""].list.i.extend([0, 0])",0
"name, op",0
input edge,0
units,0
use bias (no_bias default = False),0
weights,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [NM]"".format(source_node.name, self.data_format))",0
input edge,0
output shape,0
kernel_shape,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [{}]"".format(source_node.name, self.data_format, layout))",0
groups,0
in_channel = in_channel // group,0
use_bias (no_bias default = False),0
strides,0
dilations,0
data_format,0
padding,0
weights,0
axis,0
scale,0
epsilon,0
momentum,0
weights,0
gamma,0
beta,0
mean,0
var,0
"name, op",0
input edge,0
pooling type (sum not allowed yet),0
strides,0
kernel_shape,0
padding,0
output shape,0
"name, op",0
input edge,0
output shape,0
dim,0
"def rename_log_softmax(self, source_node):",0
"raise NotImplementedError(""not support yet"")",0
"def rename_Correlation(self, source_node):",0
"raise NotImplementedError(""not support yet"")",0
"name, op",0
input edge,0
padding,0
output shape,0
kernel_shape,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [{}]"".format(source_node.name, self.data_format, layout))",0
use_bias (no_bias default = False),0
strides,0
dilations,0
data_format,0
groups,0
weights,0
"def rename_RNN(self, source_node):",0
"raise NotImplementedError(""RNN not support yet"")",0
"name, op",0
input edge,0
input_dim,0
output_dim,0
dtype,0
output shape,0
"IR only support elu and prelu from {'elu', 'leaky', 'prelu', 'rrelu'}",0
return,0
"name, op",0
gamma,0
input edge,0
"alpha [exp(x) - alpha], but mxnet attr slope [slope*(exp(x) - 1)] when x < 0",0
"IR_node.attr[""alpha""].f = float()",0
output shape,0
"raise NotImplementedError(""slope cannot convert to alpha"")",0
"def rename_InstanceNorm(self, source_node):",0
raise NotImplementedError,0
"def rename_L2Normalization(self, source_node):",0
raise NotImplementedError,0
alpha,0
beta,0
knorm,0
nsize,0
keep_prob,0
mode,0
reverse cannot support yet,0
old API target_shape not support yet,0
output shape,0
"raise NotImplementedError(""adjust output shape"")",0
dim,0
dtype,0
output shape,0
"name, op",0
input edge,0
output shape,0
axis,0
"raise NotImplementedError(""No matching IR api"")",0
"left strip the ""_"" at the beginning of the name",0
"Issue #85, #135",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
keep dims,0
axes,0
"name, op",0
epsilon,0
moving variance (var),0
print(moving_variance.name),0
gamma (scale),0
mean,0
bias,0
input node,0
output node,0
load model files into TensorFlow graph,0
extract subgraph using in_nodes and dest_nodes,0
Build network graph,0
extract subgraph using dest_nodes,0
Graph Transform,0
Get input node name,0
TODO: Fused conv and pool with padding is different from defused operators,0
strides,0
window_shape,0
pool type,0
padding,0
shape,0
strides,0
input[1] : W,0
filter,0
padding,0
output[0] : B,0
converted [dropout],0
convert [tf.contrib.layers.batch_norm],0
normal Add,0
units,0
Weights,0
FullyConnected Layer,0
"name, op",0
get Bias,0
Matmul Layer,0
Deal Dropout,0
keep prob,0
Remove nodes,0
Mul,0
Floor,0
print (source_node),0
print (source_node.layer),0
assert False,0
"def rename_RandomShuffleQueueV2(self, source_node):",0
# print(source_node.layer),0
"IR_node = self._convert_identity_operation(source_node, in_edge_count = 0, new_op = 'DataInput')",0
# IR_node.attr['shape'].shape.MergeFromString(source_node.layer.attr['_output_shapes'].list.shape[0].SerializeToString()),0
# IR_node.attr['shape'].shape.dim[0].size = -1,0
IR_node.attr['dtype'].type = self.dtype_map[source_node.layer.attr['component_types'].list.type[0]],0
paddings,0
gamma (scale),0
bias,0
Mean,0
Var,0
TODO: Current it is only for slice,1
TODO:  only for 1D,1
strides,0
input[1] : W,0
filter,0
padding,0
output[0] : B,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
check whether flatten operator should be added,0
flatten is needed,0
"self._emit_unary_operation(IR_node, ""contrib.layers.flatten"")",0
param_code does not need parameter slice.,0
skip_prefix = [,0
"""^"",",0
"""train_op"",",0
"""save"",",0
"""gradients"",",0
"""init"",",0
"""global_step"",",0
"""distort_image"",",0
"""Adagrad"",",0
],0
"""Switch""",0
"""RequantizationRange"",",0
"""Requantize"",",0
"""Identity"",",0
"""Mean"",",0
"""Cast""",0
load model files into TensorFlow graph,0
Save it to an output file,0
keep dims,0
axes,0
ssd model is transformed,0
Ax - (Au - b),0
A,0
b,0
print(sub_content),0
epsilon,0
moving variance (var) /read,0
gamma (scale),0
print(Rsqrt.out_edges),0
beta  (bias),0
moving mean (mean),0
input node,0
epsilon,0
beta,0
gamma (scale),0
"output_node = self.get_son(source_node.name, [0, 0, 0, 0], True)",0
assert False,0
Quantized model type,0
TODO: Fused conv and pool with padding is different from defused operators,0
assert variable.get_attr('_output_shapes')[0].dim[0].size == IR_node.attr['kernel_shape'].list.i[-1],0
strides,0
window_shape,0
pool type,0
padding,0
"In facenet or other newtwork using slim.batch_norm,",0
"There are two BN(train, test) skip switch and merge.",0
epsilon,0
moving variance (var) /read,0
gamma (scale),0
beta  (bias),0
moving mean (mean),0
ReduceMean,0
keep dims,0
axes,0
Skip the node as merge,0
weights,0
alpha,0
beta,0
Skip BiasAdd,0
weights,0
"input_node_perm = self.check_const(self.get_parent(source_node.name, [1], True))",0
paddings,0
for attr.shape >= 2,0
"For models built by slim.batch_norm, remove duplicate BN (eg.facenet)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Build network graph,0
"PaddleParser._set_output_shape(source_node, IR_node)",0
"name, op",0
input edge,0
For concat axis,0
only for training,0
"name, op",0
input edge,0
input edge,0
layer and spec,0
width <=> x or height <=> y,0
output shape,0
"name, op",0
it should be in the shape of height x width x inputchannel x outputchannel,0
use_bias: TODO,1
pad_dim,0
fail report because of auto_pad,0
if dilation_x == 1 and dilation_y == 1:,0
if output_x * stride_x == input_x and output_y * stride_y == input_y:,0
"auto_pad = ""SAME""",0
kwargs['auto_pad'] = auto_pad,0
elif output_x * stride_x == input_x - width + 1 and output_y * stride_y == input_y - height + 1:,0
"auto_pad = ""VALID""",0
kwargs['auto_pad'] = auto_pad,0
defuse the activation layer,0
"name, op",0
input edge,0
layer and spec,0
output shape,0
"channels_first, then axis = 1",0
epsilon,0
compute adjusted parameters,0
Reference: parameter transformation https://github.com/apple/coremltools/issues/153,0
convert type because of tensorflow,0
flatten,0
mean,0
var,0
defuse the activation layer,0
"name, op",0
input edge,0
layer and spec,0
assert False,0
output shape,0
pad_dim,0
padding mode,0
"If padding == ""SAME"": output_spatial_shape[i] = ceil(input_spatial_shape[i] / strides[i])",0
"If padding == ""VALID"": output_spatial_shape[i] = ceil((input_spatial_shape[i] - (spatial_filter_shape[i]-1) * dilation_rate[i]) / strides[i]).",0
"name, op",0
input edge,0
layer and spec,0
units,0
output shape,0
use_bias,0
Kit weight tranpose,0
weight: N x M -> C x H x W x M -> H x W x C x M -> N x M,0
,0
weights,0
need the shape TODO,1
only for training,0
"name, op",0
input edge,0
shape,0
"name, op",0
input edge,0
layer and spec,0
output shape,0
alpha,0
beta,0
nsize,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Build network graph,0
strides,0
window_shape,0
pool type,0
padding,0
TODO: Handle square,1
self._print_layer(source_node),0
print (source_node.name),0
"print (self.src_graph.get_parent(source_node.name, [0]).real_name)",0
assert False,0
Convolution,0
Kernel,0
Attributes,0
Bias,0
Activation,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
BlockFunction node,0
redirect the composite's inputs to the true inputs,0
"BlockFunctions are short-circuited, and not added to accum[]",0
Function node,0
OutputVariable node,0
def build(self):,0
"_traverse_graph(self, self.model.root_function)",0
"super(CntkGraph, self).build()",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"IR_node.get_attr('output_dim'),",0
??,0
print(IR_node.layer),0
assert False,0
"def emit_Unstack(self, IR_node):",0
"num_str = ""{}.shape[{}]"".format(self.parent_variable_name(IR_node), IR_node.get_attr('axis'))",0
axis = IR_node.get_attr('axis'),0
"parent_variable_shape = ""list({}.shape)"".format(self.parent_variable_name(IR_node)",0
"if self.IR_graph.get_parent(IR_node.name, [0]).type != 'Embedding'",0
else self.parent_variable_name(IR_node)+'.E'),0
if axis==1:,0
"shape_str = ""tuple([{}[0]*{}[{}], 1].extend({}[{}+1:]))"".format(",0
"parent_variable_shape,",0
"parent_variable_shape,",0
"str(axis),",0
"parent_variable_shape,",0
str(axis)),0
else:,0
"shape_str = ""tuple([{}[0]*{}[{}]].extend({}[1:{}]).append(1).extend({}[{}+1:]))"".format(",0
"parent_variable_shape,",0
"parent_variable_shape,",0
"str(axis),",0
"parent_variable_shape,",0
"str(axis),",0
"parent_variable_shape,",0
str(axis)),0
"code = ""{:<15} = cntk.reshape({}, {}, name='{}')"".format(",0
"IR_node.variable_name,",0
"self.parent_variable_name(IR_node),",0
"shape_str,",0
IR_node.variable_name),0
"code = ""{: <15} = cntk.reshape({}, {}.shape, name='{}')"".format(",0
"IR_node.variable_name,",0
"self.parent_variable_name(IR_node),",0
"self.parent_variable_name(IR_node),",0
IR_node.name,0
),0
return code,0
"def emit_Fill(self, IR_node):",0
"code = ""{:<15} = cntk.Constant({}, {}, name='{}')"".format(",0
"IR_node.variable_name,",0
"IR_node.get_attr('value'),",0
"self.parent_variable_name(IR_node),",0
IR_node.name),0
return code,0
param_code does not need parameter slice.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
For padding,0
print (input_shape),0
print (kernel_shape),0
print (strides),0
Not tested,0
print (input_shape),0
print (kernel_shape),0
print (strides),0
kernel_shape[idx] = (kernel_shape[idx] - 1) * dilation_rate + 1,0
print ([0] + up_list + [0] + down_list if data_format.startswith('NC') else up_list + [0] + down_list + [0]),0
print ('-----------------------------------------------------'),0
network library,0
not support multi-threads download,0
key: layer_name    value: keras layer,0
private functions,0
next_node_info.left_in_edges -= 1,0
"print ('{} --> {}'.format(src, dst))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
share functions,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
name --> (weight_name --> ndarray),0
Generated by the protocol buffer compiler.  DO NOT EDIT!,0
source: graph.proto,0
@@protoc_insertion_point(imports),0
@@protoc_insertion_point(class_scope:GraphDef),0
@@protoc_insertion_point(class_scope:NodeDef.AttrEntry),0
@@protoc_insertion_point(class_scope:NodeDef),0
@@protoc_insertion_point(class_scope:AttrValue.ListValue),0
@@protoc_insertion_point(class_scope:AttrValue),0
@@protoc_insertion_point(class_scope:TensorShape.Dim),0
@@protoc_insertion_point(class_scope:TensorShape),0
@@protoc_insertion_point(class_scope:LiteralTensor),0
@@protoc_insertion_point(module_scope),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
First try to read it as a binary file.,0
Next try to read it as a text file.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"If pattern.inputs is empty, skips the rest and accepts all the inputs.",0
"If order doesn't matter for the inputs, then make sure we match at least",0
one permutation of the inputs.,0
"def get_tensor(self, pattern_or_name):",0
op_tensor = self._get_op_tensor(pattern_or_name),0
return op_tensor[1] if op_tensor else None,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
"Python 3.3.2+ implements `yield from`, but for now:",0
"check the same pattern scope node whether have same inputs, outputs and weights.",0
"For those don't have, rename their scope names.",0
"clear out scope node, typically input constant node.",0
get sub_scopes,0
decline the suffix number,0
Obtain nodes where the scope name that satisfies top_level is top_scope and sub_level is sub_scope,0
Obtain nodes where the scope name that satisfies in sub_level is sub_scope,0
Obtain nodes where the scope name that satisfies in top_level is top_scope,0
Obtain all nodes grouped by sub_level sub_scope,0
cover the node,0
"store idx, node into a dict and sort it later to keep its topology sort.",0
in_node's out edges replace n_name with scope node name.,0
no out nodes means the last node in scope nodes should be returned,0
"if out_node is scope node, replace the scope node's inner topology list node.",0
the input parameter shoule be sliced when call func.,0
modify the in_edges in scope inner nodes. decline the :idx.,0
1. initilize scope node,0
2. get scope nodes' topology list.,0
3. rebuild the edges connection after folding these scope nodes into one node and,0
get this scope node's return variables.,0
4. rebuild graph.,0
RNN-related attrs.,0
get input params,0
self.store_const_to_top(result),0
"self.set_top_node_prop(result, pattern_name)",0
Do not include input op.,0
"TODO: pytorch, mxnet, keras, cntk",1
-*- coding: utf-8 -*-,0
from keras.layers.core import Layer,0
Arguments,0
Input shape,0
Output shape,0
Prepare broadcasting shape.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Load the model network,0
Load the model weights,0
load model files into Keras graph,0
Keras 2.1.6,0
Keras. 2.2.2,0
"_keras.utils.plot_model(model, ""model.png"", show_shapes = True)",0
Build network graph,0
TODO: Fused conv and pool with padding is different from defused operators,0
TODO: More activation functions,1
for ELU,0
input edge,0
"name, op",0
weights,0
pads,0
filter,0
"[kd, kh, kw, channel_size, filter number]",0
use_bias,0
strides,0
"[1, sd, sh, sw, 1]",0
dilations,0
"[1, dd, dh, dw, 1]",0
activation,0
"name, op",0
input edge,0
padding,0
strides,0
"[1, sd, sh, sw, 1]",0
window_shape,0
"[1, pd, ph, pw, 1]",0
"name, op",0
input edge,0
For concat axis,0
"name, op",0
input edge,0
padding,0
only for training,0
"name, op",0
input edge,0
"name, op",0
input edge,0
Merge Layers,0
only for training,0
"name, op",0
input edge,0
shape,0
only for training,0
"name, op",0
input edge,0
Core Layers,0
"name, op",0
input edge,0
units,0
use_bias,0
weights,0
activation,0
"name, op",0
input edge,0
"name, op",0
input edge,0
size,0
"name, op",0
input edge,0
input_dim,0
output_dim,0
mask_zero,0
weights,0
"name, op",0
input edge,0
units,0
use_bias,0
"for Keras, drop_out and recurrent_dropout",0
activation,0
"name, op",0
input edge,0
units,0
activation,0
weights,0
Kit TODO : need to search the tf,1
"name, op",0
input edge,0
for target shape,0
"name, op",0
input edge,0
axis,0
"Parameter arrangement in Keras: gamma, beta, mean, variance",0
scale,0
beta,0
mean,0
var,0
"name, op",0
input edge,0
for target shape,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
rot weights,0
"print(""Converting layer {}({})"".format(current_node.name, node_type))",0
TODO,1
"return input_node, 'same'",0
"assert IR_node.get_attr('group', 1) == 1",0
change dw from filters to 1,0
reset the default dilation,0
############,0
Operators #,0
############,0
"code = self._emit_merge(IR_node, ""subtract"")",0
TODO,1
"when converting from coreml model, reshape is needed after the global pooling",0
for Keras,0
It arouses some problems:,0
it can be implemented by Lambda Layer,0
https://github.com/keras-team/keras/issues/890,0
Keras == 2.1.6,0
Keras == 2.2.2,0
TODO: arguments won't be saved in keras export model,1
param_code does not need parameter slice.,0
Prepare broadcasting shape.,0
"output = Lambda(lambda x: tf.fill(x, value))(input)",0
return output,0
def _layer_Constant(self):,0
"self.add_body(0, '''",0
class my_constant(keras.layers.Layer):,0
"def __init__(self, value, **kwargs):",0
"super(my_constant, self).__init__(**kwargs)",0
self._value = value,0
"# the input is dummy, just for creating keras graph.",0
"def call(self, dummy):",0
res = K.constant(self._value),0
self.output_shapes = K.int_shape(res),0
return res,0
"def compute_output_shape(self, input_shape):",0
return self.output_shapes,0
'''),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
Kit: TODO,1
Duplicate models for weight sharing,0
Expand the sub-models,0
"with open('modelmapbydataset.json', 'w') as outfile:",0
"json.dump(new_data, outfile)",0
generate makedown script,0
add Image Classification,0
add Object Detection,0
MMdnn introduction,0
steps for model conversion,0
Generate model converter description,0
Generate models list,0
generate makedown script,0
MMdnn introduction,0
steps for model conversion,0
Generate model converter description,0
Generate models list,0
console.info(JSON.parse(e.target.result)),0
"default, draw the model.json file",0
window.onload = () => {,0
"d3.json(filePath, (error, json) => {",0
if (error) throw error;,0
// console.info(json),0
draw(json),0
}),0
},0
generate dag,0
IR model or keras model,0
path generator,0
let arrowL = `M ${points[len - 1].x} ${points[len - 1].y} l -7 -7`,0
let arrowR = `M ${points[len - 1].x} ${points[len - 1].y} l  7 -7`,0
select a layer,0
remove list,0
draw,0
mode;,0
"let scale = Math.min(window.innerWidth*0.85 / width, (window.innerHeight-85)/height)",0
"var filter = def.append(""filter"")",0
".attr(""id"", ""dropshadow"")",0
"filter.append(""feGaussianBlur"")",0
".attr(""in"", ""SourceAlpha"")",0
".attr(""stdDeviation"", 4)",0
".attr(""result"", ""blur"");",0
"// filter.append(""feOffset"")",0
"//     .attr(""in"", ""blur"")",0
"//     .attr(""dx"", 2)",0
"//     .attr(""dy"", 2)",0
"//     .attr(""result"", ""offsetBlur"");",0
"var feMerge = filter.append(""feMerge"");",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""offsetBlur"")",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""SourceGraphic"");",0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
"svg.on('keydown', ()=>{console.info('ddd')})",0
".on('mouseover', ()=>{console.info('mouse over')})",0
k = parseFloat(k)*parseFloat(k_),0
"limit x, y",0
// a trick to make text svg transform in MS Edge,0
"d3.selectAll("".labels"").classed(""tempclass"", true);",0
"setTimeout(function () { d3.selectAll("".labels"").classed(""tempclass"", false); }, 40);",0
when shift is down,0
ulti function,0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
rm the '_',0
Base64 encode: https://developers.google.com/protocol-buffers/docs/proto3,0
Search the node,0
select by id: https://stackoverflow.com/questions/37270787/uncaught-syntaxerror-failed-to-execute-queryselector-on-document,0
Scroll,0
console.info(JSON.parse(e.target.result)),0
"default, draw the model.json file",0
window.onload = () => {,0
"d3.json(filePath, (error, json) => {",0
if (error) throw error;,0
// console.info(json),0
draw(json),0
}),0
},0
generate dag,0
IR model or keras model,0
path generator,0
let arrowL = `M ${points[len - 1].x} ${points[len - 1].y} l -7 -7`,0
let arrowR = `M ${points[len - 1].x} ${points[len - 1].y} l  7 -7`,0
select a layer,0
draw,0
"let scale = Math.min(window.innerWidth*0.85 / width, (window.innerHeight-85)/height)",0
"var filter = def.append(""filter"")",0
".attr(""id"", ""dropshadow"")",0
"filter.append(""feGaussianBlur"")",0
".attr(""in"", ""SourceAlpha"")",0
".attr(""stdDeviation"", 4)",0
".attr(""result"", ""blur"");",0
"// filter.append(""feOffset"")",0
"//     .attr(""in"", ""blur"")",0
"//     .attr(""dx"", 2)",0
"//     .attr(""dy"", 2)",0
"//     .attr(""result"", ""offsetBlur"");",0
"var feMerge = filter.append(""feMerge"");",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""offsetBlur"")",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""SourceGraphic"");",0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
"svg.on('keydown', ()=>{console.info('ddd')})",0
".on('mouseover', ()=>{console.info('mouse over')})",0
k = parseFloat(k)*parseFloat(k_),0
"limit x, y",0
// a trick to make text svg transform in MS Edge,0
"d3.selectAll("".labels"").classed(""tempclass"", true);",0
"setTimeout(function () { d3.selectAll("".labels"").classed(""tempclass"", false); }, 40);",0
when shift is down,0
ulti function,0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
space = '',0
Get the long description from the README file,0
Versions should comply with PEP440.  For a discussion on single-sourcing,0
"the version across setup.py and the project code, see",0
https://packaging.python.org/en/latest/single_source_version.html,0
The project's main homepage.,0
Author details,0
Choose your license,0
See https://pypi.python.org/pypi?%3Aaction=list_classifiers,0
How mature is this project? Common values are,0
3 - Alpha,0
4 - Beta,0
5 - Production/Stable,0
Indicate who your project is intended for,0
"Pick your license as you wish (should match ""license"" above)",0
"Specify the Python versions you support here. In particular, ensure",0
"that you indicate whether you support Python 2, Python 3 or both.",0
What does your project relate to?,0
You can just specify the packages manually here if your project is,0
simple. Or you can use find_packages().,0
"Alternatively, if you want to distribute just a my_module.py, uncomment",0
this:,0
"py_modules=[""my_module""],",0
List run-time dependencies here.  These will be installed by pip when,0
"your project is installed. For an analysis of ""install_requires"" vs pip's",0
requirements files see:,0
https://packaging.python.org/en/latest/requirements.html,0
"To provide executable scripts, use entry points in preference to the",0
"""scripts"" keyword. Entry points provide cross-platform support and allow",0
pip to create the appropriate form of executable for the target platform.,0
Communication to TensorFlow server via gRPC,0
TensorFlow serving stuff to send messages,0
Command line arguments,0
Send request,0
request.inputs['input'].CopyFrom(),0
"'xception'     : [TestModels.onnx_emit],",0
"'nasnet'       : [TestModels.onnx_emit],",0
omit tensorflow lead to crash,0
"'nasnet-a_large'       : [TestModels.mxnet_emit, TestModels.pytorch_emit, TestModels.tensorflow_emit],",0
"'inception_resnet_v2'  : [TestModels.caffe_emit, TestModels.keras_emit, TestModels.mxnet_emit, TestModels.pytorch_emit, TestModels.tensorflow_emit],",0
"Function self.assertEquals has deprecated, change to assertEqual",0
"self.assertEqual(original_predict.shape, converted_predict.shape)",0
get original model prediction result,0
original to IR,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
print(original_predict),0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
get shape,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
IR to code,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
The list is in RGB oder,0
IR to Model,0
"converted_file = original_framework + '_coreml_' + architecture_name + ""_converted""",0
"converted_file = converted_file.replace('.', '_')",0
load model,0
save model,0
"coremltools.utils.save_spec(model.get_spec(), converted_file)",0
inference,0
IR to code,0
import converted model,0
"In case of odd number add the extra padding at the end for SAME_UPPER(eg. pads:[0, 2, 2, 0, 0, 3, 3, 0]) and at the beginning for SAME_LOWER(eg. pads:[0, 3, 3, 0, 0, 2, 2, 0])",0
"'xception'     : [onnx_emit],",0
"'nasnet'       : [onnx_emit],",0
"'voc-fcn8s'     : [onnx_emit], # TODO: ConvTranspose, Crop",0
"'voc-fcn16s'    : [onnx_emit], # TODO: ConvTranspose, Crop",0
"'voc-fcn32s'    : [onnx_emit], # TODO: ConvTranspose, Crop",0
"'resnet_v1_50'          : [onnx_emit], # POOL: strides > window_shape not supported due to inconsistency between CPU and GPU implementations",0
"'resnet_v1_152'         : [onnx_emit], # POOL: strides > window_shape not supported due to inconsistency between CPU and GPU implementations",0
"'resnet_v2_50'          : [onnx_emit], # POOL: strides > window_shape not supported due to inconsistency between CPU and GPU implementations",0
"'resnet_v2_152'         : [onnx_emit], # POOL: strides > window_shape not supported due to inconsistency between CPU and GPU implementations",0
"'nasnet-a_large'        : [onnx_emit], # POOL: strides > window_shape not supported due to inconsistency between CPU and GPU implementations",0
TODO: coredump,1
"'alexnet'       : [cntk_emit, keras_emit, tensorflow_emit],",0
"'nasnet'       : [tensorflow_emit, keras_emit, coreml_emit],",0
"'facenet'      : [tensorflow_emit, coreml_emit,mxnet_emit,keras_emit]  # TODO",0
"'vgg16': [tensorflow_emit],",0
'alexnet': [tensorflow_emit],0
get original model prediction result,0
def test_caffe(self):,0
try:,0
import caffe,0
"self._test_function('caffe', self.caffe_parse)",0
except ImportError:,0
"print('Please install caffe! Or caffe is not supported in your platform.', file=sys.stderr)",0
def test_cntk(self):,0
try:,0
import cntk,0
"self._test_function('cntk', self.cntk_parse)",0
except ImportError:,0
"print('Please install cntk! Or cntk is not supported in your platform.', file=sys.stderr)",0
def test_coreml(self):,0
from coremltools.models.utils import macos_version,0
"if macos_version() < (10, 13):",0
"print('Coreml is not supported in your platform.', file=sys.stderr)",0
else:,0
"self._test_function('coreml', self.coreml_parse)",0
def test_keras(self):,0
"self._test_function('keras', self.keras_parse)",0
def test_mxnet(self):,0
"self._test_function('mxnet', self.mxnet_parse)",0
def test_darknet(self):,0
"self._test_function('darknet', self.darknet_parse)",0
def test_paddle(self):,0
# omit tensorflow lead to crash,0
import tensorflow as tf,0
try:,0
import paddle.v2 as paddle,0
"self._test_function('paddle', self.paddle_parse)",0
except ImportError:,0
"print('Please install Paddlepaddle! Or Paddlepaddle is not supported in your platform.', file=sys.stderr)",0
def test_pytorch(self):,0
"self._test_function('pytorch', self.pytorch_parse)",0
def test_tensorflow(self):,0
"self._test_function('tensorflow', self.tensorflow_parse)",0
def test_tensorflow_frozen(self):,0
"self._test_function('tensorflow_frozen', self.tensorflow_frozen_parse)",0
"'vgg19'         : [TestModels.onnx_emit],",0
"'voc-fcn8s'    : [TestModels.cntk_emit, TestModels.coreml_emit, TestModels.tensorflow_emit],",0
"'vgg19'         : [TestModels.caffe_emit, TestModels.cntk_emit, TestModels.coreml_emit, TestModels.keras_emit, TestModels.mxnet_emit, TestModels.pytorch_emit, TestModels.tensorflow_emit],",0
"'voc-fcn8s'     : [TestModels.cntk_emit, TestModels.coreml_emit, TestModels.tensorflow_emit],",0
"'voc-fcn16s'    : [TestModels.cntk_emit, TestModels.coreml_emit, TestModels.tensorflow_emit],",0
"'voc-fcn32s'    : [TestModels.cntk_emit, TestModels.coreml_emit, TestModels.tensorflow_emit],",0
"'nasnet'       : [TestModels.onnx_emit],",0
"'vgg19'         : [TestModels.onnx_emit],",0
"'alexnet'       : [TestModels.caffe_emit, TestModels.cntk_emit, TestModels.coreml_emit, TestModels.mxnet_emit, TestModels.pytorch_emit, TestModels.tensorflow_emit],",0
"'nasnet-a_large'       : [TestModels.mxnet_emit, TestModels.pytorch_emit, TestModels.tensorflow_emit],",0
"'inception_resnet_v2'  : [TestModels.caffe_emit, TestModels.keras_emit, TestModels.mxnet_emit, TestModels.pytorch_emit, TestModels.tensorflow_emit],",0
"'xception'     : [TestModels.onnx_emit],",0
"'nasnet'       : [TestModels.onnx_emit],",0
!/usr/bin/python,0
major python major_python_versions as python2 and python3,0
operating system,0
do not deal,0
I don't want to deal with auto_pad,0
Don't support auto_pad current!,0
2018-02-28,0
if group is None:,0
group = 1,0
group is not support yet too.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
node,0
input,0
edge(node a <-> node b),0
,0
"key is edge name, value is src/dst node name",0
"key is initializer name, value is TensorProto",0
print(name),0
print(layer.op_type),0
n is input data,0
n is input edge,0
omit node of some type,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"if node len(in_edges), generate additional DataInput node",0
print,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
TODO,1
"'max_pool2d': convert_maxpool,",0
"'onnx::Mul': convert_elementwise_mul,",0
"'onnx::Sub': convert_elementwise_sub,",0
"'onnx::ConvTranspose': convert_convtranspose,",0
"'onnx::LeakyRelu': convert_lrelu,",0
"'onnx::Sigmoid': convert_sigmoid,",0
"'onnx::Softmax': convert_softmax,",0
"'onnx::Selu': convert_selu,",0
"'onnx::Transpose': convert_transpose,",0
"'onnx::Reshape': convert_reshape,",0
"'onnx::MatMul': convert_matmul,",0
"'onnx::Gather': convert_gather,",0
"'onnx::ReduceSum': convert_reduce_sum,",0
"'onnx::Constant': convert_constant,",0
"'onnx::Upsample': convert_upsample,",0
"'onnx::Pad': convert_padding,",0
###########,0
property #,0
###########,0
###################,0
Public Functions #,0
###################,0
test,0
cpu: https://github.com/pytorch/pytorch/issues/5286,0
Build network graph,0
"(batch, C, H, W)  & NHWC",0
#########,0
Layers #,0
#########,0
dilation,0
handle bias,0
TODO,1
output_shape,0
epsilon,0
mean,0
var,0
Kit weight tranpose,0
weight: N x M -> C x H x W x M -> H x W x C x M -> N x M,0
,0
weights,0
use_bias,0
units,0
handle weight,0
handle bias,0
###################,0
Helper Functions #,0
###################,0
print (dir(data)),0
print (i),0
"print (""    {} with shape {}"".format(j, load_weight[i][j].shape))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Scopes created in a nested scope may have initial characters,0
that are illegal as the initial character of an op name,0
"(viz. '-', '\', '/', and '_').",0
sanity check.,0
run dce first to eliminate dead parts of the graph that might have been,0
left behind by things like symbolic_override,0
construct graph,0
nodes,0
input layer,0
TODO,1
build each layer,0
input,0
"print(node_input_name ,'->', node_name)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Base Functions,0
"padding,",0
"exstr = "", value=float('-Inf')""",0
"exstr = """"",0
Change to padding defuse,0
"input_node = self._defuse_padding(IR_node, exstr)",0
Ignore it in Pytorch,0
for Keras,0
"self.add_init(2, ""self.{:<15} = torch.from_numpy(__weights_dict['{}']['value'])"".format(",0
"IR_node.variable_name,",0
IR_node.name)),0
param_code does not need parameter slice.,0
from torch.nn.parameter import Parameter,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self._type = layer.__class__.__name__.replace('Backward', '')",0
"self._name = ""{}_{}"".format(self.type, id)",0
TODO,1
"""""""",0
build graph for pytorch 0.2.0,0
"""""""",0
dummy_input = torch.autograd.Variable(torch.randn(shape)),0
output_node = self.model(dummy_input),0
search_queue = [output_node.grad_fn],0
"tmp_node = PyTorchGraphNode(output_node.grad_fn, 0)",0
self.layer_map[tmp_node.name] = tmp_node,0
visited = {output_node.grad_fn : self.layer_map[tmp_node.name]},0
idx = 0,0
node_count = 1,0
while (idx < len(search_queue)):,0
current_node = search_queue[idx],0
current_type = visited[current_node].type,0
"if hasattr(current_node, 'next_functions'):",0
"for parent, _ in current_node.next_functions:",0
"parent_type = parent.__class__.__name__.replace('Backward', '')",0
if parent_type != 'AccumulateGrad' and \,0
(parent_type != 'Transpose' or current_type != 'Addmm'):,0
if not parent in visited:,0
"tmp_node = PyTorchGraphNode(parent, node_count)",0
self.layer_map[tmp_node.name] = tmp_node,0
node_count += 1,0
visited[parent] = tmp_node,0
search_queue.append(parent),0
"self._make_connection(visited[parent].name, visited[current_node].name)",0
idx += 1,0
try:,0
return TorchGraph._forward_torch_random_input(,0
"torch_model,",0
"input_shapes,",0
is_batch=False,0
),0
except:,0
# try batch mode,0
# return TorchGraph._forward_torch_random_input(,0
"#     torch_model,",0
"#     input_shapes,",0
#     is_batch=True,0
# ),0
pass,0
tensor = torch.rand(*shape).float(),0
multi output,0
single output,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
###########,0
property #,0
###########,0
###################,0
Public Functions #,0
###################,0
Build network graph,0
#########,0
Layers #,0
#########,0
handle weight,0
handle bias,0
handle weight,0
handle bias,0
###################,0
Helper Functions #,0
###################,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
load model file into Coreml Graph,0
model.encode() convert to str --- python2 may crash due to type 'unicode',0
Build Network Graph,0
self.data_format ? TODO,1
load the model network,0
convert to Model_pb2.Model,0
determine the type of the current_node,0
staticmethods,0
"(seq, batch, C, H, W)  & NHWC",0
TODO dtype_map,1
##### convert methods,0
convolution,0
input edge,0
important!,0
reshape the weight!,0
"[2, 3, 0, 1]",0
pads,0
use_bias,0
isDeconvolution,0
"name, op",0
kwargs['kernel_shape'] = weights.shape,0
strides,0
"[1, sd, sh, sw, 1]",0
activation,0
TODO,1
padding in conv,0
"pad in IR is [x1_b, x2_b, ..., x1_e, x2_e, ...]",0
padding,0
compute padding for 'same',0
https://discuss.mxnet.io/t/pooling-and-convolution-with-same-mode/528/3,0
https://www.jianshu.com/p/05c4f1621c7e,0
padding in pooling,0
padding,0
https://discuss.mxnet.io/t/pooling-and-convolution-with-same-mode/528/3,0
TODO,1
symmetric padding,0
"name, op",0
input edge,0
For concat axis,0
"NO axis in coreml, so set the last axis",0
"The first -1 means in coreml there is one-more axis,",0
The second -1 means the last axis,0
"name, op",0
input edge,0
padding,0
Future Module TODO,1
#### rename methods,0
"name, op",0
activation type,0
else:,0
assert False,0
input edge,0
Merge layers,0
only for training,0
"name, op",0
input edge,0
shape,0
NHWC channel last,0
"in fact, here is NWHC",0
"name, op",0
input edge,0
axis TODO,0
"channels_first, then axis = 1",0
scale,0
bias,0
epsilon,0
compute adjusted parameters,0
Reference: parameter transformation https://github.com/apple/coremltools/issues/153,0
convert type because of tensorflow,0
mean,0
var,0
"name, op",0
input edge,0
bias,0
"name, op",0
input edge,0
"MAX = 0, AVERAGE = 1, L2 = 2",0
padding,0
strides,0
"[1, sd, sh, sw, 1]",0
window_shape,0
"[1, pd, ph, pw, 1]",0
"name, op",0
input edge,0
"name, op",0
input edge,0
"name, op",0
input edge,0
units,0
use_bias,0
weights,0
change to single because of the tf matmul,0
"name, op",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
the end of the tensorflow_resnet_v2_50's squeeze shape is [unknown_rank: true] with len 0,0
1001 means the 1001 classes for tensorflow_resnet_v2_50,0
!Alert! TODO,1
Future implement can be changed to the last two layer,0
TODO - remove style transfer 1D hack,1
"Input is 1D but it goes to the width dimension: (1,1,W)",0
"assume (Batch, Channels) - Batch dimension should be dropped",0
"assume (Batch, Sequence-Length, channels)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Add classifier classes (if applicable),0
Set pre-processing paramsters,0
"image_input_names,",0
Return the protobuf spec,0
model = _MLModel(self.builder.spec),0
if IR_node.name in self.weights_dict and 'weights' in self.weights_dict[IR_node.name]:,0
pass,0
"self._emit_merge(IR_node,'DOT')",0
Get input and output names,0
Dimensions and weights,0
"W should have shape (height, width, kernel_channels, output_channels), where kernel_channel = input_channels / groups",0
Dilations,0
see protobuf,0
assert False,0
Get the weights.,0
Dilations,0
see protobuf,0
depth-wise convolution,0
Dilations,0
see protobuf,0
Get input and output names,0
Pooling layer type,0
"if it's global, set the global flag",0
TODO  global pooling modification,0
Padding,0
see protobuf,0
Padding,0
see protobuf,0
Get input and output names,0
"type: ""list(int). A 1-D values of (leftBorder, topBorder, rightBorder, bottomBorder).""",0
This is central crop,0
Now add the layer,0
"Allowed values: 'CHW', 'HW', 'C', 'H', 'W'",0
Get input and output names,0
Get input and output names,0
Get the weights from keras,0
Get input and output names,0
blob_order == 0 if the input blob needs not be rearranged,0
blob_order == 1 if the input blob needs to be rearranged,0
"using keras_layer.input.shape have a ""?"" (Dimension[None] at the front),",0
making a 3D tensor with unknown batch size 4D,0
Get input and output names,0
activation emit,0
Get input and output names,0
negate it,0
apply threshold,0
negate it back,0
for Keras,0
Get input and output names,0
Set parameters,0
"Parameter arrangement in Keras: gamma, beta, mean, variance",0
compute adjusted parameters,0
Reference: parameter transformation https://github.com/apple/coremltools/issues/153,0
padding type TODO,0
"Type of the padding. Can be one of 'constant', 'reflection' or 'replication",0
Now add the layer,0
self.emit_Flatten(IR_node),0
"depth_radius: Half-width of the 1-D normalization window.""",0
Get the weights,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
input layer,0
build each layer,0
"if A.output == B.input, then make the connection: A -> B",0
"print('{0:20}->     {1:20}'.format(layerA.name, layerB.name))",0
"if A.name == B.input, then make the connection: A -> B, here A is the input",0
build each layer,0
"if A.output == B.input, then make the connection: A -> B",0
"print('{0:20}->     {1:20}'.format(layerA.name, layerB.name))",0
"if A.name == B.input, then make the connection: A -> B, here A is the input",0
build each layer,0
"if A.output == B.input, then make the connection: A -> B",0
"print('{0:20}->     {1:20}'.format(layerA.name, layerB.name))",0
"if A.name == B.input, then make the connection: A -> B, here A is the input",0
The information of the layer,0
Generated by the protocol buffer compiler.  DO NOT EDIT!,0
source: caffe.proto,0
@@protoc_insertion_point(imports),0
@@protoc_insertion_point(class_scope:caffe.BlobShape),0
@@protoc_insertion_point(class_scope:caffe.BlobProto),0
@@protoc_insertion_point(class_scope:caffe.BlobProtoVector),0
@@protoc_insertion_point(class_scope:caffe.Datum),0
@@protoc_insertion_point(class_scope:caffe.FillerParameter),0
@@protoc_insertion_point(class_scope:caffe.NetParameter),0
@@protoc_insertion_point(class_scope:caffe.SolverParameter),0
@@protoc_insertion_point(class_scope:caffe.SolverState),0
@@protoc_insertion_point(class_scope:caffe.NetState),0
@@protoc_insertion_point(class_scope:caffe.NetStateRule),0
@@protoc_insertion_point(class_scope:caffe.ParamSpec),0
@@protoc_insertion_point(class_scope:caffe.LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.TransformationParameter),0
@@protoc_insertion_point(class_scope:caffe.LossParameter),0
@@protoc_insertion_point(class_scope:caffe.AccuracyParameter),0
@@protoc_insertion_point(class_scope:caffe.ArgMaxParameter),0
@@protoc_insertion_point(class_scope:caffe.ConcatParameter),0
@@protoc_insertion_point(class_scope:caffe.BatchNormParameter),0
@@protoc_insertion_point(class_scope:caffe.BiasParameter),0
@@protoc_insertion_point(class_scope:caffe.ContrastiveLossParameter),0
@@protoc_insertion_point(class_scope:caffe.ConvolutionParameter),0
@@protoc_insertion_point(class_scope:caffe.CropParameter),0
@@protoc_insertion_point(class_scope:caffe.DataParameter),0
@@protoc_insertion_point(class_scope:caffe.DropoutParameter),0
@@protoc_insertion_point(class_scope:caffe.DummyDataParameter),0
@@protoc_insertion_point(class_scope:caffe.EltwiseParameter),0
@@protoc_insertion_point(class_scope:caffe.ELUParameter),0
@@protoc_insertion_point(class_scope:caffe.EmbedParameter),0
@@protoc_insertion_point(class_scope:caffe.ExpParameter),0
@@protoc_insertion_point(class_scope:caffe.FlattenParameter),0
@@protoc_insertion_point(class_scope:caffe.HDF5DataParameter),0
@@protoc_insertion_point(class_scope:caffe.HDF5OutputParameter),0
@@protoc_insertion_point(class_scope:caffe.HingeLossParameter),0
@@protoc_insertion_point(class_scope:caffe.ImageDataParameter),0
@@protoc_insertion_point(class_scope:caffe.InfogainLossParameter),0
@@protoc_insertion_point(class_scope:caffe.InnerProductParameter),0
@@protoc_insertion_point(class_scope:caffe.InputParameter),0
@@protoc_insertion_point(class_scope:caffe.LogParameter),0
@@protoc_insertion_point(class_scope:caffe.LRNParameter),0
@@protoc_insertion_point(class_scope:caffe.MemoryDataParameter),0
@@protoc_insertion_point(class_scope:caffe.MVNParameter),0
@@protoc_insertion_point(class_scope:caffe.ParameterParameter),0
@@protoc_insertion_point(class_scope:caffe.PoolingParameter),0
@@protoc_insertion_point(class_scope:caffe.PowerParameter),0
@@protoc_insertion_point(class_scope:caffe.PythonParameter),0
@@protoc_insertion_point(class_scope:caffe.RecurrentParameter),0
@@protoc_insertion_point(class_scope:caffe.ReductionParameter),0
@@protoc_insertion_point(class_scope:caffe.ReLUParameter),0
@@protoc_insertion_point(class_scope:caffe.ReshapeParameter),0
@@protoc_insertion_point(class_scope:caffe.ScaleParameter),0
@@protoc_insertion_point(class_scope:caffe.SigmoidParameter),0
@@protoc_insertion_point(class_scope:caffe.SliceParameter),0
@@protoc_insertion_point(class_scope:caffe.SoftmaxParameter),0
@@protoc_insertion_point(class_scope:caffe.TanHParameter),0
@@protoc_insertion_point(class_scope:caffe.TileParameter),0
@@protoc_insertion_point(class_scope:caffe.ThresholdParameter),0
@@protoc_insertion_point(class_scope:caffe.WindowDataParameter),0
@@protoc_insertion_point(class_scope:caffe.SPPParameter),0
@@protoc_insertion_point(class_scope:caffe.V1LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.V0LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.PReLUParameter),0
@@protoc_insertion_point(module_scope),0
"TODO: We need to identify whether this is 4D image data, otherwise we shouldn't change the dimension order",1
print(node.parameters),0
assert False,0
"Stochastic pooling, for instance.",0
TODO: Axis,0
TODO: Unbiased,1
check if need the Flatten layer,0
if parent.output_shape.height > 1 or parent.output_shape.width > 1:,0
TODO: The gamma parameter has to be set (in node.data?) and this should work.,1
"Also, mean should be set to 0, and var to 1, just to be safe.",0
skip the split node,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"For some reason argparser gives us unicode, so we need to conver to str first",0
for test,0
"with open(""graph.txt"", 'w') as f:",0
for layer in self.IR_graph.topological_sort:,0
current_node = self.IR_graph.get_node(layer),0
"print(""========current_node=========\n{}"".format(current_node.layer), file=f)",0
test end,0
"print(""========current_node={}"".format(current_node.layer))",0
check if have pad layer,0
Pad_h < kernel_h (vgg19 caffe2caffe),0
implement asymmetric paddings by applying symmetric padding then cropping,0
keys = [],0
for key in self.weights_dict[IR_node.name].keys():,0
keys.append(key),0
"print(""=======Layer: {}, keys: {}"".format(IR_node.name, keys))",0
"if asymmetric padding, set offset to 1",0
Change the layer name,0
check if need crop output shape,0
"change the key ""name"" to ""variable_name"", in case of the layer name has invalid characters",0
"Convert NHWC (IR) to NCHW (Caffe): [0,1,2,3]->[0,3,1,2]",0
"def emit_Square(self, IR_node):",0
"input_layers = ', '.join(('n.' + self.IR_graph.get_node(edge).real_variable_name) for edge in IR_node.in_edges)",0
"self.add_body(1, ""n.{:<15} = L.Square({}, ntop=1)"".format(",0
"IR_node.variable_name,",0
input_layers)),0
Caffe Types,0
Same value applies to all spatial dimensions,0
Extract the value for the given spatial dimension,0
consider rewrite this function to Network.py,0
Dropout layers appear in a fair number of Caffe,0
test-time networks. These are just ignored. We'll,0
filter them out here.,0
TODO: raise error,1
TODO: raise error,1
Automatically set a name if not provided.,0
Figure out the layer inputs.,0
"print('op: %s   shape: %s' % (op, layer_output._keras_shape))",0
"print('op: %s   shape: %s' % (op, layer_output.get_shape().as_list()))",0
Add to layer LUT.,0
Old-style input specification,0
New-style input specification,0
"We most likely have a data layer on our hands. The problem is,",0
Caffe infers the dimensions of the data from the source (eg: LMDB).,0
We want to avoid reading datasets here. Fail for now.,0
This can be temporarily fixed by transforming the data layer to,0
"Caffe's ""input"" layer (as is usually used in the ""deploy"" version).",0
TODO: Find a better solution for this.,1
The .prototxt file defining the graph,0
The .caffemodel file containing the learned parameters,0
Set to true if the fallback protocol-buffer based backend was used,0
"A list containing (layer name, parameters) tuples",0
Load the parameters,0
"When using the protobuf-backend, each parameter initially has four dimensions.",0
"In certain cases (like FC layers), we want to eliminate the singleton dimensions.",0
"This implementation takes care of the common cases. However, it does leave the",0
potential for future issues.,0
The Caffe-backend does not suffer from this problem.,0
A dictionary mapping NodeKind to the transposed order.,0
The node kinds eligible for reshaping,0
"If true, the reshaped data will replace the old one.",0
"Otherwise, it's set to the reshaped_data attribute.",0
Check for 2+ dimensional data,0
The FC layer connected to the spatial layer needs to be,0
re-wired to match the new spatial ordering.,0
node.reshaped_data = weights.transpose(transpose_order),0
Set the weights,0
We're only fusing nodes with single parents,0
We can only fuse a node if its parent's,0
value isn't used by any other node.,0
Rewrite the fused node's children to its parent.,0
Disconnect the fused node from the graph.,0
Let the sub-class merge the fused node in any arbitrary way.,0
Fuse ReLUs when the parent node is one of the given types.,0
"If None, all node types are eligible.",0
Prescale the stats,0
Replace with the updated values,0
Include the scale and bias terms,0
self.graph = NodeRenamer()(graph),0
"return Graph(name, [self.map_node(node) for node in self.graph.nodes])",0
assert mapped_node is not None,0
skip when mapped_node is None,0
FIXME:,0
output = node.output,0
Decompose DAG into chains,0
Generate Python code line by line,0
try:,0
except ImportError:,0
# # Fall back to the protobuf implementation,0
# from mmdnn.conversion.caffe import caffe_pb2,0
# self.caffepb = caffe_pb2,0
# show_fallback_warning(),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
from tensorflow.core.framework.node_def_pb2 import NodeDef,0
from tensorflow.core.framework import attr_value_pb2,0
pass,0
"print(""\n"")",0
print(i),0
print(block),0
continue,0
NHWC,0
assert False,0
print(shortcut_layer['input'] ),0
spacetodepth,0
print(block),0
print(region_layer),0
assert False,0
print line,0
set default value,0
"yolo3(608) start at 1, yolo2(608) start at 0. yolo2(416) start at 1, yolo3(416) start at 0",0
load weight by original order,0
print(node_type),0
print(IR_node),0
assert False,0
strides,0
assert False,0
padding,0
only load weight conv,0
"print(""----------------"",self.start)",0
print(kernel.shape),0
print(k_bias.shape),0
"buf, start, scale_layer['name'], bn_layer['name'], conv_layer['name']",0
"print(""=============="",self.start)",0
print(bias.shape),0
print(scale.shape),0
print(mean.shape),0
print(var.shape),0
print(kernel.shape),0
print(W),0
assert False,0
no use,0
print(source_node.layer),0
for image classification(resnet) AVG pooling,0
print(source_node.layer),0
print(source_node.layer),0
kwargs['ignore_thresh'] = source_node.get_attr('ignore_thresh'),0
print(source_node.get_attr('anchors')),0
"kwargs['anchors'] = ['0.52','0.22']",0
kwargs['mask'] = source_node.get_attr('mask'),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
tensorflow dump tag,0
Reference: 1) Keras image preprocess: https://github.com/keras-team/keras/blob/master/keras/applications/imagenet_utils.py,0
2) tensorflow github issue: https://github.com/tensorflow/models/issues/517,0
"R-G-B for Imagenet === [123.68, 116.78, 103.94]",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.input, self.model, self.testop = self.MainModel.KitModel(self.args.w)",0
testop = tf.get_default_graph().get_operation_by_name(layer_name),0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result(None, False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
layer_name = 'block2_pool',0
"intermediate_layer_model = keras.Model(inputs=model.input,",0
outputs=model.get_layer(layer_name).output),0
intermediate_output = intermediate_layer_model.predict(img),0
print (intermediate_output),0
print (intermediate_output.shape),0
"print (""%.30f"" % np.sum(intermediate_output))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Load a real image and do default tf imageNet preprocessing,0
"[Batch, Height, Width, Channels] --> [Batch, Channels, Height, Width]",0
We only deal with non-recurrent networks for now,0
"(H,W,C) --> (C,H,W)",0
Load TensorFlow model,0
Prepare inputs,0
Run TF session,0
Evaluate coreml model,0
evaluate the TF model,0
evaluate CoreML,0
Test the default CoreML evaluation,0
Download model,0
Convert to coreml,0
Test predictions on an image,0
Download model,0
Convert to coreml,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
"@unittest.skip(""Failing GPU backend: related to https://github.com/tf-coreml/tf-coreml/issues/26"")",0
? style transfer image size and style number?,0
Test predictions on an image,0
evaluate the TF model,0
evaluate CoreML,0
Test the default CoreML evaluation,0
"compare_tf_coreml_outputs(tf_out_flatten, coreml_out_flatten)",0
#Download model,0
url = 'https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz',0
tf_model_dir = _download_file(url = url),0
"tf_model_path = os.path.join(TMP_MODEL_DIR, 'inception_v3_2016_08_28_frozen.pb')",0
#Convert to coreml,0
"mlmodel_path = os.path.join(TMP_MODEL_DIR, 'inception_v3_2016_08_28.mlmodel')",0
mlmodel = tf_converter.convert(,0
"tf_model_path = tf_model_path,",0
"mlmodel_path = mlmodel_path,",0
"output_feature_names = ['InceptionV3/Predictions/Softmax:0'],",0
"input_name_shape_dict = {'input:0':[1,299,299,3]},",0
"image_input_names = ['input:0'],",0
"red_bias = -1,",0
"green_bias = -1,",0
"blue_bias = -1,",0
image_scale = 2.0/255.0),0
#Test predictions on an image,0
_test_coreml_model_image_input(,0
"tf_model_path = tf_model_path,",0
"coreml_model = mlmodel,",0
"input_tensor_name = 'input:0',",0
"output_tensor_name = 'InceptionV3/Predictions/Softmax:0',",0
img_size = 299),0
Download model,0
Convert to coreml,0
Test predictions on an image,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
self.test_truth(),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
from collections import namedtuple,0
"Batch = namedtuple('Batch', ['data'])",0
TODO,1
Apple has published some of their own models. They can be downloaded from https://developer.apple.com/machine-learning/.,0
"Those published models are: SqueezeNet, Places205-GoogLeNet, ResNet50, Inception v3, VGG16",0
TODO,1
load model,0
TODO: Multiple inputs,1
TODO: Multiple outputs,1
inference,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
caffe uses NCHW,0
"self.print_intermediate_result('pooling0', False)",0
delete tmp model files,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
pylint: disable=line-too-long,0
pylint: enable=line-too-long,0
"net = caffe.Net(arch_fn, weight_fn, caffe.TEST)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
# Image Classification,0
# Semantic Segmentation,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
print(r),0
d = darknet_extractor(),0
model_filename = d.download('yolov3'),0
print(model_filename),0
"image_path = ""./mmdnn/conversion/examples/data/dog.jpg""",0
"model_path = ""./""",0
d = darknet_extractor(),0
"result = d.inference('yolov3', model_filename, model_path, image_path = image_path)",0
print(result),0
print(i),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result('pooling0', False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
need to be updated,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.input, self.model, self.testop = self.MainModel.KitModel(self.args.w)",0
testop = tf.get_default_graph().get_operation_by_name(layer_name),0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"'mobilenet_v1': mobilenet_v1.mobilenet_v1_arg_scope,",0
"'mobilenet_v1' : mobilenet_v1.mobilenet_v1,",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
raise NotImplementedError(),0
load file,0
Using cuDNN since vanilla RNN,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Notes for training NASNet Cifar Model,0
-------------------------------------,0
batch_size: 32,0
learning rate: 0.025,0
cosine (single period) learning rate decay,0
auxiliary head loss weighting: 0.4,0
clip global norm of all gradients by 5,0
600 epochs with a batch size of 32,0
This is used for the drop path probabilities since it needs to increase,0
the drop out probability over the course of training.,0
Notes for training large NASNet model on ImageNet,0
-------------------------------------,0
batch size (per replica): 16,0
learning rate: 0.015 * 100,0
learning rate decay factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 100 replicas,0
auxiliary head loss weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Notes for training the mobile NASNet ImageNet model,0
-------------------------------------,0
batch size (per replica): 32,0
learning rate: 0.04 * 50,0
learning rate scaling factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 50 replicas,0
auxiliary head weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Shape of feature map before the final layer.,0
149 x 149 x 32,0
Run the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Find where to place the reduction cells or stride normal cells,0
Setup for building in the auxiliary head.,0
Run the cells,0
true_cell_num accounts for the stem cells,0
Final softmax layer,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80,0
71 x 71 x 192,0
35 x 35 x 192,0
35 x 35 x 320,0
TODO(alemi): Register intermediate endpoints,1
"17 x 17 x 1088 if output_stride == 8,",0
33 x 33 x 1088 if output_stride == 16,0
TODO(alemi): register intermediate endpoints,1
TODO(gpapan): Properly support output_stride for the rest of the net.,1
8 x 8 x 2080,0
TODO(alemi): register intermediate endpoints,1
8 x 8 x 1536,0
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which",1
can be set to False to disable pooling here (as in resnet_*()).,0
Set weight_decay for weights in conv2d and fully_connected layers.,0
Set activation_fn and parameters for batch_norm.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
=============================================================================,0
Tensorflow mandates these.,0
Conv and DepthSepConv namedtuple define layers of the MobileNet architecture,0
Conv defines 3x3 convolution layers,0
DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.,0
stride is the stride of the convolution,0
depth is the number of channels or filters in a layer,0
_CONV_DEFS specifies the MobileNet body,0
Used to find thinned depths for each layer.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
By passing filters=None,0
separable_conv2d produces only a depthwise convolution layer,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 1024,0
Set weight_decay for weights in Conv and DepthSepConv layers.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Skip path 1,0
Skip path 2,0
"First pad with 0's on the right and bottom, then shift the filter to",0
include those 0's that were added.,0
Concat and apply BN,0
Set the prev layer to the current layer if it is none,0
Check to be sure prev layer stuff is setup correctly,0
Apply conv operations,0
Combine hidden states using 'add'.,0
Add hiddenstate to the list of hiddenstates we can choose from,0
Dont stride if this is not one of the original hiddenstates,0
"Check if a stride is needed, then use a strided 1x1 here",0
Determine if a reduction should be applied to make the number of,0
filters match.,0
Return the concat of all the states,0
Scale keep prob by layer number,0
The added 2 is for the reduction cells,0
Decrease the keep probability over time,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Inception-Resnet-A,0
Inception-Resnet-B,0
Inception-Resnet-C,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
force in-place updates of mean and variance estimates,0
Moving averages ends up in the trainable variables collection,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80,0
71 x 71 x 192,0
35 x 35 x 256,0
5 x Inception-resnet-A,0
Reduction-A,0
10 x Inception-Resnet-B,0
Reduction-B,0
5 x Inception-Resnet-C,0
pylint: disable=no-member,0
"net = slim.fully_connected(net, bottleneck_layer_size, activation_fn=None,",0
"scope='Bottleneck', reuse=False)",0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
Only insert rate to params if rate > 1.,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
import self.model,0
self.model,0
how the model can not load from `***.bin`,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
pylint: disable=line-too-long,0
pylint: enable=line-too-long,0
refer to https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/v2/tests/test_rnn_layer.py#L35,0
refer to https://github.com/PaddlePaddle/Paddle/issues/7403,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
refer to https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/v2/tests/test_rnn_layer.py#L35,0
refer to https://github.com/PaddlePaddle/Paddle/issues/7403,0
the channel of input feature is 3,0
"depth should be one of 20, 32, 44, 56, 110, 1202",0
Copyright (c) Microsoft. All rights reserved.,0
Licensed under the MIT license. See LICENSE.md file in the project root,0
for full license information.,0
==============================================================================,0
"self.model, self.testop = self.MainModel.KitModel(self.args.w)",0
"self.print_intermediate_result(None, False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
pylint: disable=line-too-long,0
pylint: enable=line-too-long,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.anchors = np.array([[10,13], [16,30],[33,23],[30,61],[62,45], [59,119],[116,90],[156,198],[373,326]])",0
print(output),0
"print(out_boxes, out_scores, out_classes)",0
get random colors,0
My kingdom for a good redistributable image drawing library.,0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
yolov3 80 classes,0
"Reshape to batch, height, width, num_anchors, box_params.",0
Adjust preditions to each spatial grid point and anchor size.,0
Note: YOLO iterates over height index before width index.,0
"TODO: It works with +1, don't know why.",1
Scale boxes back to original image shape.,0
"print(""feats,anchors, num_classes, input_shape"", feats, anchors, num_classes, input_shape)",0
"print(box_xy, box_wh, box_confidence, box_class_probs)",0
"yolo_outputs order 13,26,52",0
TODO: use keras backend instead of tf.,1
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
save network structure as JSON,0
layer_name = 'block2_pool',0
"intermediate_layer_model = keras.Model(inputs=model.input,",0
outputs=model.get_layer(layer_name).output),0
intermediate_output = intermediate_layer_model.predict(img),0
print (intermediate_output),0
print (intermediate_output.shape),0
"print (""%.30f"" % np.sum(intermediate_output))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Convert network architecture only,0
MXNet,0
Caffe,0
For CoreML,0
Caffe,0
Darknet,0
"exts = ['.pb', '.npy', '.py']",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check non-sense always input module.Module,0
if not (type(model) == mx.module.Module,0
or type(model) == mx.module.SequentialModule,0
or type(model) == mx.model),0
"raise TypeError(""MXNet layer of type %s is not supported."" % type(model))",0
"if layer[""op""] == ""null"":",0
continue,0
"raise NotImplementedError(""Cannot support multi-input"")",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"mxnet-cpu only support channel first, default convert the model and weight as channel first",0
We use the real_name for specifying the input layer in data_names,0
since MXNet API wants the actual name of the layer. On the other,0
"hand, the module API wants the last symbol in the symbol chain, so",0
for the output node we need to use the actual python variable name,0
of the last layer (real_variable_name).,0
download and show the image,0
"convert into format (batch, RGB, width, height)",0
"to show the image, change the argument show into True",0
compute the predict probabilities,0
print the top-5,0
# call function predict,0
"with open('synset.txt', 'r') as f:",0
labels = [l.rstrip() for l in f],0
"predict(model, labels, 'http://writm.com/wp-content/uploads/2016/08/Cat-hd-wallpapers.jpg')",0
"print(""Warning: MXNet Convolution Layer pad does not match IR Convolution Layer pad"")",0
"layout = IR_node.IR_layer.attr[""data_format""].s",0
if layout not in MXNetEmitter.channels_last:,0
Add LeakyReLU Elu(slope not support),0
"axis = IR_node.IR_layer.attr[""axis""].i",0
not supported yet,0
"axis = IR_node.IR_layer.attr[""axis""].i",0
not supported yet,0
"axis = IR_node.IR_layer.attr[""axis""].i",0
not supported yet,0
"print(""Warning: MXNet Pooling Layer pad does not match IR Pooling Layer pad"")",0
"def emit_ConvTranspose(self, IR_node):",0
if self.weight_loaded:,0
weight_dict = self.weights[IR_node.name],0
weights = weight_dict['weights'],0
"dim = len(IR_node.IR_layer.attr[""kernel_shape""].list.i) - 2",0
kernel = list(),0
"for idx in range(0, dim):",0
"kernel.append(IR_node.IR_layer.attr[""kernel_shape""].list.i[idx])",0
stride = list(),0
"for e in IR_node.IR_layer.attr[""strides""].list.i[1:-1]:",0
stride.append(e),0
dilate = list(),0
"for e in IR_node.IR_layer.attr[""dilations""].list.i[1:-1]:",0
dilate.append(e),0
"dilate = ', '.join('%s' % i for i in dilate)",0
defuse_pad = False,0
pad = list(),0
"if ""pads"" in IR_node.IR_layer.attr:",0
output_shape = list(),0
"for e in IR_node.IR_layer.attr[""_output_shapes""].list.shape[0].dim:",0
output_shape.append(e.size),0
"# print(""Warning: MXNet Deconvolution Layer pad does not match IR Deconvolution Layer pad"")",0
"defuse_pad, pad = MXNetEmitter.transfer_pad(IR_node.IR_layer.attr[""pads""].list.i)",0
"pad = ', '.join('%s' % i for i in pad)",0
"kernel = ', '.join('%s' % i for i in kernel)",0
"stride = ', '.join('%s' % i for i in stride)",0
"num_filter = IR_node.IR_layer.attr[""kernel_shape""].list.i[-2]",0
"no_bias = not IR_node.IR_layer.attr[""use_bias""].b",0
if not no_bias and self.weight_loaded:,0
"self.output_weights[IR_node.replace_scope(IR_node.name) + ""_bias""] = weight_dict['bias']",0
"# layout = IR_node.IR_layer.attr[""data_format""].s",0
if dim == 1:,0
layout = 'NCW',0
elif dim == 2:,0
layout = 'NCHW',0
elif dim == 3:,0
layout = 'NCDHW',0
if self.weight_loaded:,0
# if layout not in MXNetEmitter.channels_last:,0
"weights = MXNetEmitter.transpose(weights, dim)",0
"self.output_weights[IR_node.replace_scope(IR_node.name) + ""_weight""] = weights",0
"code = """"",0
if not defuse_pad:,0
"code = ""{:<15} = mx.sym.Deconvolution(data = {}, kernel = ({}), stride = ({}), dilate = ({}), pad = ({}), num_filter = {}, no_bias = {}, layout = '{}', name = '{}')"".format(",0
"IR_node.replace_scope(IR_node.name),",0
"IR_node.replace_scope(IR_node.in_edges[0]),",0
"kernel,",0
"stride,",0
"dilate,",0
"pad,",0
"num_filter,",0
"no_bias,",0
"layout,",0
IR_node.replace_scope(IR_node.name)),0
else:,0
"code = self.set_pad(IR_node, code, pad)",0
"code += ""\n    {:<15} = mx.sym.Deconvolution(data = {}, kernel = ({}), stride = ({}), dilate = ({}), num_filter = {}, no_bias = {}, layout = '{}', name = '{}')"".format(",0
"IR_node.replace_scope(IR_node.name), IR_node.replace_scope(IR_node.name) + ""_pad"", kernel, stride, dilate, num_filter, no_bias, layout, IR_node.replace_scope(IR_node.name))",0
return code,0
reverse cannot support yet,0
"code = ""{:<15} = mx.sym.transpose(data = {}, axes = (0, 2, 3, 1))\n"".format(""trans"", self.parent_variable_name(IR_node))",0
save the constant into weight dict,0
param_code does not need parameter slice.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Not support yet,0
"""softrelu""  : ""SoftReLU""",0
Load the model network and weights,0
"digraph = mx.viz.plot_network(sym, save_format='jpg') # For debugging",0
digraph.render(),0
model.bind(data_shapes = data_shapes),0
model.init_params(),0
"mod.load(model_path, epoch_num)",0
return mod.get_params(),0
load the model network,0
adjust the data format,0
load model files into MXNet graph,0
data_shape arguments added to calculate infer_shape(required),0
"if isinstance(input_arg, basestring):",0
Build network graph,0
transpose to channel last,0
"name, op",0
input edge,0
output shape,0
input edge,0
attr,0
"print(""Warning: MXNet symbol pad does not support channel last"")",0
"IR_node.attr[""pads""].list.i.extend([0, 0])",0
for e in pad:,0
"IR_node.attr[""pads""].list.i.extend([e, e])",0
"IR_node.attr[""pads""].list.i.extend([0, 0])",0
"name, op",0
input edge,0
units,0
use bias (no_bias default = False),0
weights,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [NM]"".format(source_node.name, self.data_format))",0
input edge,0
output shape,0
kernel_shape,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [{}]"".format(source_node.name, self.data_format, layout))",0
groups,0
in_channel = in_channel // group,0
use_bias (no_bias default = False),0
strides,0
dilations,0
data_format,0
padding,0
weights,0
axis,0
scale,0
epsilon,0
momentum,0
weights,0
gamma,0
beta,0
mean,0
var,0
"name, op",0
input edge,0
pooling type (sum not allowed yet),0
strides,0
kernel_shape,0
padding,0
output shape,0
"name, op",0
input edge,0
output shape,0
dim,0
"def rename_log_softmax(self, source_node):",0
"raise NotImplementedError(""not support yet"")",0
"def rename_Correlation(self, source_node):",0
"raise NotImplementedError(""not support yet"")",0
"name, op",0
input edge,0
padding,0
output shape,0
kernel_shape,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [{}]"".format(source_node.name, self.data_format, layout))",0
use_bias (no_bias default = False),0
strides,0
dilations,0
data_format,0
groups,0
weights,0
"def rename_RNN(self, source_node):",0
"raise NotImplementedError(""RNN not support yet"")",0
"name, op",0
input edge,0
input_dim,0
output_dim,0
dtype,0
output shape,0
"IR only support elu and prelu from {'elu', 'leaky', 'prelu', 'rrelu'}",0
return,0
"name, op",0
gamma,0
input edge,0
"alpha [exp(x) - alpha], but mxnet attr slope [slope*(exp(x) - 1)] when x < 0",0
"IR_node.attr[""alpha""].f = float()",0
output shape,0
"raise NotImplementedError(""slope cannot convert to alpha"")",0
"def rename_InstanceNorm(self, source_node):",0
raise NotImplementedError,0
"def rename_L2Normalization(self, source_node):",0
raise NotImplementedError,0
alpha,0
beta,0
knorm,0
nsize,0
keep_prob,0
mode,0
reverse cannot support yet,0
old API target_shape not support yet,0
output shape,0
"raise NotImplementedError(""adjust output shape"")",0
dim,0
dtype,0
output shape,0
"name, op",0
input edge,0
output shape,0
axis,0
"raise NotImplementedError(""No matching IR api"")",0
"left strip the ""_"" at the beginning of the name",0
"Issue #85, #135",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
keep dims,0
axes,0
"name, op",0
epsilon,0
moving variance (var),0
print(moving_variance.name),0
gamma (scale),0
mean,0
bias,0
input node,0
output node,0
load model files into TensorFlow graph,0
extract subgraph using in_nodes and dest_nodes,0
Build network graph,0
extract subgraph using dest_nodes,0
Graph Transform,0
Get input node name,0
TODO: Fused conv and pool with padding is different from defused operators,0
strides,0
window_shape,0
pool type,0
padding,0
shape,0
strides,0
input[1] : W,0
filter,0
padding,0
output[0] : B,0
converted [dropout],0
convert [tf.contrib.layers.batch_norm],0
normal Add,0
units,0
Weights,0
FullyConnected Layer,0
"name, op",0
get Bias,0
Matmul Layer,0
Deal Dropout,0
keep prob,0
Remove nodes,0
Mul,0
Floor,0
print (source_node),0
print (source_node.layer),0
assert False,0
"def rename_RandomShuffleQueueV2(self, source_node):",0
# print(source_node.layer),0
"IR_node = self._convert_identity_operation(source_node, in_edge_count = 0, new_op = 'DataInput')",0
# IR_node.attr['shape'].shape.MergeFromString(source_node.layer.attr['_output_shapes'].list.shape[0].SerializeToString()),0
# IR_node.attr['shape'].shape.dim[0].size = -1,0
IR_node.attr['dtype'].type = self.dtype_map[source_node.layer.attr['component_types'].list.type[0]],0
paddings,0
gamma (scale),0
bias,0
Mean,0
Var,0
TODO: Current it is only for slice,1
TODO:  only for 1D,1
strides,0
input[1] : W,0
filter,0
padding,0
output[0] : B,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
check whether flatten operator should be added,0
flatten is needed,0
"self._emit_unary_operation(IR_node, ""contrib.layers.flatten"")",0
param_code does not need parameter slice.,0
skip_prefix = [,0
"""^"",",0
"""train_op"",",0
"""save"",",0
"""gradients"",",0
"""init"",",0
"""global_step"",",0
"""distort_image"",",0
"""Adagrad"",",0
],0
"""Switch""",0
"""RequantizationRange"",",0
"""Requantize"",",0
"""Identity"",",0
"""Mean"",",0
"""Cast""",0
load model files into TensorFlow graph,0
Save it to an output file,0
keep dims,0
axes,0
ssd model is transformed,0
Ax - (Au - b),0
A,0
b,0
print(sub_content),0
epsilon,0
moving variance (var) /read,0
gamma (scale),0
print(Rsqrt.out_edges),0
beta  (bias),0
moving mean (mean),0
input node,0
epsilon,0
beta,0
gamma (scale),0
"output_node = self.get_son(source_node.name, [0, 0, 0, 0], True)",0
assert False,0
Quantized model type,0
TODO: Fused conv and pool with padding is different from defused operators,0
assert variable.get_attr('_output_shapes')[0].dim[0].size == IR_node.attr['kernel_shape'].list.i[-1],0
strides,0
window_shape,0
pool type,0
padding,0
"In facenet or other newtwork using slim.batch_norm,",0
"There are two BN(train, test) skip switch and merge.",0
epsilon,0
moving variance (var) /read,0
gamma (scale),0
beta  (bias),0
moving mean (mean),0
ReduceMean,0
keep dims,0
axes,0
Skip the node as merge,0
weights,0
alpha,0
beta,0
Skip BiasAdd,0
weights,0
"input_node_perm = self.check_const(self.get_parent(source_node.name, [1], True))",0
paddings,0
for attr.shape >= 2,0
"For models built by slim.batch_norm, remove duplicate BN (eg.facenet)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Build network graph,0
"PaddleParser._set_output_shape(source_node, IR_node)",0
"name, op",0
input edge,0
For concat axis,0
only for training,0
"name, op",0
input edge,0
input edge,0
layer and spec,0
width <=> x or height <=> y,0
output shape,0
"name, op",0
it should be in the shape of height x width x inputchannel x outputchannel,0
use_bias: TODO,1
pad_dim,0
fail report because of auto_pad,0
if dilation_x == 1 and dilation_y == 1:,0
if output_x * stride_x == input_x and output_y * stride_y == input_y:,0
"auto_pad = ""SAME""",0
kwargs['auto_pad'] = auto_pad,0
elif output_x * stride_x == input_x - width + 1 and output_y * stride_y == input_y - height + 1:,0
"auto_pad = ""VALID""",0
kwargs['auto_pad'] = auto_pad,0
defuse the activation layer,0
"name, op",0
input edge,0
layer and spec,0
output shape,0
"channels_first, then axis = 1",0
epsilon,0
compute adjusted parameters,0
Reference: parameter transformation https://github.com/apple/coremltools/issues/153,0
convert type because of tensorflow,0
flatten,0
mean,0
var,0
defuse the activation layer,0
"name, op",0
input edge,0
layer and spec,0
assert False,0
output shape,0
pad_dim,0
padding mode,0
"If padding == ""SAME"": output_spatial_shape[i] = ceil(input_spatial_shape[i] / strides[i])",0
"If padding == ""VALID"": output_spatial_shape[i] = ceil((input_spatial_shape[i] - (spatial_filter_shape[i]-1) * dilation_rate[i]) / strides[i]).",0
"name, op",0
input edge,0
layer and spec,0
units,0
output shape,0
use_bias,0
Kit weight tranpose,0
weight: N x M -> C x H x W x M -> H x W x C x M -> N x M,0
,0
weights,0
need the shape TODO,1
only for training,0
"name, op",0
input edge,0
shape,0
"name, op",0
input edge,0
layer and spec,0
output shape,0
alpha,0
beta,0
nsize,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Build network graph,0
strides,0
window_shape,0
pool type,0
padding,0
TODO: Handle square,1
self._print_layer(source_node),0
print (source_node.name),0
"print (self.src_graph.get_parent(source_node.name, [0]).real_name)",0
assert False,0
Convolution,0
Kernel,0
Attributes,0
Bias,0
Activation,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
BlockFunction node,0
redirect the composite's inputs to the true inputs,0
"BlockFunctions are short-circuited, and not added to accum[]",0
Function node,0
OutputVariable node,0
def build(self):,0
"_traverse_graph(self, self.model.root_function)",0
"super(CntkGraph, self).build()",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"IR_node.get_attr('output_dim'),",0
??,0
print(IR_node.layer),0
assert False,0
"def emit_Unstack(self, IR_node):",0
"num_str = ""{}.shape[{}]"".format(self.parent_variable_name(IR_node), IR_node.get_attr('axis'))",0
axis = IR_node.get_attr('axis'),0
"parent_variable_shape = ""list({}.shape)"".format(self.parent_variable_name(IR_node)",0
"if self.IR_graph.get_parent(IR_node.name, [0]).type != 'Embedding'",0
else self.parent_variable_name(IR_node)+'.E'),0
if axis==1:,0
"shape_str = ""tuple([{}[0]*{}[{}], 1].extend({}[{}+1:]))"".format(",0
"parent_variable_shape,",0
"parent_variable_shape,",0
"str(axis),",0
"parent_variable_shape,",0
str(axis)),0
else:,0
"shape_str = ""tuple([{}[0]*{}[{}]].extend({}[1:{}]).append(1).extend({}[{}+1:]))"".format(",0
"parent_variable_shape,",0
"parent_variable_shape,",0
"str(axis),",0
"parent_variable_shape,",0
"str(axis),",0
"parent_variable_shape,",0
str(axis)),0
"code = ""{:<15} = cntk.reshape({}, {}, name='{}')"".format(",0
"IR_node.variable_name,",0
"self.parent_variable_name(IR_node),",0
"shape_str,",0
IR_node.variable_name),0
"code = ""{: <15} = cntk.reshape({}, {}.shape, name='{}')"".format(",0
"IR_node.variable_name,",0
"self.parent_variable_name(IR_node),",0
"self.parent_variable_name(IR_node),",0
IR_node.name,0
),0
return code,0
"def emit_Fill(self, IR_node):",0
"code = ""{:<15} = cntk.Constant({}, {}, name='{}')"".format(",0
"IR_node.variable_name,",0
"IR_node.get_attr('value'),",0
"self.parent_variable_name(IR_node),",0
IR_node.name),0
return code,0
param_code does not need parameter slice.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
For padding,0
print (input_shape),0
print (kernel_shape),0
print (strides),0
Not tested,0
print (input_shape),0
print (kernel_shape),0
print (strides),0
kernel_shape[idx] = (kernel_shape[idx] - 1) * dilation_rate + 1,0
print ([0] + up_list + [0] + down_list if data_format.startswith('NC') else up_list + [0] + down_list + [0]),0
print ('-----------------------------------------------------'),0
network library,0
not support multi-threads download,0
key: layer_name    value: keras layer,0
private functions,0
next_node_info.left_in_edges -= 1,0
"print ('{} --> {}'.format(src, dst))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
share functions,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
name --> (weight_name --> ndarray),0
Generated by the protocol buffer compiler.  DO NOT EDIT!,0
source: graph.proto,0
@@protoc_insertion_point(imports),0
@@protoc_insertion_point(class_scope:GraphDef),0
@@protoc_insertion_point(class_scope:NodeDef.AttrEntry),0
@@protoc_insertion_point(class_scope:NodeDef),0
@@protoc_insertion_point(class_scope:AttrValue.ListValue),0
@@protoc_insertion_point(class_scope:AttrValue),0
@@protoc_insertion_point(class_scope:TensorShape.Dim),0
@@protoc_insertion_point(class_scope:TensorShape),0
@@protoc_insertion_point(class_scope:LiteralTensor),0
@@protoc_insertion_point(module_scope),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
First try to read it as a binary file.,0
Next try to read it as a text file.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"If pattern.inputs is empty, skips the rest and accepts all the inputs.",0
"If order doesn't matter for the inputs, then make sure we match at least",0
one permutation of the inputs.,0
"def get_tensor(self, pattern_or_name):",0
op_tensor = self._get_op_tensor(pattern_or_name),0
return op_tensor[1] if op_tensor else None,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
"Python 3.3.2+ implements `yield from`, but for now:",0
"check the same pattern scope node whether have same inputs, outputs and weights.",0
"For those don't have, rename their scope names.",0
"clear out scope node, typically input constant node.",0
get sub_scopes,0
decline the suffix number,0
Obtain nodes where the scope name that satisfies top_level is top_scope and sub_level is sub_scope,0
Obtain nodes where the scope name that satisfies in sub_level is sub_scope,0
Obtain nodes where the scope name that satisfies in top_level is top_scope,0
Obtain all nodes grouped by sub_level sub_scope,0
cover the node,0
"store idx, node into a dict and sort it later to keep its topology sort.",0
in_node's out edges replace n_name with scope node name.,0
no out nodes means the last node in scope nodes should be returned,0
"if out_node is scope node, replace the scope node's inner topology list node.",0
the input parameter shoule be sliced when call func.,0
modify the in_edges in scope inner nodes. decline the :idx.,0
1. initilize scope node,0
2. get scope nodes' topology list.,0
3. rebuild the edges connection after folding these scope nodes into one node and,0
get this scope node's return variables.,0
4. rebuild graph.,0
RNN-related attrs.,0
get input params,0
self.store_const_to_top(result),0
"self.set_top_node_prop(result, pattern_name)",0
Do not include input op.,0
"TODO: pytorch, mxnet, keras, cntk",1
-*- coding: utf-8 -*-,0
from keras.layers.core import Layer,0
Arguments,0
Input shape,0
Output shape,0
Prepare broadcasting shape.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Load the model network,0
Load the model weights,0
load model files into Keras graph,0
Keras 2.1.6,0
Keras. 2.2.2,0
"_keras.utils.plot_model(model, ""model.png"", show_shapes = True)",0
Build network graph,0
TODO: Fused conv and pool with padding is different from defused operators,0
TODO: More activation functions,1
for ELU,0
input edge,0
"name, op",0
weights,0
pads,0
filter,0
"[kd, kh, kw, channel_size, filter number]",0
use_bias,0
strides,0
"[1, sd, sh, sw, 1]",0
dilations,0
"[1, dd, dh, dw, 1]",0
activation,0
"name, op",0
input edge,0
padding,0
strides,0
"[1, sd, sh, sw, 1]",0
window_shape,0
"[1, pd, ph, pw, 1]",0
"name, op",0
input edge,0
For concat axis,0
"name, op",0
input edge,0
padding,0
only for training,0
"name, op",0
input edge,0
"name, op",0
input edge,0
Merge Layers,0
only for training,0
"name, op",0
input edge,0
shape,0
only for training,0
"name, op",0
input edge,0
Core Layers,0
"name, op",0
input edge,0
units,0
use_bias,0
weights,0
activation,0
"name, op",0
input edge,0
"name, op",0
input edge,0
size,0
"name, op",0
input edge,0
input_dim,0
output_dim,0
mask_zero,0
weights,0
"name, op",0
input edge,0
units,0
use_bias,0
"for Keras, drop_out and recurrent_dropout",0
activation,0
"name, op",0
input edge,0
units,0
activation,0
weights,0
Kit TODO : need to search the tf,1
"name, op",0
input edge,0
for target shape,0
"name, op",0
input edge,0
axis,0
"Parameter arrangement in Keras: gamma, beta, mean, variance",0
scale,0
beta,0
mean,0
var,0
"name, op",0
input edge,0
for target shape,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
rot weights,0
"print(""Converting layer {}({})"".format(current_node.name, node_type))",0
TODO,1
"return input_node, 'same'",0
"assert IR_node.get_attr('group', 1) == 1",0
change dw from filters to 1,0
reset the default dilation,0
############,0
Operators #,0
############,0
"code = self._emit_merge(IR_node, ""subtract"")",0
TODO,1
"when converting from coreml model, reshape is needed after the global pooling",0
for Keras,0
It arouses some problems:,0
it can be implemented by Lambda Layer,0
https://github.com/keras-team/keras/issues/890,0
Keras == 2.1.6,0
Keras == 2.2.2,0
TODO: arguments won't be saved in keras export model,1
param_code does not need parameter slice.,0
Prepare broadcasting shape.,0
"output = Lambda(lambda x: tf.fill(x, value))(input)",0
return output,0
def _layer_Constant(self):,0
"self.add_body(0, '''",0
class my_constant(keras.layers.Layer):,0
"def __init__(self, value, **kwargs):",0
"super(my_constant, self).__init__(**kwargs)",0
self._value = value,0
"# the input is dummy, just for creating keras graph.",0
"def call(self, dummy):",0
res = K.constant(self._value),0
self.output_shapes = K.int_shape(res),0
return res,0
"def compute_output_shape(self, input_shape):",0
return self.output_shapes,0
'''),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
Kit: TODO,1
Duplicate models for weight sharing,0
Expand the sub-models,0
"with open('modelmapbydataset.json', 'w') as outfile:",0
"json.dump(new_data, outfile)",0
generate makedown script,0
add Image Classification,0
add Object Detection,0
MMdnn introduction,0
steps for model conversion,0
Generate model converter description,0
Generate models list,0
generate makedown script,0
MMdnn introduction,0
steps for model conversion,0
Generate model converter description,0
Generate models list,0
console.info(JSON.parse(e.target.result)),0
"default, draw the model.json file",0
window.onload = () => {,0
"d3.json(filePath, (error, json) => {",0
if (error) throw error;,0
// console.info(json),0
draw(json),0
}),0
},0
generate dag,0
IR model or keras model,0
path generator,0
let arrowL = `M ${points[len - 1].x} ${points[len - 1].y} l -7 -7`,0
let arrowR = `M ${points[len - 1].x} ${points[len - 1].y} l  7 -7`,0
select a layer,0
remove list,0
draw,0
mode;,0
"let scale = Math.min(window.innerWidth*0.85 / width, (window.innerHeight-85)/height)",0
"var filter = def.append(""filter"")",0
".attr(""id"", ""dropshadow"")",0
"filter.append(""feGaussianBlur"")",0
".attr(""in"", ""SourceAlpha"")",0
".attr(""stdDeviation"", 4)",0
".attr(""result"", ""blur"");",0
"// filter.append(""feOffset"")",0
"//     .attr(""in"", ""blur"")",0
"//     .attr(""dx"", 2)",0
"//     .attr(""dy"", 2)",0
"//     .attr(""result"", ""offsetBlur"");",0
"var feMerge = filter.append(""feMerge"");",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""offsetBlur"")",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""SourceGraphic"");",0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
"svg.on('keydown', ()=>{console.info('ddd')})",0
".on('mouseover', ()=>{console.info('mouse over')})",0
k = parseFloat(k)*parseFloat(k_),0
"limit x, y",0
// a trick to make text svg transform in MS Edge,0
"d3.selectAll("".labels"").classed(""tempclass"", true);",0
"setTimeout(function () { d3.selectAll("".labels"").classed(""tempclass"", false); }, 40);",0
when shift is down,0
ulti function,0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
rm the '_',0
Base64 encode: https://developers.google.com/protocol-buffers/docs/proto3,0
Search the node,0
select by id: https://stackoverflow.com/questions/37270787/uncaught-syntaxerror-failed-to-execute-queryselector-on-document,0
Scroll,0
console.info(JSON.parse(e.target.result)),0
"default, draw the model.json file",0
window.onload = () => {,0
"d3.json(filePath, (error, json) => {",0
if (error) throw error;,0
// console.info(json),0
draw(json),0
}),0
},0
generate dag,0
IR model or keras model,0
path generator,0
let arrowL = `M ${points[len - 1].x} ${points[len - 1].y} l -7 -7`,0
let arrowR = `M ${points[len - 1].x} ${points[len - 1].y} l  7 -7`,0
select a layer,0
draw,0
"let scale = Math.min(window.innerWidth*0.85 / width, (window.innerHeight-85)/height)",0
"var filter = def.append(""filter"")",0
".attr(""id"", ""dropshadow"")",0
"filter.append(""feGaussianBlur"")",0
".attr(""in"", ""SourceAlpha"")",0
".attr(""stdDeviation"", 4)",0
".attr(""result"", ""blur"");",0
"// filter.append(""feOffset"")",0
"//     .attr(""in"", ""blur"")",0
"//     .attr(""dx"", 2)",0
"//     .attr(""dy"", 2)",0
"//     .attr(""result"", ""offsetBlur"");",0
"var feMerge = filter.append(""feMerge"");",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""offsetBlur"")",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""SourceGraphic"");",0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
"svg.on('keydown', ()=>{console.info('ddd')})",0
".on('mouseover', ()=>{console.info('mouse over')})",0
k = parseFloat(k)*parseFloat(k_),0
"limit x, y",0
// a trick to make text svg transform in MS Edge,0
"d3.selectAll("".labels"").classed(""tempclass"", true);",0
"setTimeout(function () { d3.selectAll("".labels"").classed(""tempclass"", false); }, 40);",0
when shift is down,0
ulti function,0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
space = '',0
Get the long description from the README file,0
Versions should comply with PEP440.  For a discussion on single-sourcing,0
"the version across setup.py and the project code, see",0
https://packaging.python.org/en/latest/single_source_version.html,0
The project's main homepage.,0
Author details,0
Choose your license,0
See https://pypi.python.org/pypi?%3Aaction=list_classifiers,0
How mature is this project? Common values are,0
3 - Alpha,0
4 - Beta,0
5 - Production/Stable,0
Indicate who your project is intended for,0
"Pick your license as you wish (should match ""license"" above)",0
"Specify the Python versions you support here. In particular, ensure",0
"that you indicate whether you support Python 2, Python 3 or both.",0
What does your project relate to?,0
You can just specify the packages manually here if your project is,0
simple. Or you can use find_packages().,0
"Alternatively, if you want to distribute just a my_module.py, uncomment",0
this:,0
"py_modules=[""my_module""],",0
List run-time dependencies here.  These will be installed by pip when,0
"your project is installed. For an analysis of ""install_requires"" vs pip's",0
requirements files see:,0
https://packaging.python.org/en/latest/requirements.html,0
"To provide executable scripts, use entry points in preference to the",0
"""scripts"" keyword. Entry points provide cross-platform support and allow",0
pip to create the appropriate form of executable for the target platform.,0
"Function self.assertEquals has deprecated, change to assertEqual",0
"self.assertEqual(original_predict.shape, converted_predict.shape)",0
get original model prediction result,0
original to IR,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
print(original_predict),0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
get shape,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
IR to code,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
The list is in RGB oder,0
IR to Model,0
"converted_file = original_framework + '_coreml_' + architecture_name + ""_converted""",0
"converted_file = converted_file.replace('.', '_')",0
load model,0
save model,0
"coremltools.utils.save_spec(model.get_spec(), converted_file)",0
inference,0
IR to code,0
import converted model,0
"In case of odd number add the extra padding at the end for SAME_UPPER(eg. pads:[0, 2, 2, 0, 0, 3, 3, 0]) and at the beginning for SAME_LOWER(eg. pads:[0, 3, 3, 0, 0, 2, 2, 0])",0
"'alexnet'       : [CntkEmit, KerasEmit, TensorflowEmit],",0
"'inception_resnet_v2' : [TensorflowEmit], # TODO PytorchEmit",1
get original model prediction result,0
do not deal,0
I don't want to deal with auto_pad,0
Don't support auto_pad current!,0
2018-02-28,0
if group is None:,0
group = 1,0
group is not support yet too.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
node,0
input,0
edge(node a <-> node b),0
,0
"key is edge name, value is src/dst node name",0
"key is initializer name, value is TensorProto",0
print(name),0
print(layer.op_type),0
n is input data,0
n is input edge,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"if node len(in_edges), generate additional DataInput node",0
print,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
TODO,1
"'max_pool2d': convert_maxpool,",0
"'onnx::Mul': convert_elementwise_mul,",0
"'onnx::Sub': convert_elementwise_sub,",0
"'onnx::ConvTranspose': convert_convtranspose,",0
"'onnx::LeakyRelu': convert_lrelu,",0
"'onnx::Sigmoid': convert_sigmoid,",0
"'onnx::Softmax': convert_softmax,",0
"'onnx::Tanh': convert_tanh,",0
"'onnx::Selu': convert_selu,",0
"'onnx::Transpose': convert_transpose,",0
"'onnx::Reshape': convert_reshape,",0
"'onnx::MatMul': convert_matmul,",0
"'onnx::Gather': convert_gather,",0
"'onnx::ReduceSum': convert_reduce_sum,",0
"'onnx::Constant': convert_constant,",0
"'onnx::Upsample': convert_upsample,",0
"'onnx::Pad': convert_padding,",0
###########,0
property #,0
###########,0
###################,0
Public Functions #,0
###################,0
test,0
Build network graph,0
"(batch, C, H, W)  & NHWC",0
#########,0
Layers #,0
#########,0
dilation,0
handle bias,0
TODO,1
output_shape,0
epsilon,0
mean,0
var,0
Kit weight tranpose,0
weight: N x M -> C x H x W x M -> H x W x C x M -> N x M,0
,0
weights,0
use_bias,0
units,0
axis,0
handle weight,0
handle bias,0
###################,0
Helper Functions #,0
###################,0
print (dir(data)),0
print (i),0
"print (""    {} with shape {}"".format(j, load_weight[i][j].shape))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Scopes created in a nested scope may have initial characters,0
that are illegal as the initial character of an op name,0
"(viz. '-', '\', '/', and '_').",0
sanity check.,0
run dce first to eliminate dead parts of the graph that might have been,0
left behind by things like symbolic_override,0
construct graph,0
nodes,0
input layer,0
TODO,1
build each layer,0
input,0
"print(node_input_name ,'->', node_name)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Base Functions,0
"padding,",0
"exstr = "", value=float('-Inf')""",0
"exstr = """"",0
Change to padding defuse,0
"input_node = self._defuse_padding(IR_node, exstr)",0
Ignore it in Pytorch,0
for Keras,0
"self.add_init(2, ""self.{:<15} = torch.from_numpy(__weights_dict['{}']['value'])"".format(",0
"IR_node.variable_name,",0
IR_node.name)),0
from torch.nn.parameter import Parameter,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self._type = layer.__class__.__name__.replace('Backward', '')",0
"self._name = ""{}_{}"".format(self.type, id)",0
TODO,1
"""""""",0
build graph for pytorch 0.2.0,0
"""""""",0
dummy_input = torch.autograd.Variable(torch.randn(shape)),0
output_node = self.model(dummy_input),0
search_queue = [output_node.grad_fn],0
"tmp_node = PyTorchGraphNode(output_node.grad_fn, 0)",0
self.layer_map[tmp_node.name] = tmp_node,0
visited = {output_node.grad_fn : self.layer_map[tmp_node.name]},0
idx = 0,0
node_count = 1,0
while (idx < len(search_queue)):,0
current_node = search_queue[idx],0
current_type = visited[current_node].type,0
"if hasattr(current_node, 'next_functions'):",0
"for parent, _ in current_node.next_functions:",0
"parent_type = parent.__class__.__name__.replace('Backward', '')",0
if parent_type != 'AccumulateGrad' and \,0
(parent_type != 'Transpose' or current_type != 'Addmm'):,0
if not parent in visited:,0
"tmp_node = PyTorchGraphNode(parent, node_count)",0
self.layer_map[tmp_node.name] = tmp_node,0
node_count += 1,0
visited[parent] = tmp_node,0
search_queue.append(parent),0
"self._make_connection(visited[parent].name, visited[current_node].name)",0
idx += 1,0
try:,0
return TorchGraph._forward_torch_random_input(,0
"torch_model,",0
"input_shapes,",0
is_batch=False,0
),0
except:,0
# try batch mode,0
# return TorchGraph._forward_torch_random_input(,0
"#     torch_model,",0
"#     input_shapes,",0
#     is_batch=True,0
# ),0
pass,0
tensor = torch.rand(*shape).float(),0
multi output,0
single output,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
###########,0
property #,0
###########,0
###################,0
Public Functions #,0
###################,0
Build network graph,0
#########,0
Layers #,0
#########,0
handle weight,0
handle bias,0
handle weight,0
handle bias,0
###################,0
Helper Functions #,0
###################,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
load model file into Coreml Graph,0
model.encode() convert to str --- python2 may crash due to type 'unicode',0
Build Network Graph,0
self.data_format ? TODO,1
load the model network,0
convert to Model_pb2.Model,0
determine the type of the current_node,0
staticmethods,0
"(seq, batch, C, H, W)  & NHWC",0
TODO dtype_map,1
##### convert methods,0
convolution,0
input edge,0
important!,0
reshape the weight!,0
"[2, 3, 0, 1]",0
pads,0
use_bias,0
isDeconvolution,0
"name, op",0
kwargs['kernel_shape'] = weights.shape,0
strides,0
"[1, sd, sh, sw, 1]",0
activation,0
TODO,1
padding in conv,0
"pad in IR is [x1_b, x2_b, ..., x1_e, x2_e, ...]",0
padding,0
compute padding for 'same',0
https://discuss.mxnet.io/t/pooling-and-convolution-with-same-mode/528/3,0
https://www.jianshu.com/p/05c4f1621c7e,0
padding in pooling,0
padding,0
https://discuss.mxnet.io/t/pooling-and-convolution-with-same-mode/528/3,0
TODO,1
symmetric padding,0
"name, op",0
input edge,0
For concat axis,0
"NO axis in coreml, so set the last axis",0
"The first -1 means in coreml there is one-more axis,",0
The second -1 means the last axis,0
"name, op",0
input edge,0
padding,0
Future Module TODO,1
#### rename methods,0
"name, op",0
activation type,0
else:,0
assert False,0
input edge,0
Merge layers,0
only for training,0
"name, op",0
input edge,0
shape,0
NHWC channel last,0
"in fact, here is NWHC",0
"name, op",0
input edge,0
axis TODO,0
"channels_first, then axis = 1",0
scale,0
bias,0
epsilon,0
compute adjusted parameters,0
Reference: parameter transformation https://github.com/apple/coremltools/issues/153,0
convert type because of tensorflow,0
mean,0
var,0
"name, op",0
input edge,0
bias,0
"name, op",0
input edge,0
"MAX = 0, AVERAGE = 1, L2 = 2",0
padding,0
strides,0
"[1, sd, sh, sw, 1]",0
window_shape,0
"[1, pd, ph, pw, 1]",0
"name, op",0
input edge,0
"name, op",0
input edge,0
"name, op",0
input edge,0
units,0
use_bias,0
weights,0
"name, op",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
the end of the tensorflow_resnet_v2_50's squeeze shape is [unknown_rank: true] with len 0,0
1001 means the 1001 classes for tensorflow_resnet_v2_50,0
!Alert! TODO,1
Future implement can be changed to the last two layer,0
TODO - remove style transfer 1D hack,1
"Input is 1D but it goes to the width dimension: (1,1,W)",0
"assume (Batch, Channels) - Batch dimension should be dropped",0
"assume (Batch, Sequence-Length, channels)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Add classifier classes (if applicable),0
Set pre-processing paramsters,0
"image_input_names,",0
Return the protobuf spec,0
model = _MLModel(self.builder.spec),0
Get input and output names,0
Dimensions and weights,0
"W should have shape (height, width, kernel_channels, output_channels), where kernel_channel = input_channels / groups",0
Dilations,0
see protobuf,0
assert False,0
Get the weights.,0
Dilations,0
see protobuf,0
depth-wise convolution,0
Dilations,0
see protobuf,0
Get input and output names,0
Pooling layer type,0
"if it's global, set the global flag",0
TODO  global pooling modification,0
Padding,0
see protobuf,0
Padding,0
see protobuf,0
Get input and output names,0
"type: ""list(int). A 1-D values of (leftBorder, topBorder, rightBorder, bottomBorder).""",0
This is central crop,0
Now add the layer,0
"Allowed values: 'CHW', 'HW', 'C', 'H', 'W'",0
Get input and output names,0
Get input and output names,0
Get the weights from keras,0
Get input and output names,0
blob_order == 0 if the input blob needs not be rearranged,0
blob_order == 1 if the input blob needs to be rearranged,0
"using keras_layer.input.shape have a ""?"" (Dimension[None] at the front),",0
making a 3D tensor with unknown batch size 4D,0
Get input and output names,0
activation emit,0
Get input and output names,0
negate it,0
apply threshold,0
negate it back,0
for Keras,0
Get input and output names,0
Set parameters,0
"Parameter arrangement in Keras: gamma, beta, mean, variance",0
compute adjusted parameters,0
Reference: parameter transformation https://github.com/apple/coremltools/issues/153,0
padding type TODO,0
"Type of the padding. Can be one of 'constant', 'reflection' or 'replication",0
Now add the layer,0
self.emit_Flatten(IR_node),0
"depth_radius: Half-width of the 1-D normalization window.""",0
Get the weights,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
input layer,0
build each layer,0
"if A.output == B.input, then make the connection: A -> B",0
"print('{0:20}->     {1:20}'.format(layerA.name, layerB.name))",0
"if A.name == B.input, then make the connection: A -> B, here A is the input",0
build each layer,0
"if A.output == B.input, then make the connection: A -> B",0
"print('{0:20}->     {1:20}'.format(layerA.name, layerB.name))",0
"if A.name == B.input, then make the connection: A -> B, here A is the input",0
build each layer,0
"if A.output == B.input, then make the connection: A -> B",0
"print('{0:20}->     {1:20}'.format(layerA.name, layerB.name))",0
"if A.name == B.input, then make the connection: A -> B, here A is the input",0
The information of the layer,0
Generated by the protocol buffer compiler.  DO NOT EDIT!,0
source: caffe.proto,0
@@protoc_insertion_point(imports),0
@@protoc_insertion_point(class_scope:caffe.BlobShape),0
@@protoc_insertion_point(class_scope:caffe.BlobProto),0
@@protoc_insertion_point(class_scope:caffe.BlobProtoVector),0
@@protoc_insertion_point(class_scope:caffe.Datum),0
@@protoc_insertion_point(class_scope:caffe.FillerParameter),0
@@protoc_insertion_point(class_scope:caffe.NetParameter),0
@@protoc_insertion_point(class_scope:caffe.SolverParameter),0
@@protoc_insertion_point(class_scope:caffe.SolverState),0
@@protoc_insertion_point(class_scope:caffe.NetState),0
@@protoc_insertion_point(class_scope:caffe.NetStateRule),0
@@protoc_insertion_point(class_scope:caffe.ParamSpec),0
@@protoc_insertion_point(class_scope:caffe.LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.TransformationParameter),0
@@protoc_insertion_point(class_scope:caffe.LossParameter),0
@@protoc_insertion_point(class_scope:caffe.AccuracyParameter),0
@@protoc_insertion_point(class_scope:caffe.ArgMaxParameter),0
@@protoc_insertion_point(class_scope:caffe.ConcatParameter),0
@@protoc_insertion_point(class_scope:caffe.BatchNormParameter),0
@@protoc_insertion_point(class_scope:caffe.BiasParameter),0
@@protoc_insertion_point(class_scope:caffe.ContrastiveLossParameter),0
@@protoc_insertion_point(class_scope:caffe.ConvolutionParameter),0
@@protoc_insertion_point(class_scope:caffe.CropParameter),0
@@protoc_insertion_point(class_scope:caffe.DataParameter),0
@@protoc_insertion_point(class_scope:caffe.DropoutParameter),0
@@protoc_insertion_point(class_scope:caffe.DummyDataParameter),0
@@protoc_insertion_point(class_scope:caffe.EltwiseParameter),0
@@protoc_insertion_point(class_scope:caffe.ELUParameter),0
@@protoc_insertion_point(class_scope:caffe.EmbedParameter),0
@@protoc_insertion_point(class_scope:caffe.ExpParameter),0
@@protoc_insertion_point(class_scope:caffe.FlattenParameter),0
@@protoc_insertion_point(class_scope:caffe.HDF5DataParameter),0
@@protoc_insertion_point(class_scope:caffe.HDF5OutputParameter),0
@@protoc_insertion_point(class_scope:caffe.HingeLossParameter),0
@@protoc_insertion_point(class_scope:caffe.ImageDataParameter),0
@@protoc_insertion_point(class_scope:caffe.InfogainLossParameter),0
@@protoc_insertion_point(class_scope:caffe.InnerProductParameter),0
@@protoc_insertion_point(class_scope:caffe.InputParameter),0
@@protoc_insertion_point(class_scope:caffe.LogParameter),0
@@protoc_insertion_point(class_scope:caffe.LRNParameter),0
@@protoc_insertion_point(class_scope:caffe.MemoryDataParameter),0
@@protoc_insertion_point(class_scope:caffe.MVNParameter),0
@@protoc_insertion_point(class_scope:caffe.ParameterParameter),0
@@protoc_insertion_point(class_scope:caffe.PoolingParameter),0
@@protoc_insertion_point(class_scope:caffe.PowerParameter),0
@@protoc_insertion_point(class_scope:caffe.PythonParameter),0
@@protoc_insertion_point(class_scope:caffe.RecurrentParameter),0
@@protoc_insertion_point(class_scope:caffe.ReductionParameter),0
@@protoc_insertion_point(class_scope:caffe.ReLUParameter),0
@@protoc_insertion_point(class_scope:caffe.ReshapeParameter),0
@@protoc_insertion_point(class_scope:caffe.ScaleParameter),0
@@protoc_insertion_point(class_scope:caffe.SigmoidParameter),0
@@protoc_insertion_point(class_scope:caffe.SliceParameter),0
@@protoc_insertion_point(class_scope:caffe.SoftmaxParameter),0
@@protoc_insertion_point(class_scope:caffe.TanHParameter),0
@@protoc_insertion_point(class_scope:caffe.TileParameter),0
@@protoc_insertion_point(class_scope:caffe.ThresholdParameter),0
@@protoc_insertion_point(class_scope:caffe.WindowDataParameter),0
@@protoc_insertion_point(class_scope:caffe.SPPParameter),0
@@protoc_insertion_point(class_scope:caffe.V1LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.V0LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.PReLUParameter),0
@@protoc_insertion_point(module_scope),0
"TODO: We need to identify whether this is 4D image data, otherwise we shouldn't change the dimension order",1
print(node.parameters),0
assert False,0
kwargs['gamma'] = 0.25,0
"Stochastic pooling, for instance.",0
TODO: Axis,0
TODO: Unbiased,1
check if need the Flatten layer,0
if parent.output_shape.height > 1 or parent.output_shape.width > 1:,0
TODO: The gamma parameter has to be set (in node.data?) and this should work.,1
"Also, mean should be set to 0, and var to 1, just to be safe.",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"For some reason argparser gives us unicode, so we need to conver to str first",0
for test,0
"with open(""graph.txt"", 'w') as f:",0
for layer in self.IR_graph.topological_sort:,0
current_node = self.IR_graph.get_node(layer),0
"print(""========current_node=========\n{}"".format(current_node.layer), file=f)",0
test end,0
"print(""========current_node={}"".format(current_node.layer))",0
check if have pad layer,0
keys = [],0
for key in self.weights_dict[IR_node.name].keys():,0
keys.append(key),0
"print(""=======Layer: {}, keys: {}"".format(IR_node.name, keys))",0
Change the layer name,0
check if need crop output shape,0
"change the key ""name"" to ""variable_name"", in case of the layer name has invalid characters",0
"def emit_Tanh(self, IR_node):",0
"self._emit_activation(IR_node, 'ops.tanh')",0
"Convert NHWC (IR) to NCHW (Caffe): [0,1,2,3]->[0,3,1,2]",0
Caffe Types,0
Same value applies to all spatial dimensions,0
Extract the value for the given spatial dimension,0
consider rewrite this function to Network.py,0
Dropout layers appear in a fair number of Caffe,0
test-time networks. These are just ignored. We'll,0
filter them out here.,0
TODO: raise error,1
TODO: raise error,1
Automatically set a name if not provided.,0
Figure out the layer inputs.,0
"print('op: %s   shape: %s' % (op, layer_output._keras_shape))",0
"print('op: %s   shape: %s' % (op, layer_output.get_shape().as_list()))",0
Add to layer LUT.,0
Old-style input specification,0
New-style input specification,0
"We most likely have a data layer on our hands. The problem is,",0
Caffe infers the dimensions of the data from the source (eg: LMDB).,0
We want to avoid reading datasets here. Fail for now.,0
This can be temporarily fixed by transforming the data layer to,0
"Caffe's ""input"" layer (as is usually used in the ""deploy"" version).",0
TODO: Find a better solution for this.,1
The .prototxt file defining the graph,0
The .caffemodel file containing the learned parameters,0
Set to true if the fallback protocol-buffer based backend was used,0
"A list containing (layer name, parameters) tuples",0
Load the parameters,0
"When using the protobuf-backend, each parameter initially has four dimensions.",0
"In certain cases (like FC layers), we want to eliminate the singleton dimensions.",0
"This implementation takes care of the common cases. However, it does leave the",0
potential for future issues.,0
The Caffe-backend does not suffer from this problem.,0
A dictionary mapping NodeKind to the transposed order.,0
The node kinds eligible for reshaping,0
"If true, the reshaped data will replace the old one.",0
"Otherwise, it's set to the reshaped_data attribute.",0
Check for 2+ dimensional data,0
The FC layer connected to the spatial layer needs to be,0
re-wired to match the new spatial ordering.,0
node.reshaped_data = weights.transpose(transpose_order),0
Set the weights,0
We're only fusing nodes with single parents,0
We can only fuse a node if its parent's,0
value isn't used by any other node.,0
Rewrite the fused node's children to its parent.,0
Disconnect the fused node from the graph.,0
Let the sub-class merge the fused node in any arbitrary way.,0
Fuse ReLUs when the parent node is one of the given types.,0
"If None, all node types are eligible.",0
Prescale the stats,0
Replace with the updated values,0
Include the scale and bias terms,0
self.graph = NodeRenamer()(graph),0
"return Graph(name, [self.map_node(node) for node in self.graph.nodes])",0
Kit,0
"mapped_node.input.extend(['%s:%s' % (input.name, idx) for input, idx in node.parents])",0
FIXME:,0
output = node.output,0
Decompose DAG into chains,0
Generate Python code line by line,0
try:,0
except ImportError:,0
# # Fall back to the protobuf implementation,0
# from mmdnn.conversion.caffe import caffe_pb2,0
# self.caffepb = caffe_pb2,0
# show_fallback_warning(),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
from tensorflow.core.framework.node_def_pb2 import NodeDef,0
from tensorflow.core.framework import attr_value_pb2,0
pass,0
"print(""\n"")",0
print(i),0
print(block),0
continue,0
NHWC,0
assert False,0
print(shortcut_layer['input'] ),0
spacetodepth,0
print(block),0
print(region_layer),0
assert False,0
print line,0
set default value,0
load weight by original order,0
print(node_type),0
print(IR_node),0
assert False,0
strides,0
assert False,0
padding,0
only load weight conv,0
"print(""----------------"",self.start)",0
print(kernel.shape),0
print(k_bias.shape),0
"buf, start, scale_layer['name'], bn_layer['name'], conv_layer['name']",0
"print(""=============="",self.start)",0
print(bias.shape),0
print(scale.shape),0
print(mean.shape),0
print(var.shape),0
print(kernel.shape),0
print(W),0
assert False,0
no use,0
print(source_node.layer),0
for image classification(resnet) AVG pooling,0
print(source_node.layer),0
print(source_node.layer),0
kwargs['ignore_thresh'] = source_node.get_attr('ignore_thresh'),0
print(source_node.get_attr('anchors')),0
"kwargs['anchors'] = ['0.52','0.22']",0
kwargs['mask'] = source_node.get_attr('mask'),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Reference: 1) Keras image preprocess: https://github.com/keras-team/keras/blob/master/keras/applications/imagenet_utils.py,0
2) tensorflow github issue: https://github.com/tensorflow/models/issues/517,0
"R-G-B for Imagenet === [123.68, 116.78, 103.94]",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result(None, False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
layer_name = 'block2_pool',0
"intermediate_layer_model = keras.Model(inputs=model.input,",0
outputs=model.get_layer(layer_name).output),0
intermediate_output = intermediate_layer_model.predict(img),0
print (intermediate_output),0
print (intermediate_output.shape),0
"print (""%.30f"" % np.sum(intermediate_output))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Load a real image and do default tf imageNet preprocessing,0
"[Batch, Height, Width, Channels] --> [Batch, Channels, Height, Width]",0
We only deal with non-recurrent networks for now,0
"(H,W,C) --> (C,H,W)",0
Load TensorFlow model,0
Prepare inputs,0
Run TF session,0
Evaluate coreml model,0
evaluate the TF model,0
evaluate CoreML,0
Test the default CoreML evaluation,0
Download model,0
Convert to coreml,0
Test predictions on an image,0
Download model,0
Convert to coreml,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
"@unittest.skip(""Failing GPU backend: related to https://github.com/tf-coreml/tf-coreml/issues/26"")",0
? style transfer image size and style number?,0
Test predictions on an image,0
evaluate the TF model,0
evaluate CoreML,0
Test the default CoreML evaluation,0
"compare_tf_coreml_outputs(tf_out_flatten, coreml_out_flatten)",0
#Download model,0
url = 'https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz',0
tf_model_dir = _download_file(url = url),0
"tf_model_path = os.path.join(TMP_MODEL_DIR, 'inception_v3_2016_08_28_frozen.pb')",0
#Convert to coreml,0
"mlmodel_path = os.path.join(TMP_MODEL_DIR, 'inception_v3_2016_08_28.mlmodel')",0
mlmodel = tf_converter.convert(,0
"tf_model_path = tf_model_path,",0
"mlmodel_path = mlmodel_path,",0
"output_feature_names = ['InceptionV3/Predictions/Softmax:0'],",0
"input_name_shape_dict = {'input:0':[1,299,299,3]},",0
"image_input_names = ['input:0'],",0
"red_bias = -1,",0
"green_bias = -1,",0
"blue_bias = -1,",0
image_scale = 2.0/255.0),0
#Test predictions on an image,0
_test_coreml_model_image_input(,0
"tf_model_path = tf_model_path,",0
"coreml_model = mlmodel,",0
"input_tensor_name = 'input:0',",0
"output_tensor_name = 'InceptionV3/Predictions/Softmax:0',",0
img_size = 299),0
Download model,0
Convert to coreml,0
Test predictions on an image,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
self.test_truth(),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
from collections import namedtuple,0
"Batch = namedtuple('Batch', ['data'])",0
TODO,1
Apple has published some of their own models. They can be downloaded from https://developer.apple.com/machine-learning/.,0
"Those published models are: SqueezeNet, Places205-GoogLeNet, ResNet50, Inception v3, VGG16",0
TODO,1
load model,0
TODO: Multiple inputs,1
TODO: Multiple outputs,1
inference,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
caffe uses NCHW,0
"self.print_intermediate_result('pooling0', False)",0
delete tmp model files,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
pylint: disable=line-too-long,0
pylint: enable=line-too-long,0
"net = caffe.Net(arch_fn, weight_fn, caffe.TEST)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
# Image Classification,0
# Semantic Segmentation,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
print(r),0
d = darknet_extractor(),0
model_filename = d.download('yolov3'),0
print(model_filename),0
"image_path = ""./mmdnn/conversion/examples/data/dog.jpg""",0
"model_path = ""./""",0
d = darknet_extractor(),0
"result = d.inference('yolov3', model_filename, model_path, image_path = image_path)",0
print(result),0
print(i),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result('pooling0', False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
need to be updated,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.input, self.model, self.testop = self.MainModel.KitModel(self.args.w)",0
testop = tf.get_default_graph().get_operation_by_name(layer_name),0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"'mobilenet_v1': mobilenet_v1.mobilenet_v1_arg_scope,",0
"'mobilenet_v1' : mobilenet_v1.mobilenet_v1,",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
raise NotImplementedError(),0
load file,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Notes for training NASNet Cifar Model,0
-------------------------------------,0
batch_size: 32,0
learning rate: 0.025,0
cosine (single period) learning rate decay,0
auxiliary head loss weighting: 0.4,0
clip global norm of all gradients by 5,0
600 epochs with a batch size of 32,0
This is used for the drop path probabilities since it needs to increase,0
the drop out probability over the course of training.,0
Notes for training large NASNet model on ImageNet,0
-------------------------------------,0
batch size (per replica): 16,0
learning rate: 0.015 * 100,0
learning rate decay factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 100 replicas,0
auxiliary head loss weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Notes for training the mobile NASNet ImageNet model,0
-------------------------------------,0
batch size (per replica): 32,0
learning rate: 0.04 * 50,0
learning rate scaling factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 50 replicas,0
auxiliary head weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Shape of feature map before the final layer.,0
149 x 149 x 32,0
Run the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Find where to place the reduction cells or stride normal cells,0
Setup for building in the auxiliary head.,0
Run the cells,0
true_cell_num accounts for the stem cells,0
Final softmax layer,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80,0
71 x 71 x 192,0
35 x 35 x 192,0
35 x 35 x 320,0
TODO(alemi): Register intermediate endpoints,1
"17 x 17 x 1088 if output_stride == 8,",0
33 x 33 x 1088 if output_stride == 16,0
TODO(alemi): register intermediate endpoints,1
TODO(gpapan): Properly support output_stride for the rest of the net.,1
8 x 8 x 2080,0
TODO(alemi): register intermediate endpoints,1
8 x 8 x 1536,0
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which",1
can be set to False to disable pooling here (as in resnet_*()).,0
Set weight_decay for weights in conv2d and fully_connected layers.,0
Set activation_fn and parameters for batch_norm.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
=============================================================================,0
Tensorflow mandates these.,0
Conv and DepthSepConv namedtuple define layers of the MobileNet architecture,0
Conv defines 3x3 convolution layers,0
DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.,0
stride is the stride of the convolution,0
depth is the number of channels or filters in a layer,0
_CONV_DEFS specifies the MobileNet body,0
Used to find thinned depths for each layer.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
By passing filters=None,0
separable_conv2d produces only a depthwise convolution layer,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 1024,0
Set weight_decay for weights in Conv and DepthSepConv layers.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Skip path 1,0
Skip path 2,0
"First pad with 0's on the right and bottom, then shift the filter to",0
include those 0's that were added.,0
Concat and apply BN,0
Set the prev layer to the current layer if it is none,0
Check to be sure prev layer stuff is setup correctly,0
Apply conv operations,0
Combine hidden states using 'add'.,0
Add hiddenstate to the list of hiddenstates we can choose from,0
Dont stride if this is not one of the original hiddenstates,0
"Check if a stride is needed, then use a strided 1x1 here",0
Determine if a reduction should be applied to make the number of,0
filters match.,0
Return the concat of all the states,0
Scale keep prob by layer number,0
The added 2 is for the reduction cells,0
Decrease the keep probability over time,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
Only insert rate to params if rate > 1.,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Copyright (c) Microsoft. All rights reserved.,0
Licensed under the MIT license. See LICENSE.md file in the project root,0
for full license information.,0
==============================================================================,0
"self.model, self.testop = self.MainModel.KitModel(self.args.w)",0
"self.print_intermediate_result(None, False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
pylint: disable=line-too-long,0
pylint: enable=line-too-long,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.anchors = np.array([[10,13], [16,30],[33,23],[30,61],[62,45], [59,119],[116,90],[156,198],[373,326]])",0
print(output),0
"print(out_boxes, out_scores, out_classes)",0
get random colors,0
My kingdom for a good redistributable image drawing library.,0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
yolov3 80 classes,0
"Reshape to batch, height, width, num_anchors, box_params.",0
Adjust preditions to each spatial grid point and anchor size.,0
Note: YOLO iterates over height index before width index.,0
"TODO: It works with +1, don't know why.",1
Scale boxes back to original image shape.,0
"print(""feats,anchors, num_classes, input_shape"", feats, anchors, num_classes, input_shape)",0
"print(box_xy, box_wh, box_confidence, box_class_probs)",0
"yolo_outputs order 13,26,52",0
TODO: use keras backend instead of tf.,1
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
save network structure as JSON,0
layer_name = 'block2_pool',0
"intermediate_layer_model = keras.Model(inputs=model.input,",0
outputs=model.get_layer(layer_name).output),0
intermediate_output = intermediate_layer_model.predict(img),0
print (intermediate_output),0
print (intermediate_output.shape),0
"print (""%.30f"" % np.sum(intermediate_output))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Convert network architecture only,0
MXNet,0
Caffe,0
For CoreML,0
assert args.network or args.frozen_pb,0
Caffe,0
Darknet,0
"exts = ['.pb', '.npy', '.py']",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check non-sense always input module.Module,0
if not (type(model) == mx.module.Module,0
or type(model) == mx.module.SequentialModule,0
or type(model) == mx.model),0
"raise TypeError(""MXNet layer of type %s is not supported."" % type(model))",0
"if layer[""op""] == ""null"":",0
continue,0
"raise NotImplementedError(""Cannot support multi-input"")",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"mxnet-cpu only support channel first, default convert the model and weight as channel first",0
download and show the image,0
"convert into format (batch, RGB, width, height)",0
"to show the image, change the argument show into True",0
compute the predict probabilities,0
print the top-5,0
# call function predict,0
"with open('synset.txt', 'r') as f:",0
labels = [l.rstrip() for l in f],0
"predict(model, labels, 'http://writm.com/wp-content/uploads/2016/08/Cat-hd-wallpapers.jpg')",0
"print(""Warning: MXNet Convolution Layer pad does not match IR Convolution Layer pad"")",0
"layout = IR_node.IR_layer.attr[""data_format""].s",0
if layout not in MXNetEmitter.channels_last:,0
Add LeakyReLU Elu(slope not support),0
"axis = IR_node.IR_layer.attr[""axis""].i",0
not supported yet,0
"axis = IR_node.IR_layer.attr[""axis""].i",0
not supported yet,0
"print(""Warning: MXNet Pooling Layer pad does not match IR Pooling Layer pad"")",0
"def emit_ConvTranspose(self, IR_node):",0
if self.weight_loaded:,0
weight_dict = self.weights[IR_node.name],0
weights = weight_dict['weights'],0
"dim = len(IR_node.IR_layer.attr[""kernel_shape""].list.i) - 2",0
kernel = list(),0
"for idx in range(0, dim):",0
"kernel.append(IR_node.IR_layer.attr[""kernel_shape""].list.i[idx])",0
stride = list(),0
"for e in IR_node.IR_layer.attr[""strides""].list.i[1:-1]:",0
stride.append(e),0
dilate = list(),0
"for e in IR_node.IR_layer.attr[""dilations""].list.i[1:-1]:",0
dilate.append(e),0
"dilate = ', '.join('%s' % i for i in dilate)",0
defuse_pad = False,0
pad = list(),0
"if ""pads"" in IR_node.IR_layer.attr:",0
output_shape = list(),0
"for e in IR_node.IR_layer.attr[""_output_shapes""].list.shape[0].dim:",0
output_shape.append(e.size),0
"# print(""Warning: MXNet Deconvolution Layer pad does not match IR Deconvolution Layer pad"")",0
"defuse_pad, pad = MXNetEmitter.transfer_pad(IR_node.IR_layer.attr[""pads""].list.i)",0
"pad = ', '.join('%s' % i for i in pad)",0
"kernel = ', '.join('%s' % i for i in kernel)",0
"stride = ', '.join('%s' % i for i in stride)",0
"num_filter = IR_node.IR_layer.attr[""kernel_shape""].list.i[-2]",0
"no_bias = not IR_node.IR_layer.attr[""use_bias""].b",0
if not no_bias and self.weight_loaded:,0
"self.output_weights[IR_node.replace_scope(IR_node.name) + ""_bias""] = weight_dict['bias']",0
"# layout = IR_node.IR_layer.attr[""data_format""].s",0
if dim == 1:,0
layout = 'NCW',0
elif dim == 2:,0
layout = 'NCHW',0
elif dim == 3:,0
layout = 'NCDHW',0
if self.weight_loaded:,0
# if layout not in MXNetEmitter.channels_last:,0
"weights = MXNetEmitter.transpose(weights, dim)",0
"self.output_weights[IR_node.replace_scope(IR_node.name) + ""_weight""] = weights",0
"code = """"",0
if not defuse_pad:,0
"code = ""{:<15} = mx.sym.Deconvolution(data = {}, kernel = ({}), stride = ({}), dilate = ({}), pad = ({}), num_filter = {}, no_bias = {}, layout = '{}', name = '{}')"".format(",0
"IR_node.replace_scope(IR_node.name),",0
"IR_node.replace_scope(IR_node.in_edges[0]),",0
"kernel,",0
"stride,",0
"dilate,",0
"pad,",0
"num_filter,",0
"no_bias,",0
"layout,",0
IR_node.replace_scope(IR_node.name)),0
else:,0
"code = self.set_pad(IR_node, code, pad)",0
"code += ""\n    {:<15} = mx.sym.Deconvolution(data = {}, kernel = ({}), stride = ({}), dilate = ({}), num_filter = {}, no_bias = {}, layout = '{}', name = '{}')"".format(",0
"IR_node.replace_scope(IR_node.name), IR_node.replace_scope(IR_node.name) + ""_pad"", kernel, stride, dilate, num_filter, no_bias, layout, IR_node.replace_scope(IR_node.name))",0
return code,0
reverse cannot support yet,0
"code = ""{:<15} = mx.sym.transpose(data = {}, axes = (0, 2, 3, 1))\n"".format(""trans"", self.parent_variable_name(IR_node))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Not support yet,0
"""softrelu""  : ""SoftReLU""",0
Load the model network and weights,0
"digraph = mx.viz.plot_network(sym, save_format='jpg') # For debugging",0
digraph.render(),0
model.bind(data_shapes = data_shapes),0
model.init_params(),0
"mod.load(model_path, epoch_num)",0
return mod.get_params(),0
load the model network,0
adjust the data format,0
load model files into MXNet graph,0
data_shape arguments added to calculate infer_shape(required),0
"if isinstance(input_arg, basestring):",0
Build network graph,0
transpose to channel last,0
"name, op",0
input edge,0
output shape,0
input edge,0
attr,0
"print(""Warning: MXNet symbol pad does not support channel last"")",0
"IR_node.attr[""pads""].list.i.extend([0, 0])",0
for e in pad:,0
"IR_node.attr[""pads""].list.i.extend([e, e])",0
"IR_node.attr[""pads""].list.i.extend([0, 0])",0
"name, op",0
input edge,0
units,0
use bias (no_bias default = False),0
weights,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [NM]"".format(source_node.name, self.data_format))",0
input edge,0
output shape,0
kernel_shape,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [{}]"".format(source_node.name, self.data_format, layout))",0
groups,0
in_channel = in_channel // group,0
use_bias (no_bias default = False),0
strides,0
dilations,0
data_format,0
padding,0
weights,0
axis,0
scale,0
epsilon,0
momentum,0
weights,0
gamma,0
beta,0
mean,0
var,0
"name, op",0
input edge,0
pooling type (sum not allowed yet),0
strides,0
kernel_shape,0
padding,0
output shape,0
"name, op",0
input edge,0
output shape,0
dim,0
"def rename_log_softmax(self, source_node):",0
"raise NotImplementedError(""not support yet"")",0
"def rename_Correlation(self, source_node):",0
"raise NotImplementedError(""not support yet"")",0
"name, op",0
input edge,0
padding,0
output shape,0
kernel_shape,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [{}]"".format(source_node.name, self.data_format, layout))",0
use_bias (no_bias default = False),0
strides,0
dilations,0
data_format,0
groups,0
weights,0
"def rename_RNN(self, source_node):",0
"raise NotImplementedError(""RNN not support yet"")",0
"name, op",0
input edge,0
input_dim,0
output_dim,0
dtype,0
output shape,0
"IR only support elu and prelu from {'elu', 'leaky', 'prelu', 'rrelu'}",0
return,0
"name, op",0
gamma,0
input edge,0
"alpha [exp(x) - alpha], but mxnet attr slope [slope*(exp(x) - 1)] when x < 0",0
"IR_node.attr[""alpha""].f = float()",0
output shape,0
"raise NotImplementedError(""slope cannot convert to alpha"")",0
"def rename_InstanceNorm(self, source_node):",0
raise NotImplementedError,0
"def rename_L2Normalization(self, source_node):",0
raise NotImplementedError,0
alpha,0
beta,0
knorm,0
nsize,0
keep_prob,0
mode,0
reverse cannot support yet,0
old API target_shape not support yet,0
output shape,0
"raise NotImplementedError(""adjust output shape"")",0
dim,0
dtype,0
output shape,0
"name, op",0
input edge,0
output shape,0
axis,0
"raise NotImplementedError(""No matching IR api"")",0
"left strip the ""_"" at the beginning of the name",0
"Issue #85, #135",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
keep dims,0
axes,0
"name, op",0
epsilon,0
moving variance (var),0
gamma (scale),0
mean,0
bias,0
input node,0
output node,0
load model files into TensorFlow graph,0
Build network graph,0
TODO: Fused conv and pool with padding is different from defused operators,0
strides,0
window_shape,0
pool type,0
padding,0
shape,0
strides,0
input[1] : W,0
filter,0
padding,0
output[0] : B,0
converted [dropout],0
convert [tf.contrib.layers.batch_norm],0
normal Add,0
units,0
Weights,0
FullyConnected Layer,0
"name, op",0
get Bias,0
Matmul Layer,0
Deal Dropout,0
keep prob,0
Remove nodes,0
Mul,0
Floor,0
paddings,0
gamma (scale),0
bias,0
Mean,0
Var,0
TODO: Current it is only for slice,1
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
check whether flatten operator should be added,0
flatten is needed,0
"self._emit_unary_operation(IR_node, ""contrib.layers.flatten"")",0
skip_prefix = [,0
"""^"",",0
"""train_op"",",0
"""save"",",0
"""gradients"",",0
"""init"",",0
"""global_step"",",0
"""distort_image"",",0
"""Adagrad"",",0
],0
"""Switch""",0
"""RequantizationRange"",",0
"""Requantize"",",0
"""Cast""",0
load model files into TensorFlow graph,0
model = original_gdef,0
Save it to an output file,0
for pytest,0
assert False,0
keep dims,0
axes,0
"name, op",0
ssd model is transformed,0
Ax - (Au - b),0
A,0
print(A_content),0
b,0
print(sub_content),0
print(IR_node.input),0
print(IR_node.output),0
assert False,0
epsilon,0
moving variance (var) /read,0
gamma (scale),0
"gamma = self.get_parent(son.name, [1, 1, 0, 0, 0, 1], True)",0
print(output_node.layer),0
print(Mul.layer),0
beta  (bias),0
moving mean (mean),0
input node,0
output node,0
epsilon,0
beta,0
gamma (scale),0
"output_node = self.get_son(source_node.name, [0, 0, 0, 0], True)",0
assert False,0
TODO: Fused conv and pool with padding is different from defused operators,0
assert variable.get_attr('_output_shapes')[0].dim[0].size == IR_node.attr['kernel_shape'].list.i[-1],0
strides,0
window_shape,0
pool type,0
padding,0
epsilon,0
moving variance (var) /read,0
gamma (scale),0
beta  (bias),0
moving mean (mean),0
print(source_node.in_edges),0
print(source_node.out_edges),0
print(source_node.layer),0
print(tensor_content),0
print(IR_node),0
print(source_node.layer),0
assert False,0
print(source_node.layer),0
assert False,0
print(source_node.layer),0
print(source_node.in_edges),0
print(source_node.out_edges),0
assert False,0
"input_node_0 = self.src_graph.get_parent(source_node.name, [0])",0
# mean/read,0
if input_node_0.type == 'Identity':,0
"input_node_0_read = self.src_graph.get_parent(input_node_0.name, [0])",0
tensor_content = input_node_0_read.get_attr('value'),0
tensor_content = tensor_util.MakeNdarray(tensor_content),0
"self.set_weight(source_node.name, 'weights', tensor_content)",0
"IR_node = self._convert_identity_operation(source_node, start_idx = 1)",0
else:,0
print(scopes),0
print(scopes),0
print(source_node.layer),0
"def rename_Pack(self, source_node):",0
"IR_node = self._convert_identity_operation(source_node, new_op = 'Pack')",0
kwargs['shape'] = self.tensor_shape_to_list(input_node.get_attr('_output_shapes'))[0],0
kwargs['shape'] = self.tensor_shape_to_list(input_node.get_attr('_output_shapes'))[0],0
"def rename_ExpandDims(self, source_node):",0
"IR_node = self._convert_identity_operation(source_node, new_op = 'ExpandDims')",0
"input_node = self.src_graph.get_parent(source_node.name, [0])",0
kwargs = {},0
kwargs['shape'] = self.tensor_shape_to_list(input_node.get_attr('_output_shapes'))[0],0
"input_node_indices = self.src_graph.get_parent(source_node.name, [1])",0
kwargs['exp_dim'] = input_node_indices.get_attr('value').int_val[0],0
"assign_IRnode_values(IR_node, kwargs)",0
weights,0
alpha,0
beta,0
weights,0
"def rename_Dequantize(self, source_node):",0
"IR_node = self._convert_identity_operation(source_node,start_idx=0, end_idx= 1, new_op = 'Dequantize')",0
kwargs = {},0
"input_node = self.src_graph.get_parent(source_node.name, [0])",0
kwargs['shape'] = self.tensor_shape_to_list(input_node.get_attr('_output_shapes'))[0],0
print(source_node.layer),0
print(source_node.layer),0
assert False,0
kwargs = {},0
kwargs['mode'] = 'constant',0
kwargs['constant_values'] = 0.0,0
# paddings,0
"padding = self.get_parent(source_node.name, [1]).layer.attr['value'].tensor",0
shapes = tensor_util.MakeNdarray(padding),0
kwargs['pads'] = convert_tf_pad_to_onnx(shapes),0
"assign_IRnode_values(IR_node, kwargs)",0
print(source_node.layer),0
assert False,0
print(source_node.layer),0
assert False,0
print(source_node.layer),0
assert False,0
print(source_node.layer),0
assert False,0
print(source_node.layer),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Build network graph,0
strides,0
window_shape,0
pool type,0
padding,0
TODO: Handle square,1
self._print_layer(source_node),0
print (source_node.name),0
"print (self.src_graph.get_parent(source_node.name, [0]).real_name)",0
assert False,0
Convolution,0
Kernel,0
Attributes,0
Bias,0
Activation,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
BlockFunction node,0
redirect the composite's inputs to the true inputs,0
"BlockFunctions are short-circuited, and not added to accum[]",0
Function node,0
OutputVariable node,0
def build(self):,0
"_traverse_graph(self, self.model.root_function)",0
"super(CntkGraph, self).build()",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
??,0
print(IR_node.layer),0
assert False,0
print(IR_node.layer),0
assert False,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
For padding,0
print (input_shape),0
print (kernel_shape),0
print (strides),0
Not tested,0
print (input_shape),0
print (kernel_shape),0
print (strides),0
kernel_shape[idx] = (kernel_shape[idx] - 1) * dilation_rate + 1,0
print ([0] + up_list + [0] + down_list if data_format.startswith('NC') else up_list + [0] + down_list + [0]),0
print ('-----------------------------------------------------'),0
network library,0
not support multi-threads download,0
key: layer_name    value: keras layer,0
private functions,0
"print ('{} --> {}'.format(src, dst))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
share functions,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
name --> (weight_name --> ndarray),0
Generated by the protocol buffer compiler.  DO NOT EDIT!,0
source: graph.proto,0
@@protoc_insertion_point(imports),0
@@protoc_insertion_point(class_scope:GraphDef),0
@@protoc_insertion_point(class_scope:NodeDef.AttrEntry),0
@@protoc_insertion_point(class_scope:NodeDef),0
@@protoc_insertion_point(class_scope:AttrValue.ListValue),0
@@protoc_insertion_point(class_scope:AttrValue),0
@@protoc_insertion_point(class_scope:TensorShape.Dim),0
@@protoc_insertion_point(class_scope:TensorShape),0
@@protoc_insertion_point(class_scope:LiteralTensor),0
@@protoc_insertion_point(module_scope),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
First try to read it as a binary file.,0
Next try to read it as a text file.,0
-*- coding: utf-8 -*-,0
from keras.layers.core import Layer,0
Arguments,0
Input shape,0
Output shape,0
Prepare broadcasting shape.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Load the model network,0
Load the model weights,0
load model files into Keras graph,0
"_keras.utils.plot_model(model, ""model.png"", show_shapes = True)",0
Build network graph,0
TODO: Fused conv and pool with padding is different from defused operators,0
TODO: More activation functions,1
for ELU,0
input edge,0
"name, op",0
weights,0
pads,0
filter,0
"[kd, kh, kw, channel_size, filter number]",0
use_bias,0
strides,0
"[1, sd, sh, sw, 1]",0
dilations,0
"[1, dd, dh, dw, 1]",0
activation,0
"name, op",0
input edge,0
padding,0
strides,0
"[1, sd, sh, sw, 1]",0
window_shape,0
"[1, pd, ph, pw, 1]",0
"name, op",0
input edge,0
For concat axis,0
"name, op",0
input edge,0
padding,0
only for training,0
"name, op",0
input edge,0
"name, op",0
input edge,0
Merge Layers,0
only for training,0
"name, op",0
input edge,0
shape,0
only for training,0
"name, op",0
input edge,0
Core Layers,0
"name, op",0
input edge,0
units,0
use_bias,0
weights,0
activation,0
"name, op",0
input edge,0
"name, op",0
input edge,0
input_dim,0
output_dim,0
mask_zero,0
weights,0
"name, op",0
input edge,0
units,0
use_bias,0
"for Keras, drop_out and recurrent_dropout",0
activation,0
"name, op",0
input edge,0
units,0
activation,0
weights,0
Kit TODO : need to search the tf,1
"name, op",0
input edge,0
for target shape,0
"name, op",0
input edge,0
axis,0
"Parameter arrangement in Keras: gamma, beta, mean, variance",0
scale,0
beta,0
mean,0
var,0
"name, op",0
input edge,0
for target shape,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
rot weights,0
"print(""Converting layer {}({})"".format(current_node.name, node_type))",0
TODO,1
"return input_node, 'same'",0
"assert IR_node.get_attr('group', 1) == 1",0
change dw from filters to 1,0
reset the default dilation,0
############,0
Operators #,0
############,0
TODO,1
"when converting from coreml model, reshape is needed after the global pooling",0
for Keras,0
TODO: arguments won't be saved in keras export model,1
print(IR_node.layer),0
print(IR_node.layer),0
Prepare broadcasting shape.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
Kit: TODO,1
Duplicate models for weight sharing,0
Expand the sub-models,0
console.info(JSON.parse(e.target.result)),0
"default, draw the model.json file",0
window.onload = () => {,0
"d3.json(filePath, (error, json) => {",0
if (error) throw error;,0
// console.info(json),0
draw(json),0
}),0
},0
generate dag,0
IR model or keras model,0
path generator,0
let arrowL = `M ${points[len - 1].x} ${points[len - 1].y} l -7 -7`,0
let arrowR = `M ${points[len - 1].x} ${points[len - 1].y} l  7 -7`,0
select a layer,0
remove list,0
draw,0
mode;,0
"let scale = Math.min(window.innerWidth*0.85 / width, (window.innerHeight-85)/height)",0
"var filter = def.append(""filter"")",0
".attr(""id"", ""dropshadow"")",0
"filter.append(""feGaussianBlur"")",0
".attr(""in"", ""SourceAlpha"")",0
".attr(""stdDeviation"", 4)",0
".attr(""result"", ""blur"");",0
"// filter.append(""feOffset"")",0
"//     .attr(""in"", ""blur"")",0
"//     .attr(""dx"", 2)",0
"//     .attr(""dy"", 2)",0
"//     .attr(""result"", ""offsetBlur"");",0
"var feMerge = filter.append(""feMerge"");",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""offsetBlur"")",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""SourceGraphic"");",0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
"svg.on('keydown', ()=>{console.info('ddd')})",0
".on('mouseover', ()=>{console.info('mouse over')})",0
k = parseFloat(k)*parseFloat(k_),0
"limit x, y",0
// a trick to make text svg transform in MS Edge,0
"d3.selectAll("".labels"").classed(""tempclass"", true);",0
"setTimeout(function () { d3.selectAll("".labels"").classed(""tempclass"", false); }, 40);",0
when shift is down,0
ulti function,0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
rm the '_',0
Base64 encode: https://developers.google.com/protocol-buffers/docs/proto3,0
Search the node,0
select by id: https://stackoverflow.com/questions/37270787/uncaught-syntaxerror-failed-to-execute-queryselector-on-document,0
Scroll,0
console.info(JSON.parse(e.target.result)),0
"default, draw the model.json file",0
window.onload = () => {,0
"d3.json(filePath, (error, json) => {",0
if (error) throw error;,0
// console.info(json),0
draw(json),0
}),0
},0
generate dag,0
IR model or keras model,0
path generator,0
let arrowL = `M ${points[len - 1].x} ${points[len - 1].y} l -7 -7`,0
let arrowR = `M ${points[len - 1].x} ${points[len - 1].y} l  7 -7`,0
select a layer,0
draw,0
"let scale = Math.min(window.innerWidth*0.85 / width, (window.innerHeight-85)/height)",0
"var filter = def.append(""filter"")",0
".attr(""id"", ""dropshadow"")",0
"filter.append(""feGaussianBlur"")",0
".attr(""in"", ""SourceAlpha"")",0
".attr(""stdDeviation"", 4)",0
".attr(""result"", ""blur"");",0
"// filter.append(""feOffset"")",0
"//     .attr(""in"", ""blur"")",0
"//     .attr(""dx"", 2)",0
"//     .attr(""dy"", 2)",0
"//     .attr(""result"", ""offsetBlur"");",0
"var feMerge = filter.append(""feMerge"");",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""offsetBlur"")",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""SourceGraphic"");",0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
"svg.on('keydown', ()=>{console.info('ddd')})",0
".on('mouseover', ()=>{console.info('mouse over')})",0
k = parseFloat(k)*parseFloat(k_),0
"limit x, y",0
// a trick to make text svg transform in MS Edge,0
"d3.selectAll("".labels"").classed(""tempclass"", true);",0
"setTimeout(function () { d3.selectAll("".labels"").classed(""tempclass"", false); }, 40);",0
when shift is down,0
ulti function,0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
space = '',0
Get the long description from the README file,0
Versions should comply with PEP440.  For a discussion on single-sourcing,0
"the version across setup.py and the project code, see",0
https://packaging.python.org/en/latest/single_source_version.html,0
The project's main homepage.,0
Author details,0
Choose your license,0
See https://pypi.python.org/pypi?%3Aaction=list_classifiers,0
How mature is this project? Common values are,0
3 - Alpha,0
4 - Beta,0
5 - Production/Stable,0
Indicate who your project is intended for,0
"Pick your license as you wish (should match ""license"" above)",0
"Specify the Python versions you support here. In particular, ensure",0
"that you indicate whether you support Python 2, Python 3 or both.",0
What does your project relate to?,0
You can just specify the packages manually here if your project is,0
simple. Or you can use find_packages().,0
"Alternatively, if you want to distribute just a my_module.py, uncomment",0
this:,0
"py_modules=[""my_module""],",0
List run-time dependencies here.  These will be installed by pip when,0
"your project is installed. For an analysis of ""install_requires"" vs pip's",0
requirements files see:,0
https://packaging.python.org/en/latest/requirements.html,0
"To provide executable scripts, use entry points in preference to the",0
"""scripts"" keyword. Entry points provide cross-platform support and allow",0
pip to create the appropriate form of executable for the target platform.,0
"Function self.assertEquals has deprecated, change to assertEqual",0
"self.assertEqual(original_predict.shape, converted_predict.shape)",0
get original model prediction result,0
original to IR,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
print(original_predict),0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
IR to code,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
The list is in RGB oder,0
IR to Model,0
"converted_file = original_framework + '_coreml_' + architecture_name + ""_converted""",0
"converted_file = converted_file.replace('.', '_')",0
load model,0
save model,0
"coremltools.utils.save_spec(model.get_spec(), converted_file)",0
inference,0
IR to code,0
import converted model,0
"'alexnet'       : [CntkEmit, KerasEmit, TensorflowEmit],",0
"'inception_resnet_v2' : [TensorflowEmit], # TODO PytorchEmit",1
get original model prediction result,0
do not deal,0
I don't want to deal with auto_pad,0
Don't support auto_pad current!,0
2018-02-28,0
if group is None:,0
group = 1,0
group is not support yet too.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
node,0
input,0
edge(node a <-> node b),0
,0
"key is edge name, value is src/dst node name",0
"key is initializer name, value is TensorProto",0
print(name),0
print(layer.op_type),0
n is input data,0
n is input edge,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"if node len(in_edges), generate additional DataInput node",0
print,0
print (dir(data)),0
print (i),0
"print (""    {} with shape {}"".format(j, load_weight[i][j].shape))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Base Functions,0
"padding,",0
"exstr = "", value=float('-Inf')""",0
"exstr = """"",0
"input_node = self._defuse_padding(IR_node, exstr)",0
Ignore it in Pytorch,0
for Keras,0
"self.add_init(2, ""self.{:<15} = torch.from_numpy(__weights_dict['{}']['value'])"".format(",0
"IR_node.variable_name,",0
IR_node.name)),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self._type = layer.__class__.__name__.replace('Backward', '')",0
"self._name = ""{}_{}"".format(self.type, id)",0
TODO,1
"""""""",0
build graph for pytorch 0.2.0,0
"""""""",0
dummy_input = torch.autograd.Variable(torch.randn(shape)),0
output_node = self.model(dummy_input),0
search_queue = [output_node.grad_fn],0
"tmp_node = PyTorchGraphNode(output_node.grad_fn, 0)",0
self.layer_map[tmp_node.name] = tmp_node,0
visited = {output_node.grad_fn : self.layer_map[tmp_node.name]},0
idx = 0,0
node_count = 1,0
while (idx < len(search_queue)):,0
current_node = search_queue[idx],0
current_type = visited[current_node].type,0
"if hasattr(current_node, 'next_functions'):",0
"for parent, _ in current_node.next_functions:",0
"parent_type = parent.__class__.__name__.replace('Backward', '')",0
if parent_type != 'AccumulateGrad' and \,0
(parent_type != 'Transpose' or current_type != 'Addmm'):,0
if not parent in visited:,0
"tmp_node = PyTorchGraphNode(parent, node_count)",0
self.layer_map[tmp_node.name] = tmp_node,0
node_count += 1,0
visited[parent] = tmp_node,0
search_queue.append(parent),0
"self._make_connection(visited[parent].name, visited[current_node].name)",0
idx += 1,0
try:,0
return TorchGraph._forward_torch_random_input(,0
"torch_model,",0
"input_shapes,",0
is_batch=False,0
),0
except:,0
# try batch mode,0
# return TorchGraph._forward_torch_random_input(,0
"#     torch_model,",0
"#     input_shapes,",0
#     is_batch=True,0
# ),0
pass,0
tensor = torch.rand(*shape).float(),0
multi output,0
single output,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
###########,0
property #,0
###########,0
###################,0
Public Functions #,0
###################,0
Build network graph,0
#########,0
Layers #,0
#########,0
handle weight,0
handle bias,0
handle weight,0
handle bias,0
###################,0
Helper Functions #,0
###################,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
load model file into Coreml Graph,0
model.encode() convert to str --- python2 may crash due to type 'unicode',0
Build Network Graph,0
self.data_format ? TODO,1
load the model network,0
convert to Model_pb2.Model,0
determine the type of the current_node,0
staticmethods,0
"(seq, batch, C, H, W)  & NHWC",0
TODO dtype_map,1
##### convert methods,0
convolution,0
input edge,0
important!,0
reshape the weight!,0
"[2, 3, 0, 1]",0
pads,0
use_bias,0
isDeconvolution,0
"name, op",0
kwargs['kernel_shape'] = weights.shape,0
strides,0
"[1, sd, sh, sw, 1]",0
activation,0
TODO,1
padding in conv,0
"pad in IR is [x1_b, x2_b, ..., x1_e, x2_e, ...]",0
padding,0
compute padding for 'same',0
https://discuss.mxnet.io/t/pooling-and-convolution-with-same-mode/528/3,0
https://www.jianshu.com/p/05c4f1621c7e,0
padding in pooling,0
padding,0
https://discuss.mxnet.io/t/pooling-and-convolution-with-same-mode/528/3,0
TODO,1
symmetric padding,0
"name, op",0
input edge,0
padding,0
strides,0
"[1, sd, sh, sw, 1]",0
window_shape,0
"[1, pd, ph, pw, 1]",0
"name, op",0
input edge,0
For concat axis,0
"NO axis in coreml, so set the last axis",0
"The first -1 means in coreml there is one-more axis,",0
The second -1 means the last axis,0
"name, op",0
input edge,0
padding,0
Future Module TODO,1
#### rename methods,0
"name, op",0
activation type,0
else:,0
assert False,0
input edge,0
Merge layers,0
only for training,0
"name, op",0
input edge,0
shape,0
NHWC channel last,0
"in fact, here is NWHC",0
"name, op",0
input edge,0
axis TODO,0
"channels_first, then axis = 1",0
scale,0
bias,0
epsilon,0
compute adjusted parameters,0
Reference: parameter transformation https://github.com/apple/coremltools/issues/153,0
convert type because of tensorflow,0
mean,0
var,0
"name, op",0
input edge,0
bias,0
"name, op",0
input edge,0
"MAX = 0, AVERAGE = 1, L2 = 2",0
padding,0
strides,0
"[1, sd, sh, sw, 1]",0
window_shape,0
"[1, pd, ph, pw, 1]",0
"name, op",0
input edge,0
"name, op",0
input edge,0
"name, op",0
input edge,0
units,0
use_bias,0
weights,0
"name, op",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
the end of the tensorflow_resnet_v2_50's squeeze shape is [unknown_rank: true] with len 0,0
1001 means the 1001 classes for tensorflow_resnet_v2_50,0
!Alert! TODO,1
Future implement can be changed to the last two layer,0
TODO - remove style transfer 1D hack,1
"Input is 1D but it goes to the width dimension: (1,1,W)",0
"assume (Batch, Channels) - Batch dimension should be dropped",0
"assume (Batch, Sequence-Length, channels)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Add classifier classes (if applicable),0
Set pre-processing paramsters,0
"image_input_names,",0
Return the protobuf spec,0
model = _MLModel(self.builder.spec),0
Get input and output names,0
Dimensions and weights,0
"W should have shape (height, width, kernel_channels, output_channels), where kernel_channel = input_channels / groups",0
Dilations,0
see protobuf,0
assert False,0
Get the weights.,0
Dilations,0
see protobuf,0
depth-wise convolution,0
Dilations,0
see protobuf,0
Get input and output names,0
Pooling layer type,0
"if it's global, set the global flag",0
TODO  global pooling modification,0
Padding,0
see protobuf,0
Padding,0
see protobuf,0
Get input and output names,0
"type: ""list(int). A 1-D values of (leftBorder, topBorder, rightBorder, bottomBorder).""",0
This is central crop,0
Now add the layer,0
"Allowed values: 'CHW', 'HW', 'C', 'H', 'W'",0
Get input and output names,0
Get input and output names,0
Get the weights from keras,0
Get input and output names,0
blob_order == 0 if the input blob needs not be rearranged,0
blob_order == 1 if the input blob needs to be rearranged,0
"using keras_layer.input.shape have a ""?"" (Dimension[None] at the front),",0
making a 3D tensor with unknown batch size 4D,0
Get input and output names,0
activation emit,0
Get input and output names,0
negate it,0
apply threshold,0
negate it back,0
for Keras,0
Get input and output names,0
Set parameters,0
"Parameter arrangement in Keras: gamma, beta, mean, variance",0
compute adjusted parameters,0
Reference: parameter transformation https://github.com/apple/coremltools/issues/153,0
padding type TODO,0
"Type of the padding. Can be one of 'constant', 'reflection' or 'replication",0
Now add the layer,0
self.emit_Flatten(IR_node),0
"depth_radius: Half-width of the 1-D normalization window.""",0
Get the weights,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
input layer,0
build each layer,0
"if A.output == B.input, then make the connection: A -> B",0
"print('{0:20}->     {1:20}'.format(layerA.name, layerB.name))",0
"if A.name == B.input, then make the connection: A -> B, here A is the input",0
build each layer,0
"if A.output == B.input, then make the connection: A -> B",0
"print('{0:20}->     {1:20}'.format(layerA.name, layerB.name))",0
"if A.name == B.input, then make the connection: A -> B, here A is the input",0
build each layer,0
"if A.output == B.input, then make the connection: A -> B",0
"print('{0:20}->     {1:20}'.format(layerA.name, layerB.name))",0
"if A.name == B.input, then make the connection: A -> B, here A is the input",0
The information of the layer,0
Generated by the protocol buffer compiler.  DO NOT EDIT!,0
source: caffe.proto,0
@@protoc_insertion_point(imports),0
@@protoc_insertion_point(class_scope:caffe.BlobShape),0
@@protoc_insertion_point(class_scope:caffe.BlobProto),0
@@protoc_insertion_point(class_scope:caffe.BlobProtoVector),0
@@protoc_insertion_point(class_scope:caffe.Datum),0
@@protoc_insertion_point(class_scope:caffe.FillerParameter),0
@@protoc_insertion_point(class_scope:caffe.NetParameter),0
@@protoc_insertion_point(class_scope:caffe.SolverParameter),0
@@protoc_insertion_point(class_scope:caffe.SolverState),0
@@protoc_insertion_point(class_scope:caffe.NetState),0
@@protoc_insertion_point(class_scope:caffe.NetStateRule),0
@@protoc_insertion_point(class_scope:caffe.ParamSpec),0
@@protoc_insertion_point(class_scope:caffe.LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.TransformationParameter),0
@@protoc_insertion_point(class_scope:caffe.LossParameter),0
@@protoc_insertion_point(class_scope:caffe.AccuracyParameter),0
@@protoc_insertion_point(class_scope:caffe.ArgMaxParameter),0
@@protoc_insertion_point(class_scope:caffe.ConcatParameter),0
@@protoc_insertion_point(class_scope:caffe.BatchNormParameter),0
@@protoc_insertion_point(class_scope:caffe.BiasParameter),0
@@protoc_insertion_point(class_scope:caffe.ContrastiveLossParameter),0
@@protoc_insertion_point(class_scope:caffe.ConvolutionParameter),0
@@protoc_insertion_point(class_scope:caffe.CropParameter),0
@@protoc_insertion_point(class_scope:caffe.DataParameter),0
@@protoc_insertion_point(class_scope:caffe.DropoutParameter),0
@@protoc_insertion_point(class_scope:caffe.DummyDataParameter),0
@@protoc_insertion_point(class_scope:caffe.EltwiseParameter),0
@@protoc_insertion_point(class_scope:caffe.ELUParameter),0
@@protoc_insertion_point(class_scope:caffe.EmbedParameter),0
@@protoc_insertion_point(class_scope:caffe.ExpParameter),0
@@protoc_insertion_point(class_scope:caffe.FlattenParameter),0
@@protoc_insertion_point(class_scope:caffe.HDF5DataParameter),0
@@protoc_insertion_point(class_scope:caffe.HDF5OutputParameter),0
@@protoc_insertion_point(class_scope:caffe.HingeLossParameter),0
@@protoc_insertion_point(class_scope:caffe.ImageDataParameter),0
@@protoc_insertion_point(class_scope:caffe.InfogainLossParameter),0
@@protoc_insertion_point(class_scope:caffe.InnerProductParameter),0
@@protoc_insertion_point(class_scope:caffe.InputParameter),0
@@protoc_insertion_point(class_scope:caffe.LogParameter),0
@@protoc_insertion_point(class_scope:caffe.LRNParameter),0
@@protoc_insertion_point(class_scope:caffe.MemoryDataParameter),0
@@protoc_insertion_point(class_scope:caffe.MVNParameter),0
@@protoc_insertion_point(class_scope:caffe.ParameterParameter),0
@@protoc_insertion_point(class_scope:caffe.PoolingParameter),0
@@protoc_insertion_point(class_scope:caffe.PowerParameter),0
@@protoc_insertion_point(class_scope:caffe.PythonParameter),0
@@protoc_insertion_point(class_scope:caffe.RecurrentParameter),0
@@protoc_insertion_point(class_scope:caffe.ReductionParameter),0
@@protoc_insertion_point(class_scope:caffe.ReLUParameter),0
@@protoc_insertion_point(class_scope:caffe.ReshapeParameter),0
@@protoc_insertion_point(class_scope:caffe.ScaleParameter),0
@@protoc_insertion_point(class_scope:caffe.SigmoidParameter),0
@@protoc_insertion_point(class_scope:caffe.SliceParameter),0
@@protoc_insertion_point(class_scope:caffe.SoftmaxParameter),0
@@protoc_insertion_point(class_scope:caffe.TanHParameter),0
@@protoc_insertion_point(class_scope:caffe.TileParameter),0
@@protoc_insertion_point(class_scope:caffe.ThresholdParameter),0
@@protoc_insertion_point(class_scope:caffe.WindowDataParameter),0
@@protoc_insertion_point(class_scope:caffe.SPPParameter),0
@@protoc_insertion_point(class_scope:caffe.V1LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.V0LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.PReLUParameter),0
@@protoc_insertion_point(module_scope),0
"TODO: We need to identify whether this is 4D image data, otherwise we shouldn't change the dimension order",1
print(node.parameters),0
assert False,0
kwargs['gamma'] = 0.25,0
"Stochastic pooling, for instance.",0
TODO: Axis,0
TODO: Unbiased,1
check if need the Flatten layer,0
if parent.output_shape.height > 1 or parent.output_shape.width > 1:,0
TODO: The gamma parameter has to be set (in node.data?) and this should work.,1
"Also, mean should be set to 0, and var to 1, just to be safe.",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"For some reason argparser gives us unicode, so we need to conver to str first",0
for test,0
"with open(""graph.txt"", 'w') as f:",0
for layer in self.IR_graph.topological_sort:,0
current_node = self.IR_graph.get_node(layer),0
"print(""========current_node=========\n{}"".format(current_node.layer), file=f)",0
test end,0
"print(""========current_node={}"".format(current_node.layer))",0
check if have pad layer,0
keys = [],0
for key in self.weights_dict[IR_node.name].keys():,0
keys.append(key),0
"print(""=======Layer: {}, keys: {}"".format(IR_node.name, keys))",0
Change the layer name,0
check if need crop output shape,0
"change the key ""name"" to ""variable_name"", in case of the layer name has invalid characters",0
"def emit_Tanh(self, IR_node):",0
"self._emit_activation(IR_node, 'ops.tanh')",0
"Convert NHWC (IR) to NCHW (Caffe): [0,1,2,3]->[0,3,1,2]",0
Caffe Types,0
Same value applies to all spatial dimensions,0
Extract the value for the given spatial dimension,0
consider rewrite this function to Network.py,0
Dropout layers appear in a fair number of Caffe,0
test-time networks. These are just ignored. We'll,0
filter them out here.,0
TODO: raise error,1
TODO: raise error,1
Automatically set a name if not provided.,0
Figure out the layer inputs.,0
"print('op: %s   shape: %s' % (op, layer_output._keras_shape))",0
"print('op: %s   shape: %s' % (op, layer_output.get_shape().as_list()))",0
Add to layer LUT.,0
Old-style input specification,0
New-style input specification,0
"We most likely have a data layer on our hands. The problem is,",0
Caffe infers the dimensions of the data from the source (eg: LMDB).,0
We want to avoid reading datasets here. Fail for now.,0
This can be temporarily fixed by transforming the data layer to,0
"Caffe's ""input"" layer (as is usually used in the ""deploy"" version).",0
TODO: Find a better solution for this.,1
The .prototxt file defining the graph,0
The .caffemodel file containing the learned parameters,0
Set to true if the fallback protocol-buffer based backend was used,0
"A list containing (layer name, parameters) tuples",0
Load the parameters,0
"When using the protobuf-backend, each parameter initially has four dimensions.",0
"In certain cases (like FC layers), we want to eliminate the singleton dimensions.",0
"This implementation takes care of the common cases. However, it does leave the",0
potential for future issues.,0
The Caffe-backend does not suffer from this problem.,0
A dictionary mapping NodeKind to the transposed order.,0
The node kinds eligible for reshaping,0
"If true, the reshaped data will replace the old one.",0
"Otherwise, it's set to the reshaped_data attribute.",0
Check for 2+ dimensional data,0
The FC layer connected to the spatial layer needs to be,0
re-wired to match the new spatial ordering.,0
node.reshaped_data = weights.transpose(transpose_order),0
Set the weights,0
We're only fusing nodes with single parents,0
We can only fuse a node if its parent's,0
value isn't used by any other node.,0
Rewrite the fused node's children to its parent.,0
Disconnect the fused node from the graph.,0
Let the sub-class merge the fused node in any arbitrary way.,0
Fuse ReLUs when the parent node is one of the given types.,0
"If None, all node types are eligible.",0
Prescale the stats,0
Replace with the updated values,0
Include the scale and bias terms,0
self.graph = NodeRenamer()(graph),0
"return Graph(name, [self.map_node(node) for node in self.graph.nodes])",0
Kit,0
"mapped_node.input.extend(['%s:%s' % (input.name, idx) for input, idx in node.parents])",0
FIXME:,0
output = node.output,0
Decompose DAG into chains,0
Generate Python code line by line,0
try:,0
except ImportError:,0
# # Fall back to the protobuf implementation,0
# from mmdnn.conversion.caffe import caffe_pb2,0
# self.caffepb = caffe_pb2,0
# show_fallback_warning(),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
from tensorflow.core.framework.node_def_pb2 import NodeDef,0
from tensorflow.core.framework import attr_value_pb2,0
pass,0
"print(""\n"")",0
print(i),0
print(block),0
continue,0
NHWC,0
assert False,0
print(shortcut_layer['input'] ),0
spacetodepth,0
print(block),0
print(region_layer),0
assert False,0
print line,0
set default value,0
load weight by original order,0
print(node_type),0
print(IR_node),0
assert False,0
strides,0
assert False,0
padding,0
only load weight conv,0
"print(""----------------"",self.start)",0
print(kernel.shape),0
print(k_bias.shape),0
"buf, start, scale_layer['name'], bn_layer['name'], conv_layer['name']",0
"print(""=============="",self.start)",0
print(bias.shape),0
print(scale.shape),0
print(mean.shape),0
print(var.shape),0
print(kernel.shape),0
print(W),0
assert False,0
no use,0
print(source_node.layer),0
for image classification(resnet) AVG pooling,0
print(source_node.layer),0
print(source_node.layer),0
kwargs['ignore_thresh'] = source_node.get_attr('ignore_thresh'),0
print(source_node.get_attr('anchors')),0
"kwargs['anchors'] = ['0.52','0.22']",0
kwargs['mask'] = source_node.get_attr('mask'),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Reference: 1) Keras image preprocess: https://github.com/keras-team/keras/blob/master/keras/applications/imagenet_utils.py,0
2) tensorflow github issue: https://github.com/tensorflow/models/issues/517,0
"R-G-B for Imagenet === [123.68, 116.78, 103.94]",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result(None, False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
layer_name = 'block2_pool',0
"intermediate_layer_model = keras.Model(inputs=model.input,",0
outputs=model.get_layer(layer_name).output),0
intermediate_output = intermediate_layer_model.predict(img),0
print (intermediate_output),0
print (intermediate_output.shape),0
"print (""%.30f"" % np.sum(intermediate_output))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Load a real image and do default tf imageNet preprocessing,0
"[Batch, Height, Width, Channels] --> [Batch, Channels, Height, Width]",0
We only deal with non-recurrent networks for now,0
"(H,W,C) --> (C,H,W)",0
Load TensorFlow model,0
Prepare inputs,0
Run TF session,0
Evaluate coreml model,0
evaluate the TF model,0
evaluate CoreML,0
Test the default CoreML evaluation,0
Download model,0
Convert to coreml,0
Test predictions on an image,0
Download model,0
Convert to coreml,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
"@unittest.skip(""Failing GPU backend: related to https://github.com/tf-coreml/tf-coreml/issues/26"")",0
? style transfer image size and style number?,0
Test predictions on an image,0
evaluate the TF model,0
evaluate CoreML,0
Test the default CoreML evaluation,0
"compare_tf_coreml_outputs(tf_out_flatten, coreml_out_flatten)",0
#Download model,0
url = 'https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz',0
tf_model_dir = _download_file(url = url),0
"tf_model_path = os.path.join(TMP_MODEL_DIR, 'inception_v3_2016_08_28_frozen.pb')",0
#Convert to coreml,0
"mlmodel_path = os.path.join(TMP_MODEL_DIR, 'inception_v3_2016_08_28.mlmodel')",0
mlmodel = tf_converter.convert(,0
"tf_model_path = tf_model_path,",0
"mlmodel_path = mlmodel_path,",0
"output_feature_names = ['InceptionV3/Predictions/Softmax:0'],",0
"input_name_shape_dict = {'input:0':[1,299,299,3]},",0
"image_input_names = ['input:0'],",0
"red_bias = -1,",0
"green_bias = -1,",0
"blue_bias = -1,",0
image_scale = 2.0/255.0),0
#Test predictions on an image,0
_test_coreml_model_image_input(,0
"tf_model_path = tf_model_path,",0
"coreml_model = mlmodel,",0
"input_tensor_name = 'input:0',",0
"output_tensor_name = 'InceptionV3/Predictions/Softmax:0',",0
img_size = 299),0
Download model,0
Convert to coreml,0
Test predictions on an image,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
self.test_truth(),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
from collections import namedtuple,0
"Batch = namedtuple('Batch', ['data'])",0
TODO,1
Apple has published some of their own models. They can be downloaded from https://developer.apple.com/machine-learning/.,0
"Those published models are: SqueezeNet, Places205-GoogLeNet, ResNet50, Inception v3, VGG16",0
TODO,1
load model,0
TODO: Multiple inputs,1
TODO: Multiple outputs,1
inference,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
caffe uses NCHW,0
"self.print_intermediate_result('pooling0', False)",0
delete tmp model files,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
pylint: disable=line-too-long,0
pylint: enable=line-too-long,0
"net = caffe.Net(arch_fn, weight_fn, caffe.TEST)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
# Image Classification,0
# Semantic Segmentation,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
print(r),0
d = darknet_extractor(),0
model_filename = d.download('yolov3'),0
print(model_filename),0
"image_path = ""./mmdnn/conversion/examples/data/dog.jpg""",0
"model_path = ""./""",0
d = darknet_extractor(),0
"result = d.inference('yolov3', model_filename, model_path, image_path = image_path)",0
print(result),0
print(i),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result('pooling0', False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
need to be updated,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.input, self.model, self.testop = self.MainModel.KitModel(self.args.w)",0
testop = tf.get_default_graph().get_operation_by_name(layer_name),0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"'mobilenet_v1': mobilenet_v1.mobilenet_v1_arg_scope,",0
"'mobilenet_v1' : mobilenet_v1.mobilenet_v1,",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"tf.train.export_meta_graph(""kit.meta"", as_text=True)",0
"writer = tf.summary.FileWriter('./graphs', sess.graph)",0
writer.close(),0
raise NotImplementedError(),0
load file,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Notes for training NASNet Cifar Model,0
-------------------------------------,0
batch_size: 32,0
learning rate: 0.025,0
cosine (single period) learning rate decay,0
auxiliary head loss weighting: 0.4,0
clip global norm of all gradients by 5,0
600 epochs with a batch size of 32,0
This is used for the drop path probabilities since it needs to increase,0
the drop out probability over the course of training.,0
Notes for training large NASNet model on ImageNet,0
-------------------------------------,0
batch size (per replica): 16,0
learning rate: 0.015 * 100,0
learning rate decay factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 100 replicas,0
auxiliary head loss weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Notes for training the mobile NASNet ImageNet model,0
-------------------------------------,0
batch size (per replica): 32,0
learning rate: 0.04 * 50,0
learning rate scaling factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 50 replicas,0
auxiliary head weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Shape of feature map before the final layer.,0
149 x 149 x 32,0
Run the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Find where to place the reduction cells or stride normal cells,0
Setup for building in the auxiliary head.,0
Run the cells,0
true_cell_num accounts for the stem cells,0
Final softmax layer,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80,0
71 x 71 x 192,0
35 x 35 x 192,0
35 x 35 x 320,0
TODO(alemi): Register intermediate endpoints,1
"17 x 17 x 1088 if output_stride == 8,",0
33 x 33 x 1088 if output_stride == 16,0
TODO(alemi): register intermediate endpoints,1
TODO(gpapan): Properly support output_stride for the rest of the net.,1
8 x 8 x 2080,0
TODO(alemi): register intermediate endpoints,1
8 x 8 x 1536,0
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which",1
can be set to False to disable pooling here (as in resnet_*()).,0
Set weight_decay for weights in conv2d and fully_connected layers.,0
Set activation_fn and parameters for batch_norm.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
=============================================================================,0
Tensorflow mandates these.,0
Conv and DepthSepConv namedtuple define layers of the MobileNet architecture,0
Conv defines 3x3 convolution layers,0
DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.,0
stride is the stride of the convolution,0
depth is the number of channels or filters in a layer,0
_CONV_DEFS specifies the MobileNet body,0
Used to find thinned depths for each layer.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
By passing filters=None,0
separable_conv2d produces only a depthwise convolution layer,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 1024,0
Set weight_decay for weights in Conv and DepthSepConv layers.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Skip path 1,0
Skip path 2,0
"First pad with 0's on the right and bottom, then shift the filter to",0
include those 0's that were added.,0
Concat and apply BN,0
Set the prev layer to the current layer if it is none,0
Check to be sure prev layer stuff is setup correctly,0
Apply conv operations,0
Combine hidden states using 'add'.,0
Add hiddenstate to the list of hiddenstates we can choose from,0
Dont stride if this is not one of the original hiddenstates,0
"Check if a stride is needed, then use a strided 1x1 here",0
Determine if a reduction should be applied to make the number of,0
filters match.,0
Return the concat of all the states,0
Scale keep prob by layer number,0
The added 2 is for the reduction cells,0
Decrease the keep probability over time,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
Only insert rate to params if rate > 1.,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Copyright (c) Microsoft. All rights reserved.,0
Licensed under the MIT license. See LICENSE.md file in the project root,0
for full license information.,0
==============================================================================,0
"self.model, self.testop = self.MainModel.KitModel(self.args.w)",0
"self.print_intermediate_result(None, False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
pylint: disable=line-too-long,0
pylint: enable=line-too-long,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.anchors = np.array([[10,13], [16,30],[33,23],[30,61],[62,45], [59,119],[116,90],[156,198],[373,326]])",0
print(output),0
"print(out_boxes, out_scores, out_classes)",0
get random colors,0
My kingdom for a good redistributable image drawing library.,0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
yolov3 80 classes,0
"Reshape to batch, height, width, num_anchors, box_params.",0
Adjust preditions to each spatial grid point and anchor size.,0
Note: YOLO iterates over height index before width index.,0
"TODO: It works with +1, don't know why.",1
Scale boxes back to original image shape.,0
"print(""feats,anchors, num_classes, input_shape"", feats, anchors, num_classes, input_shape)",0
"print(box_xy, box_wh, box_confidence, box_class_probs)",0
"yolo_outputs order 13,26,52",0
TODO: use keras backend instead of tf.,1
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
save network structure as JSON,0
layer_name = 'block2_pool',0
"intermediate_layer_model = keras.Model(inputs=model.input,",0
outputs=model.get_layer(layer_name).output),0
intermediate_output = intermediate_layer_model.predict(img),0
print (intermediate_output),0
print (intermediate_output.shape),0
"print (""%.30f"" % np.sum(intermediate_output))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Convert network architecture only,0
MXNet,0
Caffe,0
For CoreML,0
assert args.network or args.frozen_pb,0
Caffe,0
Darknet,0
"exts = ['.pb', '.npy', '.py']",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check non-sense always input module.Module,0
if not (type(model) == mx.module.Module,0
or type(model) == mx.module.SequentialModule,0
or type(model) == mx.model),0
"raise TypeError(""MXNet layer of type %s is not supported."" % type(model))",0
"if layer[""op""] == ""null"":",0
continue,0
"raise NotImplementedError(""Cannot support multi-input"")",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"mxnet-cpu only support channel first, default convert the model and weight as channel first",0
download and show the image,0
"convert into format (batch, RGB, width, height)",0
"to show the image, change the argument show into True",0
compute the predict probabilities,0
print the top-5,0
# call function predict,0
"with open('synset.txt', 'r') as f:",0
labels = [l.rstrip() for l in f],0
"predict(model, labels, 'http://writm.com/wp-content/uploads/2016/08/Cat-hd-wallpapers.jpg')",0
"print(""Warning: MXNet Convolution Layer pad does not match IR Convolution Layer pad"")",0
"layout = IR_node.IR_layer.attr[""data_format""].s",0
if layout not in MXNetEmitter.channels_last:,0
Add LeakyReLU Elu(slope not support),0
"axis = IR_node.IR_layer.attr[""axis""].i",0
not supported yet,0
"axis = IR_node.IR_layer.attr[""axis""].i",0
not supported yet,0
"print(""Warning: MXNet Pooling Layer pad does not match IR Pooling Layer pad"")",0
"def emit_ConvTranspose(self, IR_node):",0
if self.weight_loaded:,0
weight_dict = self.weights[IR_node.name],0
weights = weight_dict['weights'],0
"dim = len(IR_node.IR_layer.attr[""kernel_shape""].list.i) - 2",0
kernel = list(),0
"for idx in range(0, dim):",0
"kernel.append(IR_node.IR_layer.attr[""kernel_shape""].list.i[idx])",0
stride = list(),0
"for e in IR_node.IR_layer.attr[""strides""].list.i[1:-1]:",0
stride.append(e),0
dilate = list(),0
"for e in IR_node.IR_layer.attr[""dilations""].list.i[1:-1]:",0
dilate.append(e),0
"dilate = ', '.join('%s' % i for i in dilate)",0
defuse_pad = False,0
pad = list(),0
"if ""pads"" in IR_node.IR_layer.attr:",0
output_shape = list(),0
"for e in IR_node.IR_layer.attr[""_output_shapes""].list.shape[0].dim:",0
output_shape.append(e.size),0
"# print(""Warning: MXNet Deconvolution Layer pad does not match IR Deconvolution Layer pad"")",0
"defuse_pad, pad = MXNetEmitter.transfer_pad(IR_node.IR_layer.attr[""pads""].list.i)",0
"pad = ', '.join('%s' % i for i in pad)",0
"kernel = ', '.join('%s' % i for i in kernel)",0
"stride = ', '.join('%s' % i for i in stride)",0
"num_filter = IR_node.IR_layer.attr[""kernel_shape""].list.i[-2]",0
"no_bias = not IR_node.IR_layer.attr[""use_bias""].b",0
if not no_bias and self.weight_loaded:,0
"self.output_weights[IR_node.replace_scope(IR_node.name) + ""_bias""] = weight_dict['bias']",0
"# layout = IR_node.IR_layer.attr[""data_format""].s",0
if dim == 1:,0
layout = 'NCW',0
elif dim == 2:,0
layout = 'NCHW',0
elif dim == 3:,0
layout = 'NCDHW',0
if self.weight_loaded:,0
# if layout not in MXNetEmitter.channels_last:,0
"weights = MXNetEmitter.transpose(weights, dim)",0
"self.output_weights[IR_node.replace_scope(IR_node.name) + ""_weight""] = weights",0
"code = """"",0
if not defuse_pad:,0
"code = ""{:<15} = mx.sym.Deconvolution(data = {}, kernel = ({}), stride = ({}), dilate = ({}), pad = ({}), num_filter = {}, no_bias = {}, layout = '{}', name = '{}')"".format(",0
"IR_node.replace_scope(IR_node.name),",0
"IR_node.replace_scope(IR_node.in_edges[0]),",0
"kernel,",0
"stride,",0
"dilate,",0
"pad,",0
"num_filter,",0
"no_bias,",0
"layout,",0
IR_node.replace_scope(IR_node.name)),0
else:,0
"code = self.set_pad(IR_node, code, pad)",0
"code += ""\n    {:<15} = mx.sym.Deconvolution(data = {}, kernel = ({}), stride = ({}), dilate = ({}), num_filter = {}, no_bias = {}, layout = '{}', name = '{}')"".format(",0
"IR_node.replace_scope(IR_node.name), IR_node.replace_scope(IR_node.name) + ""_pad"", kernel, stride, dilate, num_filter, no_bias, layout, IR_node.replace_scope(IR_node.name))",0
return code,0
reverse cannot support yet,0
"code = ""{:<15} = mx.sym.transpose(data = {}, axes = (0, 2, 3, 1))\n"".format(""trans"", self.parent_variable_name(IR_node))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Not support yet,0
"""softrelu""  : ""SoftReLU""",0
Load the model network and weights,0
"digraph = mx.viz.plot_network(sym, save_format='jpg') # For debugging",0
digraph.render(),0
model.bind(data_shapes = data_shapes),0
model.init_params(),0
"mod.load(model_path, epoch_num)",0
return mod.get_params(),0
load the model network,0
adjust the data format,0
load model files into MXNet graph,0
data_shape arguments added to calculate infer_shape(required),0
"if isinstance(input_arg, basestring):",0
Build network graph,0
transpose to channel last,0
"name, op",0
input edge,0
output shape,0
input edge,0
attr,0
"print(""Warning: MXNet symbol pad does not support channel last"")",0
"IR_node.attr[""pads""].list.i.extend([0, 0])",0
for e in pad:,0
"IR_node.attr[""pads""].list.i.extend([e, e])",0
"IR_node.attr[""pads""].list.i.extend([0, 0])",0
"name, op",0
input edge,0
units,0
use bias (no_bias default = False),0
weights,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [NM]"".format(source_node.name, self.data_format))",0
input edge,0
output shape,0
kernel_shape,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [{}]"".format(source_node.name, self.data_format, layout))",0
groups,0
in_channel = in_channel // group,0
use_bias (no_bias default = False),0
strides,0
dilations,0
data_format,0
padding,0
weights,0
axis,0
scale,0
epsilon,0
momentum,0
weights,0
gamma,0
beta,0
mean,0
var,0
"name, op",0
input edge,0
pooling type (sum not allowed yet),0
strides,0
kernel_shape,0
padding,0
output shape,0
"name, op",0
input edge,0
output shape,0
dim,0
"def rename_log_softmax(self, source_node):",0
"raise NotImplementedError(""not support yet"")",0
"def rename_Correlation(self, source_node):",0
"raise NotImplementedError(""not support yet"")",0
"name, op",0
input edge,0
padding,0
output shape,0
kernel_shape,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [{}]"".format(source_node.name, self.data_format, layout))",0
use_bias (no_bias default = False),0
strides,0
dilations,0
data_format,0
groups,0
weights,0
"def rename_RNN(self, source_node):",0
"raise NotImplementedError(""RNN not support yet"")",0
"name, op",0
input edge,0
input_dim,0
output_dim,0
dtype,0
output shape,0
"IR only support elu and prelu from {'elu', 'leaky', 'prelu', 'rrelu'}",0
return,0
"name, op",0
gamma,0
input edge,0
"alpha [exp(x) - alpha], but mxnet attr slope [slope*(exp(x) - 1)] when x < 0",0
"IR_node.attr[""alpha""].f = float()",0
output shape,0
"raise NotImplementedError(""slope cannot convert to alpha"")",0
"def rename_InstanceNorm(self, source_node):",0
raise NotImplementedError,0
"def rename_L2Normalization(self, source_node):",0
raise NotImplementedError,0
alpha,0
beta,0
knorm,0
nsize,0
keep_prob,0
mode,0
reverse cannot support yet,0
old API target_shape not support yet,0
output shape,0
"raise NotImplementedError(""adjust output shape"")",0
dim,0
dtype,0
output shape,0
"name, op",0
input edge,0
output shape,0
axis,0
"raise NotImplementedError(""No matching IR api"")",0
"left strip the ""_"" at the beginning of the name",0
"Issue #85, #135",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
keep dims,0
axes,0
"name, op",0
epsilon,0
moving variance (var),0
gamma (scale),0
mean,0
bias,0
input node,0
output node,0
load model files into TensorFlow graph,0
Build network graph,0
TODO: Fused conv and pool with padding is different from defused operators,0
strides,0
window_shape,0
pool type,0
padding,0
shape,0
strides,0
input[1] : W,0
filter,0
padding,0
output[0] : B,0
converted [dropout],0
convert [tf.contrib.layers.batch_norm],0
normal Add,0
units,0
Weights,0
FullyConnected Layer,0
"name, op",0
get Bias,0
Matmul Layer,0
Deal Dropout,0
keep prob,0
Remove nodes,0
Mul,0
Floor,0
paddings,0
gamma (scale),0
bias,0
Mean,0
Var,0
TODO: Current it is only for slice,1
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
check whether flatten operator should be added,0
flatten is needed,0
"self._emit_unary_operation(IR_node, ""contrib.layers.flatten"")",0
skip_prefix = [,0
"""^"",",0
"""train_op"",",0
"""save"",",0
"""gradients"",",0
"""init"",",0
"""global_step"",",0
"""distort_image"",",0
"""Adagrad"",",0
],0
"""Switch""",0
"""RequantizationRange"",",0
"""Requantize"",",0
"""Cast""",0
load model files into TensorFlow graph,0
model = original_gdef,0
Save it to an output file,0
for pytest,0
assert False,0
keep dims,0
axes,0
"name, op",0
ssd model is transformed,0
Ax - (Au - b),0
A,0
print(A_content),0
b,0
print(sub_content),0
print(IR_node.input),0
print(IR_node.output),0
assert False,0
epsilon,0
moving variance (var) /read,0
gamma (scale),0
"gamma = self.get_parent(son.name, [1, 1, 0, 0, 0, 1], True)",0
print(output_node.layer),0
print(Mul.layer),0
beta  (bias),0
moving mean (mean),0
input node,0
output node,0
epsilon,0
beta,0
gamma (scale),0
"output_node = self.get_son(source_node.name, [0, 0, 0, 0], True)",0
assert False,0
TODO: Fused conv and pool with padding is different from defused operators,0
assert variable.get_attr('_output_shapes')[0].dim[0].size == IR_node.attr['kernel_shape'].list.i[-1],0
strides,0
window_shape,0
pool type,0
padding,0
epsilon,0
moving variance (var) /read,0
gamma (scale),0
beta  (bias),0
moving mean (mean),0
print(source_node.in_edges),0
print(source_node.out_edges),0
print(source_node.layer),0
print(tensor_content),0
print(IR_node),0
print(source_node.layer),0
assert False,0
print(source_node.layer),0
assert False,0
print(source_node.layer),0
print(source_node.in_edges),0
print(source_node.out_edges),0
assert False,0
"input_node_0 = self.src_graph.get_parent(source_node.name, [0])",0
# mean/read,0
if input_node_0.type == 'Identity':,0
"input_node_0_read = self.src_graph.get_parent(input_node_0.name, [0])",0
tensor_content = input_node_0_read.get_attr('value'),0
tensor_content = tensor_util.MakeNdarray(tensor_content),0
"self.set_weight(source_node.name, 'weights', tensor_content)",0
"IR_node = self._convert_identity_operation(source_node, start_idx = 1)",0
else:,0
print(scopes),0
print(scopes),0
print(source_node.layer),0
"def rename_Pack(self, source_node):",0
"IR_node = self._convert_identity_operation(source_node, new_op = 'Pack')",0
kwargs['shape'] = self.tensor_shape_to_list(input_node.get_attr('_output_shapes'))[0],0
kwargs['shape'] = self.tensor_shape_to_list(input_node.get_attr('_output_shapes'))[0],0
"def rename_ExpandDims(self, source_node):",0
"IR_node = self._convert_identity_operation(source_node, new_op = 'ExpandDims')",0
"input_node = self.src_graph.get_parent(source_node.name, [0])",0
kwargs = {},0
kwargs['shape'] = self.tensor_shape_to_list(input_node.get_attr('_output_shapes'))[0],0
"input_node_indices = self.src_graph.get_parent(source_node.name, [1])",0
kwargs['exp_dim'] = input_node_indices.get_attr('value').int_val[0],0
"assign_IRnode_values(IR_node, kwargs)",0
weights,0
alpha,0
beta,0
weights,0
"def rename_Dequantize(self, source_node):",0
"IR_node = self._convert_identity_operation(source_node,start_idx=0, end_idx= 1, new_op = 'Dequantize')",0
kwargs = {},0
"input_node = self.src_graph.get_parent(source_node.name, [0])",0
kwargs['shape'] = self.tensor_shape_to_list(input_node.get_attr('_output_shapes'))[0],0
print(source_node.layer),0
print(source_node.layer),0
assert False,0
kwargs = {},0
kwargs['mode'] = 'constant',0
kwargs['constant_values'] = 0.0,0
# paddings,0
"padding = self.get_parent(source_node.name, [1]).layer.attr['value'].tensor",0
shapes = tensor_util.MakeNdarray(padding),0
kwargs['pads'] = convert_tf_pad_to_onnx(shapes),0
"assign_IRnode_values(IR_node, kwargs)",0
print(source_node.layer),0
assert False,0
print(source_node.layer),0
assert False,0
print(source_node.layer),0
assert False,0
print(source_node.layer),0
assert False,0
print(source_node.layer),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Build network graph,0
strides,0
window_shape,0
pool type,0
padding,0
TODO: Handle square,1
self._print_layer(source_node),0
print (source_node.name),0
"print (self.src_graph.get_parent(source_node.name, [0]).real_name)",0
assert False,0
Convolution,0
Kernel,0
Attributes,0
Bias,0
Activation,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
BlockFunction node,0
redirect the composite's inputs to the true inputs,0
"BlockFunctions are short-circuited, and not added to accum[]",0
Function node,0
OutputVariable node,0
def build(self):,0
"_traverse_graph(self, self.model.root_function)",0
"super(CntkGraph, self).build()",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
??,0
print(IR_node.layer),0
assert False,0
print(IR_node.layer),0
assert False,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
For padding,0
print (input_shape),0
print (kernel_shape),0
print (strides),0
Not tested,0
print (input_shape),0
print (kernel_shape),0
print (strides),0
kernel_shape[idx] = (kernel_shape[idx] - 1) * dilation_rate + 1,0
print ([0] + up_list + [0] + down_list if data_format.startswith('NC') else up_list + [0] + down_list + [0]),0
print ('-----------------------------------------------------'),0
network library,0
not support multi-threads download,0
key: layer_name    value: keras layer,0
private functions,0
"print ('{} --> {}'.format(src, dst))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
share functions,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
name --> (weight_name --> ndarray),0
Generated by the protocol buffer compiler.  DO NOT EDIT!,0
source: graph.proto,0
@@protoc_insertion_point(imports),0
@@protoc_insertion_point(class_scope:GraphDef),0
@@protoc_insertion_point(class_scope:NodeDef.AttrEntry),0
@@protoc_insertion_point(class_scope:NodeDef),0
@@protoc_insertion_point(class_scope:AttrValue.ListValue),0
@@protoc_insertion_point(class_scope:AttrValue),0
@@protoc_insertion_point(class_scope:TensorShape.Dim),0
@@protoc_insertion_point(class_scope:TensorShape),0
@@protoc_insertion_point(class_scope:LiteralTensor),0
@@protoc_insertion_point(module_scope),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
First try to read it as a binary file.,0
Next try to read it as a text file.,0
-*- coding: utf-8 -*-,0
from keras.layers.core import Layer,0
Arguments,0
Input shape,0
Output shape,0
Prepare broadcasting shape.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Load the model network,0
Load the model weights,0
load model files into Keras graph,0
"_keras.utils.plot_model(model, ""model.png"", show_shapes = True)",0
Build network graph,0
TODO: Fused conv and pool with padding is different from defused operators,0
TODO: More activation functions,1
for ELU,0
input edge,0
"name, op",0
weights,0
pads,0
filter,0
"[kd, kh, kw, channel_size, filter number]",0
use_bias,0
strides,0
"[1, sd, sh, sw, 1]",0
dilations,0
"[1, dd, dh, dw, 1]",0
activation,0
"name, op",0
input edge,0
padding,0
strides,0
"[1, sd, sh, sw, 1]",0
window_shape,0
"[1, pd, ph, pw, 1]",0
"name, op",0
input edge,0
For concat axis,0
"name, op",0
input edge,0
padding,0
only for training,0
"name, op",0
input edge,0
"name, op",0
input edge,0
Merge Layers,0
only for training,0
"name, op",0
input edge,0
shape,0
only for training,0
"name, op",0
input edge,0
Core Layers,0
"name, op",0
input edge,0
units,0
use_bias,0
weights,0
activation,0
"name, op",0
input edge,0
"name, op",0
input edge,0
input_dim,0
output_dim,0
mask_zero,0
weights,0
"name, op",0
input edge,0
units,0
use_bias,0
"for Keras, drop_out and recurrent_dropout",0
activation,0
"name, op",0
input edge,0
units,0
activation,0
weights,0
Kit TODO : need to search the tf,1
"name, op",0
input edge,0
for target shape,0
"name, op",0
input edge,0
axis,0
"Parameter arrangement in Keras: gamma, beta, mean, variance",0
scale,0
beta,0
mean,0
var,0
"name, op",0
input edge,0
for target shape,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
rot weights,0
"print(""Converting layer {}({})"".format(current_node.name, node_type))",0
TODO,1
"return input_node, 'same'",0
"assert IR_node.get_attr('group', 1) == 1",0
change dw from filters to 1,0
reset the default dilation,0
############,0
Operators #,0
############,0
TODO,1
"when converting from coreml model, reshape is needed after the global pooling",0
for Keras,0
TODO: arguments won't be saved in keras export model,1
print(IR_node.layer),0
print(IR_node.layer),0
Prepare broadcasting shape.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
Kit: TODO,1
Duplicate models for weight sharing,0
Expand the sub-models,0
console.info(JSON.parse(e.target.result)),0
"default, draw the model.json file",0
window.onload = () => {,0
"d3.json(filePath, (error, json) => {",0
if (error) throw error;,0
// console.info(json),0
draw(json),0
}),0
},0
generate dag,0
IR model or keras model,0
path generator,0
let arrowL = `M ${points[len - 1].x} ${points[len - 1].y} l -7 -7`,0
let arrowR = `M ${points[len - 1].x} ${points[len - 1].y} l  7 -7`,0
select a layer,0
draw,0
"let scale = Math.min(window.innerWidth*0.85 / width, (window.innerHeight-85)/height)",0
"var filter = def.append(""filter"")",0
".attr(""id"", ""dropshadow"")",0
"filter.append(""feGaussianBlur"")",0
".attr(""in"", ""SourceAlpha"")",0
".attr(""stdDeviation"", 4)",0
".attr(""result"", ""blur"");",0
"// filter.append(""feOffset"")",0
"//     .attr(""in"", ""blur"")",0
"//     .attr(""dx"", 2)",0
"//     .attr(""dy"", 2)",0
"//     .attr(""result"", ""offsetBlur"");",0
"var feMerge = filter.append(""feMerge"");",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""offsetBlur"")",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""SourceGraphic"");",0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
"svg.on('keydown', ()=>{console.info('ddd')})",0
".on('mouseover', ()=>{console.info('mouse over')})",0
k = parseFloat(k)*parseFloat(k_),0
"limit x, y",0
// a trick to make text svg transform in MS Edge,0
"d3.selectAll("".labels"").classed(""tempclass"", true);",0
"setTimeout(function () { d3.selectAll("".labels"").classed(""tempclass"", false); }, 40);",0
when shift is down,0
ulti function,0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
space = '',0
console.info(JSON.parse(e.target.result)),0
"default, draw the model.json file",0
window.onload = () => {,0
"d3.json(filePath, (error, json) => {",0
if (error) throw error;,0
// console.info(json),0
draw(json),0
}),0
},0
generate dag,0
IR model or keras model,0
path generator,0
let arrowL = `M ${points[len - 1].x} ${points[len - 1].y} l -7 -7`,0
let arrowR = `M ${points[len - 1].x} ${points[len - 1].y} l  7 -7`,0
select a layer,0
draw,0
"let scale = Math.min(window.innerWidth*0.85 / width, (window.innerHeight-85)/height)",0
"var filter = def.append(""filter"")",0
".attr(""id"", ""dropshadow"")",0
"filter.append(""feGaussianBlur"")",0
".attr(""in"", ""SourceAlpha"")",0
".attr(""stdDeviation"", 4)",0
".attr(""result"", ""blur"");",0
"// filter.append(""feOffset"")",0
"//     .attr(""in"", ""blur"")",0
"//     .attr(""dx"", 2)",0
"//     .attr(""dy"", 2)",0
"//     .attr(""result"", ""offsetBlur"");",0
"var feMerge = filter.append(""feMerge"");",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""offsetBlur"")",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""SourceGraphic"");",0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
"svg.on('keydown', ()=>{console.info('ddd')})",0
".on('mouseover', ()=>{console.info('mouse over')})",0
k = parseFloat(k)*parseFloat(k_),0
"limit x, y",0
// a trick to make text svg transform in MS Edge,0
"d3.selectAll("".labels"").classed(""tempclass"", true);",0
"setTimeout(function () { d3.selectAll("".labels"").classed(""tempclass"", false); }, 40);",0
when shift is down,0
ulti function,0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
space = '',0
Get the long description from the README file,0
Versions should comply with PEP440.  For a discussion on single-sourcing,0
"the version across setup.py and the project code, see",0
https://packaging.python.org/en/latest/single_source_version.html,0
The project's main homepage.,0
Author details,0
Choose your license,0
See https://pypi.python.org/pypi?%3Aaction=list_classifiers,0
How mature is this project? Common values are,0
3 - Alpha,0
4 - Beta,0
5 - Production/Stable,0
Indicate who your project is intended for,0
"Pick your license as you wish (should match ""license"" above)",0
"Specify the Python versions you support here. In particular, ensure",0
"that you indicate whether you support Python 2, Python 3 or both.",0
What does your project relate to?,0
You can just specify the packages manually here if your project is,0
simple. Or you can use find_packages().,0
"Alternatively, if you want to distribute just a my_module.py, uncomment",0
this:,0
"py_modules=[""my_module""],",0
List run-time dependencies here.  These will be installed by pip when,0
"your project is installed. For an analysis of ""install_requires"" vs pip's",0
requirements files see:,0
https://packaging.python.org/en/latest/requirements.html,0
"To provide executable scripts, use entry points in preference to the",0
"""scripts"" keyword. Entry points provide cross-platform support and allow",0
pip to create the appropriate form of executable for the target platform.,0
"Function self.assertEquals has deprecated, change to assertEqual",0
get original model prediction result,0
original to IR,0
get original model prediction result,0
original to IR,0
get original model prediction result,0
download model,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
IR to code,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
"'alexnet'       : [CntkEmit, TensorflowEmit, KerasEmit],",0
"'inception_resnet_v2' : [CntkEmit, TensorflowEmit, KerasEmit], # TODO PytorchEmit",0
"'nasnet-a_large' : [TensorflowEmit, KerasEmit, PytorchEmit], # TODO",1
get original model prediction result,0
do not deal,0
I don't want to deal with auto_pad,0
Don't support auto_pad current!,0
2018-02-28,0
if group is None:,0
group = 1,0
group is not support yet too.,0
print (dir(data)),0
print (i),0
"print (""    {} with shape {}"".format(j, load_weight[i][j].shape))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Base Functions,0
"padding,",0
"exstr = "", value=float('-Inf')""",0
"exstr = """"",0
"input_node = self._defuse_padding(IR_node, exstr)",0
Ignore it in Pytorch,0
for Keras,0
"self.add_init(2, ""self.{:<15} = torch.from_numpy(__weights_dict['{}']['value'])"".format(",0
"IR_node.variable_name,",0
IR_node.name)),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self._type = layer.__class__.__name__.replace('Backward', '')",0
"self._name = ""{}_{}"".format(self.type, id)",0
TODO,1
"""""""",0
build graph for pytorch 0.2.0,0
"""""""",0
dummy_input = torch.autograd.Variable(torch.randn(shape)),0
output_node = self.model(dummy_input),0
search_queue = [output_node.grad_fn],0
"tmp_node = PyTorchGraphNode(output_node.grad_fn, 0)",0
self.layer_map[tmp_node.name] = tmp_node,0
visited = {output_node.grad_fn : self.layer_map[tmp_node.name]},0
idx = 0,0
node_count = 1,0
while (idx < len(search_queue)):,0
current_node = search_queue[idx],0
current_type = visited[current_node].type,0
"if hasattr(current_node, 'next_functions'):",0
"for parent, _ in current_node.next_functions:",0
"parent_type = parent.__class__.__name__.replace('Backward', '')",0
if parent_type != 'AccumulateGrad' and \,0
(parent_type != 'Transpose' or current_type != 'Addmm'):,0
if not parent in visited:,0
"tmp_node = PyTorchGraphNode(parent, node_count)",0
self.layer_map[tmp_node.name] = tmp_node,0
node_count += 1,0
visited[parent] = tmp_node,0
search_queue.append(parent),0
"self._make_connection(visited[parent].name, visited[current_node].name)",0
idx += 1,0
try:,0
return TorchGraph._forward_torch_random_input(,0
"torch_model,",0
"input_shapes,",0
is_batch=False,0
),0
except:,0
# try batch mode,0
# return TorchGraph._forward_torch_random_input(,0
"#     torch_model,",0
"#     input_shapes,",0
#     is_batch=True,0
# ),0
pass,0
tensor = torch.rand(*shape).float(),0
multi output,0
single output,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
###########,0
property #,0
###########,0
###################,0
Public Functions #,0
###################,0
Build network graph,0
#########,0
Layers #,0
#########,0
handle weight,0
handle bias,0
handle weight,0
handle bias,0
###################,0
Helper Functions #,0
###################,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
TODO - remove style transfer 1D hack,1
"Input is 1D but it goes to the width dimension: (1,1,W)",0
"assume (Batch, Channels) - Batch dimension should be dropped",0
"assume (Batch, Sequence-Length, channels)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
else:,0
"for i, in_node in enumerate(layer.input):",0
layer.input[i] = self.IR_graph.get_node(in_node).real_name,0
assert False,0
self._connect_coreml_layers(),0
Add classifier classes (if applicable),0
Set pre-processing paramsters,0
"image_input_names,",0
Return the protobuf spec,0
model = _MLModel(self.builder.spec),0
Get input and output names,0
Get the weights.,0
Dimensions and weights,0
Dilations,0
"print(self.IR_graph.get_parent(IR_node.name, [0]).layer)",0
print(input_name),0
print(IR_node.real_name),0
depth-wise convolution,0
Dilations,0
Get input and output names,0
Pooling layer type,0
"if it's global, set the global flag",0
Padding,0
Now add the layer,0
assert False,0
Get input and output names,0
Get the weights from keras,0
Get input and output names,0
blob_order == 0 if the input blob needs not be rearranged,0
blob_order == 1 if the input blob needs to be rearranged,0
"using keras_layer.input.shape have a ""?"" (Dimension[None] at the front),",0
making a 3D tensor with unknown batch size 4D,0
Get input and output names,0
Get input and output names,0
print(IR_node.name),0
input_name =,0
negate it,0
apply threshold,0
negate it back,0
for Keras,0
Get input and output names,0
print(input_name),0
print(IR_node.real_name),0
Set parameters,0
"Parameter arrangement in Keras: gamma, beta, mean, variance",0
compute adjusted parameters,0
assert False,0
Now add the layer,0
"if IR_node.name != ""MMdnn_Output"" :",0
self.emit_Flatten(IR_node),0
self.emit_Reshape(IR_node),0
Get the weights,0
Generated by the protocol buffer compiler.  DO NOT EDIT!,0
source: caffe.proto,0
@@protoc_insertion_point(imports),0
@@protoc_insertion_point(class_scope:caffe.BlobShape),0
@@protoc_insertion_point(class_scope:caffe.BlobProto),0
@@protoc_insertion_point(class_scope:caffe.BlobProtoVector),0
@@protoc_insertion_point(class_scope:caffe.Datum),0
@@protoc_insertion_point(class_scope:caffe.FillerParameter),0
@@protoc_insertion_point(class_scope:caffe.NetParameter),0
@@protoc_insertion_point(class_scope:caffe.SolverParameter),0
@@protoc_insertion_point(class_scope:caffe.SolverState),0
@@protoc_insertion_point(class_scope:caffe.NetState),0
@@protoc_insertion_point(class_scope:caffe.NetStateRule),0
@@protoc_insertion_point(class_scope:caffe.ParamSpec),0
@@protoc_insertion_point(class_scope:caffe.LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.TransformationParameter),0
@@protoc_insertion_point(class_scope:caffe.LossParameter),0
@@protoc_insertion_point(class_scope:caffe.AccuracyParameter),0
@@protoc_insertion_point(class_scope:caffe.ArgMaxParameter),0
@@protoc_insertion_point(class_scope:caffe.ConcatParameter),0
@@protoc_insertion_point(class_scope:caffe.BatchNormParameter),0
@@protoc_insertion_point(class_scope:caffe.BiasParameter),0
@@protoc_insertion_point(class_scope:caffe.ContrastiveLossParameter),0
@@protoc_insertion_point(class_scope:caffe.ConvolutionParameter),0
@@protoc_insertion_point(class_scope:caffe.CropParameter),0
@@protoc_insertion_point(class_scope:caffe.DataParameter),0
@@protoc_insertion_point(class_scope:caffe.DropoutParameter),0
@@protoc_insertion_point(class_scope:caffe.DummyDataParameter),0
@@protoc_insertion_point(class_scope:caffe.EltwiseParameter),0
@@protoc_insertion_point(class_scope:caffe.ELUParameter),0
@@protoc_insertion_point(class_scope:caffe.EmbedParameter),0
@@protoc_insertion_point(class_scope:caffe.ExpParameter),0
@@protoc_insertion_point(class_scope:caffe.FlattenParameter),0
@@protoc_insertion_point(class_scope:caffe.HDF5DataParameter),0
@@protoc_insertion_point(class_scope:caffe.HDF5OutputParameter),0
@@protoc_insertion_point(class_scope:caffe.HingeLossParameter),0
@@protoc_insertion_point(class_scope:caffe.ImageDataParameter),0
@@protoc_insertion_point(class_scope:caffe.InfogainLossParameter),0
@@protoc_insertion_point(class_scope:caffe.InnerProductParameter),0
@@protoc_insertion_point(class_scope:caffe.InputParameter),0
@@protoc_insertion_point(class_scope:caffe.LogParameter),0
@@protoc_insertion_point(class_scope:caffe.LRNParameter),0
@@protoc_insertion_point(class_scope:caffe.MemoryDataParameter),0
@@protoc_insertion_point(class_scope:caffe.MVNParameter),0
@@protoc_insertion_point(class_scope:caffe.ParameterParameter),0
@@protoc_insertion_point(class_scope:caffe.PoolingParameter),0
@@protoc_insertion_point(class_scope:caffe.PowerParameter),0
@@protoc_insertion_point(class_scope:caffe.PythonParameter),0
@@protoc_insertion_point(class_scope:caffe.RecurrentParameter),0
@@protoc_insertion_point(class_scope:caffe.ReductionParameter),0
@@protoc_insertion_point(class_scope:caffe.ReLUParameter),0
@@protoc_insertion_point(class_scope:caffe.ReshapeParameter),0
@@protoc_insertion_point(class_scope:caffe.ScaleParameter),0
@@protoc_insertion_point(class_scope:caffe.SigmoidParameter),0
@@protoc_insertion_point(class_scope:caffe.SliceParameter),0
@@protoc_insertion_point(class_scope:caffe.SoftmaxParameter),0
@@protoc_insertion_point(class_scope:caffe.TanHParameter),0
@@protoc_insertion_point(class_scope:caffe.TileParameter),0
@@protoc_insertion_point(class_scope:caffe.ThresholdParameter),0
@@protoc_insertion_point(class_scope:caffe.WindowDataParameter),0
@@protoc_insertion_point(class_scope:caffe.SPPParameter),0
@@protoc_insertion_point(class_scope:caffe.V1LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.V0LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.PReLUParameter),0
@@protoc_insertion_point(module_scope),0
"TODO: We need to identify whether this is 4D image data, otherwise we shouldn't change the dimension order",1
"Stochastic pooling, for instance.",0
TODO: Axis,0
TODO: Unbiased,1
check if need the Flatten layer,0
if parent.output_shape.height > 1 or parent.output_shape.width > 1:,0
TODO: The gamma parameter has to be set (in node.data?) and this should work.,1
"Also, mean should be set to 0, and var to 1, just to be safe.",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
for test,0
"with open(""graph.txt"", 'w') as f:",0
for layer in self.IR_graph.topological_sort:,0
current_node = self.IR_graph.get_node(layer),0
"print(""========current_node=========\n{}"".format(current_node.layer), file=f)",0
test end,0
"print(""========current_node={}"".format(current_node.layer))",0
keys = [],0
for key in self.weights_dict[IR_node.name].keys():,0
keys.append(key),0
"print(""=======Layer: {}, keys: {}"".format(IR_node.name, keys))",0
"self.weights_dict[IR_node.name].pop('scale', None)",0
"def emit_Tanh(self, IR_node):",0
"self._emit_activation(IR_node, 'ops.tanh')",0
Caffe Types,0
Same value applies to all spatial dimensions,0
Extract the value for the given spatial dimension,0
consider rewrite this function to Network.py,0
Dropout layers appear in a fair number of Caffe,0
test-time networks. These are just ignored. We'll,0
filter them out here.,0
TODO: raise error,1
TODO: raise error,1
Automatically set a name if not provided.,0
Figure out the layer inputs.,0
"print('op: %s   shape: %s' % (op, layer_output._keras_shape))",0
"print('op: %s   shape: %s' % (op, layer_output.get_shape().as_list()))",0
Add to layer LUT.,0
Old-style input specification,0
New-style input specification,0
"We most likely have a data layer on our hands. The problem is,",0
Caffe infers the dimensions of the data from the source (eg: LMDB).,0
We want to avoid reading datasets here. Fail for now.,0
This can be temporarily fixed by transforming the data layer to,0
"Caffe's ""input"" layer (as is usually used in the ""deploy"" version).",0
TODO: Find a better solution for this.,1
The .prototxt file defining the graph,0
The .caffemodel file containing the learned parameters,0
Set to true if the fallback protocol-buffer based backend was used,0
"A list containing (layer name, parameters) tuples",0
Load the parameters,0
"When using the protobuf-backend, each parameter initially has four dimensions.",0
"In certain cases (like FC layers), we want to eliminate the singleton dimensions.",0
"This implementation takes care of the common cases. However, it does leave the",0
potential for future issues.,0
The Caffe-backend does not suffer from this problem.,0
A dictionary mapping NodeKind to the transposed order.,0
The node kinds eligible for reshaping,0
"If true, the reshaped data will replace the old one.",0
"Otherwise, it's set to the reshaped_data attribute.",0
Check for 2+ dimensional data,0
The FC layer connected to the spatial layer needs to be,0
re-wired to match the new spatial ordering.,0
node.reshaped_data = weights.transpose(transpose_order),0
Set the weights,0
We're only fusing nodes with single parents,0
We can only fuse a node if its parent's,0
value isn't used by any other node.,0
Rewrite the fused node's children to its parent.,0
Disconnect the fused node from the graph.,0
Let the sub-class merge the fused node in any arbitrary way.,0
Fuse ReLUs when the parent node is one of the given types.,0
"If None, all node types are eligible.",0
Prescale the stats,0
Replace with the updated values,0
Include the scale and bias terms,0
self.graph = NodeRenamer()(graph),0
"return Graph(name, [self.map_node(node) for node in self.graph.nodes])",0
Kit,0
"mapped_node.input.extend(['%s:%s' % (input.name, idx) for input, idx in node.parents])",0
FIXME:,0
output = node.output,0
Decompose DAG into chains,0
Generate Python code line by line,0
Fall back to the protobuf implementation,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"'alexnet'        : [(657, 0.41121086), (744, 0.20789686), (847, 0.086725488), (821, 0.059082959), (595, 0.058017101)],",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result(None, False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
layer_name = 'block2_pool',0
"intermediate_layer_model = keras.Model(inputs=model.input,",0
outputs=model.get_layer(layer_name).output),0
intermediate_output = intermediate_layer_model.predict(img),0
print (intermediate_output),0
print (intermediate_output.shape),0
"print (""%.30f"" % np.sum(intermediate_output))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Load a real image and do default tf imageNet preprocessing,0
"[Batch, Height, Width, Channels] --> [Batch, Channels, Height, Width]",0
We only deal with non-recurrent networks for now,0
"(H,W,C) --> (C,H,W)",0
Load TensorFlow model,0
Prepare inputs,0
Run TF session,0
Evaluate coreml model,0
evaluate the TF model,0
evaluate CoreML,0
Test the default CoreML evaluation,0
Download model,0
Convert to coreml,0
Test predictions on an image,0
Download model,0
Convert to coreml,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
"@unittest.skip(""Failing GPU backend: related to https://github.com/tf-coreml/tf-coreml/issues/26"")",0
? style transfer image size and style number?,0
Test predictions on an image,0
evaluate the TF model,0
evaluate CoreML,0
Test the default CoreML evaluation,0
"compare_tf_coreml_outputs(tf_out_flatten, coreml_out_flatten)",0
#Download model,0
url = 'https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz',0
tf_model_dir = _download_file(url = url),0
"tf_model_path = os.path.join(TMP_MODEL_DIR, 'inception_v3_2016_08_28_frozen.pb')",0
#Convert to coreml,0
"mlmodel_path = os.path.join(TMP_MODEL_DIR, 'inception_v3_2016_08_28.mlmodel')",0
mlmodel = tf_converter.convert(,0
"tf_model_path = tf_model_path,",0
"mlmodel_path = mlmodel_path,",0
"output_feature_names = ['InceptionV3/Predictions/Softmax:0'],",0
"input_name_shape_dict = {'input:0':[1,299,299,3]},",0
"image_input_names = ['input:0'],",0
"red_bias = -1,",0
"green_bias = -1,",0
"blue_bias = -1,",0
image_scale = 2.0/255.0),0
#Test predictions on an image,0
_test_coreml_model_image_input(,0
"tf_model_path = tf_model_path,",0
"coreml_model = mlmodel,",0
"input_tensor_name = 'input:0',",0
"output_tensor_name = 'InceptionV3/Predictions/Softmax:0',",0
img_size = 299),0
Download model,0
Convert to coreml,0
Test predictions on an image,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
self.test_truth(),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
caffe uses NCHW,0
"self.print_intermediate_result('', False)",0
delete tmp model files,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
pylint: disable=line-too-long,0
pylint: enable=line-too-long,0
"net = caffe.Net(arch_fn, weight_fn, caffe.TEST)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result('InceptionV3/InceptionV3/Mixed_5b/Branch_3/AvgPool_0a_3x3/AvgPool', False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
need to be updated,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.input, self.model, self.testop = self.MainModel.KitModel(self.args.w)",0
testop = tf.get_default_graph().get_operation_by_name(layer_name),0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"'mobilenet_v1': mobilenet_v1.mobilenet_v1_arg_scope,",0
"'mobilenet_v1' : mobilenet_v1.mobilenet_v1,",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"tf.train.export_meta_graph(""kit.meta"", as_text=True)",0
"writer = tf.summary.FileWriter('./graphs', sess.graph)",0
raise NotImplementedError(),0
load file,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Notes for training NASNet Cifar Model,0
-------------------------------------,0
batch_size: 32,0
learning rate: 0.025,0
cosine (single period) learning rate decay,0
auxiliary head loss weighting: 0.4,0
clip global norm of all gradients by 5,0
600 epochs with a batch size of 32,0
This is used for the drop path probabilities since it needs to increase,0
the drop out probability over the course of training.,0
Notes for training large NASNet model on ImageNet,0
-------------------------------------,0
batch size (per replica): 16,0
learning rate: 0.015 * 100,0
learning rate decay factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 100 replicas,0
auxiliary head loss weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Notes for training the mobile NASNet ImageNet model,0
-------------------------------------,0
batch size (per replica): 32,0
learning rate: 0.04 * 50,0
learning rate scaling factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 50 replicas,0
auxiliary head weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Shape of feature map before the final layer.,0
149 x 149 x 32,0
Run the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Find where to place the reduction cells or stride normal cells,0
Setup for building in the auxiliary head.,0
Run the cells,0
true_cell_num accounts for the stem cells,0
Final softmax layer,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80,0
71 x 71 x 192,0
35 x 35 x 192,0
35 x 35 x 320,0
TODO(alemi): Register intermediate endpoints,1
"17 x 17 x 1088 if output_stride == 8,",0
33 x 33 x 1088 if output_stride == 16,0
TODO(alemi): register intermediate endpoints,1
TODO(gpapan): Properly support output_stride for the rest of the net.,1
8 x 8 x 2080,0
TODO(alemi): register intermediate endpoints,1
8 x 8 x 1536,0
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which",1
can be set to False to disable pooling here (as in resnet_*()).,0
Set weight_decay for weights in conv2d and fully_connected layers.,0
Set activation_fn and parameters for batch_norm.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
=============================================================================,0
Tensorflow mandates these.,0
Conv and DepthSepConv namedtuple define layers of the MobileNet architecture,0
Conv defines 3x3 convolution layers,0
DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.,0
stride is the stride of the convolution,0
depth is the number of channels or filters in a layer,0
_CONV_DEFS specifies the MobileNet body,0
Used to find thinned depths for each layer.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
By passing filters=None,0
separable_conv2d produces only a depthwise convolution layer,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 1024,0
Set weight_decay for weights in Conv and DepthSepConv layers.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Skip path 1,0
Skip path 2,0
"First pad with 0's on the right and bottom, then shift the filter to",0
include those 0's that were added.,0
Concat and apply BN,0
Set the prev layer to the current layer if it is none,0
Check to be sure prev layer stuff is setup correctly,0
Apply conv operations,0
Combine hidden states using 'add'.,0
Add hiddenstate to the list of hiddenstates we can choose from,0
Dont stride if this is not one of the original hiddenstates,0
"Check if a stride is needed, then use a strided 1x1 here",0
Determine if a reduction should be applied to make the number of,0
filters match.,0
Return the concat of all the states,0
Scale keep prob by layer number,0
The added 2 is for the reduction cells,0
Decrease the keep probability over time,0
Copyright (c) Microsoft. All rights reserved.,0
Licensed under the MIT license. See LICENSE.md file in the project root,0
for full license information.,0
==============================================================================,0
"self.model, self.testop = self.MainModel.KitModel(self.args.w)",0
"self.print_intermediate_result(None, False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
pylint: disable=line-too-long,0
pylint: enable=line-too-long,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result('', False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
save network structure as JSON,0
layer_name = 'block2_pool',0
"intermediate_layer_model = keras.Model(inputs=model.input,",0
outputs=model.get_layer(layer_name).output),0
intermediate_output = intermediate_layer_model.predict(img),0
print (intermediate_output),0
print (intermediate_output.shape),0
"print (""%.30f"" % np.sum(intermediate_output))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Convert network architecture only,0
MXNet,0
Caffe,0
For CoreML,0
assert args.network or args.frozen_pb,0
Caffe,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check non-sense always input module.Module,0
if not (type(model) == mx.module.Module,0
or type(model) == mx.module.SequentialModule,0
or type(model) == mx.model),0
"raise TypeError(""MXNet layer of type %s is not supported."" % type(model))",0
"if layer[""op""] == ""null"":",0
continue,0
"raise NotImplementedError(""Cannot support multi-input"")",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"mxnet-cpu only support channel first, default convert the model and weight as channel first",0
download and show the image,0
"convert into format (batch, RGB, width, height)",0
"to show the image, change the argument show into True",0
compute the predict probabilities,0
print the top-5,0
# call function predict,0
"with open('synset.txt', 'r') as f:",0
labels = [l.rstrip() for l in f],0
"predict(model, labels, 'http://writm.com/wp-content/uploads/2016/08/Cat-hd-wallpapers.jpg')",0
"print(""Warning: MXNet Convolution Layer pad does not match IR Convolution Layer pad"")",0
"layout = IR_node.IR_layer.attr[""data_format""].s",0
if layout not in MXNetEmitter.channels_last:,0
Add LeakyReLU Elu(slope not support),0
"axis = IR_node.IR_layer.attr[""axis""].i",0
not supported yet,0
"print(""Warning: MXNet Pooling Layer pad does not match IR Pooling Layer pad"")",0
"def emit_ConvTranspose(self, IR_node):",0
if self.weight_loaded:,0
weight_dict = self.weights[IR_node.name],0
weights = weight_dict['weights'],0
"dim = len(IR_node.IR_layer.attr[""kernel_shape""].list.i) - 2",0
kernel = list(),0
"for idx in range(0, dim):",0
"kernel.append(IR_node.IR_layer.attr[""kernel_shape""].list.i[idx])",0
stride = list(),0
"for e in IR_node.IR_layer.attr[""strides""].list.i[1:-1]:",0
stride.append(e),0
dilate = list(),0
"for e in IR_node.IR_layer.attr[""dilations""].list.i[1:-1]:",0
dilate.append(e),0
"dilate = ', '.join('%s' % i for i in dilate)",0
defuse_pad = False,0
pad = list(),0
"if ""pads"" in IR_node.IR_layer.attr:",0
output_shape = list(),0
"for e in IR_node.IR_layer.attr[""_output_shapes""].list.shape[0].dim:",0
output_shape.append(e.size),0
"# print(""Warning: MXNet Deconvolution Layer pad does not match IR Deconvolution Layer pad"")",0
"defuse_pad, pad = MXNetEmitter.transfer_pad(IR_node.IR_layer.attr[""pads""].list.i)",0
"pad = ', '.join('%s' % i for i in pad)",0
"kernel = ', '.join('%s' % i for i in kernel)",0
"stride = ', '.join('%s' % i for i in stride)",0
"num_filter = IR_node.IR_layer.attr[""kernel_shape""].list.i[-2]",0
"no_bias = not IR_node.IR_layer.attr[""use_bias""].b",0
if not no_bias and self.weight_loaded:,0
"self.output_weights[IR_node.replace_scope(IR_node.name) + ""_bias""] = weight_dict['bias']",0
"# layout = IR_node.IR_layer.attr[""data_format""].s",0
if dim == 1:,0
layout = 'NCW',0
elif dim == 2:,0
layout = 'NCHW',0
elif dim == 3:,0
layout = 'NCDHW',0
if self.weight_loaded:,0
# if layout not in MXNetEmitter.channels_last:,0
"weights = MXNetEmitter.transpose(weights, dim)",0
"self.output_weights[IR_node.replace_scope(IR_node.name) + ""_weight""] = weights",0
"code = """"",0
if not defuse_pad:,0
"code = ""{:<15} = mx.sym.Deconvolution(data = {}, kernel = ({}), stride = ({}), dilate = ({}), pad = ({}), num_filter = {}, no_bias = {}, layout = '{}', name = '{}')"".format(",0
"IR_node.replace_scope(IR_node.name),",0
"IR_node.replace_scope(IR_node.in_edges[0]),",0
"kernel,",0
"stride,",0
"dilate,",0
"pad,",0
"num_filter,",0
"no_bias,",0
"layout,",0
IR_node.replace_scope(IR_node.name)),0
else:,0
"code = self.set_pad(IR_node, code, pad)",0
"code += ""\n    {:<15} = mx.sym.Deconvolution(data = {}, kernel = ({}), stride = ({}), dilate = ({}), num_filter = {}, no_bias = {}, layout = '{}', name = '{}')"".format(",0
"IR_node.replace_scope(IR_node.name), IR_node.replace_scope(IR_node.name) + ""_pad"", kernel, stride, dilate, num_filter, no_bias, layout, IR_node.replace_scope(IR_node.name))",0
return code,0
"def emit_LeakyReLU(self, IR_node):",0
"# IR only support Elu, the same problem with func emit_Activation",0
"code = ""{:<15} = mx.sym.LeakyReLU(data = {}, )"".format()",0
return code,0
raise NotImplementedError,0
reverse cannot support yet,0
"code = ""{:<15} = mx.sym.transpose(data = {}, axes = (0, 2, 3, 1))\n"".format(""trans"", self.parent_variable_name(IR_node))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Not support yet,0
"""softrelu""  : ""SoftReLU""",0
Load the model network and weights,0
"digraph = mx.viz.plot_network(sym, save_format='jpg') # For debugging",0
digraph.render(),0
model.bind(data_shapes = data_shapes),0
model.init_params(),0
"mod.load(model_path, epoch_num)",0
return mod.get_params(),0
load the model network,0
adjust the data format,0
load model files into MXNet graph,0
data_shape arguments added to calculate infer_shape(required),0
"if isinstance(input_arg, basestring):",0
Build network graph,0
transpose to channel last,0
"name, op",0
input edge,0
output shape,0
input edge,0
attr,0
"print(""Warning: MXNet symbol pad does not support channel last"")",0
"IR_node.attr[""pads""].list.i.extend([0, 0])",0
for e in pad:,0
"IR_node.attr[""pads""].list.i.extend([e, e])",0
"IR_node.attr[""pads""].list.i.extend([0, 0])",0
"name, op",0
input edge,0
units,0
use bias (no_bias default = False),0
weights,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [NM]"".format(source_node.name, self.data_format))",0
input edge,0
output shape,0
kernel_shape,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [{}]"".format(source_node.name, self.data_format, layout))",0
groups,0
in_channel = in_channel // group,0
use_bias (no_bias default = False),0
strides,0
dilations,0
data_format,0
padding,0
weights,0
axis,0
scale,0
epsilon,0
momentum,0
weights,0
gamma,0
beta,0
"if MXNetParser.str2bool(source_node.get_attr(""use_global_stats"", ""False"")):",0
mean,0
var,0
"name, op",0
input edge,0
pooling type (sum not allowed yet),0
strides,0
kernel_shape,0
padding,0
output shape,0
"name, op",0
input edge,0
output shape,0
dim,0
"def rename_log_softmax(self, source_node):",0
"raise NotImplementedError(""not support yet"")",0
"def rename_Correlation(self, source_node):",0
"raise NotImplementedError(""not support yet"")",0
"name, op",0
input edge,0
padding,0
output shape,0
kernel_shape,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [{}]"".format(source_node.name, self.data_format, layout))",0
use_bias (no_bias default = False),0
strides,0
dilations,0
data_format,0
groups,0
weights,0
"def rename_RNN(self, source_node):",0
"raise NotImplementedError(""RNN not support yet"")",0
"name, op",0
input edge,0
input_dim,0
output_dim,0
dtype,0
output shape,0
"IR only support elu and prelu from {'elu', 'leaky', 'prelu', 'rrelu'}",0
return,0
"name, op",0
gamma,0
input edge,0
"alpha [exp(x) - alpha], but mxnet attr slope [slope*(exp(x) - 1)] when x < 0",0
"IR_node.attr[""alpha""].f = float()",0
output shape,0
"raise NotImplementedError(""slope cannot convert to alpha"")",0
"def rename_InstanceNorm(self, source_node):",0
raise NotImplementedError,0
"def rename_L2Normalization(self, source_node):",0
raise NotImplementedError,0
alpha,0
beta,0
knorm,0
nsize,0
keep_prob,0
mode,0
reverse cannot support yet,0
old API target_shape not support yet,0
output shape,0
"raise NotImplementedError(""adjust output shape"")",0
dim,0
dtype,0
output shape,0
"name, op",0
input edge,0
output shape,0
axis,0
"raise NotImplementedError(""No matching IR api"")",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
keep dims,0
axes,0
"name, op",0
epsilon,0
moving variance (var),0
gamma (scale),0
mean,0
bias,0
input node,0
output node,0
load model files into TensorFlow graph,0
Build network graph,0
TODO: Fused conv and pool with padding is different from defused operators,0
strides,0
window_shape,0
pool type,0
padding,0
shape,0
strides,0
input[1] : W,0
filter,0
padding,0
output[0] : B,0
converted [dropout],0
convert [tf.contrib.layers.batch_norm],0
normal Add,0
units,0
Weights,0
FullyConnected Layer,0
"name, op",0
get Bias,0
Matmul Layer,0
Deal Dropout,0
keep prob,0
Remove nodes,0
Mul,0
Floor,0
paddings,0
gamma (scale),0
bias,0
Mean,0
Var,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self._emit_unary_operation(IR_node, ""contrib.layers.flatten"")",0
skip_prefix = [,0
"""^"",",0
"""train_op"",",0
"""save"",",0
"""gradients"",",0
"""init"",",0
"""global_step"",",0
"""distort_image"",",0
"""Adagrad"",",0
],0
"""Switch""",0
"""Dequantize"",",0
"""RequantizationRange"",",0
"""Requantize"",",0
"""ExpandDims"",",0
"""Cast""",0
load model files into TensorFlow graph,0
model = original_gdef,0
Save it to an output file,0
keep dims,0
axes,0
"name, op",0
epsilon,0
moving variance (var) /read,0
gamma (scale),0
assert False,0
print(output_node.layer),0
beta  (bias),0
moving mean (mean),0
input node,0
output node,0
epsilon,0
beta,0
gamma (scale),0
"output_node = self.get_son(source_node.name, [0, 0, 0, 0], True)",0
assert False,0
print(IR_node),0
TODO: Fused conv and pool with padding is different from defused operators,0
assert variable.get_attr('_output_shapes')[0].dim[0].size == IR_node.attr['kernel_shape'].list.i[-1],0
strides,0
window_shape,0
pool type,0
padding,0
epsilon,0
moving variance (var) /read,0
gamma (scale),0
beta  (bias),0
moving mean (mean),0
print(tensor_content),0
print(IR_node),0
print(source_node.layer),0
assert False,0
print(source_node.layer),0
assert False,0
mean/read,0
print(scopes),0
kwargs['shape'] = self.tensor_shape_to_list(input_node.get_attr('_output_shapes'))[0],0
kwargs['shape'] = self.tensor_shape_to_list(input_node.get_attr('_output_shapes'))[0],0
weights,0
alpha,0
beta,0
weights,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Build network graph,0
strides,0
window_shape,0
pool type,0
padding,0
TODO: Handle square,1
self._print_layer(source_node),0
print (source_node.name),0
"print (self.src_graph.get_parent(source_node.name, [0]).real_name)",0
assert False,0
Convolution,0
Kernel,0
Attributes,0
Bias,0
Activation,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
BlockFunction node,0
redirect the composite's inputs to the true inputs,0
"BlockFunctions are short-circuited, and not added to accum[]",0
Function node,0
OutputVariable node,0
def build(self):,0
"_traverse_graph(self, self.model.root_function)",0
"super(CntkGraph, self).build()",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
For padding,0
print (input_shape),0
print (kernel_shape),0
print (strides),0
Not tested,0
print (input_shape),0
print (kernel_shape),0
print (strides),0
kernel_shape[idx] = (kernel_shape[idx] - 1) * dilation_rate + 1,0
print ([0] + up_list + [0] + down_list if data_format.startswith('NC') else up_list + [0] + down_list + [0]),0
print ('-----------------------------------------------------'),0
network library,0
not support multi-threads download,0
key: layer_name    value: keras layer,0
private functions,0
"print ('{} --> {}'.format(src, dst))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
share functions,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
name --> (weight_name --> ndarray),0
Generated by the protocol buffer compiler.  DO NOT EDIT!,0
source: graph.proto,0
@@protoc_insertion_point(imports),0
@@protoc_insertion_point(class_scope:GraphDef),0
@@protoc_insertion_point(class_scope:NodeDef.AttrEntry),0
@@protoc_insertion_point(class_scope:NodeDef),0
@@protoc_insertion_point(class_scope:AttrValue.ListValue),0
@@protoc_insertion_point(class_scope:AttrValue),0
@@protoc_insertion_point(class_scope:TensorShape.Dim),0
@@protoc_insertion_point(class_scope:TensorShape),0
@@protoc_insertion_point(class_scope:LiteralTensor),0
@@protoc_insertion_point(module_scope),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
First try to read it as a binary file.,0
Next try to read it as a text file.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Load the model network,0
Load the model weights,0
load model files into Keras graph,0
"_keras.utils.plot_model(model, ""model.png"", show_shapes = True)",0
Build network graph,0
TODO: Fused conv and pool with padding is different from defused operators,0
TODO: More activation functions,1
for ELU,0
input edge,0
"name, op",0
weights,0
pads,0
filter,0
"[kd, kh, kw, channel_size, filter number]",0
use_bias,0
strides,0
"[1, sd, sh, sw, 1]",0
dilations,0
"[1, dd, dh, dw, 1]",0
activation,0
"name, op",0
input edge,0
padding,0
strides,0
"[1, sd, sh, sw, 1]",0
window_shape,0
"[1, pd, ph, pw, 1]",0
"name, op",0
input edge,0
For concat axis,0
"name, op",0
input edge,0
padding,0
only for training,0
"name, op",0
input edge,0
"name, op",0
input edge,0
Merge Layers,0
only for training,0
"name, op",0
input edge,0
shape,0
only for training,0
"name, op",0
input edge,0
Core Layers,0
"name, op",0
input edge,0
units,0
use_bias,0
weights,0
activation,0
"name, op",0
input edge,0
"name, op",0
input edge,0
input_dim,0
output_dim,0
mask_zero,0
weights,0
"name, op",0
input edge,0
units,0
use_bias,0
"for Keras, drop_out and recurrent_dropout",0
activation,0
"name, op",0
input edge,0
units,0
activation,0
weights,0
Kit TODO : need to search the tf,1
"name, op",0
input edge,0
for target shape,0
print (source_node.layer.function),0
import marshal,0
raw_code = marshal.dumps(source_node.layer.function.__code__),0
print (raw_code),0
print (source_node.layer.get_config()),0
"name, op",0
input edge,0
arguments not implementent,0
print (type(source_node.keras_layer.arguments)),0
"name, op",0
input edge,0
axis,0
"Parameter arrangement in Keras: gamma, beta, mean, variance",0
scale,0
beta,0
mean,0
var,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
rot weights,0
"assert IR_node.get_attr('group', 1) == 1",0
############,0
Operators #,0
############,0
for Keras,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
"if hasattr(layer, '_inbound_nodes'):",0
"print (dir(node), type(node), type(layer))",0
assert False,0
for pred in node._inbound_nodes:,0
Kit: TODO,1
Duplicate models for weight sharing,0
Expand the sub-models,0
console.info(JSON.parse(e.target.result)),0
"default, draw the model.json file",0
window.onload = () => {,0
"d3.json(filePath, (error, json) => {",0
if (error) throw error;,0
// console.info(json),0
draw(json),0
}),0
},0
generate dag,0
IR model or keras model,0
path generator,0
let arrowL = `M ${points[len - 1].x} ${points[len - 1].y} l -7 -7`,0
let arrowR = `M ${points[len - 1].x} ${points[len - 1].y} l  7 -7`,0
select a layer,0
draw,0
"let scale = Math.min(window.innerWidth*0.85 / width, (window.innerHeight-85)/height)",0
"var filter = def.append(""filter"")",0
".attr(""id"", ""dropshadow"")",0
"filter.append(""feGaussianBlur"")",0
".attr(""in"", ""SourceAlpha"")",0
".attr(""stdDeviation"", 4)",0
".attr(""result"", ""blur"");",0
"// filter.append(""feOffset"")",0
"//     .attr(""in"", ""blur"")",0
"//     .attr(""dx"", 2)",0
"//     .attr(""dy"", 2)",0
"//     .attr(""result"", ""offsetBlur"");",0
"var feMerge = filter.append(""feMerge"");",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""offsetBlur"")",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""SourceGraphic"");",0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
"svg.on('keydown', ()=>{console.info('ddd')})",0
".on('mouseover', ()=>{console.info('mouse over')})",0
k = parseFloat(k)*parseFloat(k_),0
"limit x, y",0
// a trick to make text svg transform in MS Edge,0
"d3.selectAll("".labels"").classed(""tempclass"", true);",0
"setTimeout(function () { d3.selectAll("".labels"").classed(""tempclass"", false); }, 40);",0
when shift is down,0
ulti function,0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
space = '',0
console.info(JSON.parse(e.target.result)),0
"default, draw the model.json file",0
window.onload = () => {,0
"d3.json(filePath, (error, json) => {",0
if (error) throw error;,0
// console.info(json),0
draw(json),0
}),0
},0
generate dag,0
IR model or keras model,0
path generator,0
let arrowL = `M ${points[len - 1].x} ${points[len - 1].y} l -7 -7`,0
let arrowR = `M ${points[len - 1].x} ${points[len - 1].y} l  7 -7`,0
select a layer,0
draw,0
"let scale = Math.min(window.innerWidth*0.85 / width, (window.innerHeight-85)/height)",0
"var filter = def.append(""filter"")",0
".attr(""id"", ""dropshadow"")",0
"filter.append(""feGaussianBlur"")",0
".attr(""in"", ""SourceAlpha"")",0
".attr(""stdDeviation"", 4)",0
".attr(""result"", ""blur"");",0
"// filter.append(""feOffset"")",0
"//     .attr(""in"", ""blur"")",0
"//     .attr(""dx"", 2)",0
"//     .attr(""dy"", 2)",0
"//     .attr(""result"", ""offsetBlur"");",0
"var feMerge = filter.append(""feMerge"");",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""offsetBlur"")",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""SourceGraphic"");",0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
"svg.on('keydown', ()=>{console.info('ddd')})",0
".on('mouseover', ()=>{console.info('mouse over')})",0
k = parseFloat(k)*parseFloat(k_),0
"limit x, y",0
// a trick to make text svg transform in MS Edge,0
"d3.selectAll("".labels"").classed(""tempclass"", true);",0
"setTimeout(function () { d3.selectAll("".labels"").classed(""tempclass"", false); }, 40);",0
when shift is down,0
ulti function,0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
space = '',0
Get the long description from the README file,0
Versions should comply with PEP440.  For a discussion on single-sourcing,0
"the version across setup.py and the project code, see",0
https://packaging.python.org/en/latest/single_source_version.html,0
The project's main homepage.,0
Author details,0
Choose your license,0
See https://pypi.python.org/pypi?%3Aaction=list_classifiers,0
How mature is this project? Common values are,0
3 - Alpha,0
4 - Beta,0
5 - Production/Stable,0
Indicate who your project is intended for,0
"Pick your license as you wish (should match ""license"" above)",0
"Specify the Python versions you support here. In particular, ensure",0
"that you indicate whether you support Python 2, Python 3 or both.",0
What does your project relate to?,0
You can just specify the packages manually here if your project is,0
simple. Or you can use find_packages().,0
"Alternatively, if you want to distribute just a my_module.py, uncomment",0
this:,0
"py_modules=[""my_module""],",0
List run-time dependencies here.  These will be installed by pip when,0
"your project is installed. For an analysis of ""install_requires"" vs pip's",0
requirements files see:,0
https://packaging.python.org/en/latest/requirements.html,0
"To provide executable scripts, use entry points in preference to the",0
"""scripts"" keyword. Entry points provide cross-platform support and allow",0
pip to create the appropriate form of executable for the target platform.,0
"self.assertGreater(SNR, self.snr_thresh)",0
"self.assertGreater(PSNR, self.psnr_thresh)",0
get original model prediction result,0
original to IR,0
get original model prediction result,0
download model,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
download model,0
get original model prediction result,0
original to IR,0
IR to code,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
IR to code,0
import converted model,0
"'alexnet'       : [TensorflowEmit, KerasEmit],",0
"'resnet18'      : [TensorflowEmit, KerasEmit],",0
"'inception_resnet_v2' : [CntkEmit, TensorflowEmit, KerasEmit, PytorchEmit], # TODO",0
"'nasnet-a_large' : [TensorflowEmit, KerasEmit, PytorchEmit], # TODO",1
get original model prediction result,0
def test_cntk(self):,0
"self._test_function('cntk', self.CntkParse)",0
print (dir(data)),0
print (i),0
"print (""    {} with shape {}"".format(j, load_weight[i][j].shape))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Base Functions,0
"padding,",0
Ignore it in Pytorch,0
for Keras,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
TODO - remove style transfer 1D hack,1
"Input is 1D but it goes to the width dimension: (1,1,W)",0
"assume (Batch, Channels) - Batch dimension should be dropped",0
"assume (Batch, Sequence-Length, channels)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"for i, in_node in enumerate(layer.input):",0
layer.input[i] = self.IR_graph.get_node(in_node).real_name,0
self._connect_coreml_layers(),0
Add classifier classes (if applicable),0
Set pre-processing paramsters,0
"image_input_names,",0
Return the protobuf spec,0
model = _MLModel(self.builder.spec),0
Get input and output names,0
Get the weights.,0
Dimensions and weights,0
Dilations,0
depth-wise convolution,0
Dilations,0
Get input and output names,0
Pooling layer type,0
"if it's global, set the global flag",0
Padding,0
Get input and output names,0
Get the weights from keras,0
Get input and output names,0
blob_order == 0 if the input blob needs not be rearranged,0
blob_order == 1 if the input blob needs to be rearranged,0
"using keras_layer.input.shape have a ""?"" (Dimension[None] at the front),",0
making a 3D tensor with unknown batch size 4D,0
"print(""input_shape, target_shape"",input_shape,target_shape)",0
Get input and output names,0
Get input and output names,0
print(IR_node.name),0
negate it,0
apply threshold,0
negate it back,0
for Keras,0
Get input and output names,0
Set parameters,0
"Parameter arrangement in Keras: gamma, beta, mean, variance",0
compute adjusted parameters,0
Generated by the protocol buffer compiler.  DO NOT EDIT!,0
source: caffe.proto,0
@@protoc_insertion_point(imports),0
@@protoc_insertion_point(class_scope:caffe.BlobShape),0
@@protoc_insertion_point(class_scope:caffe.BlobProto),0
@@protoc_insertion_point(class_scope:caffe.BlobProtoVector),0
@@protoc_insertion_point(class_scope:caffe.Datum),0
@@protoc_insertion_point(class_scope:caffe.FillerParameter),0
@@protoc_insertion_point(class_scope:caffe.NetParameter),0
@@protoc_insertion_point(class_scope:caffe.SolverParameter),0
@@protoc_insertion_point(class_scope:caffe.SolverState),0
@@protoc_insertion_point(class_scope:caffe.NetState),0
@@protoc_insertion_point(class_scope:caffe.NetStateRule),0
@@protoc_insertion_point(class_scope:caffe.ParamSpec),0
@@protoc_insertion_point(class_scope:caffe.LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.TransformationParameter),0
@@protoc_insertion_point(class_scope:caffe.LossParameter),0
@@protoc_insertion_point(class_scope:caffe.AccuracyParameter),0
@@protoc_insertion_point(class_scope:caffe.ArgMaxParameter),0
@@protoc_insertion_point(class_scope:caffe.ConcatParameter),0
@@protoc_insertion_point(class_scope:caffe.BatchNormParameter),0
@@protoc_insertion_point(class_scope:caffe.BiasParameter),0
@@protoc_insertion_point(class_scope:caffe.ContrastiveLossParameter),0
@@protoc_insertion_point(class_scope:caffe.ConvolutionParameter),0
@@protoc_insertion_point(class_scope:caffe.CropParameter),0
@@protoc_insertion_point(class_scope:caffe.DataParameter),0
@@protoc_insertion_point(class_scope:caffe.DropoutParameter),0
@@protoc_insertion_point(class_scope:caffe.DummyDataParameter),0
@@protoc_insertion_point(class_scope:caffe.EltwiseParameter),0
@@protoc_insertion_point(class_scope:caffe.ELUParameter),0
@@protoc_insertion_point(class_scope:caffe.EmbedParameter),0
@@protoc_insertion_point(class_scope:caffe.ExpParameter),0
@@protoc_insertion_point(class_scope:caffe.FlattenParameter),0
@@protoc_insertion_point(class_scope:caffe.HDF5DataParameter),0
@@protoc_insertion_point(class_scope:caffe.HDF5OutputParameter),0
@@protoc_insertion_point(class_scope:caffe.HingeLossParameter),0
@@protoc_insertion_point(class_scope:caffe.ImageDataParameter),0
@@protoc_insertion_point(class_scope:caffe.InfogainLossParameter),0
@@protoc_insertion_point(class_scope:caffe.InnerProductParameter),0
@@protoc_insertion_point(class_scope:caffe.InputParameter),0
@@protoc_insertion_point(class_scope:caffe.LogParameter),0
@@protoc_insertion_point(class_scope:caffe.LRNParameter),0
@@protoc_insertion_point(class_scope:caffe.MemoryDataParameter),0
@@protoc_insertion_point(class_scope:caffe.MVNParameter),0
@@protoc_insertion_point(class_scope:caffe.ParameterParameter),0
@@protoc_insertion_point(class_scope:caffe.PoolingParameter),0
@@protoc_insertion_point(class_scope:caffe.PowerParameter),0
@@protoc_insertion_point(class_scope:caffe.PythonParameter),0
@@protoc_insertion_point(class_scope:caffe.RecurrentParameter),0
@@protoc_insertion_point(class_scope:caffe.ReductionParameter),0
@@protoc_insertion_point(class_scope:caffe.ReLUParameter),0
@@protoc_insertion_point(class_scope:caffe.ReshapeParameter),0
@@protoc_insertion_point(class_scope:caffe.ScaleParameter),0
@@protoc_insertion_point(class_scope:caffe.SigmoidParameter),0
@@protoc_insertion_point(class_scope:caffe.SliceParameter),0
@@protoc_insertion_point(class_scope:caffe.SoftmaxParameter),0
@@protoc_insertion_point(class_scope:caffe.TanHParameter),0
@@protoc_insertion_point(class_scope:caffe.TileParameter),0
@@protoc_insertion_point(class_scope:caffe.ThresholdParameter),0
@@protoc_insertion_point(class_scope:caffe.WindowDataParameter),0
@@protoc_insertion_point(class_scope:caffe.SPPParameter),0
@@protoc_insertion_point(class_scope:caffe.V1LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.V0LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.PReLUParameter),0
@@protoc_insertion_point(module_scope),0
"TODO: We need to identify whether this is 4D image data, otherwise we shouldn't change the dimension order",1
"Stochastic pooling, for instance.",0
TODO: Axis,0
TODO: Unbiased,1
check if need the Flatten layer,0
if parent.output_shape.height > 1 or parent.output_shape.width > 1:,0
TODO: The gamma parameter has to be set (in node.data?) and this should work.,1
"Also, mean should be set to 0, and var to 1, just to be safe.",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
for test,0
test end,0
"print(""========current_node={}"".format(current_node.layer))",0
"self.add_body(1, ""return n.{}"".format(",0
"','.join([self.IR_graph.get_node(name).real_variable_name for name in self.IR_graph.output_layers])))",0
for test,0
keys = [],0
for key in self.weights_dict[IR_node.name].keys():,0
keys.append(key),0
"print(""=======Layer: {}, keys: {}"".format(IR_node.name, keys))",0
"def emit_Tanh(self, IR_node):",0
"self._emit_activation(IR_node, 'ops.tanh')",0
Caffe Types,0
Same value applies to all spatial dimensions,0
Extract the value for the given spatial dimension,0
consider rewrite this function to Network.py,0
Dropout layers appear in a fair number of Caffe,0
test-time networks. These are just ignored. We'll,0
filter them out here.,0
TODO: raise error,1
TODO: raise error,1
Automatically set a name if not provided.,0
Figure out the layer inputs.,0
"print('op: %s   shape: %s' % (op, layer_output._keras_shape))",0
"print('op: %s   shape: %s' % (op, layer_output.get_shape().as_list()))",0
Add to layer LUT.,0
Old-style input specification,0
New-style input specification,0
"We most likely have a data layer on our hands. The problem is,",0
Caffe infers the dimensions of the data from the source (eg: LMDB).,0
We want to avoid reading datasets here. Fail for now.,0
This can be temporarily fixed by transforming the data layer to,0
"Caffe's ""input"" layer (as is usually used in the ""deploy"" version).",0
TODO: Find a better solution for this.,1
The .prototxt file defining the graph,0
The .caffemodel file containing the learned parameters,0
Set to true if the fallback protocol-buffer based backend was used,0
"A list containing (layer name, parameters) tuples",0
Load the parameters,0
"When using the protobuf-backend, each parameter initially has four dimensions.",0
"In certain cases (like FC layers), we want to eliminate the singleton dimensions.",0
"This implementation takes care of the common cases. However, it does leave the",0
potential for future issues.,0
The Caffe-backend does not suffer from this problem.,0
A dictionary mapping NodeKind to the transposed order.,0
The node kinds eligible for reshaping,0
"If true, the reshaped data will replace the old one.",0
"Otherwise, it's set to the reshaped_data attribute.",0
Check for 2+ dimensional data,0
The FC layer connected to the spatial layer needs to be,0
re-wired to match the new spatial ordering.,0
node.reshaped_data = weights.transpose(transpose_order),0
Set the weights,0
We're only fusing nodes with single parents,0
We can only fuse a node if its parent's,0
value isn't used by any other node.,0
Rewrite the fused node's children to its parent.,0
Disconnect the fused node from the graph.,0
Let the sub-class merge the fused node in any arbitrary way.,0
Fuse ReLUs when the parent node is one of the given types.,0
"If None, all node types are eligible.",0
Prescale the stats,0
Replace with the updated values,0
Include the scale and bias terms,0
self.graph = NodeRenamer()(graph),0
"return Graph(name, [self.map_node(node) for node in self.graph.nodes])",0
Kit,0
"mapped_node.input.extend(['%s:%s' % (input.name, idx) for input, idx in node.parents])",0
FIXME:,0
output = node.output,0
Decompose DAG into chains,0
Generate Python code line by line,0
Fall back to the protobuf implementation,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"'alexnet'        : [(657, 0.41121086), (744, 0.20789686), (847, 0.086725488), (821, 0.059082959), (595, 0.058017101)],",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result(None, False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
layer_name = 'block2_pool',0
"intermediate_layer_model = keras.Model(inputs=model.input,",0
outputs=model.get_layer(layer_name).output),0
intermediate_output = intermediate_layer_model.predict(img),0
print (intermediate_output),0
print (intermediate_output.shape),0
"print (""%.30f"" % np.sum(intermediate_output))",0
Load a real image and do default tf imageNet preprocessing,0
"[Batch, Height, Width, Channels] --> [Batch, Channels, Height, Width]",0
We only deal with non-recurrent networks for now,0
"(H,W,C) --> (C,H,W)",0
Load TensorFlow model,0
Prepare inputs,0
Run TF session,0
Evaluate coreml model,0
evaluate the TF model,0
evaluate CoreML,0
Test the default CoreML evaluation,0
Download model,0
Convert to coreml,0
Test predictions on an image,0
Download model,0
Convert to coreml,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
Test predictions on an image,0
"@unittest.skip(""Failing GPU backend: related to https://github.com/tf-coreml/tf-coreml/issues/26"")",0
? style transfer image size and style number?,0
Test predictions on an image,0
evaluate the TF model,0
evaluate CoreML,0
Test the default CoreML evaluation,0
"compare_tf_coreml_outputs(tf_out_flatten, coreml_out_flatten)",0
#Download model,0
url = 'https://storage.googleapis.com/download.tensorflow.org/models/inception_v3_2016_08_28_frozen.pb.tar.gz',0
tf_model_dir = _download_file(url = url),0
"tf_model_path = os.path.join(TMP_MODEL_DIR, 'inception_v3_2016_08_28_frozen.pb')",0
#Convert to coreml,0
"mlmodel_path = os.path.join(TMP_MODEL_DIR, 'inception_v3_2016_08_28.mlmodel')",0
mlmodel = tf_converter.convert(,0
"tf_model_path = tf_model_path,",0
"mlmodel_path = mlmodel_path,",0
"output_feature_names = ['InceptionV3/Predictions/Softmax:0'],",0
"input_name_shape_dict = {'input:0':[1,299,299,3]},",0
"image_input_names = ['input:0'],",0
"red_bias = -1,",0
"green_bias = -1,",0
"blue_bias = -1,",0
image_scale = 2.0/255.0),0
#Test predictions on an image,0
_test_coreml_model_image_input(,0
"tf_model_path = tf_model_path,",0
"coreml_model = mlmodel,",0
"input_tensor_name = 'input:0',",0
"output_tensor_name = 'InceptionV3/Predictions/Softmax:0',",0
img_size = 299),0
Download model,0
Convert to coreml,0
Test predictions on an image,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
self.test_truth(),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
caffe uses NCHW,0
"self.print_intermediate_result('', False)",0
delete tmp model files,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
pylint: disable=line-too-long,0
pylint: enable=line-too-long,0
"net = caffe.Net(arch_fn, weight_fn, caffe.TEST)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
self.print_intermediate_result(),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
need to be updated,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.input, self.model, self.testop = self.MainModel.KitModel(self.args.w)",0
testop = tf.get_default_graph().get_operation_by_name(layer_name),0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"'mobilenet_v1': mobilenet_v1.mobilenet_v1_arg_scope,",0
"'mobilenet_v1' : mobilenet_v1.mobilenet_v1,",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
load file,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Notes for training NASNet Cifar Model,0
-------------------------------------,0
batch_size: 32,0
learning rate: 0.025,0
cosine (single period) learning rate decay,0
auxiliary head loss weighting: 0.4,0
clip global norm of all gradients by 5,0
600 epochs with a batch size of 32,0
This is used for the drop path probabilities since it needs to increase,0
the drop out probability over the course of training.,0
Notes for training large NASNet model on ImageNet,0
-------------------------------------,0
batch size (per replica): 16,0
learning rate: 0.015 * 100,0
learning rate decay factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 100 replicas,0
auxiliary head loss weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Notes for training the mobile NASNet ImageNet model,0
-------------------------------------,0
batch size (per replica): 32,0
learning rate: 0.04 * 50,0
learning rate scaling factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 50 replicas,0
auxiliary head weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Shape of feature map before the final layer.,0
149 x 149 x 32,0
Run the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Find where to place the reduction cells or stride normal cells,0
Setup for building in the auxiliary head.,0
Run the cells,0
true_cell_num accounts for the stem cells,0
Final softmax layer,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80,0
71 x 71 x 192,0
35 x 35 x 192,0
35 x 35 x 320,0
TODO(alemi): Register intermediate endpoints,1
"17 x 17 x 1088 if output_stride == 8,",0
33 x 33 x 1088 if output_stride == 16,0
TODO(alemi): register intermediate endpoints,1
TODO(gpapan): Properly support output_stride for the rest of the net.,1
8 x 8 x 2080,0
TODO(alemi): register intermediate endpoints,1
8 x 8 x 1536,0
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which",1
can be set to False to disable pooling here (as in resnet_*()).,0
Set weight_decay for weights in conv2d and fully_connected layers.,0
Set activation_fn and parameters for batch_norm.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
=============================================================================,0
Tensorflow mandates these.,0
Conv and DepthSepConv namedtuple define layers of the MobileNet architecture,0
Conv defines 3x3 convolution layers,0
DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.,0
stride is the stride of the convolution,0
depth is the number of channels or filters in a layer,0
_CONV_DEFS specifies the MobileNet body,0
Used to find thinned depths for each layer.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
By passing filters=None,0
separable_conv2d produces only a depthwise convolution layer,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 1024,0
Set weight_decay for weights in Conv and DepthSepConv layers.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Skip path 1,0
Skip path 2,0
"First pad with 0's on the right and bottom, then shift the filter to",0
include those 0's that were added.,0
Concat and apply BN,0
Set the prev layer to the current layer if it is none,0
Check to be sure prev layer stuff is setup correctly,0
Apply conv operations,0
Combine hidden states using 'add'.,0
Add hiddenstate to the list of hiddenstates we can choose from,0
Dont stride if this is not one of the original hiddenstates,0
"Check if a stride is needed, then use a strided 1x1 here",0
Determine if a reduction should be applied to make the number of,0
filters match.,0
Return the concat of all the states,0
Scale keep prob by layer number,0
The added 2 is for the reduction cells,0
Decrease the keep probability over time,0
Copyright (c) Microsoft. All rights reserved.,0
Licensed under the MIT license. See LICENSE.md file in the project root,0
for full license information.,0
==============================================================================,0
"self.model, self.testop = self.MainModel.KitModel(self.args.w)",0
"self.print_intermediate_result(None, False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
pylint: disable=line-too-long,0
pylint: enable=line-too-long,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result('', False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
save network structure as JSON,0
layer_name = 'block2_pool',0
"intermediate_layer_model = keras.Model(inputs=model.input,",0
outputs=model.get_layer(layer_name).output),0
intermediate_output = intermediate_layer_model.predict(img),0
print (intermediate_output),0
print (intermediate_output.shape),0
"print (""%.30f"" % np.sum(intermediate_output))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Convert network architecture only,0
MXNet,0
Caffe,0
For CoreML,0
Caffe,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check non-sense always input module.Module,0
if not (type(model) == mx.module.Module,0
or type(model) == mx.module.SequentialModule,0
or type(model) == mx.model),0
"raise TypeError(""MXNet layer of type %s is not supported."" % type(model))",0
"if layer[""op""] == ""null"":",0
continue,0
"raise NotImplementedError(""Cannot support multi-input"")",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"mxnet-cpu only support channel first, default convert the model and weight as channel first",0
download and show the image,0
"convert into format (batch, RGB, width, height)",0
"to show the image, change the argument show into True",0
compute the predict probabilities,0
print the top-5,0
# call function predict,0
"with open('synset.txt', 'r') as f:",0
labels = [l.rstrip() for l in f],0
"predict(model, labels, 'http://writm.com/wp-content/uploads/2016/08/Cat-hd-wallpapers.jpg')",0
"print(""Warning: MXNet Convolution Layer pad does not match IR Convolution Layer pad"")",0
"layout = IR_node.IR_layer.attr[""data_format""].s",0
if layout not in MXNetEmitter.channels_last:,0
Add LeakyReLU Elu(slope not support),0
"axis = IR_node.IR_layer.attr[""axis""].i",0
not supported yet,0
"print(""Warning: MXNet Pooling Layer pad does not match IR Pooling Layer pad"")",0
"def emit_ConvTranspose(self, IR_node):",0
if self.weight_loaded:,0
weight_dict = self.weights[IR_node.name],0
weights = weight_dict['weights'],0
"dim = len(IR_node.IR_layer.attr[""kernel_shape""].list.i) - 2",0
kernel = list(),0
"for idx in range(0, dim):",0
"kernel.append(IR_node.IR_layer.attr[""kernel_shape""].list.i[idx])",0
stride = list(),0
"for e in IR_node.IR_layer.attr[""strides""].list.i[1:-1]:",0
stride.append(e),0
dilate = list(),0
"for e in IR_node.IR_layer.attr[""dilations""].list.i[1:-1]:",0
dilate.append(e),0
"dilate = ', '.join('%s' % i for i in dilate)",0
defuse_pad = False,0
pad = list(),0
"if ""pads"" in IR_node.IR_layer.attr:",0
output_shape = list(),0
"for e in IR_node.IR_layer.attr[""_output_shapes""].list.shape[0].dim:",0
output_shape.append(e.size),0
"# print(""Warning: MXNet Deconvolution Layer pad does not match IR Deconvolution Layer pad"")",0
"defuse_pad, pad = MXNetEmitter.transfer_pad(IR_node.IR_layer.attr[""pads""].list.i)",0
"pad = ', '.join('%s' % i for i in pad)",0
"kernel = ', '.join('%s' % i for i in kernel)",0
"stride = ', '.join('%s' % i for i in stride)",0
"num_filter = IR_node.IR_layer.attr[""kernel_shape""].list.i[-2]",0
"no_bias = not IR_node.IR_layer.attr[""use_bias""].b",0
if not no_bias and self.weight_loaded:,0
"self.output_weights[IR_node.replace_scope(IR_node.name) + ""_bias""] = weight_dict['bias']",0
"# layout = IR_node.IR_layer.attr[""data_format""].s",0
if dim == 1:,0
layout = 'NCW',0
elif dim == 2:,0
layout = 'NCHW',0
elif dim == 3:,0
layout = 'NCDHW',0
if self.weight_loaded:,0
# if layout not in MXNetEmitter.channels_last:,0
"weights = MXNetEmitter.transpose(weights, dim)",0
"self.output_weights[IR_node.replace_scope(IR_node.name) + ""_weight""] = weights",0
"code = """"",0
if not defuse_pad:,0
"code = ""{:<15} = mx.sym.Deconvolution(data = {}, kernel = ({}), stride = ({}), dilate = ({}), pad = ({}), num_filter = {}, no_bias = {}, layout = '{}', name = '{}')"".format(",0
"IR_node.replace_scope(IR_node.name),",0
"IR_node.replace_scope(IR_node.in_edges[0]),",0
"kernel,",0
"stride,",0
"dilate,",0
"pad,",0
"num_filter,",0
"no_bias,",0
"layout,",0
IR_node.replace_scope(IR_node.name)),0
else:,0
"code = self.set_pad(IR_node, code, pad)",0
"code += ""\n    {:<15} = mx.sym.Deconvolution(data = {}, kernel = ({}), stride = ({}), dilate = ({}), num_filter = {}, no_bias = {}, layout = '{}', name = '{}')"".format(",0
"IR_node.replace_scope(IR_node.name), IR_node.replace_scope(IR_node.name) + ""_pad"", kernel, stride, dilate, num_filter, no_bias, layout, IR_node.replace_scope(IR_node.name))",0
return code,0
"def emit_LeakyReLU(self, IR_node):",0
"# IR only support Elu, the same problem with func emit_Activation",0
"code = ""{:<15} = mx.sym.LeakyReLU(data = {}, )"".format()",0
return code,0
raise NotImplementedError,0
reverse cannot support yet,0
"code = ""{:<15} = mx.sym.transpose(data = {}, axes = (0, 2, 3, 1))\n"".format(""trans"", self.parent_variable_name(IR_node))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Not support yet,0
"""softrelu""  : ""SoftReLU""",0
Load the model network and weights,0
model.bind(data_shapes = data_shapes),0
model.init_params(),0
"mod.load(model_path, epoch_num)",0
return mod.get_params(),0
load the model network,0
adjust the data format,0
load model files into MXNet graph,0
data_shape arguments added to calculate infer_shape(required),0
"if isinstance(input_arg, basestring):",0
Build network graph,0
transpose to channel last,0
"name, op",0
input edge,0
output shape,0
input edge,0
attr,0
"print(""Warning: MXNet symbol pad does not support channel last"")",0
"IR_node.attr[""pads""].list.i.extend([0, 0])",0
for e in pad:,0
"IR_node.attr[""pads""].list.i.extend([e, e])",0
"IR_node.attr[""pads""].list.i.extend([0, 0])",0
"name, op",0
input edge,0
"name, op",0
input edge,0
attr,0
units,0
use bias (no_bias default = False),0
output shape,0
weights,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [NM]"".format(source_node.name, self.data_format))",0
input edge,0
output shape,0
attr,0
kernel_shape,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [{}]"".format(source_node.name, self.data_format, layout))",0
groups,0
in_channel = in_channel // group,0
use_bias (no_bias default = False),0
strides,0
dilations,0
data_format,0
padding,0
weights,0
output shape,0
"name, op",0
input edge,0
output shape,0
axis,0
scale,0
epsilon,0
momentum,0
weights,0
gamma,0
beta,0
"if MXNetParser.str2bool(layer_attr.get(""use_global_stats"", ""False"")):",0
mean,0
var,0
"name, op",0
input edge,0
pooling type (sum not allowed yet),0
strides,0
kernel_shape,0
padding,0
output shape,0
"name, op",0
input edge,0
output shape,0
"name, op",0
input edge,0
dim,0
output shape,0
"def rename_log_softmax(self, source_node):",0
"raise NotImplementedError(""not support yet"")",0
"def rename_Correlation(self, source_node):",0
"raise NotImplementedError(""not support yet"")",0
"name, op",0
input edge,0
padding,0
output shape,0
kernel_shape,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [{}]"".format(source_node.name, self.data_format, layout))",0
use_bias (no_bias default = False),0
strides,0
dilations,0
data_format,0
groups,0
weights,0
"def rename_RNN(self, source_node):",0
"raise NotImplementedError(""RNN not support yet"")",0
"name, op",0
input edge,0
attr,0
input_dim,0
output_dim,0
dtype,0
output shape,0
"IR only support elu from {'elu', 'leaky', 'prelu', 'rrelu'}",0
judge whether meaningful,0
"name, op",0
input edge,0
attr,0
"alpha [exp(x) - alpha], but mxnet attr slope [slope*(exp(x) - 1)] when x < 0",0
"IR_node.attr[""alpha""].f = float()",0
output shape,0
"raise NotImplementedError(""slope cannot convert to alpha"")",0
"def rename_InstanceNorm(self, source_node):",0
raise NotImplementedError,0
"def rename_L2Normalization(self, source_node):",0
raise NotImplementedError,0
"name, op",0
input edge,0
alpha,0
beta,0
knorm,0
nsize,0
output shape,0
"def rename_ROIPooling(self, source_node):",0
raise NotImplementedError,0
"name, op",0
input edge,0
keep_prob,0
mode,0
output shape,0
reverse cannot support yet,0
"name, op",0
input edge,0
old API target_shape not support yet,0
output shape,0
"raise NotImplementedError(""adjust output shape"")",0
"name, op",0
input edge,0
output shape,0
"name, op",0
output shape,0
input edge,0
attr,0
dim,0
"name, op",0
input edge,0
attr,0
dtype,0
output shape,0
"name, op",0
input edge,0
output shape,0
attr,0
axis,0
"name, op",0
input edge,0
output shape,0
"raise NotImplementedError(""No matching IR api"")",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
keep dims,0
axes,0
"name, op",0
epsilon,0
moving variance (var),0
gamma (scale),0
mean,0
bias,0
input node,0
output node,0
load model files into TensorFlow graph,0
Build network graph,0
TODO: Fused conv and pool with padding is different from defused operators,0
strides,0
window_shape,0
pool type,0
padding,0
shape,0
strides,0
input[1] : W,0
filter,0
padding,0
output[0] : B,0
converted [dropout],0
convert [tf.contrib.layers.batch_norm],0
normal Add,0
units,0
Weights,0
FullyConnected Layer,0
"name, op",0
get Bias,0
Matmul Layer,0
Deal Dropout,0
keep prob,0
Remove nodes,0
Mul,0
Floor,0
paddings,0
gamma (scale),0
bias,0
Mean,0
Var,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self._emit_unary_operation(IR_node, ""contrib.layers.flatten"")",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Build network graph,0
strides,0
window_shape,0
pool type,0
padding,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
BlockFunction node,0
redirect the composite's inputs to the true inputs,0
"BlockFunctions are short-circuited, and not added to accum[]",0
Function node,0
OutputVariable node,0
def build(self):,0
"_traverse_graph(self, self.model.root_function)",0
"super(CntkGraph, self).build()",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
For padding,0
print (input_shape),0
print (kernel_shape),0
print (strides),0
Not tested,0
print (input_shape),0
print (kernel_shape),0
print (strides),0
kernel_shape[idx] = (kernel_shape[idx] - 1) * dilation_rate + 1,0
print ([0] + up_list + [0] + down_list if data_format.startswith('NC') else up_list + [0] + down_list + [0]),0
print ('-----------------------------------------------------'),0
network library,0
not support multi-threads download,0
key: layer_name    value: keras layer,0
private functions,0
"print ('{} --> {}'.format(src, dst))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
share functions,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
name --> (weight_name --> ndarray),0
Generated by the protocol buffer compiler.  DO NOT EDIT!,0
source: graph.proto,0
@@protoc_insertion_point(imports),0
@@protoc_insertion_point(class_scope:GraphDef),0
@@protoc_insertion_point(class_scope:NodeDef.AttrEntry),0
@@protoc_insertion_point(class_scope:NodeDef),0
@@protoc_insertion_point(class_scope:AttrValue.ListValue),0
@@protoc_insertion_point(class_scope:AttrValue),0
@@protoc_insertion_point(class_scope:TensorShape.Dim),0
@@protoc_insertion_point(class_scope:TensorShape),0
@@protoc_insertion_point(class_scope:LiteralTensor),0
@@protoc_insertion_point(module_scope),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
First try to read it as a binary file.,0
Next try to read it as a text file.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Load the model network,0
Load the model weights,0
load model files into Keras graph,0
"_keras.utils.plot_model(model, ""model.png"", show_shapes = True)",0
Build network graph,0
TODO: Fused conv and pool with padding is different from defused operators,0
TODO: More activation functions,1
for ELU,0
input edge,0
"name, op",0
weights,0
pads,0
filter,0
"[kd, kh, kw, channel_size, filter number]",0
use_bias,0
strides,0
"[1, sd, sh, sw, 1]",0
dilations,0
"[1, dd, dh, dw, 1]",0
activation,0
"name, op",0
input edge,0
padding,0
strides,0
"[1, sd, sh, sw, 1]",0
window_shape,0
"[1, pd, ph, pw, 1]",0
"name, op",0
input edge,0
For concat axis,0
"name, op",0
input edge,0
padding,0
only for training,0
"name, op",0
input edge,0
"name, op",0
input edge,0
Merge Layers,0
only for training,0
"name, op",0
input edge,0
shape,0
only for training,0
"name, op",0
input edge,0
Core Layers,0
"name, op",0
input edge,0
units,0
use_bias,0
weights,0
activation,0
"name, op",0
input edge,0
"name, op",0
input edge,0
input_dim,0
output_dim,0
mask_zero,0
weights,0
"name, op",0
input edge,0
units,0
use_bias,0
"for Keras, drop_out and recurrent_dropout",0
activation,0
"name, op",0
input edge,0
units,0
activation,0
Kit TODO : need to search the tf,1
"name, op",0
input edge,0
for target shape,0
print (source_node.layer.function),0
import marshal,0
raw_code = marshal.dumps(source_node.layer.function.__code__),0
print (raw_code),0
print (source_node.layer.get_config()),0
"name, op",0
input edge,0
arguments not implementent,0
print (type(source_node.keras_layer.arguments)),0
"name, op",0
input edge,0
axis,0
"Parameter arrangement in Keras: gamma, beta, mean, variance",0
scale,0
beta,0
mean,0
var,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
rot weights,0
"assert IR_node.get_attr('group', 1) == 1",0
############,0
Operators #,0
############,0
for Keras,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
"if hasattr(layer, '_inbound_nodes'):",0
"print (dir(node), type(node), type(layer))",0
assert False,0
for pred in node._inbound_nodes:,0
Kit: TODO,1
Duplicate models for weight sharing,0
Expand the sub-models,0
console.info(JSON.parse(e.target.result)),0
"default, draw the model.json file",0
window.onload = () => {,0
"d3.json(filePath, (error, json) => {",0
if (error) throw error;,0
// console.info(json),0
draw(json),0
}),0
},0
generate dag,0
IR model or keras model,0
path generator,0
let arrowL = `M ${points[len - 1].x} ${points[len - 1].y} l -7 -7`,0
let arrowR = `M ${points[len - 1].x} ${points[len - 1].y} l  7 -7`,0
select a layer,0
draw,0
"let scale = Math.min(window.innerWidth*0.85 / width, (window.innerHeight-85)/height)",0
"var filter = def.append(""filter"")",0
".attr(""id"", ""dropshadow"")",0
"filter.append(""feGaussianBlur"")",0
".attr(""in"", ""SourceAlpha"")",0
".attr(""stdDeviation"", 4)",0
".attr(""result"", ""blur"");",0
"// filter.append(""feOffset"")",0
"//     .attr(""in"", ""blur"")",0
"//     .attr(""dx"", 2)",0
"//     .attr(""dy"", 2)",0
"//     .attr(""result"", ""offsetBlur"");",0
"var feMerge = filter.append(""feMerge"");",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""offsetBlur"")",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""SourceGraphic"");",0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
"svg.on('keydown', ()=>{console.info('ddd')})",0
".on('mouseover', ()=>{console.info('mouse over')})",0
k = parseFloat(k)*parseFloat(k_),0
"limit x, y",0
// a trick to make text svg transform in MS Edge,0
"d3.selectAll("".labels"").classed(""tempclass"", true);",0
"setTimeout(function () { d3.selectAll("".labels"").classed(""tempclass"", false); }, 40);",0
when shift is down,0
ulti function,0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
space = '',0
console.info(JSON.parse(e.target.result)),0
"default, draw the model.json file",0
window.onload = () => {,0
"d3.json(filePath, (error, json) => {",0
if (error) throw error;,0
// console.info(json),0
draw(json),0
}),0
},0
generate dag,0
IR model or keras model,0
path generator,0
let arrowL = `M ${points[len - 1].x} ${points[len - 1].y} l -7 -7`,0
let arrowR = `M ${points[len - 1].x} ${points[len - 1].y} l  7 -7`,0
select a layer,0
draw,0
"let scale = Math.min(window.innerWidth*0.85 / width, (window.innerHeight-85)/height)",0
"var filter = def.append(""filter"")",0
".attr(""id"", ""dropshadow"")",0
"filter.append(""feGaussianBlur"")",0
".attr(""in"", ""SourceAlpha"")",0
".attr(""stdDeviation"", 4)",0
".attr(""result"", ""blur"");",0
"// filter.append(""feOffset"")",0
"//     .attr(""in"", ""blur"")",0
"//     .attr(""dx"", 2)",0
"//     .attr(""dy"", 2)",0
"//     .attr(""result"", ""offsetBlur"");",0
"var feMerge = filter.append(""feMerge"");",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""offsetBlur"")",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""SourceGraphic"");",0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
"svg.on('keydown', ()=>{console.info('ddd')})",0
".on('mouseover', ()=>{console.info('mouse over')})",0
k = parseFloat(k)*parseFloat(k_),0
"limit x, y",0
// a trick to make text svg transform in MS Edge,0
"d3.selectAll("".labels"").classed(""tempclass"", true);",0
"setTimeout(function () { d3.selectAll("".labels"").classed(""tempclass"", false); }, 40);",0
when shift is down,0
ulti function,0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
space = '',0
Get the long description from the README file,0
Versions should comply with PEP440.  For a discussion on single-sourcing,0
"the version across setup.py and the project code, see",0
https://packaging.python.org/en/latest/single_source_version.html,0
The project's main homepage.,0
Author details,0
Choose your license,0
See https://pypi.python.org/pypi?%3Aaction=list_classifiers,0
How mature is this project? Common values are,0
3 - Alpha,0
4 - Beta,0
5 - Production/Stable,0
Indicate who your project is intended for,0
"Pick your license as you wish (should match ""license"" above)",0
"Specify the Python versions you support here. In particular, ensure",0
"that you indicate whether you support Python 2, Python 3 or both.",0
What does your project relate to?,0
You can just specify the packages manually here if your project is,0
simple. Or you can use find_packages().,0
"Alternatively, if you want to distribute just a my_module.py, uncomment",0
this:,0
"py_modules=[""my_module""],",0
List run-time dependencies here.  These will be installed by pip when,0
"your project is installed. For an analysis of ""install_requires"" vs pip's",0
requirements files see:,0
https://packaging.python.org/en/latest/requirements.html,0
"To provide executable scripts, use entry points in preference to the",0
"""scripts"" keyword. Entry points provide cross-platform support and allow",0
pip to create the appropriate form of executable for the target platform.,0
print (dir(data)),0
print (i),0
"print (""    {} with shape {}"".format(j, load_weight[i][j].shape))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Base Functions,0
"padding,",0
Ignore it in Pytorch,0
for Keras,0
Generated by the protocol buffer compiler.  DO NOT EDIT!,0
source: caffe.proto,0
@@protoc_insertion_point(imports),0
@@protoc_insertion_point(class_scope:caffe.BlobShape),0
@@protoc_insertion_point(class_scope:caffe.BlobProto),0
@@protoc_insertion_point(class_scope:caffe.BlobProtoVector),0
@@protoc_insertion_point(class_scope:caffe.Datum),0
@@protoc_insertion_point(class_scope:caffe.FillerParameter),0
@@protoc_insertion_point(class_scope:caffe.NetParameter),0
@@protoc_insertion_point(class_scope:caffe.SolverParameter),0
@@protoc_insertion_point(class_scope:caffe.SolverState),0
@@protoc_insertion_point(class_scope:caffe.NetState),0
@@protoc_insertion_point(class_scope:caffe.NetStateRule),0
@@protoc_insertion_point(class_scope:caffe.ParamSpec),0
@@protoc_insertion_point(class_scope:caffe.LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.TransformationParameter),0
@@protoc_insertion_point(class_scope:caffe.LossParameter),0
@@protoc_insertion_point(class_scope:caffe.AccuracyParameter),0
@@protoc_insertion_point(class_scope:caffe.ArgMaxParameter),0
@@protoc_insertion_point(class_scope:caffe.ConcatParameter),0
@@protoc_insertion_point(class_scope:caffe.BatchNormParameter),0
@@protoc_insertion_point(class_scope:caffe.BiasParameter),0
@@protoc_insertion_point(class_scope:caffe.ContrastiveLossParameter),0
@@protoc_insertion_point(class_scope:caffe.ConvolutionParameter),0
@@protoc_insertion_point(class_scope:caffe.CropParameter),0
@@protoc_insertion_point(class_scope:caffe.DataParameter),0
@@protoc_insertion_point(class_scope:caffe.DropoutParameter),0
@@protoc_insertion_point(class_scope:caffe.DummyDataParameter),0
@@protoc_insertion_point(class_scope:caffe.EltwiseParameter),0
@@protoc_insertion_point(class_scope:caffe.ELUParameter),0
@@protoc_insertion_point(class_scope:caffe.EmbedParameter),0
@@protoc_insertion_point(class_scope:caffe.ExpParameter),0
@@protoc_insertion_point(class_scope:caffe.FlattenParameter),0
@@protoc_insertion_point(class_scope:caffe.HDF5DataParameter),0
@@protoc_insertion_point(class_scope:caffe.HDF5OutputParameter),0
@@protoc_insertion_point(class_scope:caffe.HingeLossParameter),0
@@protoc_insertion_point(class_scope:caffe.ImageDataParameter),0
@@protoc_insertion_point(class_scope:caffe.InfogainLossParameter),0
@@protoc_insertion_point(class_scope:caffe.InnerProductParameter),0
@@protoc_insertion_point(class_scope:caffe.InputParameter),0
@@protoc_insertion_point(class_scope:caffe.LogParameter),0
@@protoc_insertion_point(class_scope:caffe.LRNParameter),0
@@protoc_insertion_point(class_scope:caffe.MemoryDataParameter),0
@@protoc_insertion_point(class_scope:caffe.MVNParameter),0
@@protoc_insertion_point(class_scope:caffe.ParameterParameter),0
@@protoc_insertion_point(class_scope:caffe.PoolingParameter),0
@@protoc_insertion_point(class_scope:caffe.PowerParameter),0
@@protoc_insertion_point(class_scope:caffe.PythonParameter),0
@@protoc_insertion_point(class_scope:caffe.RecurrentParameter),0
@@protoc_insertion_point(class_scope:caffe.ReductionParameter),0
@@protoc_insertion_point(class_scope:caffe.ReLUParameter),0
@@protoc_insertion_point(class_scope:caffe.ReshapeParameter),0
@@protoc_insertion_point(class_scope:caffe.ScaleParameter),0
@@protoc_insertion_point(class_scope:caffe.SigmoidParameter),0
@@protoc_insertion_point(class_scope:caffe.SliceParameter),0
@@protoc_insertion_point(class_scope:caffe.SoftmaxParameter),0
@@protoc_insertion_point(class_scope:caffe.TanHParameter),0
@@protoc_insertion_point(class_scope:caffe.TileParameter),0
@@protoc_insertion_point(class_scope:caffe.ThresholdParameter),0
@@protoc_insertion_point(class_scope:caffe.WindowDataParameter),0
@@protoc_insertion_point(class_scope:caffe.SPPParameter),0
@@protoc_insertion_point(class_scope:caffe.V1LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.V0LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.PReLUParameter),0
@@protoc_insertion_point(module_scope),0
"TODO: We need to identify whether this is 4D image data, otherwise we shouldn't change the dimension order",1
"Stochastic pooling, for instance.",0
TODO: Axis,0
TODO: Unbiased,1
check if need the Flatten layer,0
if parent.output_shape.height > 1 or parent.output_shape.width > 1:,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Caffe Types,0
Same value applies to all spatial dimensions,0
Extract the value for the given spatial dimension,0
consider rewrite this function to Network.py,0
Dropout layers appear in a fair number of Caffe,0
test-time networks. These are just ignored. We'll,0
filter them out here.,0
TODO: raise error,1
TODO: raise error,1
Automatically set a name if not provided.,0
Figure out the layer inputs.,0
"print('op: %s   shape: %s' % (op, layer_output._keras_shape))",0
"print('op: %s   shape: %s' % (op, layer_output.get_shape().as_list()))",0
Add to layer LUT.,0
Old-style input specification,0
New-style input specification,0
"We most likely have a data layer on our hands. The problem is,",0
Caffe infers the dimensions of the data from the source (eg: LMDB).,0
We want to avoid reading datasets here. Fail for now.,0
This can be temporarily fixed by transforming the data layer to,0
"Caffe's ""input"" layer (as is usually used in the ""deploy"" version).",0
TODO: Find a better solution for this.,1
The .prototxt file defining the graph,0
The .caffemodel file containing the learned parameters,0
Set to true if the fallback protocol-buffer based backend was used,0
"A list containing (layer name, parameters) tuples",0
Load the parameters,0
"When using the protobuf-backend, each parameter initially has four dimensions.",0
"In certain cases (like FC layers), we want to eliminate the singleton dimensions.",0
"This implementation takes care of the common cases. However, it does leave the",0
potential for future issues.,0
The Caffe-backend does not suffer from this problem.,0
A dictionary mapping NodeKind to the transposed order.,0
The node kinds eligible for reshaping,0
"If true, the reshaped data will replace the old one.",0
"Otherwise, it's set to the reshaped_data attribute.",0
Check for 2+ dimensional data,0
The FC layer connected to the spatial layer needs to be,0
re-wired to match the new spatial ordering.,0
node.reshaped_data = weights.transpose(transpose_order),0
Set the weights,0
We're only fusing nodes with single parents,0
We can only fuse a node if its parent's,0
value isn't used by any other node.,0
Rewrite the fused node's children to its parent.,0
Disconnect the fused node from the graph.,0
Let the sub-class merge the fused node in any arbitrary way.,0
Fuse ReLUs when the parent node is one of the given types.,0
"If None, all node types are eligible.",0
Prescale the stats,0
Replace with the updated values,0
Include the scale and bias terms,0
self.graph = NodeRenamer()(graph),0
"return Graph(name, [self.map_node(node) for node in self.graph.nodes])",0
Kit,0
"mapped_node.input.extend(['%s:%s' % (input.name, idx) for input, idx in node.parents])",0
FIXME:,0
output = node.output,0
Decompose DAG into chains,0
Generate Python code line by line,0
Fall back to the protobuf implementation,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result(None, False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
layer_name = 'block2_pool',0
"intermediate_layer_model = keras.Model(inputs=model.input,",0
outputs=model.get_layer(layer_name).output),0
intermediate_output = intermediate_layer_model.predict(img),0
print (intermediate_output),0
print (intermediate_output.shape),0
"print (""%.30f"" % np.sum(intermediate_output))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
pylint: disable=line-too-long,0
pylint: enable=line-too-long,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
self.print_intermediate_result(),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
need to be updated,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.input, self.model, self.testop = self.MainModel.KitModel(self.args.w)",0
testop = tf.get_default_graph().get_operation_by_name(layer_name),0
"self.print_intermediate_result('conv1_7x7_s2_1', True)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"'mobilenet_v1': mobilenet_v1.mobilenet_v1_arg_scope,",0
"'mobilenet_v1' : mobilenet_v1.mobilenet_v1,",0
Copyright (c) Microsoft. All rights reserved.,0
Licensed under the MIT license. See LICENSE.md file in the project root,0
for full license information.,0
==============================================================================,0
"self.model, self.testop = self.MainModel.KitModel(self.args.w)",0
"self.print_intermediate_result(None, False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result('block2_pool', False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
save network structure as JSON,0
layer_name = 'block2_pool',0
"intermediate_layer_model = keras.Model(inputs=model.input,",0
outputs=model.get_layer(layer_name).output),0
intermediate_output = intermediate_layer_model.predict(img),0
print (intermediate_output),0
print (intermediate_output.shape),0
"print (""%.30f"" % np.sum(intermediate_output))",0
MXNet,0
only convert network structure,0
Caffe,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check non-sense always input module.Module,0
if not (type(model) == mx.module.Module,0
or type(model) == mx.module.SequentialModule,0
or type(model) == mx.model),0
"raise TypeError(""MXNet layer of type %s is not supported."" % type(model))",0
"if layer[""op""] == ""null"":",0
continue,0
"raise NotImplementedError(""Cannot support multi-input"")",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"mxnet-cpu only support channel first, default convert the model and weight as channel first",0
download and show the image,0
"convert into format (batch, RGB, width, height)",0
"to show the image, change the argument show into True",0
compute the predict probabilities,0
print the top-5,0
# call function predict,0
"with open('synset.txt', 'r') as f:",0
labels = [l.rstrip() for l in f],0
"predict(model, labels, 'http://writm.com/wp-content/uploads/2016/08/Cat-hd-wallpapers.jpg')",0
"var = raw_input(""Input layer not detected, please type data shape manually(i.e. X, X, X, X): "")",0
"print(""Warning: MXNet Convolution Layer pad does not match IR Convolution Layer pad"")",0
"layout = IR_node.IR_layer.attr[""data_format""].s",0
if layout not in MXNetEmitter.channels_last:,0
Add LeakyReLU Elu(slope not support),0
"axis = IR_node.IR_layer.attr[""axis""].i",0
not supported yet,0
"print(""Warning: MXNet Pooling Layer pad does not match IR Pooling Layer pad"")",0
"def emit_ConvTranspose(self, IR_node):",0
if self.weight_loaded:,0
weight_dict = self.weights[IR_node.name],0
weights = weight_dict['weights'],0
"dim = len(IR_node.IR_layer.attr[""kernel_shape""].list.i) - 2",0
kernel = list(),0
"for idx in range(0, dim):",0
"kernel.append(IR_node.IR_layer.attr[""kernel_shape""].list.i[idx])",0
stride = list(),0
"for e in IR_node.IR_layer.attr[""strides""].list.i[1:-1]:",0
stride.append(e),0
dilate = list(),0
"for e in IR_node.IR_layer.attr[""dilations""].list.i[1:-1]:",0
dilate.append(e),0
"dilate = ', '.join('%s' % i for i in dilate)",0
defuse_pad = False,0
pad = list(),0
"if ""pads"" in IR_node.IR_layer.attr:",0
output_shape = list(),0
"for e in IR_node.IR_layer.attr[""_output_shapes""].list.shape[0].dim:",0
output_shape.append(e.size),0
"# print(""Warning: MXNet Deconvolution Layer pad does not match IR Deconvolution Layer pad"")",0
"defuse_pad, pad = MXNetEmitter.transfer_pad(IR_node.IR_layer.attr[""pads""].list.i)",0
"pad = ', '.join('%s' % i for i in pad)",0
"kernel = ', '.join('%s' % i for i in kernel)",0
"stride = ', '.join('%s' % i for i in stride)",0
"num_filter = IR_node.IR_layer.attr[""kernel_shape""].list.i[-2]",0
"no_bias = not IR_node.IR_layer.attr[""use_bias""].b",0
if not no_bias and self.weight_loaded:,0
"self.output_weights[IR_node.replace_scope(IR_node.name) + ""_bias""] = weight_dict['bias']",0
"# layout = IR_node.IR_layer.attr[""data_format""].s",0
if dim == 1:,0
layout = 'NCW',0
elif dim == 2:,0
layout = 'NCHW',0
elif dim == 3:,0
layout = 'NCDHW',0
if self.weight_loaded:,0
# if layout not in MXNetEmitter.channels_last:,0
"weights = MXNetEmitter.transpose(weights, dim)",0
"self.output_weights[IR_node.replace_scope(IR_node.name) + ""_weight""] = weights",0
"code = """"",0
if not defuse_pad:,0
"code = ""{:<15} = mx.sym.Deconvolution(data = {}, kernel = ({}), stride = ({}), dilate = ({}), pad = ({}), num_filter = {}, no_bias = {}, layout = '{}', name = '{}')"".format(",0
"IR_node.replace_scope(IR_node.name),",0
"IR_node.replace_scope(IR_node.in_edges[0]),",0
"kernel,",0
"stride,",0
"dilate,",0
"pad,",0
"num_filter,",0
"no_bias,",0
"layout,",0
IR_node.replace_scope(IR_node.name)),0
else:,0
"code = self.set_pad(IR_node, code, pad)",0
"code += ""\n    {:<15} = mx.sym.Deconvolution(data = {}, kernel = ({}), stride = ({}), dilate = ({}), num_filter = {}, no_bias = {}, layout = '{}', name = '{}')"".format(",0
"IR_node.replace_scope(IR_node.name), IR_node.replace_scope(IR_node.name) + ""_pad"", kernel, stride, dilate, num_filter, no_bias, layout, IR_node.replace_scope(IR_node.name))",0
return code,0
"def emit_LeakyReLU(self, IR_node):",0
"# IR only support Elu, the same problem with func emit_Activation",0
"code = ""{:<15} = mx.sym.LeakyReLU(data = {}, )"".format()",0
return code,0
raise NotImplementedError,0
reverse cannot support yet,0
"code = ""{:<15} = mx.sym.transpose(data = {}, axes = (0, 2, 3, 1))\n"".format(""trans"", self.parent_variable_name(IR_node))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Not support yet,0
"""softrelu""  : ""SoftReLU""",0
Load the model network and weights,0
model.bind(data_shapes = data_shapes),0
model.init_params(),0
"mod.load(model_path, epoch_num)",0
return mod.get_params(),0
load the model network,0
adjust the data format,0
load model files into MXNet graph,0
data_shape arguments added to calculate infer_shape(required),0
"if isinstance(input_arg, basestring):",0
Build network graph,0
transpose to channel last,0
"name, op",0
input edge,0
output shape,0
input edge,0
attr,0
"print(""Warning: MXNet symbol pad does not support channel last"")",0
"IR_node.attr[""pads""].list.i.extend([0, 0])",0
for e in pad:,0
"IR_node.attr[""pads""].list.i.extend([e, e])",0
"IR_node.attr[""pads""].list.i.extend([0, 0])",0
"name, op",0
input edge,0
"name, op",0
input edge,0
attr,0
units,0
use bias (no_bias default = False),0
output shape,0
weights,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [NM]"".format(source_node.name, self.data_format))",0
input edge,0
output shape,0
attr,0
kernel_shape,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [{}]"".format(source_node.name, self.data_format, layout))",0
use_bias (no_bias default = False),0
strides,0
dilations,0
data_format,0
groups,0
padding,0
weights,0
output shape,0
"name, op",0
input edge,0
output shape,0
axis,0
scale,0
epsilon,0
momentum,0
weights,0
gamma,0
beta,0
"if MXNetParser.str2bool(layer_attr.get(""use_global_stats"", ""False"")):",0
mean,0
var,0
"name, op",0
input edge,0
pooling type (sum not allowed yet),0
strides,0
kernel_shape,0
padding,0
output shape,0
"name, op",0
input edge,0
output shape,0
"name, op",0
input edge,0
dim,0
output shape,0
"def rename_log_softmax(self, source_node):",0
"raise NotImplementedError(""not support yet"")",0
"def rename_Correlation(self, source_node):",0
"raise NotImplementedError(""not support yet"")",0
"name, op",0
input edge,0
padding,0
output shape,0
kernel_shape,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [{}]"".format(source_node.name, self.data_format, layout))",0
use_bias (no_bias default = False),0
strides,0
dilations,0
data_format,0
groups,0
weights,0
"def rename_RNN(self, source_node):",0
"raise NotImplementedError(""RNN not support yet"")",0
"name, op",0
input edge,0
attr,0
input_dim,0
output_dim,0
dtype,0
output shape,0
"IR only support elu from {'elu', 'leaky', 'prelu', 'rrelu'}",0
judge whether meaningful,0
"name, op",0
input edge,0
attr,0
"alpha [exp(x) - alpha], but mxnet attr slope [slope*(exp(x) - 1)] when x < 0",0
"IR_node.attr[""alpha""].f = float()",0
output shape,0
"raise NotImplementedError(""slope cannot convert to alpha"")",0
"def rename_InstanceNorm(self, source_node):",0
raise NotImplementedError,0
"def rename_L2Normalization(self, source_node):",0
raise NotImplementedError,0
"name, op",0
input edge,0
alpha,0
beta,0
knorm,0
nsize,0
output shape,0
"def rename_ROIPooling(self, source_node):",0
raise NotImplementedError,0
"name, op",0
input edge,0
keep_prob,0
mode,0
output shape,0
reverse cannot support yet,0
"name, op",0
input edge,0
old API target_shape not support yet,0
output shape,0
"raise NotImplementedError(""adjust output shape"")",0
"name, op",0
input edge,0
output shape,0
"name, op",0
output shape,0
input edge,0
attr,0
dim,0
"name, op",0
input edge,0
attr,0
dtype,0
output shape,0
"name, op",0
input edge,0
output shape,0
attr,0
axis,0
"name, op",0
input edge,0
output shape,0
"raise NotImplementedError(""No matching IR api"")",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
keep dims,0
axes,0
"name, op",0
epsilon,0
moving variance (var),0
gamma (scale),0
mean,0
bias,0
input node,0
output node,0
load model files into Keras graph,0
Build network graph,0
TODO: Fused conv and pool with padding is different from defused operators,0
strides,0
window_shape,0
pool type,0
padding,0
shape,0
strides,0
input[1] : W,0
filter,0
padding,0
output[0] : B,0
converted [dropout],0
convert [tf.contrib.layers.batch_norm],0
normal Add,0
units,0
Weights,0
FullyConnected Layer,0
"name, op",0
get Bias,0
Matmul Layer,0
Deal Dropout,0
keep prob,0
Remove nodes,0
Mul,0
Floor,0
paddings,0
gamma (scale),0
bias,0
Mean,0
Var,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self._emit_unary_operation(IR_node, ""contrib.layers.flatten"")",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
For padding,0
print (input_shape),0
print (kernel_shape),0
print (strides),0
Not tested,0
print (input_shape),0
print (kernel_shape),0
print (strides),0
kernel_shape[idx] = (kernel_shape[idx] - 1) * dilation_rate + 1,0
print ([0] + up_list + [0] + down_list if data_format.startswith('NC') else up_list + [0] + down_list + [0]),0
print ('-----------------------------------------------------'),0
network library,0
not support multi-threads download,0
key: layer_name    value: keras layer,0
private functions,0
"print (""Warning: Graph Construct a self-loop node {}. Ignored."".format(src))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
share functions,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
name --> (weight_name --> ndarray),0
Generated by the protocol buffer compiler.  DO NOT EDIT!,0
source: graph.proto,0
@@protoc_insertion_point(imports),0
@@protoc_insertion_point(class_scope:GraphDef),0
@@protoc_insertion_point(class_scope:NodeDef.AttrEntry),0
@@protoc_insertion_point(class_scope:NodeDef),0
@@protoc_insertion_point(class_scope:AttrValue.ListValue),0
@@protoc_insertion_point(class_scope:AttrValue),0
@@protoc_insertion_point(class_scope:TensorShape.Dim),0
@@protoc_insertion_point(class_scope:TensorShape),0
@@protoc_insertion_point(class_scope:LiteralTensor),0
@@protoc_insertion_point(module_scope),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
First try to read it as a binary file.,0
Next try to read it as a text file.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Load the model network,0
Load the model weights,0
load model files into Keras graph,0
"_keras.utils.plot_model(model, ""model.png"", show_shapes = True)",0
Build network graph,0
TODO: Fused conv and pool with padding is different from defused operators,0
TODO: More activation functions,1
for ELU,0
input edge,0
"name, op",0
weights,0
pads,0
filter,0
"[kd, kh, kw, channel_size, filter number]",0
use_bias,0
strides,0
"[1, sd, sh, sw, 1]",0
dilations,0
"[1, dd, dh, dw, 1]",0
activation,0
"name, op",0
input edge,0
padding,0
strides,0
"[1, sd, sh, sw, 1]",0
window_shape,0
"[1, pd, ph, pw, 1]",0
"name, op",0
input edge,0
For concat axis,0
"name, op",0
input edge,0
padding,0
only for training,0
"name, op",0
input edge,0
"name, op",0
input edge,0
Merge Layers,0
only for training,0
"name, op",0
input edge,0
shape,0
only for training,0
"name, op",0
input edge,0
Core Layers,0
"name, op",0
input edge,0
units,0
use_bias,0
weights,0
activation,0
"name, op",0
input edge,0
"name, op",0
input edge,0
input_dim,0
output_dim,0
mask_zero,0
weights,0
"name, op",0
input edge,0
units,0
use_bias,0
"for Keras, drop_out and recurrent_dropout",0
activation,0
"name, op",0
input edge,0
units,0
activation,0
Kit TODO : need to search the tf,1
"name, op",0
input edge,0
for target shape,0
"name, op",0
input edge,0
arguments not implementent,0
print (type(source_node.keras_layer.arguments)),0
"name, op",0
input edge,0
axis,0
"Parameter arrangement in Keras: gamma, beta, mean, variance",0
scale,0
beta,0
mean,0
var,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
rot weights,0
############,0
Operators #,0
############,0
for Keras,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
Kit: TODO,1
Duplicate models for weight sharing,0
Expand the sub-models,0
console.info(JSON.parse(e.target.result)),0
"default, draw the model.json file",0
window.onload = () => {,0
"d3.json(filePath, (error, json) => {",0
if (error) throw error;,0
// console.info(json),0
draw(json),0
}),0
},0
generate dag,0
IR model or keras model,0
path generator,0
let arrowL = `M ${points[len - 1].x} ${points[len - 1].y} l -7 -7`,0
let arrowR = `M ${points[len - 1].x} ${points[len - 1].y} l  7 -7`,0
select a layer,0
draw,0
"let scale = Math.min(window.innerWidth*0.85 / width, (window.innerHeight-85)/height)",0
"var filter = def.append(""filter"")",0
".attr(""id"", ""dropshadow"")",0
"filter.append(""feGaussianBlur"")",0
".attr(""in"", ""SourceAlpha"")",0
".attr(""stdDeviation"", 4)",0
".attr(""result"", ""blur"");",0
"// filter.append(""feOffset"")",0
"//     .attr(""in"", ""blur"")",0
"//     .attr(""dx"", 2)",0
"//     .attr(""dy"", 2)",0
"//     .attr(""result"", ""offsetBlur"");",0
"var feMerge = filter.append(""feMerge"");",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""offsetBlur"")",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""SourceGraphic"");",0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
"svg.on('keydown', ()=>{console.info('ddd')})",0
".on('mouseover', ()=>{console.info('mouse over')})",0
k = parseFloat(k)*parseFloat(k_),0
"limit x, y",0
// a trick to make text svg transform in MS Edge,0
"d3.selectAll("".labels"").classed(""tempclass"", true);",0
"setTimeout(function () { d3.selectAll("".labels"").classed(""tempclass"", false); }, 40);",0
when shift is down,0
ulti function,0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
space = '',0
Get the long description from the README file,0
Versions should comply with PEP440.  For a discussion on single-sourcing,0
"the version across setup.py and the project code, see",0
https://packaging.python.org/en/latest/single_source_version.html,0
The project's main homepage.,0
Author details,0
Choose your license,0
See https://pypi.python.org/pypi?%3Aaction=list_classifiers,0
How mature is this project? Common values are,0
3 - Alpha,0
4 - Beta,0
5 - Production/Stable,0
Indicate who your project is intended for,0
"Pick your license as you wish (should match ""license"" above)",0
"Specify the Python versions you support here. In particular, ensure",0
"that you indicate whether you support Python 2, Python 3 or both.",0
What does your project relate to?,0
You can just specify the packages manually here if your project is,0
simple. Or you can use find_packages().,0
"Alternatively, if you want to distribute just a my_module.py, uncomment",0
this:,0
"py_modules=[""my_module""],",0
List run-time dependencies here.  These will be installed by pip when,0
"your project is installed. For an analysis of ""install_requires"" vs pip's",0
requirements files see:,0
https://packaging.python.org/en/latest/requirements.html,0
"To provide executable scripts, use entry points in preference to the",0
"""scripts"" keyword. Entry points provide cross-platform support and allow",0
pip to create the appropriate form of executable for the target platform.,0
print (dir(data)),0
print (i),0
"print (""    {} with shape {}"".format(j, load_weight[i][j].shape))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Base Functions,0
https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/conv.py,0
"padding,",0
Ignore it in Pytorch,0
for Keras,0
Generated by the protocol buffer compiler.  DO NOT EDIT!,0
source: caffe.proto,0
@@protoc_insertion_point(imports),0
@@protoc_insertion_point(class_scope:caffe.BlobShape),0
@@protoc_insertion_point(class_scope:caffe.BlobProto),0
@@protoc_insertion_point(class_scope:caffe.BlobProtoVector),0
@@protoc_insertion_point(class_scope:caffe.Datum),0
@@protoc_insertion_point(class_scope:caffe.FillerParameter),0
@@protoc_insertion_point(class_scope:caffe.NetParameter),0
@@protoc_insertion_point(class_scope:caffe.SolverParameter),0
@@protoc_insertion_point(class_scope:caffe.SolverState),0
@@protoc_insertion_point(class_scope:caffe.NetState),0
@@protoc_insertion_point(class_scope:caffe.NetStateRule),0
@@protoc_insertion_point(class_scope:caffe.ParamSpec),0
@@protoc_insertion_point(class_scope:caffe.LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.TransformationParameter),0
@@protoc_insertion_point(class_scope:caffe.LossParameter),0
@@protoc_insertion_point(class_scope:caffe.AccuracyParameter),0
@@protoc_insertion_point(class_scope:caffe.ArgMaxParameter),0
@@protoc_insertion_point(class_scope:caffe.ConcatParameter),0
@@protoc_insertion_point(class_scope:caffe.BatchNormParameter),0
@@protoc_insertion_point(class_scope:caffe.BiasParameter),0
@@protoc_insertion_point(class_scope:caffe.ContrastiveLossParameter),0
@@protoc_insertion_point(class_scope:caffe.ConvolutionParameter),0
@@protoc_insertion_point(class_scope:caffe.CropParameter),0
@@protoc_insertion_point(class_scope:caffe.DataParameter),0
@@protoc_insertion_point(class_scope:caffe.DropoutParameter),0
@@protoc_insertion_point(class_scope:caffe.DummyDataParameter),0
@@protoc_insertion_point(class_scope:caffe.EltwiseParameter),0
@@protoc_insertion_point(class_scope:caffe.ELUParameter),0
@@protoc_insertion_point(class_scope:caffe.EmbedParameter),0
@@protoc_insertion_point(class_scope:caffe.ExpParameter),0
@@protoc_insertion_point(class_scope:caffe.FlattenParameter),0
@@protoc_insertion_point(class_scope:caffe.HDF5DataParameter),0
@@protoc_insertion_point(class_scope:caffe.HDF5OutputParameter),0
@@protoc_insertion_point(class_scope:caffe.HingeLossParameter),0
@@protoc_insertion_point(class_scope:caffe.ImageDataParameter),0
@@protoc_insertion_point(class_scope:caffe.InfogainLossParameter),0
@@protoc_insertion_point(class_scope:caffe.InnerProductParameter),0
@@protoc_insertion_point(class_scope:caffe.InputParameter),0
@@protoc_insertion_point(class_scope:caffe.LogParameter),0
@@protoc_insertion_point(class_scope:caffe.LRNParameter),0
@@protoc_insertion_point(class_scope:caffe.MemoryDataParameter),0
@@protoc_insertion_point(class_scope:caffe.MVNParameter),0
@@protoc_insertion_point(class_scope:caffe.ParameterParameter),0
@@protoc_insertion_point(class_scope:caffe.PoolingParameter),0
@@protoc_insertion_point(class_scope:caffe.PowerParameter),0
@@protoc_insertion_point(class_scope:caffe.PythonParameter),0
@@protoc_insertion_point(class_scope:caffe.RecurrentParameter),0
@@protoc_insertion_point(class_scope:caffe.ReductionParameter),0
@@protoc_insertion_point(class_scope:caffe.ReLUParameter),0
@@protoc_insertion_point(class_scope:caffe.ReshapeParameter),0
@@protoc_insertion_point(class_scope:caffe.ScaleParameter),0
@@protoc_insertion_point(class_scope:caffe.SigmoidParameter),0
@@protoc_insertion_point(class_scope:caffe.SliceParameter),0
@@protoc_insertion_point(class_scope:caffe.SoftmaxParameter),0
@@protoc_insertion_point(class_scope:caffe.TanHParameter),0
@@protoc_insertion_point(class_scope:caffe.TileParameter),0
@@protoc_insertion_point(class_scope:caffe.ThresholdParameter),0
@@protoc_insertion_point(class_scope:caffe.WindowDataParameter),0
@@protoc_insertion_point(class_scope:caffe.SPPParameter),0
@@protoc_insertion_point(class_scope:caffe.V1LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.V0LayerParameter),0
@@protoc_insertion_point(class_scope:caffe.PReLUParameter),0
@@protoc_insertion_point(module_scope),0
"TODO: We need to identify whether this is 4D image data, otherwise we shouldn't change the dimension order",1
"Stochastic pooling, for instance.",0
TODO: Axis,0
TODO: Unbiased,1
check if need the Flatten layer,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Caffe Types,0
Same value applies to all spatial dimensions,0
Extract the value for the given spatial dimension,0
consider rewrite this function to Network.py,0
Dropout layers appear in a fair number of Caffe,0
test-time networks. These are just ignored. We'll,0
filter them out here.,0
TODO: raise error,1
TODO: raise error,1
Automatically set a name if not provided.,0
Figure out the layer inputs.,0
"print('op: %s   shape: %s' % (op, layer_output._keras_shape))",0
"print('op: %s   shape: %s' % (op, layer_output.get_shape().as_list()))",0
Add to layer LUT.,0
Old-style input specification,0
New-style input specification,0
"We most likely have a data layer on our hands. The problem is,",0
Caffe infers the dimensions of the data from the source (eg: LMDB).,0
We want to avoid reading datasets here. Fail for now.,0
This can be temporarily fixed by transforming the data layer to,0
"Caffe's ""input"" layer (as is usually used in the ""deploy"" version).",0
TODO: Find a better solution for this.,1
The .prototxt file defining the graph,0
The .caffemodel file containing the learned parameters,0
Set to true if the fallback protocol-buffer based backend was used,0
"A list containing (layer name, parameters) tuples",0
Load the parameters,0
"When using the protobuf-backend, each parameter initially has four dimensions.",0
"In certain cases (like FC layers), we want to eliminate the singleton dimensions.",0
"This implementation takes care of the common cases. However, it does leave the",0
potential for future issues.,0
The Caffe-backend does not suffer from this problem.,0
A dictionary mapping NodeKind to the transposed order.,0
The node kinds eligible for reshaping,0
"If true, the reshaped data will replace the old one.",0
"Otherwise, it's set to the reshaped_data attribute.",0
Check for 2+ dimensional data,0
The FC layer connected to the spatial layer needs to be,0
re-wired to match the new spatial ordering.,0
node.reshaped_data = weights.transpose(transpose_order),0
Set the weights,0
We're only fusing nodes with single parents,0
We can only fuse a node if its parent's,0
value isn't used by any other node.,0
Rewrite the fused node's children to its parent.,0
Disconnect the fused node from the graph.,0
Let the sub-class merge the fused node in any arbitrary way.,0
Fuse ReLUs when the parent node is one of the given types.,0
"If None, all node types are eligible.",0
Prescale the stats,0
Replace with the updated values,0
Include the scale and bias terms,0
"return Graph(name, [self.map_node(node) for node in self.graph.nodes])",0
Kit,0
"mapped_node.input.extend(['%s:%s' % (input.name, idx) for input, idx in node.parents])",0
FIXME:,0
output = node.output,0
Decompose DAG into chains,0
Generate Python code line by line,0
Fall back to the protobuf implementation,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
work for tf 1.4 in windows & linux,0
work for tf 1.3 & 1.4 in linux,0
from tensorflow.contrib.keras.python.keras.preprocessing import image,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result(None, False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
self.print_intermediate_result(),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.input, self.model, self.testop = KitModel(os.path.abspath('.') + '/kit_imagenet.npy')",0
testop = self.testop,0
"self.print_intermediate_result('conv1_7x7_s2_1', False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"'mobilenet_v1': mobilenet_v1.mobilenet_v1_arg_scope,",0
"'mobilenet_v1' : mobilenet_v1.mobilenet_v1,",0
Copyright (c) Microsoft. All rights reserved.,0
Licensed under the MIT license. See LICENSE.md file in the project root,0
for full license information.,0
==============================================================================,0
"self.model, self.testop = self.MainModel.KitModel(self.args.w)",0
"self.print_intermediate_result(None, False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self.print_intermediate_result('block2_pool', False)",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
save network structure as JSON,0
layer_name = 'block2_pool',0
"intermediate_layer_model = keras.Model(inputs=model.input,",0
outputs=model.get_layer(layer_name).output),0
intermediate_output = intermediate_layer_model.predict(img),0
print (intermediate_output),0
print (intermediate_output.shape),0
"print (""%.30f"" % np.sum(intermediate_output))",0
MXNet,0
only convert network structure,0
Caffe,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check non-sense always input module.Module,0
if not (type(model) == mx.module.Module,0
or type(model) == mx.module.SequentialModule,0
or type(model) == mx.model),0
"raise TypeError(""MXNet layer of type %s is not supported."" % type(model))",0
"if layer[""op""] == ""null"":",0
continue,0
"raise NotImplementedError(""Cannot support multi-input"")",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"mxnet-cpu only support channel first, default convert the model and weight as channel first",0
download and show the image,0
"convert into format (batch, RGB, width, height)",0
"to show the image, change the argument show into True",0
compute the predict probabilities,0
print the top-5,0
# call function predict,0
"with open('synset.txt', 'r') as f:",0
labels = [l.rstrip() for l in f],0
"predict(model, labels, 'http://writm.com/wp-content/uploads/2016/08/Cat-hd-wallpapers.jpg')",0
"var = raw_input(""Input layer not detected, please type data shape manually(i.e. X, X, X, X): "")",0
same_pad = int(math.ceil(float(data_shape) / float(stride))),0
valid_pad = int(math.ceil(float(data_shape - kernel + 1) / float(stride))),0
# if (same_pad - valid_pad) % 2 == 0:,0
"#     return True, (same_pad - valid_pad)",0
# else:,0
"#     return False, (same_pad - valid_pad)",0
return (same_pad - valid_pad),0
# raise NotImplementedError,0
raise NotImplementedError,0
"print(data_shape, kernel, stride)",0
"print(""Warning: MXNet Convolution Layer pad does not match IR Convolution Layer pad"")",0
if layout == '':,0
if dim == 1:,0
layout = 'NCW',0
elif dim == 2:,0
layout = 'NHWC',0
elif dim == 3:,0
layout = 'NDHWC',0
if layout not in MXNetEmitter.channels_last:,0
"code = ""{:<15} = mx.sym.transpose(data = {}, axes = (0, 3, 1, 2))\n"".format(IR_node.replace_scope(IR_node.name) + ""_input"", IR_node.replace_scope(IR_node.in_edges[0]))",0
"code += ""    {:<15} = mx.sym.transpose(data = {}, axes = (0, 2, 3, 1))\n"".format(IR_node.replace_scope(IR_node.name), IR_node.replace_scope(IR_node.name))",0
"code = ""{:<15} = mx.sym.transpose(data = {}, axes = (0, 3, 1, 2))\n"".format(IR_node.replace_scope(IR_node.name) + ""_input"", IR_node.replace_scope(IR_node.in_edges[0]))",0
"code += ""    {:<15} = mx.sym.transpose(data = {}, axes = (0, 2, 3, 1))\n"".format(IR_node.replace_scope(IR_node.name), IR_node.replace_scope(IR_node.name))",0
Add LeakyReLU Elu(slope not support),0
"axis = IR_node.IR_layer.attr[""axis""].i",0
not supported yet,0
"print(""Warning: MXNet Pooling Layer pad does not match IR Pooling Layer pad"")",0
"code = ""{:<15} = mx.sym.transpose(data = {}, axes = (0, 3, 1, 2))\n"".format(IR_node.replace_scope(IR_node.name) + ""_input"", IR_node.replace_scope(IR_node.in_edges[0]))",0
"code += ""    {:<15} = mx.sym.transpose(data = {}, axes = (0, 2, 3, 1))\n"".format(IR_node.replace_scope(IR_node.name), IR_node.replace_scope(IR_node.name))",0
"code = ""{:<15} = mx.sym.transpose(data = {}, axes = (0, 3, 1, 2))\n"".format(IR_node.replace_scope(IR_node.name) + ""_input"", IR_node.replace_scope(IR_node.in_edges[0]))",0
"code += ""    {:<15} = mx.sym.transpose(data = {}, axes = (0, 2, 3, 1))\n"".format(IR_node.replace_scope(IR_node.name), IR_node.replace_scope(IR_node.name))",0
"print(""Warning: MXNet Deconvolution Layer pad does not match IR Deconvolution Layer pad"")",0
if layout == '':,0
if dim == 1:,0
layout = 'NCW',0
elif dim == 2:,0
layout = 'NHWC',0
elif dim == 3:,0
layout = 'NDHWC',0
if layout not in MXNetEmitter.channels_last:,0
"def emit_LeakyReLU(self, IR_node):",0
"# IR only support Elu, the same problem with func emit_Activation",0
"code = ""{:<15} = mx.sym.LeakyReLU(data = {}, )"".format()",0
return code,0
raise NotImplementedError,0
reverse cannot support yet,0
"if ""data_format"" in IR_node.IR_layer.attr:",0
"data_format = IR_node.IR_layer.attr[""data_format""].s",0
else:,0
"data_format = ""NHWC""",0
"print(""set the conv format before flatten as default value NHWC"")",0
if data_format in MXNetEmitter.channels_last:,0
else:,0
"code += ""{:<15} = mx.sym.flatten(data = {}, name = '{}')"".format(",0
"IR_node.replace_scope(IR_node.name),",0
"IR_node.replace_scope(IR_node.in_edges[0]),",0
IR_node.replace_scope(IR_node.name)),0
if not pad_width[2] == 0 or not pad_width[3] == 0:,0
"print(""Warning: please check padding layer manually"")",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Not support yet,0
"""softrelu""  : ""SoftReLU""",0
raise NotImplementedError,0
Load the model network and weights,0
model.bind(data_shapes = data_shapes),0
model.init_params(),0
"mod.load(model_path, epoch_num)",0
return mod.get_params(),0
raise NotImplementedError,0
load the model network,0
adjust the data format,0
raise NotImplementedError,0
load model files into MXNet graph,0
data_shape arguments added to calculate infer_shape(required),0
"if isinstance(input_arg, basestring):",0
Build network graph,0
raise NotImplementedError,0
"for i, j in self.weights.items():",0
"print (""parameter [{}] have weights [{}]"".format(i, len(j)))",0
transpose to channel last,0
"name, op",0
input edge,0
output shape,0
input edge,0
attr,0
"print(""Warning: MXNet symbol pad does not support channel last"")",0
output shape,0
"self.set_output_shape(source_node, IR_node)",0
self.mxnet_graph.layer_name_map[source_node.name] = IR_node.name,0
raise NotImplementedError,0
"name, op",0
input edge,0
raise NotImplementedError,0
"name, op",0
input edge,0
attr,0
units,0
use bias (no_bias default = False),0
output shape,0
weights,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [NM]"".format(source_node.name, self.data_format))",0
raise NotImplementedError,0
"name, op",0
input edge,0
attr,0
filter,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [{}]"".format(source_node.name, self.data_format, layout))",0
use_bias (no_bias default = False),0
strides,0
dilations,0
data_format,0
padding,0
"IR only support pad = ""SAME"" or ""VALID""",0
Using Padding Layer API,0
"pad = self.check_pad_mode(source_node, IR_node)",0
"if pad == ""SAME"":",0
"IR_node.attr[""padding""].s = b""SAME""",0
"elif pad == ""VALID"":",0
"IR_node.attr[""padding""].s = b""VALID""",0
else:,0
self._defuse_padding(source_node),0
del IR_node.input[:],0
"IR_node.input.append(source_node.name + ""_pad"")",0
output shape,0
weights,0
raise NotImplementedError,0
"if source_node.layer.act_type == ""relu"":",0
"MXNetParser._copy_and_reop(source_node, IR_node, ""Relu"")",0
"elif source_node.layer.act_type == ""sigmoid"":",0
"MXNetParser._copy_and_reop(source_node, IR_node, ""Sigmoid"")",0
"elif source_node.layer.act_type == ""tanh"":",0
"MXNetParser._copy_and_reop(source_node, IR_node, ""Tanh"")",0
"elif source_node.layer.act_type == ""softrelu"":",0
"MXNetParser._copy_and_reop(source_node, IR_node, ""SoftReLU"")",0
output shape,0
raise NotImplementedError,0
"name, op",0
input edge,0
axis,0
if self.data_format not in MXNetParser.channels_last:,0
"IR_node.attr[""axis""].i = int(layer_attr.get(""axis"", ""1""))",0
else:,0
scale,0
epsilon,0
momentum,0
output shape,0
weights,0
gamma,0
beta,0
"if MXNetParser.str2bool(layer_attr.get(""use_global_stats"", ""False"")):",0
mean,0
var,0
"raise NotImplementedError(""bool type for scale and offset"")",0
"name, op",0
input edge,0
pooling type (sum not allowed yet),0
strides,0
window_shape,0
padding,0
"IR only support pad = ""SAME"" or ""Valid""",0
"pad = self.check_pad_mode(source_node, IR_node)",0
"if pad == b""SAME"":",0
"IR_node.attr[""padding""].s = b""SAME""",0
"elif pad == b""VALID"":",0
"IR_node.attr[""padding""].s = b""VALID""",0
else:,0
"IR_node.attr[""padding""].s = b""VALID""",0
self._defuse_padding(source_node),0
del IR_node.input[:],0
"IR_node.input.append(source_node.name + ""_pad"")",0
output shape,0
raise NotImplementedError,0
"name, op",0
input edge,0
output shape,0
raise NotImplementedError,0
"name, op",0
input edge,0
dim,0
output shape,0
raise NotImplementedError,0
"def rename_log_softmax(self, source_node):",0
"raise NotImplementedError(""not support yet"")",0
"def rename_Correlation(self, source_node):",0
"raise NotImplementedError(""not support yet"")",0
"name, op",0
input edge,0
padding,0
"pad = self.check_pad_mode(source_node, IR_node)",0
"if pad == ""SAME"":",0
"IR_node.attr[""padding""].s = ""SAME""",0
"elif pad == ""VALID"":",0
"IR_node.attr[""padding""].s = ""VALID""",0
else:,0
self._defuse_padding(source_node),0
del IR_node.input[:],0
"IR_node.input.append(source_node.name + ""_pad"")",0
output shape,0
filter,0
"print(""Warning: Layer [{}] has changed model data format from [{}] to [{}]"".format(source_node.name, self.data_format, layout))",0
use_bias (no_bias default = False),0
strides,0
dilations,0
data_format,0
weights,0
raise NotImplementedError,0
"def rename_RNN(self, source_node):",0
"raise NotImplementedError(""RNN not support yet"")",0
"name, op",0
input edge,0
attr,0
input_dim,0
output_dim,0
dtype,0
output shape,0
raise NotImplementedError,0
"IR only support elu from {'elu', 'leaky', 'prelu', 'rrelu'}",0
judge whether meaningful,0
"name, op",0
input edge,0
attr,0
"alpha [exp(x) - alpha], but mxnet attr slope [slope*(exp(x) - 1)] when x < 0",0
"IR_node.attr[""alpha""].f = float()",0
output shape,0
"raise NotImplementedError(""slope cannot convert to alpha"")",0
"def rename_InstanceNorm(self, source_node):",0
raise NotImplementedError,0
"def rename_L2Normalization(self, source_node):",0
raise NotImplementedError,0
"name, op",0
input edge,0
alpha,0
beta,0
knorm,0
nsize,0
output shape,0
"def rename_ROIPooling(self, source_node):",0
raise NotImplementedError,0
"name, op",0
input edge,0
keep_prob,0
mode,0
output shape,0
raise NotImplementedError,0
reverse cannot support yet,0
"name, op",0
input edge,0
old API target_shape not support yet,0
output shape,0
"raise NotImplementedError(""adjust output shape"")",0
"name, op",0
input edge,0
output shape,0
"IR_node.attr[""data_format""].s = self.data_format",0
raise NotImplementedError,0
"name, op",0
output shape,0
input edge,0
attr,0
dim,0
"name, op",0
input edge,0
attr,0
dtype,0
output shape,0
raise NotImplementedError,0
"name, op",0
input edge,0
attr,0
axis,0
output shape,0
raise NotImplementedError,0
"name, op",0
input edge,0
output shape,0
"raise NotImplementedError(""No matching IR api"")",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
keep dims,0
axes,0
"name, op",0
epsilon,0
moving variance (var),0
gamma (scale),0
mean,0
bias,0
input node,0
output node,0
load model files into Keras graph,0
Build network graph,0
TODO: Fused conv and pool with padding is different from defused operators,0
strides,0
window_shape,0
pool type,0
padding,0
shape,0
strides,0
input[1] : W,0
filter,0
padding,0
output[0] : B,0
converted [dropout],0
convert [tf.contrib.layers.batch_norm],0
normal Add,0
for target shape,0
units,0
Weights,0
FullyConnected Layer,0
"name, op",0
get Bias,0
Deal Dropout,0
keep prob,0
Remove nodes,0
Mul,0
Floor,0
paddings,0
gamma (scale),0
bias,0
Mean,0
Var,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
"self._emit_unary_operation(IR_node, ""contrib.layers.flatten"")",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
For padding,0
print (input_shape),0
print (kernel_shape),0
print (strides),0
Not tested,0
print (input_shape),0
print (kernel_shape),0
print (strides),0
kernel_shape[idx] = (kernel_shape[idx] - 1) * dilation_rate + 1,0
print ([0] + up_list + [0] + down_list if data_format.startswith('NC') else up_list + [0] + down_list + [0]),0
print ('-----------------------------------------------------'),0
key: layer_name    value: keras layer,0
private functions,0
"print (""Warning: Graph Construct a self-loop node {}. Ignored."".format(src))",0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
share functions,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
name --> (weight_name --> ndarray),0
Generated by the protocol buffer compiler.  DO NOT EDIT!,0
source: graph.proto,0
@@protoc_insertion_point(imports),0
@@protoc_insertion_point(class_scope:GraphDef),0
@@protoc_insertion_point(class_scope:NodeDef.AttrEntry),0
@@protoc_insertion_point(class_scope:NodeDef),0
@@protoc_insertion_point(class_scope:AttrValue.ListValue),0
@@protoc_insertion_point(class_scope:AttrValue),0
@@protoc_insertion_point(class_scope:TensorShape.Dim),0
@@protoc_insertion_point(class_scope:TensorShape),0
@@protoc_insertion_point(class_scope:LiteralTensor),0
@@protoc_insertion_point(module_scope),0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
First try to read it as a binary file.,0
Next try to read it as a text file.,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
Load the model network,0
Load the model weights,0
load model files into Keras graph,0
"_keras.utils.plot_model(model, ""model.png"", show_shapes = True)",0
Build network graph,0
TODO: Fused conv and pool with padding is different from defused operators,0
TODO: More activation functions,1
for ELU,0
input edge,0
"name, op",0
weights,0
pads,0
filter,0
"[kd, kh, kw, channel_size, filter number]",0
use_bias,0
strides,0
"[1, sd, sh, sw, 1]",0
dilations,0
"[1, dd, dh, dw, 1]",0
activation,0
"name, op",0
input edge,0
padding,0
strides,0
"[1, sd, sh, sw, 1]",0
window_shape,0
"[1, pd, ph, pw, 1]",0
"name, op",0
input edge,0
For concat axis,0
"name, op",0
input edge,0
padding,0
only for training,0
"name, op",0
input edge,0
"name, op",0
input edge,0
Merge Layers,0
only for training,0
"name, op",0
input edge,0
shape,0
only for training,0
"name, op",0
input edge,0
Core Layers,0
"name, op",0
input edge,0
units,0
use_bias,0
weights,0
activation,0
"name, op",0
input edge,0
"name, op",0
input edge,0
input_dim,0
output_dim,0
mask_zero,0
weights,0
"name, op",0
input edge,0
units,0
use_bias,0
"for Keras, drop_out and recurrent_dropout",0
activation,0
"name, op",0
input edge,0
units,0
activation,0
Kit TODO : need to search the tf,1
"name, op",0
input edge,0
for target shape,0
"name, op",0
input edge,0
arguments not implementent,0
print (type(source_node.keras_layer.arguments)),0
"name, op",0
input edge,0
axis,0
"Parameter arrangement in Keras: gamma, beta, mean, variance",0
scale,0
beta,0
mean,0
var,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
rot weights,0
############,0
Operators #,0
############,0
for Keras,0
----------------------------------------------------------------------------------------------,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License. See License.txt in the project root for license information.,0
----------------------------------------------------------------------------------------------,0
sanity check.,0
Kit: TODO,1
Duplicate models for weight sharing,0
Expand the sub-models,0
console.info(JSON.parse(e.target.result)),0
"default, draw the model.json file",0
window.onload = () => {,0
"d3.json(filePath, (error, json) => {",0
if (error) throw error;,0
// console.info(json),0
draw(json),0
}),0
},0
generate dag,0
IR model or keras model,0
path generator,0
let arrowL = `M ${points[len - 1].x} ${points[len - 1].y} l -7 -7`,0
let arrowR = `M ${points[len - 1].x} ${points[len - 1].y} l  7 -7`,0
select a layer,0
draw,0
"let scale = Math.min(window.innerWidth*0.85 / width, (window.innerHeight-85)/height)",0
"var filter = def.append(""filter"")",0
".attr(""id"", ""dropshadow"")",0
"filter.append(""feGaussianBlur"")",0
".attr(""in"", ""SourceAlpha"")",0
".attr(""stdDeviation"", 4)",0
".attr(""result"", ""blur"");",0
"// filter.append(""feOffset"")",0
"//     .attr(""in"", ""blur"")",0
"//     .attr(""dx"", 2)",0
"//     .attr(""dy"", 2)",0
"//     .attr(""result"", ""offsetBlur"");",0
"var feMerge = filter.append(""feMerge"");",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""offsetBlur"")",0
"feMerge.append(""feMergeNode"")",0
".attr(""in"", ""SourceGraphic"");",0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
"svg.on('keydown', ()=>{console.info('ddd')})",0
".on('mouseover', ()=>{console.info('mouse over')})",0
"limit x, y",0
a trick to make text svg transform in MS Edge,0
when shift is down,0
ulti function,0
".attr('rx', nodeH / 5)",0
".attr('ry', nodeH / 5)",0
space = '',0
