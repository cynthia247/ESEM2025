Commit Message,predict
This will read version from pyproject.toml,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Making the DetectionImageFolder class available for import from this module,0
Define the allowed image extensions,0
Get image filename and path,0
Load and convert image to RGB,0
Apply transformation if specified,0
TODO: Under development for efficiency improvement,1
Only run recognition on animal detections,0
Get image path and corresponding bbox xyxy for cropping,0
Load and crop image with supervision,0
Apply transformation if specified,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Making the provided classes available for import from this module,0
Convert PIL Image to Torch Tensor,0
Original shape,0
New shape,0
Scale ratio (new / old) and compute padding,0
Resize image,0
Pad image,0
Convert the image to a PyTorch tensor and normalize it,0
Resize and pad the image using a customized letterbox function.,0
Normalization constants,0
Define the sequence of transformations,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
!!! Output paths need to be optimized !!!,1
Category filtering,0
if not all([x in exclude_category_ids for x in category]):,0
Category filtering,0
if not all([x in exclude_category_ids for x in category]):,0
if not all([x in exclude_category_ids for x in category_id_list]):,0
Find classifications for this detection,0
Load JSON data from the file,0
Ensure the destination directories exist,0
Process each image detection,0
Check if there is any category '0' with confidence above the threshold,0
Construct the source and destination file paths,0
Copy the file to the appropriate directory,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Importing basic libraries,0
Placeholder class-level attributes to be defined in derived classes,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%%,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Importing basic libraries,0
self.predictor.args.device = device # Will uncomment later,0
Creating a DataLoader for batching and parallel processing of the images,0
Normalize the coordinates for timelapse compatibility,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Importing basic libraries,0
Creating a DataLoader for batching and parallel processing of the images,0
Normalize the coordinates for timelapse compatibility,0
backbone,0
"self.cls_dla_up = dla_modules.DLAUp(channels[-3:], scales=scales[:3])",0
bottleneck conv,0
localization head,0
classification head,0
decode_cls = self.cls_dla_up(encode[-3:]),0
clsmap = self.cls_head(decode_cls),0
Assert that the dataset is either 'general' or 'ennedi',0
"checkpoint = load_state_dict_from_url(url, map_location=torch.device(self.device)) # NOTE: This function is not used in the current implementation",0
Load the class names and other metadata from the checkpoint,0
Load the model architecture,0
Load checkpoint into model,0
Remove 'model.' prefix from the state_dict keys if the key starts with 'model.',0
Load the new state_dict,0
Creating a Dataloader for batching and parallel processing of the images,0
Flatten the lists since we know its a single image,0
Calculate the total number of detections,0
Pre-allocate based on total possible detections,0
Loop through each species,0
Get the detections for this species,0
Apply the confidence threshold,0
Fill the preds_array with the valid detections,0
Call the forward method of the model in evaluation mode,0
x = self.fc(x),0
y = self.softmax(self.up(x)),0
patches' height & width,0
unfold on height,0
if non-perfect division on height,0
get the residual patch and add it to the fold,0
unfold on width,0
"if non-perfect division on width, the same",0
reshaping,0
patches' height & width,0
lists of pixels numbers,0
cut into patches to get limits,0
if non-perfect division on height,0
if non-perfect division on width,0
@property,0
def area(self) -> int:,0
''' To get area ''',0
return 1 # always 1 pixel,0
local maxima,0
adaptive threshold for counting,0
negative sample,0
count,0
locations and scores,0
upsample class map,0
softmax,0
cat to heatmap,0
LMDS,0
step 1 - get patches and limits,0
step 2 - inference to get maps,0
step 3 - patch the maps into initial coordinates system,0
(step 4 - upsample),0
outputs = self.model(patch)[0],0
cat,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Image size for the Opossum classifier,0
Class names for prediction,0
"If pretrained, use the provided URL to fetch the weights",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Making the PlainResNetInference class available for import from this module,0
Following the ResNet structure to extract features,0
Initialize the network and weights,0
... [Missing weight URL definition for ResNet18],0
... [Missing weight URL definition for ResNet50],0
Print missing and unused keys for debugging purposes,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Image size for the classifier,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Image size for the Opossum classifier,0
Class names for prediction,0
"If pretrained, use the provided URL to fetch the weights",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Image size for the Opossum classifier,0
Class names for prediction,0
"If pretrained, use the provided URL to fetch the weights",0
Load and set configurations from the YAML file,0
Configuration file for the Sphinx documentation builder.,0
,0
"For the full list of built-in configuration values, see the documentation:",0
https://www.sphinx-doc.org/en/master/usage/configuration.html,0
-- Project information -----------------------------------------------------,0
https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information,0
-- General configuration ---------------------------------------------------,0
https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration,0
-- Options for HTML output -------------------------------------------------,0
https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output,0
-- Options for todo extension ----------------------------------------------,1
https://www.sphinx-doc.org/en/master/usage/extensions/todo.html#configuration,0
"Determine the factor, which shifts the decimal point of x",0
just behind the last significant digit,0
"Shift decimal point by multiplicatipon with factor, flooring, and",0
division by factor,0
_ = pretty_print_object(obj),0
Sloppy that I'm making a module-wide change here...,1
Determine the coordinates of the intersection rectangle,0
The intersection of two axis-aligned bounding boxes is always an,0
axis-aligned bounding box,0
Compute the area of both AABBs,0
Compute the intersection over union by taking the intersection,0
area and dividing it by the sum of prediction + ground-truth,0
areas - the intersection area.,0
%% Functions for running commands as subprocesses,0
%%,0
%% Test driver for execute_and_print,0
%% Parallel test driver for execute_command_and_print,0
Should we use threads (vs. processes) for parallelization?,0
"Only relevant if n_workers == 1, i.e. if we're not parallelizing",0
%% Constants and imports,0
%% Classes,0
Collect all names,0
Make names unique and sort,0
cast location to string as the entries in locations are strings,0
Convert classnames to lowercase to simplify comparisons later,0
Normalize paths to simplify comparisons later,0
"Make custom replacements in filenames, typically used to",0
accommodate changes in root paths after DB construction,0
## Build useful mappings to facilitate working with the DB,0
Category ID <--> name,0
Image filename --> ID,0
Image ID --> image object,0
Image ID --> annotations,0
"Each image can potentially multiple annotations, hence using lists",0
...__init__,0
...class IndexedJsonDb,0
%% Functions,0
Find all unique locations,0
i_location = 0; location = locations[i_location],0
Sorting datetimes fails when there are None's in the list.  So instead of sorting datetimes,0
"directly, sort tuples with a boolean for none-ness, then the datetime itself.",0
,0
https://stackoverflow.com/questions/18411560/sort-list-while-pushing-none-values-to-the-end,0
previous_datetime = sorted_images_this_location[0]['datetime'],0
im = sorted_images_this_location[1],0
"Start a new sequence if necessary, including the case where this datetime is invalid",0
"If this was an invalid datetime, this will record the previous datetime",0
"as None, which will force the next image to start a new sequence.",0
...for each image in this location,0
Fill in seq_num_frames,0
...for each location,0
...create_sequences(),0
,0
cct_to_md.py,0
,0
"""Converts"" a COCO Camera Traps file to a MD results file.  Currently ignores",0
"non-bounding-box annotations, and gives all annotations a confidence of 1.0.",0
,0
The only reason to do this is if you are going to add information to an existing,0
"CCT-formatted dataset, and want to do that in Timelapse.",0
,0
"Currently assumes that width and height are present in the input data, does not",0
read them from images.,0
,0
%% Constants and imports,0
%% Functions,0
# Validate input,0
# Read input,0
# Prepare metadata,0
ann = d['annotations'][0],0
# Process images,0
im = d['images'][0],0
"MegaDetector: [x,y,width,height] (normalized, origin upper-left)",0
"CCT: [x,y,width,height] (absolute, origin upper-left)",0
...if there's a bounding box,0
...for each annotation,0
This field is no longer included in MD output files by default,0
im_out['max_detection_conf'] = max_detection_conf,0
...for each image,0
# Write output,0
...cct_to_md(),0
%% Command-line driver,0
TODO,1
%% Interactive driver,0
%%,0
%%,0
,0
cct_json_to_filename_json.py,0
,0
"Given a .json file in COCO Camera Traps format, outputs a .json-formatted list of",0
relative file names present in the CCT file.,0
,0
%% Constants and environment,0
%% Main function,0
"json.dump(s,open(outputFilename,'w'))",0
%% Command-line driver,0
%% Interactive driver,0
%%,0
,0
cct_to_csv.py,0
,0
"""Converts"" a COCO Camera Traps .json file to .csv, in quotes because",0
"all kinds of assumptions are made here, and if you have a particular .csv",0
"format in mind, YMMV.  Most notably, does not include any bounding box information",0
or any non-standard fields that may be present in the .json file.  Does not,0
propagate information about sequence-level vs. image-level annotations.,0
,0
"Does not assume access to the images, therefore does not open .jpg files to find",0
"datetime information if it's not in the metadata, just writes datetime as 'unknown'.",0
,0
%% Imports,0
%% Main function,0
#%% Read input,0
#%% Build internal mappings,0
annotation = annotations[0],0
#%% Write output file,0
im = images[0],0
Write out one line per class:,0
...for each class name,0
...for each image,0
...with open(output_file),0
...def cct_to_csv,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
,0
remove_exif.py,0
,0
"Removes all EXIF/IPTC/XMP metadata from a folder of images, without making",0
"backup copies, using pyexiv2.",0
,0
%% Imports and constants,0
%% List files,0
%% Remove EXIF data (support),0
PYEXIV2 IS NOT THREAD SAFE; DO NOT CALL THIS IN PARALLEL FROM A SINGLE PROCESS,1
data = img.read_exif(); print(data),0
%% Debug,0
%%,0
%%,0
%% Remove EXIF data (execution),0
fn = image_files[0],0
"joblib.Parallel defaults to a process-based backend, but let's be sure",0
"results = Parallel(n_jobs=n_exif_threads,verbose=2,prefer='processes')(delayed(remove_exif)(fn) for fn in image_files[0:10])",0
,0
yolo_to_coco.py,0
,0
Converts a YOLO-formatted dataset to a COCO-formatted dataset.,0
,0
"Currently supports only a single folder (i.e., no recursion).  Treats images without",0
corresponding .txt files as empty.,0
,0
%% Imports and constants,0
from ai4eutils,0
%% Support functions,0
Validate input,0
Class names,0
Blank lines should only appear at the end,0
Enumerate images,0
fn = image_files[0],0
Create the image object for this image,0
Is there an annotation file for this image?,0
"This is an image with no annotations, currently don't do anything special",0
here,0
s = lines[0],0
"COCO: [x_min, y_min, width, height] in absolute coordinates",0
"YOLO: [class, x_center, y_center, width, height] in normalized coordinates",0
...for each annotation,0
...if this image has annotations,0
...for each image,0
...def yolo_to_coco(),0
%% Interactive driver,0
%% Convert YOLO folders to COCO,0
%% Check DB integrity,0
%% Preview some images,0
%% Command-line driver,0
TODO,1
,0
read_exif.py,0
,0
"Given a folder of images, read relevant metadata (EXIF/IPTC/XMP) fields from all images,",0
and write them to  a .json or .csv file.,0
,0
This module can use either PIL (which can only reliably read EXIF data) or exiftool (which,0
can read everything).  The latter approach expects that exiftool is available on the system,0
path.  No attempt is made to be consistent in format across the two approaches.,0
,0
%% Imports and constants,0
From ai4eutils,0
%% Options,0
Number of concurrent workers,0
Should we use threads (vs. processes) for parallelization?,0
,0
Not relevant if n_workers is 1.,0
Should we use exiftool or pil?,0
%% Functions,0
exif_tags = img.info['exif'] if ('exif' in img.info) else None,0
print('Warning: unrecognized EXIF tag: {}'.format(k)),0
"-G means ""Print group name for each tag"", e.g. print:",0
,0
[File]          Bits Per Sample                 : 8,0
,0
...instead of:,0
,0
Bits Per Sample                 : 8,0
A list of three-element lists (type/tag/value),0
line_raw = exif_lines[0],0
A typical line:,0
,0
[ExifTool]      ExifTool Version Number         : 12.13,0
"Split on the first occurrence of "":""",0
...for each output line,0
...which processing library are we using?,0
...read_exif_tags_for_image(),0
...populate_exif_data(),0
Enumerate *relative* paths,0
Find all EXIF tags that exist in any image,0
...for each tag in this image,0
...for each image,0
Write header,0
...for each key that *might* be present in this image,0
...for each image,0
...with open(),0
...if we're writing to .json/.csv,0
https://stackoverflow.com/questions/11210104/check-if-a-program-exists-from-a-python-script,0
%% Interactive driver,0
%%,0
output_file = os.path.expanduser('~/data/test-exif.csv'),0
options.processing_library = 'pil',0
"file_path = os.path.join(input_folder,'KRU_S1_11_R1_IMAG0148.JPG')",0
%%,0
%% Command-line driver,0
,0
"Given a json-formatted list of image filenames, retrieve the width and height of every image.",0
,0
%% Constants and imports,0
%% Processing functions,0
Is this image on disk?,0
"all_results = list(tqdm(pool.imap(process_image, filenames), total=len(filenames)))",0
%% Interactive driver,0
%%,0
List images in a test folder,0
%%,0
"process_list_file(image_list_file,image_size_file,image_prefix=base_dir)",0
%% Command-line driver,0
"python sanity_check_json_db.py ""e:\wildlife_data\wellington_data\wellington_camera_traps.json"" --baseDir ""e:\wildlife_data\wellington_data\images"" --bFindUnusedImages --bCheckImageSizes",0
"python sanity_check_json_db.py ""D:/wildlife_data/mcgill_test/mcgill_test.json"" --baseDir ""D:/wildlife_data/mcgill_test"" --bFindUnusedImages --bCheckImageSizes",0
"Here the '-u' prevents buffering, which makes tee happier",0
,0
python -u sanity_check_json_db.py '/datadrive1/nacti_metadata.json' --baseDir '/datadrive1/nactiUnzip/' --bFindUnusedImages --bCheckImageSizes | tee ~/nactiTest.out,0
,0
Partially-formed stub to get from MegaDetector output files to COCO Camera Traps data.,0
,0
"Was actually written to convert *many* MD .json files to a single CCT file, hence",0
the loop over .json files.,0
,0
"THIS CODE HAS NEVER BEEN RUN, it was added as a demonstration of how to do this.  YMMV.",0
,0
"You may find a more polished, command-line-ready version of this code at:",0
,0
https://github.com/StewartWILDlab/mdtools,0
,0
%% Constants and imports,0
"Images sizes are required to convert between absolute and relative coordinates,",0
so we need to read the images.,0
Only required if you want to write a database preview,0
%% Create CCT dictionaries,0
image_ids_to_images = {},0
Force the empty category to be ID 0,0
Load .json annotations for this data set,0
i_entry = 0; entry = data['images'][i_entry],0
,0
"PERF: Not exactly trivially parallelizable, but about 100% of the",0
time here is spent reading image sizes (which we need to do to get from,0
"absolute to relative coordinates), so worth parallelizing.",0
Generate a unique ID from the path,0
detection = detections[0],0
Have we seen this category before?,0
Create an annotation,0
"MegaDetector: [x,y,width,height] (normalized, origin upper-left)",0
"CCT: [x,y,width,height] (absolute, origin upper-left)",0
...for each detection,0
...for each image,0
Remove non-reviewed images and associated annotations,0
%% Create info struct,0
%% Write .json output,0
%% Clean start,0
## Everything after this should work from a clean start ###,0
%% Validate output,0
%% Preview animal labels,0
%% Preview empty labels,0
"viz_options.classes_to_exclude = ['empty','human']",0
,0
generate_crops_from_cct.py,0
,0
"Given a .json file in COCO Camera Traps format, create a cropped image for",0
each bounding box.,0
,0
%% Imports and constants,0
%% Functions,0
# Read and validate input,0
# Find annotations for each image,0
"This actually maps image IDs to annotations, but only to annotations",0
containing boxes,0
# Generate crops,0
TODO: parallelize this loop,1
im = d['images'][0],0
Load the image,0
Generate crops,0
i_ann = 0; ann = annotations_this_image[i_ann],0
"x/y/w/h, origin at the upper-left",0
...for each box,0
...for each image,0
...generate_crops_from_cct(),0
%% Interactive driver,0
%%,0
%%,0
%%,0
%% Command-line driver,0
TODO,1
%% Scrap,0
%%,0
,0
coco_to_yolo.py,0
,0
Converts a COCO-formatted dataset to a YOLO-formatted dataset.,0
,0
"If the input and output folders are the same, writes .txt files to the input folder,",0
and neither moves nor modifies images.,0
,0
"Currently ignores segmentation masks, and errors if an annotation has a",0
segmentation polygon but no bbox,0
,0
Has only been tested on a handful of COCO Camera Traps data sets; if you,0
"use it for more general COCO conversion, YMMV.",0
,0
%% Imports and constants,0
%% Support functions,0
Validate input,0
Read input data,0
Parse annotations,0
i_ann = 0; ann = data['annotations'][0],0
Make sure no annotations have *only* segmentation data,0
Re-map class IDs to make sure they run from 0...n-classes-1,0
,0
"TODO: this allows unused categories in the output data set, which I *think* is OK,",1
but I'm only 81% sure.,0
Process images (everything but I/O),0
"List of dictionaries with keys 'source_image','dest_image','bboxes','dest_txt'",0
i_image = 0; im = data['images'][i_image],0
"assert os.path.isfile(source_image), 'Could not find image {}'.format(source_image)",0
If this annotation has no bounding boxes...,0
"This is not entirely clear from the COCO spec, but it seems to be consensus",0
"that if you want to specify an image with no objects, you don't include any",0
annotations for that image.,0
We allow empty bbox lists in COCO camera traps; this is typically a negative,0
"example in a dataset that has bounding boxes, and 0 is typically the empty",0
category.,0
...if this is an empty annotation,0
"COCO: [x_min, y_min, width, height] in absolute coordinates",0
"YOLO: [class, x_center, y_center, width, height] in normalized coordinates",0
Convert from COCO coordinates to YOLO coordinates,0
...for each annotation,0
...if this image has annotations,0
...for each image,0
Write output,0
Category IDs should range from 0..N-1,0
TODO: parallelize this loop,1
,0
output_info = images_to_copy[0],0
Only write an annotation file if there are bounding boxes.  Images with,0
"no .txt files are treated as hard negatives, at least by YOLOv5:",0
,0
https://github.com/ultralytics/yolov5/issues/3218,0
,0
"I think this is also true for images with empty annotation files, but",0
"I'm using the convention suggested on that issue, i.e. hard negatives",0
are expressed as images without .txt files.,0
bbox = bboxes[0],0
...for each image,0
...def coco_to_yolo(),0
%% Interactive driver,0
%% CCT data,0
"If preview_export is True, I'm exporting to preview these with BoundingBoxEditor:",0
,0
https://github.com/mfl28/BoundingBoxEditor,0
,0
"This export will be compatible, other than the fact that you need to move",0
"""object.data"" into the ""labels"" folder.",0
,0
"Otherwise I'm exporting for training, in the YOLOv5 flat format.",0
%% Command-line driver,0
TODO,1
,0
cct_to_wi.py,0
,0
Converts COCO Camera Traps .json files to the Wildlife Insights,0
batch upload format,0
,0
Also see:,0
,0
https://github.com/ConservationInternational/Wildlife-Insights----Data-Migration,0
,0
https://data.naturalsciences.org/wildlife-insights/taxonomy/search,0
,0
%% Imports,0
%% Paths,0
A COCO camera traps file with information about this dataset,0
A .json dictionary mapping common names in this dataset to dictionaries with the,0
"WI taxonomy fields: common_name, wi_taxon_id, class, orer, family, genus, species",0
%% Constants,0
%% Project information,0
%% Read templates,0
%% Compare dictionary to template lists,0
Write the header,0
Write values,0
%% Project file,0
%% Camera file,0
%% Deployment file,0
%% Images file,0
Read .json file with image information,0
Read taxonomy dictionary,0
Populate output information,0
df = pd.DataFrame(columns = images_fields),0
annotation = annotations[0],0
im = input_data['images'][0],0
"We don't have counts, but we can differentiate between zero and 1",0
This is the label mapping used for our incoming iMerit annotations,0
"Only used to parse the incoming annotations. In our database, the string name is used to avoid confusion",0
MegaDetector outputs,0
,0
add_bounding_boxes_to_megadb.py,0
,0
"Given COCO-formatted JSONs containing manually labeled bounding box annotations, add them to",0
"MegaDB sequence entries, which can then be ingested into MegaDB.",0
the category map that comes in the COCO JSONs for iMerit batch 12 - to check that each,0
JSON,0
"dataset name : (seq_id, frame_num) : [bbox, bbox]",0
where bbox is a dict with str 'category' and list 'bbox',0
iterate over image_id_to_image rather than image_id_to_annotations so we include,0
the confirmed empty images,0
The file_name field in the incoming json looks like,0
alka_squirrels.seq2020_05_07_25C.frame119221.jpg,0
there seems to be a bug in the annotations where sometimes there's a,0
non-empty label along with a label of category_id 5,0
ignore the empty label (they seem to be actually non-empty),0
,0
add_bounding_boxes_to_json.py,0
,0
This script takes a image database in the COCO Camera Traps format and merges in a set of bounding,0
box annotations in the format that iMerit uses (a .json where actually only each row is a valid json).,0
,0
"If you need to update an existing bbox database, please get all the original annotation files and",0
re-generate from scratch,0
,0
%% Imports,0
%% Configurations and paths,0
images database,0
output bboxes database,0
annotation files (pseudo json) obtained from our annotation vendor that contain annotations for this dataset,0
None or a string or tuple of strings that is the prefix to all file_name of interest / in this dataset in the annotation files,0
functions for mapping the image_id in the annotation files (pseudo jsons) to the image_id used in the image DB,0
our img_id doesn't contain frame info,0
"batch3 - ""file_name"":""ASG0000019_0_S1_B06_R1_PICT0007.JPG""",0
"batch5 and 7 - ""file_name"":""datasetsnapshotserengeti.seqASG000002m-frame0.imgS1_B06_R1_PICT0056.JPG""",0
sometimes - 'datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG',0
"id in DB (old_token): 'S6/J01/J01_R1/S6_J01_R1_IMAG0001', 'S1/B05/B05_R1/S1_B05_R1_PICT0036'",0
specify which one to use for your dataset here,0
%% Load the image database and fill in DB info for the output bbox database,0
load the images database,0
%% Find the height and width of images from the annotation files,0
,0
...if they are not available in the images DB,0
each row in this pseudo-json is a COCO formatted entry for an image sequence,0
%% Other functions required by specific datasets,0
the IDFG image database does not include images from unlabeled folders that were annotated with bounding boxes,0
%% Create the bbox database from all annotation files pertaining to this dataset,0
"for the incoming annotations, look up by category name (common) and convert to the numerical id used in our databases",0
"for each annotation pseudo-json, check that the image it refers to exists in the original database",0
each row in this pseudo-json is a COCO formatted entry for an image sequence,0
check that entry is for this dataset,0
category map for this entry in the annotation file - usually the same across all entries but just in case,0
rspb_add_image_entry(img_id),0
use the image length and width in the image DB,0
"[top left x, top left y, width, height] in relative coordinates",0
"add all images that have been sent to annotation, some of which may be empty of bounding boxes",0
rspb_add_image_entry(db_image_id),0
%% Common queries,0
This query is used when preparing tfrecords for object detector training.,0
We do not want to get the whole seq obj where at least one image has bbox because,0
some images in that sequence will not be bbox labeled so will be confusing.,0
Include images with bbox length 0 - these are confirmed empty by bbox annotators.,0
"If frame_num is not available, it will not be a field in the result iterable.",0
"Note that the seq_id is the Cosmos DB assigned ID for that sequence, not the",0
"seq_id field, which may contain ""/"" characters.",0
"For public datasets to be converted to the CCT format, we get the whole seq object because",0
sequence level attributes need to be included too. megadb/converters/megadb_to_cct.py handles,0
the case of bbox-only JSONs with the flag exclude_species_class,0
Getting all sequences in a dataset - for updating or deleting entries which need the id field,0
%% Parameters,0
Use None if querying across all partitions,0
"The `sequences` table has the `dataset` as the partition key, so if only querying",0
"entries from one dataset, set the dataset name here.",0
"e.g. {'name': '@top_n', 'value': 100} - see query_and_upsert_examples/query_for_data.ipynb",0
Use False if do not want all results stored in a single JSON.,0
%% Script,0
execute the query,0
loop through and save the results,0
MODIFY HERE depending on the query,0
wiitigers Unicode issue - no good mapping from DB file names to file names in blob URL,0
build filename,0
if need to re-download a dataset's images in case of corruption,0
entries_to_download = {,0
"filename: entry for filename, entry in entries_to_download.items()",0
if entry['dataset'] == DATASET,0
},0
input validation,0
"existing files, with paths relative to <store_dir>",0
parse JSON or TXT file,0
"create a new storage container client for this dataset,",0
and cache it,0
schema already checks that the min possible value of frame_num is 0,0
"if there are more than one image item, each needs a frame_num",0
checks across all sequence items,0
per sequence item checks,0
load the schema,0
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory,0
load the schema,0
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory,0
checks across all datasets items,0
check for expiry date of the SAS keys,0
update the sequences,0
"`id` is from the database, as well as all attributes starting with _",0
"if valuable sequence information is available, add them to the image",0
required fields for an image object,0
add seq-level class labels for this image,0
add other sequence-level properties to each image too,0
add other image-level properties,0
... for im in seq['images'],0
... for seq in mega_db,0
consolidate categories,0
some property names have changed in the new schema,0
a dummy sequence ID will be generated if the image entry does not have a seq_id field,0
seq_id only needs to be unique within this dataset; MegaDB does not rely on it as the _id field,0
"""annotations"" fields are opened and have its sub-field surfaced one level up",0
set the `dataset` property on each sequence to the provided dataset_name,0
check that the location field is the same for all images in a sequence,0
check which fields in a CCT image entry are sequence-level,0
image-level properties that really should be sequence-level,0
need to add (misidentified) seq properties not present for each image in a sequence to img_level_properties,0
"(some properties act like flags - all have the same value, but not present on each img)",0
add the sequence-level properties to the sequence objects,0
not every sequence have to have all the seq_level_properties,0
get the value of this sequence-level property from the first image entry,0
check which fields are really dataset-level and should be included in the dataset table instead.,0
delete sequence-level properties that should be dataset-level,0
make all `class` fields lower-case; cast `seq_id` to type string in case they're integers,0
turn all float NaN values into None so it gets converted to null when serialized,0
this was an issue in the Snapshot Safari datasets,0
%% validation,0
"at first a dict of image_id: image_obj with annotations embedded, then it becomes",0
an array of image objects,0
%% integrate the image DB,0
takes in image entries and species and other annotations in the image DB,0
convert the species category to explicit string name,0
there may be other fields in the annotation object,0
these fields should already be gotten from the image object,0
%% integrate the bbox DB,0
add any images that are not in the image DB,0
also add any fields in the image object that are not present already,0
add bbox to the annotations field,0
for any newly added images,0
"'bbox_abs': bbox_anno['bbox'],",0
not keeping height and width,0
,0
jb_csv_to_json.py,0
,0
Convert a particular .csv file to CCT format.  Images were not available at,0
"the time I wrote this script, so this is much shorter than other scripts",0
in this folder.,0
,0
%% Constants and environment,0
%% Read source data,0
%% Confirm filename uniqueness (this data set has one label per image),0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
%% Create CCT dictionaries,0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
imageName = imageFilenames[0],0
"As per above, this is convenient and appears to be true; asserting to be safe",0
"Filenames look like ""290716114012001a1116.jpg""",0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
%% Create info struct,0
%% Write output,0
%% Sanity-check,0
%% Imports and constants,0
%% Load data,0
"with open(image_json,'r') as f:",0
data = json.load(f),0
%% Sanity-check data,0
%% Label previews,0
%% Collect images to annotate,0
%% Sort by sequence and frame,0
%% Copy to a folder by GUID,0
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension,0
im = images_to_annotate[0],0
...for each image,0
%% Write out the annotation list,0
%% Constants and imports,0
%% Enumerate files,0
edited_image_folder = edited_image_folders[0],0
fn = edited_image_files[0],0
%% Read metadata and capture location information,0
i_row = 0; row = df.iloc[i_row],0
Sometimes '2017' was just '17' in the date column,0
%% Read the .json files and build output dictionaries,0
json_fn = json_files[0],0
if 'partial' in json_fn:,0
continue,0
line = lines[0],0
if image_fn == 'SD1_238_6_26_17_16_76.73.jpg':,0
asdfad,0
SD29_079_5_14_2018_17_52.85.jpg,0
Re-write two-digit years as four-digit years,0
Sometimes the year was written with two digits instead of 4,0
assert len(tokens[4]) == 4 and tokens[4].startswith('20'),0
Have we seen this location already?,0
"Not a typo, it's actually ""formateddata""",0
An image shouldn't be annotated as both empty and non-empty,0
An image shouldn't be annotated as both empty and non-empty,0
box = formatteddata[0],0
"open_file(os.path.join(base_folder,jpeg_image_folder,image_fn))",0
...for each box,0
...if there are boxes on this image,0
...for each line,0
...with open(),0
...for each json file,0
%% Prepare the output .json,0
%% Check DB integrity,0
%% Print unique locations,0
SD12_202_6_23_2017_1_31.85.jpg,0
%% Preview some images,0
%% Statistics,0
,0
"Import a Snapshot Safari project (one project, one season)",0
,0
Before running this script:,0
,0
"* Mount the blob container where the images live, or copy the",0
images to local storage,0
,0
What this script does:,0
,0
* Creates a .json file,0
* Creates zip archives of the season without humans.,0
* Copies animals and humans to separate folders,0
,0
After running this script:,0
,0
* Create or update LILA page,0
* Push zipfile and unzipped images to LILA,0
* Push unzipped humans to wildlifeblobssc,0
* Delete images from UMN uplaod storage,0
,0
"Snapshot Serengeti is handled specially, because we're dealing with bounding",0
boxes too.  See snapshot_serengeti_lila.py.,0
,0
%% Imports,0
From ai4eutils,0
From CameraTraps,0
%% Constants,0
project_name = 'XXX'; season_name = 'S1'; project_friendly_name = 'Snapshot Unknown',0
project_name = 'SER'; season_name = 'S1-11'; project_friendly_name = 'Snapshot Serengeti',0
project_name = 'KRU'; season_name = 'S1'; project_friendly_name = 'Snapshot Kruger',0
project_name = 'CDB'; season_name = 'S1'; project_friendly_name = 'Snapshot Camdeboo',0
project_name = 'MTZ'; season_name = 'S1'; project_friendly_name = 'Snapshot Mountain Zebra',0
project_name = 'ENO'; season_name = 'S1'; project_friendly_name = 'Snapshot Enonkishu',0
project_name = 'KAR'; season_name = 'S1'; project_friendly_name = 'Snapshot Karoo',0
project_name = 'KGA'; season_name = 'S1'; project_friendly_name = 'Snapshot Kgalagadi',0
%% Folder/file creation,0
E.g. KRU_S1,0
E.g. Z:\KRU,0
E.g. Z:\KRU\KRU_S1,0
Contains annotations for each capture event (sequence),0
Maps image IDs to filenames; each line looks like:,0
,0
"KRU_S1#1#1#2,3,KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0004.JPG",0
"Total number of each answer to each question, e.g. total number of times each species was identified",0
,0
Not used here,1
Create output folders,0
Images will be placed in a season-specific folder inside this (the source data includes,0
this in path names),0
%% Load metadata files,0
%% Convert to dictionaries (prep),0
%% Convert to dictionaries (loops),0
"TODO: iterrows() is a terrible way to do this, but this is one of those days",1
"where I want to get this done, not get better at Python.",0
irow = 0; row = image_table.iloc[0],0
"Loaded as an int64, converting to int here",0
...for each row in the image table,0
Make sure image IDs are what we think they are,0
...for each row in the annotation table,0
%% Take a look at categories (just sanity-checking),0
print('\nCategories by species:'),0
pp.pprint(categories_by_species),0
%% Fill in some image fields we didn't have when we created the image table,0
"width, height, corrupt, seq_num_frames, location, datetime",0
Every annotation in this list should have the same sequence ID,0
"Figure out ""seq_num_frames"", which really should be done in a separate lopp;",0
there's no reason to do this redundantly for every image,0
Every image in this sequence should point back to the same equence,0
Every annotation in this list should have the same location,0
Every annotation in this list should have the same datetime,0
Is this image on disk?,0
iImage = 0; im = images[0],0
...for each image,0
"images_processed = pool.map(process_image, images)",0
"images_processed = list(tqdm(pool.imap_unordered(process_image, images), total=len(images)))",0
%% Count missing/corrupted images,0
%% Print distribution of sequence lengths (sanity-check),0
%% Replicate annotations across images,0
iAnn = 0; ann = annotations[iAnn],0
%% See what files are on disk but not annotated,0
%% Sanity-check image and annotation uniqueness,0
%% Minor updates to fields,0
%% Write .json file,0
%% Create a list of human files,0
ann = annotations[0],0
%% Create public archive and public/private folders,0
im = images[0],0
E.g. KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0001.JPG,0
Copy to private output folder,0
Add to zipfile,0
Possibly start a new archive,0
Copy to public output folder,0
...for each image,0
%% Sanity-check .json file,0
"This will produce some validation errors, because this zipfile doesn't include humans",0
%% Zip up .json and .csv files,0
%% When I skip to this part (using a pre-rendered .json file),0
%%,0
%%,0
ann = annotations[0],0
%% Summary prep for LILA,0
"%% Generate preview, sanity-check labels",0
"viz_options.classes_to_include = ['jackalblackbacked','bustardkori']",0
%% Scrap,0
%% Find annotations for a particular image,0
%% Write a list of missing images,0
,0
save_the_elephants_survey_A.py,0
,0
Convert the .csv file provided for the Save the Elephants Survey A data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
Handle all unstructured fields in the source data as extra fields in the annotations,0
"photo_type really should be an image property, but there are a few conflicts",0
that forced me to handle it as an annotation proprerty,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
Maps relative paths to row indices in input_metadata,0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
Ignore directories,0
%% Make sure the multiple-annotation cases make sense,0
%%,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
Force the empty category to be ID 0,0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image],0
Example filename:,0
,0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG',0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG',0
i_row = row_indices[0],0
There are a small handful of datetime mismatches across annotations,0
for the same image,0
assert im['datetime'] == timestamp,0
Special cases based on the 'photo type' field,0
Various spellings of 'community',0
Have we seen this category before?,0
Create an annotation,0
fieldname = list(mapped_fields.keys())[0],0
...for each row,0
...for each image,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
%% Scrap,0
%% Find unique photo types,0
##,0
,0
timelapse_csv_set_to_json.py,0
,0
Given a directory full of reasonably-consistent Timelapse-exported,0
".csvs, assemble a CCT .json.",0
,0
"Assumes that you have a list of all files in the directory tree, including",0
image and .csv files.,0
,0
##,0
%% Constants and imports,0
Text file with relative paths to all files (images and .csv files),0
"%% Read file list, make a list of all image files and all .csv files",0
"%% Verify column consistency, create a giant array with all rows from all .csv files",0
i_csv = 0; csv_filename = csv_files[0],0
%% Prepare some data structures we'll need for mapping image rows in .csv files to actual image files,0
Enumerate all folders containing image files,0
"In this data set, a site folder looks like:",0
,0
Processed Images\\site_name,0
%% Map .csv files to candidate camera folders,0
fn = valid_csv_files[0],0
"Some site folders appear as ""XXNNNN"", some appear as ""XXNNNN_complete""",0
...for each .csv file,0
%% Map camera folders to candidate image folders,0
%% Main loop over labels (prep),0
Force the empty category to be ID 0,0
Images that are marked empty and also have a species label,0
%% Main loop over labels (loop),0
i_row = 0; row = input_metadata.iloc[i_row],0
"for i_row,row in input_metadata.iterrows():",0
"Usually this is just a single folder name, sometimes it's a full path,",0
which we don't want,0
Check whether this file exists on disk,0
Retrieve image width and height,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each row in the big table of concatenated .csv files,0
%% Fix cases where an image was annotated as 'unlabeled' and as something else,0
This annotation is 'unlabeled',0
Was there another category associated with this image?,0
%% Check for un-annnotated images,0
Enumerate all images,0
list(relative_path_to_image.keys())[0],0
%% Create info struct,0
%% Write output,0
%% Sanity-check the database's integrity,0
%% Render a bunch of images to make sure the labels got carried along correctly,0
,0
auckland_doc_to_json.py,0
,0
Convert Auckland DOC data set to COCO camera traps format.  This was,0
for a training data set where class names were encoded in path names.,0
,0
%% Constants and imports,0
Filenames will be stored in the output .json relative to this base dir,0
%% Enumerate files,0
%% Assemble dictionaries,0
Force the empty category to be ID 0,0
fn = image_files[0]; print(fn),0
Typically y:\Maukahuka_Auckland_Island\1_Training\Winter_Trial_2019\cat\cat\eat\20190903_IDdY_34_E3_tmp_201908240051.JPG,0
"This data set has two top-level folders, ""1_Training"" (which has class names encoded",0
"in paths) and ""2_Testing"" (which has no class information).",0
...for each image,0
%% Write output .json,0
%% Write train/test .jsons,0
%% Validate .json files,0
%% Preview labels,0
checkpoint,0
,0
awc_to_json.py,0
,0
Convert a particular .csv file to CCT format.,0
,0
%% Constants and environment,0
%% Read source data,0
%% Main loop over labels,0
Force the empty category to be ID 0,0
iRow = 0; row = input_metadata.iloc[iRow],0
"ImageID,FileName,FilePath,SpeciesID,CommonName",0
"This makes an assumption of one annotation per image, which happens to be",0
true in this data set.,0
Retrieve image width and height,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
list(relativePathToImage.keys())[0],0
%% Create info struct,0
%% Write output,0
%% Sanity-check the database's integrity,0
%% Render a bunch of images to make sure the labels got carried along correctly,0
,0
idaho-camera-traps.py,0
,0
Prepare the Idaho Camera Traps dataset for release on LILA.,0
,0
%% Imports and constants,0
Multi-threading for .csv file comparison and image existence validation,0
"We are going to map the original filenames/locations to obfuscated strings, but once",0
"we've done that, we will re-use the mappings every time we run this script.",0
This is the file to which mappings get saved,0
The maximum time (in seconds) between images within which two images are considered the,0
same sequence.,0
"This is a two-column file, where each line is [string in the original metadata],[category name we want to map it to]",0
"The output file, using the original strings",0
"The output file, using obfuscated strings for everything but filenamed",0
"The output file, using obfuscated strings and obfuscated filenames",0
"One time only, I ran MegaDetector on the whole dataset...",0
...then set aside any images that *may* have contained humans that had not already been,0
annotated as such.  Those went in this folder...,0
...and the ones that *actually* had humans (identified via manual review) got,0
copied to this folder...,0
"...which was enumerated to this text file, which is a manually-curated list of",0
images that were flagged as human.,0
Unopinionated .json conversion of the .csv metadata,0
%% List files (images + .csv),0
Ignore .csv files in folders with multiple .csv files,0
...which would require some extra work to decipher.,0
fn = csv_files[0],0
%% Parse each .csv file into sequences (function),0
csv_file = csv_files[-1],0
os.startfile(csv_file_absolute),0
survey = csv_file.split('\\')[0],0
Sample paths from which we need to derive locations:,0
,0
St.Joe_elk\AM99\Trip 1\100RECNX\TimelapseData.csv,0
Beaverhead_elk\AM34\Trip 1\100RECNX\TimelapseData.csv,0
,0
ClearCreek_mustelids\Winter2015-16\FS-001-P\FS-001-P.csv,0
ClearCreek_mustelids\Summer2015\FS-001\FS-001.csv,0
ClearCreek_mustelids\Summer2016\IDFG-016\IDFG-016.csv,0
,0
I:\idfg-images\ClearCreek_mustelids\Summer2016\IDFG-017b,0
I:\idfg-images\ClearCreek_mustelids\Summer2016\IDFG-017a,0
Load .csv file,0
Validate the opstate column,0
# Create datetimes,0
print('Creating datetimes'),0
i_row = 0; row = df.iloc[i_row],0
Make sure data are sorted chronologically,0
,0
"In odd circumstances, they are not... so sort them first, but warn",0
Debugging when I was trying to see what was up with the unsorted dates,0
# Parse into sequences,0
print('Creating sequences'),0
i_row = 0; row = df.iloc[i_row],0
Start a new sequence if:,0
,0
* This image has no timestamp,0
* This image has a frame number of zero,0
* We have no previous image timestamp,0
,0
Start a new sequence if necessary,0
...for each row,0
# Parse labels for each sequence,0
sequence_id = location_sequences[0],0
Row indices in a sequence should be adjacent,0
sequence_df = df[df['seq_id']==sequence_id],0
# Determine what's present,0
Be conservative; assume humans are present in all maintenance images,0
The presence columns are *almost* always identical for all images in a sequence,0
assert single_presence_value,0
print('Warning: presence value for {} is inconsistent for {}'.format(,0
"presence_column,sequence_id))",0
...for each presence column,0
Tally up the standard (survey) species,0
"If no presence columns are marked, all counts should be zero",0
count_column = count_columns[0],0
Occasionally a count gets entered (correctly) without the presence column being marked,0
"assert len(values) == 1 and values[0] == 0, 'Non-zero counts with no presence",0
columns marked for sequence {}'.format(sequence_id),0
"Handle this by virtually checking the ""right"" box",0
Make sure we found a match,0
Handle 'other' tags,0
column_name = otherpresent_columns[0],0
print('Found non-survey counted species column: {}'.format(column_name)),0
...for each non-empty presence column,0
"Very rarely, the ""otherpresent"" column is checked, but no more detail is available",0
...handling non-survey species,0
Build the sequence data,0
i_row = 0; row = sequence_df.iloc[i_row],0
Only one folder used a single .csv file for two subfolders,0
...for each sequence,0
...def csv_to_sequences(),0
%% Parse each .csv file into sequences (loop),0
%%,0
%%,0
i_file = -1; csv_file = csv_files[i_file],0
%% Save sequence data,0
%% Load sequence data,0
%%,0
%% Validate file mapping (based on the existing enumeration),0
sequences = sequences_by_file[0],0
sequence = sequences[0],0
"Actually, one folder has relative paths",0
assert '\\' not in image_file_relative and '/' not in image_file_relative,0
os.startfile(csv_folder),0
assert os.path.isfile(image_file_absolute),0
found_file = os.path.isfile(image_file_absolute),0
...for each image,0
...for each sequence,0
...for each .csv file,0
%% Load manual category mappings,0
The second column is blank when the first column already represents the category name,0
%% Convert to CCT .json (original strings),0
Force the empty category to be ID 0,0
For each .csv file...,0
,0
sequences = sequences_by_file[0],0
For each sequence...,0
,0
sequence = sequences[0],0
Find categories for this image,0
"When 'unknown' is used in combination with another label, use that",0
"label; the ""unknown"" here doesn't mean ""another unknown species"", it means",0
there is some other unknown property about the main species.,0
category_name_string = species_present[0],0
"This piece of text had a lot of complicated syntax in it, and it would have",0
been too complicated to handle in a general way,0
print('Ignoring category {}'.format(category_name_string)),0
Don't process redundant labels,0
category_name = category_names[0],0
If we've seen this category before...,0
If this is a new category...,0
print('Adding new category for {}'.format(category_name)),0
...for each category (inner),0
...for each category (outer),0
...if we do/don't have species in this sequence,0
"We should have at least one category assigned (which may be ""empty"" or ""unknown"")",0
assert len(sequence_category_ids) > 0,0
Was any image in this sequence manually flagged as human?,0
print('Flagging sequence {} as human based on manual review'.format(sequence_id)),0
For each image in this sequence...,0
,0
i_image = 0; im = images[i_image],0
Create annotations for this image,0
...for each image in this sequence,0
...for each sequence,0
...for each .csv file,0
Verify that all images have annotations,0
ann = ict_data['annotations'][0],0
For debugging only,0
%% Create output (original strings),0
%% Validate .json file,0
%% Preview labels,0
%% Look for humans that were found by MegaDetector that haven't already been identified as human,0
This whole step only needed to get run once,0
%%,0
Load MD results,0
Get a list of filenames that MD tagged as human,0
im = md_results['images'][0],0
...for each detection,0
...for each image,0
Map images to annotations in ICT,0
ann = ict_data['annotations'][0],0
For every image,0
im = ict_data['images'][0],0
Does this image already have a human annotation?,0
...for each annotation,0
...for each image,0
%% Copy images for review to a new folder,0
fn = missing_human_images[0],0
%% Manual step...,0
Copy any images from that list that have humans in them to...,0
%% Create a list of the images we just manually flagged,0
fn = human_tagged_filenames[0],0
E.g. '0000_Beaverhead_elk~AM174~Trip 1~100RECNX~IMG_1397.JPG',0
"%% Translate location, image, sequence IDs",0
Load mappings if available,0
Generate mappings,0
If we've seen this location before...,0
Otherwise assign a string-formatted int as the ID,0
If we've seen this sequence before...,0
Otherwise assign a string-formatted int as the ID,0
Assign an image ID,0
...for each image,0
Assign annotation mappings,0
Save mappings,0
"Back this file up, lest we should accidentally re-run this script",0
with force_generate_mappings = True and overwrite the mappings we used.,0
...if we are/aren't re-generating mappings,0
%% Apply mappings,0
"%% Write new dictionaries (modified strings, original files)",0
"%% Validate .json file (modified strings, original files)",0
%% Preview labels (original files),0
"viz_options.classes_to_exclude = ['empty','deer','elk']",0
viz_options.classes_to_include = ['bobcat'],0
%% Copy images to final output folder (prep),0
ann = d['annotations'][0],0
Is this a public or private image?,0
Generate absolute path,0
Copy to output,0
Update the filename reference,0
...def process_image(im),0
%% Copy images to final output folder (execution),0
For each image,0
im = images[0],0
Write output .json,0
%% Make sure the right number of images got there,0
%% Validate .json file (final filenames),0
%% Preview labels (final filenames),0
"viz_options.classes_to_exclude = ['empty','deer','elk']",0
viz_options.classes_to_include = ['horse'],0
viz_options.classes_to_include = [viz_options.multiple_categories_tag],0
"viz_options.classes_to_include = ['human','vehicle','domestic dog']",0
%% Create zipfiles,0
%% List public files,0
%% Find the size of each file,0
fn = all_public_output_files[0],0
%% Split into chunks of approximately-equal size,0
...for each file,0
%% Create a zipfile for each chunk,0
...for each filename,0
with ZipFile(),0
...def create_zipfile(),0
i_file_list = 0; file_list = file_lists[i_file_list],0
"....if __name__ == ""__main__""",0
,0
wellington_to_json.py,0
,0
Convert the .csv file provided for the Wellington data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
%% Read source data,0
"Filenames were provided as *.jpg, but images were *.JPG, converting here",0
"%% Map filenames to rows, verify image existence",0
"Takes ~30 seconds, since it's checking the existence of ~270k images",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
"I didn't expect this to be true a priori, but it appears to be true, and",0
it saves us the trouble of checking consistency across multiple occurrences,0
of an image.,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
imageName = imageFilenames[0],0
"As per above, this is convenient and appears to be true; asserting to be safe",0
"Filenames look like ""290716114012001a1116.jpg""",0
This gets imported as an int64,0
"These appear as ""image1"", ""image2"", etc.",0
"In the form ""001a""",0
Can be in the form '111' or 's46',0
"In the form ""7/29/2016 11:40""",0
Check image height and width,0
"Use 'empty', to be consistent with other data on lila",0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
,0
mcgill_to_json.py,0
,0
Convert the .csv file provided for the McGill test data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
Create an additional column for concatenated filenames,0
Maps relative filenames to rows,0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
row = input_metadata.iloc[0],0
"I didn't expect this to be true a priori, but it appears to be true, and",0
it saves us the trouble of checking consistency across multiple occurrences,0
of an image.,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
row = input_metadata.iloc[0],0
"Filenames look like ""290716114012001a1116.jpg""",0
"In the form ""001a""",0
Can be in the form '111' or 's46',0
"In the form ""7/29/2016 11:40""",0
Check image height and width,0
NaN is the only thing we should see that's not a string,0
NaN is the only thing we should see that's not a string,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
,0
bellevue_to_json.py,0
,0
"""Bellevue Camera Traps"" is the rather unremarkable camera trap data set",0
used by one of the repo's maintainers for testing.  It's organized as:,0
,0
approximate_date/[loose_camera_specifier/]/species,0
,0
E.g.:,0
,0
"""2018.03.30\coyote\DSCF0091.JPG""",0
"""2018.07.18\oldcam\empty\DSCF0001.JPG""",0
,0
%% Constants and imports,0
from the ai4eutils repo,0
Filenames will be stored in the output .json relative to this base dir,0
%% Exif functions,0
"%% Enumerate files, create image/annotation/category info",0
Force the empty category to be ID 0,0
Keep track of unique camera folders,0
Each element will be a dictionary with fields:,0
,0
"relative_path, width, height, datetime",0
fname = image_files[0],0
Corrupt or not an image,0
Store file info,0
E.g. 2018.03.30/coyote/DSCF0091.JPG,0
...for each image file,0
%% Synthesize sequence information,0
Sort images by time within each folder,0
camera_path = camera_folders[0],0
previous_datetime = sorted_images_this_camera[0]['datetime'],0
im = sorted_images_this_camera[1],0
Start a new sequence if necessary,0
...for each image in this camera,0
...for each camera,0
Fill in seq_num_frames,0
%% A little cleanup,0
%% Write output .json,0
%% Sanity-check data,0
%% Label previews,0
,0
snapshot_safar_importer_reprise.py,0
,0
This is a 2023 update to snapshot_safari_importer.py.  We do a bunch of things now that,0
we didn't do the last time we imported Snapshot data (like updating the big taxonomy),0
"file, and we skip a bunch of things now that we used to do (like generating massive",0
"zipfiles).  So, new year, new importer.",0
,0
%% Constants and imports,0
%% List files,0
"Do a one-time enumeration of the entire drive; this will take a long time,",0
but will save a lot of hassle later.,0
%% Create derived lists,0
Takes about 60 seconds,0
CSV files are one of:,0
,0
"_report_lila.csv (this is the one we want to use, with the species/count/etc. for each sequence)",0
_report_lila_image_inventory.csv (maps captures to images),0
_report_lila_overview.csv (distrubution of species),0
%% List project folders,0
Project folders look like one of these:,0
,0
APN,0
Snapshot Cameo/DEB,0
%% Map report and inventory files to codes,0
fn = csv_files[0],0
%% Make sure that every report has a corresponding inventory file,0
%% Count species based on overview and report files,0
%% Print counts,0
%% Make sure that capture IDs in the reports/inventory files match,0
...and that all the images in the inventory tables are actually present on disk.,0
assert image_path_relative in all_files_relative_set,0
Make sure this isn't just a case issue,0
...for each report on this project,0
...for each project,0
"%% For all the files we have on disk, see which are and aren't in the inventory files",0
"There aren't any capital-P .PNG files, but if I don't include that",0
"in this list, I'll look at this in a year and wonder whether I forgot",0
to include it.,0
fn = all_files_relative[0],0
print('Skipping project {}'.format(project_code)),0
,0
plot_wni_giraffes.py,0
,0
Plot keypoints on a random sample of images from the wni-giraffes data set.,0
,0
%% Constants and imports,0
%% Load and select data,0
%% Support functions,0
https://stackoverflow.com/questions/32504246/draw-ellipse-in-python-pil-with-line-thickness,0
Use a single channel image (mode='L') as mask.,0
The size of the mask can be increased relative to the imput image,0
to get smoother looking results.,0
draw outer shape in white (color) and inner shape in black (transparent),0
downsample the mask using PIL.Image.LANCZOS,0
(a high-quality downsampling filter).,0
paste outline color to input image through the mask,0
%% Plot some images,0
ann = annotations_to_plot[0],0
i_tool = 0; tool_name = short_tool_names[i_tool],0
Don't plot tools that don't have a consensus annotation,0
...for each tool,0
...for each annotation,0
,0
idfg_iwildcam_lila_prep.py,0
,0
Adding class labels (from the private test .csv) to the iWildCam 2019 IDFG,0
"test set, in preparation for release on LILA.",0
,0
This version works with the public iWildCam release images.,0
,0
"%% ############ Take one, from iWildCam .json files ############",0
%% Imports and constants,0
%% Read input files,0
Remove the header line,0
%% Parse annotations,0
Lines look like:,0
,0
"b005e5b2-2c0b-11e9-bcad-06f1011196c4,1,Private",0
%% Minor cleanup re: images,0
%% Create annotations,0
%% Prepare info,0
%% Minor adjustments to categories,0
Remove unused categories,0
Name adjustments,0
%% Create output,0
%% Write output,0
%% Validate .json file,0
%% Preview labels,0
viz_options.classes_to_exclude = ['test'],0
"%% ############ Take two, from pre-iWildCam .json files created from IDFG .csv files ############",0
%% Imports and constants,0
%% One-time line break addition,0
%% Read input files,0
%% Prepare info,0
%% Minor adjustments to categories,0
%% Minor adjustments to annotations,0
%% Create output,0
%% Write output,0
%% Validate .json file,0
%% Preview labels,0
viz_options.classes_to_exclude = ['test'],0
,0
missouri_to_json.py,0
,0
Create .json files from the original source files for the Missouri Camera Traps,0
data set.  Metadata was provided here in two formats:,0
,0
"1) In one subset of the data, folder names indicated species names.  In Set 1,",0
there are no empty sequences.  Set 1 has a metadata file to indicate image-level,0
bounding boxes.,0
,0
2) A subset of the data (overlapping with (1)) was annotated with bounding,0
"boxes, specified in a whitespace-delimited text file.  In set 2, there are",0
"some sequences omitted from the metadata file, which implied emptiness.",0
,0
"In the end, set 2 labels were not reliable enough to publish, so LILA includes only set 1.",0
,0
%% Constants and imports,0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Filenames will be stored in the output .json relative to this base dir,0
This will be a list of filenames that need re-annotation due to redundant boxes,0
"%% Enumerate files, read image sizes (both sets)",0
"Takes a few minutes, since we're reading image sizes.",0
Each element will be a list of relative path/full path/width/height,0
"Only process leaf nodes corresponding to sequences, which look like:",0
,0
E:\wildlife_data\missouri_camera_traps\Set1\1.02-Agouti\SEQ75583,0
E:\wildlife_data\missouri_camera_traps\Set2\p1d101,0
,0
assert len(files) <= 2,0
Read the image,0
Not an image...,0
Store file info,0
"...if we didn't hit the max file limit, keep going",0
...for each file,0
%% Add sequence lengths (both sets),0
%% Load the set 1 metadata file,0
"List of lists, length varies according to number of bounding boxes",0
,0
Preserves original ordering,0
iLine = 0; line = metadataSet1Lines[0],0
"Lines should be filename, number of bounding boxes, boxes (four values per box)",0
Make sure we have image info for this image,0
%% Print missing files from Set 1 metadata,0
Manual changes I made to the metadata file:,0
,0
'IMG' --> 'IMG_',0
Red_Brocket_Deer --> Red_Deer,0
European-Hare --> European_Hare,0
Wood-Mouse --> Wood_Mouse,0
Coiban-Agouti --> Coiban_Agouti,0
%% Load the set 2 metadata file,0
"This metadata file contains most (but not all) images, and a class label (person/animal/empty)",0
"for each, plus bounding boxes.",0
"List of lists, length varies according to number of bounding boxes",0
,0
Preserves original ordering,0
"Create class IDs for each *sequence*, which we'll use to attach classes to",0
images for which we don't have metadata,0
,0
This only contains mappings for sequences that appear in the metadata.,0
iLine = 0; line = metadataSet2Lines[0],0
"Lines should be filename, number of bounding boxes, labeled boxes (five values per box)",0
,0
Empty images look like filename\t0\t0,0
E.g. 'Set2\\p1d101\\p1d101s100i10.JPG',0
Make sure we don't have mixed classes within an image,0
"Figure out what class this *sequence* is, so we know how to handle unlabeled",0
images from this sequence,0
Can't un-do a mixed sequence,0
Previously-empty sequences get the image class label,0
"If the sequence has a non-empty class, possibly change it",0
Make sure we have image info for this image,0
...for each line in the set 2 metadata file,0
%% What Set 2 images do I not have metadata for?,0
These are *mostly* empty images,0
iImage = 0; imageID = set2ImageIDs[iImage],0
%% Create categories and annotations for set 1,0
"Though we have no empty sequences, we do have empty images in this set",0
For each image,0
,0
iImage = 0; imageID = set1ImageIDs[iImage],0
E.g. Set1\\1.80-Coiban_Agouti\\SEQ83155\\SEQ83155_IMG_0010.JPG,0
Find the species name,0
This image may still be empty...,0
category['count'] = category['count'] + 1,0
"If we have bounding boxes, create image-level annotations",0
"filename, number of bounding boxes, boxes (four values per box)",0
"Make sure the relative filename matches, allowing for the fact that",0
some of the filenames in the metadata aren't quite right,0
This image is non-empty,0
"Some redundant bounding boxes crept in, don't add them twice",0
Check this bbox against previous bboxes,0
,0
Inefficient?  Yes.  In an important way?  No.,0
"Bounding box values are in absolute coordinates, with the origin",0
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1].",0
,0
"Convert to floats and to x/y/w/h, as per CCT standard",0
...for each box,0
if we do/don't have boxes for this image,0
Else create a sequence-level annotation,0
...for each image,0
%% Write out the list of images with redundant boxes,0
%% Create categories and annotations for set 2,0
For each image,0
,0
iImage = 0; imageID = set2ImageIDs[iImage],0
E.g. 'Set2\\p1d100\\p1d100s10i1.JPG',0
"Find the sequence ID, sanity check filename against what we stored",0
"If we have bounding boxes or an explicit empty label, create image-level annotations",0
"filename, number of bounding boxes, labeled boxes (five values per box)",0
"Make sure the relative filename matches, allowing for the fact that",0
some of the filenames in the metadata aren't quite right,0
"Bounding box values are in absolute coordinates, with the origin",0
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1].",0
,0
Convert to floats and to x/y/w/h,0
...for each box,0
...if we do/don't have boxes for this image,0
Else create a sequence-level annotation,0
...for each image,0
"%% The 'count' field isn't really meaningful, delete it",0
"It's really the count of image-level annotations, not total images assigned to a class",0
%% Write output .json files,0
%% Sanity-check final set 1 .json file,0
"python sanity_check_json_db.py --bCheckImageSizes --baseDir ""E:\wildlife_data\missouri_camera_traps"" ""E:\wildlife_data\missouri_camera_traps\missouri_camera_traps_set1.json""",0
%% Generate previews,0
,0
filenames_to_json.py,0
,0
Take a directory of images in which species labels are encoded by folder,0
"names, and produces a COCO-style .json file",0
,0
%% Constants and imports,0
from the ai4eutils repo,0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Filenames will be stored in the output .json relative to this base dir,0
"rawClassListFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_classes.csv')",0
"classMappingsFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_class_mapping.csv')",0
"%% Enumerate files, read image sizes",0
Each element will be a list of relative path/full path/width/height,0
Read the image,0
Corrupt or not an image,0
Store file info,0
Write to output file,0
...for each image file,0
...csv file output,0
...if the file list is/isn't available,0
%% Enumerate classes,0
Maps classes to counts,0
We like 'empty' to be class 0,0
%% Assemble dictionaries,0
...for each category,0
Each element is a list of relative path/full path/width/height/className,0
...for each image,0
%% External class mapping,0
%% Write raw class table,0
cat = categories[0],0
%% Read the mapped class table,0
"id, source, count, target",0
"%% Make classMappings contain *all* classes, not just remapped classes",0
cat = categories[0],0
%% Create new class list,0
"Start at 1, explicitly assign 0 to ""empty""",0
One-off issue with character encoding,0
%% Re-map annotations,0
ann = annotations[0],0
%% Write output .json,0
%% Utilities,0
%%,0
Find images with a particular tag,0
%% Randomly sample annotations,0
,0
umn_to_json.py,0
,0
Prepare images and metadata for the Orinoqua Camera Traps dataset.,0
,0
%% Imports and constants,0
"For two deployments, we're only processing imagse in the ""detections"" subfolder",0
%% Enumerate deployment folders,0
%% Load ground truth,0
i_row = 0; row = ground_truth_df.iloc[i_row],0
%% Create relative paths for ground truth data,0
"Some deployment folders have no subfolders, e.g. this is a valid file name:",0
,0
M00/01010132.JPG,0
,0
"But some deployment folders have subfolders, e.g. this is also a valid file name:",0
,0
N17/100EK113/07160020.JPG,0
,0
"So we can't find files by just concatenating folder and file names, we have to enumerate and explicitly",0
"map what will appear in the ground truth as ""folder/filename"" to complete relative paths.",0
deployment_name = list(deployment_folders)[0],0
Enumerate all files in this folder,0
"print('Enumerated {} files for deployment {}'.format(len(files),deployment_name))",0
filename = files[100],0
...for each file in this deployment,0
...for each deployment,0
%% Add relative paths to our ground truth table,0
i_row = 0; row = ground_truth_df.iloc[i_row],0
"row['filename'] looks like, e.g. A01/01080001.JPG.  This is not actually a path, it's",0
"just the deployment ID and the image name, separated by a slash.",0
Find the relative path for this image,0
Make sure this image file exists,0
...for each row in the ground truth table,0
%% Take everything out of Pandas,0
%% Convert string timestamps to Python datetimes,0
im = ground_truth_dicts[0],0
"Filenames look like, e.g., N36/100EK113/06040726.JPG",0
%% Synthesize sequence information,0
Sort images by time within each location,0
i_location=0; location = locations[i_location],0
previous_datetime = sorted_images_this_location[0]['datetime'],0
im = sorted_images_this_camera[1],0
"Timestamp for this image, may be None",0
Start a new sequence if:,0
,0
* This image has no timestamp,0
* This iamge has a frame number of zero,0
* We have no previous image timestamp,0
,0
Start a new sequence if necessary,0
...for each image in this location,0
...for each location,0
%% Create category dict and category IDs,0
im = images[0],0
%% Imports and constants (.json generation),0
%% Count frames in each sequence,0
%% Double check images with multiple annotations,0
im = images[0],0
%% Assemble dictionaries,0
Force the empty category to be ID 0,0
input_im = images[0],0
...for each image,0
%% Write output .json,0
%% Validate .json file,0
%% Map relative paths to annotation categories,0
ann = data['annotations'][0],0
%% Copy images to output,0
EXCLUDE HUMAN AND MISSING,0
im = data['images'][0],0
im = images[0],0
%% Preview labels,0
viz_options.classes_to_exclude = ['test'],0
,0
nacti_fieldname_adjustments.py,0
,0
"NACTI metadata was posted with ""filename"" in images instead of ""file_name"", and",0
"used string (rather than int) category IDs (in categories, but not in annotations).",0
,0
This script fixes those issues and rev's the version number.,0
,0
%% Constants and environment,0
%% Read .json file,0
"%% Rev version number, update field names and types",0
%% Write json file,0
,0
cct_field_adjustments.py,0
,0
CCT metadata was posted with int locations instead of strings.,0
,0
This script fixes those issues and rev's the version number.,0
,0
%% Constants and environment,0
%% Read .json file,0
"%% Rev version number, update field names and types",0
%% Write json file,0
%% Check output data file,0
,0
carrizo_shrubfree_2018.py,0
,0
Convert the .csv file provided for the Carrizo Mojave data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
%% Read source data,0
Original .csv file had superfluous spaces in column names,0
"%% Map filenames to rows, verify image existence",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
58 missing files (of 17652),0
%% Check for images that aren't included in the metadata file,0
3012 of 20606 files are not in metadata,0
%% Create CCT dictionaries,0
Map categories to integer IDs,0
,0
The category '0' is reserved for 'empty',0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
Each filename should just match one row,0
Don't include images that don't exist on disk,0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
,0
snapshot_serengeti_lila.py,0
,0
Create zipfiles of Snapshot Serengeti S1-S11.,0
,0
"Create a metadata file for S1-S10, plus separate metadata files",0
"for S1-S11.  At the time this code was written, S11 was under embargo.",0
,0
Create zip archives of each season without humans.,0
,0
Create a human zip archive.,0
,0
%% Constants and imports,0
import sys; sys.path.append(r'c:\git\ai4eutils'),0
import sys; sys.path.append(r'c:\git\cameratraps'),0
assert(os.path.isdir(metadata_base)),0
"There are two redundant categories, and we re-map ""blank"" to ""empty"" as per CCT convention",0
"%% Load metadata files, concatenate into a single table",0
iSeason = 1,0
%% Convert to dictionaries (prep),0
%% Convert to dictionaries (loops),0
"TODO: iterrows() is a terrible way to do this, but this is one of those days",1
"where I want to get this done, not get better at Python.",0
irow = 0; row = image_table.iloc[0],0
"Loaded as an int64, converting to int here",0
...for each row in the image table,0
Make sure image IDs are what we think they are,0
...for each row in the annotation table,0
%% Load previously-saved dictionaries when re-starting mid-script,0
%%,0
%% Take a look at categories (just sanity-checking),0
%%,0
%% Fill in some image fields we didn't have when we created the image table,0
"width, height, corrupt, seq_num_frames, location, datetime",0
Every annotation in this list should have the same sequence ID,0
"Figure out ""seq_num_frames"", which really should be done in a separate lopp;",0
there's no reason to do this redundantly for every image,0
Every image in this sequence should point back to the same equence,0
Every annotation in this list should have the same location,0
Every annotation in this list should have the same datetime,0
Is this image on disk?,0
iImage = 0; im = images[0],0
...for each image,0
"images_processed = pool.map(process_image, images)",0
"images_processed = list(tqdm(pool.imap_unordered(process_image, images), total=len(images)))",0
%%,0
%% Count missing/corrupted images,0
%% Print distribution of sequence lengths (sanity-check),0
%% Replicate annotations across images,0
iAnn = 0; ann = annotations[iAnn],0
%% See what files are on disk but not annotated (~15 mins),0
247370 files not in the database (of 7425810),0
%% Load old image database,0
%% Look for old images not in the new DB and vice-versa,0
"At the time this was written, ""old"" was S1-S6",0
old_im = cct_old['images'][0],0
new_im = images[0],0
4 old images not in new db,0
12 new images not in old db,0
%% Save our work,0
%% Load our work,0
%%,0
%% Examine size mismatches,0
i_mismatch = -1; old_im = size_mismatches[i_mismatch],0
%% Sanity-check image and annotation uniqueness,0
"%% Split data by seasons, create master list for public seasons",0
ann = annotations[0],0
%% Minor updates to fields,0
"%% Write master .json out for S1-10, write individual season .jsons (including S11)",0
"Loop over all seasons, plus one iteration for the ""all public data"" iteration, and",0
"one for the ""all data"" iteration",0
%% Find categories that only exist in S11,0
List of categories in each season,0
Category 55 (fire) only in S11,0
Category 56 (hyenabrown) only in S11,0
Category 57 (wilddog) only in S11,0
Category 58 (kudu) only in S11,0
Category 59 (pangolin) only in S11,0
Category 60 (lioncub) only in S11,0
%% Prepare season-specific .csv files,0
iSeason = 1,0
%% Create a list of human files,0
ann = annotations[0],0
%% Save our work,0
%% Load our work,0
%%,0
"%% Create archives (human, per-season) (prep)",0
im = images[0],0
im = images[0],0
Don't include humans,0
Only include files from this season,0
Possibly start a new archive,0
...for each image,0
i_season = 0,0
"for i_season in range(0,nSeasons):",0
create_season_archive(i_season),0
%% Create archives (loop),0
pool = ThreadPool(nSeasons+1),0
"n_images = pool.map(create_archive, range(-1,nSeasons))",0
"seasons_to_zip = range(-1,nSeasons)",0
...for each season,0
%% Sanity-check .json files,0
%logstart -o r'E:\snapshot_temp\python.txt',0
%% Zip up .json and .csv files,0
pool = ThreadPool(len(files_to_zip)),0
"pool.map(zip_single_file, files_to_zip)",0
%% Super-sanity-check that S11 info isn't leaking,0
im = data_public['images'][0],0
ann = data_public['annotations'][0],0
iRow = 0; row = annotation_df.iloc[iRow],0
iRow = 0; row = image_df.iloc[iRow],0
%% Create bounding box archive,0
i_image = 0; im = data['images'][0],0
i_box = 0; boxann = bbox_data['annotations'][0],0
%% Sanity-check a few files to make sure bounding boxes are still sensible,0
import sys; sys.path.append(r'C:\git\CameraTraps'),0
%% Check categories,0
%% Summary prep for LILA,0
,0
wi_to_json,0
,0
Prepares CCT-formatted metadata based on a Wildlife Insights data export.,0
,0
"Mostly assumes you have the images also, for validation/QA.",0
,0
%% Imports and constants,0
"Because WI filenames are GUIDs, it's not practical to page through sequences in an",0
image viewer.  So we're going to (optionally) create a copy of the data set where,0
images are ordered.,0
%% Load ground truth,0
%% Take everything out of Pandas,0
%% Synthesize common names when they're not available,0
"Blank rows should always have ""Blank"" as the common name",0
"print('Warning: missing common name for row {} ({})'.format(i_row,row['filename']))",0
%% Convert string timestamps to Python datetimes,0
im = ground_truth_dicts[0],0
"The field called ""location"" in the WI .csv file is a URL, we want to reclaim",0
"the ""location"" keyword for CCT output",0
"Filenames look like, e.g., N36/100EK113/06040726.JPG",0
%% Synthesize sequence information,0
Sort images by time within each location,0
i_location=0; location = locations[i_location],0
previous_datetime = sorted_images_this_location[0]['datetime'],0
im = sorted_images_this_camera[1],0
"Timestamp for this image, may be None",0
Start a new sequence if:,0
,0
* This image has no timestamp,0
* This iamge has a frame number of zero,0
* We have no previous image timestamp,0
,0
Start a new sequence if necessary,0
...for each image in this location,0
...for each location,0
%% Create category dict and category IDs,0
im = images[0],0
%% Count frames in each sequence,0
%% Build relative paths,0
im = images[0],0
Sample URL:,0
,0
gs://project-asfasdfd/deployment/21444549/asdfasdfd-616a-4d10-a921-45ac456c568a.jpg',0
%% Double check images with multiple annotations,0
im = images[0],0
%% Assemble dictionaries,0
Force the empty category to be ID 0,0
input_im = images[0],0
"print('Warning: image ID {} ({}) has multiple annotations'.format(im['id'],im['id'].replace('_','/')))",0
...for each image,0
%% Write output .json,0
%% Validate .json file,0
%% Preview labels,0
"open_file(os.path.join(image_base,'2100703/1141a545-88d2-498b-a684-7431f7aeb324.jpg'))",0
%%,0
%% Create ordered dataset,0
"Because WI filenames are GUIDs, it's not practical to page through sequences in an",0
image viewer.  So we're going to create a copy of the data set where images are,0
ordered.,0
im = images_out[0]; im,0
%% Create ordered .json,0
%% Copy files to their new locations,0
im = ordered_images[0],0
im = data_ordered['images'][0],0
%% Preview labels in the ordered dataset,0
"open_file(os.path.join(image_base,'2100703/1141a545-88d2-498b-a684-7431f7aeb324.jpg'))",0
%% Open an ordered filename from the unordered filename,0
,0
rspb_to_json.py,0
,0
Convert the .csv file provided for the RSPB data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
[location] is an obfuscation,0
%% Create info struct,0
%% Read source data,0
metadataTable.columns.values,0
,0
"array(['Project', 'inDir', 'FileName', 'Station', 'Camera',",0
"'StationCameraFileName', 'DateTimeOriginal', 'DateReadable',",0
"'outDir', 'filename_new', 'fileExistsAlready', 'CopyStatus',",0
"'Species'], dtype=object)",0
We'll populate these later,0
keys should be lowercase,0
"%% Enumerate images, confirm filename uniqueness",0
"%% Update metadata filenames to include site and camera folders, check existence",0
,0
Takes ~1min,0
iRow = 0; row = metadataTable.iloc[iRow],0
There's a bug in the metadata; the 'camera' column isn't correct.,0
camera = row['Camera'],0
"These appear as, e.g., '3.22e12'",0
camera = str(int(float(camera))),0
Let's pull this out of the file name instead,0
,0
Filenames look like one of the following:,0
,0
A1__03224850850507__2015-11-28__10-45-04(1).JPG,0
Bayama2PH__C05__NA(NA).JPG,0
assert(os.path.isfile(fullPath)),0
metadataTable.iloc[iRow] = row,0
Re-assemble into an updated table,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
Write to a text file,0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
iRow = 0; row = metadataTable.iloc[iRow],0
A1__03224850850507__2015-11-28__10-45-04(1).JPG,0
'A1\\03224850850507\\A1__03224850850507__2015-11-28__10-45-04(1).JPG',0
Not currently populated,0
"Often -1, sometimes a semi-meaningful int",0
A1,0
03224850850507,0
"In variable form, but sometimes '28/11/2015 10:45'",0
Check image height and width,0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Write output,0
%% Check database integrity,0
%% Preview a few images to make sure labels were passed along sensibly,0
%% One-time processing step: copy images to a flat directory for annotation,0
%%,0
,0
pc_to_json.py,0
,0
Convert a particular collection of .csv files to CCT format.,0
,0
%% Constants and environment,0
%% Read and concatenate source data,0
List files,0
"List of dataframes, one per .csv file; we'll concatenate later",0
i_file = 87; fn = input_files[i_file],0
Concatenate into a giant data frame,0
%% List files,0
%% Main loop over labels (prep),0
Force the empty category to be ID 0,0
%% Main loop over labels (loop),0
iRow = 0; row = input_metadata.iloc[iRow],0
"ImageID,FileName,FilePath,SpeciesID,CommonName",0
assert os.path.isfile(full_path),0
Retrieve image width and height,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
%% See what's up with missing files,0
s = list(image_relative_paths)[0],0
s = missing_files[0],0
%% Check for images that aren't included in the metadata file,0
%% Sample the database,0
%%,0
Collect the images we want,0
%% Create info struct,0
%% Write output,0
%% Sanity-check the database's integrity,0
%% Render a bunch of images to make sure the labels got carried along correctly,0
options.classes_to_exclude = ['unlabeled'],0
%% Write out a list of files to annotate,0
,0
save_the_elephants_survey_B.py,0
,0
Convert the .csv file provided for the Save the Elephants Survey B data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
input_base = r'/mnt/blobfuse/wildlifeblobssc/ste_2019_08_drop',0
output_base = r'/home/gramener/survey_b',0
Handle all unstructured fields in the source data as extra fields in the annotations,0
"photo_type really should be an image property, but there are a few conflicts",0
that forced me to handle it as an annotation proprerty,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
"%% Map filenames to rows, verify image existence",0
Maps relative paths to row indices in input_metadata,0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
Ignore directories,0
%% Make sure the multiple-annotation cases make sense,0
%%,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
Force the empty category to be ID 0,0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image],0
Example filename:,0
,0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG',0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG',0
i_row = row_indices[0],0
timestamp = row['Date'],0
There are a small handful of datetime mismatches across annotations,0
for the same image,0
assert im['datetime'] == timestamp,0
Special cases based on the 'photo type' field,0
Various spellings of 'community',0
Have we seen this category before?,0
Create an annotation,0
fieldname = list(mapped_fields.keys())[0],0
...for each row,0
...for each image,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
%% Scrap,0
%% Find unique photo types,0
,0
ubc_to_json.py,0
,0
Convert the .csv file provided for the UBC data set to a,0
COCO-camera-traps .json file,0
,0
"Images were provided in eight folders, each of which contained a .csv",0
file with annotations.  Those annotations came in two slightly different,0
"formats, the two formats corresponding to folders starting with ""SC_"" and",0
otherwise.,0
,0
%% Constants and environment,0
Map Excel column names - which vary a little across spreadsheets - to a common set of names,0
%% Enumerate images,0
Load from file if we've already enumerated,0
%% Create CCT dictionaries,0
Force the empty category to be ID 0,0
To simplify debugging of the loop below,0
#%% Create CCT dictionaries (loop),0
#%%,0
Read source data for this folder,0
Rename columns,0
Folder name is the first two characters of the filename,0
,0
Create relative path names from the filename itself,0
Folder name is the camera name,0
,0
Create relative path names from camera name and filename,0
Which of our images are in the spreadsheet?,0
i_row = 0; fn = input_metadata['image_relative_path'][i_row],0
#%% Check for images that aren't included in the metadata file,0
Find all the images in this folder,0
Which of these aren't in the spreadsheet?,0
#%% Create entries in CCT dictionaries,0
Only process images we have on disk,0
"This is redundant, but doing this for clarity, at basically no performance",0
cost since we need to *read* the images below to check validity.,0
i_row = row_indices[0],0
"These generally represent zero-byte images in this data set, don't try",0
to find the very small handful that might be other kinds of failures we,0
might want to keep around.,0
print('Error opening image {}'.format(image_relative_path)),0
If we've seen this category before...,0
...make sure it used the same latin --> common mapping,0
,0
"If the previous instance had no mapping, use the new one.",0
assert common_name == category['common_name'],0
Create an annotation,0
...for each annotation we found for this image,0
...for each image,0
...for each dataset,0
Print all of our species mappings,0
"%% Copy images for which we actually have annotations to a new folder, lowercase everything",0
im = images[0],0
%% Create info struct,0
"%% Convert image IDs to lowercase in annotations, tag as sequence level",0
"While there isn't any sequence information, the nature of false positives",0
"here leads me to believe the images were labeled at the sequence level, so",0
we should trust labels more when positives are verified.  Overall false,0
positive rate looks to be between 1% and 5%.,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
,0
helena_to_cct.py,0
,0
Convert the Helena Detections data set to a COCO-camera-traps .json file,0
,0
%% Constants and environment,0
This is one time process,0
%% Create Filenames and timestamps mapping CSV,0
import pdb;pdb.set_trace(),0
%% To create CCT JSON for RSPB dataset,0
%% Read source data,0
Original Excel file had timestamp in different columns,0
"%% Map filenames to rows, verify image existence",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
%% Skipping this check because one image has multiple species,0
assert len(duplicate_rows) == 0,0
%% Check for images that aren't included in the metadata file,0
%% Create CCT dictionaries,0
Map categories to integer IDs,0
,0
The category '0' is reserved for 'empty',0
For each image,0
Don't include images that don't exist on disk,0
Some filenames will match to multiple rows,0
assert(len(rows) == 1),0
iRow = rows[0],0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
ann['datetime'] = row['datetime'],0
ann['site'] = row['site'],0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
%% Imports and constants,0
from github.com/microsoft/ai4eutils,0
from github.com/ecologize/CameraTraps,0
A list of files in the lilablobssc container for this data set,0
The raw detection files provided by NOAA,0
A version of the above with filename columns added,0
%% Read input .csv,0
%% Read list of files,0
%% Convert paths to full paths,0
i_row = 0; row = df.iloc[i_row],0
assert ir_image_path in all_files,0
...for each row,0
%% Write results,0
"%% Load output file, just to be sure",0
%% Render annotations on an image,0
i_image = 2004,0
%% Download the image,0
%% Find all the rows (detections) associated with this image,0
"as l,r,t,b",0
%% Render the detections on the image(s),0
In pixel coordinates,0
In pixel coordinates,0
%% Save images,0
%% Clean up,0
,0
carrizo_trail_cam_2017.py,0
,0
"Convert the .csv files provided for the ""Trail Cam Carrizo"" 2017 data set to",0
a COCO-camera-traps .json file.,0
,0
%% Constants and environment,0
%% Read source data,0
Removing the empty records,0
"%% Map filenames to rows, verify image existence",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
908 missing files (of 60562),0
%% Check for images that aren't included in the metadata file,0
105329 of 164983 files are not in metadata,0
%% Create CCT dictionaries,0
Map categories to integer IDs,0
,0
The category '0' is reserved for 'empty',0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
Each filename should just match one row,0
Don't include images that don't exist on disk,0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
,0
channel_islands_to_cct.py,0
,0
Convert the Channel Islands data set to a COCO-camera-traps .json file,0
,0
"Uses the command-line tool ExifTool (exiftool.org) to pull EXIF tags from images,",0
"because every Python package we tried failed to pull the ""Maker Notes"" field properly.",0
,0
"%% Imports, constants, paths",0
# Imports ##,0
# Constants ##,0
# Paths ##,0
Confirm that exiftool is available,0
"assert which(exiftool_command_name) is not None, 'Could not locate the ExifTool executable'",0
%% Load information from every .json file,0
"Ignore the sample file... actually, first make sure there is a sample file",0
...and now ignore that sample file.,0
json_file = json_files[0],0
ann = annotations[0],0
...for each annotation in this file,0
...for each .json file,0
"%% Confirm URL uniqueness, handle redundant tags",0
Have we already added this image?,0
"One .json file was basically duplicated, but as:",0
,0
Ellie_2016-2017 SC12.json,0
Ellie_2016-2017-SC12.json,0
"If the new image has no output, just leave the old one there",0
"If the old image has no output, and the new one has output, default to the one with output",0
Don't worry about the cases where someone tagged 'fox' and someone tagged 'fox_partial',0
...for each image we've already added,0
...if this URL is/isn't in the list of URLs we've already processed,0
...for each image,0
%% Save progress,0
%%,0
%%,0
%% Download files (functions),0
https://www.quickprogrammingtips.com/python/how-to-download-multiple-files-concurrently-in-python.html,0
"This is returned with a leading slash, remove it",0
%% Download files (execution),0
%% Read required fields from EXIF data (functions),0
"-G means ""Print group name for each tag"", e.g. print:",0
,0
[File]          Bits Per Sample                 : 8,0
,0
...instead of:,0
,0
Bits Per Sample                 : 8,0
"If we don't get any EXIF information, this probably isn't an image",0
line_raw = exif_lines[0],0
"Split on the first occurrence of "":""",0
Typically:,0
,0
"'[MakerNotes]    Sequence                        ', '1 of 3']",0
Not a typo; we are using serial number as a location,0
"If there are multiple timestamps, make sure they're *almost* the same",0
"If there are multiple timestamps, make sure they're *almost* the same",0
...for each line in the exiftool output,0
"This isn't directly related to the lack of maker notes, but it happens that files that are missing",0
maker notes also happen to be missing EXIF date information,0
...process_exif(),0
"This is returned with a leading slash, remove it",0
Ignore non-image files,0
%% Read EXIF data (execution),0
ann = images[0],0
%% Save progress,0
Use default=str to handle datetime objects,0
%%,0
%%,0
"Not deserializing datetimes yet, will do this if I actually need to run this",0
%% Check for EXIF read errors,0
%% Remove junk,0
Ignore non-image files,0
%% Fill in some None values,0
"...so we can sort by datetime later, and let None's be sorted arbitrarily",0
%% Find unique locations,0
%% Synthesize sequence information,0
Sort images by time within each location,0
i_location=0; location = locations[i_location],0
previous_datetime = sorted_images_this_location[0]['datetime'],0
im = sorted_images_this_camera[1],0
"Timestamp for this image, may be None",0
Start a new sequence if:,0
,0
* This image has no timestamp,0
* This image has a frame number of zero,0
* We have no previous image timestamp,0
,0
Start a new sequence if necessary,0
...for each image in this location,0
...for each location,0
%% Count frames in each sequence,0
images_this_sequence = [im for im in images if im['seq_id'] == seq_id],0
"%% Create output filenames for each image, store original filenames",0
i_location = 0; location = locations[i_location],0
i_image = 0; im = sorted_images_this_location[i_image],0
%% Save progress,0
Use default=str to handle datetime objects,0
%%,0
%%,0
%% Copy images to their output files (functions),0
%% Copy images to output files (execution),0
%% Rename the main image list for consistency with other scripts,0
%% Create CCT dictionaries,0
Make sure this is really a box,0
Transform to CCT format,0
Force the empty category to be ID 0,0
i_image = 0; input_im = all_image_info[0],0
"This issue only impacted one image that wasn't a real image, it was just a screenshot",0
"showing ""no images available for this camera""",0
Convert datetime if necessary,0
Process temperature if available,0
Read width and height if necessary,0
I don't know what this field is; confirming that it's always None,0
Process object and bbox,0
os.startfile(output_image_full_path),0
"Zero is hard-coded as the empty category, but check to be safe",0
"I can't figure out the 'index' field, but I'm not losing sleep about it",0
assert input_annotation['index'] == 1+i_ann,0
"Some annotators (but not all) included ""_partial"" when animals were partially obscured",0
"Annotators *mostly* used 'none', but sometimes 'empty'.  'empty' is CCT-correct.",0
If we've seen this category before...,0
If this is a new category...,0
...if this is an empty/non-empty annotation,0
Create an annotation,0
...for each annotation on this image,0
...for each image,0
%% Change *two* annotations on images that I discovered contains a human after running MDv4,0
%% Move human images,0
ann = annotations[0],0
%% Count images by location,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
viz_options.classes_to_exclude = [0],0
,0
ena24_to_json_2017.py,0
,0
Convert the ENA24 data set to a COCO-camera-traps .json file,0
,0
%% Constants and environment,0
Temporary folders for human and non-human images,0
Clean existing output folders/zipfiles,0
%% Support functions,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
Check image height and width,0
"Each row is category, [box coordinates]",0
"If there's just one row, loadtxt reads it as a 1d array; make it a 2d array",0
with one row,0
Each row is a bounding box,0
Have we seen this category before?,0
Create an annotation,0
...for each bounding box,0
"This was here for debugging; nearly every instance is Human+Horse, Human+Vehicle,",0
"or Human+Dog, but there is one Rabbit+Opossium, and a few Deer+Chicken!",0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
%% Create ZIP files for human and non human,0
%% Validate output,0
%% Preview labels,0
%% Imports and constants,0
configurations and paths,0
%% Helper functions,0
"dest_path = copy(source_path, dest_folder)",0
num_workers = multiprocessing.cpu_count(),0
pool = ThreadPool(num_workers),0
"results = pool.starmap(_copy_unzip, zip(sources, itertools.repeat(dest_folder)))",0
,0
print('Waiting for processes to finish...'),0
pool.close(),0
pool.join(),0
sequential,0
%% Command-line driver,0
if the blob container is already mounted on the VM,0
or you can download them using the storage Python SDK,0
store storage account key in environment variable AZ_STORAGE_KEY,0
,0
eMammal_helpers.py,0
,0
Support functions for processing eMammal metadata,0
,0
%% Constants and imports,0
%% Support functions,0
"pad to a total of 3 digits if < 1000, or 4 digits otherwise",0
img_frame is a string from the xml tree,0
"length 4 frame order is returned as is, others are left padded to be 3 digit long",0
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels,0
pad to a total of 4 digits,0
img_frame is a string from the xml tree,0
"length 4 frame order is returned as is, others are left padded to be 3 digit long",0
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels,0
others column,0
summer day hours: 6am - 7pm,0
others day hours: 7am - 6pm,0
,0
make_eMammal_json.py,0
,0
"Produces the COCO-formatted json database for an eMammal dataset, i.e. a",0
"collection of folders, each of which contains a deployment_manifest.xml file.",0
,0
"In this process, each image needs to be loaded to size it.",0
,0
"To add bounding box annotations to the resulting database, use",0
add_annotations_to_eMammal_json.py.,0
,0
%% Constants and imports,0
"Either add the eMammal directory to your path, or run from there",0
os.chdir(r'd:\git\CameraTraps\database_tools\eMammal'),0
import warnings,0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
warnings.filterwarnings('ignore'),0
Should we run the image size retrieval in parallel?,0
%% Support functions,0
%% Main loop (metadata processing; image sizes are retrieved later),0
deployment = folders[0],0
sequence = image_sequences[0],0
get species info for this sequence,0
add each image's info to database,0
img = images[0],0
"some manifests don't have the ImageOrder info, but the info is in the file name",0
full_img_id has no frame info,0
,0
frame number only used in requests to iMerit for ordering,0
...for each image,0
...for each sequence,0
...for each deployment,0
%% Get image sizes,0
"'tasks' is currently a list of 2-tuples, with each entry as [image dictionary,path].",0
,0
"Go through that and copy just the image dictionaries to 'db_images', adding size",0
information to each entry.  Takes a couple hours.,0
opening each image seems too fast for this multi-threaded version to be faster than sequential code.,0
%% Assemble top-level dictionaries,0
%% Write out .json,0
,0
make_full_SS_json.py,0
,0
Create a COCO-camera-traps .json file for Snapshot Serengeti data from,0
the original .csv files provided on Dryad.,0
,0
"This was used for ""version 1.0"" of the public Snapshot Serengeti archive; it's no",0
longer used as of v2.0 (early 2020).  See snapshot_serengeti_lila.py for the updated,0
Snapshot Safari preparation process.,0
,0
%% Imports and constants,0
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays",0
Count the number of images with multiple species,0
"%% Read image .csv file, format into a dictionary mapping images to capture events",0
%% Create CCT-style .json,0
still need image width and height,0
...for each image,0
%% Write output files,0
,0
make_per_season_SS_json.py,0
,0
Create a COCO-camera-traps .json file for each Snapshot Serengeti season from,0
the original .csv files provided on Dryad.,0
,0
"This was used for ""version 1.0"" of the public Snapshot Serengeti archive; it's no",0
longer used as of v2.0 (early 2020).  See snapshot_serengeti_lila.py for the updated,0
Snapshot Safari preparation process.,0
,0
%% Imports and constants,0
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays",0
"%% Read image .csv file, format into a dictionary mapping images to capture events",0
%% Create CCT-style .json,0
still need image width and height,0
...for each image ID,0
%% Write output files,0
...for each season,0
,0
generate_lila_per_image_labels.py,0
,0
"Generate a .csv file with one row per annotation, containing full URLs to every",0
"camera trap image on LILA, with taxonomically expanded labels.",0
,0
"Typically there will be one row per image, though images with multiple annotations",0
will have multiple rows.,0
,0
"Some images may not physically exist, particularly images that are labeled as ""human"".",0
This script does not validate image URLs.,0
,0
Does not include bounding box annotations.,0
,0
%% Constants and imports,0
"We'll write images, metadata downloads, and temporary files here",0
"Some datasets don't have ""sequence_level_annotation"" fields populated, but we know their",0
annotation level,0
%% Download and parse the metadata file,0
To select an individual data set for debugging,0
%% Download and extract metadata for the datasets we're interested in,0
%% Load taxonomy data,0
"%% Build a dictionary that maps each [dataset,query] pair to the full taxonomic label set",0
i_row = 0; row = taxonomy_df.iloc[i_row],0
%% Process annotations for each dataset,0
ds_name = list(metadata_table.keys())[0],0
"Go through annotations, marking each image with the categories that are present",0
,0
ann = annotations[0],0
im = images[10],0
This field name was only used for Caltech Camera Traps,0
raise ValueError('Suspicious date parsing result'),0
Special case we don't want to print a warning about,0
"Location, sequence, and image IDs are only guaranteed to be unique within",0
"a dataset, so for the output .csv file, include both",0
category_name = list(categories_this_image)[0],0
Only print a warning the first time we see an unmapped label,0
...for each category that was applied at least once to this image,0
...for each image in this dataset,0
print('Warning: no date information available for this dataset'),0
print('Warning: no location information available for this dataset'),0
...for each dataset,0
...with open(),0
%% Read the .csv back,0
%% Do some post-hoc integrity checking,0
"-1 isn't *really* valid, but we use it sometimes for sequences of unknown length",0
i_row = 0; row = df.iloc[i_row],0
%% Preview constants,0
%% Choose images to download,0
ds_name = list(metadata_table.keys())[2],0
Find all rows for this dataset,0
...for each dataset,0
%% Download images,0
i_image = 0; image = images_to_download[i_image],0
%% Write preview HTML,0
im = images_to_download[0],0
,0
Common constants and functions related to LILA data management/retrieval.,0
,0
%% Imports and constants,0
LILA camera trap master metadata file,0
"wildlife_insights_taxonomy_url = 'https://api.wildlifeinsights.org/api/v1/taxonomy?fields=class,order,family,genus,species,authority,taxonomyType,uniqueIdentifier,commonNameEnglish&page[size]={}'.format(wildlife_insights_page_size)",0
from ai4eutils,0
%% Common functions,0
"We haven't implemented paging, make sure that's not an issue",0
d['data'] is a list of items that look like:,0
Put the master metadata file in the same folder where we're putting images,0
Read lines from the master metadata file,0
Parse those lines into a table,0
Each line in this file is name/sas_url/json_url/[bbox_json_url],0
Create a separate entry for bounding boxes if they exist,0
Unzip if necessary,0
,0
get_lila_category_list.py,0
,0
Generates a .json-formatted dictionary mapping each LILA dataset to all categories,0
"that exist for that dataset, with counts for the number of occurrences of each category",0
"(the number of *annotations* for each category, not the number of *images*).",0
,0
"Also loads the taxonomy mapping file, to include scientific names for each category.",0
,0
get_lila_category_counts counts the number of *images* for each category in each dataset.,0
,0
%% Constants and imports,0
array to fill for output,0
"We'll write images, metadata downloads, and temporary files here",0
Created by get_lila_category_list.py... contains counts for each category,0
%% Load category and taxonomy files,0
%% Map dataset names and category names to scientific names,0
i_row = 1; row = taxonomy_df.iloc[i_row]; row,0
%% Download and parse the metadata file,0
%% Download and extract metadata for the datasets we're interested in,0
%% Get category names and counts for each dataset,0
ds_name = 'NACTI',0
Open the metadata file,0
Collect list of categories and mappings to category name,0
ann = annotations[0],0
c = categories[0],0
"Don't do taxonomy mapping for bbox data sets, which are sometimes just binary and are",0
always redundant with the class-level data sets.,0
"As of right now, this is the only quirky case",0
...for each dataset,0
%% Save dict,0
%% Print the results,0
ds_name = list(dataset_to_categories.keys())[0],0
...for each dataset,0
,0
download_lila_subset.py,0
,0
"Example of how to download a list of files from LILA, e.g. all the files",0
in a data set corresponding to a particular species.,0
,0
%% Constants and imports,0
LILA camera trap master metadata file,0
"In this example, we're using the Missouri Camera Traps data set and the Caltech Camera Traps dataset",0
All lower-case; we'll convert category names to lower-case when comparing,0
"We'll write images, metadata downloads, and temporary files here",0
"We will demonstrate two approaches to downloading, one that loops over files",0
"and downloads directly in Python, another that uses AzCopy.",0
,0
AzCopy will generally be more performant and supports resuming if the,0
transfers are interrupted.  This script assumes that azcopy is on the system path.,0
Number of concurrent download threads (when not using AzCopy) (AzCopy does its,0
own magical parallelism),0
%% Support functions,0
remove the leading '/',0
%% Download and parse the metadata file,0
Put the master metadata file in the same folder where we're putting images,0
Read lines from the master metadata file,0
Parse those lines into a table,0
Each line in this file is name/base_url/json_url/[box_url],0
%% Download and extract metadata for the datasets we're interested in,0
Unzip if necessary,0
...for each dataset of interest,0
%% List of files we're going to download (for all data sets),0
"Flat list or URLS, for use with direct Python downloads",0
For use with azcopy,0
This may or may not be a SAS URL,0
# Open the metadata file,0
# Build a list of image files (relative path names) that match the target species,0
Retrieve all the images that match that category,0
Retrieve image file names,0
Convert to URLs,0
...for each dataset,0
%% Download those image files,0
ds_name = 'Caltech Camera Traps',0
ds_name = 'SWG Camera Traps',0
We want to use the whole relative path for this script (relative to the base of the container),0
"to build the output filename, to make sure that different data sets end up in different folders.",0
This may or may not be a SAS URL,0
For example:,0
,0
caltech-unzipped/cct_images,0
swg-camera-traps,0
Check whether the URL includes a folder,0
E.g. caltech-unzipped,0
E.g. cct_images,0
E.g. swg-camera-traps,0
"Write out a list of files, and use the azcopy ""list-of-files"" option to download those files.",0
,0
This azcopy feature is unofficially documented at:,0
,0
https://github.com/Azure/azure-storage-azcopy/wiki/Listing-specific-files-to-transfer,0
,0
import clipboard; clipboard.copy(cmd),0
Loop over files,0
,0
get_lila_category_counts.py,0
,0
Count the number of images and bounding boxes with each label in one or more LILA datasets.,0
,0
"This script doesn't write these counts out anywhere other than the console, it's just intended",0
as a template for doing operations like this on LILA data.  get_lila_category_list.py writes,0
"information out to a .json file, but it counts *annotations*, not *images*, for each category.",0
,0
%% Constants and imports,0
"If None, will use all datasets",0
"We'll write images, metadata downloads, and temporary files here",0
%% Download and parse the metadata file,0
%% Download and extract metadata for the datasets we're interested in,0
%% Count categories,0
ds_name = datasets_of_interest[0],0
"Go through annotations, marking each image with the categories that are present",0
,0
ann = annotations[0],0
Now go through images and count categories,0
im = images[0],0
...for each dataset,0
%% Print the results,0
...for each dataset,0
,0
add_url_to_database.py,0
,0
"Adds a ""url"" field to a coco-camera-traps .json database, specifically to allow the db to",0
be reviewed in the Visipedia annotation tool.,0
,0
,0
remove_corrupted_images_from_database.py,0
,0
"Given a coco-camera-traps .json file, checks all images for TF-friendliness and generates",0
a new .json file that only contains the non-corrupted images.,0
,0
%% Imports and constants,0
import multiprocessing,0
"I leave this at an annoying low number, since by definition weird stuff will",0
"be happening in the TF kernel, and it's useful to keep having content in the console.",0
%% Function definitions,0
"I sometimes pass in a list of images, sometimes a dict with a single",0
element mapping a job ID to the list of images,0
"We're about to start a lot of TF sessions, and we don't want gobs",0
of debugging information printing out for every session.,0
At some point we were creating a single session and looping over images,0
"within that session, but the only way I found to reliably not run out",0
of GPU memory was to create a session per image and gc.collect() after,0
each session.,0
Map Image IDs to boolean (should I keep this image?),0
"Convert to lists, append job numbers to the image lists",0
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)",0
Merge results,0
%% Interactive driver,0
%%,0
base_dir = r'D:\temp\snapshot_serengeti_tfrecord_generation',0
Load annotations,0
Check for corruption,0
Write out only the uncorrupted data,0
%% Command-line driver,0
#######,0
,0
integrity_check_json_db.py,0
,0
"Does some integrity-checking and computes basic statistics on a db, specifically:",0
,0
* Verifies that required fields are present and have the right types,0
* Verifies that annotations refer to valid images,0
* Verifies that annotations refer to valid categories,0
"* Verifies that image, category, and annotation IDs are unique",0
,0
* Optionally checks file existence,0
,0
* Finds un-annotated images,0
* Finds unused categories,0
,0
* Prints a list of categories sorted by count,0
,0
#######,0
%% Constants and environment,0
%% Functions,0
"If baseDir is non-empty, checks image existence",0
This is used in a medium-hacky way to share modified options across threads,1
print('Image path {} does not exist'.format(filePath)),0
"#%% Read .json file if necessary, integrity-check fields",0
info = data['info'],0
"#%% Build dictionaries, checking ID uniqueness and internal validity as we go",0
Confirm that required fields are present,0
Confirm ID uniqueness,0
...for each category,0
image = images[0],0
Confirm that required fields are present,0
Confirm ID uniqueness,0
We previously supported ints here; this should be strings now,0
"assert isinstance(image['location'], str) or isinstance(image['location'], int), \",0
'Illegal image location type',0
Are we checking for unused images?,0
Recursively enumerate images,0
print('Image {} is unused'.format(p)),0
Are we checking file existence and/or image size?,0
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)",0
...for each image,0
Confirm that required fields are present,0
Confirm ID uniqueness,0
Confirm validity,0
...for each annotation,0
#%% Print statistics,0
Find un-annotated images and multi-annotation images,0
Find unused categories,0
Prints a list of categories sorted by count,0
,0
https://stackoverflow.com/questions/72899/how-do-i-sort-a-list-of-dictionaries-by-a-value-of-the-dictionary,0
...def integrity_check_json_db(),0
%% Command-line driver,0
%% Interactive driver(s),0
%%,0
Integrity-check .json files for LILA,0
options.iMaxNumImages = 10,0
,0
combine_two_json_files.py,0
,0
"Merges two coco-camera-traps .json files. In particular, categories are combined and re-numbered.",0
,0
Combined Info,0
Combined Images,0
Combined Categories,0
## categories to merge,0
Combined Annotations,0
,0
make_detection_db_for_viewing.py,0
,0
"Given a .json file with ground truth bounding boxes, and a .p file containing detections for the same images,",0
"creates a new .json file with separate classes for ground truth and detection, suitable for viewing in the Visipedia",0
annotation tool.,0
,0
%% Imports and constants,0
%% Main function,0
im_id_to_im = {im['id']:im for im in images},0
make new categories to distinguish between ground truth and detections,0
"update all gt annotations to be class ""gt""",0
collect all detections by image,0
keep any detection with score above det_thresh,0
"need to convert bbox from [x1,y1,x2,y2] to [x,y,w,h]",0
"add ""info"" and ""licenses"" for annotation tools to function",0
create new db,0
%% Command-line handling,0
,0
analyze_json_database.py,0
,0
Plots location/class/etc. distributions for classes in a coco-camera-traps .json file.,0
,0
Currently includes some one-off code for specific species.,0
,0
%% Constants and imports,0
%% Path configuration,0
%% Load source data,0
%% Build image/category dictionaries,0
%% Make plot of category distribution,0
%% make plots of location distribution,0
"plt.title('Number of images per location, by category')",0
"plt.tight_layout(rect=[0,0,1,0.9])",0
#make plot of images per season,0
%% Make plot of lions per location,0
%% Make plot of elephants per location,0
for loc in sorted_by_total[:25]:,0
"print('Location:' + loc[0] +', Lions: ' + str(len(loc_to_lion_ims[loc[0]])) + ', Elephants: ' + str(len(loc_to_elephant_ims[loc[0]])) + ', Total ims: ' + str(len(loc_to_ims[loc[0]])))",0
,0
subset_json_db.py,0
,0
Select a subset of images (and associated annotations) from a .json file,0
in COCO Camera Traps format.,0
,0
"To subset the .json files produced by our batch processing API, see",0
subset_json_detector_output.py,0
,0
Sample invocation:,0
,0
"python subset_json_db.py ""E:\Statewide_wolf_container\idfg_20190409.json"" ""E:\Statewide_wolf_container\idfg_20190409_clearcreek.json"" ""clearcreek"" --ignore_case",0
,0
%% Constants and imports,0
%% Functions,0
Load the input file if necessary,0
Find images matching the query,0
Find annotations referring to those images,0
Write the output file if requested,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
%% Constants and imports,0
%% Merge functions,0
i_input_dict = 0; input_dict = input_dicts[i_input_dict],0
We will prepend an index to every ID to guarantee uniqueness,0
Map detection categories from the original data set into the merged data set,0
...for each category,0
Merge original image list into the merged data set,0
Create a unique ID,0
...for each image,0
Same for annotations,0
...for each annotation,0
"Merge info dicts, don't check completion time fields",0
...for each dictionary,0
Convert merged image dictionaries to a sorted list,0
%% Driver,0
,0
add_width_and_height_to_database.py,0
,0
Grabs width and height from actual image files for a .json database that is missing w/h.,0
,0
Originally used when we created a .json file for snapshot serengeti from .csv.,0
,0
"This file converts the JSON output of the batch processing API to a pickle file, which can be used by the",0
script ./make_classification_dataset.py,0
"Convert boxes from JSON   [x_min, y_min, width_of_box, height_of_box]",0
"to PICKLE [ymin,  xmin,  ymax,         xmax]",0
Write detections to file with pickle,0
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the",0
script ./make_classification_dataset.py,0
Write detections to file with pickle,0
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the",0
script ./make_classification_dataset.py,0
Assumes that the root of the CameraTrap repo is in the PYTHONPATH,0
Minimum threshold to put a detection into the output JSON file,0
Parameter check,0
Load detections from input,0
Load COCO style annotations,0
Build output JSON in format version 1.0,0
Adding the only known metadata info,0
"The pickle file does not contain category information, so we assume the default",0
For each image with detections,0
for each detection,0
"Convert boxes from [ymin, xmin, ymax, xmax] format to",0
"[x_min, y_min, width_of_box, height_of_box]",0
Write output json,0
global flag for whether or not we encounter missing images,0
"- will only print ""missing image"" warning once",0
TFRecords variables,0
1.3 for the cropping during test time and 1.3 for the context that the,0
CNN requires in the left-over image,0
Create output directories,0
Load COCO style annotations from the input dataset,0
"Get all categories, their names, and create updated ID for the json file",0
Prepare the coco-style json files,0
Split the dataset by locations,0
Load detections,0
Write out COCO-style json files to the output directory,0
Write detections to file with pickle,0
## Preparations: get all the output tensors,0
For all images listed in the annotations file,0
Skip the image if it is annotated with more than one category,0
"Get ""old"" and ""new"" category ID and category name for this image.",0
Skip if in excluded categories.,0
get path to image,0
"If we already have detection results, we can use them",0
Otherwise run detector and add detections to the collection,0
Only select detections with confidence larger than DETECTION_THRESHOLD,0
Skip image if no detection selected,0
whether it belongs to a training or testing location,0
Skip images that we do not have available right now,0
- this is useful for processing parts of large datasets,0
Load image,0
Run inference,0
"remove batch dimension, and convert from float32 to appropriate type",0
convert normalized bbox coordinates to pixel coordinates,0
Pad the detected animal to a square box and additionally by,0
"PADDING_FACTOR. The result will be in crop_boxes. However, we need to make",0
sure that its box coordinates are still within the image.,0
"for each bounding box, crop the image to the padded box and save it",0
"Create the file path as it will appear in the annotation json,",0
adding the box number if there are multiple boxes,0
"if the cropped file already exists, verify its size",0
Add annotations to the appropriate json,0
js_keys = ['/'.join(im['file_name'].split('/')[1:])[:-4] for im in js['images']],0
for tk in js_keys:,0
"assert np.isclose(1, np.sum(detections[tk]['detection_scores'] > 0.5))",0
,0
separate_detections_by_size,0
,0
Not-super-well-maintained script to break a list of API output files up,0
based on bounding box size.,0
,0
%% Imports and constants,0
Folder with one or more .json files in it that we want to split up,0
Enumerate .json files,0
Define size thresholds and confidence thresholds,0
"Not used directly in this script, but useful if we want to generate previews",0
%% Split by size,0
For each size threshold...,0
For each file...,0
fn = input_files[0],0
Just double-checking; we already filtered this out above,0
Don't reprocess .json files we generated with this script,0
Load the input file,0
For each image...,0
1.1 is the same as infinity here; no box can be bigger than a whole image,0
What's the smallest detection above threshold?,0
"[x_min, y_min, width_of_box, height_of_box]",0
,0
size = w * h,0
...for each detection,0
Which list do we put this image on?,0
...for each image in this file,0
Make sure the number of images adds up,0
Write out all files,0
...for each size threshold,0
...for each file,0
,0
tile_images.py,0
,0
Split a folder of images into tiles.  Preserves relative folder structure in a,0
"new output folder, with a/b/c/d.jpg becoming, e.g.:",0
,0
a/b/c/d_row_0_col_0.jpg,0
a/b/c/d_row_0_col_1.jpg,0
,0
%% Imports and constants,0
from ai4eutils,0
%% Main function,0
TODO: parallelization,1
,0
i_fn = 2; relative_fn = image_relative_paths[i_fn],0
Can we skip this image because we've already generated all the tiles?,0
TODO: super-sloppy that I'm pasting this code from below,1
From:,0
,0
https://github.com/whiplashoo/split-image/blob/main/src/split_image/split.py,0
i_col = 0; i_row = 1,0
left/top/right/bottom,0
...for each row,0
...for each column,0
...for each image,0
%% Interactive driver,0
%%,0
%%,0
,0
rde_debug.py,0
,0
Some useful cells for comparing the outputs of the repeat detection,0
"elimination process, specifically to make sure that after optimizations,",0
results are the same up to ordering.,0
,0
%% Compare two RDE files,0
i_dir = 0,0
break,0
"Regardless of ordering within a directory, we should have the same",0
number of unique detections,0
Re-sort,0
Make sure that we have the same number of instances for each detection,0
Make sure the box values match,0
,0
aggregate_video.py,0
,0
Aggregate results and render output video for a video that's already been run through MD,0
,0
%% Constants,0
%% Processing,0
im = d['images'][0],0
...for each detection,0
This is no longer included in output files by default,0
# Split into frames,0
# Render output video,0
## Render detections to images,0
## Combine into a video,0
,0
ocr_sandbox.py,0
,0
sandbox for experimenting with using OCR to pull metadata from camera trap images,0
,0
The general approach is:,0
,0
"* Crop a fixed percentage from the top and bottom of an image, slightly larger",0
than the largest examples we've seen of how much space is used for metadata.,0
,0
"* Refine that crop by blurring a little, then looking for huge peaks in the",0
"color histogram suggesting a solid background, then finding rows that are",0
mostly that color.,0
,0
"* Crop to the refined crop, then run pytesseract to extract text",0
,0
"* Use regular expressions to find time and date, in the future can add, e.g.,",0
"temperature (which is often present *only* in the images, unlike time/date which",0
are also usually in EXIF but often wrong or lost in processing),0
,0
"The metadata extraction (EXIF, IPTC) here is just sample code that seemed to",0
belong in this file.,0
,0
Contact: Dan Morris (cameratraps@lila.science),0
,0
%% Constants and imports,0
pip install pytesseract,0
,0
"Also intall tesseract from: https://github.com/UB-Mannheim/tesseract/wiki, and add",0
"the installation dir to your path (on Windows, typically C:\Program Files (x86)\Tesseract-OCR)",0
pip install IPTCInfo3,0
from the ai4eutils repo: https://github.com/Microsoft/ai4eutils,0
,0
"Only used for writing out a summary, not important for the core metadata extraction",0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Using a semi-arbitrary metric of how much it feels like we found the,0
"text-containing region, discard regions that appear to be extraction failures",0
Pad each crop with a few pixels to make tesseract happy,0
Discard text from the top,0
"When we're looking for pixels that match the background color, allow some",0
tolerance around the dominant color,0
We need to see a consistent color in at least this fraction of pixels in our rough,0
crop to believe that we actually found a candidate metadata region.,0
"What fraction of the [top,bottom] of the image should we use for our rough crop?",0
A row is considered a probable metadata row if it contains at least this fraction,0
"of the background color.  This is used only to find the top and bottom of the crop area,",0
"so it's not that *every* row needs to hit this criteria, only the rows that are generally",0
above and below the text.,0
%% Support functions,0
"%% Load some images, pull EXIF and IPTC data for fun",0
%% Rough crop,0
"This will be an nImages x 1 list of 2 x 1 lists (image top, image bottom)",0
image = images[0],0
"l,t,r,b",0
,0
"0,0 is upper-left",0
"%% Close-crop around the text, return a revised image and success metric",0
Did we find a sensible mode that looks like a background value?,0
"This looks very scientific, right?  Definitely a probability?",0
"print('Failed min background fraction test: {} of {}'.format(pBackGroundValue,minBackgroundFraction))",0
"Notes to self, things I tried that didn't really go anywhere...",0
"analysisImage = cv2.blur(analysisImage, (3,3))",0
"analysisImage = cv2.medianBlur(analysisImage,5)",0
"analysisImage = cv2.Canny(analysisImage,100,100)",0
imagePil = Image.fromarray(analysisImage); imagePil,0
Use row heuristics to refine the crop,0
,0
This egregious block of code makes me miss my fluency in Matlab.,1
"print('Cropping to {},{},{},{}'.format(x,y,w,h))",0
Crop the image,0
"For some reason, tesseract doesn't like characters really close to the edge",0
imagePil = Image.fromarray(croppedImage); imagePil,0
%% Go to OCR-town,0
"An nImages x 2 list of strings, extracted from the top and bottom of each image",0
An nImages x 2 list of cropped images,0
iImage = 0; iRegion = 1; regionSet = imageRegions[iImage]; region = regionSet[iRegion],0
text = pytesseract.image_to_string(region),0
pil --> cv2,0
"image = cv2.medianBlur(image, 3)",0
"image = cv2.erode(image, None, iterations=2)",0
"image = cv2.dilate(image, None, iterations=4)",0
"image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]",0
"image = cv2.blur(image, (3,3))",0
"image = cv2.copyMakeBorder(image,10,10,10,10,cv2.BORDER_CONSTANT,value=[0,0,0])",0
"text = pytesseract.image_to_string(imagePil, lang='eng')",0
https://github.com/tesseract-ocr/tesseract/wiki/Command-Line-Usage,0
"psm 6: ""assume a single uniform block of text""",0
,0
...for each cropped region,0
...for each image,0
%% Extract dates and times,0
s = '1:22 pm',0
s = '1:23:44 pm',0
%% Write results to a handy html file,0
Add image name and resized image,0
Add results and individual region images,0
"textStyle = ""font-family:calibri,verdana,arial;font-weight:bold;font-size:150%;text-align:left;margin-left:50px;""",0
%% Scrap,0
Alternative approaches to finding the text/background  region,0
Using findCountours(),0
imagePil = Image.fromarray(analysisImage); imagePil,0
"analysisImage = cv2.erode(analysisImage, None, iterations=3)",0
"analysisImage = cv2.dilate(analysisImage, None, iterations=3)",0
"analysisImage = cv2.threshold(analysisImage, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]",0
Find object with the biggest bounding box,0
Using connectedComponents(),0
analysisImage = image,0
print('Found {} components'.format(nb_components)),0
We just want the *background* image,0
,0
"Takes a .json file with MD results for an individual video, and converts to a .csv that includes",0
frame times.  This is very bespoke to animal detection and does not include other classes.,0
,0
%% Imports and constants,0
Only necessary if you want to extract the sample rate from the video,0
%% Extract the sample rate if necessary,0
%% Load results,0
%% Convert to .csv,0
i_image = 0; im = results['images'][i_image],0
,0
umn-pr-analysis.py,0
,0
Precision/recall analysis for UMN data,0
,0
%% Imports and constants,0
results_file = results_file_filtered,0
"For two deployments, we're only processing imagse in the ""detections"" subfolder",0
String to remove from MegaDetector results,0
%% Enumerate deployment folders,0
%% Load MD results,0
im = md_results['images'][0],0
%% Load ground truth,0
i_row = 0; row = ground_truth_df.iloc[i_row],0
%% Create relative paths for ground truth data,0
"Some deployment folders have no subfolders, e.g. this is a valid file name:",0
,0
M00/01010132.JPG,0
,0
"But some deployment folders have subfolders, e.g. this is also a valid file name:",0
,0
N17/100EK113/07160020.JPG,0
,0
"So we can't find files by just concatenating folder and file names, we have to enumerate and explicitly",0
"map what will appear in the ground truth as ""folder/filename"" to complete relative paths.",0
deployment_name = list(deployment_folders)[0],0
Enumerate all files in this folder,0
"print('Enumerated {} files for deployment {}'.format(len(files),deployment_name))",0
filename = files[100],0
...for each file in this deployment,0
...for each deployment,0
%% Map relative paths to MD results,0
%% Add relative paths to our ground truth table,0
i_row = 0; row = ground_truth_df.iloc[i_row],0
"row['filename'] looks like, e.g. A01/01080001.JPG.  This is not actually a path, it's",0
"just the deployment ID and the image name, separated by a slash.",0
Find the relative path for this image,0
Make sure we have MegaDetector results for this file,0
Make sure this image file exists,0
...for each row in the ground truth table,0
%% Take everything out of Pandas,0
%% Some additional error-checking of the ground truth,0
An early version of the data required consistency between common_name and is_blank,0
%% Combine MD and ground truth results,0
d = ground_truth_dicts[0],0
Find the maximum confidence for each category,0
...for each detection,0
...for each image,0
%% Precision/recall analysis,0
...for each image,0
Thresholds go up throughout precisions/recalls/thresholds; find the last,0
value where recall is at or above target.  That's our precision @ target recall.,0
Flatten the confusion matrix,0
Write precision/recall plot to .png file in output directory,0
pr_figure_relative_filename = 'prec_recall.png',0
"pr_figure_filename = os.path.join(output_dir, pr_figure_relative_filename)",0
plt.show(block=False),0
"precision_recall_analysis(human_gt_labels,human_prediction_probs,'humans')",0
%% Find and manually review all images of humans,0
%%,0
"...if this image is annotated as ""human""",0
...for each image,0
%% Find and manually review all MegaDetector animal misses,0
%%,0
im = merged_images[0],0
GT says this is not an animal,0
GT says this is an animal,0
%% Convert .json to .csv,0
%%,0
,0
kga-pr-analysis.py,0
,0
Precision/recall analysis for KGA data,0
,0
%% Imports and constants,0
%% Load and filter MD results,0
%% Load and filter ground truth,0
%% Map images to image-level results,0
%% Map sequence IDs to images and annotations to images,0
Verify consistency of annotation within a sequence,0
TODO,1
%% Find max confidence values for each category for each sequence,0
seq_id = list(sequence_id_to_images.keys())[1000],0
im = images_this_sequence[0],0
det = md_result['detections'][],0
...for each detection,0
...for each image in this sequence,0
...for each sequence,0
%% Prepare for precision/recall analysis,0
seq_id = list(sequence_id_to_images.keys())[1000],0
cat_id = list(category_ids_this_sequence)[0],0
...for each category in this sequence,0
...for each sequence,0
%% Precision/recall analysis,0
"Confirm that thresholds are increasing, recall is decreasing",0
This is not necessarily true,0
assert np.all(precisions[:-1] <= precisions[1:]),0
Thresholds go up throughout precisions/recalls/thresholds; find the max,0
value where recall is at or above target.  That's our precision @ target recall.,0
"This is very slightly optimistic in its handling of non-monotonic recall curves,",0
but is an easy scheme to deal with.,0
Flatten the confusion matrix,0
Write precision/recall plot to .png file in output directory,0
pr_figure_relative_filename = 'prec_recall.png',0
"pr_figure_filename = os.path.join(output_dir, pr_figure_relative_filename)",0
plt.show(block=False),0
"precision_recall_analysis(human_gt_labels,human_prediction_probs,'humans')",0
%% Scrap,0
%% Find and manually review all sequence-level MegaDetector animal misses,0
input_base = '/media/user/lila-01/lila/snapshot-safari/KGA/KGA_public',0
i_sequence = 0; seq_id = pr_sequence_ids[i_sequence],0
i_seq = 0; seq_id = false_negative_sequences[i_seq],0
"sequence_folder = os.path.join(sequence_preview_dir,'seq_{}'.format(str(i_seq).zfill(3)))",0
fn = image_files[0],0
"print('Copying {} to {}'.format(input_path,output_path))",0
...for each file in this sequence.,0
...for each sequence,0
%% Image-level postprocessing,0
parse arguments,0
check if a GPU is available,0
load a pretrained embedding model,0
setup experiment,0
load the embedding model,0
setup the target dataset,0
setup finetuning criterion,0
setup an active learning environment,0
create a classifier,0
the main active learning loop,0
Active Learning,0
finetune the embedding model and load new embedding values,0
gather labeled pool and train the classifier,0
save a snapshot,0
Load a checkpoint if necessary,0
setup the training dataset and the validation dataset,0
setup data loaders,0
check if a GPU is available,0
create a model,0
setup loss criterion,0
define optimizer,0
load a checkpoint if provided,0
setup a deep learning engine and start running,0
train the model,0
train for one epoch,0
evaluate on validation set,0
save a checkpoint,0
open the file,0
read it,0
do the substitution,0
matplotlib.use('Agg'),0
from UIComponents.DBObjects import *,0
Initialize Database,0
# database connection credentials,0
# try to connect as USER to database DB_NAME through peewee,0
Load the saved embedding model,0
dataset_query = Detection.select().limit(5),0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})",0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getalllabels()) , dataset.getallpaths(), {})",0
Random examples to start,0
"random_ids = np.random.choice(dataset.current_set, 1000, replace=False).tolist()",0
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)",0
print(random_ids),0
Move Records,0
"moveRecords(dataset, DetectionKind.ModelDetection.value, DetectionKind.UserDetection.value, random_ids)",0
#print([len(x) for x in dataset.set_indices]),0
# Finetune the embedding model,0
#dataset.set_kind(DetectionKind.UserDetection.value),0
#dataset.train(),0
"#train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)",0
"#finetune_embedding(model, checkpoint['loss_type'], dataset, 32, 4, 100)",0
#save_checkpoint({,0
"#        'arch': model.arch,",0
"#        'state_dict': model.state_dict(),",0
"#        'optimizer' : optimizer.state_dict(),",0
"#        'loss_type' : loss_type,",0
"#        }, False, ""%s%s_%s_%04d.tar""%('finetuned', loss_type, model.arch, len(dataset.set_indices[DetectionKind.UserDetection.value])))",0
Get indices of samples to get user to label,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
"kwargs[""already_selected""].extend(indices)",0
Train on samples that have been labeled so far,0
Test on the samples that have not been labeled,0
"'optimizer' : optimizer.state_dict(),",0
num_classes= len(train_dataset.getClassesInfo()[0]),0
"print(""Num Classes= ""+str(num_classes))",0
define loss function (criterion) and optimizer,0
"optimizer = torch.optim.SGD(params, momentum = 0.9, lr = args.lr, weight_decay = args.weight_decay)",0
optimizer.load_state_dict(checkpoint['optimizer']),0
train for one epoch,0
"adjust_lr(optimizer,epoch)",0
if epoch % 1 == 0 and epoch > 0:,0
"a, b, c = e.predict(train_embd_loader, load_info = True, dim = args.feat_dim)",0
"plot_embedding(reduce_dimensionality(a), b, c, {})",0
evaluate on validation set,0
--------some stuff needed to get AJAX to work with bottle?--------#,0
-------------------------------------------------------------------------------- #,0
PREPARE TO QUEUE IMAGES FOR LABELING,0
-------------------------------------------------------------------------------- #,0
# Connect as USER to database DB_NAME through peewee and initialize database proxy,0
# Load embedding model,0
---------------------------------------------------------------------- #,0
CREATE QUEUE OF IMAGES TO LABEL,0
---------------------------------------------------------------------- #,0
Use classifier to generate predictions,0
# Update model predicted class in PostgreSQL database,0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?,0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
-------------------------------------------------------------------------------- #,0
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI,0
-------------------------------------------------------------------------------- #,0
"# static routes (to serve CSS, etc.)",0
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps",0
"return bottle.static_file(filename, root='../../../../../../../../../../../.')",0
# dynamic routes,0
if data['display_grayscale']:,0
indices_to_exclude.update(set(color_indices)),0
elif not data['display_grayscale']:,0
indices_to_exclude.update(set(grayscale_indices)),0
data['display_images'] = {},0
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices],0
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices],0
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices],0
data['display_images']['detection_categories'] = [],0
for i in indices:,0
if str(dataset.samples[i][1]) == 'None':,0
data['display_images']['detection_categories'].append('None'),0
else:,0
existing_category_entries = {cat.id: cat.name for cat in Category.select()},0
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()",0
data['display_images']['detection_categories'].append(cat_name),0
Use image ids in images_to_label to get the corresponding dataset indices,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
print(set(dataset.set_indices[4]).update(set(indices_to_label))),0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Use image id images_to_label to get the corresponding dataset index,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?,0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Train on samples that have been labeled so far,0
dataset.set_kind(DetectionKind.UserDetection.value),0
print(y_train),0
Predict on the samples that have not been labeled,0
print(y_pred),0
Update model predicted class in PostgreSQL database,0
timer = time.time(),0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
print('Updating the database took %0.2f seconds'%(time.time() - timer)),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids)),0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
"once the classifier has been trained the first time, switch to AL sampling",0
# sourceMappingURL=bootstrap.bundle.min.js.map,0
"print(ap_distances.size(),an_distances.size())",0
losses = -(((-ap_distances)/128)+1+1e-16).log() - (((-(128-an_distances))/128)+1+1e-16).log(),0
import pdb,0
pdb.set_trace(),0
losses = ap_distances - an_distances + self.margin,0
print(losses.size()),0
from UIComponents.DBObjects import *,0
TODO: should this also change self.kind?,1
get the embedding representations for all samples (i.e. set current_set to all indices),0
"return PILImage.open(os.path.join(self.img_base,path)).convert('RGB')",0
"print(self.labels_set, self.n_classes)",0
from sklearn.manifold import TSNE,0
embedding= TSNE(n_components=2).fit_transform(X),0
embedding= PCA(n_components=2).fit_transform(X),0
return X,0
"print(dir(event), type(sc))",0
"print(label,bgcolor)",0
"patches.append(mpatches.Patch(color=indexcolors[i], label=label))",0
"plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,",0
"ncol=12, mode=""expand"", borderaxespad=0., handles=patches)",0
plt.legend(handles=patches),0
"plt.xlabel('Dim 1', fontsize=12)",0
"plt.ylabel('Dim 2', fontsize=12)",0
plt.grid(True),0
"plt.xlabel('Dim 1', fontsize=12)",0
"plt.ylabel('Dim 2', fontsize=12)",0
plt.grid(True),0
print(thumb),0
"img.thumbnail((16, 12), PILImage.ANTIALIAS)",0
plt.show(),0
Add all negatives for all positive pairs,0
print(triplets.shape[0]),0
compute output,0
measure accuracy,0
compute loss on this batch,0
"train on a batch, record loss, and measure accuracy (if calc_accuracy)",0
compute output,0
measure accuracy and record loss,0
switch to evaluate mode,0
compute output,0
switch to evaluate mode,0
"self.fc13 = nn.Linear(128, 64)",0
self.bn2 = nn.BatchNorm1d(64),0
x = F.relu(self.fc12(x)),0
x = F.relu(self.bn1(self.fc13(x))),0
x = F.relu(self.fc13(x)),0
"x = F.dropout(x, training=self.training)",0
save features last FC layer,0
x = F.relu(x),0
save features last FC layer,0
import matplotlib.pyplot as plt,0
plt.switch_backend('agg'),0
class EmbeddingNet(nn.Module):,0
"def __init__(self, architecture, feat_dim, use_pretrained=False):",0
"super(EmbeddingNet, self).__init__()",0
self.feat_dim= feat_dim,0
self.inner_model = models.__dict__[architecture](pretrained=use_pretrained),0
if architecture.startswith('resnet'):,0
in_feats= self.inner_model.fc.in_features,0
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)",0
elif architecture.startswith('inception'):,0
in_feats= self.inner_model.fc.in_features,0
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)",0
if architecture.startswith('densenet'):,0
in_feats= self.inner_model.classifier.in_features,0
"self.inner_model.classifier = nn.Linear(in_feats, feat_dim)",0
if architecture.startswith('vgg'):,0
in_feats= self.inner_model.classifier._modules['6'].in_features,0
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)",0
if architecture.startswith('alexnet'):,0
in_feats= self.inner_model.classifier._modules['6'].in_features,0
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)",0
"def forward(self, x):",0
return self.inner_model.forward(x),0
class NormalizedEmbeddingNet(EmbeddingNet):,0
"def __init__(self, architecture, feat_dim, use_pretrained=False):",0
"EmbeddingNet.__init__(self, architecture, feat_dim, use_pretrained = use_pretrained)",0
"def forward(self, x):",0
embedding =  F.normalize(self.inner_model.forward(x))*10.0,0
"return embedding, embedding",0
"def get_random_images(num, image_dir, test_transforms):",0
"data = datasets.ImageFolder(image_dir, transform=test_transforms) # slight abuse; this expects subfolders corresponding to classes but we have no classes here",0
indices = list(range(len(data))),0
np.random.shuffle(indices),0
idx = indices[:num],0
from torch.utils.data.sampler import SubsetRandomSampler,0
sampler = SubsetRandomSampler(idx),0
"loader = torch.utils.data.DataLoader(data,",0
"sampler=sampler, batch_size=num)",0
dataiter = iter(loader),0
"images, labels = dataiter.next()",0
"return images, labels",0
"def predict_image(image, model, test_transforms):",0
"device = torch.device(""cuda"" if torch.cuda.is_available()",0
"else ""cpu"")",0
image_tensor = test_transforms(image).float(),0
image_tensor = image_tensor.unsqueeze_(0),0
input = Variable(image_tensor),0
input = input.to(device),0
output = model(input)[0],0
return output.data.cpu().numpy(),0
Connect to database and initialize db_proxy,0
# database connection credentials,0
# load the dataset,0
Load the saved embedding model from the checkpoint,0
# update the dataset embedding,0
# Create a folder for saving embedding visualizations with this model checkpoint,0
model_emb_dirname = os.path.basename(args.base_model).split('.')[0],0
"os.makedirs(model_emb_dirname, exist_ok=True)",0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')",0
dataset.embedding_mode(),0
"assert 2==3, 'break'",0
datasetindices = list(range(len(dataset))),0
np.random.shuffle(datasetindices),0
random_indices = datasetindices[:args.num],0
print(random_indices),0
"selected_sample_features = np.array([]).reshape(0, 256)",0
selected_sample_labels = [],0
for idx in random_indices:,0
"selected_sample_features = np.vstack([selected_sample_features, X_train[idx]])",0
selected_sample_labels.append(y_train[idx]),0
img_path = imagepaths[idx].split('.JPG')[0],0
image = dataset.loader(img_path),0
selected_sample_images.append(image),0
# TRY NEAREST NEIGHBORS WALK THROUGH EMBEDDING,0
nbrs = NearestNeighbors(n_neighbors=args.num).fit(selected_sample_features),0
"distances, indices = nbrs.kneighbors(selected_sample_features)",0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')",0
"idx_w_closest_nbr = np.where(distances[:,1] == min(distances[:,1]))[0][0]",0
order = [idx_w_closest_nbr],0
for ii in range(len(distances)):,0
"distances[ii, 0] = np.inf",0
while len(order)<args.num:,0
curr_idx = order[-1],0
curr_neighbors = indices[curr_idx],0
curr_dists = list(distances[curr_idx]),0
# print(min(curr_dists)),0
next_closest_pos = curr_dists.index(min(curr_dists)),0
next_closest = curr_neighbors[next_closest_pos],0
order.append(next_closest),0
# make sure you can't revisit past nodes,0
for vi in order:,0
vi_pos = list(indices[next_closest]).index(vi),0
"distances[next_closest, vi_pos] = np.inf",0
for ii in range(len(order)):,0
imgidx = order[ii],0
image = selected_sample_images[imgidx],0
"image.save(model_emb_dirname+""/img""+str(ii)+""_""+str(selected_sample_labels[imgidx])+"".png"")",0
# Specify the transformations on the input images before inference,0
"# test_transforms = transforms.Compose([transforms.Resize([224, 224]), transforms.ToTensor()])",0
"test_transforms = transforms.Compose([transforms.Resize([256, 256]), transforms.RandomCrop([224, 224]), transforms.RandomHorizontalFlip(), transforms.ColorJitter(), transforms.ToTensor(), transforms.Normalize([0.407328, 0.407328, 0.407328], [0.118641, 0.118641, 0.118641])])",0
"images, labels = get_random_images(num, image_dir, test_transforms)",0
"all_features = np.array([]).reshape(0, 256)",0
for ii in range(len(images)):,0
image = to_pil(images[ii]),0
"features = predict_image(image, model, test_transforms)",0
"all_features = np.vstack([all_features, features])",0
# for ii in range(len(images)):,0
#     image = to_pil(images[ii]),0
"#     image.save(""img""+str(ii)+"".png"")",0
# TRY CLUSTERING,0
kmeans1 = KMeans(n_clusters=5).fit(StandardScaler().fit_transform(all_features)),0
print(kmeans1.labels_),0
for ii in range(len(images)):,0
image = to_pil(images[ii]),0
"filename = str(kmeans1.labels_[ii])+""/img""+str(ii)+"".png""",0
if not os.path.exists(os.path.dirname(filename)):,0
os.makedirs(os.path.dirname(filename)),0
image.save(filename),0
class SaveFeatures():,0
"def __init__(self, module):",0
self.hook = module.register_forward_hook(self.hook_fn),0
"def hook_fn(self, module, input, output):",0
"self.features = torch.tensor(output, requires_grad=True).cuda()",0
def close(self):,0
self.hook.remove(),0
Load the saved embedding model from the checkpoint,0
"Get a sample from the database, with eval transforms applied, etc.",0
Connect to database and sample a dataset,0
output = model.forward(sample_image.unsqueeze(0)),0
print(output),0
with torch.no_grad():,0
sample_image_input = sample_image.cuda(non_blocking=True),0
"_, output = model(sample_image_input) # compute output",0
print(output),0
sample_image = PILImage.open(sample_image_path).convert('RGB'),0
"sample_image = transforms.Compose([Resize([256, 256]), CenterCrop(([[224,224]])), ToTensor(), Normalize([0.369875, 0.388726, 0.347536], [0.136821, 0.143952, 0.145229])])(sample_image)",0
print(list(model_inner_resnet.children())),0
print(model_inner_resnet.fc),0
print(model_inner_resnet.fc0),0
# print(model_inner_resnet.layer4[0].conv2),0
# print(type(model)),0
# print(len(list(model_inner_resnet.children()))),0
# print(list(model.children())),0
# print(list(list(model.children())[0].children())),0
"img = np.uint8(np.random.uniform(150, 180, (56, 56, 3)))/255",0
"img_tensor = torch.unsqueeze(torch.from_numpy(img), 0)",0
full_out = model_inner_resnet.forward(img_tensor),0
print(full_out),0
model(img_tensor),0
activations = SaveFeatures(model_inner_resnet.layer4[0].conv2),0
print(activations.features),0
print(type(activations.features)),0
activations.close(),0
Connect to database and sample a dataset,0
Load the saved embedding model from the checkpoint,0
Update the dataset embedding,0
Get a random query image,0
# # # IMAGES IN THE SAME SEQUENCE # # # #,0
"assert 2==3, 'break'",0
# # # CLOSEST IN (EMBEDDING) FEATURE SPACE # # # #,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Variables for the hierarchical cluster,0
Variables for the AL algorithm,0
Data variables,0
"connectivity = kneighbors_graph(self.transformed_X,max_features)",0
Fit cluster and update cluster variables,0
The sklearn hierarchical clustering algo numbers leaves which correspond,0
to actual datapoints 0 to n_points - 1 and all internal nodes have,0
ids greater than n_points - 1 with the root having the highest node id,0
"If no labels have been observed, simply return uniform distribution",0
"If no observations, return worst possible upper lower bounds",0
Loop through generations from bottom to top,0
Update admissible labels for node,0
Calculate score,0
Determine if node should be split,0
Make sure label set for node so that we can flow to children,0
if necessary,0
Only split if all ancestors are admissible nodes,0
This is part  of definition of admissible pruning,0
Check that pruning covers all leave nodes,0
Fill in labels,0
Observe labels for previously recommended batches,0
TODO(lishal): implement multiple selection methods,1
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
A list of initialized samplers is allowed as an input because,0
"for AL_methods that search over different mixtures, may want mixtures to",0
have shared AL_methods so that initialization is only performed once for,0
computation intensive methods like HierarchicalClusteringAL and,0
states are shared between mixtures.,0
"If initialized samplers are not provided, initialize them ourselves.",0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copy these objects to make sure they are not modified while simulating,0
trajectories as they are used later by the main run_experiment script.,0
Assumes that model has already by fit using all labeled data so,0
the probabilities can be used immediately to hallucinate labels,0
All models need to have predict_proba method,0
Hallucinate labels for selected datapoints to be label,0
using class probabilities from model,0
"Not saving already_selected here, if saving then should sort",0
only for the input to fit but preserve ordering of indices in,0
already_selected,0
Useful to know how accuracy compares for model trained on hallucinated,0
labels vs trained on true labels.  But can remove this train to speed,0
up simulations.  Won't speed up significantly since many more models,0
are being trained inside the loop above.,0
Save trajectory for reference,0
Delete created copies,0
THE INPUTS CANNOT BE MODIFIED SO WE MAKE COPIES FOR THE CHECK LATER,0
Should check model but kernel_svm does not have coef_ so need better,0
handling here,0
Make sure that model object fed in did not change during simulations,0
Return indices based on return type specified,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Set gamma for gaussian kernel to be equal to 1/n_features,0
kneighbors graph is constructed using k=10,0
"Make connectivity matrix symmetric, if a point is a k nearest neighbor of",0
"another point, make it vice versa",0
Graph edges are weighted by applying gaussian kernel to manhattan dist.,0
"By default, gamma for rbf kernel is equal to 1/n_features but may",0
get better results if gamma is tuned.,0
Define graph density for an observation to be sum of weights for all,0
edges to the node representing the datapoint.  Normalize sum weights,0
by total number of neighbors.,0
"If a neighbor has already been sampled, reduce the graph density",0
for its direct neighbors to promote diversity.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Update observed reward and arm probabilities,0
Sample an arm,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
y only used for determining how many clusters there should be,0
probably not practical to assume we know # of classes before hand,0
should also probably scale with dimensionality of data,0
Probably okay to always use MiniBatchKMeans,0
Should standardize data before clustering,0
Can cluster on standardized data but train on raw features if desired,0
TODO(lishal): have MarginSampler and this share margin function,1
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Probably okay to always use MiniBatchKMeans,0
Should standardize data before clustering,0
Can cluster on standardized data but train on raw features if desired,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Update min_distances for all examples given new cluster center.,0
Assumes that the transform function takes in original data and not,0
flattened data.,0
Initialize centers with a randomly selected datapoint,0
New examples should not be in already selected since those points,0
should have min_distance of zero to a cluster center.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
This is uniform given the remaining pool but biased wrt the entire pool.,0
sample = [i for i in range(self.X.shape[0]) if i not in already_selected],0
return sample[0:N],0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Fields for hierarchical clustering AL,0
Setting parent and storing nodes in dict for fast access,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Connect to database DB_NAME as USER and initialize tables,0
Populate Info table,0
Populate Category table,0
"# For now, we have a predefined list of species we expect to see in the camera trap database (e.g. maybe from a quick look through the images)",0
# TODO: allow user to update the class list through the labeling tool UI as they see different species,1
Populate Image and Detection tables,0
Image entry data,0
Detection entry data,0
Connect to database DB_NAME as USER and initialize tables,0
Populate Info table,0
Populate Category table,0
# Create category records from species present in the COCO camera trap classification dataset (COCO classes minus any excluded classes),0
# TODO: allow user to update the class list through the labeling tool UI as they see different species,1
Populate Image and Detection tables,0
"with open(os.path.join(args.crop_dir,'crops.json'), 'r') as infile:",0
crops_json = json.load(infile),0
counter = 0,0
timer = time.time(),0
num_detections = len(crops_json),0
for detectionid in crops_json:,0
counter += 1,0
detection_data = crops_json[detectionid],0
# Image entry data,0
existing_image_entries = Image.select().where((Image.file_name == detection_data['file_name'])),0
try:,0
existing_image_entry = existing_image_entries.get(),0
except:,0
"image_entry = Image.create(id=detectionid, file_name=detection_data['file_name'], width=detection_data['width'], height=detection_data['height'], grayscale=detection_data['grayscale'],",0
"source_file_name=detection_data['source_file_name'], relative_size=detection_data['relative_size'],",0
"seq_id=detection_data['seq_id'], seq_num_frames=detection_data['seq_num_frames'], frame_num=detection_data['frame_num'])",0
image_entry.save(),0
# Detection entry data,0
"detection_entry = Detection.create(id=detectionid, image=detectionid, bbox_confidence=detection_data['bbox_confidence'],",0
"bbox_X1=detection_data['bbox_X1'], bbox_Y1=detection_data['bbox_Y1'], bbox_X2=detection_data['bbox_X2'], bbox_Y2=detection_data['bbox_Y2'],",0
kind=DetectionKind.ModelDetection.value),0
detection_entry.save(),0
if counter%100 == 0:,0
"print('Updated database with Image and Detection table entries for %d out of %d crops in %0.2f seconds'%(counter, num_detections, time.time() - timer))",0
# data related to original image,0
Get class names from .txt list,0
Initialize Oracle table,0
Map filenames to classes (NOTE: we assume a single image does not contain more than one class),0
"For each detection, use source image path to get class",0
TODO update: Assumes that crops have already,1
been generated for the images using make_active_learning_classification_dataset.py. The created DB contains tables:,0
- info: information about the dataset,0
- image: images present in the dataset,0
- detections: crops of images with detections with confidence greater than a specified threshold,0
Initialize Database,0
# database connection credentials,0
HOST = 'localhost',0
PORT = 5432,0
"# first, make sure the (user, password) has been created",0
"# sudo -u postgres psql -c ""CREATE USER <db_user> WITH PASSWORD <db_password>;""",0
"# sudo -u postgres psql -c ""CREATE DATABASE <db_name> WITH OWNER <db_user> CONNECTION LIMIT -1;""",0
"# sudo -u postgres psql -c ""GRANT CONNECT ON DATABASE <db_name> TO <db_user>;""",0
"# sudo -u postgres psql -d <db_name> -c ""CREATE EXTENSION IF NOT EXISTS \""uuid-ossp\"";""",0
# Try to connect as USER to database DB_NAME through peewee,0
Populate Tables,0
# create Info table,0
# get class names for Category table,0
Faster anD available in Python 3.5 and above,0
# iterate through images in each class folder,0
killing this process after over 38 hours adding over 500k white-tailed deer crops from emammal,0
resuming for remaining classes,0
# get cropped image data for Image table,0
"if mean of each channel is about the same, image is likely grayscale",0
# still have no info on these:,0
seq_id = CharField(null= True)                # sequence identifier for the original image,0
seq_num_frames = IntegerField(null = True)    # number of frames in sequence,0
frame_num = IntegerField(null = True)         # which frame number in sequence,0
location = CharField(null = True)             # location of camera trap,0
datetime = DateTimeField(null = True),0
# store info about the detection corresponding to this image,0
# store info about the true labels for the detection,0
#  - for pretrain dataset this is the same as the detection_category if the detection categories,0
print(classes),0
Connect to database and sample a dataset,0
Load the saved embedding model from the checkpoint,0
Update the dataset embedding,0
save the images,0
save the features,0
"with open(os.path.join(args.output_dir, 'lastlayer_features.mat'), 'wb') as f:",0
"pickle.dump(sample_features, f)",0
"with open(os.path.join(args.output_dir, 'labels.mat'), 'wb') as f:",0
"pickle.dump(sample_labels, f)",0
"parser.add_argument('--db_name', default='missouricameratraps', type=str, help='Name of the training (target) data Postgres DB.')",0
"parser.add_argument('--db_user', default='user', type=str, help='Name of the user accessing the Postgres DB.')",0
"parser.add_argument('--db_password', default='password', type=str, help='Password of the user accessing the Postgres DB.')",0
"parser.add_argument('--base_model', type=str, help='Path to latest embedding model checkpoint.')",0
"parser.add_argument('--output_dir', type=str, help='Output directory for subset of crops')",0
Add json entry for this crop,0
Copy file for this crop to subset dataset crop dir,0
Copy file for its full-size source image to subset dataset image dir,0
Write crops.json to subset dataset crop dir,0
store info about the crops produced in a JSON file,0
------------------------------------------------------------------------------------------------------------#,0
COMMENT OUT IF NOT USING A SPECIFIC PROJECT WITHIN ROBERT LONG EMAMMAL DATASET,0
------------------------------------------------------------------------------------------------------------#,0
get some information about the source image,0
------------------------------------------------------------------------------------------------------------#,0
NOTE: EDIT THIS SECTION BASED ON DATASET SOURCE,0
get info about sequence the source image belongs to from path and directory,0
# missouricameratraps:,0
imgframenum = int(os.path.basename(imgfile).split('.JPG')[0].split('_')[-1]),0
imgseqid = int(os.path.split(os.path.dirname(imgfile))[-1]),0
"imgseqnumframes = len([name for name in os.listdir(os.path.dirname(imgfile)) if os.path.isfile(os.path.join(os.path.dirname(imgfile), name))])",0
# emammal:,0
------------------------------------------------------------------------------------------------------------#,0
utility function,0
compute output,0
measure accuracy and record loss,0
switch to train mode,0
measure accuracy and record loss,0
compute output,0
measure accuracy and record loss,0
switch to evaluate mode,0
compute output,0
switch to evaluate mode,0
compute output,0
switch to evaluate mode,0
"print(self.labels_set, self.n_classes)",0
Add all negatives for all positive pairs,0
print(triplets.shape[0]),0
constructor,0
update embedding values after a finetuning,0
select either the default or active pools,0
gather test set,0
gather train set,0
finetune the embedding model over the labeled pool,0
a utility function for saving the snapshot,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Variables for the hierarchical cluster,0
Variables for the AL algorithm,0
Data variables,0
"connectivity = kneighbors_graph(self.transformed_X,max_features)",0
Fit cluster and update cluster variables,0
The sklearn hierarchical clustering algo numbers leaves which correspond,0
to actual datapoints 0 to n_points - 1 and all internal nodes have,0
ids greater than n_points - 1 with the root having the highest node id,0
"If no labels have been observed, simply return uniform distribution",0
"If no observations, return worst possible upper lower bounds",0
Loop through generations from bottom to top,0
Update admissible labels for node,0
Calculate score,0
Determine if node should be split,0
Make sure label set for node so that we can flow to children,0
if necessary,0
Only split if all ancestors are admissible nodes,0
This is part  of definition of admissible pruning,0
Check that pruning covers all leave nodes,0
Fill in labels,0
Observe labels for previously recommended batches,0
TODO(lishal): implement multiple selection methods,1
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
A list of initialized samplers is allowed as an input because,0
"for AL_methods that search over different mixtures, may want mixtures to",0
have shared AL_methods so that initialization is only performed once for,0
computation intensive methods like HierarchicalClusteringAL and,0
states are shared between mixtures.,0
"If initialized samplers are not provided, initialize them ourselves.",0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copy these objects to make sure they are not modified while simulating,0
trajectories as they are used later by the main run_experiment script.,0
Assumes that model has already by fit using all labeled data so,0
the probabilities can be used immediately to hallucinate labels,0
All models need to have predict_proba method,0
Hallucinate labels for selected datapoints to be label,0
using class probabilities from model,0
"Not saving already_selected here, if saving then should sort",0
only for the input to fit but preserve ordering of indices in,0
already_selected,0
Useful to know how accuracy compares for model trained on hallucinated,0
labels vs trained on true labels.  But can remove this train to speed,0
up simulations.  Won't speed up significantly since many more models,0
are being trained inside the loop above.,0
Save trajectory for reference,0
Delete created copies,0
THE INPUTS CANNOT BE MODIFIED SO WE MAKE COPIES FOR THE CHECK LATER,0
Should check model but kernel_svm does not have coef_ so need better,0
handling here,0
Make sure that model object fed in did not change during simulations,0
Return indices based on return type specified,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Set gamma for gaussian kernel to be equal to 1/n_features,0
kneighbors graph is constructed using k=10,0
"Make connectivity matrix symmetric, if a point is a k nearest neighbor of",0
"another point, make it vice versa",0
Graph edges are weighted by applying gaussian kernel to manhattan dist.,0
"By default, gamma for rbf kernel is equal to 1/n_features but may",0
get better results if gamma is tuned.,0
Define graph density for an observation to be sum of weights for all,0
edges to the node representing the datapoint.  Normalize sum weights,0
by total number of neighbors.,0
"If a neighbor has already been sampled, reduce the graph density",0
for its direct neighbors to promote diversity.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Update observed reward and arm probabilities,0
Sample an arm,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
y only used for determining how many clusters there should be,0
probably not practical to assume we know # of classes before hand,0
should also probably scale with dimensionality of data,0
Probably okay to always use MiniBatchKMeans,0
Should standardize data before clustering,0
Can cluster on standardized data but train on raw features if desired,0
TODO(lishal): have MarginSampler and this share margin function,1
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Probably okay to always use MiniBatchKMeans,0
Should standardize data before clustering,0
Can cluster on standardized data but train on raw features if desired,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Update min_distances for all examples given new cluster center.,0
Assumes that the transform function takes in original data and not,0
flattened data.,0
Initialize centers with a randomly selected datapoint,0
New examples should not be in already selected since those points,0
should have min_distance of zero to a cluster center.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
This is uniform given the remaining pool but biased wrt the entire pool.,0
sample = [i for i in range(self.X.shape[0]) if i not in already_selected],0
return sample[0:N],0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Fields for hierarchical clustering AL,0
Setting parent and storing nodes in dict for fast access,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Faster and available in Python 3.5 and above,0
transform_list.append(CenterCrop((processed_size))),0
transform_list.append(Lambda(lambda X: normalize(X))),0
"print(self.labels_set, self.n_classes)",0
"from PyQt5 import QtCore, QtWidgets,QtGui",0
from collections import deque,0
from peewee import *,0
from UIComponents.Tag import Tag,0
policy.setHeightForWidth(True),0
"print(self.tab1.parentWidget(),self)",0
self.tab4.add.clicked.connect(self.addSpecies),0
self.tab4.update.clicked.connect(self.updateSpecies),0
checkpoint= load_checkpoint('../merge/triplet_model_0054.tar'),0
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))",0
"embedding_net = EmbeddingNet(checkpoint['arch'], checkpoint['feat_dim'])",0
if checkpoint['loss_type'].lower()=='center':,0
"model = torch.nn.DataParallel(ClassificationNet(embedding_net, n_classes=14)).cuda()",0
else:,0
model= torch.nn.DataParallel(embedding_net).cuda(),0
model.load_state_dict(checkpoint['state_dict']),0
self.parentWidget().progressBar.setMaximum(len(run_dataset)//2048),0
"e=Engine(model,None,None, verbose=True,progressBar= self.parentWidget().progressBar)",0
label = [ x[1] for x in run_dataset.samples],0
"print(indices,selected_set)",0
print(query.sql()),0
src.delete().where(src.image_id<<rList)),0
"det= UserDetection.create(category_id=0, id=str(index+label[1][2]),image_id=final[0], bbox_X=label[1][0], bbox_Y=label[1][1], bbox_W=label[1][2], bbox_H=label[1][3])",0
for x in self.tab1.grid.tags:,0
x.delete_instance(),0
db.create_tables([Detection]),0
This is simply to show the bar,0
"p = Process(target=ex.active, args=())",0
p.start(),0
p.join(),0
ex.active(),0
ex.centralWidget().setCurrentIndex(1),0
main(),0
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))",0
print(row),0
"unq_id= ""crops_""+str(uuid.uuid1())",0
"print(line,imageWidth,imageHeight)",0
"print(""%s,%s,%s,%0.6f,%0.6f,%0.6f,%0.6f,%0.6f""%(line[0], line[1],line[2],float(line[3]),topRel,leftRel,bottomRel,rightRel))",0
if not os.path.exists(dest):,0
os.mkdir(dest),0
raise,0
out.close(),0
"print length,(i-1)*length,i*length",0
matplotlib.use('Agg'),0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
"conf.reset()""""""",0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
conf.reset(),0
for clf in classifiers:,0
"clf.fit(X_train, y_train)",0
preds= clf.predict_proba(X),0
uncertainty+= preds.max(axis=1),0
print(uncertainty[ind]),0
print(uncertainty[ind]),0
true_labels = y[unlabeled_indices],0
select up to 5 digit examples that the classifier is most uncertain about,0
remember best acc@1 and save checkpoint,0
"completeClassificationLoop(run_dataset, model,num_classes)",0
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)",0
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd),0
labels = db.labels_,0
"mapp=(find_probablemap(label,labels, K=args.K))",0
"print(""Clusters"")",0
"for i,x in enumerate(labels):",0
labels[i]= mapp[x],0
print(np.sum(labels == label)/labels.size),0
"print(""Confidence Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning(embd, label, idx)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(""Entropy Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning_entropy(embd, label, idx)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(idx,idx.shape)",0
for i in idx:,0
print(paths[i]),0
"print(""Silohette active learning"")",0
"idx= active_learning2(embd, 1000, args.num_clusters)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
embd= reduce_dimensionality(embd)#[0:10000]),0
labels= labels[0:10000],0
label= label[0:10000],0
paths= paths[0:10000],0
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])",0
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])",0
plt.show(),0
"np.save(args.name_prefix+""_embeddings.npy"",embd)",0
"np.save(args.name_prefix+""_labels.npy"",label)",0
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")",0
del output,0
define loss function (criterion) and optimizer,0
"optimizer = torch.optim.Adam(params, lr=args.lr, weight_decay= args.weight_decay)",0
optimizer.load_state_dict(checkpoint['optimizer']),0
train for one epoch,0
evaluate on validation set,0
matplotlib.use('Agg'),0
selected_set.add(rand_ind[i]),0
"embedding_net = EmbeddingNet('resnet50', 256, True)",0
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)",0
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)",0
print('Embedding Done'),0
sys.stdout.flush(),0
Random examples to start,0
"random_ids = noveltySamples(unlabeled_dataset.em, unlabeled_dataset.getIDs(), 1000)",0
print(random_ids),0
Move Records,0
Finetune the embedding model,0
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)",0
unlabeled_dataset.updateEmbedding(model),0
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})",0
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()",0
matplotlib.use('Agg'),0
"uncertainty= np.apply_along_axis(stats.entropy,1,probs) * (1 - probs.max(axis=1))",0
"copy(paths[srt[i]], ""active"")",0
"plot_together( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()), preds, base_ind, dataset.getpaths(), {})",0
"return np.random.choice(range(0,prob_list[0].shape[0]), 100, replace=False).tolist()",0
selected_set.add(rand_ind[i]),0
"query= Detection.update(kind = destKind.value).where(Detection.id.in_(rList), Detection.kind == srcKind.value)",0
query.execute(),0
"embedding_net = EmbeddingNet('resnet50', 256, True)",0
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)",0
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)",0
print('Embedding Done'),0
sys.stdout.flush(),0
"plot_embedding(dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})",0
Random examples to start,0
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)",0
print(random_ids),0
Move Records,0
Finetune the embedding model,0
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)",0
unlabeled_dataset.updateEmbedding(model),0
"print(dataset.em[dataset.current_set].shape, np.asarray(dataset.getlabels()).shape, len(dataset.getpaths()))",0
"plot_embedding( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})",0
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})",0
train_eval_classifier(),0
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()",0
"names = [""Linear SVM"", ""RBF SVM"", ""Random Forest"", ""Neural Net"", ""Naive Bayes""]",0
"classifiers = [SVC(kernel=""linear"", C=0.025, probability= True, class_weight='balanced'),",0
"SVC(gamma=2, C=1, probability= True, class_weight='balanced'),",0
"RandomForestClassifier(max_depth=None, n_estimators=100, class_weight='balanced'),",0
"MLPClassifier(alpha=1),",0
GaussianNB()],0
estimators= [],0
"for name, clf in zip(names, classifiers):",0
"estimators.append((name, clf))",0
"eclf1 = VotingClassifier(estimators= estimators, voting='hard')",0
"eclf2 = VotingClassifier(estimators= estimators, voting='soft')",0
"names.append(""ensemble hard"")",0
classifiers.append(eclf1),0
"names.append(""ensemble soft"")",0
classifiers.append(eclf2),0
dataset.image_mode(),0
dataset.updateEmbedding(model),0
y_pred= clf.predict(X_test),0
"print(confusion_matrix(y_test, y_pred))",0
paths= dataset.getpaths(),0
"for i, (yp, yt) in enumerate(zip(y_pred, y_test)):",0
if yp != yt:,0
"copy(paths[i],""mistakes"")",0
"print(yt, yp, paths[i],i)",0
"clf_output= clf_e.embedding(eval_loader, dim=48)",0
"self.train_transform = transforms.Compose([Resize(raw_size), CenterCrop((processed_size)), ToTensor(), Normalize(mean, std)])",0
"print(self.labels_set, self.n_classes)",0
"print(line,imageWidth,imageHeight)",0
"print length,(i-1)*length,i*length",0
#########################################################,0
## Configuration,0
#########################################################,0
## The actual code,0
Check arguments,0
Create output directories,0
Padding around the detected objects when cropping,0
1.3 for the cropping during test time and 1.3 for,0
the context that the CNN requires in the left-over,0
image,0
Load a (frozen) Tensorflow model into memory.,0
## Preparations: get all the output tensors,0
The following processing is only for single image,0
Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.,0
Follow the convention by adding back the batch dimension,0
For all images in the image directoryig,0
Load image,0
Run inference,0
"all outputs are float32 numpy arrays, so convert types as appropriate",0
Add detections to the collection,0
Get info about the image,0
Select detections with a confidence larger than DETECTION_CONFIDENCE,0
Get these boxes and convert normalized coordinates to pixel coordinates,0
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes",0
"However, we need to make sure that it box coordinates are still within the image",0
"For each detected bounding box with high confidence, we will",0
crop the image to the padded box and save it,0
generate a unique identifier for the detection,0
"bbox is the detected box, crop_box the padded / enlarged box",0
Add numbering to the original file name if there are multiple boxes,0
The absolute file path where we will store the image,0
"if COCO_OUTPUT_DIR is set, then we will only use the shape",0
of cropped_img in the following code. So instead of reading,0
cropped_img = np.array(Image.open(out_file)),0
we can speed everything up by reading only the size of the image,0
matplotlib.use('Agg'),0
conf= ConfusionMatrix(24),0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
"conf.reset()""""""",0
define loss function (criterion) and optimizer,0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
conf.reset(),0
for clf in classifiers:,0
"clf.fit(X_train, y_train)",0
preds= clf.predict_proba(X),0
uncertainty+= preds.max(axis=1),0
print(uncertainty[ind]),0
print(uncertainty[ind]),0
true_labels = y[unlabeled_indices],0
select up to 5 digit examples that the classifier is most uncertain about,0
"print(indices,selected_set)",0
print(query.sql()),0
remember best acc@1 and save checkpoint,0
"completeClassificationLoop(run_dataset, model,num_classes)",0
"embd, label, paths = extract_embeddings(run_loader, model)",0
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)",0
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd),0
labels = db.labels_,0
"mapp=(find_probablemap(label,labels, K=args.K))",0
"print(""Clusters"")",0
"for i,x in enumerate(labels):",0
labels[i]= mapp[x],0
print(np.sum(labels == label)/labels.size),0
"print(""Confidence Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning(embd, label, idx)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(""Entropy Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning_entropy(embd, label, idx)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"new_selected= selectSamples(embd,paths,3000)",0
"print(idx,idx.shape)",0
for i in idx:,0
print(paths[i]),0
"print(""Silohette active learning"")",0
"idx= active_learning2(embd, 1000, args.num_clusters)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(""Random"")",0
"idx = np.random.choice(np.arange(len(paths)), 1000, replace=False)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
embd= reduce_dimensionality(embd)#[0:10000]),0
labels= labels[0:10000],0
label= label[0:10000],0
paths= paths[0:10000],0
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])",0
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])",0
plt.show(),0
"np.save(args.name_prefix+""_embeddings.npy"",embd)",0
"np.save(args.name_prefix+""_labels.npy"",label)",0
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")",0
combo.setEnabled(not finalized),0
print(self.bbox),0
self.addWidget(self.child),0
Moving container with arrows,0
Left - Bottom,0
Right-Bottom,0
Left-Top,0
Right-Top,0
Left - Bottom,0
Right - Bottom,0
Left - Top,0
Right - Top,0
check cursor horizontal position,0
check cursor vertical position,0
self.resizeEvent=self.onResize,0
"print(""Parent"", parent, parent.width(), parent.height())",0
"self.setGeometry(0,0,410,307)",0
"print(w,h,""w,h"")",0
"print(""final"",tag.getFinal())",0
pass,0
"self.tags.append(TContainer(self,Category.get(-1),[0,0,0.1,0.1],True, Qt.red))",0
label= CharField(),0
fullname=str(self.model),0
"self.name= (fullname[fullname.find("":"")+2:fullname.find("">"")].strip()+'_set').lower()",0
"print(""Parent"", self.parentWidget().width(), self.parentWidget().height())",0
"print(self.model,self.name,query.sql())",0
self.tab4.speciesList.setModel(species),0
"self.tab4.speciesList.setRowHidden(len(species.stringList())-1, True)",0
self.speciesList.itemChanged.connect(self.itemChanged),0
#####,0
,0
video_utils.py,0
,0
"Utilities for splitting, rendering, and assembling videos.",0
,0
#####,0
"%% Constants, imports, environment",0
from ai4eutils,0
%% Path utilities,0
%% Function for rendering frames to video and vice-versa,0
http://tsaith.github.io/combine-images-into-a-video-with-python-3-and-opencv-3.html,0
Determine the width and height from the first image,0
Define the codec and create VideoWriter object,0
"If we're not over-writing, check whether all frame images already exist",0
"OpenCV seems to over-report the number of frames by 1 in some cases, or fails",0
"to read the last frame; either way, I'm allowing one missing frame.",0
"print(""Rendering video {}, couldn't find frame {}"".format(",0
"input_video_file,missing_frame_number))",0
...if we need to check whether to skip this video entirely,0
"for frame_number in tqdm(range(0,n_frames)):",0
print('Skipping frame {}'.format(frame_filename)),0
Recursively enumerate video files,0
Create the target output folder,0
Render frames,0
input_video_file = input_fn_absolute; output_folder = output_folder_video,0
For each video,0
,0
input_fn_relative = input_files_relative_paths[0],0
"process_detection_with_options = partial(process_detection, options=options)",0
zero-indexed,0
Load results,0
# Break into videos,0
im = images[0],0
# For each video...,0
video_name = list(video_to_frames.keys())[0],0
frame = frames[0],0
At most one detection for each category for the whole video,0
category_id = list(detection_categories.keys())[0],0
Find the nth-highest-confidence video to choose a confidence value,0
Prepare the output representation for this video,0
'max_detection_conf' is no longer included in output files by default,0
...for each video,0
Write the output file,0
%% Test driver,0
%% Constants,0
%% Split videos into frames,0
"%% List image files, break into folders",0
Find unique folders,0
fn = frame_files[0],0
%% Load detector output,0
%% Render detector frames,0
folder = list(folders)[0],0
d = detection_results_this_folder[0],0
...for each file in this folder,0
...for each folder,0
%% Render output videos,0
folder = list(folders)[0],0
...for each video,0
,0
run_inference_with_yolov5_val.py,0
,0
Runs a folder of images through MegaDetector (or another YOLOv5 model) with YOLOv5's,0
"val.py, converting the output to the standard MD format.  The main goal is to leverage",0
YOLO's test-time augmentation tools.,0
,0
"YOLOv5's val.py uses each file's base name as a unique identifier, which doesn't work",0
when you have typical camera trap images like:,0
,0
a/b/c/RECONYX0001.JPG,0
d/e/f/RECONYX0001.JPG,0
,0
...so this script jumps through a bunch of hoops to put a symlinks in a flat,0
"folder, run YOLOv5 on that folder, and map the results back to the real files.",0
,0
"Currently requires the user to supply the path where a working YOLOv5 install lives,",0
and assumes that the current conda environment is all set up for YOLOv5.,0
,0
TODO:,1
,0
* Figure out what happens when images are corrupted... right now this is the #1,0
"reason not to use this script, it may be the case that corrupted images look the",0
same as empty images.,0
,0
* Multiple GPU support,0
,0
* Checkpointing,0
,0
* Windows support (I have no idea what all the symlink operations will do on Windows),0
,0
"* Support alternative class names at the command line (currently defaults to MD classes,",0
though other class names can be supplied programmatically),0
,0
%% Imports,0
%% Options class,0
# Required ##,0
# Optional ##,0
%% Main function,0
#%% Path handling,0
#%% Enumerate images,0
#%% Create symlinks to give a unique ID to each image,0
i_image = 0; image_fn = image_files_absolute[i_image],0
...for each image,0
#%% Create the dataset file,0
Category IDs need to be continuous integers starting at 0,0
#%% Prepare YOLOv5 command,0
#%% Run YOLOv5 command,0
#%% Convert results to MD format,0
"We'll use the absolute path as a relative path, and pass '/'",0
as the base path in this case.,0
#%% Clean up,0
...def run_inference_with_yolo_val(),0
%% Command-line driver,0
%% Scrap,0
%% Test driver (folder),0
%% Test driver (file),0
%% Preview results,0
options.sample_seed = 0,0
...for each prediction file,0
%% Compare results,0
Choose all pairwise combinations of the files in [filenames],0
"%% Constants, imports, environment",0
from multiprocessing.pool import ThreadPool as workerpool,0
Number of images to pre-fetch,0
Useful hack to force CPU inference.,1
,0
"Need to do this before any PT/TF imports, which happen when we import",0
from run_detector.,0
Numpy FutureWarnings from tensorflow import,0
%% Support functions for multiprocessing,0
TODO,1
,0
The queue system is a little more elegant if we start one thread for reading and one,1
"for processing, and this works fine on Windows, but because we import TF at module load,",0
"CUDA will only work in the main process, so currently the consumer function runs here.",0
,0
"To enable proper multi-GPU support, we may need to move the TF import to a separate module",0
that isn't loaded until very close to where inference actually happens.,0
%% Other support funtions,0
%% Image processing functions,0
%% Main function,0
Handle the case where image_file_names is not yet actually a list,0
"Find the images to score; images can be a directory, may need to recurse",0
A json list of image paths,0
A single image file,0
Load the detector,0
Does not count those already processed,0
Will not add additional entries not in the starter checkpoint,0
Write a checkpoint if necessary,0
"Back up any previous checkpoints, to protect against crashes while we're writing",0
the checkpoint file.,0
Write the new checkpoint,0
Remove the backup checkpoint if it exists,0
...if it's time to make a checkpoint,0
"When using multiprocessing, let the workers load the model",0
"Results may have been modified in place, but we also return it for",0
backwards-compatibility.,0
The typical case: we need to build the 'info' struct,0
"If the caller supplied the entire ""info"" struct",0
"The 'max_detection_conf' field used to be included by default, and it caused all kinds",0
"of headaches, so it's no longer included unless the user explicitly requests it.",0
%% Interactive driver,0
%%,0
%% Command-line driver,0
This is an experimental hack to allow the use of non-MD YOLOv5 models through,1
the same infrastructure; it disables the code that enforces MDv5-like class lists.,0
Load the checkpoint if available,0
,0
Relative file names are only output at the end; all file paths in the checkpoint are,0
still full paths.,0
"Find the images to score; images can be a directory, may need to recurse",0
A json list of image paths,0
A single image file,0
Test that we can write to the output_file's dir if checkpointing requested,0
"Don't overwrite existing checkpoint files, this is a sure-fire way to eventually",0
erase someone's checkpoint.,0
"Commenting this out for now... the scenario where we are resuming from a checkpoint,",0
then immediately overwrite that checkpoint with empty data is higher-risk than the,0
annoyance of crashing a few minutes after starting a job.,0
Confirm that we can write to the checkpoint path; this avoids issues where,0
we crash after several thousand images.,0
%% Imports,0
import pre- and post-processing functions from the YOLOv5 repo,0
scale_coords() became scale_boxes() in later YOLOv5 versions,0
%% Classes,0
padded resize,0
"Image size can be an int (which translates to a square target size) or (h,w)",0
...if the caller has specified an image size,0
NMS,0
"As of v1.13.0.dev20220824, nms is not implemented for MPS.",0
,0
Send predication back to the CPU to fix.,0
format detections/bounding boxes,0
"This is a loop over detection batches, which will always be length 1 in our case,",0
since we're not doing batch inference.,0
Rescale boxes from img_size to im0 size,0
"normalized center-x, center-y, width and height",0
"MegaDetector output format's categories start at 1, but the MD",0
model's categories start at 0.,0
...for each detection in this batch,0
...if this is a non-empty batch,0
...for each detection batch,0
...try,0
for testing,0
"MegaDetector was trained with batch size of 1, and the resizing function is a part",0
of the inference graph,0
"change from [y1, x1, y2, x2] to [x1, y1, width, height]",0
convert numpy floats to Python floats,0
need to change the above line to the following if supporting a batch size > 1 and resizing to the same size,0
"np_images = [np.asarray(image, np.uint8) for image in images]",0
"images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)",0
performs inference,0
our batch size is 1; need to loop the batch dim if supporting batch size > 1,0
"%% Constants, imports, environment",0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Numpy FutureWarnings from tensorflow import,0
Useful hack to force CPU inference,1
os.environ['CUDA_VISIBLE_DEVICES'] = '-1',0
An enumeration of failure reasons,0
Number of decimal places to round to for confidence and bbox coordinates,0
Label mapping for MegaDetector,0
Should we allow classes that don't look anything like the MegaDetector classes?,0
"Each version of the detector is associated with some ""typical"" values",0
"that are included in output files, so that downstream applications can",0
use them as defaults.,0
%% Classes,0
Stick this into filenames before the extension for the rendered result,0
%% Utility functions,0
mps backend only available in torch >= 1.12.0,0
%% Main function,0
Dictionary mapping output file names to a collision-avoidance count.,0
,0
"Since we'll be writing a bunch of files to the same folder, we rename",0
as necessary to avoid collisions.,0
...def input_file_to_detection_file(),0
Image is modified in place,0
...for each image,0
...def load_and_run_detector(),0
%% Command-line driver,0
Must specify either an image file or a directory,0
"but for a single image, args.image_dir is also None",0
%% Interactive driver,0
%%,0
#####,0
,0
process_video.py,0
,0
"Split a video (or folder of videos) into frames, run the frames through run_detector_batch.py,",0
and optionally stitch together results into a new video with detection boxes.,0
,0
TODO: allow video rendering when processing a whole folder,1
TODO: allow video rendering from existing results,1
,0
#####,0
"%% Constants, imports, environment",0
Only relevant if render_output_video is True,0
Folder to use for extracted frames,0
Folder to use for rendered frames (if rendering output video),0
Should we render a video with detection boxes?,0
,0
"Only supported when processing a single video, not a folder.",0
"If we are rendering boxes to a new video, should we keep the temporary",0
rendered frames?,0
Should we keep the extracted frames?,0
%% Main functions,0
"TODO: keep track of whether we created this folder, delete if we're deleting the extracted",0
"frames and we created the folder, and the output files aren't in the same folder.  For now,",0
we're just deleting the extracted frames and leaving the empty folder around in this case.,0
Render detections to images,0
"TODO: keep track of whether we created this folder, delete if we're deleting the rendered",0
"frames and we created the folder, and the output files aren't in the same folder.  For now,",0
we're just deleting the rendered frames and leaving the empty folder around in this case.,0
Combine into a video,0
Delete the temporary directory we used for detection images,0
shutil.rmtree(rendering_output_dir),0
(Optionally) delete the frames on which we ran MegaDetector,0
shutil.rmtree(frame_output_folder),0
...process_video(),0
# Validate options,0
# Split every video into frames,0
# Run MegaDetector on the extracted frames,0
# (Optionally) delete the frames on which we ran MegaDetector,0
shutil.rmtree(frame_output_folder),0
# Convert frame-level results to video-level results,0
...process_video_folder(),0
%% Interactive driver,0
%% Process a folder of videos,0
import clipboard; clipboard.copy(cmd),0
%% Process a single video,0
import clipboard; clipboard.copy(cmd),0
"%% Render a folder of videos, one file at a time",0
import clipboard; clipboard.copy(s),0
%% Command-line driver,0
Lint as: python3,0
Copyright 2020 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TPU is automatically inferred if tpu_name is None and,0
we are running under cloud ai-platform.,0
,0
copy_checkpoints.py,0
,0
Run this script with specified source_dir and target_dir while the model is training to make a copy,0
of every checkpoint (checkpoints are kept once an hour by default and is difficult to adjust),0
,0
do not copy event or evaluation results,0
input validation,0
plot the images,0
adjust the figure,0
"read in dataset CSV and create merged (dataset, location) col",0
map label to label_index,0
load the splits,0
only weight the training set by detection confidence,0
TODO: consider weighting val and test set as well,1
isotonic regression calibration of MegaDetector confidence,0
treat each split separately,0
new_weight[i] = confidence[i] * (n / c) / total_confidence(i's label),0
- n = # examples in split (weighted by confidence); c = # labels,0
- weight allocated to each label is n/c,0
"- within each label, weigh each example proportional to confidence",0
- new weights sum to n,0
error checking,0
"maps output label name to set of (dataset, dataset_label) tuples",0
find which other label (label_b) has intersection,0
input validation,0
create label index JSON,0
look into sklearn.preprocessing.MultiLabelBinarizer,0
Note: JSON always saves keys as strings!,0
"True for ground truth, False for MegaDetector",0
always save as .jpg for consistency,0
get bounding boxes,0
convert from category ID to category name,0
"check if crops are already downloaded, and ignore bboxes below the",0
confidence threshold,0
assign all images without location info to 'unknown_location',0
remove images from labels that have fewer than min_locs locations,0
merge dataset and location into a single string '<dataset>/<location>',0
"create DataFrame of counts. rows = locations, columns = labels",0
label_count: label => number of examples,0
loc_count: label => number of locs containing that label,0
generate a new split,0
score the split,0
SSE for # of images per label (with 2x weight),0
SSE for # of locs per label,0
label => list of datasets to prioritize for test and validation sets,0
"merge dataset and location into a tuple (dataset, location)",0
sorted smallest to largest,0
greedily add to test set until it has >= 15% of images,0
sort the resulting locs,0
"modify loc_to_size in place, so copy its keys before iterating",0
arguments relevant to both creating the dataset CSV and splits.json,0
arguments only relevant for creating the dataset CSV,0
arguments only relevant for creating the splits JSON,0
comment lines starting with '#' are allowed,0
,0
prepare_classification_script.py,0
,0
Notebook-y script used to prepare a series of shell commands to run a classifier,0
(other than MegaClassifier) on a MegaDetector result set.,0
,0
Differs from prepare_classification_script_mc.py only in the final class mapping step.,0
,0
%% Job options,0
%% Constants,0
"This is just passed along to the metadata in the output file, it has no impact",0
on how the classification scripts run.,0
%% Set up environment,0
commands.append('cd CameraTraps/classification\n'),0
commands.append('conda activate cameratraps-classifier\n'),0
%% Crop images,0
fn = input_files[0],0
%% Run classifier,0
fn = input_files[0],0
%% Merge classification and detection outputs,0
fn = input_files[0],0
%% Write everything out,0
input validation,0
"Evaluating with accimage is much faster than Pillow or Pillow-SIMD, but accimage",0
is Linux-only.,0
create output directory,0
override saved params with kwargs,0
"For now, we don't weight crops by detection confidence during",0
evaluation. But consider changing this.,0
"create model, compile with TorchScript if given checkpoint is not compiled",0
"verify that target names matches original ""label names"" from dataset",0
"if the dataset does not already have a 'other' category, then the",0
'other' category must come last in label_names to avoid conflicting,0
with an existing label_id,0
define loss function (criterion),0
"this file ends up being huge, so we GZIP compress it",0
double check that the accuracy metrics are computed properly,0
save the confusion matrices to .npz,0
save per-label statistics,0
set dropout and BN layers to eval mode,0
"even if batch contains sample weights, don't use them",0
Do target mapping on the outputs (unnormalized logits) instead of,0
"the normalized (softmax) probabilities, because the loss function",0
uses unnormalized logits. Summing probabilities is equivalent to,0
log-sum-exp of unnormalized logits.,0
"a confusion matrix C is such that C[i,j] is the # of observations known to",0
be in group i and predicted to be in group j.,0
match pytorch EfficientNet model names,0
images dataset,0
"for smaller disk / memory usage, we cache the raw JPEG bytes instead",0
of the decoded Tensor,0
convert JPEG bytes to a 3D uint8 Tensor,0
"keras EfficientNet already includes normalization from [0, 255] to [0, 1],",0
so we don't need to do that here,0
labels dataset,0
img_files dataset,0
weights dataset,0
define the transforms,0
efficientnet data preprocessing:,0
- train:,0
"1) random crop: aspect_ratio_range=(0.75, 1.33), area_range=(0.08, 1.0)",0
2) bicubic resize to img_size,0
3) random horizontal flip,0
- test:,0
1) center crop,0
2) bicubic resize to img_size,0
weights sums to:,0
- if weight_by_detection_conf: (# images in split - conf delta),0
- otherwise: (# images in split),0
"freeze the base model's weights, including BatchNorm statistics",0
https://www.tensorflow.org/guide/keras/transfer_learning#fine-tuning,0
rebuild output,0
input validation,0
set seed,0
create logdir and save params,0
create dataloaders and log the index_to_label mapping,0
define loss function and optimizer,0
using EfficientNet training defaults,0
- batch norm momentum: 0.99,0
"- optimizer: RMSProp, decay 0.9 and momentum 0.9",0
- epochs: 350,0
"- learning rate: 0.256, decays by 0.97 every 2.4 epochs",0
- weight decay: 1e-5,0
TODO: change weighted to False if oversampling minority classes,1
stop training after 8 epochs without improvement,0
log metrics,0
log confusion matrix,0
log tp/fp/fn images,0
"tf.summary.image requires input of shape [N, H, W, C]",0
false positive for top3_pred[0],0
false negative for label,0
"if evaluating or finetuning, set dropout & BN layers to eval mode",0
"for each label, track 5 most-confident and least-confident examples",0
"even if batch contains sample weights, don't use them",0
we do not track L2-regularization loss in the loss metric,0
This dictionary will get written out at the end of this process; store,0
diagnostic variables here,0
error checking,0
refresh detection cache,0
save log of bad images,0
cache of Detector outputs: dataset name => {img_path => detection_dict},0
img_path: <dataset-name>/<img-filename>,0
get SAS URL for images container,0
strip image paths of dataset name,0
save list of dataset names and task IDs for resuming,0
complete task name: 'detect_for_classifier_caltech_20200722_110816_task01',0
HACK! Sleep for 10s between task submissions in the hopes that it,1
"decreases the chance of backend JSON ""database"" corruption",0
task still running => continue,0
"task finished successfully, save response to disk",0
error checking before we download and crop any images,0
convert from category ID to category name,0
we need the datasets table for getting SAS keys,0
"we already did all error checking above, so we don't do any here",0
get ContainerClient,0
get bounding boxes,0
we must include the dataset <ds> in <crop_path_template> because,0
'{img_path}' actually gets populated with <img_file> in,0
load_and_crop(),0
"get the image, either from disk or from Blob Storage",0
inelegant way to close the container_clients,0
,0
prepare_classification_script_mc.py,0
,0
Notebook-y script used to prepare a series of shell commands to run MegaClassifier,0
on a MegaDetector result set.,0
,0
Differs from prepare_classification_script.py only in the final class mapping step.,0
,0
%% Job options,0
%% Constants,0
"This is just passed along to the metadata in the output file, it has no impact",0
on how the classification scripts run.,0
%% Set up environment,0
commands.append('cd CameraTraps/classification\n'),0
commands.append('conda activate cameratraps-classifier\n'),0
%% Crop images,0
fn = input_files[0],0
%% Run classifier,0
fn = input_files[0],0
%% Remap classifier outputs,0
fn = input_files[0],0
%% Merge classification and detection outputs,0
fn = input_files[0],0
%% Write everything out,0
mean/std values from https://pytorch.org/docs/stable/torchvision/models.html,0
define the transforms,0
resizes smaller edge to img_size,0
weights sums to:,0
- if weight_by_detection_conf: (# images in split - conf delta),0
- otherwise: # images in split,0
for normal (non-weighted) shuffling,0
set all parameters to not require gradients except final FC layer,0
replace final fully-connected layer (which has 1000 ImageNet classes),0
"detect GPU, use all if available",0
input validation,0
set seed,0
create logdir and save params,0
create dataloaders and log the index_to_label mapping,0
create model,0
define loss function and optimizer,0
using EfficientNet training defaults,0
- batch norm momentum: 0.99,0
"- optimizer: RMSProp, decay 0.9 and momentum 0.9",0
- epochs: 350,0
"- learning rate: 0.256, decays by 0.97 every 2.4 epochs",0
- weight decay: 1e-5,0
stop training after 8 epochs without improvement,0
do a complete evaluation run,0
log metrics,0
log confusion matrix,0
log tp/fp/fn images,0
"for every image: undo normalization, clamp to [0, 1], CHW -> HWC",0
"- cannot be in-place, because the HeapItem might be in multiple heaps",0
writer.add_figure() has issues => using add_image() instead,0
"writer.add_figure(f'{label_name}/{tag}', fig, global_step=epoch)",0
false positive for top3_pred[0],0
false negative for label,0
"preds and labels both have shape [N, k]",0
"if evaluating or finetuning, set dropout and BN layers to eval mode",0
"for each label, track k_extreme most-confident and least-confident images",0
"even if batch contains sample weights, don't use them",0
lock before changing ImageFile.LOAD_TRUNCATED_IMAGES,0
input validation,0
use MegaDB to generate list of images,0
only keep images that:,0
"1) end in a supported file extension, and",0
2) actually exist in Azure Blob Storage,0
3) belong to a label with at least min_locs locations,0
write out log of images / labels that were removed,0
"save label counts, pre-subsampling",0
"save label counts, post-subsampling",0
spec_dict['taxa']: list of dict,0
[,0
"{'level': 'family', 'name': 'cervidae', 'datasets': ['idfg']},",0
"{'level': 'genus',  'name': 'meleagris'}",0
],0
"spec_dict['dataset_labels']: dict, dataset => list of dataset_label",0
{,0
"""idfg"": [""deer"", ""elk"", ""prong""],",0
"""idfg_swwlf_2019"": [""elk"", ""muledeer"", ""whitetaileddeer""]",0
},0
"maps output label name to set of (dataset, dataset_label) tuples",0
"because MegaDB is organized by dataset, we do the same",0
ds_to_labels = {,0
'dataset_name': {,0
"'dataset_label': [output_label1, output_label2]",0
},0
},0
we need the datasets table for getting full image paths,0
The line,0
"[img.class[0], seq.class[0]][0] as class",0
selects the image-level class label if available. Otherwise it selects the,0
"sequence-level class label. This line assumes the following conditions,",0
expressed in the WHERE clause:,0
- at least one of the image or sequence class label is given,0
- the image and sequence class labels are arrays with length at most 1,0
- the image class label takes priority over the sequence class label,0
,0
"In Azure Cosmos DB, if a field is not defined, then it is simply excluded",0
"from the result. For example, on the following JSON object,",0
{,0
"""dataset"": ""camera_traps"",",0
"""seq_id"": ""1234"",",0
"""location"": ""A1"",",0
"""images"": [{""file"": ""abcd.jpeg""}],",0
"""class"": [""deer""],",0
},0
"the array [img.class[0], seq.class[0]] just gives ['deer'] because",0
img.class is undefined and therefore excluded.,0
"if no path prefix, set it to the empty string '', because",0
"os.path.join('', x, '') = '{x}/'",0
result keys,0
"- already has: ['dataset', 'location', 'file', 'class', 'bbox']",0
"- add ['label'], remove ['file']",0
"if img is mislabeled, but we don't know the correct class, skip it",0
"otherwise, update the img with the correct class, but skip the",0
img if the correct class is not one we queried for,0
sort keys for determinism,0
we need the datasets table for getting SAS keys,0
strip leading '?' from SAS token,0
only check Azure Blob Storage,0
check local directory first before checking Azure Blob Storage,0
1st pass: populate label_to_locs,0
"label (tuple of str) => set of (dataset, location)",0
2nd pass: eliminate bad images,0
prioritize is a list of prioritization levels,0
number of already matching images,0
main(,0
"label_spec_json_path='idfg_classes.json',",0
"taxonomy_csv_path='../../camera-traps-private/camera_trap_taxonomy_mapping.csv',",0
"output_dir='run_idfg',",0
json_indent=4),0
recursively find all files in cropped_images_dir,0
only find crops of images from detections JSON,0
resizes smaller edge to img_size,0
"Evaluating with accimage is much faster than Pillow or Pillow-SIMD, but accimage",0
is Linux-only.,0
create dataset,0
create model,0
set dropout and BN layers to eval mode,0
load files,0
dataset => set of img_file,0
crop_path: <dataset>/<img_file>___cropXX_mdvY.Y.jpg,0
[----<img_path>----],0
error checking,0
any row with 'correct_class' should be marked 'mislabeled',0
filter to only the mislabeled rows,0
convert '\' to '/',0
verify that overlapping indices are the same,0
"""add"" any new mislabelings",0
write out results,0
error checking,0
load detections JSON,0
get detector version,0
convert from category ID to category name,0
copy keys to modify dict in-place,0
This will be removed later when we filter for animals,0
save log of bad images,0
"True for ground truth, False for MegaDetector",0
always save as .jpg for consistency,0
"we already did all error checking above, so we don't do any here",0
"get the image, either from disk or from Blob Storage",0
inelegant way to close the container_client,0
"crop_path => normalized bbox coordinates [xmin, ymin, width, height]",0
"only ground-truth bboxes do not have a ""confidence"" value",0
try loading image from local directory,0
try to download image from Blob Storage,0
crop the image,0
"expand box width or height to be square, but limit to img size",0
"Image.crop() takes box=[left, upper, right, lower]",0
pad to square using 0s,0
Author: lukemelas (github username),0
Github repo: https://github.com/lukemelas/EfficientNet-PyTorch,0
With adjustments and added comments by workingcoder (github username).,0
Support the construction of 'efficientnet-l2' without pretrained weights,0
Expansion phase (Inverted Bottleneck),0
"image_size = calculate_output_image_size(image_size, 1) <-- this wouldn't modify image_size",0
Depthwise convolution phase,0
"Squeeze and Excitation layer, if desired",0
Pointwise convolution phase,0
Expansion and Depthwise Convolution,0
Squeeze and Excitation,0
Pointwise Convolution,0
Skip connection and drop connect,0
The combination of skip connection and drop connect brings about stochastic depth.,0
Batch norm parameters,0
Get stem static or dynamic convolution depending on image size,0
Stem,0
Build blocks,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
"image_size = calculate_output_image_size(image_size, block_args.stride)  # stride = 1",0
Head,0
Final linear layer,0
Stem,0
Blocks,0
Head,0
Stem,0
Blocks,0
Head,0
Convolution layers,0
Pooling and final linear layer,0
Author: lukemelas (github username),0
Github repo: https://github.com/lukemelas/EfficientNet-PyTorch,0
With adjustments and added comments by workingcoder (github username).,0
###############################################################################,0
## Help functions for model architecture,0
###############################################################################,0
GlobalParams and BlockArgs: Two namedtuples,0
Swish and MemoryEfficientSwish: Two implementations of the method,0
round_filters and round_repeats:,0
Functions to calculate params for scaling model width and depth ! ! !,0
get_width_and_height_from_size and calculate_output_image_size,0
drop_connect: A structural design,0
get_same_padding_conv2d:,0
Conv2dDynamicSamePadding,0
Conv2dStaticSamePadding,0
get_same_padding_maxPool2d:,0
MaxPool2dDynamicSamePadding,0
MaxPool2dStaticSamePadding,0
"It's an additional function, not used in EfficientNet,",0
but can be used in other model (such as EfficientDet).,0
"Parameters for the entire model (stem, all blocks, and head)",0
Parameters for an individual model block,0
Set GlobalParams and BlockArgs's defaults,0
An ordinary implementation of Swish function,0
A memory-efficient implementation of Swish function,0
TODO: modify the params names.,1
"maybe the names (width_divisor,min_width)",0
"are more suitable than (depth_divisor,min_depth).",0
follow the formula transferred from official TensorFlow implementation,0
follow the formula transferred from official TensorFlow implementation,0
"generate binary_tensor mask according to probability (p for 0, 1-p for 1)",0
Note:,0
The following 'SamePadding' functions make output size equal ceil(input size/stride).,0
"Only when stride equals 1, can the output size be the same as input size.",0
Don't be confused by their function names ! ! !,0
Tips for 'SAME' mode padding.,0
Given the following:,0
i: width or height,0
s: stride,0
k: kernel size,0
d: dilation,0
p: padding,0
Output after Conv2d:,0
o = floor((i+p-((k-1)*d+1))/s+1),0
"If o equals i, i = floor((i+p-((k-1)*d+1))/s+1),",0
=> p = (i-1)*s+((k-1)*d+1)-i,0
With the same calculation as Conv2dDynamicSamePadding,0
Calculate padding based on image size and save it,0
Calculate padding based on image size and save it,0
###############################################################################,0
## Helper functions for loading model params,0
###############################################################################,0
BlockDecoder: A Class for encoding and decoding BlockArgs,0
efficientnet_params: A function to query compound coefficient,0
get_model_params and efficientnet:,0
Functions to get BlockArgs and GlobalParams for efficientnet,0
url_map and url_map_advprop: Dicts of url_map for pretrained weights,0
load_pretrained_weights: A function to load pretrained weights,0
Check stride,0
"Coefficients:   width,depth,res,dropout",0
Blocks args for the whole model(efficientnet-b0 by default),0
It will be modified in the construction of EfficientNet Class according to model,0
note: all models have drop connect rate = 0.2,0
ValueError will be raised here if override_params has fields not included in global_params.,0
train with Standard methods,0
check more details in paper(EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks),0
train with Adversarial Examples(AdvProp),0
check more details in paper(Adversarial Examples Improve Image Recognition),0
TODO: add the petrained weights url map of 'efficientnet-l2',1
AutoAugment or Advprop (different preprocessing),0
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils),0
from data_management.megadb.schema import sequences_schema_check,0
resize is for displaying them more quickly,0
dataset and seq_id are required fields,0
sort the images in the sequence,0
"if no class label on the image, show class label on the sequence",0
pool = ThreadPool(),0
"print('len of rendering_info', len(rendering_info))",0
"tqdm(pool.imap_unordered(render_image_info_partial, rendering_info), total=len(rendering_info))",0
options = write_html_image_list(),0
options['headerHtml'],0
print('Checking that the MegaDB entries conform to the schema...'),0
sequences_schema_check.sequences_schema_check(sequences),0
#######,0
,0
visualize_db.py,0
,0
Outputs an HTML page visualizing annotations (class labels and/or bounding boxes),0
on a sample of images in a database in the COCO Camera Traps format,0
,0
#######,0
%% Imports,0
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils),0
Assumes the cameratraps repo root is on the path,0
%% Settings,0
Set to None to visualize all images,0
Target size for rendering; set either dimension to -1 to preserve aspect ratio,0
"These are mutually exclusive; both are category names, not IDs",0
"Special tag used to say ""show me all images with multiple categories""",0
We sometimes flatten image directories by replacing a path separator with,0
another character.  Leave blank for the typical case where this isn't necessary.,0
Control rendering parallelization,0
Process-based parallelization in this function is currently unsupported,0
"due to pickling issues I didn't care to look at, but I'm going to just",0
"flip this with a warning, since I intend to support it in the future.",0
%% Helper functions,0
"Translate the file name in an image entry in the json database to a path, possibly doing",0
some manipulation of path separators,0
%% Core functions,0
"Optionally remove all images without bounding boxes, *before* sampling",0
"Optionally include/remove images with specific labels, *before* sampling",0
Put the annotations in a dataframe so we can select all annotations for a given image,0
Construct label map,0
Take a sample of images,0
Set of dicts representing inputs to render_db_bounding_boxes:,0
,0
"bboxes, boxClasses, image_path",0
All the class labels we've seen for this image (with out without bboxes),0
Iterate over annotations for this image,0
iAnn = 0; anno = annos_i.iloc[iAnn],0
...for each of this image's annotations,0
"We're adding html for an image before we render it, so it's possible this image will",0
fail to render.  For applications where this script is being used to debua a database,0
"(the common case?), this is useful behavior, for other applications, this is annoying.",0
...for each image,0
...def render_image_info,0
def process_images(...),0
%% Command-line driver,0
"Copy all fields from a Namespace (i.e., the output from parse_args) to an object.",0
,0
Skips fields starting with _.  Does not check existence in the target object.,0
Convert to an options object,0
%% Interactive driver(s),0
%%,0
os.startfile(htmlOutputFile),0
%% Constants and imports,0
convert category ID from int to str,0
Retry on blob storage read failures,0
%% Functions,0
PIL.Image.convert() returns a converted copy of this image,0
Alter orientation as needed according to EXIF tag 0x112 (274) for Orientation,0
,0
https://gist.github.com/dangtrinhnt/a577ece4cbe5364aad28,0
https://www.media.mit.edu/pia/Research/deepview/exif.html,0
,0
Null operation,0
Aspect ratio as width over height,0
ar = w / h,0
h = w / ar,0
w = ar * h,0
The following three functions are modified versions of those at:,0
,0
https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py,0
Convert to pixels so we can use the PIL crop() function,0
PIL's crop() does surprising things if you provide values outside of,0
"the image, clip inputs",0
...if this detection is above threshold,0
...for each detection,0
"list of lists, one list of strings for each bounding box (to accommodate multiple labels)",0
for color selection,0
"Always render objects with a confidence of ""None"", this is typically used",0
for ground truth data.,0
"{} is the default, which means ""show labels with no mapping"", so don't use ""if label_map"" here",0
if label_map:,0
"To avoid duplicate colors with detection-only visualization, offset",0
the classification class index by the number of detection classes,0
...for each classification,0
...if we have classification results,0
...if the confidence of this detection is above threshold,0
...for each detection,0
"print('Input must be of size [N, 4], but is ' + str(boxes_shape))",0
Deliberately trimming to the width of the image only in the case where,0
"box expansion is turned on.  There's not an obvious correct behavior here,",0
but the thinking is that if the caller provided an out-of-range bounding,0
"box, they meant to do that, but at least in the eyes of the person writing",0
"this comment, if you expand a box for visualization reasons, you don't want",0
to end up with part of a box.,0
,0
A slightly more sophisticated might check whether it was in fact the expansion,0
"that made this box larger than the image, but this is the case 99.999% of the time",0
"here, so that doesn't seem necessary.",1
...if we need to expand boxes,0
If the total height of the display strings added to the top of the bounding,0
"box exceeds the top of the image, stack the strings below the bounding box",0
instead of above.,0
Each display_str has a top and bottom margin of 0.05x.,0
Reverse list and print from bottom to top.,0
Skip empty strings,0
"list of lists, one list of strings for each bounding box (to accommodate multiple labels)",0
need to be a string here because PIL needs to iterate through chars,0
,0
stacked bar charts are made with each segment starting from a y position,0
"To fit the legend in, shrink current axis by 20%",0
Put a legend to the right of the current axis,0
####,0
,0
visualize_incoming_annotations.py,0
,0
Spot-check the annotations received from iMerit by visualizing annotated bounding,0
boxes on a sample of images and display them in HTML.,0
,0
"Modified in 2021 March to use the new format (iMerit batch 12 onwards), which is a",0
COCO formatted JSON with relative coordinates for the bbox.,0
,0
from data_management.megadb.schema import sequences_schema_check,0
we used frame_num of 1 when sending out images to annotators when it is not explicitly stored (wcs esp.),0
The file_name field in the incoming json looks like,0
alka_squirrels.seq2020_05_07_25C.frame119221.jpg,0
"we need to use the dataset, sequence and frame info to find the actual path in blob storage",0
using the sequences,0
category_id 5 is No Object Visible,0
download the image,0
Write to HTML,0
allow forward references in typing annotations,0
class variables,0
instance variables,0
get path to root,0
taxonomy CSV rows without 'taxonomy_string' entries are excluded,0
"from the taxonomy graph, but can be included in a classification",0
label specification JSON via the 'dataset_labels' key,0
use the lower parent,0
special cases,0
%% Imports,0
%% Taxnomy checking,0
taxonomy CSV rows without 'taxonomy_string' entries are excluded,0
"from the taxonomy graph, but can be included in a classification",0
label specification JSON via the 'dataset_labels' key,0
...for each row in the taxnomy file,0
%% Command-line driver,0
%% Interactive driver,0
%%,0
which datasets are already processed?,0
"sequence-level query should be fairly fast, ~1 sec",0
cases when the class field is on the image level (images in a sequence,0
"that had different class labels, 'caltech' dataset is like this)",0
"this query may take a long time, >1hr",0
"this query should be fairly fast, ~1 sec",0
read species presence info from the JSON files for each dataset,0
has this class name appeared in a previous dataset?,0
columns to populate the spreadsheet,0
sort by descending species count,0
make the spreadsheet,0
hyperlink Bing search URLs,0
hyperlink example image SAS URLs,0
TODO hardcoded columns: change if # of examples or col_order changes,1
,0
map_lila_taxonomy_to_wi_taxonomy.py,0
,0
Loads the LILA category mapping (in which taxonomy information comes from an iNat taxonomy snapshot),0
and tries to map each class to the Wildlife Insights taxonomy.,0
,0
%% Constants and imports,0
Created by get_lila_category_list.py... contains counts for each category,0
This is a manually-curated file used to store mappings that had to be made manually,0
This is the main output file from this whole process,0
%% Load category and taxonomy files,0
%% Pull everything out of pandas,0
%% Cache WI taxonomy lookups,0
This is just a handy lookup table that we'll use to debug mismatches,0
taxon = wi_taxonomy[21653]; print(taxon),0
Look for keywords that don't refer to specific taxa: blank/animal/unknown,0
Do we have a species name?,0
"If 'species' is populated, 'genus' should always be populated; one item currently breaks",0
this rule.,0
...for each taxon,0
%% Find redundant taxa,0
%% Manual review of redundant taxa,0
%% Clean up redundant taxa,0
taxon_name = list(taxon_name_to_preferred_taxon_id.keys())[0],0
"If we've gotten this far, we should be choosing from multiple taxa.",0
,0
"This will become untrue if any of these are resolved later, at which point we shoudl",0
remove them from taxon_name_to_preferred_id,0
Choose the preferred taxa,0
%% Read supplementary mappings,0
"Each line is [lila query],[WI taxon name],[notes]",0
%% Map LILA categories to WI categories,0
Must be ordered from kingdom --> species,0
TODO:,1
"['subspecies','variety']",0
i_taxon = 0; taxon = lila_taxonomy[i_taxon]; print(taxon),0
"Go from kingdom --> species, choosing the lowest-level description as the query",0
"E.g., 'car'",0
"print('Made a supplementary mapping from {} to {}'.format(query,wi_taxon['taxon_name']))",0
print('No match for {}'.format(query)),0
...for each LILA taxon,0
%% Manual mapping,0
%% Build a dictionary from LILA dataset names and categories to LILA taxa,0
i_d = 0; d = lila_taxonomy[i_d],0
"%% Map LILA datasets to WI taxa, and count the number of each taxon available in each dataset",0
dataset_name = list(lila_dataset_to_categories.keys())[0],0
dataset_category = dataset_categories[0],0
"Write out the dataset name, category name, WI GUID, WI scientific name, WI common name,",0
and count,0
...for each category in this dataset,0
...for each dataset,0
...with open(),0
,0
retrieve_sample_image.py,0
,0
"Downloader that retrieves images from Google images, used for verifying taxonomy",0
"lookups and looking for egregious mismappings (e.g., ""snake"" being mapped to a fish called",0
"""snake"").",0
,0
"Simple wrapper around simple_image_download, but I've had to swap in and out the underlying",0
downloader a few times.,0
,0
%% Imports and environment,0
%%,0
%% Main entry point,0
%% Test driver,0
%%,0
,0
"Using the taxonomy .csv file, map all LILA datasets to the standard taxonomy",0
,0
Does not currently produce results; this is just used to confirm that all category names,0
have mappings in the taxonomy file.,0
,0
%% Constants and imports,0
Created by get_lila_category_list.py... contains counts for each category,0
%% Load category and taxonomy files,0
%% Map dataset names and category names to scientific names,0
i_row = 1; row = taxonomy_df.iloc[i_row]; row,0
"%% For each dataset, make sure we can map every category to the taxonomy",0
dataset_name = list(lila_dataset_to_categories.keys())[0],0
c = categories[0],0
,0
"Takes the megadb taxonomy mapping, extracts the rows that are relevant to",0
"LILA, and does some cleanup.",0
,0
%% Constants and imports,0
This is a partially-completed taxonomy file that was created from a different set of,0
"scripts, but covers *most* of LILA as of June 2022",0
Created by get_lila_category_list.py,0
%% Read the input files,0
Get everything out of pandas,0
"%% Find all unique dataset names in the input list, compare them with data names from LILA",0
d = input_taxonomy_rows[0],0
The script that generates this dictionary creates a separate entry for bounding box,0
"metadata files, but those don't represent new dataset names",0
%% Map input columns to output datasets,0
Make sure all of those datasets actually correspond to datasets on LILA,0
%% Re-write the input taxonomy file to refer to LILA datasets,0
Map the string datasetname:token to a taxonomic tree json,0
mapping = input_taxonomy_rows[0],0
Make sure that all occurrences of this mapping_string give us the same output,0
assert taxonomy_string == taxonomy_mappings[mapping_string],0
%% Re-write the input file in the target format,0
mapping_string = list(taxonomy_mappings.keys())[0],0
,0
prepare_lila_taxonomy_release.py,0
,0
"Given the private intermediate taxonomy mapping, prepare the public (release)",0
taxonomy mapping file.,0
,0
%% Imports and constants,0
Created by get_lila_category_list.py... contains counts for each category,0
%% Find out which categories are actually used,0
dataset_name = datasets_to_map[0],0
i_row = 0; row = df.iloc[i_row]; row,0
%% Generate the final output file,0
i_row = 0; row = df.iloc[i_row]; row,0
match_at_level = taxonomic_match[0],0
i_row = 0; row = df.iloc[i_row]; row,0
"E.g.: (43117, 'genus', 'lepus', ['hares and jackrabbits']",0
###############,0
---> CONSTANTS,0
###############,0
max_progressbar = count * (list(range(limit+1))[-1]+1),0
"bar = progressbar.ProgressBar(maxval=max_progressbar,",0
"widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()]).start()",0
bar.update(bar.currval + 1),0
bar.finish(),0
,0
"Given a subset of LILA datasets, find all the categories, and start the taxonomy",0
mapping process.,0
,0
%% Constants and imports,0
Created by get_lila_category_list.py,0
'NACTI',0
'Channel Islands Camera Traps',0
%% Read the list of datasets,0
The script that generates this dictionary creates a separate entry for bounding box,0
"metadata files, but those don't represent new dataset names",0
%% Find all categories,0
dataset_name = datasets_to_map[0],0
%% Initialize taxonomic lookup,0
from taxonomy_mapping.species_lookup import (,0
"get_taxonomic_info, print_taxonomy_matche)",0
%% Manual lookup,0
%%,0
q = 'white-throated monkey',0
raise ValueError(''),0
%% Match every query against our taxonomies,0
mapping_string = category_mappings[1]; print(mapping_string),0
...for each mapping,0
%% Write output rows,0
,0
"Does some consistency-checking on the LILA taxonomy file, and generates",0
an HTML preview page that we can use to determine whether the mappings,0
make sense.,0
,0
%% Imports and constants,0
"lila_taxonomy_file = r""G:\git\agentmorrisprivate\lila-taxonomy\lila-taxonomy-mapping.csv""",0
"lila_taxonomy_file = r""G:\temp\lila\lila_additions_2022.06.29.csv""",0
%% Support functions,0
%% Read the taxonomy mapping file,0
%% Prepare taxonomy lookup,0
from taxonomy_mapping.species_lookup import (,0
"get_taxonomic_info, print_taxonomy_matche)",0
%% Optionally remap all gbif-based mappings to inat (or vice-versa),0
%%,0
i_row = 1; row = df.iloc[i_row]; row,0
This should be zero for the release .csv,0
%%,0
%% Check for mappings that disagree with the taxonomy string,0
Look for internal inconsistency,0
Look for outdated mappings,0
i_row = 0; row = df.iloc[i_row],0
%% List null mappings,0
,0
"These should all be things like ""unidentified"" and ""fire""",0
,0
i_row = 0; row = df.iloc[i_row],0
%% List mappings with scientific names but no common names,0
%% List mappings that map to different things in different data sets,0
x = suppress_multiple_matches[-1],0
...for each row where we saw this query,0
...for each row,0
"%% Verify that nothing ""unidentified"" maps to a species or subspecies",0
"E.g., ""unidentified skunk"" should never map to a specific species of skunk",0
%% Make sure there are valid source and level values for everything with a mapping,0
%% Find WCS mappings that aren't species or aren't the same as the input,0
"WCS used scientific names, so these remappings are slightly more controversial",0
then the standard remappings.,0
row = df.iloc[-500],0
"Anything marked ""species"" or ""unknown"" by definition doesn't map to a species,",0
so ignore these.,0
print('WCS query {} ({}) remapped to {} ({})'.format(,0
"query,common_name,scientific_name,common_names_from_taxonomy))",0
%% Download sample images for all scientific names,0
i_row = 0; row = df.iloc[i_row],0
if s != 'mirafra':,0
continue,0
Check whether we already have enough images for this query,0
"print('Skipping query {}, already have {} images'.format(s,len(sizes_above_threshold)))",0
Check whether we've already run this query for a previous row,0
...for each row in the mapping table,0
%% Rename .jpeg to .jpg,0
"print('Renaming {} to {}'.format(fn,new_fn))",0
%% Choose representative images for each scientific name,0
s = list(scientific_name_to_paths.keys())[0],0
Be suspicious of duplicate sizes,0
...for each scientific name,0
%% Delete unused images,0
%% Produce HTML preview,0
i_row = 2; row = df.iloc[i_row],0
"image_paths = [os.path.relpath(p, output_base) for p in image_paths]",0
...for each row,0
%% Open HTML preview,0
######,0
,0
species_lookup.py,0
,0
Look up species names (common or scientific) in the GBIF and iNaturalist,0
taxonomies.,0
,0
Run initialize_taxonomy_lookup() before calling any other function.,0
,0
######,0
%% Constants and imports,0
As of 2020.05.12:,0
,0
"GBIF: ~777MB zipped, ~1.6GB taxonomy",0
"iNat: ~2.2GB zipped, ~51MB taxonomy",0
These are un-initialized globals that must be initialized by,0
the initialize_taxonomy_lookup() function below.,0
%% Functions,0
Initialization function,0
# Load serialized taxonomy info if we've already saved it,0
"# If we don't have serialized taxonomy info, create it from scratch.",0
Download and unzip taxonomy files,0
taxonomy_name = list(taxonomy_urls.items())[0][0]; zip_url = list(taxonomy_urls.items())[0][1],0
Don't download the zipfile if we've already unzipped what we need,0
Bypasses download if the file exists already,0
Unzip the files we need,0
...for each file that we need from this zipfile,0
Remove the zipfile,0
os.remove(zipfile_path),0
...for each taxonomy,0
"Create dataframes from each of the taxonomy files, and the GBIF common",0
name file,0
Load iNat taxonomy,0
Load GBIF taxonomy,0
Remove questionable rows from the GBIF taxonomy,0
Load GBIF vernacular name mapping,0
Only keep English mappings,0
Convert everything to lowercase,0
"For each taxonomy table, create a mapping from taxon IDs to rows",0
Create name mapping dictionaries,0
Build iNat dictionaries,0
row = inat_taxonomy.iloc[0],0
Build GBIF dictionaries,0
"The canonical name is the Latin name; the ""scientific name""",0
include the taxonomy name.,0
,0
http://globalnames.org/docs/glossary/,0
This only seems to happen for really esoteric species that aren't,0
"likely to apply to our problems, but doing this for completeness.",0
Don't include taxon IDs that were removed from the master table,0
Save everything to file,0
...def initialize_taxonomy_lookup(),0
"list of dicts: {'source': source_name, 'taxonomy': match_details}",0
i_match = 0,0
"list of (taxon_id, taxonRank, scientific name, [vernacular names])",0
corresponding to an exact match and its parents,0
Walk taxonomy hierarchy,0
This can happen because we remove questionable rows from the,0
GBIF taxonomy,0
"print(f'Warning: no row exists for parent_taxon_id {parent_taxon_id},' + \",0
"f'child taxon_id: {taxon_id}, query: {query}')",0
The GBIF taxonomy contains unranked entries,0
...while there is taxonomy left to walk,0
...for each match,0
Remove redundant matches,0
i_tree_a = 0; tree_a = matching_trees[i_tree_a],0
i_tree_b = 1; tree_b = matching_trees[i_tree_b],0
"If tree a's primary taxon ID is inside tree b, discard tree a",0
,0
taxonomy_level_b = tree_b['taxonomy'][0],0
...for each level in taxonomy B,0
...for each tree (inner),0
...for each tree (outer),0
...def traverse_taxonomy(),0
"print(""Finding taxonomy information for: {0}"".format(query))",0
"In GBIF, some queries hit for both common and scientific, make sure we end",0
up with unique inputs,0
"If the species is not found in either taxonomy, return None",0
Both GBIF and iNat have a 1-to-1 mapping between taxon_id and row number,0
Walk both taxonomies,0
...def get_taxonomic_info(),0
m = matches[0],0
"For example: [(9761484, 'species', 'anas platyrhynchos')]",0
...for each taxonomy level,0
...for each match,0
...def print_taxonomy_matches(),0
%% Taxonomy functions that make subjective judgements,0
query = 'person',0
Do we have an iNat match?,0
"print_taxonomy_matches(inat_matches, verbose=True)",0
"print_taxonomy_matches(gbif_matches, verbose=True)",0
print('Warning: multiple iNat matches for {}'.format(query)),0
Prefer chordates... most of the names that aren't what we want,0
"are esoteric insects, like a moth called ""cheetah""",0
,0
"If we can't find a chordate, just take the first match.",0
,0
i_test_match = 0,0
"This is (taxonID, taxonLevel, scientific, [list of common])",0
print(f'Warning: multiple iNat common names for {query}'),0
Default to returning the query,0
"print(f'Matched iNat {query} to {scientific_name},{common_name}')",0
...if we had iNat matches,0
If we either prefer GBIF or didn't have iNat matches,0
,0
Code is deliberately redundant here; I'm expecting some subtleties in how,0
handle GBIF and iNat.,0
print('Warning: multiple GBIF matches for {}'.format(query)),0
Prefer chordates... most of the names that aren't what we want,0
"are esoteric insects, like a moth called ""cheetah""",0
,0
"If we can't find a chordate, just take the first match.",0
,0
i_test_match = 0,0
"This is (taxonID, taxonLevel, scientific, [list of common])",0
print(f'Warning: multiple GBIF common names for {query}'),0
Default to returning the query,0
...if we needed to look in the GBIF taxonomy,0
...def _get_preferred_taxonomic_match(),0
%% Interactive drivers and debug,0
%% Initialization,0
%% Taxonomic lookup,0
query = 'lion',0
print(matches),0
Print the taxonomy in the taxonomy spreadsheet format,0
%% Directly access the taxonomy tables,0
%% Command-line driver,0
Read command line inputs (absolute path),0
Read the tokens from the input text file,0
Loop through each token and get scientific name,0
,0
process_species_by_dataset,0
,0
We generated a list of all the annotations in our universe; this script is,0
used to (interactively) map them onto the GBIF and iNat taxonomies.  Don't,0
"try to run this script from top to bottom; it's used like a notebook, not like",0
"a script, since manual review steps are required.",0
,0
%% Imports,0
%autoreload 0,0
%autoreload -species_lookup,0
%% Constants,0
Input file,0
Output file after automatic remapping,0
File to which we manually copy that file and do all the manual review; this,0
should never be programmatically written to,0
The final output spreadsheet,0
HTML file generated to facilitate the identificaiton of egregious mismappings,0
%% Functions,0
Prefer iNat matches over GBIF matches,0
query = 'person',0
Do we have an iNat match?,0
"print_taxonomy_matches(inat_matches, verbose=True)",0
"print_taxonomy_matches(gbif_matches, verbose=True)",0
print('Warning: multiple iNat matches for {}'.format(query)),0
Prefer chordates... most of the names that aren't what we want,0
"are esoteric insects, like a moth called ""cheetah""",0
,0
"If we can't find a chordate, just take the first match.",0
,0
i_test_match = 0,0
"This is (taxonID, taxonLevel, scientific, [list of common])",0
print(f'Warning: multiple iNat common names for {query}'),0
Default to returning the query,0
"print(f'Matched iNat {query} to {scientific_name},{common_name}')",0
...if we had iNat matches,0
If we either prefer GBIF or didn't have iNat matches,0
,0
Code is deliberately redundant here; I'm expecting some subtleties in how,0
handle GBIF and iNat.,0
print('Warning: multiple GBIF matches for {}'.format(query)),0
Prefer chordates... most of the names that aren't what we want,0
"are esoteric insects, like a moth called ""cheetah""",0
,0
"If we can't find a chordate, just take the first match.",0
,0
i_test_match = 0,0
"This is (taxonID, taxonLevel, scientific, [list of common])",0
print(f'Warning: multiple GBIF common names for {query}'),0
Default to returning the query,0
...if we needed to look in the GBIF taxonomy,0
...def get_preferred_taxonomic_match(),0
%% Initialization,0
%% Test single-query lookup,0
%%,0
%%,0
"q = ""grevy's zebra""",0
%% Read the input data,0
%% Run all our taxonomic lookups,0
i_row = 0; row = df.iloc[i_row],0
query = 'lion',0
...for each query,0
Write to the excel file that we'll use for manual review,0
%% Download preview images for everything we successfully mapped,0
uncomment this to load saved output_file,0
"output_df = pd.read_excel(output_file, keep_default_na=False)",0
i_row = 0; row = output_df.iloc[i_row],0
...for each query,0
%% Write HTML file with representative images to scan for obvious mis-mappings,0
i_row = 0; row = output_df.iloc[i_row],0
...for each row,0
%% Look for redundancy with the master table,0
Note: `master_table_file` is a CSV file that is the concatenation of the,0
"manually-remapped files (""manual_remapped.xlsx""), which are the output of",0
this script run across from different groups of datasets. The concatenation,0
"should be done manually. If `master_table_file` doesn't exist yet, skip this",0
"code cell. Then, after going through the manual steps below, set the final",0
manually-remapped version to be the `master_table_file`.,0
%% Manual review,0
Copy the spreadsheet to another file; you're about to do a ton of manual,0
review work and you don't want that programmatically overwrriten.,0
,0
See manual_review_xlsx above,0
%% Read back the results of the manual review process,0
%% Look for manual mapping errors,0
Manually inspect df for typos in 'dataset_names' and 'taxonomy_level' columns,0
Identify rows where:,0
,0
- 'taxonomy_level' does not match level of 1st element in 'taxonomy_string',0
- 'scientific_name' does not match name of 1st element in 'taxonomy_string',0
,0
...both of which typically represent manual mapping errors.,0
i_row = 0; row = df.iloc[i_row],0
"I'm not sure why both of these checks are necessary, best guess is that",1
the Excel parser was reading blanks as na on one OS/Excel version and as '',0
on another.,0
The taxonomy_string column is a .json-formatted string; expand it into,0
an object via eval(),0
"%% Find scientific names that were added manually, and match them to taxonomies",0
i_row = 0; row = df.iloc[i_row],0
...for each query,0
%% Write out final version,0
,0
"Generate read-only SAS URLs for all LILA containers, to facilitate partial downloads.",0
,0
The results of this script end up here:,0
,0
http://lila.science/wp-content/uploads/2020/03/lila_sas_urls.txt,0
,0
"Update: that file is manually maintained now, it can't be programmatically generated",0
,0
%% Imports,0
Read-only,0
%% Enumerate containers,0
%% Generate SAS tokens,0
%% Generate SAS URLs,0
%% Write to output file,0
--------some stuff needed to get AJAX to work with bottle?--------#,0
-------------------------------------------------------------------------------- #,0
PREPARE TO QUEUE IMAGES FOR LABELING,0
-------------------------------------------------------------------------------- #,0
# Connect as USER to database DB_NAME through peewee and initialize database proxy,0
# Load embedding model,0
---------------------------------------------------------------------- #,0
CREATE QUEUE OF IMAGES TO LABEL,0
---------------------------------------------------------------------- #,0
Use classifier to generate predictions,0
# Update model predicted class in PostgreSQL database,0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?,0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
-------------------------------------------------------------------------------- #,0
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI,0
-------------------------------------------------------------------------------- #,0
"# static routes (to serve CSS, etc.)",0
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps",0
"return bottle.static_file(filename, root='../../../../../../../../../../../.')",0
# dynamic routes,0
if data['display_grayscale']:,0
indices_to_exclude.update(set(color_indices)),0
elif not data['display_grayscale']:,0
indices_to_exclude.update(set(grayscale_indices)),0
data['display_images'] = {},0
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices],0
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices],0
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices],0
data['display_images']['detection_categories'] = [],0
for i in indices:,0
if str(dataset.samples[i][1]) == 'None':,0
data['display_images']['detection_categories'].append('None'),0
else:,0
existing_category_entries = {cat.id: cat.name for cat in Category.select()},0
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()",0
data['display_images']['detection_categories'].append(cat_name),0
Use image ids in images_to_label to get the corresponding dataset indices,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
print(set(dataset.set_indices[4]).update(set(indices_to_label))),0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Use image id images_to_label to get the corresponding dataset index,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?,0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Train on samples that have been labeled so far,0
dataset.set_kind(DetectionKind.UserDetection.value),0
print(y_train),0
Predict on the samples that have not been labeled,0
print(y_pred),0
Update model predicted class in PostgreSQL database,0
timer = time.time(),0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
print('Updating the database took %0.2f seconds'%(time.time() - timer)),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids)),0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
"once the classifier has been trained the first time, switch to AL sampling",0
,0
make_oneclass_json.py,0
,0
"Takes a coco-camera-traps .json database and collapses species classes to binary,",0
optionally removing labels from empty images (to be detector-friendly) (depending on,0
"""experiment_type"").",0
,0
"Assumes that empty images are labeled as ""empty"".",0
,0
%% Imports and environment,0
%% Core conversion function,0
"We're removing empty images from the annotation list, but not from",0
"the ""images"" list; they'll still get used in detector training.",0
print('Ignoring empty annotation'),0
%% Interactive driver,0
%%,0
Load annotations,0
Convert from multi-class to one-class,0
Write out the one-class data,0
%% Command-line driver,0
,0
plot_bounding_boxes.py,0
,0
Takes a .json database containing bounding boxes and renders those boxes on the,0
source images.,0
,0
"This assumes annotations in coco-camera-traps format, with absolute bbox",0
coordinates.,0
,0
%% Imports and environment,0
How many images should we process?  Set to -1 to process all images.,0
Should we randomize the image order?,0
%%  Read database and build up convenience mappings,0
Category ID to category name,0
Image ID to image info,0
Image ID to image path,0
Image ID to all annotations referring to this image,0
"%% Iterate over images, draw bounding boxes, write to file",0
For each image,0
image = images[0],0
Build up a list of bounding boxes to draw on this image,0
Load the image,0
Create figure and axes,0
Display the image,0
ann = imageAnnotations[0],0
"For each annotation associated with this image, render bounding box and label",0
"In the Rectangle() function, the first argument (""location"") is the bottom-left",0
of the rectangle.,0
,0
Origin is the upper-left of the image.,0
Add the patch to the Axes,0
Add a class label,0
This is magic goop that removes whitespace around image plots (sort of),0
Write the output image,0
...for each image,0
,0
plot_imerit_annotations.py,0
,0
Takes a .json file full of bounding box annotations and renders those boxes on the,0
source images.,0
,0
"This assumes annotations in the format we receive them, specifically:",0
,0
1) Relative bbox coordinates,0
"2) A list of .json objects, not a well-formatted .json file",0
,0
"I.e., don't use this on a COCO-style .json file.  See plot_bounding_boxes.py",0
for the same operation performed on a proper COCO-camera-traps database.,0
,0
%% Imports and environment,0
"os.makedirs(outputBase, exist_ok=True)",0
%%  Read all source images and build up a hash table from image name to full path,0
"This spans training and validation directories, so it's not the same as",0
just joining the image name to a base path,0
"%% Iterate over annotations, draw bounding boxes, write to file",0
annData has keys:,0
,0
"annotations, categories, images",0
,0
Each of these are lists of dictionaries,0
%% Render all annotations on each image in the sequence,0
%% Pull out image metadata,0
Build up a list of bounding boxes to draw on this image,0
Pull out just the image name from the filename,0
,0
File names look like:,0
,0
seq6efffac2-5567-11e8-b3fe-dca9047ef277.frame1.img59a94e52-23d2-11e8-a6a3-ec086b02610b.jpg,0
"m = re.findall(r'img(.*\.jpg)$', imgFileName, re.M|re.I)",0
print(m),0
assert(len(m) == 1),0
queryFileName = m[0],0
Map this image back to the original directory,0
"%% Loop over annotations, find annotations that match this image",0
%%,0
"x,y,w,h",0
,0
"x,y is the bottom-left of the rectangle",0
,0
"x,y origin is the upper-left",0
...for each annotation,0
%% Render with PIL (scrap),0
%% Render with Matplotlib,0
Create figure and axes,0
Display the image,0
Location is the bottom-left of the rect,0
,0
Origin is the upper-left,0
Add the patch to the Axes,0
This is magic goop that removes whitespace around image plots (sort of),0
os.startfile(outputFileName),0
%% Showing figures on-screen during debugging,0
plt.show(),0
Various (mostly unsuccessful) approaches to getting the plot window to show up,0
"in the foreground, which is a backend-specific operation...",0
,0
fig.canvas.manager.window.activateWindow(),0
fig.canvas.manager.window.raise_(),0
fm = plt.get_current_fig_manager(),0
"fm.window.attributes('-topmost', 1)",0
"fm.window.attributes('-topmost', 0)",0
,0
# This is the one that I found to be most robust... at like 80% robust.,0
plt.get_current_fig_manager().window.raise_(),0
%%,0
...for each image,0
...for each file,0
,0
convert_imerit_json_to_coco_json.py,0
,0
"Takes a .json file with bounding boxes but no class labels, and a .json file containing the",0
"class labels for those images, and creates a new json file with class labels and bounding",0
boxes.,0
,0
"The bounding box .json file is in the format returned by our annotators, which is not",0
actually a fully-formed .json file; rather it's a series of .json objects,0
,0
"Leaves filenames intact.  Stores imerit ""category IDs"" (empty/human/group/animal) in a new",0
"field called ""annotation_type"".",0
,0
%% Imports and constants,0
%% Configure files and paths,0
"For Snapshot Serengeti, we stored image IDs in our annotation files as:",0
,0
S1_B06_R1_PICT0008,0
,0
...but the corresponding ID in the master database is actually:,0
,0
S1\B06\R1\S1_B06_R1_PICT0008,0
,0
"If this is ""True"", we'll expand the former to the latter",0
Handling a one-off issue in which .'s were mysteriously replaced with -'s,0
"in our annotations.  This will be set dynamically, but I keep it here as",0
a constant to remind me to remove this code when we clean this issue up.,1
Used in the (rare) case where a bounding box was added to an image that was originally,0
annotated as empty,0
Used in the (rare) case where we added bounding boxes to an image with multiple species,0
"Should we include ambiguous bounding boxes (with the ""ambiguous"" category label)",0
in the output file?  Ambiguous boxes are boxes drawn on images with multiple species.,0
"%%  Read metadata from the master database, bounding boxes from the annotations file",0
"The bounding box .json file is in the format returned by our annotators, which is not",0
actually a fully-formed .json file; rather it's a series of .json objects,0
"Each element of annData is a dictionary corresponding to a single sequence, with keys:",0
,0
"annotations, categories, images",0
sequence = annData[0],0
%% Build convenience mappings,0
Image ID to images,0
Category ID to categories (referring to the database categories),0
"Image ID to categories (i.e., species labels)",0
Utility function we'll use to create annotations for images in empty,0
sequences (empty images in non-empty sequences already have annotations),0
"%% Reformat annotations, grabbing category IDs from the master database (prep)",0
iSequence = 0; sequence = annData[0],0
"%% Reformat annotations, grabbing category IDs from the master database (loop)",0
Make a copy here; we're going to manipulate the sequence annotations,0
when we need to add synthetic annotations for empty images,0
im = sequenceImages[0],0
Are there any annotations in this sequence?,0
Which images in this sequence have annotations?,0
For each image in this sequence...,0
imeritImageID = im['id'],0
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG,0
Confirm that the file exists,0
Hande a one-off issue with our annotations,0
datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG,0
,0
...had become:,0
,0
datasetsnapshotserengeti.seqASG000001a-frame0.imgS1_B06_R1_PICT0008.JPG,0
Does it look like we encountered this issue?,0
Convert:,0
,0
S1_B06_R1_PICT0008,0
,0
...to:,0
,0
S1/B06/B06_R1/S1_B06_R1_PICT0008,0
Make sure we only see each image once,0
Create empty annotations for empty images,0
,0
Here we use the *unmodified* file name,0
Create an empty annotation for this image,0
Annotations still use the annotation filename (not database ID) at this point;,0
these will get converted to database IDs below when we process the,0
whole sequence.,0
Sanity-check image size,0
"print('Warning: img {} was listed in DB as {}x{}, annotated as {}x{}, actual size{}x{}'.format(",0
"old_id,new_im['width'],new_im['height'],im['width'],im['height'],imgObj.width,imgObj.height))",0
...for each image in this sequence,0
For each annotation in this sequence...,0
ann = sequenceAnnotations[0],0
Prepare an annotation using the category ID from the database and,0
the bounding box from the annotations file,0
Maintain iMerit's annotation category,0
Generate an (arbitrary) ID for this annotation; the COCO format has a concept,0
"of annotation ID, but our annotation files don't",0
This was a one-off quirk with our file naming,1
We'll do special handling of images with multiple categories later,0
Store the annotation type (group/human/animal/empty),0
This annotation has no bounding box but the image wasn't originally,0
annotated as empty,0
This annotation has a bounding box but the image was originally,0
annotated as empty,0
unnormalize the bbox,0
... for each annotation in this sequence,0
... for each sequence,0
%% Post-processing,0
Count empty images,0
...for each file,0
%% Sanity-check empty images,0
,0
make_ss_annotation_image_folder.py,0
,0
Take a directory full of images with the very long filenames we give annotators:,0
,0
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension,0
,0
"...along with a COCO-camera-traps database referring to those files, and:",0
,0
1) Creates a new COCO-camera-traps database with the original filenames in them,0
(copying the annotations),0
,0
2) Optionally creates a new directory with those images named according to the,0
"Snapshot Serengeti naming convention, including complete relative paths.",0
,0
See convert_imerit_json_to_coco_json to see how we get from the original annotation,0
.json to a COCO-camera-traps database.,0
,0
%% Constants and imports,0
%% Configure files/paths,0
%% Read the annotations (referring to the old filenames),0
"%% Update filenames, optionally copying files",0
im = data['images'][0],0
For each image...,0
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG,0
"Find the image name, e.g. S1_B06_R1_PICT0008",0
Convert:,0
,0
S1_B06_R1_PICT0008.JPG,0
,0
...to:,0
,0
S1/B06/B06_R1/S1_B06_R1_PICT0008.JPG,0
...for each image,0
%% Write the revised database,0
,0
get_annotation_tool_link.py,0
,0
"Takes a COCO-camera-traps-style .json file with URLs already embedded, and prepares",0
a link to the visipedia annotation tool that reviews a subset of those images.,0
,0
,0
create_new_annotation_json.py,0
,0
"Creates a subset of a larger .json database, in this case specifically to pick some images",0
from Snapshot Serengeti.,0
,0
from utils import get_db_dicts,0
for seq in already_annotated:,0
seq_to_ims.pop(seq),0
remove already annotated images,0
add lion images,0
lion_seqs = cat_to_seqs[cat_to_id['lionMale']] + cat_to_seqs[cat_to_id['lionFemale']],0
#print(len(lion_seqs)),0
lion_seqs = [seq for seq in lion_seqs if seq not in already_annotated],0
#print(len(lion_seqs)),0
"lion_seqs_to_annotate.extend(random.sample(lion_seqs, min(len(lion_seqs),num_lions)))",0
print(len(lion_seqs_to_annotate)),0
num_elephants = 1000,0
elephant_seqs = cat_to_seqs[cat_to_id['elephant']],0
#print(len(lion_seqs)),0
elephant_seqs = [seq for seq in elephant_seqs if seq not in already_annotated],0
#print(len(lion_seqs)),0
"elephant_seqs_to_annotate = random.sample(elephant_seqs, num_elephants)",0
num_empty = 10,0
empty_seqs_to_annotate = [],0
for loc in loc_to_seqs:,0
empty_seqs = cats_per_location[loc][cat_to_id['empty']],0
empty_seqs = [seq for seq in empty_seqs if seq not in already_annotated],0
empty_seqs = [seq for seq in empty_seqs if seq_to_season[seq] in seasons_to_keep],0
"empty_seqs_to_annotate.extend(random.sample(empty_seqs, min(len(empty_seqs),num_empty)))",0
ims_to_annotate.extend(empty_ims_to_annotate),0
,0
filter_database.py,0
,0
"Look through a COCO-ct database and find images matching some crtieria, writing",0
a subset of images and annotations to a new file.,0
,0
%% Constants and imports,0
%% Configuration,0
%%  Read database and build up convenience mappings,0
Category ID to category name,0
Image ID to image info,0
Image ID to image path,0
%% Filter,0
ann = annotations[0],0
Is this a tiny box or a group annotation?,0
"x,y,w,h",0
All empty annotations should be classified as either empty or ambiguous,0
,0
"The ambiguous cases are basically minor misses on the annotators' part,",0
where two different small animals were present somewhere.,0
%% Write output file,0
,0
find_images_for_classes.py,0
,0
"Given a .json database, find images that are associated with one or more",0
classes.,0
,0
%% Constants and imports,0
%% Configuration,0
%%  Read database and build up convenience mappings,0
Category ID to category name,0
Image ID to image info,0
Image ID to image path,0
%% Look for target-class annotations,0
ann = annotations[0],0
,0
create_tfrecords_format.py,0
,0
This script creates a tfrecords file from a dataset in VOTT format.,0
%% Imports and environment,0
set up the filenames and annotations,0
This loop reads the bboxes and corresponding labels and assigns them,0
the correct image. Kind of slow at the moment...,0
If needed: merging all classes,0
bbox_labels = ['Animal' for _ in bbox_labels],0
BBox coords are stored in the format,0
"x_min (of width axis) y_min (of height axis), x_max, y_max",0
Coordinate system starts in top left corner,0
"In this framework, we need ('ymin', 'xmin', 'ymax', 'xmax') format",0
print out some stats,0
To make sure we loaded the bboxes correctly:,0
self.validate_bboxes(),0
For each image in the data set...,0
Make sure all are greater equal 0,0
%% Main tfrecord generation function,0
Propagate optional metadata to tfrecords,0
endfor each annotation for the current image,0
endfor each image,0
"Calculate number of shards to get the desired number of images per record,",0
ensure it is evenly divisible by the number of threads,0
%% Command-line driver,0
%% Driver,0
,0
make_tfrecords_cis_trans.py,0
,0
Given a .json file that contains a three-element list (train/val/test) of image IDs and a .json database that contains,0
"those image IDs, generates tfrecords whose filenames include ""train""/""val""/""test""",0
,0
"dataset = json.load(open('/ai4efs/databases/snapshotserengeti/oneclass/SnapshotSerengeti_Seasons_1_to_4_tfrecord_format_valid_ims.json','r'))",0
print('Creating trans_val tfrecords'),0
dataset = [im_id_to_im[idx] for idx in trans_val],0
,0
create_classification_tfrecords_from_json.py,0
,0
Called from make_tfrecords_cis_trans.py if you're running a classification experiment.,0
,0
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/',0
datafile = 'eccv_18_annotation_files_oneclass/CaltechCameraTrapsECCV18',0
image_file_root = datafolder+'eccv_18_all_images/',0
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/',0
datafolder = '/data/iwildcam/',0
datafile = 'combined_iwildcam_annotations_oneclass/eccv_train_and_imerit_2',0
need consecutive category ids,0
old_cat_id_to_new_cat_id = {categories[idx]['id']:idx+1 for idx in range(len(categories))},0
print(old_cat_id_to_new_cat_id),0
remove multiclass images,0
print(images[0]),0
print(vis_data[0]),0
,0
make_tfrecords_from_json.py,0
,0
"Given a coco-camera-traps .json file, creates tfrecords",0
,0
Thin wrapper for create_tfrecords_from_json.,0
,0
%% Constants and imports,0
%% Constants and imports (interactive),0
%%,0
%% Main tfrecord generation function,0
"check whether the input file has already been converted to the tfrecords format,",0
"if not, convert",0
"Calculate number of shards to get the desired number of images per record,",0
ensure it is evenly divisible by the number of threads,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
eMammal_make_splits.py,0
,0
"Based on a tfrecords_format json file of the database, creates 3 splits according to",0
the specified fractions based on location (images from the same location should be in,0
one split) or based on images.,0
,0
"If a previous split is provided (append_to_previous_split is True), the entries in",0
"each split will be preserved, and new entries will be appended, so that new models",0
can warm start with a model trained on the original splits.,0
configurations and paths,0
approximate fraction for the new entries,0
read in the previous splits of image ID or location ID if available,0
"find new locations and assign them to a split, without reassigning any previous locations",0
"find out which images are new, shuffle and split them",0
do NOT sort the IDs to keep the shuffled order,0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
add empty category,0
"add all images that don't have annotations, with cat empty",0
print(detection_results.keys()),0
group the detections and gts by image id:,0
print(image_id),0
print(len(scores)),0
print(len(labels)),0
recall_thresh = 0.9,0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall]),0
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])",0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall]),0
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])",0
exp_name = 'eccv_train',0
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json',0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(len(detection_results['images'])),0
print(len(seqs)),0
"print(len(seqs[0]),len(seqs[",0
print(detection_results.keys()),0
group the detections by image id:,0
group the ground truth annotations by image id:,0
print(seq),0
"for image_id, dets in per_image_detections.iteritems():",0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
print(num_gts),0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
if sum(detected_class_labels)>0:,0
print('detected animal box'),0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])",0
"print(scores, tp_fp_labels)",0
num_total_gts += num_gts,0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
print('valid box'),0
"print(best_im, best_score)",0
exp_name = 'eccv_train',0
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json',0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
need to loop over confidence values,0
"for each value, check if any detections on the image are > conf",0
"If so, that image gets class ""animal""",0
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0",0
"for image_id, dets in per_image_detections.iteritems():",0
"calc prec, rec for this confidence thresh",0
group the ground truth annotations by image id:,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(detection_results.keys()),0
group the detections and gts by image id:,0
print(image_id),0
print(len(scores)),0
print(len(labels)),0
recall_thresh = 0.9,0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall]),0
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])",0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall]),0
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])",0
print(sorted_ap),0
plt.bar(sorted_ap),0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(detection_results.keys()),0
group the detections by image id:,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
print(gts),0
print(num_gts),0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[:2])",0
"print(scores, tp_fp_labels)",0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
print('detected animal box'),0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])",0
"print(scores, tp_fp_labels)",0
num_total_gts += num_gts,0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
print('valid box'),0
best_im = np.argmax(max_im_scores),0
"print(best_im, best_score)",0
"temp_labels = np.zeros(len(im_detection_labels),  dtype=np.int32)",0
"temp_scores = np.zeros(len(im_detection_scores), dtype=np.float32)",0
"for j in range(min(im_num_gts, len(im_detection_labels))):",0
temp_labels[j] = True #TODO: this currently only works for oneclass?,1
temp_scores[j] = best_score,0
im_detection_labels = temp_labels,0
im_detection_scores = temp_scores,0
num_total_gts+=im_num_gts,0
"print(len(detection_scores), len(detection_scores[0]), len(detection_scores[1]))",0
exp_name = 'small_balanced_cct',0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(seq),0
"for image_id, dets in per_image_detections.iteritems():",0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
seq_num_gts.append(num_gts),0
print(num_gts),0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
num_total_gts += 1,0
print('valid box'),0
"print(best_im, best_score)",0
print('no valid box'),0
if sum(seq_num_gts)>0:,0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
need to loop over confidence values,0
"for each value, check if any detections on the image are > conf",0
"If so, that image gets class ""animal""",0
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0",0
"calc prec, rec for this confidence thresh",0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
group the detections by image id:,0
group the ground truth annotations by image id:,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
print('detected animal box'),0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])",0
"print(scores, tp_fp_labels)",0
num_total_gts += num_gts,0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
print('valid box'),0
"print(best_im, best_score)",0
,0
evaluate_detections.py,0
,0
Adapted from analyze_detection.py which is now archived.,0
,0
%% Imports and constants,0
%% Functions,0
"labels input to compute_object_detection_metrics() needs to start at 0, not 1",0
num_detections = len(dets['boxes']),0
to prevent 'Invalid dimensions for box data.' error,0
this box will not match any detections,0
compute one-class precision/recall/average precision (if every box is just of an object class),0
%% Command-line driver,0
,0
detection_eval_utils.py,0
,0
Utility functions used in evaluate_detections.py,0
,0
group the ground truth annotations by image id,0
#####,0
,0
detect_and_predict_image.py,0
,0
"Functions to load a TensorFlow detection and a classification model, run inference,",0
"render bounding boxes on images, and write out the resulting",0
images (with bounding boxes and classes).,0
,0
"See the ""test driver"" cell for example invocation.",0
,0
,0
#####,0
"%% Constants, imports, environment",0
Minimum detection confidence for showing a bounding box on the output image,0
Stick this into filenames before the extension for the rendered result,0
Number of top-scoring classes to show at each bounding box,0
%% Core detection functions,0
Load images if they're not already numpy arrays,0
iImage = 0; image = images[iImage],0
"Load the image as an nparray of size h,w,nChannels",0
"There was a time when I was loading with PIL and switched to mpimg,",0
"but I can't remember why, and converting to RGB is a very good reason",0
"to load with PIL, since mpimg doesn't give any indication of color",0
"order, which basically breaks all .png files.",0
,0
"So if you find a bug related to using PIL, update this comment",0
"to indicate what it was, but also disable .png support.",0
image = mpimg.imread(image),0
This shouldn't be necessarily when loading with PIL and converting to RGB,0
Actual detection,0
...for each image,0
"Currently ""boxes"" is a list of length nImages, where each element is shaped as",0
,0
"1,nDetections,4",0
,0
"This implicitly banks on TF giving us back a fixed number of boxes, let's assert on this",0
to make sure this doesn't silently break in the future.,0
iBox = 0; box = boxes[iBox],0
"""scores"" is a length-nImages list of elements with size 1,nDetections",0
"""classes"" is a length-nImages list of elements with size 1,nDetections",0
,0
"Still as floats, but really representing ints",0
Squeeze out the empty axis,0
boxes is nImages x nDetections x 4,0
scores and classes are both nImages x nDetections,0
Get input and output tensors of classification model,0
"imsize = cur_image['width'], cur_image['height']",0
Select detections with a confidence larger 0.5,0
Get these boxes and convert normalized coordinates to pixel coordinates,0
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes",0
"However, we need to make sure that it box coordinates are still within the image",0
For convenience:,0
Create an array with contains the index of the corresponding crop_box for each selected box,0
i.e. [False False 0 False 1 2 3 False False],0
For each box,0
If this box should be classified,0
Run inference,0
if box should not be classified,0
...for each box,0
species_scores should have shape len(images) x len(boxes) x num_species,0
...for each image,0
...with tf.Session,0
with classification_graph,0
species_scores should have shape len(images) x len(boxes) x num_species,0
%% Rendering functions,0
Display the image,0
plt.show(),0
"top, left, bottom, right",0
,0
"x,y origin is the upper-left",0
Generate bounding box text,0
Choose color based on class,0
Location is the bottom-left of the rect,0
,0
Origin is the upper-left,0
iRight = x + w,0
iTop = y + h,0
Add the patch to the Axes,0
Add class description,0
First determine best location by finding the corner that is closest to the image center,0
relative corner coordinates,0
relative coordinates of image center,0
Compute pair-wise squared distance and get the index of the one with minimal distance,0
Get the corresponding coordinates ...,0
... and alignment for the text box,0
Plot the text box with background,0
...for each box,0
This is magic goop that removes whitespace around image plots (sort of),0
"plt.savefig(outputFileName, bbox_inches='tight', pad_inches=0.0, dpi=dpi, transparent=True)",0
os.startfile(outputFileName),0
...for each image,0
...def render_bounding_boxes,0
Load and run detector on target images,0
Read the name of all classes,0
remove empty lines,0
%% Interactive driver,0
%%,0
%%,0
%% File helper functions,0
%% Command-line driver,0
Hack to avoid running on already-detected images,1
,0
"Script for evaluating a frozen graph given a COCO-style dataset, which was generated by",0
the ../data_management/databases/classification/make_classification_dataset.py script.,0
This is useful for debugging as the accuracy reported by this script should match the,0
accuracy reported by the Tensorflow training.,0
,0
Check that all files exists for easier debugging,0
Load frozen graph,0
Collect tensors for input and output,0
Read image,0
Run inference,0
Check if correct,0
,0
Script for evaluating precision/recall on a two-class problem given,0
"a frozen graph and a COCO-style dataset, which was generated by",0
the ../data_management/databases/classification/make_classification_dataset.py script.,0
We assume the positive class is at index 0 (with a zero-based indexing),0
,0
Check that all files exists for easier debugging,0
Load frozen graph,0
Collect tensors for input and output,0
Read image,0
"with open(image_path, 'rb') as fi:",0
"image =  sess.run(tf.image.decode_jpeg(fi.read(), channels=3))",0
image = image / 255.,0
Run inference,0
predicted_class = np.argmax(predictions),0
Check if correct,0
if coco.imgToAnns[image_id][0]['category_id'] == predicted_class:,0
correct = correct + 1,0
"In matplotlib < 1.5, plt.fill_between does not have a 'step' argument",0
,0
Script for selecting testing images of a COCO-style dataset generated by the script,0
../data_management/databases/classification/make_classification_dataset.py in a consistent,0
manner and predicting the class for it.,0
,0
Mandatory parameters,0
Optional parameters,0
Validate parameters,0
Derived parameters,0
"We assume that the dataset was generated with the make_classification_dataset.py script,",0
hence the images should be located in the same folder as the json,0
Make seletion deterministic,0
Load frozen graph,0
Get dataset information,0
Get classes,0
...and the class list corresponding to the model outputs by assuming,0
that they are in order of their ids,0
Get images of each class,0
Shuffle the image list,0
Start the image sampling,0
"Set of avaiable class IDs, will be filled below",0
If there are still images left for that class,0
"Get image for the sampled class, we already shuffled the class images before so",0
we can simply pop(),0
Start prediction,0
Collect tensors for input and output,0
Read image,0
Run inference,0
Print output to log file,0
#####,0
,0
api_apply_classifier_single_node.py,0
,0
Takes the JSON file produced by the detection API and,0
classifies all boxes above a confidence threshold.,0
,0
#####,0
"%% Constants, imports, environment",0
Assumes that the root of the CameraTraps repo is on the PYTHONPATH,0
Minimum detection confidence for classifying an object,0
Number of top-scoring classes to show at each bounding box,0
Enlargment factor applied to boxes before passing them to the classifier,0
,0
Provides more context and can lead to better results,0
List of detection categories for which we will run the classification,0
,0
"Currently there are {""1"": ""animal"", ""2"": ""person"", ""4"": ""vehicle""}",0
,0
Should be a list of string-formatted ints.,0
Number of significant float digits in JSON output,0
%% Core detection functions,0
Read the name of all classes,0
remove empty lines,0
Create field with name *classification_categories*,0
Add classes using 0-based indexing,0
def add_classification_categories,0
Make sure we have the right json object,0
Get input and output tensors of classification model,0
For each image,0
Read image,0
"Scale pixel values to [0,1]",0
For each box,0
Skip detections with low confidence,0
Skip if detection category is not in whitelist,0
Skip if already classified,0
"Get current box in relative coordinates and format [x_min, y_min, width_of_box, height_of_box]",0
"Convert to [ymin, xmin, ymax, xmax] and store it as 1x4 numpy array so we can",0
re-use the generic multi-box padding code,0
Convert normalized coordinates to pixel coordinates,0
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes.",0
,0
"However, we need to make sure that it box coordinates are still within the image.",0
Get the first (and only) row as our bbox to classify,0
Get the image data for that box,0
Run inference,0
Add an empty list to the json for our predictions,0
Add the *num_annotated_classes* top scoring classes,0
...for each box,0
...for each image,0
...with tf.Session,0
with classification_graph,0
def classify_boxes,0
Load classification model,0
Load detector json,0
Add classes to detector_json,0
"Run classifier on all images, changes will be writting directly to the json",0
Write output json,0
def load_and_run_classifier,0
%% Command-line driver,0
,0
Script for selecting testing images of a COCO-style dataset generated by the script,0
../data_management/databases/classification/make_classification_dataset.py in a consistent,0
manner and predicting the class for it.,0
,0
Assumes the cameratraps repo root is on the path,0
Make seletion deterministic,0
Mandatory parameters,0
Optional parameters,0
Validate parameters,0
Tranfer parameters to post-processing format,0
Load frozen graph,0
Reading image list,0
Reading class list,0
Image sampling,0
Start prediction,0
Collect tensors for input and output,0
Array for collecting infos for rendering the html,0
Read image,0
"Scale pixel values to [0,1]",0
Run inference,0
Add links to all available classes,0
,0
predict_image.py,0
,0
"Given a pointer to a frozen detection graph, runs inference on a single image,",0
printing the top classes to the console,0
,0
%% Imports,0
%% Command-line processing,0
Check that all files exist for easier debugging,0
%% Inference,0
Load frozen graph,0
Load class list,0
Remove empty lines,0
Collect tensors for input and output,0
Read image,0
Run inference,0
Print output,0
,0
Mostly unmodified script for freezing a model,0
Added for convenience and for possible future optimizations,0
,0
Copyright 2015 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
'input_checkpoint' may be a prefix if we're using Saver V2 format,0
Remove all the explicit device specifications for this node. This helps to,0
make the graph more portable.,0
List of all partition variables. Because the condition is heuristic,0
"based, the list could include false positives.",0
This tensor doesn't exist in the graph (for example it's,0
'global_step' or a similar housekeeping element) so skip it.,0
`var_list` is required to be a map of variable names to Variable,0
tensors. Partition variables are Identity tensors that cannot be,0
handled by Saver.,0
Models that have been frozen previously do not contain Variables.,0
optimized_output_graph_def = optimize_for_inference_lib.optimize_for_inference(,0
"output_graph_def,",0
"input_node_names.replace("" "", """").split("",""),",0
"output_node_names.replace("" "", """").split("",""),",0
tf.float32.as_datatype_enum),0
Write GraphDef to file if output path has been given.,0
,0
"Creates a graph description, which is required to create a frozen graph.",0
Adapted from from ./tf-slim/export_inference_graph.py,0
Added preprocessing to the definition for easier handling,0
,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
tf.app.flags.DEFINE_integer(,0
"'batch_size', None,",0
"'Batch size for the exported model. Defaulted to ""None"" so batch size can '",0
'be specified at model runtime.'),0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
#####################,0
Optimization Flags #,0
#####################,0
######################,0
Learning Rate Flags #,0
######################,0
######################,0
Dataset Flags #,0
######################,0
####################,0
Fine-Tuning Flags #,0
####################,0
"Note: when num_clones is > 1, this will actually have each clone to go",0
over each epoch FLAGS.num_epochs_per_decay times. This is different,0
behavior from sync replicas and is expected to produce different results.,0
Warn the user if a checkpoint exists in the train_dir. Then we'll be,0
ignoring the checkpoint anyway.,0
TODO(sguada) variables.filter_variables(),1
######################,0
Config model_deploy #,0
######################,0
Create global_step,0
#####################,0
Select the dataset #,0
#####################,0
#####################,0
Select the network #,0
#####################,0
####################################,0
Select the preprocessing function #,0
####################################,0
#############################################################,0
Create a dataset provider that loads data from the dataset #,0
#############################################################,0
###################,0
Define the model #,0
###################,0
############################,0
Specify the loss function #,0
############################,0
Gather initial summaries.,0
"Gather update_ops from the first clone. These contain, for example,",0
the updates for the batch_norm variables created by network_fn.,0
Add summaries for end_points.,0
Add summaries for losses.,0
Add summaries for variables.,0
################################,0
Configure the moving averages #,0
################################,0
########################################,0
Configure the optimization procedure. #,0
########################################,0
"If sync_replicas is enabled, the averaging will be done in the chief",0
queue runner.,0
Update ops executed locally by trainer.,0
Variables to train.,0
and returns a train_tensor and summary_op,0
Add total_loss to summary.,0
Create gradient updates.,0
Add the summaries from the first clone. These contain the summaries,0
created by model_fn and either optimize_clones() or _gather_clone_loss().,0
Merge all summaries together.,0
##########################,0
Kicks off the training. #,0
##########################,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
#####################,0
Select the dataset #,0
#####################,0
###################,0
Select the model #,0
###################,0
#############################################################,0
Create a dataset provider that loads data from the dataset #,0
#############################################################,0
####################################,0
Select the preprocessing function #,0
####################################,0
###################,0
Define the model #,0
###################,0
Define the metrics:,0
Print the summaries to screen.,0
TODO(sguada) use num_epochs=1,1
This ensures that we make a single pass over all of the data.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Transform the image to floats.,0
"Randomly crop a [height, width] section of the image.",0
Randomly flip the image horizontally.,0
"Because these operations are not commutative, consider randomizing",0
the order their operation.,0
Subtract off the mean and divide by the variance of the pixels.,0
Transform the image to floats.,0
Resize and crop if needed.,0
Subtract off the mean and divide by the variance of the pixels.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Use tf.slice instead of crop_to_bounding box as it accepts tensors to,0
define the crop size.,0
Compute the rank assertions.,0
Create a random bounding box.,0
,0
Use tf.random_uniform and not numpy.random.rand as doing the former would,0
"generate random numbers at graph eval time, unlike the latter which",0
generates random numbers at graph definition time.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Pass the real x only to one of the func calls.,0
The random_* ops do not necessarily clamp.,0
"Each bounding box has shape [1, num_boxes, box coords] and",0
"the coordinates are ordered [ymin, xmin, ymax, xmax].",0
A large fraction of image datasets contain a human-annotated bounding,0
box delineating the region of the image containing the object of interest.,0
We choose to create a new bounding box for the object which is a randomly,0
distorted version of the human-annotated bounding box that obeys an,0
"allowed range of aspect ratios, sizes and overlap with the human-annotated",0
"bounding box. If no box is supplied, then we assume the bounding box is",0
the entire image.,0
Crop the image to the specified bounding box.,0
"Each bounding box has shape [1, num_boxes, box coords] and",0
"the coordinates are ordered [ymin, xmin, ymax, xmax].",0
Restore the shape since the dynamic slice based upon the bbox_size loses,0
the third dimension.,0
This resizing operation may distort the images because the aspect,0
ratio is not respected. We select a resize method in a round robin,0
fashion based on the thread number.,0
Note that ResizeMethod contains 4 enumerated resizing methods.,0
We select only 1 case for fast_mode bilinear.,0
Randomly flip the image horizontally.,0
Randomly distort the colors. There are 1 or 4 ways to do it.,0
Crop the central region of the image with an area containing 87.5% of,0
the original image.,0
Resize the image to the specified height and width.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TODO(sguada) docstring paragraph by (a) motivating the need for the file and,1
(b) defining clones.,0
TODO(sguada) describe the high-level components of model deployment.,1
"E.g. ""each model deployment is composed of several parts: a DeploymentConfig,",0
"which captures A, B and C, an input_fn which loads data.. etc",0
Set up DeploymentConfig,0
Create the global step on the device storing the variables.,0
Define the inputs,0
Define the optimizer.,0
Define the model including the loss.,0
Run training.,0
Namedtuple used to represent a clone during deployment.,0
"Namedtuple used to represent a DeployedModel, returned by deploy().",0
Default parameters for DeploymentConfig,0
Create clones.,0
The return value.,0
Individual components of the loss that will need summaries.,0
Compute and aggregate losses on the clone device.,0
Add the summaries out of the clone device block.,0
Only use regularization_losses for the first clone,0
Compute the total_loss summing all the clones_losses.,0
Sum the gradients across clones.,0
Gather initial summaries.,0
Create Clones.,0
"Gather update_ops from the first clone. These contain, for example,",0
the updates for the batch_norm variables created by model_fn.,0
Place the global step on the device storing the variables.,0
Compute the gradients for the clones.,0
Add summaries to the gradients.,0
Create gradient updates.,0
Only use regularization_losses for the first clone,0
Add the summaries from the first clone. These contain the summaries,0
created by model_fn and either optimize_clones() or _gather_clone_loss().,0
Add total_loss to summary.,0
Merge all summaries together.,0
Note that each grad_and_vars looks like the following:,0
"((grad_var0_clone0, var0), ... (grad_varN_cloneN, varN))",0
!/usr/bin/python,0
Copyright 2016 Google Inc. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Failed to find ""index"" occurrence of item.",0
pylint: disable=broad-except,0
pylint: enable=broad-except,0
Grab the 'index' annotation.,0
Some images contain bounding box annotations that,0
"extend outside of the supplied image. See, e.g.",0
n03127925/n03127925_147.xml,0
"Additionally, for some bounding boxes, the min > max",0
or the box is entirely outside of the image.,0
Example: <...>/n06470073/n00141669_6790.xml,0
Determine if the annotation is from an ImageNet Challenge label.,0
Note: There is a slight bug in the bounding box annotation data.,0
Many of the dog labels have the human label 'Scottish_deerhound',0
instead of the synset ID 'n02092002' in the bbox.label field. As a,0
"simple hack to overcome this issue, we only exclude bbox labels",1
*which are synset ID's* that do not match original synset label for,0
the XML file.,0
Guard against improperly specified boxes.,0
Note bbox.filename occasionally contains '%s' in the name. This is,0
data set noise that is fixed by just using the basename of the XML file.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The URLs where the MNIST data can be downloaded.,0
The names of the classes.,0
"First, process the training data:",0
"Next, process the testing data:",0
"Finally, write the labels file:",0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The URL where the Flowers data can be downloaded.,0
The number of images in the validation set.,0
Seed for repeatability.,0
The number of shards per dataset split.,0
Initializes function that decodes RGB JPEG data.,0
Read the filename:,0
Divide into train and test:,0
"First, convert the training and validation sets.",0
"Finally, write the labels file:",0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TODO(nsilberman): Add tfrecord file type once the script is updated.,1
"If set to false, will not try to set label_to_names in dataset",0
by reading them from labels.txt or github.,0
n01440764,0
n01443537,0
n02119247    black fox,0
n02119359    silver fox,0
pylint: disable=g-line-too-long,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The URL where the CIFAR data can be downloaded.,0
The number of training files.,0
The height and width of each image.,0
The names of the classes.,0
"First, process the training data:",0
"Next, process the testing data:",0
"Finally, write the labels file:",0
Copyright 2016 Google Inc. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The labels file contains a list of valid labels are held in this file.,0
Assumes that the file contains entries as such:,0
n01440764,0
n01443537,0
n01484850,0
where each line corresponds to a label expressed as a synset. We map,0
each synset contained in the file to an integer (based on the alphabetical,0
ordering). See below for details.,0
This file containing mapping from synset to human-readable label.,0
Assumes each line of the file looks like:,0
,0
n02119247    black fox,0
n02119359    silver fox,0
"n02119477    red fox, Vulpes fulva",0
,0
where each line corresponds to a unique mapping. Note that each line is,0
formatted as <synset>\t<human readable label>.,0
This file is the output of process_bounding_box.py,0
Assumes each line of the file looks like:,0
,0
"n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940",0
,0
where each line corresponds to one bounding box annotation associated,0
with an image. Each line can be parsed as:,0
,0
"<JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>",0
,0
Note that there might exist mulitple bounding box annotations associated,0
with an image file.,0
pylint: disable=expression-not-assigned,0
pylint: enable=expression-not-assigned,0
Create a single Session to run all image coding calls.,0
Initializes function that converts PNG to JPEG data.,0
Initializes function that converts CMYK JPEG data to RGB JPEG data.,0
Initializes function that decodes RGB JPEG data.,0
File list from:,0
https://groups.google.com/forum/embed/?place=forum/torch7#!topic/torch7/fOSTXHIESSU,0
File list from:,0
https://github.com/cytsai/ilsvrc-cmyk-image-list,0
Read the image file.,0
Clean the dirty data.,0
1 image is a PNG.,0
22 JPEG images are in CMYK colorspace.,0
Decode the RGB JPEG.,0
Check that image converted to RGB,0
Each thread produces N shards where N = int(num_shards / num_threads).,0
"For instance, if num_shards = 128, and the num_threads = 2, then the first",0
"thread would produce shards [0, 64).",0
"Generate a sharded version of the file name, e.g. 'train-00002-of-00010'",0
"Break all images into batches with a [ranges[i][0], ranges[i][1]].",0
Launch a thread for each batch.,0
Create a mechanism for monitoring when all threads are finished.,0
Create a generic TensorFlow-based utility for converting all image codings.,0
Wait for all the threads to terminate.,0
Leave label index 0 empty as a background class.,0
Construct the list of JPEG files and labels.,0
Shuffle the ordering of all image files in order to guarantee,0
random ordering of the images with respect to label in the,0
saved TFRecord files. Make the randomization repeatable.,0
Build a map from synset to human-readable label.,0
Run it!,0
Allowing None in the signature so that dataset_factory can use the default.,0
!/usr/bin/python,0
Copyright 2016 Google Inc. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Read in the 50000 synsets associated with the validation data set.,0
Make all sub-directories in the validation data dir.,0
Move all of the image to the appropriate sub-directory.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
"The dataset has classes with no images (empty and human), which have ID 0 and 1,",0
so we need to specify 49 here despite having only 47 classes with images,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Final pooling and prediction,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The current_stride variable keeps track of the effective stride of the,0
activations. This allows us to invoke atrous convolution whenever applying,0
the next residual unit would result in the activations having stride larger,0
than the target output_stride.,0
The atrous convolution rate parameter.,0
Move stride from the block's last unit to the end of the block.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Collect activations at the block's end before performing subsampling.,0
Subsampling of the block's output activations.,0
"The following implies padding='SAME' for pool1, which makes feature",0
alignment easier for dense prediction tasks. This is also used in,0
https://github.com/facebook/fb.resnet.torch. However the accompanying,0
code of 'Deep Residual Learning for Image Recognition' uses,0
padding='VALID' for pool1. You can switch to that choice by setting,0
"slim.arg_scope([slim.max_pool2d], padding='VALID').",0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Collect outputs for conv2d, fully_connected and max_pool2d",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"inputs has shape [batch, 224, 224, 3]",0
"inputs has shape [batch, 513, 513, 3]",0
We do not include batch normalization or activation functions in,0
conv1 because the first ResNet unit will perform these. Cf.,0
Appendix of [2].,0
This is needed because the pre-activation variant does not have batch,1
normalization or activation functions in the residual unit output. See,0
Appendix of [2].,0
Convert end_points_collection into a dictionary of end_points.,0
Global average pooling.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
=============================================================================,0
"These parameters come from the online port, which don't necessarily match",0
those in the paper.,0
TODO(nsilberman): confirm these values with Philip.,1
##########,0
Encoder #,0
##########,0
"No normalizer for the first encoder layers as per 'Image-to-Image',",0
Section 5.1.1,0
First layer doesn't use normalizer_fn,0
Last layer doesn't use activation_fn nor normalizer_fn,0
##########,0
Decoder #,0
##########,0
"Dropout is used at both train and test time as per 'Image-to-Image',",0
Section 2.1 (last paragraph).,0
The Relu comes BEFORE the upsample op:,0
Explicitly set the normalizer_fn to None to override any default value,0
"that may come from an arg_scope, such as pix2pix_arg_scope.",0
No normalization on the input layer.,0
Stride 1 on the last layer.,0
"1-dim logits, stride 1, no activation, no normalization.",0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
Alias,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pylint: disable=unused-import,0
pylint: enable=unused-import,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
If we are fine tuning a checkpoint we need to start at a lower learning,0
rate since we are farther along on training.,0
We can start quantizing immediately if we are finetuning.,0
We need to wait for the model to train a bit before we quantize if we are,0
training from scratch.,0
Call rewriter to produce graph with fake quant ops and folded batch norms,0
"quant_delay delays start of quantization till quant_delay steps, allowing",0
for better model accuracy.,0
Configure the learning rate using an exponential decay.,0
"When restoring from a floating point model, the min/max values for",0
quantized weights and activations are not present.,0
We instruct slim to ignore variables that are missing during restoration,0
by setting ignore_missing_vars=True,0
"If we are restoring from a floating point model, we need to initialize",0
the global step to zero for the exponential decay to result in,0
reasonable learning rates.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Orignaly, arg_scope = slim.arg_scope and layers = slim, now switch to more",0
update-to-date tf.contrib.* API.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Turns off fused batch norm.,0
collection containing the moving mean and moving variance.,0
"For inverted pyramid models, we start with gating switched off.",0
batch_size x 32 x 112 x 112 x 64,0
Separable conv is slow when used at first conv layer.,0
batch_size x 32 x 56 x 56 x 64,0
batch_size x 32 x 56 x 56 x 64,0
batch_size x 32 x 56 x 56 x 192,0
batch_size x 32 x 28 x 28 x 192,0
batch_size x 32 x 28 x 28 x 256,0
batch_size x 16 x 14 x 14 x 512,0
batch_size x 16 x 14 x 14 x 512,0
batch_size x 16 x 14 x 14 x 512,0
batch_size x 16 x 14 x 14 x 528,0
batch_size x 16 x 14 x 14 x 832,0
batch_size x 8 x 7 x 7 x 832,0
batch_size x 8 x 7 x 7 x 1024,0
Final pooling and prediction,0
Temporal average pooling.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
299 x 299 x 3,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 160,0
71 x 71 x 192,0
35 x 35 x 384,0
4 x Inception-A blocks,0
35 x 35 x 384,0
Reduction-A block,0
17 x 17 x 1024,0
7 x Inception-B blocks,0
17 x 17 x 1024,0
Reduction-B block,0
8 x 8 x 1536,0
3 x Inception-C blocks,0
Auxiliary Head logits,0
17 x 17 x 1024,0
Final pooling and prediction,0
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which",1
can be set to False to disable pooling here (as in resnet_*()).,0
8 x 8 x 1536,0
1 x 1 x 1536,0
1536,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN,1
setups need the gradient of gradient FusedBatchNormGrad.,0
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN,1
setups need the gradient of gradient FusedBatchNormGrad.,0
First upscaling is different because it takes the input vector.,0
Last layer has different normalizer and activation.,0
Convert to proper channels.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80,0
71 x 71 x 192,0
35 x 35 x 192,0
35 x 35 x 320,0
TODO(alemi): Register intermediate endpoints,1
"17 x 17 x 1088 if output_stride == 8,",0
33 x 33 x 1088 if output_stride == 16,0
TODO(alemi): register intermediate endpoints,1
TODO(gpapan): Properly support output_stride for the rest of the net.,1
8 x 8 x 2080,0
TODO(alemi): register intermediate endpoints,1
8 x 8 x 1536,0
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which",1
can be set to False to disable pooling here (as in resnet_*()).,0
Set weight_decay for weights in conv2d and fully_connected layers.,0
Set activation_fn and parameters for batch_norm.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
collection containing update_ops.,0
use fused batch norm if possible.,0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"inputs has shape [batch, 224, 224, 3]",0
"inputs has shape [batch, 513, 513, 3]",0
Use clip_by_value to simulate bandpass activation.,0
Convert end_points_collection into a dictionary of end_points.,0
Global average pooling.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"end_points will collect relevant activations for external use, for example",0
summaries or losses.,0
299 x 299 x 3,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80.,0
71 x 71 x 192.,0
35 x 35 x 192.,0
Inception blocks,0
mixed: 35 x 35 x 256.,0
mixed_1: 35 x 35 x 288.,0
mixed_2: 35 x 35 x 288.,0
mixed_3: 17 x 17 x 768.,0
mixed4: 17 x 17 x 768.,0
mixed_5: 17 x 17 x 768.,0
mixed_6: 17 x 17 x 768.,0
mixed_7: 17 x 17 x 768.,0
mixed_8: 8 x 8 x 1280.,0
mixed_9: 8 x 8 x 2048.,0
mixed_10: 8 x 8 x 2048.,0
Auxiliary Head logits,0
Shape of feature map before the final layer.,0
Final pooling and prediction,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 2048,0
2048,0
1000,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Reflection pad by 1 in spatial dimensions (axes 1, 2 = h, w) to make a 3x3",0
'valid' convolution produce an output with the same dimension as the,0
input.,0
This corrects 1 pixel offset for images with even width and height.,0
conv2d is left aligned and conv2d_transpose is right aligned for even,0
sized images (while doing 'SAME' padding).,0
Note: This doesn't reflect actual model in paper.,0
Neither dropout nor batch norm -> dont need is_training,0
##########,0
Encoder #,0
##########,0
7x7 input stage,0
##################,0
Residual Blocks #,0
##################,0
##########,0
Decoder #,0
##########,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"end_points will collect relevant activations for external use, for example",0
summaries or losses.,0
Used to find thinned depths for each layer.,0
Note that sizes in the comments below assume an input spatial size of,0
"224x224, however, the inputs can be of any size greater 32x32.",0
224 x 224 x 3,0
depthwise_multiplier here is different from depth_multiplier.,0
depthwise_multiplier determines the output channels of the initial,0
"depthwise conv (see docs for tf.nn.separable_conv2d), while",0
depth_multiplier controls the # channels of the subsequent 1x1,0
convolution. Must have,0
in_channels * depthwise_multipler <= out_channels,0
so that the separable convolution is not overparameterized.,0
Use a normal convolution instead of a separable convolution.,0
112 x 112 x 64,0
56 x 56 x 64,0
56 x 56 x 64,0
56 x 56 x 192,0
28 x 28 x 192,0
Inception module.,0
28 x 28 x 256,0
28 x 28 x 320,0
14 x 14 x 576,0
14 x 14 x 576,0
14 x 14 x 576,0
14 x 14 x 576,0
14 x 14 x 576,0
7 x 7 x 1024,0
7 x 7 x 1024,0
Final pooling and prediction,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 1024,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
=============================================================================,0
Tensorflow mandates these.,0
Conv and DepthSepConv namedtuple define layers of the MobileNet architecture,0
Conv defines 3x3 convolution layers,0
DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.,0
stride is the stride of the convolution,0
depth is the number of channels or filters in a layer,0
MOBILENETV1_CONV_DEFS specifies the MobileNet body,0
Used to find thinned depths for each layer.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
By passing filters=None,0
separable_conv2d produces only a depthwise convolution layer,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 1024,0
Set weight_decay for weights in Conv and DepthSepConv layers.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Turns off fused batch norm.,0
collection containing the moving mean and moving variance.,0
Final pooling and prediction,0
Temporal average pooling.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Orignaly, add_arg_scope = slim.add_arg_scope and layers = slim, now switch to",0
more update-to-date tf.contrib.* API.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
Only insert rate to params if rate > 1.,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Notes for training NASNet Cifar Model,0
-------------------------------------,0
batch_size: 32,0
learning rate: 0.025,0
cosine (single period) learning rate decay,0
auxiliary head loss weighting: 0.4,0
clip global norm of all gradients by 5,0
600 epochs with a batch size of 32,0
This is used for the drop path probabilities since it needs to increase,0
the drop out probability over the course of training.,0
Notes for training large NASNet model on ImageNet,0
-------------------------------------,0
batch size (per replica): 16,0
learning rate: 0.015 * 100,0
learning rate decay factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 100 replicas,0
auxiliary head loss weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Notes for training the mobile NASNet ImageNet model,0
-------------------------------------,0
batch size (per replica): 32,0
learning rate: 0.04 * 50,0
learning rate scaling factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 50 replicas,0
auxiliary head weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Shape of feature map before the final layer.,0
149 x 149 x 32,0
Run the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Find where to place the reduction cells or stride normal cells,0
Setup for building in the auxiliary head.,0
Run the cells,0
true_cell_num accounts for the stem cells,0
Final softmax layer,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Find where to place the reduction cells or stride normal cells,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Setup for building in the auxiliary head.,0
Run the cells,0
true_cell_num accounts for the stem cells,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Final softmax layer,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Calculate the total number of cells in the network.,0
There is no distinction between reduction and normal cells in PNAS so the,0
total number of cells is equal to the number normal cells plus the number,0
of stem cells (two by default).,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Calculate the total number of cells in the network.,0
There is no distinction between reduction and normal cells in PNAS so the,0
total number of cells is equal to the number normal cells plus the number,0
of stem cells (two by default).,0
Configuration for the PNASNet-5 model.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"The cap for tf.clip_by_value, it's hinted from the activation distribution",0
"that the majority of activation values are in the range [-6, 6].",0
Skip path 1,0
Skip path 2,0
"First pad with 0's on the right and bottom, then shift the filter to",0
include those 0's that were added.,0
"If odd number of filters, add an additional one to the second path.",0
Concat and apply BN,0
Set the prev layer to the current layer if it is none,0
Check to be sure prev layer stuff is setup correctly,0
num_or_size_splits=1,0
Apply conv operations,0
Combine hidden states using 'add'.,0
Add hiddenstate to the list of hiddenstates we can choose from,0
Dont stride if this is not one of the original hiddenstates,0
"Check if a stride is needed, then use a strided 1x1 here",0
Determine if a reduction should be applied to make the number of,0
filters match.,0
Return the concat of all the states,0
Scale keep prob by layer number,0
The added 2 is for the reduction cells,0
Decrease the keep probability over time,0
,0
Script for generating a two-class dataset in COCO format for training an obscured image classifier,0
,0
Requires Python >= 3.6 because of the glob ** expression,0
,0
Collect images and labels,0
"Labels: clean = 0, obscured = 1",0
,0
top_folders_to_bottom.py,0
,0
Given a base folder with files like:,0
,0
A/1/2/a.jpg,0
B/3/4/b.jpg,0
,0
"...moves the top-level folders to the bottom in a new output folder, i.e., creates:",0
,0
1/2/A/a.jpg,0
3/4/B/b.jpg,0
,0
"In practice, this is used to make this:",0
,0
animal/camera01/image01.jpg,0
,0
...look like:,0
,0
camera01/animal/image01.jpg,0
,0
%% Constants and imports,0
%% Support functions,0
%% Main functions,0
Find top-level folder,0
Find file/folder names,0
Move or copy,0
...def process_file(),0
Enumerate input folder,0
Convert absolute paths to relative paths,0
Standardize delimiters,0
Make sure each input file maps to a unique output file,0
relative_filename = relative_files[0],0
Loop,0
...def top_folders_to_bottom(),0
%% Interactive driver,0
%%,0
%%,0
%% Command-line driver,0
"python top_folders_to_bottom.py ""g:\temp\separated_images"" ""g:\temp\separated_images_inverted"" --n_threads 100",0
Convert to an options object,0
%% Constants and imports,0
Assumes ai4eutils is on the python path (https://github.com/Microsoft/ai4eutils),0
Assumes the cameratraps repo root is on the path,0
%% Options,0
"Make sure there is no overlap between the two sets, because this will cause",0
issues in the code,0
## Required inputs,0
## Options,0
Can be a folder or a SAS URL,0
# These apply only when we're doing ground-truth comparisons,0
Classes we'll treat as negative,0
,0
"Include the token ""#NO_LABELS#"" to indicate that an image with no annotations",0
should be considered empty.,0
Classes we'll treat as neither positive nor negative,0
"A list of output sets that we should count, but not render images for.",0
,0
"Typically used to preview sets with lots of empties, where you don't want to",0
"subset but also don't want to render 100,000 empty images.",0
,0
"detections, non_detections",0
"detections_animal, detections_person, detections_vehicle",0
"By default, choose a confidence threshold based on the detector version",0
Used for summary statistics only,0
"Number of images to sample, -1 for ""all images""",0
"Random seed for sampling, or None",0
Optionally separate detections into categories (animal/vehicle/human),0
,0
Currently only supported when ground truth is unavailable,0
Optionally replace one or more strings in filenames with other strings;,0
useful for taking a set of results generated for one folder structure,0
and applying them to a slightly different folder structure.,0
Allow bypassing API output loading when operating on previously-loaded,0
results,0
Should we also split out a separate report about the detections that were,0
just below our main confidence threshold?,0
,0
Currently only supported when ground truth is unavailable,0
Control rendering parallelization,0
...PostProcessingOptions,0
#%% Helper classes and functions,0
Anything greater than this isn't clearly positive or negative,0
image has annotations suggesting both negative and positive,0
"image is not annotated or is annotated with 'unknown', 'unlabeled', ETC.",0
image has not yet been assigned a state,0
"In some analyses, we add an additional class that lets us look at",0
detections just below our main confidence threshold,0
count the # of images with each type of DetectionStatus,0
Check whether this image has:,0
- unknown / unassigned-type labels,0
- negative-type labels,0
"- positive labels (i.e., labels that are neither unknown nor negative)",0
"assert has_unknown_labels is False, '{} has unknown labels'.format(annotations)",0
If there are no image annotations...,0
n_negative += 1,0
im['_detection_status'] = DetectionStatus.DS_NEGATIVE,0
"If the image has more than one type of labels, it's ambiguous",0
"note: bools are automatically converted to 0/1, so we can sum",0
"After the check above, we can be sure it's only one of positive,",0
"negative, or unknown.",0
,0
Important: do not merge the following 'unknown' branch with the first,0
"'unknown' branch above, where we tested 'if len(categories) == 0'",0
,0
If the image has only unknown labels,0
If the image has only negative labels,0
If the images has only positive labels,0
"Annotate the category, if it is unambiguous",0
...for each image,0
...mark_detection_status(),0
"Leaving code in place for reading from blob storage, may support this",0
in the future.,0
resize is to display them in this notebook or in the HTML more quickly,0
os.path.isfile() is slow when mounting remote directories; much faster,0
to just try/except on the image open.,0
return '',0
Render images to a flat folder... we can use os.sep here because we've,0
already normalized paths,0
"Create class labels like ""gt_1"" or ""gt_27""",0
"for i_box,box in enumerate(ground_truth_boxes):",0
gt_classes.append('_' + str(box[-1])),0
"errno.ENAMETOOLONG doesn't get thrown properly on Windows, so",0
we awkwardly check against a hard-coded limit,0
Use slashes regardless of os,0
Optionally add links back to the original images,0
...render_bounding_boxes,0
Count items in each category,0
Optionally sort by filename before writing to html,0
Write the individual HTML files,0
...prepare_html_subpages(),0
Get unique categories above the threshold for this image,0
Render an image (with no ground truth information),0
"This is a list of [class,confidence] pairs, sorted by confidence",0
"If we either don't have a confidence threshold, or we've met our",0
confidence threshold,0
...if this detection has classification info,0
...for each detection,0
...def render_image_no_gt(),0
This should already have been normalized to either '/' or '\',0
...def render_image_with_gt(),0
%% Main function,0
#%% Expand some options for convenience,0
#%% Prepare output dir,0
#%% Load ground truth if available,0
Mark images in the ground truth as positive or negative,0
#%% Load detection (and possibly classification) results,0
"If the caller hasn't supplied results, load them",0
Determine confidence thresholds if necessary,0
Remove failed rows,0
Convert keys and values to lowercase,0
"Add column 'pred_detection_label' to indicate predicted detection status,",0
not separating out the classes,0
#%% Pull out descriptive metadata,0
This is rare; it only happens during debugging when the caller,0
is supplying already-loaded API results.,0
"#%% If we have ground truth, remove images we can't match to ground truth",0
#%% Sample images for visualization,0
#%% Fork here depending on whether or not ground truth is available,0
"If we have ground truth, we'll compute precision/recall and sample tp/fp/tn/fn.",0
,0
Otherwise we'll just visualize detections/non-detections.,0
#%% Detection evaluation: compute precision/recall,0
numpy array of detection probabilities,0
"numpy array of bools (0.0/1.0), and -1 as null value",0
Don't include ambiguous/unknown ground truth in precision/recall analysis,0
"For completeness, include the result at a confidence threshold of 1.0",0
Compute and print summary statistics,0
Thresholds go up throughout precisions/recalls/thresholds; find the last,0
value where recall is at or above target.  That's our precision @ target recall.,0
"np.where returns a tuple of arrays, but in this syntax where we're",0
"comparing an array with a scalar, there will only be one element.",0
Convert back to a list,0
Flatten the confusion matrix,0
"#%% Collect classification results, if they exist",0
Mapping of classnames to idx for the confusion matrix.,0
,0
"The lambda is actually kind of a hack, because we use assume that",1
the following code does not reassign classname_to_idx,0
Confusion matrix as defaultdict of defaultdict,0
,0
"Rows / first index is ground truth, columns / second index is predicted category",0
iDetection = 0; fn = detector_files[iDetection]; print(fn),0
"If this image has classification predictions, and an unambiguous class",0
"annotated, and is a positive image...",0
"The unambiguous category, we make this a set for easier handling afterward",0
"Compute the accuracy as intersection of union,",0
i.e. (# of categories in both prediciton and GT),0
divided by (# of categories in either prediction or GT,0
,0
"In case of only one GT category, the result will be 1.0, if",0
prediction is one category and this category matches GT,0
,0
"It is 1.0/(# of predicted top-1 categories), if the GT is",0
one of the predicted top-1 categories.,0
,0
"It is 0.0, if none of the predicted categories is correct",0
Distribute this accuracy across all predicted categories in the,0
confusion matrix,0
...for each file in the detection results,0
If we have classification results,0
Build confusion matrix as array from classifier_cm,0
Print some statistics,0
Prepare confusion matrix output,0
Get confusion matrix as string,0
Get fixed-size classname for each idx,0
Prepend class name on each line and add to the top,0
Print formatted confusion matrix,0
"Actually don't, this gets really messy in all but the widest consoles",0
Plot confusion matrix,0
To manually add more space at bottom: plt.rcParams['figure.subplot.bottom'] = 0.1,0
,0
"Add 0.5 to figsize for every class. For two classes, this will result in",0
"fig = plt.figure(figsize=[4,4])",0
...if we have classification results,0
#%% Render output,0
Write p/r table to .csv file in output directory,0
Write precision/recall plot to .png file in output directory,0
#%% Sampling,0
Sample true/false positives/negatives with correct/incorrect top-1,0
classification and render to html,0
Accumulate html image structs (in the format expected by write_html_image_lists),0
"for each category, e.g. 'tp', 'fp', ..., 'class_bird', ...",0
Add default entries by accessing them for the first time,0
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]",0
"Each element will be a three-tuple with elements file,max_conf,detections",0
"Assemble the information we need for rendering, so we can parallelize without",0
dealing with Pandas,0
i_row = 0; row = images_to_visualize.iloc[0],0
Filenames should already have been normalized to either '/' or '\',0
Map all the rendering results in the list rendering_results into the,0
dictionary images_html,0
Prepare the individual html image files,0
Write index.html,0
Show links to each GT class,0
,0
We could do this without classification results; currently we don't.,0
Add links to all available classes,0
Close body and html tags,0
...for each image,0
"#%% Otherwise, if we don't have ground truth...",0
#%% Sample detections/non-detections,0
Accumulate html image structs (in the format expected by write_html_image_list),0
for each category,0
Add default entries by accessing them for the first time,0
"Maps detection categories - e.g. ""human"" - to result set names, e.g.",0
"""detections_human""",0
Add a set of results for each category and combination of categories,0
Create output directories,0
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]",0
"list of 3-tuples with elements (file, max_conf, detections)",0
"Assemble the information we need for rendering, so we can parallelize without",0
dealing with Pandas,0
i_row = 0; row = images_to_visualize.iloc[0],0
Filenames should already have been normalized to either '/' or '\',0
"render_image_no_gt(file_info,detection_categories_to_results_name,",0
"detection_categories,classification_categories)",0
Map all the rendering results in the list rendering_results into the,0
dictionary images_html,0
Prepare the individual html image files,0
Write index.html,0
"We can't just sum these, because image_counts includes images in both their",0
detection and classification classes,0
total_images = sum(image_counts.values()),0
Don't print classification classes here; we'll do that later with a slightly,0
different structure,0
Add links to all available classes,0
os.startfile(output_html_file),0
...if we do/don't have ground truth,0
...process_batch_results,0
%% Interactive driver(s),0
%%,0
options.unlabeled_classes = ['human'],0
os.start(ppresults.output_html_file),0
%% Command-line driver,0
,0
load_api_results.py,0
,0
Loads the output of the batch processing API (json) into a pandas dataframe.,0
,0
Also functions to group entries by seq_id.,0
,0
Includes the deprecated functions that worked with the old CSV API output format.,0
,0
%% Constants and imports,0
%% Functions for grouping by sequence_id,0
example,0
input 'file': 'SER/S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG',0
output 'id': 'S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG',0
%% Functions for loading the result as a Pandas DataFrame,0
Validate that this is really a detector output file,0
Fields in the API output json other than 'images',0
Normalize paths to simplify comparisons later,0
"image['file'] = image['file'].replace('\\','/')",0
Replace some path tokens to match local paths to original blob structure,0
"If this is a newer file that doesn't include maximum detection confidence values,",0
"add them, because our unofficial internal dataframe format includes this.",0
Pack the json output into a Pandas DataFrame,0
Sanity-check that this is really a detector output file,0
Normalize paths to simplify comparisons later,0
De-serialize detections,0
Optionally replace some path tokens to match local paths to the original blob structure,0
string_to_replace = list(options.detector_output_filename_replacements.keys())[0],0
"TODO: hit some silly issues with vectorized str() and escaped characters, vectorize",1
this later.,0
,0
"detection_results['image_path'].str.replace(string_to_replace,replacement_string)",0
iRow = 0,0
%% Imports,0
Assumes ai4eutils is on the python path (https://github.com/Microsoft/ai4eutils),0
%% Constants and support classes,0
We will confirm that this matches what we load from each file,0
Process-based parallelization isn't supported yet,0
%% Main function,0
"Warn the user if some ""detections"" might not get rendered",0
#%% Validate inputs,0
#%% Load both result sets,0
assert results_a['detection_categories'] == default_detection_categories,0
assert results_b['detection_categories'] == default_detection_categories,0
#%% Make sure they represent the same set of images,0
#%% Find differences,0
"Each of these maps a filename to a two-element list (the image in set A, the image in set B)",0
,0
"Right now, we only handle a very simple notion of class transition, where the detection",0
of maximum confidence changes class *and* both images have an above-threshold detection.,0
fn = filenames_a[0],0
We shouldn't have gotten this far if error_on_non_matching_lists is set,0
det = im_a['detections'][0],0
...for each filename,0
#%% Sample and plot differences,0
"Render two sets of results (i.e., a comparison) for a single",0
image.,0
...def render_image_pair(),0
fn = image_filenames[0],0
...def render_detection_comparisons(),0
"For each category, generate comparison images and the",0
comparison HTML page.,0
,0
category = 'common_detections',0
Choose detection pairs we're going to render for this category,0
...for each category,0
#%% Write the top-level HTML file content,0
...def compare_batch_results(),0
%% Interactive driver,0
%% KRU,0
%% Command-line driver,0
# TODO,1
,0
"Merge high-confidence detections from one results file into another file,",0
when the target file does not detect anything on an image.,0
,0
Does not currently attempt to merge every detection based on whether individual,0
detections are missing; only merges detections into images that would otherwise,0
be considered blank.,0
,0
"If you want to literally merge two .json files, see combine_api_outputs.py.",0
,0
%% Constants and imports,0
%% Structs,0
Don't bother merging into target images where the max detection is already,0
higher than this threshold,0
"If you want to merge only certain categories, specify one",0
(but not both) of these.,0
%% Main function,0
im = output_data['images'][0],0
"Determine whether we should be processing all categories, or just a subset",0
of categories.,0
i_source_file = 0; source_file = source_files[i_source_file],0
source_im = source_data['images'][0],0
detection_category = list(detection_categories)[0],0
"This is already a detection, no need to proceed looking for detections to",0
transfer,0
Boxes are x/y/w/h,0
source_sizes = [det['bbox'][2]*det['bbox'][3] for det in source_detections_this_category_raw],0
Only look at boxes below the size threshold,0
...for each detection category,0
"print('Adding {} detections to image {}'.format(len(detections_to_transfer),image_filename))",0
Update the max_detection_conf field (if present),0
...for each image,0
...for each source file,0
%% Test driver,0
%%,0
%% Command-line driver (TODO),0
,0
separate_detections_into_folders.py,0
,0
## Overview,0
,0
"Given a .json file with batch processing results, separate the files in that",0
"set of results into folders that contain animals/people/vehicles/nothing,",0
according to per-class thresholds.,0
,0
"Image files are copied, not moved.",0
,0
,0
## Output structure,0
,0
Preserves relative paths within each of those folders; cannot be used with .json,0
files that have absolute paths in them.,0
,0
"For example, if your .json file has these images:",0
,0
a/b/c/1.jpg,0
a/b/d/2.jpg,0
a/b/e/3.jpg,0
a/b/f/4.jpg,0
a/x/y/5.jpg,0
,0
And let's say:,0
,0
"* The results say that the first three images are empty/person/vehicle, respectively",0
"* The fourth image is above threshold for ""animal"" and ""person""",0
* The fifth image contains an animal,0
,0
* You specify an output base folder of c:\out,0
,0
You will get the following files:,0
,0
c:\out\empty\a\b\c\1.jpg,0
c:\out\people\a\b\d\2.jpg,0
c:\out\vehicles\a\b\e\3.jpg,0
c:\out\animal_person\a\b\f\4.jpg,0
c:\out\animals\a\x\y\5.jpg,0
,0
## Rendering bounding boxes,0
,0
"By default, images are just copied to the target output folder.  If you specify --render_boxes,",0
bounding boxes will be rendered on the output images.  Because this is no longer strictly,0
"a copy operation, this may result in the loss of metadata.  More accurately, this *may*",0
result in the loss of some EXIF metadata; this *will* result in the loss of IPTC/XMP metadata.,0
,0
Rendering boxes also makes this script a lot slower.,0
,0
## Classification-based separation,0
,0
"If you have a results file with classification data, you can also specify classes to put",0
"in their own folders, within the ""animals"" folder, like this:",0
,0
"--classification_thresholds ""deer=0.75,cow=0.75""",0
,0
"So, e.g., you might get:",0
,0
c:\out\animals\deer\a\x\y\5.jpg,0
,0
"In this scenario, the folders within ""animals"" will be:",0
,0
"deer, cow, multiple, unclassified",0
,0
"""multiple"" in this case only means ""deer and cow""; if an image is classified as containing a",0
"bird and a bear, that would end up in ""unclassified"", since the folder separation is based only",0
on the categories you provide at the command line.,0
,0
"No classification-based separation is done within the animal_person, animal_vehicle, or",0
animal_person_vehicle folders.,0
,0
%% Constants and imports,0
Occasionally we have near-zero confidence detections associated with COCO classes that,0
didn't quite get squeezed out of the model in training.  As long as they're near zero,0
"confidence, we just ignore them.",0
%% Options class,0
"Dictionary mapping categories (plus combinations of categories, and 'empty') to output folders",0
Populated only when using classification results,0
"Originally specified as a string, converted to a dict mapping name:threshold",0
...__init__(),0
...class SeparateDetectionsIntoFoldersOptions,0
%% Support functions,0
Find the maximum confidence for each category,0
,0
det = detections[0],0
"For zero-confidence detections, we occasionally have leftover goop",0
from COCO classes,0
assert det['conf'] < invalid_category_epsilon,0
...for each detection on this image,0
Count the number of thresholds exceeded,0
...for each category,0
If this is above multiple thresholds,0
"TODO: handle species-based separation in, e.g., the animal_person case",1
"Are we making species classification folders, and is this an animal?",0
Do we need to put this into a specific species folder?,0
Find the animal-class detections that are above threshold,0
Count the number of classification categories that are above threshold for at,0
least one detection,0
d = valid_animal_detections[0],0
classification = d['classifications'][0],0
"Do we have a threshold for this category, and if so, is",0
this classification above threshold?,0
...for each classification,0
...for each detection,0
...if we have to deal with classification subfolders,0
...if we have 0/1/more categories above threshold,0
...if this is/isn't a failure case,0
Skip this image if it's empty and we're not processing empty images,0
"At this point, this image is getting copied; we may or may not also need to",0
draw bounding boxes.,0
Do a simple copy operation if we don't need to render any boxes,0
Open the source image,0
"Render bounding boxes for each category separately, beacuse",0
we allow different thresholds for each category.,0
"When we're not using classification folders, remove classification",0
information to maintain standard detection colors.,0
...for each category,0
Read EXIF metadata,0
"Write output with EXIF metadata if available, and quality='keep' if this is a JPEG",0
"image.  Unfortunately, neither parameter likes ""None"", so we get a slightly",0
icky cascade of if's here.,0
Also see:,0
,0
https://discuss.dizzycoding.com/determining-jpg-quality-in-python-pil/,0
,0
...for more ways to preserve jpeg quality if quality='keep' doesn't do the trick.,0
...if we don't/do need to render boxes,0
...def process_detections(),0
%% Main function,0
Create output folder if necessary,0
Load detection results,0
Map class names to output folders,0
Create all combinations of categories,0
category_name = category_names[0],0
Do we have a custom threshold for this category?,0
Create folder mappings for each category,0
Create the actual folders,0
"Handle species classification thresholds, if specified",0
"E.g. deer=0.75,cow=0.75",0
token = tokens[0],0
...for each token,0
...if classification thresholds are still in string format,0
Validate the classes in the threshold list,0
...if we need to deal with classification categories,0
i_image = 14; im = images[i_image]; im,0
...for each image,0
...def separate_detections_into_folders,0
%% Interactive driver,0
%%,0
%%,0
%%,0
%% Testing various command-line invocations,0
"With boxes, no classification",0
"No boxes, no classification (default)",0
"With boxes, with classification",0
"No boxes, with classification",0
%% Command-line driver,0
"python api\batch_processing\postprocessing\separate_detections_into_folders.py ""d:\temp\rspb_mini.json"" ""d:\temp\demo_images\rspb_2018_2019_mini"" ""d:\temp\separation_test"" --nthreads 2",0
Convert to an options object,0
"print('{} {}'.format(v,name))",0
List of category numbers to use in separation; uses all categories if None,0
"Can be ""size"", ""width"", or ""height""",0
For each image...,0
,0
im = images[0],0
d = im['detections'][0],0
Are there really any detections here?,0
Is this a category we're supposed to process?,0
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing#detector-outputs,0
...for each detection,0
...for each image,0
...def categorize_detections_by_size(),0
,0
add_max_conf.py,0
,0
"The MD output format included a ""max_detection_conf"" field with each image",0
up to and including version 1.2; it was removed as of version 1.3 (it's,0
redundant with the individual detection confidence values).,1
,0
"Just in case someone took a dependency on that field, this script allows you",0
to add it back to an existing .json file.,0
,0
%% Imports and constants,0
%% Main function,0
%% Driver,0
#######,0
,0
remove_repeat_detections.py,0
,0
"Used after running find_repeat_detections, then manually filtering the results,",0
to create a final filtered output file.,0
,0
"If you want to use this script, we recommend that you read the user's guide:",0
,0
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms,0
,0
#######,0
%% Constants and imports,0
%% Main function,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
#######,0
,0
repeat_detections_core.py,0
,0
Core utilities shared by find_repeat_detections and remove_repeat_detections.,0
,0
#######,0
%% Imports and environment,0
"from ai4eutils; this is assumed to be on the path, as per repo convention",0
"""PIL cannot read EXIF metainfo for the images""",0
"""Metadata Warning, tag 256 had too many entries: 42, expected 1""",0
%% Constants,0
%% Classes,0
Relevant for rendering the folder of images for filtering,0
,0
"imageBase can also be a SAS URL, in which case some error-checking is",0
disabled.,0
Don't consider detections with confidence lower than this as suspicious,0
Don't consider detections with confidence higher than this as suspicious,0
What's the IOU threshold for considering two boxes the same?,0
How many occurrences of a single location (as defined by the IOU threshold),0
are required before we declare it suspicious?,0
"Ignore ""suspicious"" detections larger than some size; these are often animals",0
taking up the whole image.  This is expressed as a fraction of the image size.,0
"Ignore ""suspicious"" detections smaller than some size",0
Ignore folders with more than this many images in them,0
A list of classes we don't want to treat as suspicious. Each element is an int.,0
Load detections from a filter file rather than finding them from the detector output,0
".json file containing detections, should be called detectionIndex.json in the filtering_* folder",0
produced in the first pass,0
(optional) List of filenames remaining after deletion of identified,0
repeated detections that are actually animals.  This should be a flat,0
"text file, one relative filename per line.  See enumerate_images().",0
Turn on/off optional outputs,0
Determines whether bounding-box rendering errors (typically network errors) should,0
be treated as failures,0
Box rendering options,0
State variables,0
"Replace filename tokens after reading, useful when the directory structure",0
has changed relative to the structure the detector saw,0
How many folders up from the leaf nodes should we be going to aggregate images?,0
An optional function that takes a string (an image file name) and returns,0
"a string (the corresponding  folder ID), typically used when multiple folders",0
actually correspond to the same camera in a manufacturer-specific way (e.g.,0
a/b/c/RECONYX100 and a/b/c/RECONYX101 may really be the same camera).,0
Include/exclude specific folders... only one of these may be,0
"specified; ""including"" folders includes *only* those folders.",0
"Optionally show *other* detections (i.e., detections other than the",0
one the user is evaluating) in a light gray,0
"If bRenderOtherDetections is True, what color should we use to render the",0
(hopefully pretty subtle) non-target detections?,0
,0
"In theory I'd like these ""other detection"" rectangles to be partially",0
"transparent, but this is not straightforward, and the alpha is ignored",0
"here.  But maybe if I leave it here and wish hard enough, someday it",0
will work.,0
,0
otherDetectionsColors = ['dimgray'],0
Sort detections within a directory so nearby detections are adjacent,0
"in the list, for faster review.",0
,0
"Can be None, 'xsort', or 'clustersort'",0
,0
* None sorts detections chronologically by first occurrence,0
* 'xsort' sorts detections from left to right,0
* 'clustersort' clusters detections and sorts by cluster,0
Only relevant if smartSort == 'clustersort',0
"The data table (Pandas DataFrame), as loaded from the input json file via",0
load_api_results(),0
"The other fields in the input json file, loaded via load_api_results()",0
The data table after modification,0
dict mapping folder names to whole rows from the data table,0
dict mapping filenames to rows in the master table,0
"An array of length nDirs, where each element is a list of DetectionLocation",0
objects for that directory that have been flagged as suspicious,0
"This is a bit of a hack right now, but for future-proofing, I don't want to call this",1
"to retrieve anything other than the highest-confidence detection, and I'm assuming this",0
"is already sorted, so assert() that.",0
It's not clear whether it's better to use instances[0].bbox or self.bbox,1
"here... they should be very similar, unless iouThreshold is very low.",0
self.bbox is a better representation of the overal DetectionLocation.,0
%% Helper functions,0
"We store detetions as x/y/w/h, rtree and pyqtree use l/b/r/t",0
"We store detetions as x/y/w/h, rtree and pyqtree use l/b/r/t",0
%% Sort a list of candidate detections to make them visually easier to review,0
Just sort by the X location of each box,0
"Prepare a list of points to represent each box,",0
that's what we'll use for clustering,0
Upper-left,0
"points.append([det.bbox[0],det.bbox[1]])",0
Center,0
"Labels *could* be any unique labels according to the docs, but in practice",0
they are unique integers from 0:nClusters,0
Make sure the labels are unique incrementing integers,0
Store the label assigned to each cluster,0
"Now sort the clusters by their x coordinate, and re-assign labels",0
so the labels are sortable,0
"Compute the centroid for debugging, but we're only going to use the x",0
"coordinate.  This is the centroid of points used to represent detections,",0
which may be box centers or box corners.,0
old_cluster_label_to_new_cluster_label[old_cluster_label] =\,0
new_cluster_labels[old_cluster_label],0
%% Look for matches (one directory),0
List of DetectionLocations,0
candidateDetections = [],0
Create a tree to store candidate detections,0
For each image in this directory,0
,0
iDirectoryRow = 0; row = rows.iloc[iDirectoryRow],0
,0
"iDirectoryRow is a pandas index, so it may not start from zero;",0
"for debugging, we maintain i_iteration as a loop index.",0
print('Searching row {} of {} (index {}) in dir {}'.\,0
"format(i_iteration,len(rows),iDirectoryRow,dirName))",0
Don't bother checking images with no detections above threshold,0
"Array of dicts, where each element is",0
{,0
"'category': '1',  # str value, category ID",0
"'conf': 0.926,  # confidence of this detections",0
,0
"# (x_min, y_min) is upper-left, all in relative coordinates",0
"'bbox': [x_min, y_min, width_of_box, height_of_box]",0
,0
},0
For each detection in this image,0
"This is no longer strictly true; I sometimes run RDE in stages, so",0
some probabilities have already been made negative,0
,0
assert confidence >= 0.0 and confidence <= 1.0,0
Optionally exclude some classes from consideration as suspicious,0
Is this detection too big to be suspicious?,0
print('Illegal zero-size bounding box on image {}'.format(filename)),0
These are relative coordinates,0
print('Ignoring very small detection with area {}'.format(area)),0
print('Ignoring very large detection with area {}'.format(area)),0
This will return candidates of all classes,0
For each detection in our candidate list,0
Don't match across categories,0
Is this a match?,0
"If so, add this example to the list for this detection",0
We *don't* break here; we allow this instance to possibly,0
match multiple candidates.  There isn't an obvious right or,0
wrong here.,0
...for each detection on our candidate list,0
"If we found no matches, add this to the candidate list",0
candidateDetections.append(candidate),0
pyqtree,0
...for each detection,0
...for each row,0
Get all candidate detections,0
print('Found {} candidate detections for folder {}'.format(,0
"len(candidateDetections),dirName))",0
"For debugging only, it's convenient to have these sorted",0
as if they had never gone into a tree structure.  Typically,0
this is in practce a sort by filename.,0
...def find_matches_in_directory(dirName),0
"%% Update the detection table based on suspicious results, write .csv output",0
"An array of length nDirs, where each element is a list of DetectionLocation",0
objects for that directory that have been flagged as suspicious,0
For each directory,0
For each suspicious detection group in this directory,0
For each instance of this suspicious detection,0
This should match the bbox for the detection event,0
The bbox for this instance should be almost the same as the bbox,0
"for this detection group, where ""almost"" is defined by the IOU",0
threshold.,0
if iou < options.iouThreshold:,0
"print('IOU warning: {},{}'.format(iou,options.iouThreshold))",0
Make sure the bounding box matches,0
"Make the probability negative, if it hasn't been switched by",0
another bounding box,0
...for each instance,0
...for each detection,0
...for each directory,0
Update maximum probabilities,0
For each row...,0
No longer strictly true; sometimes I run RDE on RDE output,0
assert maxPOriginal >= 0,0
We should only be making detections *less* likely in this process,0
"If there was a meaningful change, count it",0
"Negative probabilities should be the only reason maxP changed, so",0
we should have found at least one negative value if we reached,0
this point.,0
...if there was a meaningful change to the max probability for this row,0
...for each row,0
If we're also writing output...,0
"...def update_detection_table(RepeatDetectionResults,options)",0
%% Main function,0
#%% Input handling,0
Validate some options,0
Load the filtering file,0
Load the same options we used when finding repeat detections,0
...except for things that explicitly tell this function not to,0
find repeat detections.,0
...if we're loading from an existing filtering file,0
Check early to avoid problems with the output folder,0
"Load file to a pandas dataframe.  Also populates 'max_detection_conf', even if it's",0
not present in the .json file.,0
detectionResults[detectionResults['failure'].notna()],0
"Before doing any real work, make sure we can *probably* access images",0
"This is just a cursory check on the first image, but it heads off most",0
"problems related to incorrect mount points, etc.  Better to do this before",0
spending 20 minutes finding repeat detections.,0
#%% Separate files into directories,0
This will be a map from a directory name to smaller data frames,0
This is a mapping back into the rows of the original table,0
iRow = 0; row = detectionResults.iloc[0],0
Create a new DataFrame with just this row,0
rowsByDirectory[dirName] = pd.DataFrame(row),0
...for each unique detection,0
Convert lists of rows to proper DataFrames,0
#% Look for matches (or load them from file),0
"Are we actually looking for matches, or just loading from a file?",0
length-nDirs list of lists of DetectionLocation objects,0
We're actually looking for matches...,0
"We get slightly nicer progress bar behavior using threads, by passing a pbar",0
object and letting it get updated.  We can't serialize this object across,0
processes.,0
#%% Find suspicious locations based on match results,0
For each directory,0
,0
iDir = 51,0
A list of DetectionLocation objects,0
A list of DetectionLocation objects,0
occurrenceList is a list of file/detection pairs,0
Sort the above-threshold detections for easier review,0
...for each directory,0
If we're just loading detections from a file...,0
"We're skipping detection-finding, but to see which images are actually legit false",0
"positives, we may be looking for physical files or loading from a text file.",0
For each directory,0
iDir = 0; detections = suspiciousDetections[0],0
,0
"suspiciousDetections is an array of DetectionLocation objects,",0
one per directory.,0
For each detection that was present before filtering,0
iDetection = 0; detection = detections[iDetection],0
Are we checking the directory to see whether detections were actually false,0
"positives, or reading from a list?",0
Is the image still there?,0
"If not, remove this from the list of suspicious detections",0
...for each detection,0
...for each directory,0
...if we are/aren't finding detections (vs. loading from file),0
Create filtering directory,0
iDir = 0; suspiciousDetectionsThisDir = suspiciousDetections[iDir],0
suspiciousDetectionsThisDir is a list of DetectionLocation objects,0
iDetection = 0; detection = suspiciousDetectionsThisDir[0],0
Sort instances in descending order by confidence,0
Choose the highest-confidence index,0
Should we render (typically in a very light color) detections,0
*other* than the one we're highlighting here?,0
Render other detections first (typically in a thin+light box),0
Now render the example detection (on top of at least one,0
of the other detections),0
This converts the *first* instance to an API standard detection;,0
"because we just sorted this list in descending order by confidence,",0
this is the highest-confidence detection.,0
...if we are/aren't rendering other bounding boxes,0
...for each detection in this folder,0
...for each folder,0
Write out the detection index,0
...if we're writing filtering info,0
...find_repeat_detections(),0
#######,0
,0
find_repeat_detections.py,0
,0
"If you want to use this script, we recommend that you read the user's guide:",0
,0
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms,0
,0
"Really, don't try to run this script without reading the user's guide, you'll think",0
it's more magical than it is.,0
,0
"This script looks through a sequence of detections in the API output json file, and finds",0
"candidates that might be ""repeated false positives"", i.e. that random branch that the",0
detector thinks is an animal/person/vehicle.,0
,0
"Typically after running this script, you would do a manual step to remove",0
"true positives, then run remove_repeat_detections to produce a final output file.",0
,0
There's no way that statement was self-explanatory; see the user's guide.,0
,0
#######,0
%% Constants and imports,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
Convert to an options object,0
,0
If a request has been sent to AML for batch scoring but the monitoring thread of the API was,0
"interrupted (uncaught exception or having to re-start the API container), we could manually",0
"aggregate results from each shard using this script, assuming all jobs submitted to AML have finished.",0
,0
Need to have set environment variables STORAGE_ACCOUNT_NAME and STORAGE_ACCOUNT_KEY to those of the,0
"storage account backing the API. Also need to adjust the INTERNAL_CONTAINER, AML_CONTAINER and",0
AML_CONFIG fields in api_core/orchestrator_api/api_config.py to match the instance of the API that this,0
request was submitted to.,0
,0
May need to change the import statement in api_core/orchestrator_api/orchestrator.py,0
"""from sas_blob_utils import SasBlob"" to",0
"""from .sas_blob_utils import SasBlob"" to not confuse with the module in AI4Eutils;",0
"and change ""import api_config"" to",0
"""from api.batch_processing.api_core.orchestrator_api import api_config""",0
Execute this script from the root of the repository. You may need to add the repository to PYTHONPATH.,0
"list_jobs_submitted cannot be serialized (""can't pickle _thread.RLock objects ""), but",0
do not need it for aggregating results,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%% helper classes and functions,0
TODO log exception when we have more telemetry,1
TODO check that the expiry date of input_container_sas is at least a month,0
into the future,0
"if no permission specified explicitly but has an access policy, assumes okay",0
TODO - check based on access policy as well,1
return current UTC time as a string in the ISO 8601 format (so we can query by,0
timestamp in the Cosmos DB job status table.,0
example: '2021-02-08T20:02:05.699689Z',0
"image_paths will have length at least 1, otherwise would have ended before this step",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
a job moves from created to running/problem after the Batch Job has been submitted,0
"job_id should be unique across all instances, and is also the partition key",0
TODO do not read the entry first to get the call_params when the Cosmos SDK add a,1
patching functionality:,0
https://feedback.azure.com/forums/263030-azure-cosmos-db/suggestions/6693091-be-able-to-do-partial-updates-on-document,0
need to retain other fields in 'status' to be able to restart monitoring thread,0
retain existing fields; update as needed,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
sentinel should change if new configurations are available,0
configs have not changed,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Gunicorn logger handler will get attached if needed in server.py,0
set for all tasks in the job,0
cannot execute the scoring script that is in the mounted directory; has to be copied to cwd,0
not luck giving the commandline arguments via formatted string - set as env vars instead,0
form shards of images and assign each shard to a Task,0
for persisting stdout and stderr,0
persist stdout and stderr (will be removed when node removed),0
paths are relative to the Task working directory,0
can also just upload on failure,0
first try submitting Tasks,0
retry submitting Tasks,0
Change the Job's on_all_tasks_complete option to 'terminateJob' so the Job's status changes automatically,0
after all submitted tasks are done,0
This is so that we do not take up the quota for active Jobs in the Batch account.,0
return type: TaskAddCollectionResult,0
actually we should probably only re-submit if it's a server_error,0
docs: # https://docs.microsoft.com/en-us/rest/api/batchservice/odata-filters-in-batch#list-tasks,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%% Flask app,0
reference: https://trstringer.com/logging-flask-gunicorn-the-manageable-way/,0
%% Helper classes,0
%% Flask endpoints,0
required params,0
can be an URL to a file not hosted in an Azure blob storage container,0
"if use_url, then images_requested_json_sas is required",0
optional params,0
check model_version is among the available model versions,0
check request_name has only allowed characters,0
optional params for telemetry collection - logged to status table for now as part of call_params,0
All API instances / node pools share a quota on total number of active Jobs;,0
we cannot accept new Job submissions if we are at the quota,0
required fields,0
request_status is either completed or failed,0
the create_batch_job thread will stop when it wakes up the next time,0
"Fix for Zooniverse - deleting any ""-"" characters in the job_id",0
"If the status is running, it could be a Job submitted before the last restart of this",0
"API instance. If that is the case, we should start to monitor its progress again.",0
WARNING model_version could be wrong (a newer version number gets written to the output file) around,0
"the time that  the model is updated, if this request was submitted before the model update",0
and the API restart; this should be quite rare,0
conform to previous schemes,0
%% undocumented endpoints,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Gunicorn logger handler will get attached if needed in server.py,0
request_name and request_submission_timestamp are for appending to,0
output file names,0
image_paths can be a list of strings (Azure blob names or public URLs),0
"or a list of length-2 lists where each is a [image_id, metadata] pair",0
Case 1: listing all images in the container,0
- not possible to have attached metadata if listing images in a blob,0
list all images to process,0
+ 1 so if the number of images listed > MAX_NUMBER_IMAGES_ACCEPTED_PER_JOB,0
we will know and not proceed,0
Case 2: user supplied a list of images to process; can include metadata,0
filter down to those conforming to the provided prefix and accepted suffixes (image file types),0
prefix is case-sensitive; suffix is not,0
"Although urlparse(p).path preserves the extension on local paths, it will not work for",0
"blob file names that contains ""#"", which will be treated as indication of a query.",0
"If the URL is generated via Azure Blob Storage, the ""#"" char will be properly encoded",0
apply the first_n and sample_n filters,0
OK if first_n > total number of images,0
sample by shuffling image paths and take the first sample_n images,0
"upload the image list to the container, which is also mounted on all nodes",0
all sharding and scoring use the uploaded list,0
now request_status moves from created to running,0
an extra field to allow the monitoring thread to restart after an API restart: total number of tasks,0
also record the number of images to process for reporting,0
start the monitor thread with the same name,0
"both succeeded and failed tasks are marked ""completed"" on Batch",0
"preserving format from before, but SAS URL to 'failed_images' and 'images' are no longer provided",0
"failures should be contained in the output entries, indicated by an 'error' field",0
"when people download this, the timestamp will have : replaced by _",0
check if the result blob has already been written (could be another instance of the API / worker thread),0
"and if so, skip aggregating and uploading the results, and just generate the SAS URL, which",0
could be needed still if the previous request_status was `problem`.,0
upload the output JSON to the Job folder,0
%% Helper functions *copied* from ct_utils.py and visualization/visualization_utils.py,0
"Determine the factor, which shifts the decimal point of x",0
just behind the last significant digit,0
"Shift decimal point by multiplicatipon with factor, flooring, and",0
division by factor,0
PIL.Image.convert() returns a converted copy of this image,0
alter orientation as needed according to EXIF tag 0x112 (274) for Orientation,0
https://gist.github.com/dangtrinhnt/a577ece4cbe5364aad28,0
https://www.media.mit.edu/pia/Research/deepview/exif.html,0
"%% TFDetector class, an unmodified *copy* of the class in detection/tf_detector.py,",0
so we do not have to import the packages required by run_detector.py,0
Number of decimal places to round to for confidence and bbox coordinates,0
"MegaDetector was trained with batch size of 1, and the resizing function is a part",0
of the inference graph,0
An enumeration of failure reasons,0
"change from [y1, x1, y2, x2] to [x1, y1, width, height]",0
convert numpy floats to Python floats,0
need to change the above line to the following if supporting a batch size > 1 and resizing to the same size,0
"np_images = [np.asarray(image, np.uint8) for image in images]",0
"images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)",0
performs inference,0
our batch size is 1; need to loop the batch dim if supporting batch size > 1,0
%% Scoring script,0
determine if there is metadata attached to each image_id,0
information to determine input and output locations,0
other parameters for the task,0
test that we can write to output path; also in case there is no image to process,0
list images to process,0
"items in this list can be strings or [image_id, metadata]",0
model path,0
"Path to .pb TensorFlow detector model file, relative to the",0
models/megadetector_copies folder in mounted container,0
score the images,0
,0
manage_video_batch.py,0
,0
Notebook-esque script to manage the process of running a local batch of videos,0
through MD.  Defers most of the heavy lifting to manage_local_batch.py .,0
,0
%% Imports and constants,0
%% Split videos into frames,0
"%% List frame files, break into folders",0
Find unique (relative) folders,0
fn = frame_files[0],0
%% List videos,0
%% Check for videos that are missing entirely,0
list(folder_to_frame_files.keys())[0],0
video_filenames[0],0
fn = video_filenames[0],0
%% Check for videos with very few frames,0
%% Print the list of videos that are problematic,0
%% Process images like we would for any other camera trap job,0
"...typically using manage_local_batch.py, but do this however you like, as long",0
as you get a results file at the end.,0
,0
"If you do RDE, remember to use the second folder from the bottom, rather than the",0
bottom-most folder.,0
%% Convert frame results to video results,0
%% Confirm that the videos in the .json file are what we expect them to be,0
%% Scrap,0
%% Test a possibly-broken video,0
%% List videos in a folder,0
%% Imports,0
%% Constants,0
%% Classes,0
class variables,0
"instance variables, in order of when they are typically set",0
Leaving this commented out to remind us that we don't want this check here; let,0
the API fail on these images.  It's a huge hassle to remove non-image,0
files.,0
,0
for path_or_url in images_list:,0
if not is_image_file_or_url(path_or_url):,0
raise ValueError('{} is not an image'.format(path_or_url)),0
Commented out as a reminder: don't check task status (which is a rest API call),0
in __repr__; require the caller to explicitly request status,0
"status=getattr(self, 'status', None))",0
estimate # of failed images from failed shards,0
Download all three JSON urls to memory,0
Remove files that were submitted but don't appear to be images,0
assert all(is_image_file_or_url(s) for s in submitted_images),0
Diff submitted and processed images,0
Confirm that the procesed images are a subset of the submitted images,0
https://www.geeksforgeeks.org/break-list-chunks-size-n-python/,0
%% Interactive driver,0
%%,0
%%,0
%%,0
%% Imports and constants,0
%% Constants I set per script,0
## Required,0
base_output_folder_name = os.path.expanduser('~/postprocessing/' + organization_name_short),0
Shared Access Signature (SAS) tokens for the Azure Blob Storage container.,0
Leading question mark is optional.,0
,0
The read-only token is used for accessing images; the write-enabled token is,0
used for writing file lists.,0
## Typically left as default,0
"Pre-pended to all folder names/prefixes, if they're defined below",0
"This is how we break the container up into multiple taskgroups, e.g., for",0
separate surveys. The typical case is to do the whole container as a single,0
taskgroup.,0
"If your ""folders"" are really logical folders corresponding to multiple folders,",0
map them here,0
"A list of .json files to load images from, instead of enumerating.  Formatted as a",0
"dictionary, like folder_prefixes.",0
This is only necessary if you will be performing postprocessing steps that,0
"don't yet support SAS URLs, specifically the ""subsetting"" step, or in some",0
cases the splitting of files into multiple output directories for,0
empty/animal/vehicle/people.,0
,0
"For those applications, you will need to mount the container to a local drive.",0
For this case I recommend using rclone whether you are on Windows or Linux;,0
rclone is much easier than blobfuse for transient mounting.,0
,0
"But most of the time, you can ignore this.",0
"Supported model_versions: '4', '3', '4_prelim'",0
,0
Also available at the /supported_model_versions and /default_model_version,0
endpoints,0
,0
"Unless you have any specific reason to set this to a non-default value, leave",0
"it at the default, which as of 2020.04.28 is MegaDetector 4.1",0
,0
"additional_task_args = {""model_version"":""4_prelim""}",0
,0
"file_lists_by_folder will contain a list of local JSON file names,",0
each JSON file contains a list of blob names corresponding to an API taskgroup,0
"%% Derived variables, path setup",0
local folders,0
Turn warnings into errors if more than this many images are missing,0
%% Support functions,0
"scheme, netloc, path, query, fragment",0
%% Read images from lists or enumerate blobs to files,0
folder_name = folder_names[0],0
"Load file lists for this ""folder""",0
,0
file_list = input_file_lists[folder][0],0
Write to file,0
A flat list of blob paths for each folder,0
folder_name = folder_names[0],0
"If we don't/do have multiple prefixes to enumerate for this ""folder""",0
"If this is intended to be a folder, it needs to end in '/', otherwise",0
files that start with the same string will match too,0
...for each prefix,0
Write to file,0
...for each folder,0
%% Some just-to-be-safe double-checking around enumeration,0
Make sure each folder has at least one image matched; the opposite is usually a sign of a copy/paste issue,0
...for each image,0
...for each prefix,0
...for each folder,0
Make sure each image comes from one of our folders; the opposite is usually a sign of a bug up above,0
...for each prefix,0
...for each folder,0
...for each image,0
%% Divide images into chunks for each folder,0
The JSON file at folder_chunks[i][j] corresponds to task j of taskgroup i,0
list_file = file_lists_by_folder[0],0
"%% Create taskgroups and tasks, and upload image lists to blob storage",0
periods not allowed in task names,0
%% Generate API calls for each task,0
clipboard.copy(request_strings[0]),0
clipboard.copy('\n\n'.join(request_strings)),0
%% Run the tasks (don't run this cell unless you are absolutely sure!),0
I really want to make sure I'm sure...,0
%% Estimate total time,0
Around 0.8s/image on 16 GPUs,0
%% Manually create task groups if we ran the tasks manually,0
%%,0
"%% Write task information out to disk, in case we need to resume",0
%% Status check,0
print(task.id),0
%% Resume jobs if this notebook closes,0
%% For multiple tasks (use this only when we're merging with another job),0
%% For just the one task,0
%% Load into separate taskgroups,0
p = task_cache_paths[0],0
%% Typically merge everything into one taskgroup,0
"%% Look for failed shards or missing images, start new tasks if necessary",0
List of lists of paths,0
i_taskgroup = 0; taskgroup = taskgroups[i_taskgroup];,0
"Make a copy, because we append to taskgroup",0
i_task = 0; task = tasks[i_task],0
Each taskgroup corresponds to one of our folders,0
Check that we have (almost) all the images,0
Now look for failed images,0
Write it out as a flat list as well (without explanation of failures),0
...for each task,0
...for each task group,0
%% Pull results,0
i_taskgroup = 0; taskgroup = taskgroups[i_taskgroup]; task = taskgroup[0],0
Each taskgroup corresponds to one of our folders,0
...for each task,0
...for each task group,0
%% Combine results from task groups into final output files,0
Check that we have (almost) all the images,0
The only reason we should ever have a repeated request is the case where an,0
"image was missing and we reprocessed it, or where it failed and later succeeded",0
"There may be non-image files in the request list, ignore those",0
Something has gone bonkers if there are images in the results that,0
aren't in the request,0
...for each folder,0
%% Post-processing (no ground truth),0
i_folder = 0; folder_name_raw = folder_names[i_folder],0
"Omit some pages from the output, useful when animals are rare",0
%% Manual processing follows,0
,0
"Everything after this should be considered mostly manual, and no longer includes",0
looping over folders.,0
,0
%% RDE (sample directory collapsing),0
"In this example, the camera created folders called ""100EK113"", ""101EK113"", etc., for every N images",0
"%% Repeat detection elimination, phase 1",0
"Deliberately leaving these imports here, rather than at the top, because this cell is not",0
typically executed,0
To invoke custom collapsing of folders for a particular manufacturer's naming scheme,0
options.customDirNameFunction = remove_overflow_folders,0
Exclude people and vehicles from RDE,0
"options.excludeClasses = [2,3]",0
%% Manual RDE step,0
# DELETE THE VALID DETECTIONS ##,0
%% Re-filtering,0
%% Post-processing (post-RDE),0
i_folder = 0; folder_name_raw = folder_names[i_folder],0
"Omit some pages from the output, useful when animals are rare",0
"print('No RDE file available for {}, skipping'.format(folder_name))",0
continue,0
%% Create a new category for large boxes,0
%% Subsetting,0
i_folder = 0; folder_name = folders[i_folder],0
%% String replacement,0
%% Folder splitting,0
i_folder = 0; folder_name_raw = folder_names[i_folder],0
api_output_file = folder_name_to_combined_output_file[folder_name],0
%% Imports and constants,0
from ai4eutils,0
To specify a non-default confidence threshold for including detections in the .json file,0
Turn warnings into errors if more than this many images are missing,0
Only relevant when we're using a single GPU,0
"Specify a target image size when running MD... strongly recommended to leave this at ""None""",0
Only relevant when running on CPU,0
OS-specific script line continuation character,0
OS-specific script comment character,0
"Prefer threads on Windows, processes on Linux",0
"This is for things like image rendering, not for MegaDetector",0
Should we use YOLOv5's val.py instead of run_detector_batch.py?,1
Directory in which to run val.py.  Only relevant if use_yolo_inference_scripts is True.,0
Should we remove intermediate files used for running YOLOv5's val.py?,0
,0
Only relevant if use_yolo_inference_scripts is True.,0
Should we apply YOLOv5's augmentation?  Only allowed when use_yolo_inference_scripts,0
is True.,0
%% Constants I set per script,0
Optional descriptor,0
model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v5.0.0/md_v5b.0.0.pt'),0
model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v4.1.0/md_v4.1.0.pb'),0
"Number of jobs to split data into, typically equal to the number of available GPUs",0
Only used to print out a time estimate,0
"%% Derived variables, constant validation, path setup",0
%% Enumerate files,0
%% Load files from prior enumeration,0
%% Divide images into chunks,0
%% Estimate total time,0
%% Write file lists,0
%% Generate commands,0
"A list of the scripts tied to each GPU, as absolute paths.  We'll write this out at",0
the end so each GPU's list of commands can be run at once.  Generally only used when,0
"running lots of small batches via YOLOv5's val.py, which doesn't support checkpointing.",0
i_task = 0; task = task_info[i_task],0
Generate the script to run MD,0
Check whether this output file exists,0
Generate the script to resume from the checkpoint (only supported with MD inference code),0
...for each task,0
Write out a script for each GPU that runs all of the commands associated with,0
that GPU.  Typically only used when running lots of little scripts in lieu,0
of checkpointing.,0
...for each GPU,0
%% Run the tasks,0
%%% Run the tasks (commented out),0
i_task = 0; task = task_info[i_task],0
"This will write absolute paths to the file, we'll fix this later",1
...for each chunk,0
...if False,0
"%% Load results, look for failed or missing images in each task",0
i_task = 0; task = task_info[i_task],0
im = task_results['images'][0],0
...for each task,0
%% Merge results files and make images relative,0
im = combined_results['images'][0],0
%% Post-processing (pre-RDE),0
options.sample_seed = 0,0
"Omit some pages from the output, useful when animals are rare",0
%% RDE (sample directory collapsing),0
"In this example, the camera created folders called ""100EK113"", ""101EK113"", etc., for every N images",0
%%,0
%%,0
%%,0
relativePath = image_filenames[0],0
"%% Repeat detection elimination, phase 1",0
"Deliberately leaving these imports here, rather than at the top, because this",0
cell is not typically executed,0
options.minSuspiciousDetectionSize = 0.05,0
This will cause a very light gray box to get drawn around all the detections,0
we're *not* considering as suspicious.,0
options.lineThickness = 5,0
options.boxExpansion = 8,0
To invoke custom collapsing of folders for a particular manufacturer's naming scheme,0
options.customDirNameFunction = remove_overflow_folders,0
Exclude people and vehicles from RDE,0
"options.excludeClasses = [2,3]",0
options.maxImagesPerFolder = 50000,0
options.includeFolders = ['a/b/c'],0
options.excludeFolder = ['a/b/c'],0
"Can be None, 'xsort', or 'clustersort'",0
import clipboard; clipboard.copy(os.path.dirname(suspiciousDetectionResults.filterFile)),0
path_utils.open_file(os.path.dirname(suspiciousDetectionResults.filterFile)),0
%% Manual RDE step,0
# DELETE THE VALID DETECTIONS ##,0
%% Re-filtering,0
%% Post-processing (post-RDE),0
options.sample_seed = 0,0
"Omit some pages from the output, useful when animals are rare",0
"%% Run MegaClassifier (actually, write out a script that runs MegaClassifier)",0
"This is just passed along to the metadata in the output file, it has no impact",0
on how the classification scripts run.,0
#%% Set up environment,0
commands.append('cd CameraTraps/classification\n'),0
commands.append('conda activate cameratraps-classifier\n'),0
#%% Crop images,0
fn = input_files[0],0
#%% Run classifier,0
fn = input_files[0],0
#%% Remap classifier outputs,0
fn = input_files[0],0
#%% Merge classification and detection outputs,0
fn = input_files[0],0
#%% Write  out classification script,0
"%% Run a non-MegaClassifier classifier (i.e., a classifier with no output mapping)",0
"This is just passed along to the metadata in the output file, it has no impact",0
on how the classification scripts run.,0
#%% Set up environment,0
#%% Crop images,0
fn = input_files[0],0
#%% Run classifier,0
fn = input_files[0],0
#%% Merge classification and detection outputs,0
fn = input_files[0],0
#%% Write everything out,0
%% Run the classifier(s) via the .sh script(s) or batch file(s) we just wrote,0
...,0
%% Within-image classification smoothing,0
,0
Only count detections with a classification confidence threshold above,0
"*classification_confidence_threshold*, which in practice means we're only",0
looking at one category per detection.,0
,0
If an image has at least *min_detections_above_threshold* such detections,0
"in the most common category, and no more than *max_detections_secondary_class*",0
"in the second-most-common category, flip all detections to the most common",0
category.,0
,0
"Optionally treat some classes as particularly unreliable, typically used to overwrite an",0
"""other"" class.",0
,0
This cell also removes everything but the non-dominant classification for each detection.,0
,0
How many detections do we need above the classification threshold to determine a dominant category,0
for an image?,0
"Even if we have a dominant class, if a non-dominant class has at least this many classifications",0
"in an image, leave them alone.",0
"If the dominant class has at least this many classifications, overwrite ""other"" classifications",0
What confidence threshold should we use for assessing the dominant category in an image?,0
Which classifications should we even bother over-writing?,0
Detection confidence threshold for things we count when determining a dominant class,0
Which detections should we even bother over-writing?,0
"Before we do anything else, get rid of everything but the top classification",0
for each detection.,0
...for each detection in this image,0
...for each image,0
im = d['images'][0],0
...for each classification,0
...if there are classifications for this detection,0
...for each detection,0
"Handle a quirky special case: if the most common category is ""other"" and",0
"it's ""tied"" with the second-most-common category, swap them",0
secondary_count = category_to_count[keys[1]],0
The 'secondary count' is the most common non-other class,0
If we have at least *min_detections_to_overwrite_other* in a category that isn't,0
"""other"", change all ""other"" classifications to that category",0
...for each classification,0
...if there are classifications for this detection,0
...for each detection,0
"...if we should overwrite all ""other"" classifications",0
"At this point, we know we have a dominant category; change all other above-threshold",0
"classifications to that category.  That category may have been ""other"", in which",0
case we may have already made the relevant changes.,0
det = detections[0],0
...for each classification,0
...if there are classifications for this detection,0
...for each detection,0
...for each image,0
...for each file we want to smooth,0
"%% Post-processing (post-classification, post-within-image-smoothing)",0
classification_detection_file = classification_detection_files[1],0
%% Read EXIF data from all images,0
%% Prepare COCO-camera-traps-compatible image objects for EXIF results,0
import dateutil,0
"This is a standard format for EXIF datetime, and dateutil.parser",0
doesn't handle it correctly.,0
return dateutil.parser.parse(s),0
exif_result = exif_results[0],0
Currently we assume that each leaf-node folder is a location,0
"We collected this image this century, but not today, make sure the parsed datetime",0
jives with that.,0
,0
The latter check is to make sure we don't repeat a particular pathological approach,0
"to datetime parsing, where dateutil parses time correctly, but swaps in the current",0
date when it's not sure where the date is.,0
...for each exif image result,0
%% Assemble into sequences,0
Make a list of images appearing at each location,0
im = image_info[0],0
%% Load classification results,0
Map each filename to classification results for that file,0
%% Smooth classification results over sequences (prep),0
These are the only classes to which we're going to switch other classifications,0
Only switch classifications to the dominant class if we see the dominant class at least,0
this many times,0
"If we see more than this many of a class that are above threshold, don't switch those",0
classifications to the dominant class.,0
"If the ratio between a dominant class and a secondary class count is greater than this,",0
"regardless of the secondary class count, switch those classificaitons (i.e., ignore",0
max_secondary_class_classifications_above_threshold_for_class_smoothing).,0
,0
"This may be different for different dominant classes, e.g. if we see lots of cows, they really",0
"tend to be cows.  Less so for canids, so we set a higher ""override ratio"" for canids.",0
"If there are at least this many classifications for the dominant class in a sequence,",0
"regardless of what that class is, convert all 'other' classifications (regardless of",0
confidence) to that class.,0
"If there are at least this many classifications for the dominant class in a sequence,",0
"regardless of what that class is, classify all previously-unclassified detections",0
as that class.,0
Only count classifications above this confidence level when determining the dominant,0
"class, and when deciding whether to switch other classifications.",0
Confidence values to use when we change a detection's classification (the,0
original confidence value is irrelevant at that point),0
%% Smooth classification results over sequences (supporting functions),0
im = images_this_sequence[0],0
det = results_this_image['detections'][0],0
Only process animal detections,0
Only process detections with classification information,0
"We only care about top-1 classifications, remove everything else",0
Make sure the list of classifications is already sorted by confidence,0
...and just keep the first one,0
"Confidence values should be sorted within a detection; verify this, and ignore",0
...for each detection in this image,0
...for each image in this sequence,0
...top_classifications_for_sequence(),0
Count above-threshold classifications in this sequence,0
Sort the dictionary in descending order by count,0
"Handle a quirky special case: if the most common category is ""other"" and",0
"it's ""tied"" with the second-most-common category, swap them.",0
...def count_above_threshold_classifications(),0
%% Smooth classifications at the sequence level (main loop),0
Break if this token is contained in a filename (set to None for normal operation),0
i_sequence = 0; seq_id = all_sequences[i_sequence],0
Count top-1 classifications in this sequence (regardless of confidence),0
Handy debugging code for looking at the numbers for a particular sequence,0
Count above-threshold classifications for each category,0
"If our dominant category ID isn't something we want to smooth to, don't mess around with this sequence",0
"# Smooth ""other"" classifications ##",0
"By not re-computing ""max_count"" here, we are making a decision that the count used",0
"to decide whether a class should overwrite another class does not include any ""other""",0
classifications we changed to be the dominant class.  If we wanted to include those...,0
,0
sorted_category_to_count = count_above_threshold_classifications(classifications_this_sequence),0
max_count = get_first_value_from_sorted_dictionary(sorted_category_to_count),0
assert dominant_category_id == get_first_key_from_sorted_dictionary(sorted_category_to_count),0
# Smooth non-dominant classes ##,0
Don't flip classes to the dominant class if they have a large number of classifications,0
"Don't smooth over this class if there are a bunch of them, and the ratio",0
if primary to secondary class count isn't too large,0
Default ratio,0
Does this dominant class have a custom ratio?,0
# Smooth unclassified detections ##,0
...for each sequence,0
%% Write smoothed classification results,0
"%% Post-processing (post-classification, post-within-image-and-within-sequence-smoothing)",0
%% Zip .json files,0
%% 99.9% of jobs end here,0
Everything after this is run ad hoc and/or requires some manual editing.,0
%% Compare results files for different model versions (or before/after RDE),0
Choose all pairwise combinations of the files in [filenames],0
%% Merge in high-confidence detections from another results file,0
%% Create a new category for large boxes,0
"This is a size threshold, not a confidence threshold",0
size_options.categories_to_separate = [3],0
%% Preview large boxes,0
%% .json splitting,0
options.query = None,0
options.replacement = None,0
Reminder: 'n_from_bottom' with a parameter of zero is the same as 'bottom',0
%% Custom splitting/subsetting,0
i_folder = 0; folder_name = folders[i_folder],0
"This doesn't do anything in this case, since we're not splitting folders",0
options.make_folder_relative = True,0
%% String replacement,0
%% Splitting images into folders,0
%% Generate commands for a subset of tasks,0
i_task = 8,0
...for each task,0
%% End notebook: turn this script into a notebook (how meta!),0
Exclude everything before the first cell,0
Remove the first [first_non_empty_lines] from the list,0
Add the last cell,0
,0
xmp_integration.py,0
,0
"Tools for loading MegaDetector batch API results into XMP metadata, specifically",0
for consumption in digiKam:,0
,0
https://cran.r-project.org/web/packages/camtrapR/vignettes/camtrapr2.html,0
,0
%% Imports and constants,0
%% Class definitions,0
Folder where images are stored,0
.json file containing MegaDetector output,0
"String to remove from all path names, typically representing a",0
prefix that was added during MegaDetector processing,0
Optionally *rename* (not copy) all images that have no detections,0
above [rename_conf] for the categories in rename_cats from x.jpg to,0
x.check.jpg,0
"Comma-deleted list of category names (or ""all"") to apply the rename_conf",0
behavior to.,0
"Minimum detection threshold (applies to all classes, defaults to None,",0
i.e. 0.0,0
%% Functions,0
Relative image path,0
Absolute image path,0
List of categories to write to XMP metadata,0
Categories with above-threshold detections present for,0
this image,0
Maximum confidence for each category,0
Have we already added this to the list of categories to,0
write out to this image?,0
If we're supposed to compare to a threshold...,0
Else we treat *any* detection as valid...,0
Keep track of the highest-confidence detection for this class,0
If we're doing the rename/.check behavior...,0
Legacy code to rename files where XMP writing failed,0
%% Interactive/test driver,0
%%,0
%% Command-line driver,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Cosmos DB `batch-api-jobs` table for job status,0
"aggregate the number of images, country and organization names info from each job",0
submitted during yesterday (UTC time),0
create the card,0
,0
api_frontend.py,0
,0
"Defines the Flask app, which takes requests (one or more images) from",0
"remote callers and pushes the images onto the shared Redis queue, to be processed",0
by the main service in api_backend.py .,0
,0
%% Imports,0
%% Initialization,0
%% Support functions,0
Make a dict that the request_processing_function can return to the endpoint,0
function to notify it of an error,0
Verify that the content uploaded is not too big,0
,0
request.content_length is the length of the total payload,0
Verify that the number of images is acceptable,0
...def check_posted_data(request),0
%% Main loop,0
Check whether the request_processing_function had an error,0
Write images to temporary files,0
,0
TODO: read from memory rather than using intermediate files,1
"Submit the image(s) for processing by api_backend.py, who is waiting on this queue",0
TODO: convert to a blocking read and eliminate the sleep() statement in this loop,1
"image = Image.open(os.path.join(temp_direc, image_name))",0
...if we do/don't have a request available on the queue,0
...while(True),0
...def detect_sync(),0
%% Command-line driver,0
use --non-docker if you are testing without Docker,0
,0
python api_frontend.py --non-docker,0
# Camera trap real-time API configuration,0
"Full path to the temporary folder for image storage, only meaningful",0
within the Docker container,0
Upper limit on total content length (all images and parameters),0
Minimum confidence threshold for detections,0
Minimum confidence threshold for showing a bounding box on the output image,0
Use this when testing without Docker,0
,0
api_backend.py,0
,0
"Defines the model execution service, which pulls requests (one or more images)",0
"from the shared Redis queue, and runs them through the TF model.",0
,0
%% Imports,0
%% Initialization,0
%% Main loop,0
TODO: convert to a blocking read and eliminate the sleep() statement in this loop,1
Filter the detections by the confidence threshold,0
,0
"Each result is [ymin, xmin, ymax, xmax, confidence, category]",0
,0
"Coordinates are relative, with the origin in the upper-left",0
...if serialized_entry,0
...while(True),0
...def detect_process(),0
%% Command-line driver,0
use --non-docker if you are testing without Docker,0
,0
python api_frontend.py --non-docker,0
run detections on a test image to load the model,0
%%,0
Importing libraries,0
%%,0
%%,0
%%,0
%%,0
GPU configuration: set up GPUs based on availability and user specification,0
Environment variable setup for numpy multi-threading. It is important to avoid cpu and ram issues.,0
Load and set configurations from the YAML file,0
Set a global seed for reproducibility,0
"If the annotation directory does not have a data split, split the data first",0
Replace annotation dir from config with the directory containing the split files,0
Split the data according to the split type,0
"Get the path to the annotation files, and we only want to do this if we are not predicting",0
Crop test data (most likely we don't need this),0
Crop training data,0
Crop validation data,0
Dataset and algorithm loading based on the configuration,0
Logger setup based on the specified logger type,0
Callbacks for model checkpointing and learning rate monitoring,0
Trainer configuration in PyTorch Lightning,0
"Training, validation, or evaluation execution based on the mode",0
%%,0
%%,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%%,0
PyTorch imports,0
"Importing the model, dataset, transformations and utility functions from PytorchWildlife",0
Importing the utility function for saving cropped images,0
Setting the device to use for computations ('cuda' indicates GPU),0
Initializing the MegaDetectorV5 model for image detection,0
Performing batch detection on the images,0
Saving the detected objects as cropped images,0
%%,0
Read the original CSV file,0
Prepare a list to store new records for the new CSV,0
Process the data if the name of the file is in the dataframe,0
Save the crop into a new csv,0
Add record to the new CSV data,0
Create a DataFrame from the new records,0
Define the path for the new CSV file,0
Save the new DataFrame to CSV,0
# DATA SPLITTING,0
Load the data from the csv file,0
Separate the features and the targets,0
First split to separate out the test set,0
Adjust val_size to account for the initial split,0
Second split to separate out the validation set,0
"Combine features, labels, and classification back into dataframes",0
Create the output directory in case that it does not exist,0
Save the splits to new CSV files,0
Return the dataframes,0
Load the data from the csv file,0
Calculate train size based on val and test size,0
Get unique locations,0
Split locations into train and temp (temporary holding for val and test),0
Adjust the proportions for val and test based on the remaining locations,0
"Allocate images to train, validation, and test sets based on their location",0
Save the datasets to CSV files,0
Return the split datasets,0
Load the data from the csv file,0
Convert 'Photo_Time' from string to datetime,0
Calculate train size based on val and test size,0
Sort by 'Photo_Time' to ensure chronological order,0
Group photos into sequences based on a 30-second interval,0
Assign unique sequence IDs to each group,0
Get unique sequence IDs,0
Split sequence IDs into train and temp (temporary holding for val and test),0
Adjust the proportions for val and test based on the remaining sequences,0
"Allocate images to train, validation, and test sets based on their sequence ID",0
Save the datasets to CSV files,0
Return the split datasets,0
Exportable class names for external use,0
Applying the ResNet layers and operations,0
Initialize the network with the specified settings,0
Selecting the appropriate ResNet architecture and pre-trained weights,0
self.pretrained_weights = ResNet18_Weights.IMAGENET1K_V1,0
self.pretrained_weights = ResNet50_Weights.IMAGENET1K_V1,0
Constructing the feature extractor and classifier,0
Criterion for binary classification,0
Load pre-trained weights and adjust for the current model,0
init_weights = self.pretrained_weights.get_state_dict(progress=True),0
Load the weights into the feature extractor,0
Identify missing and unused keys in the loaded weights,0
Import necessary libraries,0
Exportable class names for external use,0
Define the allowed image extensions,0
Define normalization mean and standard deviation for image preprocessing,0
Define data transformations for training and validation datasets,0
Load data for prediction,0
"self.data = glob(os.path.join(self.img_root,""*.{}"".format(self.extension)))",0
Load data for training/validation,0
"Load datasets for different modes (training, validation, testing, prediction)",0
Calculate class counts and label mappings,0
Define parameters for the optimizer,0
Optimizer parameters for feature extraction,0
Optimizer parameters for the classifier,0
Setup optimizer and optimizer scheduler,0
Forward pass,0
Calculate loss,0
Forward pass,0
Forward pass,0
Concatenate outputs from all test steps,0
Calculate the metrics and save the output,0
Forward pass,0
Concatenate outputs from all predict steps,0
Compute the confusion matrix from true labels and predictions,0
Calculate class-wise accuracy (accuracy for each class),0
Calculate micro accuracy (overall accuracy),0
Calculate macro accuracy (mean of class-wise accuracies),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%%,0
Importing necessary basic libraries and modules,0
%%,0
PyTorch imports for tensor operations,0
%%,0
"Importing the models, transformations, and utility functions from PytorchWildlife",0
%%,0
Setting the device to use for computations ('cuda' indicates GPU),0
%%,0
Initializing the MegaDetectorV6 model for image detection,0
"Valid versions are MDV6-yolov9-c, MDV6-yolov9-e, MDV6-yolov10-c, MDV6-yolov10-e or MDV6-rtdetr-c",0
Uncomment the following line to use MegaDetectorV5 instead of MegaDetectorV6,0
"detection_model = pw_detection.MegaDetectorV5(device=DEVICE, pretrained=True, version=""a"")",0
%%,0
Initializing the model for image classification,0
%%,0
Initializing a box annotator for visualizing detections,0
Processing the video and saving the result with annotated detections and classifications,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Importing necessary basic libraries and modules,0
PyTorch imports,0
"Importing the model, dataset, transformations and utility functions from PytorchWildlife",0
Importing basic libraries,0
"Importing the models, dataset, transformations, and utility functions from PytorchWildlife",0
Setting the device to use for computations ('cuda' indicates GPU),0
Initializing a supervision box annotator for visualizing detections,0
Create a temp folder,0
Initializing the detection and classification models,0
Defining functions for different detection scenarios,0
Create an exception for custom weights,0
"If the detection model is HerdNet, use dot annotator, else use box annotator",0
Herdnet receives both clf and det confidence thresholds,0
Only run classifier when detection class is animal,0
Clean the temp folder if it contains files,0
Check the contents of the extracted folder,0
If the detection model is HerdNet set batch_size to 1,0
Building Gradio UI,0
The timelapse checkbox is only visible when the detection model is not HerdNet,0
Show timelapsed checkbox only when detection model is not HerdNet,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%%,0
Importing necessary basic libraries and modules,0
PyTorch imports,0
%%,0
"Importing the model, dataset, transformations and utility functions from PytorchWildlife",0
%%,0
Setting the device to use for computations ('cuda' indicates GPU),0
%%,0
Initializing the MegaDetectorV6 model for image detection,0
"Valid versions are MDV6-yolov9-c, MDV6-yolov9-e, MDV6-yolov10-c, MDV6-yolov10-e or MDV6-rtdetr-c",0
Uncomment the following line to use MegaDetectorV5 instead of MegaDetectorV6,0
"detection_model = pw_detection.MegaDetectorV5(device=DEVICE, pretrained=True, version=""a"")",0
%% Single image detection,0
Specifying the path to the target image TODO: Allow argparsing,0
Performing the detection on the single image,0
Saving the detection results,0
Saving the detected objects as cropped images,0
%% Batch detection,0
Specifying the folder path containing multiple images for batch detection,0
Performing batch detection on the images,0
%% Output to annotated images,0
Saving the batch detection results as annotated images,0
%% Output to cropped images,0
Saving the detected objects as cropped images,0
%% Output to JSON results,0
Saving the detection results in JSON format,0
Saving the detection results in timelapse JSON format,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%%,0
Importing necessary basic libraries and modules,0
PyTorch imports,0
%%,0
"Importing the model, dataset, transformations and utility functions from PytorchWildlife",0
%%,0
Setting the device to use for computations ('cuda' indicates GPU),0
%%,0
Initializing the HerdNet model for image detection,0
"If you want to use ennedi dataset weigths, you can use the following line:",0
"detection_model = pw_detection.HerdNet(device=DEVICE, version=""ennedi"")",0
%% Single image detection,0
Performing the detection on the single image,0
%% Output to annotated images,0
Saving the batch detection results as annotated images,0
%% Batch image detection,0
Specifying the folder path containing multiple images for batch detection,0
Performing batch detection on the images,0
%% Output to annotated images,0
Saving the batch detection results as annotated images,0
Saving the detection results in JSON format,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%%,0
Importing necessary basic libraries and modules,0
PyTorch imports,0
%% Argument parsing,0
%%,0
Setting the device to use for computations ('cuda' indicates GPU),0
%%,0
Initializing the MegaDetectorV6 model for image detection,0
"Valid versions are MDV6-yolov9-c, MDV6-yolov9-e, MDV6-yolov10-c, MDV6-yolov10-e or MDV6-rtdetr-c",0
Uncomment the following line to use MegaDetectorV5 instead of MegaDetectorV6,0
"detection_model = pw_detection.MegaDetectorV5(device=DEVICE, pretrained=True, version=""a"")",0
%% Batch detection,0
Performing batch detection on the images,0
%% Output to JSON results,0
Saving the detection results in JSON format,0
Separate the positive and negative detections through file copying:,0
This will read version from pyproject.toml,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Making the DetectionImageFolder class available for import from this module,0
Define the allowed image extensions,0
Get image filename and path,0
Load and convert image to RGB,0
Apply transformation if specified,0
Only run recognition on animal detections,0
Get image path and corresponding bbox xyxy for cropping,0
Load and crop image with supervision,0
Apply transformation if specified,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Making the provided classes available for import from this module,0
Convert PIL Image to Torch Tensor,0
Original shape,0
New shape,0
Scale ratio (new / old) and compute padding,0
Resize image,0
Pad image,0
Convert the image to a PyTorch tensor and normalize it,0
Resize and pad the image using a customized letterbox function.,0
Normalization constants,0
Define the sequence of transformations,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
!!! Output paths need to be optimized !!!,1
!!! Output paths need to be optimized !!!,1
Category filtering,0
if not all([x in exclude_category_ids for x in category]):,0
Category filtering,0
if not all([x in exclude_category_ids for x in category]):,0
if not all([x in exclude_category_ids for x in category_id_list]):,0
Find classifications for this detection,0
Load JSON data from the file,0
Ensure the destination directories exist,0
Process each image detection,0
Check if there is any category '0' with confidence above the threshold,0
Construct the source and destination file paths,0
Copy the file to the appropriate directory,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Importing basic libraries,0
Placeholder class-level attributes to be defined in derived classes,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%%,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Importing basic libraries,0
self.predictor.args.device = device # Will uncomment later,0
Creating a DataLoader for batching and parallel processing of the images,0
Normalize the coordinates for timelapse compatibility,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Importing basic libraries,0
Creating a DataLoader for batching and parallel processing of the images,0
Normalize the coordinates for timelapse compatibility,0
backbone,0
"self.cls_dla_up = dla_modules.DLAUp(channels[-3:], scales=scales[:3])",0
bottleneck conv,0
localization head,0
classification head,0
decode_cls = self.cls_dla_up(encode[-3:]),0
clsmap = self.cls_head(decode_cls),0
Assert that the dataset is either 'general' or 'ennedi',0
"checkpoint = load_state_dict_from_url(url, map_location=torch.device(self.device)) # NOTE: This function is not used in the current implementation",0
Load the class names and other metadata from the checkpoint,0
Load the model architecture,0
Load checkpoint into model,0
Remove 'model.' prefix from the state_dict keys if the key starts with 'model.',0
Load the new state_dict,0
Creating a Dataloader for batching and parallel processing of the images,0
Flatten the lists since we know its a single image,0
Calculate the total number of detections,0
Pre-allocate based on total possible detections,0
Loop through each species,0
Get the detections for this species,0
Apply the confidence threshold,0
Fill the preds_array with the valid detections,0
Call the forward method of the model in evaluation mode,0
x = self.fc(x),0
y = self.softmax(self.up(x)),0
patches' height & width,0
unfold on height,0
if non-perfect division on height,0
get the residual patch and add it to the fold,0
unfold on width,0
"if non-perfect division on width, the same",0
reshaping,0
patches' height & width,0
lists of pixels numbers,0
cut into patches to get limits,0
if non-perfect division on height,0
if non-perfect division on width,0
@property,0
def area(self) -> int:,0
''' To get area ''',0
return 1 # always 1 pixel,0
local maxima,0
adaptive threshold for counting,0
negative sample,0
count,0
locations and scores,0
upsample class map,0
softmax,0
cat to heatmap,0
LMDS,0
step 1 - get patches and limits,0
step 2 - inference to get maps,0
step 3 - patch the maps into initial coordinates system,0
(step 4 - upsample),0
outputs = self.model(patch)[0],0
cat,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Image size for the Opossum classifier,0
Class names for prediction,0
"If pretrained, use the provided URL to fetch the weights",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Making the PlainResNetInference class available for import from this module,0
Following the ResNet structure to extract features,0
Initialize the network and weights,0
... [Missing weight URL definition for ResNet18],0
... [Missing weight URL definition for ResNet50],0
Print missing and unused keys for debugging purposes,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Image size for the classifier,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Image size for the Opossum classifier,0
Class names for prediction,0
"If pretrained, use the provided URL to fetch the weights",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Image size for the Opossum classifier,0
Class names for prediction,0
"If pretrained, use the provided URL to fetch the weights",0
Configuration file for the Sphinx documentation builder.,0
,0
"For the full list of built-in configuration values, see the documentation:",0
https://www.sphinx-doc.org/en/master/usage/configuration.html,0
-- Project information -----------------------------------------------------,0
https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information,0
-- General configuration ---------------------------------------------------,0
https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration,0
-- Options for HTML output -------------------------------------------------,0
https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output,0
-- Options for todo extension ----------------------------------------------,1
https://www.sphinx-doc.org/en/master/usage/extensions/todo.html#configuration,0
"Determine the factor, which shifts the decimal point of x",0
just behind the last significant digit,0
"Shift decimal point by multiplicatipon with factor, flooring, and",0
division by factor,0
_ = pretty_print_object(obj),0
Sloppy that I'm making a module-wide change here...,1
Determine the coordinates of the intersection rectangle,0
The intersection of two axis-aligned bounding boxes is always an,0
axis-aligned bounding box,0
Compute the area of both AABBs,0
Compute the intersection over union by taking the intersection,0
area and dividing it by the sum of prediction + ground-truth,0
areas - the intersection area.,0
%% Functions for running commands as subprocesses,0
%%,0
%% Test driver for execute_and_print,0
%% Parallel test driver for execute_command_and_print,0
Should we use threads (vs. processes) for parallelization?,0
"Only relevant if n_workers == 1, i.e. if we're not parallelizing",0
%% Constants and imports,0
%% Classes,0
Collect all names,0
Make names unique and sort,0
cast location to string as the entries in locations are strings,0
Convert classnames to lowercase to simplify comparisons later,0
Normalize paths to simplify comparisons later,0
"Make custom replacements in filenames, typically used to",0
accommodate changes in root paths after DB construction,0
## Build useful mappings to facilitate working with the DB,0
Category ID <--> name,0
Image filename --> ID,0
Image ID --> image object,0
Image ID --> annotations,0
"Each image can potentially multiple annotations, hence using lists",0
...__init__,0
...class IndexedJsonDb,0
%% Functions,0
Find all unique locations,0
i_location = 0; location = locations[i_location],0
Sorting datetimes fails when there are None's in the list.  So instead of sorting datetimes,0
"directly, sort tuples with a boolean for none-ness, then the datetime itself.",0
,0
https://stackoverflow.com/questions/18411560/sort-list-while-pushing-none-values-to-the-end,0
previous_datetime = sorted_images_this_location[0]['datetime'],0
im = sorted_images_this_location[1],0
"Start a new sequence if necessary, including the case where this datetime is invalid",0
"If this was an invalid datetime, this will record the previous datetime",0
"as None, which will force the next image to start a new sequence.",0
...for each image in this location,0
Fill in seq_num_frames,0
...for each location,0
...create_sequences(),0
,0
cct_to_md.py,0
,0
"""Converts"" a COCO Camera Traps file to a MD results file.  Currently ignores",0
"non-bounding-box annotations, and gives all annotations a confidence of 1.0.",0
,0
The only reason to do this is if you are going to add information to an existing,0
"CCT-formatted dataset, and want to do that in Timelapse.",0
,0
"Currently assumes that width and height are present in the input data, does not",0
read them from images.,0
,0
%% Constants and imports,0
%% Functions,0
# Validate input,0
# Read input,0
# Prepare metadata,0
ann = d['annotations'][0],0
# Process images,0
im = d['images'][0],0
"MegaDetector: [x,y,width,height] (normalized, origin upper-left)",0
"CCT: [x,y,width,height] (absolute, origin upper-left)",0
...if there's a bounding box,0
...for each annotation,0
This field is no longer included in MD output files by default,0
im_out['max_detection_conf'] = max_detection_conf,0
...for each image,0
# Write output,0
...cct_to_md(),0
%% Command-line driver,0
TODO,1
%% Interactive driver,0
%%,0
%%,0
,0
cct_json_to_filename_json.py,0
,0
"Given a .json file in COCO Camera Traps format, outputs a .json-formatted list of",0
relative file names present in the CCT file.,0
,0
%% Constants and environment,0
%% Main function,0
"json.dump(s,open(outputFilename,'w'))",0
%% Command-line driver,0
%% Interactive driver,0
%%,0
,0
cct_to_csv.py,0
,0
"""Converts"" a COCO Camera Traps .json file to .csv, in quotes because",0
"all kinds of assumptions are made here, and if you have a particular .csv",0
"format in mind, YMMV.  Most notably, does not include any bounding box information",0
or any non-standard fields that may be present in the .json file.  Does not,0
propagate information about sequence-level vs. image-level annotations.,0
,0
"Does not assume access to the images, therefore does not open .jpg files to find",0
"datetime information if it's not in the metadata, just writes datetime as 'unknown'.",0
,0
%% Imports,0
%% Main function,0
#%% Read input,0
#%% Build internal mappings,0
annotation = annotations[0],0
#%% Write output file,0
im = images[0],0
Write out one line per class:,0
...for each class name,0
...for each image,0
...with open(output_file),0
...def cct_to_csv,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
,0
remove_exif.py,0
,0
"Removes all EXIF/IPTC/XMP metadata from a folder of images, without making",0
"backup copies, using pyexiv2.",0
,0
%% Imports and constants,0
%% List files,0
%% Remove EXIF data (support),0
PYEXIV2 IS NOT THREAD SAFE; DO NOT CALL THIS IN PARALLEL FROM A SINGLE PROCESS,1
data = img.read_exif(); print(data),0
%% Debug,0
%%,0
%%,0
%% Remove EXIF data (execution),0
fn = image_files[0],0
"joblib.Parallel defaults to a process-based backend, but let's be sure",0
"results = Parallel(n_jobs=n_exif_threads,verbose=2,prefer='processes')(delayed(remove_exif)(fn) for fn in image_files[0:10])",0
,0
yolo_to_coco.py,0
,0
Converts a YOLO-formatted dataset to a COCO-formatted dataset.,0
,0
"Currently supports only a single folder (i.e., no recursion).  Treats images without",0
corresponding .txt files as empty.,0
,0
%% Imports and constants,0
from ai4eutils,0
%% Support functions,0
Validate input,0
Class names,0
Blank lines should only appear at the end,0
Enumerate images,0
fn = image_files[0],0
Create the image object for this image,0
Is there an annotation file for this image?,0
"This is an image with no annotations, currently don't do anything special",0
here,0
s = lines[0],0
"COCO: [x_min, y_min, width, height] in absolute coordinates",0
"YOLO: [class, x_center, y_center, width, height] in normalized coordinates",0
...for each annotation,0
...if this image has annotations,0
...for each image,0
...def yolo_to_coco(),0
%% Interactive driver,0
%% Convert YOLO folders to COCO,0
%% Check DB integrity,0
%% Preview some images,0
%% Command-line driver,0
TODO,1
,0
read_exif.py,0
,0
"Given a folder of images, read relevant metadata (EXIF/IPTC/XMP) fields from all images,",0
and write them to  a .json or .csv file.,0
,0
This module can use either PIL (which can only reliably read EXIF data) or exiftool (which,0
can read everything).  The latter approach expects that exiftool is available on the system,0
path.  No attempt is made to be consistent in format across the two approaches.,0
,0
%% Imports and constants,0
From ai4eutils,0
%% Options,0
Number of concurrent workers,0
Should we use threads (vs. processes) for parallelization?,0
,0
Not relevant if n_workers is 1.,0
Should we use exiftool or pil?,0
%% Functions,0
exif_tags = img.info['exif'] if ('exif' in img.info) else None,0
print('Warning: unrecognized EXIF tag: {}'.format(k)),0
"-G means ""Print group name for each tag"", e.g. print:",0
,0
[File]          Bits Per Sample                 : 8,0
,0
...instead of:,0
,0
Bits Per Sample                 : 8,0
A list of three-element lists (type/tag/value),0
line_raw = exif_lines[0],0
A typical line:,0
,0
[ExifTool]      ExifTool Version Number         : 12.13,0
"Split on the first occurrence of "":""",0
...for each output line,0
...which processing library are we using?,0
...read_exif_tags_for_image(),0
...populate_exif_data(),0
Enumerate *relative* paths,0
Find all EXIF tags that exist in any image,0
...for each tag in this image,0
...for each image,0
Write header,0
...for each key that *might* be present in this image,0
...for each image,0
...with open(),0
...if we're writing to .json/.csv,0
https://stackoverflow.com/questions/11210104/check-if-a-program-exists-from-a-python-script,0
%% Interactive driver,0
%%,0
output_file = os.path.expanduser('~/data/test-exif.csv'),0
options.processing_library = 'pil',0
"file_path = os.path.join(input_folder,'KRU_S1_11_R1_IMAG0148.JPG')",0
%%,0
%% Command-line driver,0
,0
"Given a json-formatted list of image filenames, retrieve the width and height of every image.",0
,0
%% Constants and imports,0
%% Processing functions,0
Is this image on disk?,0
"all_results = list(tqdm(pool.imap(process_image, filenames), total=len(filenames)))",0
%% Interactive driver,0
%%,0
List images in a test folder,0
%%,0
"process_list_file(image_list_file,image_size_file,image_prefix=base_dir)",0
%% Command-line driver,0
"python sanity_check_json_db.py ""e:\wildlife_data\wellington_data\wellington_camera_traps.json"" --baseDir ""e:\wildlife_data\wellington_data\images"" --bFindUnusedImages --bCheckImageSizes",0
"python sanity_check_json_db.py ""D:/wildlife_data/mcgill_test/mcgill_test.json"" --baseDir ""D:/wildlife_data/mcgill_test"" --bFindUnusedImages --bCheckImageSizes",0
"Here the '-u' prevents buffering, which makes tee happier",0
,0
python -u sanity_check_json_db.py '/datadrive1/nacti_metadata.json' --baseDir '/datadrive1/nactiUnzip/' --bFindUnusedImages --bCheckImageSizes | tee ~/nactiTest.out,0
,0
Partially-formed stub to get from MegaDetector output files to COCO Camera Traps data.,0
,0
"Was actually written to convert *many* MD .json files to a single CCT file, hence",0
the loop over .json files.,0
,0
"THIS CODE HAS NEVER BEEN RUN, it was added as a demonstration of how to do this.  YMMV.",0
,0
"You may find a more polished, command-line-ready version of this code at:",0
,0
https://github.com/StewartWILDlab/mdtools,0
,0
%% Constants and imports,0
"Images sizes are required to convert between absolute and relative coordinates,",0
so we need to read the images.,0
Only required if you want to write a database preview,0
%% Create CCT dictionaries,0
image_ids_to_images = {},0
Force the empty category to be ID 0,0
Load .json annotations for this data set,0
i_entry = 0; entry = data['images'][i_entry],0
,0
"PERF: Not exactly trivially parallelizable, but about 100% of the",0
time here is spent reading image sizes (which we need to do to get from,0
"absolute to relative coordinates), so worth parallelizing.",0
Generate a unique ID from the path,0
detection = detections[0],0
Have we seen this category before?,0
Create an annotation,0
"MegaDetector: [x,y,width,height] (normalized, origin upper-left)",0
"CCT: [x,y,width,height] (absolute, origin upper-left)",0
...for each detection,0
...for each image,0
Remove non-reviewed images and associated annotations,0
%% Create info struct,0
%% Write .json output,0
%% Clean start,0
## Everything after this should work from a clean start ###,0
%% Validate output,0
%% Preview animal labels,0
%% Preview empty labels,0
"viz_options.classes_to_exclude = ['empty','human']",0
,0
generate_crops_from_cct.py,0
,0
"Given a .json file in COCO Camera Traps format, create a cropped image for",0
each bounding box.,0
,0
%% Imports and constants,0
%% Functions,0
# Read and validate input,0
# Find annotations for each image,0
"This actually maps image IDs to annotations, but only to annotations",0
containing boxes,0
# Generate crops,0
TODO: parallelize this loop,1
im = d['images'][0],0
Load the image,0
Generate crops,0
i_ann = 0; ann = annotations_this_image[i_ann],0
"x/y/w/h, origin at the upper-left",0
...for each box,0
...for each image,0
...generate_crops_from_cct(),0
%% Interactive driver,0
%%,0
%%,0
%%,0
%% Command-line driver,0
TODO,1
%% Scrap,0
%%,0
,0
coco_to_yolo.py,0
,0
Converts a COCO-formatted dataset to a YOLO-formatted dataset.,0
,0
"If the input and output folders are the same, writes .txt files to the input folder,",0
and neither moves nor modifies images.,0
,0
"Currently ignores segmentation masks, and errors if an annotation has a",0
segmentation polygon but no bbox,0
,0
Has only been tested on a handful of COCO Camera Traps data sets; if you,0
"use it for more general COCO conversion, YMMV.",0
,0
%% Imports and constants,0
%% Support functions,0
Validate input,0
Read input data,0
Parse annotations,0
i_ann = 0; ann = data['annotations'][0],0
Make sure no annotations have *only* segmentation data,0
Re-map class IDs to make sure they run from 0...n-classes-1,0
,0
"TODO: this allows unused categories in the output data set, which I *think* is OK,",1
but I'm only 81% sure.,0
Process images (everything but I/O),0
"List of dictionaries with keys 'source_image','dest_image','bboxes','dest_txt'",0
i_image = 0; im = data['images'][i_image],0
"assert os.path.isfile(source_image), 'Could not find image {}'.format(source_image)",0
If this annotation has no bounding boxes...,0
"This is not entirely clear from the COCO spec, but it seems to be consensus",0
"that if you want to specify an image with no objects, you don't include any",0
annotations for that image.,0
We allow empty bbox lists in COCO camera traps; this is typically a negative,0
"example in a dataset that has bounding boxes, and 0 is typically the empty",0
category.,0
...if this is an empty annotation,0
"COCO: [x_min, y_min, width, height] in absolute coordinates",0
"YOLO: [class, x_center, y_center, width, height] in normalized coordinates",0
Convert from COCO coordinates to YOLO coordinates,0
...for each annotation,0
...if this image has annotations,0
...for each image,0
Write output,0
Category IDs should range from 0..N-1,0
TODO: parallelize this loop,1
,0
output_info = images_to_copy[0],0
Only write an annotation file if there are bounding boxes.  Images with,0
"no .txt files are treated as hard negatives, at least by YOLOv5:",0
,0
https://github.com/ultralytics/yolov5/issues/3218,0
,0
"I think this is also true for images with empty annotation files, but",0
"I'm using the convention suggested on that issue, i.e. hard negatives",0
are expressed as images without .txt files.,0
bbox = bboxes[0],0
...for each image,0
...def coco_to_yolo(),0
%% Interactive driver,0
%% CCT data,0
"If preview_export is True, I'm exporting to preview these with BoundingBoxEditor:",0
,0
https://github.com/mfl28/BoundingBoxEditor,0
,0
"This export will be compatible, other than the fact that you need to move",0
"""object.data"" into the ""labels"" folder.",0
,0
"Otherwise I'm exporting for training, in the YOLOv5 flat format.",0
%% Command-line driver,0
TODO,1
,0
cct_to_wi.py,0
,0
Converts COCO Camera Traps .json files to the Wildlife Insights,0
batch upload format,0
,0
Also see:,0
,0
https://github.com/ConservationInternational/Wildlife-Insights----Data-Migration,0
,0
https://data.naturalsciences.org/wildlife-insights/taxonomy/search,0
,0
%% Imports,0
%% Paths,0
A COCO camera traps file with information about this dataset,0
A .json dictionary mapping common names in this dataset to dictionaries with the,0
"WI taxonomy fields: common_name, wi_taxon_id, class, orer, family, genus, species",0
%% Constants,0
%% Project information,0
%% Read templates,0
%% Compare dictionary to template lists,0
Write the header,0
Write values,0
%% Project file,0
%% Camera file,0
%% Deployment file,0
%% Images file,0
Read .json file with image information,0
Read taxonomy dictionary,0
Populate output information,0
df = pd.DataFrame(columns = images_fields),0
annotation = annotations[0],0
im = input_data['images'][0],0
"We don't have counts, but we can differentiate between zero and 1",0
This is the label mapping used for our incoming iMerit annotations,0
"Only used to parse the incoming annotations. In our database, the string name is used to avoid confusion",0
MegaDetector outputs,0
,0
add_bounding_boxes_to_megadb.py,0
,0
"Given COCO-formatted JSONs containing manually labeled bounding box annotations, add them to",0
"MegaDB sequence entries, which can then be ingested into MegaDB.",0
the category map that comes in the COCO JSONs for iMerit batch 12 - to check that each,0
JSON,0
"dataset name : (seq_id, frame_num) : [bbox, bbox]",0
where bbox is a dict with str 'category' and list 'bbox',0
iterate over image_id_to_image rather than image_id_to_annotations so we include,0
the confirmed empty images,0
The file_name field in the incoming json looks like,0
alka_squirrels.seq2020_05_07_25C.frame119221.jpg,0
there seems to be a bug in the annotations where sometimes there's a,0
non-empty label along with a label of category_id 5,0
ignore the empty label (they seem to be actually non-empty),0
,0
add_bounding_boxes_to_json.py,0
,0
This script takes a image database in the COCO Camera Traps format and merges in a set of bounding,0
box annotations in the format that iMerit uses (a .json where actually only each row is a valid json).,0
,0
"If you need to update an existing bbox database, please get all the original annotation files and",0
re-generate from scratch,0
,0
%% Imports,0
%% Configurations and paths,0
images database,0
output bboxes database,0
annotation files (pseudo json) obtained from our annotation vendor that contain annotations for this dataset,0
None or a string or tuple of strings that is the prefix to all file_name of interest / in this dataset in the annotation files,0
functions for mapping the image_id in the annotation files (pseudo jsons) to the image_id used in the image DB,0
our img_id doesn't contain frame info,0
"batch3 - ""file_name"":""ASG0000019_0_S1_B06_R1_PICT0007.JPG""",0
"batch5 and 7 - ""file_name"":""datasetsnapshotserengeti.seqASG000002m-frame0.imgS1_B06_R1_PICT0056.JPG""",0
sometimes - 'datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG',0
"id in DB (old_token): 'S6/J01/J01_R1/S6_J01_R1_IMAG0001', 'S1/B05/B05_R1/S1_B05_R1_PICT0036'",0
specify which one to use for your dataset here,0
%% Load the image database and fill in DB info for the output bbox database,0
load the images database,0
%% Find the height and width of images from the annotation files,0
,0
...if they are not available in the images DB,0
each row in this pseudo-json is a COCO formatted entry for an image sequence,0
%% Other functions required by specific datasets,0
the IDFG image database does not include images from unlabeled folders that were annotated with bounding boxes,0
%% Create the bbox database from all annotation files pertaining to this dataset,0
"for the incoming annotations, look up by category name (common) and convert to the numerical id used in our databases",0
"for each annotation pseudo-json, check that the image it refers to exists in the original database",0
each row in this pseudo-json is a COCO formatted entry for an image sequence,0
check that entry is for this dataset,0
category map for this entry in the annotation file - usually the same across all entries but just in case,0
rspb_add_image_entry(img_id),0
use the image length and width in the image DB,0
"[top left x, top left y, width, height] in relative coordinates",0
"add all images that have been sent to annotation, some of which may be empty of bounding boxes",0
rspb_add_image_entry(db_image_id),0
%% Common queries,0
This query is used when preparing tfrecords for object detector training.,0
We do not want to get the whole seq obj where at least one image has bbox because,0
some images in that sequence will not be bbox labeled so will be confusing.,0
Include images with bbox length 0 - these are confirmed empty by bbox annotators.,0
"If frame_num is not available, it will not be a field in the result iterable.",0
"Note that the seq_id is the Cosmos DB assigned ID for that sequence, not the",0
"seq_id field, which may contain ""/"" characters.",0
"For public datasets to be converted to the CCT format, we get the whole seq object because",0
sequence level attributes need to be included too. megadb/converters/megadb_to_cct.py handles,0
the case of bbox-only JSONs with the flag exclude_species_class,0
Getting all sequences in a dataset - for updating or deleting entries which need the id field,0
%% Parameters,0
Use None if querying across all partitions,0
"The `sequences` table has the `dataset` as the partition key, so if only querying",0
"entries from one dataset, set the dataset name here.",0
"e.g. {'name': '@top_n', 'value': 100} - see query_and_upsert_examples/query_for_data.ipynb",0
Use False if do not want all results stored in a single JSON.,0
%% Script,0
execute the query,0
loop through and save the results,0
MODIFY HERE depending on the query,0
wiitigers Unicode issue - no good mapping from DB file names to file names in blob URL,0
build filename,0
if need to re-download a dataset's images in case of corruption,0
entries_to_download = {,0
"filename: entry for filename, entry in entries_to_download.items()",0
if entry['dataset'] == DATASET,0
},0
input validation,0
"existing files, with paths relative to <store_dir>",0
parse JSON or TXT file,0
"create a new storage container client for this dataset,",0
and cache it,0
schema already checks that the min possible value of frame_num is 0,0
"if there are more than one image item, each needs a frame_num",0
checks across all sequence items,0
per sequence item checks,0
load the schema,0
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory,0
load the schema,0
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory,0
checks across all datasets items,0
check for expiry date of the SAS keys,0
update the sequences,0
"`id` is from the database, as well as all attributes starting with _",0
"if valuable sequence information is available, add them to the image",0
required fields for an image object,0
add seq-level class labels for this image,0
add other sequence-level properties to each image too,0
add other image-level properties,0
... for im in seq['images'],0
... for seq in mega_db,0
consolidate categories,0
some property names have changed in the new schema,0
a dummy sequence ID will be generated if the image entry does not have a seq_id field,0
seq_id only needs to be unique within this dataset; MegaDB does not rely on it as the _id field,0
"""annotations"" fields are opened and have its sub-field surfaced one level up",0
set the `dataset` property on each sequence to the provided dataset_name,0
check that the location field is the same for all images in a sequence,0
check which fields in a CCT image entry are sequence-level,0
image-level properties that really should be sequence-level,0
need to add (misidentified) seq properties not present for each image in a sequence to img_level_properties,0
"(some properties act like flags - all have the same value, but not present on each img)",0
add the sequence-level properties to the sequence objects,0
not every sequence have to have all the seq_level_properties,0
get the value of this sequence-level property from the first image entry,0
check which fields are really dataset-level and should be included in the dataset table instead.,0
delete sequence-level properties that should be dataset-level,0
make all `class` fields lower-case; cast `seq_id` to type string in case they're integers,0
turn all float NaN values into None so it gets converted to null when serialized,0
this was an issue in the Snapshot Safari datasets,0
%% validation,0
"at first a dict of image_id: image_obj with annotations embedded, then it becomes",0
an array of image objects,0
%% integrate the image DB,0
takes in image entries and species and other annotations in the image DB,0
convert the species category to explicit string name,0
there may be other fields in the annotation object,0
these fields should already be gotten from the image object,0
%% integrate the bbox DB,0
add any images that are not in the image DB,0
also add any fields in the image object that are not present already,0
add bbox to the annotations field,0
for any newly added images,0
"'bbox_abs': bbox_anno['bbox'],",0
not keeping height and width,0
,0
jb_csv_to_json.py,0
,0
Convert a particular .csv file to CCT format.  Images were not available at,0
"the time I wrote this script, so this is much shorter than other scripts",0
in this folder.,0
,0
%% Constants and environment,0
%% Read source data,0
%% Confirm filename uniqueness (this data set has one label per image),0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
%% Create CCT dictionaries,0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
imageName = imageFilenames[0],0
"As per above, this is convenient and appears to be true; asserting to be safe",0
"Filenames look like ""290716114012001a1116.jpg""",0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
%% Create info struct,0
%% Write output,0
%% Sanity-check,0
%% Imports and constants,0
%% Load data,0
"with open(image_json,'r') as f:",0
data = json.load(f),0
%% Sanity-check data,0
%% Label previews,0
%% Collect images to annotate,0
%% Sort by sequence and frame,0
%% Copy to a folder by GUID,0
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension,0
im = images_to_annotate[0],0
...for each image,0
%% Write out the annotation list,0
%% Constants and imports,0
%% Enumerate files,0
edited_image_folder = edited_image_folders[0],0
fn = edited_image_files[0],0
%% Read metadata and capture location information,0
i_row = 0; row = df.iloc[i_row],0
Sometimes '2017' was just '17' in the date column,0
%% Read the .json files and build output dictionaries,0
json_fn = json_files[0],0
if 'partial' in json_fn:,0
continue,0
line = lines[0],0
if image_fn == 'SD1_238_6_26_17_16_76.73.jpg':,0
asdfad,0
SD29_079_5_14_2018_17_52.85.jpg,0
Re-write two-digit years as four-digit years,0
Sometimes the year was written with two digits instead of 4,0
assert len(tokens[4]) == 4 and tokens[4].startswith('20'),0
Have we seen this location already?,0
"Not a typo, it's actually ""formateddata""",0
An image shouldn't be annotated as both empty and non-empty,0
An image shouldn't be annotated as both empty and non-empty,0
box = formatteddata[0],0
"open_file(os.path.join(base_folder,jpeg_image_folder,image_fn))",0
...for each box,0
...if there are boxes on this image,0
...for each line,0
...with open(),0
...for each json file,0
%% Prepare the output .json,0
%% Check DB integrity,0
%% Print unique locations,0
SD12_202_6_23_2017_1_31.85.jpg,0
%% Preview some images,0
%% Statistics,0
,0
"Import a Snapshot Safari project (one project, one season)",0
,0
Before running this script:,0
,0
"* Mount the blob container where the images live, or copy the",0
images to local storage,0
,0
What this script does:,0
,0
* Creates a .json file,0
* Creates zip archives of the season without humans.,0
* Copies animals and humans to separate folders,0
,0
After running this script:,0
,0
* Create or update LILA page,0
* Push zipfile and unzipped images to LILA,0
* Push unzipped humans to wildlifeblobssc,0
* Delete images from UMN uplaod storage,0
,0
"Snapshot Serengeti is handled specially, because we're dealing with bounding",0
boxes too.  See snapshot_serengeti_lila.py.,0
,0
%% Imports,0
From ai4eutils,0
From CameraTraps,0
%% Constants,0
project_name = 'XXX'; season_name = 'S1'; project_friendly_name = 'Snapshot Unknown',0
project_name = 'SER'; season_name = 'S1-11'; project_friendly_name = 'Snapshot Serengeti',0
project_name = 'KRU'; season_name = 'S1'; project_friendly_name = 'Snapshot Kruger',0
project_name = 'CDB'; season_name = 'S1'; project_friendly_name = 'Snapshot Camdeboo',0
project_name = 'MTZ'; season_name = 'S1'; project_friendly_name = 'Snapshot Mountain Zebra',0
project_name = 'ENO'; season_name = 'S1'; project_friendly_name = 'Snapshot Enonkishu',0
project_name = 'KAR'; season_name = 'S1'; project_friendly_name = 'Snapshot Karoo',0
project_name = 'KGA'; season_name = 'S1'; project_friendly_name = 'Snapshot Kgalagadi',0
%% Folder/file creation,0
E.g. KRU_S1,0
E.g. Z:\KRU,0
E.g. Z:\KRU\KRU_S1,0
Contains annotations for each capture event (sequence),0
Maps image IDs to filenames; each line looks like:,0
,0
"KRU_S1#1#1#2,3,KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0004.JPG",0
"Total number of each answer to each question, e.g. total number of times each species was identified",0
,0
Not used here,1
Create output folders,0
Images will be placed in a season-specific folder inside this (the source data includes,0
this in path names),0
%% Load metadata files,0
%% Convert to dictionaries (prep),0
%% Convert to dictionaries (loops),0
"TODO: iterrows() is a terrible way to do this, but this is one of those days",1
"where I want to get this done, not get better at Python.",0
irow = 0; row = image_table.iloc[0],0
"Loaded as an int64, converting to int here",0
...for each row in the image table,0
Make sure image IDs are what we think they are,0
...for each row in the annotation table,0
%% Take a look at categories (just sanity-checking),0
print('\nCategories by species:'),0
pp.pprint(categories_by_species),0
%% Fill in some image fields we didn't have when we created the image table,0
"width, height, corrupt, seq_num_frames, location, datetime",0
Every annotation in this list should have the same sequence ID,0
"Figure out ""seq_num_frames"", which really should be done in a separate lopp;",0
there's no reason to do this redundantly for every image,0
Every image in this sequence should point back to the same equence,0
Every annotation in this list should have the same location,0
Every annotation in this list should have the same datetime,0
Is this image on disk?,0
iImage = 0; im = images[0],0
...for each image,0
"images_processed = pool.map(process_image, images)",0
"images_processed = list(tqdm(pool.imap_unordered(process_image, images), total=len(images)))",0
%% Count missing/corrupted images,0
%% Print distribution of sequence lengths (sanity-check),0
%% Replicate annotations across images,0
iAnn = 0; ann = annotations[iAnn],0
%% See what files are on disk but not annotated,0
%% Sanity-check image and annotation uniqueness,0
%% Minor updates to fields,0
%% Write .json file,0
%% Create a list of human files,0
ann = annotations[0],0
%% Create public archive and public/private folders,0
im = images[0],0
E.g. KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0001.JPG,0
Copy to private output folder,0
Add to zipfile,0
Possibly start a new archive,0
Copy to public output folder,0
...for each image,0
%% Sanity-check .json file,0
"This will produce some validation errors, because this zipfile doesn't include humans",0
%% Zip up .json and .csv files,0
%% When I skip to this part (using a pre-rendered .json file),0
%%,0
%%,0
ann = annotations[0],0
%% Summary prep for LILA,0
"%% Generate preview, sanity-check labels",0
"viz_options.classes_to_include = ['jackalblackbacked','bustardkori']",0
%% Scrap,0
%% Find annotations for a particular image,0
%% Write a list of missing images,0
,0
save_the_elephants_survey_A.py,0
,0
Convert the .csv file provided for the Save the Elephants Survey A data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
Handle all unstructured fields in the source data as extra fields in the annotations,0
"photo_type really should be an image property, but there are a few conflicts",0
that forced me to handle it as an annotation proprerty,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
Maps relative paths to row indices in input_metadata,0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
Ignore directories,0
%% Make sure the multiple-annotation cases make sense,0
%%,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
Force the empty category to be ID 0,0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image],0
Example filename:,0
,0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG',0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG',0
i_row = row_indices[0],0
There are a small handful of datetime mismatches across annotations,0
for the same image,0
assert im['datetime'] == timestamp,0
Special cases based on the 'photo type' field,0
Various spellings of 'community',0
Have we seen this category before?,0
Create an annotation,0
fieldname = list(mapped_fields.keys())[0],0
...for each row,0
...for each image,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
%% Scrap,0
%% Find unique photo types,0
##,0
,0
timelapse_csv_set_to_json.py,0
,0
Given a directory full of reasonably-consistent Timelapse-exported,0
".csvs, assemble a CCT .json.",0
,0
"Assumes that you have a list of all files in the directory tree, including",0
image and .csv files.,0
,0
##,0
%% Constants and imports,0
Text file with relative paths to all files (images and .csv files),0
"%% Read file list, make a list of all image files and all .csv files",0
"%% Verify column consistency, create a giant array with all rows from all .csv files",0
i_csv = 0; csv_filename = csv_files[0],0
%% Prepare some data structures we'll need for mapping image rows in .csv files to actual image files,0
Enumerate all folders containing image files,0
"In this data set, a site folder looks like:",0
,0
Processed Images\\site_name,0
%% Map .csv files to candidate camera folders,0
fn = valid_csv_files[0],0
"Some site folders appear as ""XXNNNN"", some appear as ""XXNNNN_complete""",0
...for each .csv file,0
%% Map camera folders to candidate image folders,0
%% Main loop over labels (prep),0
Force the empty category to be ID 0,0
Images that are marked empty and also have a species label,0
%% Main loop over labels (loop),0
i_row = 0; row = input_metadata.iloc[i_row],0
"for i_row,row in input_metadata.iterrows():",0
"Usually this is just a single folder name, sometimes it's a full path,",0
which we don't want,0
Check whether this file exists on disk,0
Retrieve image width and height,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each row in the big table of concatenated .csv files,0
%% Fix cases where an image was annotated as 'unlabeled' and as something else,0
This annotation is 'unlabeled',0
Was there another category associated with this image?,0
%% Check for un-annnotated images,0
Enumerate all images,0
list(relative_path_to_image.keys())[0],0
%% Create info struct,0
%% Write output,0
%% Sanity-check the database's integrity,0
%% Render a bunch of images to make sure the labels got carried along correctly,0
,0
auckland_doc_to_json.py,0
,0
Convert Auckland DOC data set to COCO camera traps format.  This was,0
for a training data set where class names were encoded in path names.,0
,0
%% Constants and imports,0
Filenames will be stored in the output .json relative to this base dir,0
%% Enumerate files,0
%% Assemble dictionaries,0
Force the empty category to be ID 0,0
fn = image_files[0]; print(fn),0
Typically y:\Maukahuka_Auckland_Island\1_Training\Winter_Trial_2019\cat\cat\eat\20190903_IDdY_34_E3_tmp_201908240051.JPG,0
"This data set has two top-level folders, ""1_Training"" (which has class names encoded",0
"in paths) and ""2_Testing"" (which has no class information).",0
...for each image,0
%% Write output .json,0
%% Write train/test .jsons,0
%% Validate .json files,0
%% Preview labels,0
checkpoint,0
,0
awc_to_json.py,0
,0
Convert a particular .csv file to CCT format.,0
,0
%% Constants and environment,0
%% Read source data,0
%% Main loop over labels,0
Force the empty category to be ID 0,0
iRow = 0; row = input_metadata.iloc[iRow],0
"ImageID,FileName,FilePath,SpeciesID,CommonName",0
"This makes an assumption of one annotation per image, which happens to be",0
true in this data set.,0
Retrieve image width and height,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
list(relativePathToImage.keys())[0],0
%% Create info struct,0
%% Write output,0
%% Sanity-check the database's integrity,0
%% Render a bunch of images to make sure the labels got carried along correctly,0
,0
idaho-camera-traps.py,0
,0
Prepare the Idaho Camera Traps dataset for release on LILA.,0
,0
%% Imports and constants,0
Multi-threading for .csv file comparison and image existence validation,0
"We are going to map the original filenames/locations to obfuscated strings, but once",0
"we've done that, we will re-use the mappings every time we run this script.",0
This is the file to which mappings get saved,0
The maximum time (in seconds) between images within which two images are considered the,0
same sequence.,0
"This is a two-column file, where each line is [string in the original metadata],[category name we want to map it to]",0
"The output file, using the original strings",0
"The output file, using obfuscated strings for everything but filenamed",0
"The output file, using obfuscated strings and obfuscated filenames",0
"One time only, I ran MegaDetector on the whole dataset...",0
...then set aside any images that *may* have contained humans that had not already been,0
annotated as such.  Those went in this folder...,0
...and the ones that *actually* had humans (identified via manual review) got,0
copied to this folder...,0
"...which was enumerated to this text file, which is a manually-curated list of",0
images that were flagged as human.,0
Unopinionated .json conversion of the .csv metadata,0
%% List files (images + .csv),0
Ignore .csv files in folders with multiple .csv files,0
...which would require some extra work to decipher.,0
fn = csv_files[0],0
%% Parse each .csv file into sequences (function),0
csv_file = csv_files[-1],0
os.startfile(csv_file_absolute),0
survey = csv_file.split('\\')[0],0
Sample paths from which we need to derive locations:,0
,0
St.Joe_elk\AM99\Trip 1\100RECNX\TimelapseData.csv,0
Beaverhead_elk\AM34\Trip 1\100RECNX\TimelapseData.csv,0
,0
ClearCreek_mustelids\Winter2015-16\FS-001-P\FS-001-P.csv,0
ClearCreek_mustelids\Summer2015\FS-001\FS-001.csv,0
ClearCreek_mustelids\Summer2016\IDFG-016\IDFG-016.csv,0
,0
I:\idfg-images\ClearCreek_mustelids\Summer2016\IDFG-017b,0
I:\idfg-images\ClearCreek_mustelids\Summer2016\IDFG-017a,0
Load .csv file,0
Validate the opstate column,0
# Create datetimes,0
print('Creating datetimes'),0
i_row = 0; row = df.iloc[i_row],0
Make sure data are sorted chronologically,0
,0
"In odd circumstances, they are not... so sort them first, but warn",0
Debugging when I was trying to see what was up with the unsorted dates,0
# Parse into sequences,0
print('Creating sequences'),0
i_row = 0; row = df.iloc[i_row],0
Start a new sequence if:,0
,0
* This image has no timestamp,0
* This image has a frame number of zero,0
* We have no previous image timestamp,0
,0
Start a new sequence if necessary,0
...for each row,0
# Parse labels for each sequence,0
sequence_id = location_sequences[0],0
Row indices in a sequence should be adjacent,0
sequence_df = df[df['seq_id']==sequence_id],0
# Determine what's present,0
Be conservative; assume humans are present in all maintenance images,0
The presence columns are *almost* always identical for all images in a sequence,0
assert single_presence_value,0
print('Warning: presence value for {} is inconsistent for {}'.format(,0
"presence_column,sequence_id))",0
...for each presence column,0
Tally up the standard (survey) species,0
"If no presence columns are marked, all counts should be zero",0
count_column = count_columns[0],0
Occasionally a count gets entered (correctly) without the presence column being marked,0
"assert len(values) == 1 and values[0] == 0, 'Non-zero counts with no presence",0
columns marked for sequence {}'.format(sequence_id),0
"Handle this by virtually checking the ""right"" box",0
Make sure we found a match,0
Handle 'other' tags,0
column_name = otherpresent_columns[0],0
print('Found non-survey counted species column: {}'.format(column_name)),0
...for each non-empty presence column,0
"Very rarely, the ""otherpresent"" column is checked, but no more detail is available",0
...handling non-survey species,0
Build the sequence data,0
i_row = 0; row = sequence_df.iloc[i_row],0
Only one folder used a single .csv file for two subfolders,0
...for each sequence,0
...def csv_to_sequences(),0
%% Parse each .csv file into sequences (loop),0
%%,0
%%,0
i_file = -1; csv_file = csv_files[i_file],0
%% Save sequence data,0
%% Load sequence data,0
%%,0
%% Validate file mapping (based on the existing enumeration),0
sequences = sequences_by_file[0],0
sequence = sequences[0],0
"Actually, one folder has relative paths",0
assert '\\' not in image_file_relative and '/' not in image_file_relative,0
os.startfile(csv_folder),0
assert os.path.isfile(image_file_absolute),0
found_file = os.path.isfile(image_file_absolute),0
...for each image,0
...for each sequence,0
...for each .csv file,0
%% Load manual category mappings,0
The second column is blank when the first column already represents the category name,0
%% Convert to CCT .json (original strings),0
Force the empty category to be ID 0,0
For each .csv file...,0
,0
sequences = sequences_by_file[0],0
For each sequence...,0
,0
sequence = sequences[0],0
Find categories for this image,0
"When 'unknown' is used in combination with another label, use that",0
"label; the ""unknown"" here doesn't mean ""another unknown species"", it means",0
there is some other unknown property about the main species.,0
category_name_string = species_present[0],0
"This piece of text had a lot of complicated syntax in it, and it would have",0
been too complicated to handle in a general way,0
print('Ignoring category {}'.format(category_name_string)),0
Don't process redundant labels,0
category_name = category_names[0],0
If we've seen this category before...,0
If this is a new category...,0
print('Adding new category for {}'.format(category_name)),0
...for each category (inner),0
...for each category (outer),0
...if we do/don't have species in this sequence,0
"We should have at least one category assigned (which may be ""empty"" or ""unknown"")",0
assert len(sequence_category_ids) > 0,0
Was any image in this sequence manually flagged as human?,0
print('Flagging sequence {} as human based on manual review'.format(sequence_id)),0
For each image in this sequence...,0
,0
i_image = 0; im = images[i_image],0
Create annotations for this image,0
...for each image in this sequence,0
...for each sequence,0
...for each .csv file,0
Verify that all images have annotations,0
ann = ict_data['annotations'][0],0
For debugging only,0
%% Create output (original strings),0
%% Validate .json file,0
%% Preview labels,0
%% Look for humans that were found by MegaDetector that haven't already been identified as human,0
This whole step only needed to get run once,0
%%,0
Load MD results,0
Get a list of filenames that MD tagged as human,0
im = md_results['images'][0],0
...for each detection,0
...for each image,0
Map images to annotations in ICT,0
ann = ict_data['annotations'][0],0
For every image,0
im = ict_data['images'][0],0
Does this image already have a human annotation?,0
...for each annotation,0
...for each image,0
%% Copy images for review to a new folder,0
fn = missing_human_images[0],0
%% Manual step...,0
Copy any images from that list that have humans in them to...,0
%% Create a list of the images we just manually flagged,0
fn = human_tagged_filenames[0],0
E.g. '0000_Beaverhead_elk~AM174~Trip 1~100RECNX~IMG_1397.JPG',0
"%% Translate location, image, sequence IDs",0
Load mappings if available,0
Generate mappings,0
If we've seen this location before...,0
Otherwise assign a string-formatted int as the ID,0
If we've seen this sequence before...,0
Otherwise assign a string-formatted int as the ID,0
Assign an image ID,0
...for each image,0
Assign annotation mappings,0
Save mappings,0
"Back this file up, lest we should accidentally re-run this script",0
with force_generate_mappings = True and overwrite the mappings we used.,0
...if we are/aren't re-generating mappings,0
%% Apply mappings,0
"%% Write new dictionaries (modified strings, original files)",0
"%% Validate .json file (modified strings, original files)",0
%% Preview labels (original files),0
"viz_options.classes_to_exclude = ['empty','deer','elk']",0
viz_options.classes_to_include = ['bobcat'],0
%% Copy images to final output folder (prep),0
ann = d['annotations'][0],0
Is this a public or private image?,0
Generate absolute path,0
Copy to output,0
Update the filename reference,0
...def process_image(im),0
%% Copy images to final output folder (execution),0
For each image,0
im = images[0],0
Write output .json,0
%% Make sure the right number of images got there,0
%% Validate .json file (final filenames),0
%% Preview labels (final filenames),0
"viz_options.classes_to_exclude = ['empty','deer','elk']",0
viz_options.classes_to_include = ['horse'],0
viz_options.classes_to_include = [viz_options.multiple_categories_tag],0
"viz_options.classes_to_include = ['human','vehicle','domestic dog']",0
%% Create zipfiles,0
%% List public files,0
%% Find the size of each file,0
fn = all_public_output_files[0],0
%% Split into chunks of approximately-equal size,0
...for each file,0
%% Create a zipfile for each chunk,0
...for each filename,0
with ZipFile(),0
...def create_zipfile(),0
i_file_list = 0; file_list = file_lists[i_file_list],0
"....if __name__ == ""__main__""",0
,0
wellington_to_json.py,0
,0
Convert the .csv file provided for the Wellington data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
%% Read source data,0
"Filenames were provided as *.jpg, but images were *.JPG, converting here",0
"%% Map filenames to rows, verify image existence",0
"Takes ~30 seconds, since it's checking the existence of ~270k images",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
"I didn't expect this to be true a priori, but it appears to be true, and",0
it saves us the trouble of checking consistency across multiple occurrences,0
of an image.,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
imageName = imageFilenames[0],0
"As per above, this is convenient and appears to be true; asserting to be safe",0
"Filenames look like ""290716114012001a1116.jpg""",0
This gets imported as an int64,0
"These appear as ""image1"", ""image2"", etc.",0
"In the form ""001a""",0
Can be in the form '111' or 's46',0
"In the form ""7/29/2016 11:40""",0
Check image height and width,0
"Use 'empty', to be consistent with other data on lila",0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
,0
mcgill_to_json.py,0
,0
Convert the .csv file provided for the McGill test data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
Create an additional column for concatenated filenames,0
Maps relative filenames to rows,0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
row = input_metadata.iloc[0],0
"I didn't expect this to be true a priori, but it appears to be true, and",0
it saves us the trouble of checking consistency across multiple occurrences,0
of an image.,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
row = input_metadata.iloc[0],0
"Filenames look like ""290716114012001a1116.jpg""",0
"In the form ""001a""",0
Can be in the form '111' or 's46',0
"In the form ""7/29/2016 11:40""",0
Check image height and width,0
NaN is the only thing we should see that's not a string,0
NaN is the only thing we should see that's not a string,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
,0
bellevue_to_json.py,0
,0
"""Bellevue Camera Traps"" is the rather unremarkable camera trap data set",0
used by one of the repo's maintainers for testing.  It's organized as:,0
,0
approximate_date/[loose_camera_specifier/]/species,0
,0
E.g.:,0
,0
"""2018.03.30\coyote\DSCF0091.JPG""",0
"""2018.07.18\oldcam\empty\DSCF0001.JPG""",0
,0
%% Constants and imports,0
from the ai4eutils repo,0
Filenames will be stored in the output .json relative to this base dir,0
%% Exif functions,0
"%% Enumerate files, create image/annotation/category info",0
Force the empty category to be ID 0,0
Keep track of unique camera folders,0
Each element will be a dictionary with fields:,0
,0
"relative_path, width, height, datetime",0
fname = image_files[0],0
Corrupt or not an image,0
Store file info,0
E.g. 2018.03.30/coyote/DSCF0091.JPG,0
...for each image file,0
%% Synthesize sequence information,0
Sort images by time within each folder,0
camera_path = camera_folders[0],0
previous_datetime = sorted_images_this_camera[0]['datetime'],0
im = sorted_images_this_camera[1],0
Start a new sequence if necessary,0
...for each image in this camera,0
...for each camera,0
Fill in seq_num_frames,0
%% A little cleanup,0
%% Write output .json,0
%% Sanity-check data,0
%% Label previews,0
,0
snapshot_safar_importer_reprise.py,0
,0
This is a 2023 update to snapshot_safari_importer.py.  We do a bunch of things now that,0
we didn't do the last time we imported Snapshot data (like updating the big taxonomy),0
"file, and we skip a bunch of things now that we used to do (like generating massive",0
"zipfiles).  So, new year, new importer.",0
,0
%% Constants and imports,0
%% List files,0
"Do a one-time enumeration of the entire drive; this will take a long time,",0
but will save a lot of hassle later.,0
%% Create derived lists,0
Takes about 60 seconds,0
CSV files are one of:,0
,0
"_report_lila.csv (this is the one we want to use, with the species/count/etc. for each sequence)",0
_report_lila_image_inventory.csv (maps captures to images),0
_report_lila_overview.csv (distrubution of species),0
%% List project folders,0
Project folders look like one of these:,0
,0
APN,0
Snapshot Cameo/DEB,0
%% Map report and inventory files to codes,0
fn = csv_files[0],0
%% Make sure that every report has a corresponding inventory file,0
%% Count species based on overview and report files,0
%% Print counts,0
%% Make sure that capture IDs in the reports/inventory files match,0
...and that all the images in the inventory tables are actually present on disk.,0
assert image_path_relative in all_files_relative_set,0
Make sure this isn't just a case issue,0
...for each report on this project,0
...for each project,0
"%% For all the files we have on disk, see which are and aren't in the inventory files",0
"There aren't any capital-P .PNG files, but if I don't include that",0
"in this list, I'll look at this in a year and wonder whether I forgot",0
to include it.,0
fn = all_files_relative[0],0
print('Skipping project {}'.format(project_code)),0
,0
plot_wni_giraffes.py,0
,0
Plot keypoints on a random sample of images from the wni-giraffes data set.,0
,0
%% Constants and imports,0
%% Load and select data,0
%% Support functions,0
https://stackoverflow.com/questions/32504246/draw-ellipse-in-python-pil-with-line-thickness,0
Use a single channel image (mode='L') as mask.,0
The size of the mask can be increased relative to the imput image,0
to get smoother looking results.,0
draw outer shape in white (color) and inner shape in black (transparent),0
downsample the mask using PIL.Image.LANCZOS,0
(a high-quality downsampling filter).,0
paste outline color to input image through the mask,0
%% Plot some images,0
ann = annotations_to_plot[0],0
i_tool = 0; tool_name = short_tool_names[i_tool],0
Don't plot tools that don't have a consensus annotation,0
...for each tool,0
...for each annotation,0
,0
idfg_iwildcam_lila_prep.py,0
,0
Adding class labels (from the private test .csv) to the iWildCam 2019 IDFG,0
"test set, in preparation for release on LILA.",0
,0
This version works with the public iWildCam release images.,0
,0
"%% ############ Take one, from iWildCam .json files ############",0
%% Imports and constants,0
%% Read input files,0
Remove the header line,0
%% Parse annotations,0
Lines look like:,0
,0
"b005e5b2-2c0b-11e9-bcad-06f1011196c4,1,Private",0
%% Minor cleanup re: images,0
%% Create annotations,0
%% Prepare info,0
%% Minor adjustments to categories,0
Remove unused categories,0
Name adjustments,0
%% Create output,0
%% Write output,0
%% Validate .json file,0
%% Preview labels,0
viz_options.classes_to_exclude = ['test'],0
"%% ############ Take two, from pre-iWildCam .json files created from IDFG .csv files ############",0
%% Imports and constants,0
%% One-time line break addition,0
%% Read input files,0
%% Prepare info,0
%% Minor adjustments to categories,0
%% Minor adjustments to annotations,0
%% Create output,0
%% Write output,0
%% Validate .json file,0
%% Preview labels,0
viz_options.classes_to_exclude = ['test'],0
,0
missouri_to_json.py,0
,0
Create .json files from the original source files for the Missouri Camera Traps,0
data set.  Metadata was provided here in two formats:,0
,0
"1) In one subset of the data, folder names indicated species names.  In Set 1,",0
there are no empty sequences.  Set 1 has a metadata file to indicate image-level,0
bounding boxes.,0
,0
2) A subset of the data (overlapping with (1)) was annotated with bounding,0
"boxes, specified in a whitespace-delimited text file.  In set 2, there are",0
"some sequences omitted from the metadata file, which implied emptiness.",0
,0
"In the end, set 2 labels were not reliable enough to publish, so LILA includes only set 1.",0
,0
%% Constants and imports,0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Filenames will be stored in the output .json relative to this base dir,0
This will be a list of filenames that need re-annotation due to redundant boxes,0
"%% Enumerate files, read image sizes (both sets)",0
"Takes a few minutes, since we're reading image sizes.",0
Each element will be a list of relative path/full path/width/height,0
"Only process leaf nodes corresponding to sequences, which look like:",0
,0
E:\wildlife_data\missouri_camera_traps\Set1\1.02-Agouti\SEQ75583,0
E:\wildlife_data\missouri_camera_traps\Set2\p1d101,0
,0
assert len(files) <= 2,0
Read the image,0
Not an image...,0
Store file info,0
"...if we didn't hit the max file limit, keep going",0
...for each file,0
%% Add sequence lengths (both sets),0
%% Load the set 1 metadata file,0
"List of lists, length varies according to number of bounding boxes",0
,0
Preserves original ordering,0
iLine = 0; line = metadataSet1Lines[0],0
"Lines should be filename, number of bounding boxes, boxes (four values per box)",0
Make sure we have image info for this image,0
%% Print missing files from Set 1 metadata,0
Manual changes I made to the metadata file:,0
,0
'IMG' --> 'IMG_',0
Red_Brocket_Deer --> Red_Deer,0
European-Hare --> European_Hare,0
Wood-Mouse --> Wood_Mouse,0
Coiban-Agouti --> Coiban_Agouti,0
%% Load the set 2 metadata file,0
"This metadata file contains most (but not all) images, and a class label (person/animal/empty)",0
"for each, plus bounding boxes.",0
"List of lists, length varies according to number of bounding boxes",0
,0
Preserves original ordering,0
"Create class IDs for each *sequence*, which we'll use to attach classes to",0
images for which we don't have metadata,0
,0
This only contains mappings for sequences that appear in the metadata.,0
iLine = 0; line = metadataSet2Lines[0],0
"Lines should be filename, number of bounding boxes, labeled boxes (five values per box)",0
,0
Empty images look like filename\t0\t0,0
E.g. 'Set2\\p1d101\\p1d101s100i10.JPG',0
Make sure we don't have mixed classes within an image,0
"Figure out what class this *sequence* is, so we know how to handle unlabeled",0
images from this sequence,0
Can't un-do a mixed sequence,0
Previously-empty sequences get the image class label,0
"If the sequence has a non-empty class, possibly change it",0
Make sure we have image info for this image,0
...for each line in the set 2 metadata file,0
%% What Set 2 images do I not have metadata for?,0
These are *mostly* empty images,0
iImage = 0; imageID = set2ImageIDs[iImage],0
%% Create categories and annotations for set 1,0
"Though we have no empty sequences, we do have empty images in this set",0
For each image,0
,0
iImage = 0; imageID = set1ImageIDs[iImage],0
E.g. Set1\\1.80-Coiban_Agouti\\SEQ83155\\SEQ83155_IMG_0010.JPG,0
Find the species name,0
This image may still be empty...,0
category['count'] = category['count'] + 1,0
"If we have bounding boxes, create image-level annotations",0
"filename, number of bounding boxes, boxes (four values per box)",0
"Make sure the relative filename matches, allowing for the fact that",0
some of the filenames in the metadata aren't quite right,0
This image is non-empty,0
"Some redundant bounding boxes crept in, don't add them twice",0
Check this bbox against previous bboxes,0
,0
Inefficient?  Yes.  In an important way?  No.,0
"Bounding box values are in absolute coordinates, with the origin",0
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1].",0
,0
"Convert to floats and to x/y/w/h, as per CCT standard",0
...for each box,0
if we do/don't have boxes for this image,0
Else create a sequence-level annotation,0
...for each image,0
%% Write out the list of images with redundant boxes,0
%% Create categories and annotations for set 2,0
For each image,0
,0
iImage = 0; imageID = set2ImageIDs[iImage],0
E.g. 'Set2\\p1d100\\p1d100s10i1.JPG',0
"Find the sequence ID, sanity check filename against what we stored",0
"If we have bounding boxes or an explicit empty label, create image-level annotations",0
"filename, number of bounding boxes, labeled boxes (five values per box)",0
"Make sure the relative filename matches, allowing for the fact that",0
some of the filenames in the metadata aren't quite right,0
"Bounding box values are in absolute coordinates, with the origin",0
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1].",0
,0
Convert to floats and to x/y/w/h,0
...for each box,0
...if we do/don't have boxes for this image,0
Else create a sequence-level annotation,0
...for each image,0
"%% The 'count' field isn't really meaningful, delete it",0
"It's really the count of image-level annotations, not total images assigned to a class",0
%% Write output .json files,0
%% Sanity-check final set 1 .json file,0
"python sanity_check_json_db.py --bCheckImageSizes --baseDir ""E:\wildlife_data\missouri_camera_traps"" ""E:\wildlife_data\missouri_camera_traps\missouri_camera_traps_set1.json""",0
%% Generate previews,0
,0
filenames_to_json.py,0
,0
Take a directory of images in which species labels are encoded by folder,0
"names, and produces a COCO-style .json file",0
,0
%% Constants and imports,0
from the ai4eutils repo,0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Filenames will be stored in the output .json relative to this base dir,0
"rawClassListFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_classes.csv')",0
"classMappingsFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_class_mapping.csv')",0
"%% Enumerate files, read image sizes",0
Each element will be a list of relative path/full path/width/height,0
Read the image,0
Corrupt or not an image,0
Store file info,0
Write to output file,0
...for each image file,0
...csv file output,0
...if the file list is/isn't available,0
%% Enumerate classes,0
Maps classes to counts,0
We like 'empty' to be class 0,0
%% Assemble dictionaries,0
...for each category,0
Each element is a list of relative path/full path/width/height/className,0
...for each image,0
%% External class mapping,0
%% Write raw class table,0
cat = categories[0],0
%% Read the mapped class table,0
"id, source, count, target",0
"%% Make classMappings contain *all* classes, not just remapped classes",0
cat = categories[0],0
%% Create new class list,0
"Start at 1, explicitly assign 0 to ""empty""",0
One-off issue with character encoding,0
%% Re-map annotations,0
ann = annotations[0],0
%% Write output .json,0
%% Utilities,0
%%,0
Find images with a particular tag,0
%% Randomly sample annotations,0
,0
umn_to_json.py,0
,0
Prepare images and metadata for the Orinoqua Camera Traps dataset.,0
,0
%% Imports and constants,0
"For two deployments, we're only processing imagse in the ""detections"" subfolder",0
%% Enumerate deployment folders,0
%% Load ground truth,0
i_row = 0; row = ground_truth_df.iloc[i_row],0
%% Create relative paths for ground truth data,0
"Some deployment folders have no subfolders, e.g. this is a valid file name:",0
,0
M00/01010132.JPG,0
,0
"But some deployment folders have subfolders, e.g. this is also a valid file name:",0
,0
N17/100EK113/07160020.JPG,0
,0
"So we can't find files by just concatenating folder and file names, we have to enumerate and explicitly",0
"map what will appear in the ground truth as ""folder/filename"" to complete relative paths.",0
deployment_name = list(deployment_folders)[0],0
Enumerate all files in this folder,0
"print('Enumerated {} files for deployment {}'.format(len(files),deployment_name))",0
filename = files[100],0
...for each file in this deployment,0
...for each deployment,0
%% Add relative paths to our ground truth table,0
i_row = 0; row = ground_truth_df.iloc[i_row],0
"row['filename'] looks like, e.g. A01/01080001.JPG.  This is not actually a path, it's",0
"just the deployment ID and the image name, separated by a slash.",0
Find the relative path for this image,0
Make sure this image file exists,0
...for each row in the ground truth table,0
%% Take everything out of Pandas,0
%% Convert string timestamps to Python datetimes,0
im = ground_truth_dicts[0],0
"Filenames look like, e.g., N36/100EK113/06040726.JPG",0
%% Synthesize sequence information,0
Sort images by time within each location,0
i_location=0; location = locations[i_location],0
previous_datetime = sorted_images_this_location[0]['datetime'],0
im = sorted_images_this_camera[1],0
"Timestamp for this image, may be None",0
Start a new sequence if:,0
,0
* This image has no timestamp,0
* This iamge has a frame number of zero,0
* We have no previous image timestamp,0
,0
Start a new sequence if necessary,0
...for each image in this location,0
...for each location,0
%% Create category dict and category IDs,0
im = images[0],0
%% Imports and constants (.json generation),0
%% Count frames in each sequence,0
%% Double check images with multiple annotations,0
im = images[0],0
%% Assemble dictionaries,0
Force the empty category to be ID 0,0
input_im = images[0],0
...for each image,0
%% Write output .json,0
%% Validate .json file,0
%% Map relative paths to annotation categories,0
ann = data['annotations'][0],0
%% Copy images to output,0
EXCLUDE HUMAN AND MISSING,0
im = data['images'][0],0
im = images[0],0
%% Preview labels,0
viz_options.classes_to_exclude = ['test'],0
,0
nacti_fieldname_adjustments.py,0
,0
"NACTI metadata was posted with ""filename"" in images instead of ""file_name"", and",0
"used string (rather than int) category IDs (in categories, but not in annotations).",0
,0
This script fixes those issues and rev's the version number.,0
,0
%% Constants and environment,0
%% Read .json file,0
"%% Rev version number, update field names and types",0
%% Write json file,0
,0
cct_field_adjustments.py,0
,0
CCT metadata was posted with int locations instead of strings.,0
,0
This script fixes those issues and rev's the version number.,0
,0
%% Constants and environment,0
%% Read .json file,0
"%% Rev version number, update field names and types",0
%% Write json file,0
%% Check output data file,0
,0
carrizo_shrubfree_2018.py,0
,0
Convert the .csv file provided for the Carrizo Mojave data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
%% Read source data,0
Original .csv file had superfluous spaces in column names,0
"%% Map filenames to rows, verify image existence",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
58 missing files (of 17652),0
%% Check for images that aren't included in the metadata file,0
3012 of 20606 files are not in metadata,0
%% Create CCT dictionaries,0
Map categories to integer IDs,0
,0
The category '0' is reserved for 'empty',0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
Each filename should just match one row,0
Don't include images that don't exist on disk,0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
,0
snapshot_serengeti_lila.py,0
,0
Create zipfiles of Snapshot Serengeti S1-S11.,0
,0
"Create a metadata file for S1-S10, plus separate metadata files",0
"for S1-S11.  At the time this code was written, S11 was under embargo.",0
,0
Create zip archives of each season without humans.,0
,0
Create a human zip archive.,0
,0
%% Constants and imports,0
import sys; sys.path.append(r'c:\git\ai4eutils'),0
import sys; sys.path.append(r'c:\git\cameratraps'),0
assert(os.path.isdir(metadata_base)),0
"There are two redundant categories, and we re-map ""blank"" to ""empty"" as per CCT convention",0
"%% Load metadata files, concatenate into a single table",0
iSeason = 1,0
%% Convert to dictionaries (prep),0
%% Convert to dictionaries (loops),0
"TODO: iterrows() is a terrible way to do this, but this is one of those days",1
"where I want to get this done, not get better at Python.",0
irow = 0; row = image_table.iloc[0],0
"Loaded as an int64, converting to int here",0
...for each row in the image table,0
Make sure image IDs are what we think they are,0
...for each row in the annotation table,0
%% Load previously-saved dictionaries when re-starting mid-script,0
%%,0
%% Take a look at categories (just sanity-checking),0
%%,0
%% Fill in some image fields we didn't have when we created the image table,0
"width, height, corrupt, seq_num_frames, location, datetime",0
Every annotation in this list should have the same sequence ID,0
"Figure out ""seq_num_frames"", which really should be done in a separate lopp;",0
there's no reason to do this redundantly for every image,0
Every image in this sequence should point back to the same equence,0
Every annotation in this list should have the same location,0
Every annotation in this list should have the same datetime,0
Is this image on disk?,0
iImage = 0; im = images[0],0
...for each image,0
"images_processed = pool.map(process_image, images)",0
"images_processed = list(tqdm(pool.imap_unordered(process_image, images), total=len(images)))",0
%%,0
%% Count missing/corrupted images,0
%% Print distribution of sequence lengths (sanity-check),0
%% Replicate annotations across images,0
iAnn = 0; ann = annotations[iAnn],0
%% See what files are on disk but not annotated (~15 mins),0
247370 files not in the database (of 7425810),0
%% Load old image database,0
%% Look for old images not in the new DB and vice-versa,0
"At the time this was written, ""old"" was S1-S6",0
old_im = cct_old['images'][0],0
new_im = images[0],0
4 old images not in new db,0
12 new images not in old db,0
%% Save our work,0
%% Load our work,0
%%,0
%% Examine size mismatches,0
i_mismatch = -1; old_im = size_mismatches[i_mismatch],0
%% Sanity-check image and annotation uniqueness,0
"%% Split data by seasons, create master list for public seasons",0
ann = annotations[0],0
%% Minor updates to fields,0
"%% Write master .json out for S1-10, write individual season .jsons (including S11)",0
"Loop over all seasons, plus one iteration for the ""all public data"" iteration, and",0
"one for the ""all data"" iteration",0
%% Find categories that only exist in S11,0
List of categories in each season,0
Category 55 (fire) only in S11,0
Category 56 (hyenabrown) only in S11,0
Category 57 (wilddog) only in S11,0
Category 58 (kudu) only in S11,0
Category 59 (pangolin) only in S11,0
Category 60 (lioncub) only in S11,0
%% Prepare season-specific .csv files,0
iSeason = 1,0
%% Create a list of human files,0
ann = annotations[0],0
%% Save our work,0
%% Load our work,0
%%,0
"%% Create archives (human, per-season) (prep)",0
im = images[0],0
im = images[0],0
Don't include humans,0
Only include files from this season,0
Possibly start a new archive,0
...for each image,0
i_season = 0,0
"for i_season in range(0,nSeasons):",0
create_season_archive(i_season),0
%% Create archives (loop),0
pool = ThreadPool(nSeasons+1),0
"n_images = pool.map(create_archive, range(-1,nSeasons))",0
"seasons_to_zip = range(-1,nSeasons)",0
...for each season,0
%% Sanity-check .json files,0
%logstart -o r'E:\snapshot_temp\python.txt',0
%% Zip up .json and .csv files,0
pool = ThreadPool(len(files_to_zip)),0
"pool.map(zip_single_file, files_to_zip)",0
%% Super-sanity-check that S11 info isn't leaking,0
im = data_public['images'][0],0
ann = data_public['annotations'][0],0
iRow = 0; row = annotation_df.iloc[iRow],0
iRow = 0; row = image_df.iloc[iRow],0
%% Create bounding box archive,0
i_image = 0; im = data['images'][0],0
i_box = 0; boxann = bbox_data['annotations'][0],0
%% Sanity-check a few files to make sure bounding boxes are still sensible,0
import sys; sys.path.append(r'C:\git\CameraTraps'),0
%% Check categories,0
%% Summary prep for LILA,0
,0
wi_to_json,0
,0
Prepares CCT-formatted metadata based on a Wildlife Insights data export.,0
,0
"Mostly assumes you have the images also, for validation/QA.",0
,0
%% Imports and constants,0
"Because WI filenames are GUIDs, it's not practical to page through sequences in an",0
image viewer.  So we're going to (optionally) create a copy of the data set where,0
images are ordered.,0
%% Load ground truth,0
%% Take everything out of Pandas,0
%% Synthesize common names when they're not available,0
"Blank rows should always have ""Blank"" as the common name",0
"print('Warning: missing common name for row {} ({})'.format(i_row,row['filename']))",0
%% Convert string timestamps to Python datetimes,0
im = ground_truth_dicts[0],0
"The field called ""location"" in the WI .csv file is a URL, we want to reclaim",0
"the ""location"" keyword for CCT output",0
"Filenames look like, e.g., N36/100EK113/06040726.JPG",0
%% Synthesize sequence information,0
Sort images by time within each location,0
i_location=0; location = locations[i_location],0
previous_datetime = sorted_images_this_location[0]['datetime'],0
im = sorted_images_this_camera[1],0
"Timestamp for this image, may be None",0
Start a new sequence if:,0
,0
* This image has no timestamp,0
* This iamge has a frame number of zero,0
* We have no previous image timestamp,0
,0
Start a new sequence if necessary,0
...for each image in this location,0
...for each location,0
%% Create category dict and category IDs,0
im = images[0],0
%% Count frames in each sequence,0
%% Build relative paths,0
im = images[0],0
Sample URL:,0
,0
gs://project-asfasdfd/deployment/21444549/asdfasdfd-616a-4d10-a921-45ac456c568a.jpg',0
%% Double check images with multiple annotations,0
im = images[0],0
%% Assemble dictionaries,0
Force the empty category to be ID 0,0
input_im = images[0],0
"print('Warning: image ID {} ({}) has multiple annotations'.format(im['id'],im['id'].replace('_','/')))",0
...for each image,0
%% Write output .json,0
%% Validate .json file,0
%% Preview labels,0
"open_file(os.path.join(image_base,'2100703/1141a545-88d2-498b-a684-7431f7aeb324.jpg'))",0
%%,0
%% Create ordered dataset,0
"Because WI filenames are GUIDs, it's not practical to page through sequences in an",0
image viewer.  So we're going to create a copy of the data set where images are,0
ordered.,0
im = images_out[0]; im,0
%% Create ordered .json,0
%% Copy files to their new locations,0
im = ordered_images[0],0
im = data_ordered['images'][0],0
%% Preview labels in the ordered dataset,0
"open_file(os.path.join(image_base,'2100703/1141a545-88d2-498b-a684-7431f7aeb324.jpg'))",0
%% Open an ordered filename from the unordered filename,0
,0
rspb_to_json.py,0
,0
Convert the .csv file provided for the RSPB data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
[location] is an obfuscation,0
%% Create info struct,0
%% Read source data,0
metadataTable.columns.values,0
,0
"array(['Project', 'inDir', 'FileName', 'Station', 'Camera',",0
"'StationCameraFileName', 'DateTimeOriginal', 'DateReadable',",0
"'outDir', 'filename_new', 'fileExistsAlready', 'CopyStatus',",0
"'Species'], dtype=object)",0
We'll populate these later,0
keys should be lowercase,0
"%% Enumerate images, confirm filename uniqueness",0
"%% Update metadata filenames to include site and camera folders, check existence",0
,0
Takes ~1min,0
iRow = 0; row = metadataTable.iloc[iRow],0
There's a bug in the metadata; the 'camera' column isn't correct.,0
camera = row['Camera'],0
"These appear as, e.g., '3.22e12'",0
camera = str(int(float(camera))),0
Let's pull this out of the file name instead,0
,0
Filenames look like one of the following:,0
,0
A1__03224850850507__2015-11-28__10-45-04(1).JPG,0
Bayama2PH__C05__NA(NA).JPG,0
assert(os.path.isfile(fullPath)),0
metadataTable.iloc[iRow] = row,0
Re-assemble into an updated table,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
Write to a text file,0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
iRow = 0; row = metadataTable.iloc[iRow],0
A1__03224850850507__2015-11-28__10-45-04(1).JPG,0
'A1\\03224850850507\\A1__03224850850507__2015-11-28__10-45-04(1).JPG',0
Not currently populated,0
"Often -1, sometimes a semi-meaningful int",0
A1,0
03224850850507,0
"In variable form, but sometimes '28/11/2015 10:45'",0
Check image height and width,0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Write output,0
%% Check database integrity,0
%% Preview a few images to make sure labels were passed along sensibly,0
%% One-time processing step: copy images to a flat directory for annotation,0
%%,0
,0
pc_to_json.py,0
,0
Convert a particular collection of .csv files to CCT format.,0
,0
%% Constants and environment,0
%% Read and concatenate source data,0
List files,0
"List of dataframes, one per .csv file; we'll concatenate later",0
i_file = 87; fn = input_files[i_file],0
Concatenate into a giant data frame,0
%% List files,0
%% Main loop over labels (prep),0
Force the empty category to be ID 0,0
%% Main loop over labels (loop),0
iRow = 0; row = input_metadata.iloc[iRow],0
"ImageID,FileName,FilePath,SpeciesID,CommonName",0
assert os.path.isfile(full_path),0
Retrieve image width and height,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
%% See what's up with missing files,0
s = list(image_relative_paths)[0],0
s = missing_files[0],0
%% Check for images that aren't included in the metadata file,0
%% Sample the database,0
%%,0
Collect the images we want,0
%% Create info struct,0
%% Write output,0
%% Sanity-check the database's integrity,0
%% Render a bunch of images to make sure the labels got carried along correctly,0
options.classes_to_exclude = ['unlabeled'],0
%% Write out a list of files to annotate,0
,0
save_the_elephants_survey_B.py,0
,0
Convert the .csv file provided for the Save the Elephants Survey B data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
input_base = r'/mnt/blobfuse/wildlifeblobssc/ste_2019_08_drop',0
output_base = r'/home/gramener/survey_b',0
Handle all unstructured fields in the source data as extra fields in the annotations,0
"photo_type really should be an image property, but there are a few conflicts",0
that forced me to handle it as an annotation proprerty,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
"%% Map filenames to rows, verify image existence",0
Maps relative paths to row indices in input_metadata,0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
Ignore directories,0
%% Make sure the multiple-annotation cases make sense,0
%%,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
Force the empty category to be ID 0,0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image],0
Example filename:,0
,0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG',0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG',0
i_row = row_indices[0],0
timestamp = row['Date'],0
There are a small handful of datetime mismatches across annotations,0
for the same image,0
assert im['datetime'] == timestamp,0
Special cases based on the 'photo type' field,0
Various spellings of 'community',0
Have we seen this category before?,0
Create an annotation,0
fieldname = list(mapped_fields.keys())[0],0
...for each row,0
...for each image,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
%% Scrap,0
%% Find unique photo types,0
,0
ubc_to_json.py,0
,0
Convert the .csv file provided for the UBC data set to a,0
COCO-camera-traps .json file,0
,0
"Images were provided in eight folders, each of which contained a .csv",0
file with annotations.  Those annotations came in two slightly different,0
"formats, the two formats corresponding to folders starting with ""SC_"" and",0
otherwise.,0
,0
%% Constants and environment,0
Map Excel column names - which vary a little across spreadsheets - to a common set of names,0
%% Enumerate images,0
Load from file if we've already enumerated,0
%% Create CCT dictionaries,0
Force the empty category to be ID 0,0
To simplify debugging of the loop below,0
#%% Create CCT dictionaries (loop),0
#%%,0
Read source data for this folder,0
Rename columns,0
Folder name is the first two characters of the filename,0
,0
Create relative path names from the filename itself,0
Folder name is the camera name,0
,0
Create relative path names from camera name and filename,0
Which of our images are in the spreadsheet?,0
i_row = 0; fn = input_metadata['image_relative_path'][i_row],0
#%% Check for images that aren't included in the metadata file,0
Find all the images in this folder,0
Which of these aren't in the spreadsheet?,0
#%% Create entries in CCT dictionaries,0
Only process images we have on disk,0
"This is redundant, but doing this for clarity, at basically no performance",0
cost since we need to *read* the images below to check validity.,0
i_row = row_indices[0],0
"These generally represent zero-byte images in this data set, don't try",0
to find the very small handful that might be other kinds of failures we,0
might want to keep around.,0
print('Error opening image {}'.format(image_relative_path)),0
If we've seen this category before...,0
...make sure it used the same latin --> common mapping,0
,0
"If the previous instance had no mapping, use the new one.",0
assert common_name == category['common_name'],0
Create an annotation,0
...for each annotation we found for this image,0
...for each image,0
...for each dataset,0
Print all of our species mappings,0
"%% Copy images for which we actually have annotations to a new folder, lowercase everything",0
im = images[0],0
%% Create info struct,0
"%% Convert image IDs to lowercase in annotations, tag as sequence level",0
"While there isn't any sequence information, the nature of false positives",0
"here leads me to believe the images were labeled at the sequence level, so",0
we should trust labels more when positives are verified.  Overall false,0
positive rate looks to be between 1% and 5%.,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
,0
helena_to_cct.py,0
,0
Convert the Helena Detections data set to a COCO-camera-traps .json file,0
,0
%% Constants and environment,0
This is one time process,0
%% Create Filenames and timestamps mapping CSV,0
import pdb;pdb.set_trace(),0
%% To create CCT JSON for RSPB dataset,0
%% Read source data,0
Original Excel file had timestamp in different columns,0
"%% Map filenames to rows, verify image existence",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
%% Skipping this check because one image has multiple species,0
assert len(duplicate_rows) == 0,0
%% Check for images that aren't included in the metadata file,0
%% Create CCT dictionaries,0
Map categories to integer IDs,0
,0
The category '0' is reserved for 'empty',0
For each image,0
Don't include images that don't exist on disk,0
Some filenames will match to multiple rows,0
assert(len(rows) == 1),0
iRow = rows[0],0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
ann['datetime'] = row['datetime'],0
ann['site'] = row['site'],0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
%% Imports and constants,0
from github.com/microsoft/ai4eutils,0
from github.com/ecologize/CameraTraps,0
A list of files in the lilablobssc container for this data set,0
The raw detection files provided by NOAA,0
A version of the above with filename columns added,0
%% Read input .csv,0
%% Read list of files,0
%% Convert paths to full paths,0
i_row = 0; row = df.iloc[i_row],0
assert ir_image_path in all_files,0
...for each row,0
%% Write results,0
"%% Load output file, just to be sure",0
%% Render annotations on an image,0
i_image = 2004,0
%% Download the image,0
%% Find all the rows (detections) associated with this image,0
"as l,r,t,b",0
%% Render the detections on the image(s),0
In pixel coordinates,0
In pixel coordinates,0
%% Save images,0
%% Clean up,0
,0
carrizo_trail_cam_2017.py,0
,0
"Convert the .csv files provided for the ""Trail Cam Carrizo"" 2017 data set to",0
a COCO-camera-traps .json file.,0
,0
%% Constants and environment,0
%% Read source data,0
Removing the empty records,0
"%% Map filenames to rows, verify image existence",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
908 missing files (of 60562),0
%% Check for images that aren't included in the metadata file,0
105329 of 164983 files are not in metadata,0
%% Create CCT dictionaries,0
Map categories to integer IDs,0
,0
The category '0' is reserved for 'empty',0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
Each filename should just match one row,0
Don't include images that don't exist on disk,0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
,0
channel_islands_to_cct.py,0
,0
Convert the Channel Islands data set to a COCO-camera-traps .json file,0
,0
"Uses the command-line tool ExifTool (exiftool.org) to pull EXIF tags from images,",0
"because every Python package we tried failed to pull the ""Maker Notes"" field properly.",0
,0
"%% Imports, constants, paths",0
# Imports ##,0
# Constants ##,0
# Paths ##,0
Confirm that exiftool is available,0
"assert which(exiftool_command_name) is not None, 'Could not locate the ExifTool executable'",0
%% Load information from every .json file,0
"Ignore the sample file... actually, first make sure there is a sample file",0
...and now ignore that sample file.,0
json_file = json_files[0],0
ann = annotations[0],0
...for each annotation in this file,0
...for each .json file,0
"%% Confirm URL uniqueness, handle redundant tags",0
Have we already added this image?,0
"One .json file was basically duplicated, but as:",0
,0
Ellie_2016-2017 SC12.json,0
Ellie_2016-2017-SC12.json,0
"If the new image has no output, just leave the old one there",0
"If the old image has no output, and the new one has output, default to the one with output",0
Don't worry about the cases where someone tagged 'fox' and someone tagged 'fox_partial',0
...for each image we've already added,0
...if this URL is/isn't in the list of URLs we've already processed,0
...for each image,0
%% Save progress,0
%%,0
%%,0
%% Download files (functions),0
https://www.quickprogrammingtips.com/python/how-to-download-multiple-files-concurrently-in-python.html,0
"This is returned with a leading slash, remove it",0
%% Download files (execution),0
%% Read required fields from EXIF data (functions),0
"-G means ""Print group name for each tag"", e.g. print:",0
,0
[File]          Bits Per Sample                 : 8,0
,0
...instead of:,0
,0
Bits Per Sample                 : 8,0
"If we don't get any EXIF information, this probably isn't an image",0
line_raw = exif_lines[0],0
"Split on the first occurrence of "":""",0
Typically:,0
,0
"'[MakerNotes]    Sequence                        ', '1 of 3']",0
Not a typo; we are using serial number as a location,0
"If there are multiple timestamps, make sure they're *almost* the same",0
"If there are multiple timestamps, make sure they're *almost* the same",0
...for each line in the exiftool output,0
"This isn't directly related to the lack of maker notes, but it happens that files that are missing",0
maker notes also happen to be missing EXIF date information,0
...process_exif(),0
"This is returned with a leading slash, remove it",0
Ignore non-image files,0
%% Read EXIF data (execution),0
ann = images[0],0
%% Save progress,0
Use default=str to handle datetime objects,0
%%,0
%%,0
"Not deserializing datetimes yet, will do this if I actually need to run this",0
%% Check for EXIF read errors,0
%% Remove junk,0
Ignore non-image files,0
%% Fill in some None values,0
"...so we can sort by datetime later, and let None's be sorted arbitrarily",0
%% Find unique locations,0
%% Synthesize sequence information,0
Sort images by time within each location,0
i_location=0; location = locations[i_location],0
previous_datetime = sorted_images_this_location[0]['datetime'],0
im = sorted_images_this_camera[1],0
"Timestamp for this image, may be None",0
Start a new sequence if:,0
,0
* This image has no timestamp,0
* This image has a frame number of zero,0
* We have no previous image timestamp,0
,0
Start a new sequence if necessary,0
...for each image in this location,0
...for each location,0
%% Count frames in each sequence,0
images_this_sequence = [im for im in images if im['seq_id'] == seq_id],0
"%% Create output filenames for each image, store original filenames",0
i_location = 0; location = locations[i_location],0
i_image = 0; im = sorted_images_this_location[i_image],0
%% Save progress,0
Use default=str to handle datetime objects,0
%%,0
%%,0
%% Copy images to their output files (functions),0
%% Copy images to output files (execution),0
%% Rename the main image list for consistency with other scripts,0
%% Create CCT dictionaries,0
Make sure this is really a box,0
Transform to CCT format,0
Force the empty category to be ID 0,0
i_image = 0; input_im = all_image_info[0],0
"This issue only impacted one image that wasn't a real image, it was just a screenshot",0
"showing ""no images available for this camera""",0
Convert datetime if necessary,0
Process temperature if available,0
Read width and height if necessary,0
I don't know what this field is; confirming that it's always None,0
Process object and bbox,0
os.startfile(output_image_full_path),0
"Zero is hard-coded as the empty category, but check to be safe",0
"I can't figure out the 'index' field, but I'm not losing sleep about it",0
assert input_annotation['index'] == 1+i_ann,0
"Some annotators (but not all) included ""_partial"" when animals were partially obscured",0
"Annotators *mostly* used 'none', but sometimes 'empty'.  'empty' is CCT-correct.",0
If we've seen this category before...,0
If this is a new category...,0
...if this is an empty/non-empty annotation,0
Create an annotation,0
...for each annotation on this image,0
...for each image,0
%% Change *two* annotations on images that I discovered contains a human after running MDv4,0
%% Move human images,0
ann = annotations[0],0
%% Count images by location,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
viz_options.classes_to_exclude = [0],0
,0
ena24_to_json_2017.py,0
,0
Convert the ENA24 data set to a COCO-camera-traps .json file,0
,0
%% Constants and environment,0
Temporary folders for human and non-human images,0
Clean existing output folders/zipfiles,0
%% Support functions,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
Check image height and width,0
"Each row is category, [box coordinates]",0
"If there's just one row, loadtxt reads it as a 1d array; make it a 2d array",0
with one row,0
Each row is a bounding box,0
Have we seen this category before?,0
Create an annotation,0
...for each bounding box,0
"This was here for debugging; nearly every instance is Human+Horse, Human+Vehicle,",0
"or Human+Dog, but there is one Rabbit+Opossium, and a few Deer+Chicken!",0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
%% Create ZIP files for human and non human,0
%% Validate output,0
%% Preview labels,0
%% Imports and constants,0
configurations and paths,0
%% Helper functions,0
"dest_path = copy(source_path, dest_folder)",0
num_workers = multiprocessing.cpu_count(),0
pool = ThreadPool(num_workers),0
"results = pool.starmap(_copy_unzip, zip(sources, itertools.repeat(dest_folder)))",0
,0
print('Waiting for processes to finish...'),0
pool.close(),0
pool.join(),0
sequential,0
%% Command-line driver,0
if the blob container is already mounted on the VM,0
or you can download them using the storage Python SDK,0
store storage account key in environment variable AZ_STORAGE_KEY,0
,0
eMammal_helpers.py,0
,0
Support functions for processing eMammal metadata,0
,0
%% Constants and imports,0
%% Support functions,0
"pad to a total of 3 digits if < 1000, or 4 digits otherwise",0
img_frame is a string from the xml tree,0
"length 4 frame order is returned as is, others are left padded to be 3 digit long",0
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels,0
pad to a total of 4 digits,0
img_frame is a string from the xml tree,0
"length 4 frame order is returned as is, others are left padded to be 3 digit long",0
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels,0
others column,0
summer day hours: 6am - 7pm,0
others day hours: 7am - 6pm,0
,0
make_eMammal_json.py,0
,0
"Produces the COCO-formatted json database for an eMammal dataset, i.e. a",0
"collection of folders, each of which contains a deployment_manifest.xml file.",0
,0
"In this process, each image needs to be loaded to size it.",0
,0
"To add bounding box annotations to the resulting database, use",0
add_annotations_to_eMammal_json.py.,0
,0
%% Constants and imports,0
"Either add the eMammal directory to your path, or run from there",0
os.chdir(r'd:\git\CameraTraps\database_tools\eMammal'),0
import warnings,0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
warnings.filterwarnings('ignore'),0
Should we run the image size retrieval in parallel?,0
%% Support functions,0
%% Main loop (metadata processing; image sizes are retrieved later),0
deployment = folders[0],0
sequence = image_sequences[0],0
get species info for this sequence,0
add each image's info to database,0
img = images[0],0
"some manifests don't have the ImageOrder info, but the info is in the file name",0
full_img_id has no frame info,0
,0
frame number only used in requests to iMerit for ordering,0
...for each image,0
...for each sequence,0
...for each deployment,0
%% Get image sizes,0
"'tasks' is currently a list of 2-tuples, with each entry as [image dictionary,path].",0
,0
"Go through that and copy just the image dictionaries to 'db_images', adding size",0
information to each entry.  Takes a couple hours.,0
opening each image seems too fast for this multi-threaded version to be faster than sequential code.,0
%% Assemble top-level dictionaries,0
%% Write out .json,0
,0
make_full_SS_json.py,0
,0
Create a COCO-camera-traps .json file for Snapshot Serengeti data from,0
the original .csv files provided on Dryad.,0
,0
"This was used for ""version 1.0"" of the public Snapshot Serengeti archive; it's no",0
longer used as of v2.0 (early 2020).  See snapshot_serengeti_lila.py for the updated,0
Snapshot Safari preparation process.,0
,0
%% Imports and constants,0
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays",0
Count the number of images with multiple species,0
"%% Read image .csv file, format into a dictionary mapping images to capture events",0
%% Create CCT-style .json,0
still need image width and height,0
...for each image,0
%% Write output files,0
,0
make_per_season_SS_json.py,0
,0
Create a COCO-camera-traps .json file for each Snapshot Serengeti season from,0
the original .csv files provided on Dryad.,0
,0
"This was used for ""version 1.0"" of the public Snapshot Serengeti archive; it's no",0
longer used as of v2.0 (early 2020).  See snapshot_serengeti_lila.py for the updated,0
Snapshot Safari preparation process.,0
,0
%% Imports and constants,0
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays",0
"%% Read image .csv file, format into a dictionary mapping images to capture events",0
%% Create CCT-style .json,0
still need image width and height,0
...for each image ID,0
%% Write output files,0
...for each season,0
,0
generate_lila_per_image_labels.py,0
,0
"Generate a .csv file with one row per annotation, containing full URLs to every",0
"camera trap image on LILA, with taxonomically expanded labels.",0
,0
"Typically there will be one row per image, though images with multiple annotations",0
will have multiple rows.,0
,0
"Some images may not physically exist, particularly images that are labeled as ""human"".",0
This script does not validate image URLs.,0
,0
Does not include bounding box annotations.,0
,0
%% Constants and imports,0
"We'll write images, metadata downloads, and temporary files here",0
"Some datasets don't have ""sequence_level_annotation"" fields populated, but we know their",0
annotation level,0
%% Download and parse the metadata file,0
To select an individual data set for debugging,0
%% Download and extract metadata for the datasets we're interested in,0
%% Load taxonomy data,0
"%% Build a dictionary that maps each [dataset,query] pair to the full taxonomic label set",0
i_row = 0; row = taxonomy_df.iloc[i_row],0
%% Process annotations for each dataset,0
ds_name = list(metadata_table.keys())[0],0
"Go through annotations, marking each image with the categories that are present",0
,0
ann = annotations[0],0
im = images[10],0
This field name was only used for Caltech Camera Traps,0
raise ValueError('Suspicious date parsing result'),0
Special case we don't want to print a warning about,0
"Location, sequence, and image IDs are only guaranteed to be unique within",0
"a dataset, so for the output .csv file, include both",0
category_name = list(categories_this_image)[0],0
Only print a warning the first time we see an unmapped label,0
...for each category that was applied at least once to this image,0
...for each image in this dataset,0
print('Warning: no date information available for this dataset'),0
print('Warning: no location information available for this dataset'),0
...for each dataset,0
...with open(),0
%% Read the .csv back,0
%% Do some post-hoc integrity checking,0
"-1 isn't *really* valid, but we use it sometimes for sequences of unknown length",0
i_row = 0; row = df.iloc[i_row],0
%% Preview constants,0
%% Choose images to download,0
ds_name = list(metadata_table.keys())[2],0
Find all rows for this dataset,0
...for each dataset,0
%% Download images,0
i_image = 0; image = images_to_download[i_image],0
%% Write preview HTML,0
im = images_to_download[0],0
,0
Common constants and functions related to LILA data management/retrieval.,0
,0
%% Imports and constants,0
LILA camera trap master metadata file,0
"wildlife_insights_taxonomy_url = 'https://api.wildlifeinsights.org/api/v1/taxonomy?fields=class,order,family,genus,species,authority,taxonomyType,uniqueIdentifier,commonNameEnglish&page[size]={}'.format(wildlife_insights_page_size)",0
from ai4eutils,0
%% Common functions,0
"We haven't implemented paging, make sure that's not an issue",0
d['data'] is a list of items that look like:,0
Put the master metadata file in the same folder where we're putting images,0
Read lines from the master metadata file,0
Parse those lines into a table,0
Each line in this file is name/sas_url/json_url/[bbox_json_url],0
Create a separate entry for bounding boxes if they exist,0
Unzip if necessary,0
,0
get_lila_category_list.py,0
,0
Generates a .json-formatted dictionary mapping each LILA dataset to all categories,0
"that exist for that dataset, with counts for the number of occurrences of each category",0
"(the number of *annotations* for each category, not the number of *images*).",0
,0
"Also loads the taxonomy mapping file, to include scientific names for each category.",0
,0
get_lila_category_counts counts the number of *images* for each category in each dataset.,0
,0
%% Constants and imports,0
array to fill for output,0
"We'll write images, metadata downloads, and temporary files here",0
Created by get_lila_category_list.py... contains counts for each category,0
%% Load category and taxonomy files,0
%% Map dataset names and category names to scientific names,0
i_row = 1; row = taxonomy_df.iloc[i_row]; row,0
%% Download and parse the metadata file,0
%% Download and extract metadata for the datasets we're interested in,0
%% Get category names and counts for each dataset,0
ds_name = 'NACTI',0
Open the metadata file,0
Collect list of categories and mappings to category name,0
ann = annotations[0],0
c = categories[0],0
"Don't do taxonomy mapping for bbox data sets, which are sometimes just binary and are",0
always redundant with the class-level data sets.,0
"As of right now, this is the only quirky case",0
...for each dataset,0
%% Save dict,0
%% Print the results,0
ds_name = list(dataset_to_categories.keys())[0],0
...for each dataset,0
,0
download_lila_subset.py,0
,0
"Example of how to download a list of files from LILA, e.g. all the files",0
in a data set corresponding to a particular species.,0
,0
%% Constants and imports,0
LILA camera trap master metadata file,0
"In this example, we're using the Missouri Camera Traps data set and the Caltech Camera Traps dataset",0
All lower-case; we'll convert category names to lower-case when comparing,0
"We'll write images, metadata downloads, and temporary files here",0
"We will demonstrate two approaches to downloading, one that loops over files",0
"and downloads directly in Python, another that uses AzCopy.",0
,0
AzCopy will generally be more performant and supports resuming if the,0
transfers are interrupted.  This script assumes that azcopy is on the system path.,0
Number of concurrent download threads (when not using AzCopy) (AzCopy does its,0
own magical parallelism),0
%% Support functions,0
remove the leading '/',0
%% Download and parse the metadata file,0
Put the master metadata file in the same folder where we're putting images,0
Read lines from the master metadata file,0
Parse those lines into a table,0
Each line in this file is name/base_url/json_url/[box_url],0
%% Download and extract metadata for the datasets we're interested in,0
Unzip if necessary,0
...for each dataset of interest,0
%% List of files we're going to download (for all data sets),0
"Flat list or URLS, for use with direct Python downloads",0
For use with azcopy,0
This may or may not be a SAS URL,0
# Open the metadata file,0
# Build a list of image files (relative path names) that match the target species,0
Retrieve all the images that match that category,0
Retrieve image file names,0
Convert to URLs,0
...for each dataset,0
%% Download those image files,0
ds_name = 'Caltech Camera Traps',0
ds_name = 'SWG Camera Traps',0
We want to use the whole relative path for this script (relative to the base of the container),0
"to build the output filename, to make sure that different data sets end up in different folders.",0
This may or may not be a SAS URL,0
For example:,0
,0
caltech-unzipped/cct_images,0
swg-camera-traps,0
Check whether the URL includes a folder,0
E.g. caltech-unzipped,0
E.g. cct_images,0
E.g. swg-camera-traps,0
"Write out a list of files, and use the azcopy ""list-of-files"" option to download those files.",0
,0
This azcopy feature is unofficially documented at:,0
,0
https://github.com/Azure/azure-storage-azcopy/wiki/Listing-specific-files-to-transfer,0
,0
import clipboard; clipboard.copy(cmd),0
Loop over files,0
,0
get_lila_category_counts.py,0
,0
Count the number of images and bounding boxes with each label in one or more LILA datasets.,0
,0
"This script doesn't write these counts out anywhere other than the console, it's just intended",0
as a template for doing operations like this on LILA data.  get_lila_category_list.py writes,0
"information out to a .json file, but it counts *annotations*, not *images*, for each category.",0
,0
%% Constants and imports,0
"If None, will use all datasets",0
"We'll write images, metadata downloads, and temporary files here",0
%% Download and parse the metadata file,0
%% Download and extract metadata for the datasets we're interested in,0
%% Count categories,0
ds_name = datasets_of_interest[0],0
"Go through annotations, marking each image with the categories that are present",0
,0
ann = annotations[0],0
Now go through images and count categories,0
im = images[0],0
...for each dataset,0
%% Print the results,0
...for each dataset,0
,0
add_url_to_database.py,0
,0
"Adds a ""url"" field to a coco-camera-traps .json database, specifically to allow the db to",0
be reviewed in the Visipedia annotation tool.,0
,0
,0
remove_corrupted_images_from_database.py,0
,0
"Given a coco-camera-traps .json file, checks all images for TF-friendliness and generates",0
a new .json file that only contains the non-corrupted images.,0
,0
%% Imports and constants,0
import multiprocessing,0
"I leave this at an annoying low number, since by definition weird stuff will",0
"be happening in the TF kernel, and it's useful to keep having content in the console.",0
%% Function definitions,0
"I sometimes pass in a list of images, sometimes a dict with a single",0
element mapping a job ID to the list of images,0
"We're about to start a lot of TF sessions, and we don't want gobs",0
of debugging information printing out for every session.,0
At some point we were creating a single session and looping over images,0
"within that session, but the only way I found to reliably not run out",0
of GPU memory was to create a session per image and gc.collect() after,0
each session.,0
Map Image IDs to boolean (should I keep this image?),0
"Convert to lists, append job numbers to the image lists",0
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)",0
Merge results,0
%% Interactive driver,0
%%,0
base_dir = r'D:\temp\snapshot_serengeti_tfrecord_generation',0
Load annotations,0
Check for corruption,0
Write out only the uncorrupted data,0
%% Command-line driver,0
#######,0
,0
integrity_check_json_db.py,0
,0
"Does some integrity-checking and computes basic statistics on a db, specifically:",0
,0
* Verifies that required fields are present and have the right types,0
* Verifies that annotations refer to valid images,0
* Verifies that annotations refer to valid categories,0
"* Verifies that image, category, and annotation IDs are unique",0
,0
* Optionally checks file existence,0
,0
* Finds un-annotated images,0
* Finds unused categories,0
,0
* Prints a list of categories sorted by count,0
,0
#######,0
%% Constants and environment,0
%% Functions,0
"If baseDir is non-empty, checks image existence",0
This is used in a medium-hacky way to share modified options across threads,1
print('Image path {} does not exist'.format(filePath)),0
"#%% Read .json file if necessary, integrity-check fields",0
info = data['info'],0
"#%% Build dictionaries, checking ID uniqueness and internal validity as we go",0
Confirm that required fields are present,0
Confirm ID uniqueness,0
...for each category,0
image = images[0],0
Confirm that required fields are present,0
Confirm ID uniqueness,0
We previously supported ints here; this should be strings now,0
"assert isinstance(image['location'], str) or isinstance(image['location'], int), \",0
'Illegal image location type',0
Are we checking for unused images?,0
Recursively enumerate images,0
print('Image {} is unused'.format(p)),0
Are we checking file existence and/or image size?,0
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)",0
...for each image,0
Confirm that required fields are present,0
Confirm ID uniqueness,0
Confirm validity,0
...for each annotation,0
#%% Print statistics,0
Find un-annotated images and multi-annotation images,0
Find unused categories,0
Prints a list of categories sorted by count,0
,0
https://stackoverflow.com/questions/72899/how-do-i-sort-a-list-of-dictionaries-by-a-value-of-the-dictionary,0
...def integrity_check_json_db(),0
%% Command-line driver,0
%% Interactive driver(s),0
%%,0
Integrity-check .json files for LILA,0
options.iMaxNumImages = 10,0
,0
combine_two_json_files.py,0
,0
"Merges two coco-camera-traps .json files. In particular, categories are combined and re-numbered.",0
,0
Combined Info,0
Combined Images,0
Combined Categories,0
## categories to merge,0
Combined Annotations,0
,0
make_detection_db_for_viewing.py,0
,0
"Given a .json file with ground truth bounding boxes, and a .p file containing detections for the same images,",0
"creates a new .json file with separate classes for ground truth and detection, suitable for viewing in the Visipedia",0
annotation tool.,0
,0
%% Imports and constants,0
%% Main function,0
im_id_to_im = {im['id']:im for im in images},0
make new categories to distinguish between ground truth and detections,0
"update all gt annotations to be class ""gt""",0
collect all detections by image,0
keep any detection with score above det_thresh,0
"need to convert bbox from [x1,y1,x2,y2] to [x,y,w,h]",0
"add ""info"" and ""licenses"" for annotation tools to function",0
create new db,0
%% Command-line handling,0
,0
analyze_json_database.py,0
,0
Plots location/class/etc. distributions for classes in a coco-camera-traps .json file.,0
,0
Currently includes some one-off code for specific species.,0
,0
%% Constants and imports,0
%% Path configuration,0
%% Load source data,0
%% Build image/category dictionaries,0
%% Make plot of category distribution,0
%% make plots of location distribution,0
"plt.title('Number of images per location, by category')",0
"plt.tight_layout(rect=[0,0,1,0.9])",0
#make plot of images per season,0
%% Make plot of lions per location,0
%% Make plot of elephants per location,0
for loc in sorted_by_total[:25]:,0
"print('Location:' + loc[0] +', Lions: ' + str(len(loc_to_lion_ims[loc[0]])) + ', Elephants: ' + str(len(loc_to_elephant_ims[loc[0]])) + ', Total ims: ' + str(len(loc_to_ims[loc[0]])))",0
,0
subset_json_db.py,0
,0
Select a subset of images (and associated annotations) from a .json file,0
in COCO Camera Traps format.,0
,0
"To subset the .json files produced by our batch processing API, see",0
subset_json_detector_output.py,0
,0
Sample invocation:,0
,0
"python subset_json_db.py ""E:\Statewide_wolf_container\idfg_20190409.json"" ""E:\Statewide_wolf_container\idfg_20190409_clearcreek.json"" ""clearcreek"" --ignore_case",0
,0
%% Constants and imports,0
%% Functions,0
Load the input file if necessary,0
Find images matching the query,0
Find annotations referring to those images,0
Write the output file if requested,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
%% Constants and imports,0
%% Merge functions,0
i_input_dict = 0; input_dict = input_dicts[i_input_dict],0
We will prepend an index to every ID to guarantee uniqueness,0
Map detection categories from the original data set into the merged data set,0
...for each category,0
Merge original image list into the merged data set,0
Create a unique ID,0
...for each image,0
Same for annotations,0
...for each annotation,0
"Merge info dicts, don't check completion time fields",0
...for each dictionary,0
Convert merged image dictionaries to a sorted list,0
%% Driver,0
,0
add_width_and_height_to_database.py,0
,0
Grabs width and height from actual image files for a .json database that is missing w/h.,0
,0
Originally used when we created a .json file for snapshot serengeti from .csv.,0
,0
"This file converts the JSON output of the batch processing API to a pickle file, which can be used by the",0
script ./make_classification_dataset.py,0
"Convert boxes from JSON   [x_min, y_min, width_of_box, height_of_box]",0
"to PICKLE [ymin,  xmin,  ymax,         xmax]",0
Write detections to file with pickle,0
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the",0
script ./make_classification_dataset.py,0
Write detections to file with pickle,0
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the",0
script ./make_classification_dataset.py,0
Assumes that the root of the CameraTrap repo is in the PYTHONPATH,0
Minimum threshold to put a detection into the output JSON file,0
Parameter check,0
Load detections from input,0
Load COCO style annotations,0
Build output JSON in format version 1.0,0
Adding the only known metadata info,0
"The pickle file does not contain category information, so we assume the default",0
For each image with detections,0
for each detection,0
"Convert boxes from [ymin, xmin, ymax, xmax] format to",0
"[x_min, y_min, width_of_box, height_of_box]",0
Write output json,0
global flag for whether or not we encounter missing images,0
"- will only print ""missing image"" warning once",0
TFRecords variables,0
1.3 for the cropping during test time and 1.3 for the context that the,0
CNN requires in the left-over image,0
Create output directories,0
Load COCO style annotations from the input dataset,0
"Get all categories, their names, and create updated ID for the json file",0
Prepare the coco-style json files,0
Split the dataset by locations,0
Load detections,0
Write out COCO-style json files to the output directory,0
Write detections to file with pickle,0
## Preparations: get all the output tensors,0
For all images listed in the annotations file,0
Skip the image if it is annotated with more than one category,0
"Get ""old"" and ""new"" category ID and category name for this image.",0
Skip if in excluded categories.,0
get path to image,0
"If we already have detection results, we can use them",0
Otherwise run detector and add detections to the collection,0
Only select detections with confidence larger than DETECTION_THRESHOLD,0
Skip image if no detection selected,0
whether it belongs to a training or testing location,0
Skip images that we do not have available right now,0
- this is useful for processing parts of large datasets,0
Load image,0
Run inference,0
"remove batch dimension, and convert from float32 to appropriate type",0
convert normalized bbox coordinates to pixel coordinates,0
Pad the detected animal to a square box and additionally by,0
"PADDING_FACTOR. The result will be in crop_boxes. However, we need to make",0
sure that its box coordinates are still within the image.,0
"for each bounding box, crop the image to the padded box and save it",0
"Create the file path as it will appear in the annotation json,",0
adding the box number if there are multiple boxes,0
"if the cropped file already exists, verify its size",0
Add annotations to the appropriate json,0
js_keys = ['/'.join(im['file_name'].split('/')[1:])[:-4] for im in js['images']],0
for tk in js_keys:,0
"assert np.isclose(1, np.sum(detections[tk]['detection_scores'] > 0.5))",0
,0
separate_detections_by_size,0
,0
Not-super-well-maintained script to break a list of API output files up,0
based on bounding box size.,0
,0
%% Imports and constants,0
Folder with one or more .json files in it that we want to split up,0
Enumerate .json files,0
Define size thresholds and confidence thresholds,0
"Not used directly in this script, but useful if we want to generate previews",0
%% Split by size,0
For each size threshold...,0
For each file...,0
fn = input_files[0],0
Just double-checking; we already filtered this out above,0
Don't reprocess .json files we generated with this script,0
Load the input file,0
For each image...,0
1.1 is the same as infinity here; no box can be bigger than a whole image,0
What's the smallest detection above threshold?,0
"[x_min, y_min, width_of_box, height_of_box]",0
,0
size = w * h,0
...for each detection,0
Which list do we put this image on?,0
...for each image in this file,0
Make sure the number of images adds up,0
Write out all files,0
...for each size threshold,0
...for each file,0
,0
tile_images.py,0
,0
Split a folder of images into tiles.  Preserves relative folder structure in a,0
"new output folder, with a/b/c/d.jpg becoming, e.g.:",0
,0
a/b/c/d_row_0_col_0.jpg,0
a/b/c/d_row_0_col_1.jpg,0
,0
%% Imports and constants,0
from ai4eutils,0
%% Main function,0
TODO: parallelization,1
,0
i_fn = 2; relative_fn = image_relative_paths[i_fn],0
Can we skip this image because we've already generated all the tiles?,0
TODO: super-sloppy that I'm pasting this code from below,1
From:,0
,0
https://github.com/whiplashoo/split-image/blob/main/src/split_image/split.py,0
i_col = 0; i_row = 1,0
left/top/right/bottom,0
...for each row,0
...for each column,0
...for each image,0
%% Interactive driver,0
%%,0
%%,0
,0
rde_debug.py,0
,0
Some useful cells for comparing the outputs of the repeat detection,0
"elimination process, specifically to make sure that after optimizations,",0
results are the same up to ordering.,0
,0
%% Compare two RDE files,0
i_dir = 0,0
break,0
"Regardless of ordering within a directory, we should have the same",0
number of unique detections,0
Re-sort,0
Make sure that we have the same number of instances for each detection,0
Make sure the box values match,0
,0
aggregate_video.py,0
,0
Aggregate results and render output video for a video that's already been run through MD,0
,0
%% Constants,0
%% Processing,0
im = d['images'][0],0
...for each detection,0
This is no longer included in output files by default,0
# Split into frames,0
# Render output video,0
## Render detections to images,0
## Combine into a video,0
,0
ocr_sandbox.py,0
,0
sandbox for experimenting with using OCR to pull metadata from camera trap images,0
,0
The general approach is:,0
,0
"* Crop a fixed percentage from the top and bottom of an image, slightly larger",0
than the largest examples we've seen of how much space is used for metadata.,0
,0
"* Refine that crop by blurring a little, then looking for huge peaks in the",0
"color histogram suggesting a solid background, then finding rows that are",0
mostly that color.,0
,0
"* Crop to the refined crop, then run pytesseract to extract text",0
,0
"* Use regular expressions to find time and date, in the future can add, e.g.,",0
"temperature (which is often present *only* in the images, unlike time/date which",0
are also usually in EXIF but often wrong or lost in processing),0
,0
"The metadata extraction (EXIF, IPTC) here is just sample code that seemed to",0
belong in this file.,0
,0
Contact: Dan Morris (cameratraps@lila.science),0
,0
%% Constants and imports,0
pip install pytesseract,0
,0
"Also intall tesseract from: https://github.com/UB-Mannheim/tesseract/wiki, and add",0
"the installation dir to your path (on Windows, typically C:\Program Files (x86)\Tesseract-OCR)",0
pip install IPTCInfo3,0
from the ai4eutils repo: https://github.com/Microsoft/ai4eutils,0
,0
"Only used for writing out a summary, not important for the core metadata extraction",0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Using a semi-arbitrary metric of how much it feels like we found the,0
"text-containing region, discard regions that appear to be extraction failures",0
Pad each crop with a few pixels to make tesseract happy,0
Discard text from the top,0
"When we're looking for pixels that match the background color, allow some",0
tolerance around the dominant color,0
We need to see a consistent color in at least this fraction of pixels in our rough,0
crop to believe that we actually found a candidate metadata region.,0
"What fraction of the [top,bottom] of the image should we use for our rough crop?",0
A row is considered a probable metadata row if it contains at least this fraction,0
"of the background color.  This is used only to find the top and bottom of the crop area,",0
"so it's not that *every* row needs to hit this criteria, only the rows that are generally",0
above and below the text.,0
%% Support functions,0
"%% Load some images, pull EXIF and IPTC data for fun",0
%% Rough crop,0
"This will be an nImages x 1 list of 2 x 1 lists (image top, image bottom)",0
image = images[0],0
"l,t,r,b",0
,0
"0,0 is upper-left",0
"%% Close-crop around the text, return a revised image and success metric",0
Did we find a sensible mode that looks like a background value?,0
"This looks very scientific, right?  Definitely a probability?",0
"print('Failed min background fraction test: {} of {}'.format(pBackGroundValue,minBackgroundFraction))",0
"Notes to self, things I tried that didn't really go anywhere...",0
"analysisImage = cv2.blur(analysisImage, (3,3))",0
"analysisImage = cv2.medianBlur(analysisImage,5)",0
"analysisImage = cv2.Canny(analysisImage,100,100)",0
imagePil = Image.fromarray(analysisImage); imagePil,0
Use row heuristics to refine the crop,0
,0
This egregious block of code makes me miss my fluency in Matlab.,1
"print('Cropping to {},{},{},{}'.format(x,y,w,h))",0
Crop the image,0
"For some reason, tesseract doesn't like characters really close to the edge",0
imagePil = Image.fromarray(croppedImage); imagePil,0
%% Go to OCR-town,0
"An nImages x 2 list of strings, extracted from the top and bottom of each image",0
An nImages x 2 list of cropped images,0
iImage = 0; iRegion = 1; regionSet = imageRegions[iImage]; region = regionSet[iRegion],0
text = pytesseract.image_to_string(region),0
pil --> cv2,0
"image = cv2.medianBlur(image, 3)",0
"image = cv2.erode(image, None, iterations=2)",0
"image = cv2.dilate(image, None, iterations=4)",0
"image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]",0
"image = cv2.blur(image, (3,3))",0
"image = cv2.copyMakeBorder(image,10,10,10,10,cv2.BORDER_CONSTANT,value=[0,0,0])",0
"text = pytesseract.image_to_string(imagePil, lang='eng')",0
https://github.com/tesseract-ocr/tesseract/wiki/Command-Line-Usage,0
"psm 6: ""assume a single uniform block of text""",0
,0
...for each cropped region,0
...for each image,0
%% Extract dates and times,0
s = '1:22 pm',0
s = '1:23:44 pm',0
%% Write results to a handy html file,0
Add image name and resized image,0
Add results and individual region images,0
"textStyle = ""font-family:calibri,verdana,arial;font-weight:bold;font-size:150%;text-align:left;margin-left:50px;""",0
%% Scrap,0
Alternative approaches to finding the text/background  region,0
Using findCountours(),0
imagePil = Image.fromarray(analysisImage); imagePil,0
"analysisImage = cv2.erode(analysisImage, None, iterations=3)",0
"analysisImage = cv2.dilate(analysisImage, None, iterations=3)",0
"analysisImage = cv2.threshold(analysisImage, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]",0
Find object with the biggest bounding box,0
Using connectedComponents(),0
analysisImage = image,0
print('Found {} components'.format(nb_components)),0
We just want the *background* image,0
,0
"Takes a .json file with MD results for an individual video, and converts to a .csv that includes",0
frame times.  This is very bespoke to animal detection and does not include other classes.,0
,0
%% Imports and constants,0
Only necessary if you want to extract the sample rate from the video,0
%% Extract the sample rate if necessary,0
%% Load results,0
%% Convert to .csv,0
i_image = 0; im = results['images'][i_image],0
,0
umn-pr-analysis.py,0
,0
Precision/recall analysis for UMN data,0
,0
%% Imports and constants,0
results_file = results_file_filtered,0
"For two deployments, we're only processing imagse in the ""detections"" subfolder",0
String to remove from MegaDetector results,0
%% Enumerate deployment folders,0
%% Load MD results,0
im = md_results['images'][0],0
%% Load ground truth,0
i_row = 0; row = ground_truth_df.iloc[i_row],0
%% Create relative paths for ground truth data,0
"Some deployment folders have no subfolders, e.g. this is a valid file name:",0
,0
M00/01010132.JPG,0
,0
"But some deployment folders have subfolders, e.g. this is also a valid file name:",0
,0
N17/100EK113/07160020.JPG,0
,0
"So we can't find files by just concatenating folder and file names, we have to enumerate and explicitly",0
"map what will appear in the ground truth as ""folder/filename"" to complete relative paths.",0
deployment_name = list(deployment_folders)[0],0
Enumerate all files in this folder,0
"print('Enumerated {} files for deployment {}'.format(len(files),deployment_name))",0
filename = files[100],0
...for each file in this deployment,0
...for each deployment,0
%% Map relative paths to MD results,0
%% Add relative paths to our ground truth table,0
i_row = 0; row = ground_truth_df.iloc[i_row],0
"row['filename'] looks like, e.g. A01/01080001.JPG.  This is not actually a path, it's",0
"just the deployment ID and the image name, separated by a slash.",0
Find the relative path for this image,0
Make sure we have MegaDetector results for this file,0
Make sure this image file exists,0
...for each row in the ground truth table,0
%% Take everything out of Pandas,0
%% Some additional error-checking of the ground truth,0
An early version of the data required consistency between common_name and is_blank,0
%% Combine MD and ground truth results,0
d = ground_truth_dicts[0],0
Find the maximum confidence for each category,0
...for each detection,0
...for each image,0
%% Precision/recall analysis,0
...for each image,0
Thresholds go up throughout precisions/recalls/thresholds; find the last,0
value where recall is at or above target.  That's our precision @ target recall.,0
Flatten the confusion matrix,0
Write precision/recall plot to .png file in output directory,0
pr_figure_relative_filename = 'prec_recall.png',0
"pr_figure_filename = os.path.join(output_dir, pr_figure_relative_filename)",0
plt.show(block=False),0
"precision_recall_analysis(human_gt_labels,human_prediction_probs,'humans')",0
%% Find and manually review all images of humans,0
%%,0
"...if this image is annotated as ""human""",0
...for each image,0
%% Find and manually review all MegaDetector animal misses,0
%%,0
im = merged_images[0],0
GT says this is not an animal,0
GT says this is an animal,0
%% Convert .json to .csv,0
%%,0
,0
kga-pr-analysis.py,0
,0
Precision/recall analysis for KGA data,0
,0
%% Imports and constants,0
%% Load and filter MD results,0
%% Load and filter ground truth,0
%% Map images to image-level results,0
%% Map sequence IDs to images and annotations to images,0
Verify consistency of annotation within a sequence,0
TODO,1
%% Find max confidence values for each category for each sequence,0
seq_id = list(sequence_id_to_images.keys())[1000],0
im = images_this_sequence[0],0
det = md_result['detections'][],0
...for each detection,0
...for each image in this sequence,0
...for each sequence,0
%% Prepare for precision/recall analysis,0
seq_id = list(sequence_id_to_images.keys())[1000],0
cat_id = list(category_ids_this_sequence)[0],0
...for each category in this sequence,0
...for each sequence,0
%% Precision/recall analysis,0
"Confirm that thresholds are increasing, recall is decreasing",0
This is not necessarily true,0
assert np.all(precisions[:-1] <= precisions[1:]),0
Thresholds go up throughout precisions/recalls/thresholds; find the max,0
value where recall is at or above target.  That's our precision @ target recall.,0
"This is very slightly optimistic in its handling of non-monotonic recall curves,",0
but is an easy scheme to deal with.,0
Flatten the confusion matrix,0
Write precision/recall plot to .png file in output directory,0
pr_figure_relative_filename = 'prec_recall.png',0
"pr_figure_filename = os.path.join(output_dir, pr_figure_relative_filename)",0
plt.show(block=False),0
"precision_recall_analysis(human_gt_labels,human_prediction_probs,'humans')",0
%% Scrap,0
%% Find and manually review all sequence-level MegaDetector animal misses,0
input_base = '/media/user/lila-01/lila/snapshot-safari/KGA/KGA_public',0
i_sequence = 0; seq_id = pr_sequence_ids[i_sequence],0
i_seq = 0; seq_id = false_negative_sequences[i_seq],0
"sequence_folder = os.path.join(sequence_preview_dir,'seq_{}'.format(str(i_seq).zfill(3)))",0
fn = image_files[0],0
"print('Copying {} to {}'.format(input_path,output_path))",0
...for each file in this sequence.,0
...for each sequence,0
%% Image-level postprocessing,0
parse arguments,0
check if a GPU is available,0
load a pretrained embedding model,0
setup experiment,0
load the embedding model,0
setup the target dataset,0
setup finetuning criterion,0
setup an active learning environment,0
create a classifier,0
the main active learning loop,0
Active Learning,0
finetune the embedding model and load new embedding values,0
gather labeled pool and train the classifier,0
save a snapshot,0
Load a checkpoint if necessary,0
setup the training dataset and the validation dataset,0
setup data loaders,0
check if a GPU is available,0
create a model,0
setup loss criterion,0
define optimizer,0
load a checkpoint if provided,0
setup a deep learning engine and start running,0
train the model,0
train for one epoch,0
evaluate on validation set,0
save a checkpoint,0
open the file,0
read it,0
do the substitution,0
matplotlib.use('Agg'),0
from UIComponents.DBObjects import *,0
Initialize Database,0
# database connection credentials,0
# try to connect as USER to database DB_NAME through peewee,0
Load the saved embedding model,0
dataset_query = Detection.select().limit(5),0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})",0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getalllabels()) , dataset.getallpaths(), {})",0
Random examples to start,0
"random_ids = np.random.choice(dataset.current_set, 1000, replace=False).tolist()",0
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)",0
print(random_ids),0
Move Records,0
"moveRecords(dataset, DetectionKind.ModelDetection.value, DetectionKind.UserDetection.value, random_ids)",0
#print([len(x) for x in dataset.set_indices]),0
# Finetune the embedding model,0
#dataset.set_kind(DetectionKind.UserDetection.value),0
#dataset.train(),0
"#train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)",0
"#finetune_embedding(model, checkpoint['loss_type'], dataset, 32, 4, 100)",0
#save_checkpoint({,0
"#        'arch': model.arch,",0
"#        'state_dict': model.state_dict(),",0
"#        'optimizer' : optimizer.state_dict(),",0
"#        'loss_type' : loss_type,",0
"#        }, False, ""%s%s_%s_%04d.tar""%('finetuned', loss_type, model.arch, len(dataset.set_indices[DetectionKind.UserDetection.value])))",0
Get indices of samples to get user to label,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
"kwargs[""already_selected""].extend(indices)",0
Train on samples that have been labeled so far,0
Test on the samples that have not been labeled,0
"'optimizer' : optimizer.state_dict(),",0
num_classes= len(train_dataset.getClassesInfo()[0]),0
"print(""Num Classes= ""+str(num_classes))",0
define loss function (criterion) and optimizer,0
"optimizer = torch.optim.SGD(params, momentum = 0.9, lr = args.lr, weight_decay = args.weight_decay)",0
optimizer.load_state_dict(checkpoint['optimizer']),0
train for one epoch,0
"adjust_lr(optimizer,epoch)",0
if epoch % 1 == 0 and epoch > 0:,0
"a, b, c = e.predict(train_embd_loader, load_info = True, dim = args.feat_dim)",0
"plot_embedding(reduce_dimensionality(a), b, c, {})",0
evaluate on validation set,0
--------some stuff needed to get AJAX to work with bottle?--------#,0
-------------------------------------------------------------------------------- #,0
PREPARE TO QUEUE IMAGES FOR LABELING,0
-------------------------------------------------------------------------------- #,0
# Connect as USER to database DB_NAME through peewee and initialize database proxy,0
# Load embedding model,0
---------------------------------------------------------------------- #,0
CREATE QUEUE OF IMAGES TO LABEL,0
---------------------------------------------------------------------- #,0
Use classifier to generate predictions,0
# Update model predicted class in PostgreSQL database,0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?,0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
-------------------------------------------------------------------------------- #,0
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI,0
-------------------------------------------------------------------------------- #,0
"# static routes (to serve CSS, etc.)",0
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps",0
"return bottle.static_file(filename, root='../../../../../../../../../../../.')",0
# dynamic routes,0
if data['display_grayscale']:,0
indices_to_exclude.update(set(color_indices)),0
elif not data['display_grayscale']:,0
indices_to_exclude.update(set(grayscale_indices)),0
data['display_images'] = {},0
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices],0
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices],0
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices],0
data['display_images']['detection_categories'] = [],0
for i in indices:,0
if str(dataset.samples[i][1]) == 'None':,0
data['display_images']['detection_categories'].append('None'),0
else:,0
existing_category_entries = {cat.id: cat.name for cat in Category.select()},0
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()",0
data['display_images']['detection_categories'].append(cat_name),0
Use image ids in images_to_label to get the corresponding dataset indices,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
print(set(dataset.set_indices[4]).update(set(indices_to_label))),0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Use image id images_to_label to get the corresponding dataset index,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?,0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Train on samples that have been labeled so far,0
dataset.set_kind(DetectionKind.UserDetection.value),0
print(y_train),0
Predict on the samples that have not been labeled,0
print(y_pred),0
Update model predicted class in PostgreSQL database,0
timer = time.time(),0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
print('Updating the database took %0.2f seconds'%(time.time() - timer)),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids)),0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
"once the classifier has been trained the first time, switch to AL sampling",0
# sourceMappingURL=bootstrap.bundle.min.js.map,0
"print(ap_distances.size(),an_distances.size())",0
losses = -(((-ap_distances)/128)+1+1e-16).log() - (((-(128-an_distances))/128)+1+1e-16).log(),0
import pdb,0
pdb.set_trace(),0
losses = ap_distances - an_distances + self.margin,0
print(losses.size()),0
from UIComponents.DBObjects import *,0
TODO: should this also change self.kind?,1
get the embedding representations for all samples (i.e. set current_set to all indices),0
"return PILImage.open(os.path.join(self.img_base,path)).convert('RGB')",0
"print(self.labels_set, self.n_classes)",0
from sklearn.manifold import TSNE,0
embedding= TSNE(n_components=2).fit_transform(X),0
embedding= PCA(n_components=2).fit_transform(X),0
return X,0
"print(dir(event), type(sc))",0
"print(label,bgcolor)",0
"patches.append(mpatches.Patch(color=indexcolors[i], label=label))",0
"plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,",0
"ncol=12, mode=""expand"", borderaxespad=0., handles=patches)",0
plt.legend(handles=patches),0
"plt.xlabel('Dim 1', fontsize=12)",0
"plt.ylabel('Dim 2', fontsize=12)",0
plt.grid(True),0
"plt.xlabel('Dim 1', fontsize=12)",0
"plt.ylabel('Dim 2', fontsize=12)",0
plt.grid(True),0
print(thumb),0
"img.thumbnail((16, 12), PILImage.ANTIALIAS)",0
plt.show(),0
Add all negatives for all positive pairs,0
print(triplets.shape[0]),0
compute output,0
measure accuracy,0
compute loss on this batch,0
"train on a batch, record loss, and measure accuracy (if calc_accuracy)",0
compute output,0
measure accuracy and record loss,0
switch to evaluate mode,0
compute output,0
switch to evaluate mode,0
"self.fc13 = nn.Linear(128, 64)",0
self.bn2 = nn.BatchNorm1d(64),0
x = F.relu(self.fc12(x)),0
x = F.relu(self.bn1(self.fc13(x))),0
x = F.relu(self.fc13(x)),0
"x = F.dropout(x, training=self.training)",0
save features last FC layer,0
x = F.relu(x),0
save features last FC layer,0
import matplotlib.pyplot as plt,0
plt.switch_backend('agg'),0
class EmbeddingNet(nn.Module):,0
"def __init__(self, architecture, feat_dim, use_pretrained=False):",0
"super(EmbeddingNet, self).__init__()",0
self.feat_dim= feat_dim,0
self.inner_model = models.__dict__[architecture](pretrained=use_pretrained),0
if architecture.startswith('resnet'):,0
in_feats= self.inner_model.fc.in_features,0
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)",0
elif architecture.startswith('inception'):,0
in_feats= self.inner_model.fc.in_features,0
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)",0
if architecture.startswith('densenet'):,0
in_feats= self.inner_model.classifier.in_features,0
"self.inner_model.classifier = nn.Linear(in_feats, feat_dim)",0
if architecture.startswith('vgg'):,0
in_feats= self.inner_model.classifier._modules['6'].in_features,0
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)",0
if architecture.startswith('alexnet'):,0
in_feats= self.inner_model.classifier._modules['6'].in_features,0
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)",0
"def forward(self, x):",0
return self.inner_model.forward(x),0
class NormalizedEmbeddingNet(EmbeddingNet):,0
"def __init__(self, architecture, feat_dim, use_pretrained=False):",0
"EmbeddingNet.__init__(self, architecture, feat_dim, use_pretrained = use_pretrained)",0
"def forward(self, x):",0
embedding =  F.normalize(self.inner_model.forward(x))*10.0,0
"return embedding, embedding",0
"def get_random_images(num, image_dir, test_transforms):",0
"data = datasets.ImageFolder(image_dir, transform=test_transforms) # slight abuse; this expects subfolders corresponding to classes but we have no classes here",0
indices = list(range(len(data))),0
np.random.shuffle(indices),0
idx = indices[:num],0
from torch.utils.data.sampler import SubsetRandomSampler,0
sampler = SubsetRandomSampler(idx),0
"loader = torch.utils.data.DataLoader(data,",0
"sampler=sampler, batch_size=num)",0
dataiter = iter(loader),0
"images, labels = dataiter.next()",0
"return images, labels",0
"def predict_image(image, model, test_transforms):",0
"device = torch.device(""cuda"" if torch.cuda.is_available()",0
"else ""cpu"")",0
image_tensor = test_transforms(image).float(),0
image_tensor = image_tensor.unsqueeze_(0),0
input = Variable(image_tensor),0
input = input.to(device),0
output = model(input)[0],0
return output.data.cpu().numpy(),0
Connect to database and initialize db_proxy,0
# database connection credentials,0
# load the dataset,0
Load the saved embedding model from the checkpoint,0
# update the dataset embedding,0
# Create a folder for saving embedding visualizations with this model checkpoint,0
model_emb_dirname = os.path.basename(args.base_model).split('.')[0],0
"os.makedirs(model_emb_dirname, exist_ok=True)",0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')",0
dataset.embedding_mode(),0
"assert 2==3, 'break'",0
datasetindices = list(range(len(dataset))),0
np.random.shuffle(datasetindices),0
random_indices = datasetindices[:args.num],0
print(random_indices),0
"selected_sample_features = np.array([]).reshape(0, 256)",0
selected_sample_labels = [],0
for idx in random_indices:,0
"selected_sample_features = np.vstack([selected_sample_features, X_train[idx]])",0
selected_sample_labels.append(y_train[idx]),0
img_path = imagepaths[idx].split('.JPG')[0],0
image = dataset.loader(img_path),0
selected_sample_images.append(image),0
# TRY NEAREST NEIGHBORS WALK THROUGH EMBEDDING,0
nbrs = NearestNeighbors(n_neighbors=args.num).fit(selected_sample_features),0
"distances, indices = nbrs.kneighbors(selected_sample_features)",0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')",0
"idx_w_closest_nbr = np.where(distances[:,1] == min(distances[:,1]))[0][0]",0
order = [idx_w_closest_nbr],0
for ii in range(len(distances)):,0
"distances[ii, 0] = np.inf",0
while len(order)<args.num:,0
curr_idx = order[-1],0
curr_neighbors = indices[curr_idx],0
curr_dists = list(distances[curr_idx]),0
# print(min(curr_dists)),0
next_closest_pos = curr_dists.index(min(curr_dists)),0
next_closest = curr_neighbors[next_closest_pos],0
order.append(next_closest),0
# make sure you can't revisit past nodes,0
for vi in order:,0
vi_pos = list(indices[next_closest]).index(vi),0
"distances[next_closest, vi_pos] = np.inf",0
for ii in range(len(order)):,0
imgidx = order[ii],0
image = selected_sample_images[imgidx],0
"image.save(model_emb_dirname+""/img""+str(ii)+""_""+str(selected_sample_labels[imgidx])+"".png"")",0
# Specify the transformations on the input images before inference,0
"# test_transforms = transforms.Compose([transforms.Resize([224, 224]), transforms.ToTensor()])",0
"test_transforms = transforms.Compose([transforms.Resize([256, 256]), transforms.RandomCrop([224, 224]), transforms.RandomHorizontalFlip(), transforms.ColorJitter(), transforms.ToTensor(), transforms.Normalize([0.407328, 0.407328, 0.407328], [0.118641, 0.118641, 0.118641])])",0
"images, labels = get_random_images(num, image_dir, test_transforms)",0
"all_features = np.array([]).reshape(0, 256)",0
for ii in range(len(images)):,0
image = to_pil(images[ii]),0
"features = predict_image(image, model, test_transforms)",0
"all_features = np.vstack([all_features, features])",0
# for ii in range(len(images)):,0
#     image = to_pil(images[ii]),0
"#     image.save(""img""+str(ii)+"".png"")",0
# TRY CLUSTERING,0
kmeans1 = KMeans(n_clusters=5).fit(StandardScaler().fit_transform(all_features)),0
print(kmeans1.labels_),0
for ii in range(len(images)):,0
image = to_pil(images[ii]),0
"filename = str(kmeans1.labels_[ii])+""/img""+str(ii)+"".png""",0
if not os.path.exists(os.path.dirname(filename)):,0
os.makedirs(os.path.dirname(filename)),0
image.save(filename),0
class SaveFeatures():,0
"def __init__(self, module):",0
self.hook = module.register_forward_hook(self.hook_fn),0
"def hook_fn(self, module, input, output):",0
"self.features = torch.tensor(output, requires_grad=True).cuda()",0
def close(self):,0
self.hook.remove(),0
Load the saved embedding model from the checkpoint,0
"Get a sample from the database, with eval transforms applied, etc.",0
Connect to database and sample a dataset,0
output = model.forward(sample_image.unsqueeze(0)),0
print(output),0
with torch.no_grad():,0
sample_image_input = sample_image.cuda(non_blocking=True),0
"_, output = model(sample_image_input) # compute output",0
print(output),0
sample_image = PILImage.open(sample_image_path).convert('RGB'),0
"sample_image = transforms.Compose([Resize([256, 256]), CenterCrop(([[224,224]])), ToTensor(), Normalize([0.369875, 0.388726, 0.347536], [0.136821, 0.143952, 0.145229])])(sample_image)",0
print(list(model_inner_resnet.children())),0
print(model_inner_resnet.fc),0
print(model_inner_resnet.fc0),0
# print(model_inner_resnet.layer4[0].conv2),0
# print(type(model)),0
# print(len(list(model_inner_resnet.children()))),0
# print(list(model.children())),0
# print(list(list(model.children())[0].children())),0
"img = np.uint8(np.random.uniform(150, 180, (56, 56, 3)))/255",0
"img_tensor = torch.unsqueeze(torch.from_numpy(img), 0)",0
full_out = model_inner_resnet.forward(img_tensor),0
print(full_out),0
model(img_tensor),0
activations = SaveFeatures(model_inner_resnet.layer4[0].conv2),0
print(activations.features),0
print(type(activations.features)),0
activations.close(),0
Connect to database and sample a dataset,0
Load the saved embedding model from the checkpoint,0
Update the dataset embedding,0
Get a random query image,0
# # # IMAGES IN THE SAME SEQUENCE # # # #,0
"assert 2==3, 'break'",0
# # # CLOSEST IN (EMBEDDING) FEATURE SPACE # # # #,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Variables for the hierarchical cluster,0
Variables for the AL algorithm,0
Data variables,0
"connectivity = kneighbors_graph(self.transformed_X,max_features)",0
Fit cluster and update cluster variables,0
The sklearn hierarchical clustering algo numbers leaves which correspond,0
to actual datapoints 0 to n_points - 1 and all internal nodes have,0
ids greater than n_points - 1 with the root having the highest node id,0
"If no labels have been observed, simply return uniform distribution",0
"If no observations, return worst possible upper lower bounds",0
Loop through generations from bottom to top,0
Update admissible labels for node,0
Calculate score,0
Determine if node should be split,0
Make sure label set for node so that we can flow to children,0
if necessary,0
Only split if all ancestors are admissible nodes,0
This is part  of definition of admissible pruning,0
Check that pruning covers all leave nodes,0
Fill in labels,0
Observe labels for previously recommended batches,0
TODO(lishal): implement multiple selection methods,1
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
A list of initialized samplers is allowed as an input because,0
"for AL_methods that search over different mixtures, may want mixtures to",0
have shared AL_methods so that initialization is only performed once for,0
computation intensive methods like HierarchicalClusteringAL and,0
states are shared between mixtures.,0
"If initialized samplers are not provided, initialize them ourselves.",0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copy these objects to make sure they are not modified while simulating,0
trajectories as they are used later by the main run_experiment script.,0
Assumes that model has already by fit using all labeled data so,0
the probabilities can be used immediately to hallucinate labels,0
All models need to have predict_proba method,0
Hallucinate labels for selected datapoints to be label,0
using class probabilities from model,0
"Not saving already_selected here, if saving then should sort",0
only for the input to fit but preserve ordering of indices in,0
already_selected,0
Useful to know how accuracy compares for model trained on hallucinated,0
labels vs trained on true labels.  But can remove this train to speed,0
up simulations.  Won't speed up significantly since many more models,0
are being trained inside the loop above.,0
Save trajectory for reference,0
Delete created copies,0
THE INPUTS CANNOT BE MODIFIED SO WE MAKE COPIES FOR THE CHECK LATER,0
Should check model but kernel_svm does not have coef_ so need better,0
handling here,0
Make sure that model object fed in did not change during simulations,0
Return indices based on return type specified,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Set gamma for gaussian kernel to be equal to 1/n_features,0
kneighbors graph is constructed using k=10,0
"Make connectivity matrix symmetric, if a point is a k nearest neighbor of",0
"another point, make it vice versa",0
Graph edges are weighted by applying gaussian kernel to manhattan dist.,0
"By default, gamma for rbf kernel is equal to 1/n_features but may",0
get better results if gamma is tuned.,0
Define graph density for an observation to be sum of weights for all,0
edges to the node representing the datapoint.  Normalize sum weights,0
by total number of neighbors.,0
"If a neighbor has already been sampled, reduce the graph density",0
for its direct neighbors to promote diversity.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Update observed reward and arm probabilities,0
Sample an arm,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
y only used for determining how many clusters there should be,0
probably not practical to assume we know # of classes before hand,0
should also probably scale with dimensionality of data,0
Probably okay to always use MiniBatchKMeans,0
Should standardize data before clustering,0
Can cluster on standardized data but train on raw features if desired,0
TODO(lishal): have MarginSampler and this share margin function,1
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Probably okay to always use MiniBatchKMeans,0
Should standardize data before clustering,0
Can cluster on standardized data but train on raw features if desired,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Update min_distances for all examples given new cluster center.,0
Assumes that the transform function takes in original data and not,0
flattened data.,0
Initialize centers with a randomly selected datapoint,0
New examples should not be in already selected since those points,0
should have min_distance of zero to a cluster center.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
This is uniform given the remaining pool but biased wrt the entire pool.,0
sample = [i for i in range(self.X.shape[0]) if i not in already_selected],0
return sample[0:N],0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Fields for hierarchical clustering AL,0
Setting parent and storing nodes in dict for fast access,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Connect to database DB_NAME as USER and initialize tables,0
Populate Info table,0
Populate Category table,0
"# For now, we have a predefined list of species we expect to see in the camera trap database (e.g. maybe from a quick look through the images)",0
# TODO: allow user to update the class list through the labeling tool UI as they see different species,1
Populate Image and Detection tables,0
Image entry data,0
Detection entry data,0
Connect to database DB_NAME as USER and initialize tables,0
Populate Info table,0
Populate Category table,0
# Create category records from species present in the COCO camera trap classification dataset (COCO classes minus any excluded classes),0
# TODO: allow user to update the class list through the labeling tool UI as they see different species,1
Populate Image and Detection tables,0
"with open(os.path.join(args.crop_dir,'crops.json'), 'r') as infile:",0
crops_json = json.load(infile),0
counter = 0,0
timer = time.time(),0
num_detections = len(crops_json),0
for detectionid in crops_json:,0
counter += 1,0
detection_data = crops_json[detectionid],0
# Image entry data,0
existing_image_entries = Image.select().where((Image.file_name == detection_data['file_name'])),0
try:,0
existing_image_entry = existing_image_entries.get(),0
except:,0
"image_entry = Image.create(id=detectionid, file_name=detection_data['file_name'], width=detection_data['width'], height=detection_data['height'], grayscale=detection_data['grayscale'],",0
"source_file_name=detection_data['source_file_name'], relative_size=detection_data['relative_size'],",0
"seq_id=detection_data['seq_id'], seq_num_frames=detection_data['seq_num_frames'], frame_num=detection_data['frame_num'])",0
image_entry.save(),0
# Detection entry data,0
"detection_entry = Detection.create(id=detectionid, image=detectionid, bbox_confidence=detection_data['bbox_confidence'],",0
"bbox_X1=detection_data['bbox_X1'], bbox_Y1=detection_data['bbox_Y1'], bbox_X2=detection_data['bbox_X2'], bbox_Y2=detection_data['bbox_Y2'],",0
kind=DetectionKind.ModelDetection.value),0
detection_entry.save(),0
if counter%100 == 0:,0
"print('Updated database with Image and Detection table entries for %d out of %d crops in %0.2f seconds'%(counter, num_detections, time.time() - timer))",0
# data related to original image,0
Get class names from .txt list,0
Initialize Oracle table,0
Map filenames to classes (NOTE: we assume a single image does not contain more than one class),0
"For each detection, use source image path to get class",0
TODO update: Assumes that crops have already,1
been generated for the images using make_active_learning_classification_dataset.py. The created DB contains tables:,0
- info: information about the dataset,0
- image: images present in the dataset,0
- detections: crops of images with detections with confidence greater than a specified threshold,0
Initialize Database,0
# database connection credentials,0
HOST = 'localhost',0
PORT = 5432,0
"# first, make sure the (user, password) has been created",0
"# sudo -u postgres psql -c ""CREATE USER <db_user> WITH PASSWORD <db_password>;""",0
"# sudo -u postgres psql -c ""CREATE DATABASE <db_name> WITH OWNER <db_user> CONNECTION LIMIT -1;""",0
"# sudo -u postgres psql -c ""GRANT CONNECT ON DATABASE <db_name> TO <db_user>;""",0
"# sudo -u postgres psql -d <db_name> -c ""CREATE EXTENSION IF NOT EXISTS \""uuid-ossp\"";""",0
# Try to connect as USER to database DB_NAME through peewee,0
Populate Tables,0
# create Info table,0
# get class names for Category table,0
Faster anD available in Python 3.5 and above,0
# iterate through images in each class folder,0
killing this process after over 38 hours adding over 500k white-tailed deer crops from emammal,0
resuming for remaining classes,0
# get cropped image data for Image table,0
"if mean of each channel is about the same, image is likely grayscale",0
# still have no info on these:,0
seq_id = CharField(null= True)                # sequence identifier for the original image,0
seq_num_frames = IntegerField(null = True)    # number of frames in sequence,0
frame_num = IntegerField(null = True)         # which frame number in sequence,0
location = CharField(null = True)             # location of camera trap,0
datetime = DateTimeField(null = True),0
# store info about the detection corresponding to this image,0
# store info about the true labels for the detection,0
#  - for pretrain dataset this is the same as the detection_category if the detection categories,0
print(classes),0
Connect to database and sample a dataset,0
Load the saved embedding model from the checkpoint,0
Update the dataset embedding,0
save the images,0
save the features,0
"with open(os.path.join(args.output_dir, 'lastlayer_features.mat'), 'wb') as f:",0
"pickle.dump(sample_features, f)",0
"with open(os.path.join(args.output_dir, 'labels.mat'), 'wb') as f:",0
"pickle.dump(sample_labels, f)",0
"parser.add_argument('--db_name', default='missouricameratraps', type=str, help='Name of the training (target) data Postgres DB.')",0
"parser.add_argument('--db_user', default='user', type=str, help='Name of the user accessing the Postgres DB.')",0
"parser.add_argument('--db_password', default='password', type=str, help='Password of the user accessing the Postgres DB.')",0
"parser.add_argument('--base_model', type=str, help='Path to latest embedding model checkpoint.')",0
"parser.add_argument('--output_dir', type=str, help='Output directory for subset of crops')",0
Add json entry for this crop,0
Copy file for this crop to subset dataset crop dir,0
Copy file for its full-size source image to subset dataset image dir,0
Write crops.json to subset dataset crop dir,0
store info about the crops produced in a JSON file,0
------------------------------------------------------------------------------------------------------------#,0
COMMENT OUT IF NOT USING A SPECIFIC PROJECT WITHIN ROBERT LONG EMAMMAL DATASET,0
------------------------------------------------------------------------------------------------------------#,0
get some information about the source image,0
------------------------------------------------------------------------------------------------------------#,0
NOTE: EDIT THIS SECTION BASED ON DATASET SOURCE,0
get info about sequence the source image belongs to from path and directory,0
# missouricameratraps:,0
imgframenum = int(os.path.basename(imgfile).split('.JPG')[0].split('_')[-1]),0
imgseqid = int(os.path.split(os.path.dirname(imgfile))[-1]),0
"imgseqnumframes = len([name for name in os.listdir(os.path.dirname(imgfile)) if os.path.isfile(os.path.join(os.path.dirname(imgfile), name))])",0
# emammal:,0
------------------------------------------------------------------------------------------------------------#,0
utility function,0
compute output,0
measure accuracy and record loss,0
switch to train mode,0
measure accuracy and record loss,0
compute output,0
measure accuracy and record loss,0
switch to evaluate mode,0
compute output,0
switch to evaluate mode,0
compute output,0
switch to evaluate mode,0
"print(self.labels_set, self.n_classes)",0
Add all negatives for all positive pairs,0
print(triplets.shape[0]),0
constructor,0
update embedding values after a finetuning,0
select either the default or active pools,0
gather test set,0
gather train set,0
finetune the embedding model over the labeled pool,0
a utility function for saving the snapshot,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Variables for the hierarchical cluster,0
Variables for the AL algorithm,0
Data variables,0
"connectivity = kneighbors_graph(self.transformed_X,max_features)",0
Fit cluster and update cluster variables,0
The sklearn hierarchical clustering algo numbers leaves which correspond,0
to actual datapoints 0 to n_points - 1 and all internal nodes have,0
ids greater than n_points - 1 with the root having the highest node id,0
"If no labels have been observed, simply return uniform distribution",0
"If no observations, return worst possible upper lower bounds",0
Loop through generations from bottom to top,0
Update admissible labels for node,0
Calculate score,0
Determine if node should be split,0
Make sure label set for node so that we can flow to children,0
if necessary,0
Only split if all ancestors are admissible nodes,0
This is part  of definition of admissible pruning,0
Check that pruning covers all leave nodes,0
Fill in labels,0
Observe labels for previously recommended batches,0
TODO(lishal): implement multiple selection methods,1
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
A list of initialized samplers is allowed as an input because,0
"for AL_methods that search over different mixtures, may want mixtures to",0
have shared AL_methods so that initialization is only performed once for,0
computation intensive methods like HierarchicalClusteringAL and,0
states are shared between mixtures.,0
"If initialized samplers are not provided, initialize them ourselves.",0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copy these objects to make sure they are not modified while simulating,0
trajectories as they are used later by the main run_experiment script.,0
Assumes that model has already by fit using all labeled data so,0
the probabilities can be used immediately to hallucinate labels,0
All models need to have predict_proba method,0
Hallucinate labels for selected datapoints to be label,0
using class probabilities from model,0
"Not saving already_selected here, if saving then should sort",0
only for the input to fit but preserve ordering of indices in,0
already_selected,0
Useful to know how accuracy compares for model trained on hallucinated,0
labels vs trained on true labels.  But can remove this train to speed,0
up simulations.  Won't speed up significantly since many more models,0
are being trained inside the loop above.,0
Save trajectory for reference,0
Delete created copies,0
THE INPUTS CANNOT BE MODIFIED SO WE MAKE COPIES FOR THE CHECK LATER,0
Should check model but kernel_svm does not have coef_ so need better,0
handling here,0
Make sure that model object fed in did not change during simulations,0
Return indices based on return type specified,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Set gamma for gaussian kernel to be equal to 1/n_features,0
kneighbors graph is constructed using k=10,0
"Make connectivity matrix symmetric, if a point is a k nearest neighbor of",0
"another point, make it vice versa",0
Graph edges are weighted by applying gaussian kernel to manhattan dist.,0
"By default, gamma for rbf kernel is equal to 1/n_features but may",0
get better results if gamma is tuned.,0
Define graph density for an observation to be sum of weights for all,0
edges to the node representing the datapoint.  Normalize sum weights,0
by total number of neighbors.,0
"If a neighbor has already been sampled, reduce the graph density",0
for its direct neighbors to promote diversity.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Update observed reward and arm probabilities,0
Sample an arm,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
y only used for determining how many clusters there should be,0
probably not practical to assume we know # of classes before hand,0
should also probably scale with dimensionality of data,0
Probably okay to always use MiniBatchKMeans,0
Should standardize data before clustering,0
Can cluster on standardized data but train on raw features if desired,0
TODO(lishal): have MarginSampler and this share margin function,1
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Probably okay to always use MiniBatchKMeans,0
Should standardize data before clustering,0
Can cluster on standardized data but train on raw features if desired,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Update min_distances for all examples given new cluster center.,0
Assumes that the transform function takes in original data and not,0
flattened data.,0
Initialize centers with a randomly selected datapoint,0
New examples should not be in already selected since those points,0
should have min_distance of zero to a cluster center.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
This is uniform given the remaining pool but biased wrt the entire pool.,0
sample = [i for i in range(self.X.shape[0]) if i not in already_selected],0
return sample[0:N],0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Fields for hierarchical clustering AL,0
Setting parent and storing nodes in dict for fast access,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Faster and available in Python 3.5 and above,0
transform_list.append(CenterCrop((processed_size))),0
transform_list.append(Lambda(lambda X: normalize(X))),0
"print(self.labels_set, self.n_classes)",0
"from PyQt5 import QtCore, QtWidgets,QtGui",0
from collections import deque,0
from peewee import *,0
from UIComponents.Tag import Tag,0
policy.setHeightForWidth(True),0
"print(self.tab1.parentWidget(),self)",0
self.tab4.add.clicked.connect(self.addSpecies),0
self.tab4.update.clicked.connect(self.updateSpecies),0
checkpoint= load_checkpoint('../merge/triplet_model_0054.tar'),0
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))",0
"embedding_net = EmbeddingNet(checkpoint['arch'], checkpoint['feat_dim'])",0
if checkpoint['loss_type'].lower()=='center':,0
"model = torch.nn.DataParallel(ClassificationNet(embedding_net, n_classes=14)).cuda()",0
else:,0
model= torch.nn.DataParallel(embedding_net).cuda(),0
model.load_state_dict(checkpoint['state_dict']),0
self.parentWidget().progressBar.setMaximum(len(run_dataset)//2048),0
"e=Engine(model,None,None, verbose=True,progressBar= self.parentWidget().progressBar)",0
label = [ x[1] for x in run_dataset.samples],0
"print(indices,selected_set)",0
print(query.sql()),0
src.delete().where(src.image_id<<rList)),0
"det= UserDetection.create(category_id=0, id=str(index+label[1][2]),image_id=final[0], bbox_X=label[1][0], bbox_Y=label[1][1], bbox_W=label[1][2], bbox_H=label[1][3])",0
for x in self.tab1.grid.tags:,0
x.delete_instance(),0
db.create_tables([Detection]),0
This is simply to show the bar,0
"p = Process(target=ex.active, args=())",0
p.start(),0
p.join(),0
ex.active(),0
ex.centralWidget().setCurrentIndex(1),0
main(),0
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))",0
print(row),0
"unq_id= ""crops_""+str(uuid.uuid1())",0
"print(line,imageWidth,imageHeight)",0
"print(""%s,%s,%s,%0.6f,%0.6f,%0.6f,%0.6f,%0.6f""%(line[0], line[1],line[2],float(line[3]),topRel,leftRel,bottomRel,rightRel))",0
if not os.path.exists(dest):,0
os.mkdir(dest),0
raise,0
out.close(),0
"print length,(i-1)*length,i*length",0
matplotlib.use('Agg'),0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
"conf.reset()""""""",0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
conf.reset(),0
for clf in classifiers:,0
"clf.fit(X_train, y_train)",0
preds= clf.predict_proba(X),0
uncertainty+= preds.max(axis=1),0
print(uncertainty[ind]),0
print(uncertainty[ind]),0
true_labels = y[unlabeled_indices],0
select up to 5 digit examples that the classifier is most uncertain about,0
remember best acc@1 and save checkpoint,0
"completeClassificationLoop(run_dataset, model,num_classes)",0
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)",0
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd),0
labels = db.labels_,0
"mapp=(find_probablemap(label,labels, K=args.K))",0
"print(""Clusters"")",0
"for i,x in enumerate(labels):",0
labels[i]= mapp[x],0
print(np.sum(labels == label)/labels.size),0
"print(""Confidence Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning(embd, label, idx)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(""Entropy Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning_entropy(embd, label, idx)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(idx,idx.shape)",0
for i in idx:,0
print(paths[i]),0
"print(""Silohette active learning"")",0
"idx= active_learning2(embd, 1000, args.num_clusters)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
embd= reduce_dimensionality(embd)#[0:10000]),0
labels= labels[0:10000],0
label= label[0:10000],0
paths= paths[0:10000],0
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])",0
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])",0
plt.show(),0
"np.save(args.name_prefix+""_embeddings.npy"",embd)",0
"np.save(args.name_prefix+""_labels.npy"",label)",0
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")",0
del output,0
define loss function (criterion) and optimizer,0
"optimizer = torch.optim.Adam(params, lr=args.lr, weight_decay= args.weight_decay)",0
optimizer.load_state_dict(checkpoint['optimizer']),0
train for one epoch,0
evaluate on validation set,0
matplotlib.use('Agg'),0
selected_set.add(rand_ind[i]),0
"embedding_net = EmbeddingNet('resnet50', 256, True)",0
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)",0
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)",0
print('Embedding Done'),0
sys.stdout.flush(),0
Random examples to start,0
"random_ids = noveltySamples(unlabeled_dataset.em, unlabeled_dataset.getIDs(), 1000)",0
print(random_ids),0
Move Records,0
Finetune the embedding model,0
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)",0
unlabeled_dataset.updateEmbedding(model),0
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})",0
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()",0
matplotlib.use('Agg'),0
"uncertainty= np.apply_along_axis(stats.entropy,1,probs) * (1 - probs.max(axis=1))",0
"copy(paths[srt[i]], ""active"")",0
"plot_together( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()), preds, base_ind, dataset.getpaths(), {})",0
"return np.random.choice(range(0,prob_list[0].shape[0]), 100, replace=False).tolist()",0
selected_set.add(rand_ind[i]),0
"query= Detection.update(kind = destKind.value).where(Detection.id.in_(rList), Detection.kind == srcKind.value)",0
query.execute(),0
"embedding_net = EmbeddingNet('resnet50', 256, True)",0
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)",0
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)",0
print('Embedding Done'),0
sys.stdout.flush(),0
"plot_embedding(dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})",0
Random examples to start,0
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)",0
print(random_ids),0
Move Records,0
Finetune the embedding model,0
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)",0
unlabeled_dataset.updateEmbedding(model),0
"print(dataset.em[dataset.current_set].shape, np.asarray(dataset.getlabels()).shape, len(dataset.getpaths()))",0
"plot_embedding( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})",0
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})",0
train_eval_classifier(),0
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()",0
"names = [""Linear SVM"", ""RBF SVM"", ""Random Forest"", ""Neural Net"", ""Naive Bayes""]",0
"classifiers = [SVC(kernel=""linear"", C=0.025, probability= True, class_weight='balanced'),",0
"SVC(gamma=2, C=1, probability= True, class_weight='balanced'),",0
"RandomForestClassifier(max_depth=None, n_estimators=100, class_weight='balanced'),",0
"MLPClassifier(alpha=1),",0
GaussianNB()],0
estimators= [],0
"for name, clf in zip(names, classifiers):",0
"estimators.append((name, clf))",0
"eclf1 = VotingClassifier(estimators= estimators, voting='hard')",0
"eclf2 = VotingClassifier(estimators= estimators, voting='soft')",0
"names.append(""ensemble hard"")",0
classifiers.append(eclf1),0
"names.append(""ensemble soft"")",0
classifiers.append(eclf2),0
dataset.image_mode(),0
dataset.updateEmbedding(model),0
y_pred= clf.predict(X_test),0
"print(confusion_matrix(y_test, y_pred))",0
paths= dataset.getpaths(),0
"for i, (yp, yt) in enumerate(zip(y_pred, y_test)):",0
if yp != yt:,0
"copy(paths[i],""mistakes"")",0
"print(yt, yp, paths[i],i)",0
"clf_output= clf_e.embedding(eval_loader, dim=48)",0
"self.train_transform = transforms.Compose([Resize(raw_size), CenterCrop((processed_size)), ToTensor(), Normalize(mean, std)])",0
"print(self.labels_set, self.n_classes)",0
"print(line,imageWidth,imageHeight)",0
"print length,(i-1)*length,i*length",0
#########################################################,0
## Configuration,0
#########################################################,0
## The actual code,0
Check arguments,0
Create output directories,0
Padding around the detected objects when cropping,0
1.3 for the cropping during test time and 1.3 for,0
the context that the CNN requires in the left-over,0
image,0
Load a (frozen) Tensorflow model into memory.,0
## Preparations: get all the output tensors,0
The following processing is only for single image,0
Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.,0
Follow the convention by adding back the batch dimension,0
For all images in the image directoryig,0
Load image,0
Run inference,0
"all outputs are float32 numpy arrays, so convert types as appropriate",0
Add detections to the collection,0
Get info about the image,0
Select detections with a confidence larger than DETECTION_CONFIDENCE,0
Get these boxes and convert normalized coordinates to pixel coordinates,0
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes",0
"However, we need to make sure that it box coordinates are still within the image",0
"For each detected bounding box with high confidence, we will",0
crop the image to the padded box and save it,0
generate a unique identifier for the detection,0
"bbox is the detected box, crop_box the padded / enlarged box",0
Add numbering to the original file name if there are multiple boxes,0
The absolute file path where we will store the image,0
"if COCO_OUTPUT_DIR is set, then we will only use the shape",0
of cropped_img in the following code. So instead of reading,0
cropped_img = np.array(Image.open(out_file)),0
we can speed everything up by reading only the size of the image,0
matplotlib.use('Agg'),0
conf= ConfusionMatrix(24),0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
"conf.reset()""""""",0
define loss function (criterion) and optimizer,0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
conf.reset(),0
for clf in classifiers:,0
"clf.fit(X_train, y_train)",0
preds= clf.predict_proba(X),0
uncertainty+= preds.max(axis=1),0
print(uncertainty[ind]),0
print(uncertainty[ind]),0
true_labels = y[unlabeled_indices],0
select up to 5 digit examples that the classifier is most uncertain about,0
"print(indices,selected_set)",0
print(query.sql()),0
remember best acc@1 and save checkpoint,0
"completeClassificationLoop(run_dataset, model,num_classes)",0
"embd, label, paths = extract_embeddings(run_loader, model)",0
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)",0
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd),0
labels = db.labels_,0
"mapp=(find_probablemap(label,labels, K=args.K))",0
"print(""Clusters"")",0
"for i,x in enumerate(labels):",0
labels[i]= mapp[x],0
print(np.sum(labels == label)/labels.size),0
"print(""Confidence Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning(embd, label, idx)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(""Entropy Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning_entropy(embd, label, idx)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"new_selected= selectSamples(embd,paths,3000)",0
"print(idx,idx.shape)",0
for i in idx:,0
print(paths[i]),0
"print(""Silohette active learning"")",0
"idx= active_learning2(embd, 1000, args.num_clusters)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(""Random"")",0
"idx = np.random.choice(np.arange(len(paths)), 1000, replace=False)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
embd= reduce_dimensionality(embd)#[0:10000]),0
labels= labels[0:10000],0
label= label[0:10000],0
paths= paths[0:10000],0
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])",0
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])",0
plt.show(),0
"np.save(args.name_prefix+""_embeddings.npy"",embd)",0
"np.save(args.name_prefix+""_labels.npy"",label)",0
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")",0
combo.setEnabled(not finalized),0
print(self.bbox),0
self.addWidget(self.child),0
Moving container with arrows,0
Left - Bottom,0
Right-Bottom,0
Left-Top,0
Right-Top,0
Left - Bottom,0
Right - Bottom,0
Left - Top,0
Right - Top,0
check cursor horizontal position,0
check cursor vertical position,0
self.resizeEvent=self.onResize,0
"print(""Parent"", parent, parent.width(), parent.height())",0
"self.setGeometry(0,0,410,307)",0
"print(w,h,""w,h"")",0
"print(""final"",tag.getFinal())",0
pass,0
"self.tags.append(TContainer(self,Category.get(-1),[0,0,0.1,0.1],True, Qt.red))",0
label= CharField(),0
fullname=str(self.model),0
"self.name= (fullname[fullname.find("":"")+2:fullname.find("">"")].strip()+'_set').lower()",0
"print(""Parent"", self.parentWidget().width(), self.parentWidget().height())",0
"print(self.model,self.name,query.sql())",0
self.tab4.speciesList.setModel(species),0
"self.tab4.speciesList.setRowHidden(len(species.stringList())-1, True)",0
self.speciesList.itemChanged.connect(self.itemChanged),0
#####,0
,0
video_utils.py,0
,0
"Utilities for splitting, rendering, and assembling videos.",0
,0
#####,0
"%% Constants, imports, environment",0
from ai4eutils,0
%% Path utilities,0
%% Function for rendering frames to video and vice-versa,0
http://tsaith.github.io/combine-images-into-a-video-with-python-3-and-opencv-3.html,0
Determine the width and height from the first image,0
Define the codec and create VideoWriter object,0
"If we're not over-writing, check whether all frame images already exist",0
"OpenCV seems to over-report the number of frames by 1 in some cases, or fails",0
"to read the last frame; either way, I'm allowing one missing frame.",0
"print(""Rendering video {}, couldn't find frame {}"".format(",0
"input_video_file,missing_frame_number))",0
...if we need to check whether to skip this video entirely,0
"for frame_number in tqdm(range(0,n_frames)):",0
print('Skipping frame {}'.format(frame_filename)),0
Recursively enumerate video files,0
Create the target output folder,0
Render frames,0
input_video_file = input_fn_absolute; output_folder = output_folder_video,0
For each video,0
,0
input_fn_relative = input_files_relative_paths[0],0
"process_detection_with_options = partial(process_detection, options=options)",0
zero-indexed,0
Load results,0
# Break into videos,0
im = images[0],0
# For each video...,0
video_name = list(video_to_frames.keys())[0],0
frame = frames[0],0
At most one detection for each category for the whole video,0
category_id = list(detection_categories.keys())[0],0
Find the nth-highest-confidence video to choose a confidence value,0
Prepare the output representation for this video,0
'max_detection_conf' is no longer included in output files by default,0
...for each video,0
Write the output file,0
%% Test driver,0
%% Constants,0
%% Split videos into frames,0
"%% List image files, break into folders",0
Find unique folders,0
fn = frame_files[0],0
%% Load detector output,0
%% Render detector frames,0
folder = list(folders)[0],0
d = detection_results_this_folder[0],0
...for each file in this folder,0
...for each folder,0
%% Render output videos,0
folder = list(folders)[0],0
...for each video,0
,0
run_inference_with_yolov5_val.py,0
,0
Runs a folder of images through MegaDetector (or another YOLOv5 model) with YOLOv5's,0
"val.py, converting the output to the standard MD format.  The main goal is to leverage",0
YOLO's test-time augmentation tools.,0
,0
"YOLOv5's val.py uses each file's base name as a unique identifier, which doesn't work",0
when you have typical camera trap images like:,0
,0
a/b/c/RECONYX0001.JPG,0
d/e/f/RECONYX0001.JPG,0
,0
...so this script jumps through a bunch of hoops to put a symlinks in a flat,0
"folder, run YOLOv5 on that folder, and map the results back to the real files.",0
,0
"Currently requires the user to supply the path where a working YOLOv5 install lives,",0
and assumes that the current conda environment is all set up for YOLOv5.,0
,0
TODO:,1
,0
* Figure out what happens when images are corrupted... right now this is the #1,0
"reason not to use this script, it may be the case that corrupted images look the",0
same as empty images.,0
,0
* Multiple GPU support,0
,0
* Checkpointing,0
,0
* Windows support (I have no idea what all the symlink operations will do on Windows),0
,0
"* Support alternative class names at the command line (currently defaults to MD classes,",0
though other class names can be supplied programmatically),0
,0
%% Imports,0
%% Options class,0
# Required ##,0
# Optional ##,0
%% Main function,0
#%% Path handling,0
#%% Enumerate images,0
#%% Create symlinks to give a unique ID to each image,0
i_image = 0; image_fn = image_files_absolute[i_image],0
...for each image,0
#%% Create the dataset file,0
Category IDs need to be continuous integers starting at 0,0
#%% Prepare YOLOv5 command,0
#%% Run YOLOv5 command,0
#%% Convert results to MD format,0
"We'll use the absolute path as a relative path, and pass '/'",0
as the base path in this case.,0
#%% Clean up,0
...def run_inference_with_yolo_val(),0
%% Command-line driver,0
%% Scrap,0
%% Test driver (folder),0
%% Test driver (file),0
%% Preview results,0
options.sample_seed = 0,0
...for each prediction file,0
%% Compare results,0
Choose all pairwise combinations of the files in [filenames],0
"%% Constants, imports, environment",0
from multiprocessing.pool import ThreadPool as workerpool,0
Number of images to pre-fetch,0
Useful hack to force CPU inference.,1
,0
"Need to do this before any PT/TF imports, which happen when we import",0
from run_detector.,0
Numpy FutureWarnings from tensorflow import,0
%% Support functions for multiprocessing,0
TODO,1
,0
The queue system is a little more elegant if we start one thread for reading and one,1
"for processing, and this works fine on Windows, but because we import TF at module load,",0
"CUDA will only work in the main process, so currently the consumer function runs here.",0
,0
"To enable proper multi-GPU support, we may need to move the TF import to a separate module",0
that isn't loaded until very close to where inference actually happens.,0
%% Other support funtions,0
%% Image processing functions,0
%% Main function,0
Handle the case where image_file_names is not yet actually a list,0
"Find the images to score; images can be a directory, may need to recurse",0
A json list of image paths,0
A single image file,0
Load the detector,0
Does not count those already processed,0
Will not add additional entries not in the starter checkpoint,0
Write a checkpoint if necessary,0
"Back up any previous checkpoints, to protect against crashes while we're writing",0
the checkpoint file.,0
Write the new checkpoint,0
Remove the backup checkpoint if it exists,0
...if it's time to make a checkpoint,0
"When using multiprocessing, let the workers load the model",0
"Results may have been modified in place, but we also return it for",0
backwards-compatibility.,0
The typical case: we need to build the 'info' struct,0
"If the caller supplied the entire ""info"" struct",0
"The 'max_detection_conf' field used to be included by default, and it caused all kinds",0
"of headaches, so it's no longer included unless the user explicitly requests it.",0
%% Interactive driver,0
%%,0
%% Command-line driver,0
This is an experimental hack to allow the use of non-MD YOLOv5 models through,1
the same infrastructure; it disables the code that enforces MDv5-like class lists.,0
Load the checkpoint if available,0
,0
Relative file names are only output at the end; all file paths in the checkpoint are,0
still full paths.,0
"Find the images to score; images can be a directory, may need to recurse",0
A json list of image paths,0
A single image file,0
Test that we can write to the output_file's dir if checkpointing requested,0
"Don't overwrite existing checkpoint files, this is a sure-fire way to eventually",0
erase someone's checkpoint.,0
"Commenting this out for now... the scenario where we are resuming from a checkpoint,",0
then immediately overwrite that checkpoint with empty data is higher-risk than the,0
annoyance of crashing a few minutes after starting a job.,0
Confirm that we can write to the checkpoint path; this avoids issues where,0
we crash after several thousand images.,0
%% Imports,0
import pre- and post-processing functions from the YOLOv5 repo,0
scale_coords() became scale_boxes() in later YOLOv5 versions,0
%% Classes,0
padded resize,0
"Image size can be an int (which translates to a square target size) or (h,w)",0
...if the caller has specified an image size,0
NMS,0
"As of v1.13.0.dev20220824, nms is not implemented for MPS.",0
,0
Send predication back to the CPU to fix.,0
format detections/bounding boxes,0
"This is a loop over detection batches, which will always be length 1 in our case,",0
since we're not doing batch inference.,0
Rescale boxes from img_size to im0 size,0
"normalized center-x, center-y, width and height",0
"MegaDetector output format's categories start at 1, but the MD",0
model's categories start at 0.,0
...for each detection in this batch,0
...if this is a non-empty batch,0
...for each detection batch,0
...try,0
for testing,0
"MegaDetector was trained with batch size of 1, and the resizing function is a part",0
of the inference graph,0
"change from [y1, x1, y2, x2] to [x1, y1, width, height]",0
convert numpy floats to Python floats,0
need to change the above line to the following if supporting a batch size > 1 and resizing to the same size,0
"np_images = [np.asarray(image, np.uint8) for image in images]",0
"images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)",0
performs inference,0
our batch size is 1; need to loop the batch dim if supporting batch size > 1,0
"%% Constants, imports, environment",0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Numpy FutureWarnings from tensorflow import,0
Useful hack to force CPU inference,1
os.environ['CUDA_VISIBLE_DEVICES'] = '-1',0
An enumeration of failure reasons,0
Number of decimal places to round to for confidence and bbox coordinates,0
Label mapping for MegaDetector,0
Should we allow classes that don't look anything like the MegaDetector classes?,0
"Each version of the detector is associated with some ""typical"" values",0
"that are included in output files, so that downstream applications can",0
use them as defaults.,0
%% Classes,0
Stick this into filenames before the extension for the rendered result,0
%% Utility functions,0
mps backend only available in torch >= 1.12.0,0
%% Main function,0
Dictionary mapping output file names to a collision-avoidance count.,0
,0
"Since we'll be writing a bunch of files to the same folder, we rename",0
as necessary to avoid collisions.,0
...def input_file_to_detection_file(),0
Image is modified in place,0
...for each image,0
...def load_and_run_detector(),0
%% Command-line driver,0
Must specify either an image file or a directory,0
"but for a single image, args.image_dir is also None",0
%% Interactive driver,0
%%,0
#####,0
,0
process_video.py,0
,0
"Split a video (or folder of videos) into frames, run the frames through run_detector_batch.py,",0
and optionally stitch together results into a new video with detection boxes.,0
,0
TODO: allow video rendering when processing a whole folder,1
TODO: allow video rendering from existing results,1
,0
#####,0
"%% Constants, imports, environment",0
Only relevant if render_output_video is True,0
Folder to use for extracted frames,0
Folder to use for rendered frames (if rendering output video),0
Should we render a video with detection boxes?,0
,0
"Only supported when processing a single video, not a folder.",0
"If we are rendering boxes to a new video, should we keep the temporary",0
rendered frames?,0
Should we keep the extracted frames?,0
%% Main functions,0
"TODO: keep track of whether we created this folder, delete if we're deleting the extracted",0
"frames and we created the folder, and the output files aren't in the same folder.  For now,",0
we're just deleting the extracted frames and leaving the empty folder around in this case.,0
Render detections to images,0
"TODO: keep track of whether we created this folder, delete if we're deleting the rendered",0
"frames and we created the folder, and the output files aren't in the same folder.  For now,",0
we're just deleting the rendered frames and leaving the empty folder around in this case.,0
Combine into a video,0
Delete the temporary directory we used for detection images,0
shutil.rmtree(rendering_output_dir),0
(Optionally) delete the frames on which we ran MegaDetector,0
shutil.rmtree(frame_output_folder),0
...process_video(),0
# Validate options,0
# Split every video into frames,0
# Run MegaDetector on the extracted frames,0
# (Optionally) delete the frames on which we ran MegaDetector,0
shutil.rmtree(frame_output_folder),0
# Convert frame-level results to video-level results,0
...process_video_folder(),0
%% Interactive driver,0
%% Process a folder of videos,0
import clipboard; clipboard.copy(cmd),0
%% Process a single video,0
import clipboard; clipboard.copy(cmd),0
"%% Render a folder of videos, one file at a time",0
import clipboard; clipboard.copy(s),0
%% Command-line driver,0
Lint as: python3,0
Copyright 2020 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TPU is automatically inferred if tpu_name is None and,0
we are running under cloud ai-platform.,0
,0
copy_checkpoints.py,0
,0
Run this script with specified source_dir and target_dir while the model is training to make a copy,0
of every checkpoint (checkpoints are kept once an hour by default and is difficult to adjust),0
,0
do not copy event or evaluation results,0
input validation,0
plot the images,0
adjust the figure,0
"read in dataset CSV and create merged (dataset, location) col",0
map label to label_index,0
load the splits,0
only weight the training set by detection confidence,0
TODO: consider weighting val and test set as well,1
isotonic regression calibration of MegaDetector confidence,0
treat each split separately,0
new_weight[i] = confidence[i] * (n / c) / total_confidence(i's label),0
- n = # examples in split (weighted by confidence); c = # labels,0
- weight allocated to each label is n/c,0
"- within each label, weigh each example proportional to confidence",0
- new weights sum to n,0
error checking,0
"maps output label name to set of (dataset, dataset_label) tuples",0
find which other label (label_b) has intersection,0
input validation,0
create label index JSON,0
look into sklearn.preprocessing.MultiLabelBinarizer,0
Note: JSON always saves keys as strings!,0
"True for ground truth, False for MegaDetector",0
always save as .jpg for consistency,0
get bounding boxes,0
convert from category ID to category name,0
"check if crops are already downloaded, and ignore bboxes below the",0
confidence threshold,0
assign all images without location info to 'unknown_location',0
remove images from labels that have fewer than min_locs locations,0
merge dataset and location into a single string '<dataset>/<location>',0
"create DataFrame of counts. rows = locations, columns = labels",0
label_count: label => number of examples,0
loc_count: label => number of locs containing that label,0
generate a new split,0
score the split,0
SSE for # of images per label (with 2x weight),0
SSE for # of locs per label,0
label => list of datasets to prioritize for test and validation sets,0
"merge dataset and location into a tuple (dataset, location)",0
sorted smallest to largest,0
greedily add to test set until it has >= 15% of images,0
sort the resulting locs,0
"modify loc_to_size in place, so copy its keys before iterating",0
arguments relevant to both creating the dataset CSV and splits.json,0
arguments only relevant for creating the dataset CSV,0
arguments only relevant for creating the splits JSON,0
comment lines starting with '#' are allowed,0
,0
prepare_classification_script.py,0
,0
Notebook-y script used to prepare a series of shell commands to run a classifier,0
(other than MegaClassifier) on a MegaDetector result set.,0
,0
Differs from prepare_classification_script_mc.py only in the final class mapping step.,0
,0
%% Job options,0
%% Constants,0
"This is just passed along to the metadata in the output file, it has no impact",0
on how the classification scripts run.,0
%% Set up environment,0
commands.append('cd CameraTraps/classification\n'),0
commands.append('conda activate cameratraps-classifier\n'),0
%% Crop images,0
fn = input_files[0],0
%% Run classifier,0
fn = input_files[0],0
%% Merge classification and detection outputs,0
fn = input_files[0],0
%% Write everything out,0
input validation,0
"Evaluating with accimage is much faster than Pillow or Pillow-SIMD, but accimage",0
is Linux-only.,0
create output directory,0
override saved params with kwargs,0
"For now, we don't weight crops by detection confidence during",0
evaluation. But consider changing this.,0
"create model, compile with TorchScript if given checkpoint is not compiled",0
"verify that target names matches original ""label names"" from dataset",0
"if the dataset does not already have a 'other' category, then the",0
'other' category must come last in label_names to avoid conflicting,0
with an existing label_id,0
define loss function (criterion),0
"this file ends up being huge, so we GZIP compress it",0
double check that the accuracy metrics are computed properly,0
save the confusion matrices to .npz,0
save per-label statistics,0
set dropout and BN layers to eval mode,0
"even if batch contains sample weights, don't use them",0
Do target mapping on the outputs (unnormalized logits) instead of,0
"the normalized (softmax) probabilities, because the loss function",0
uses unnormalized logits. Summing probabilities is equivalent to,0
log-sum-exp of unnormalized logits.,0
"a confusion matrix C is such that C[i,j] is the # of observations known to",0
be in group i and predicted to be in group j.,0
match pytorch EfficientNet model names,0
images dataset,0
"for smaller disk / memory usage, we cache the raw JPEG bytes instead",0
of the decoded Tensor,0
convert JPEG bytes to a 3D uint8 Tensor,0
"keras EfficientNet already includes normalization from [0, 255] to [0, 1],",0
so we don't need to do that here,0
labels dataset,0
img_files dataset,0
weights dataset,0
define the transforms,0
efficientnet data preprocessing:,0
- train:,0
"1) random crop: aspect_ratio_range=(0.75, 1.33), area_range=(0.08, 1.0)",0
2) bicubic resize to img_size,0
3) random horizontal flip,0
- test:,0
1) center crop,0
2) bicubic resize to img_size,0
weights sums to:,0
- if weight_by_detection_conf: (# images in split - conf delta),0
- otherwise: (# images in split),0
"freeze the base model's weights, including BatchNorm statistics",0
https://www.tensorflow.org/guide/keras/transfer_learning#fine-tuning,0
rebuild output,0
input validation,0
set seed,0
create logdir and save params,0
create dataloaders and log the index_to_label mapping,0
define loss function and optimizer,0
using EfficientNet training defaults,0
- batch norm momentum: 0.99,0
"- optimizer: RMSProp, decay 0.9 and momentum 0.9",0
- epochs: 350,0
"- learning rate: 0.256, decays by 0.97 every 2.4 epochs",0
- weight decay: 1e-5,0
TODO: change weighted to False if oversampling minority classes,1
stop training after 8 epochs without improvement,0
log metrics,0
log confusion matrix,0
log tp/fp/fn images,0
"tf.summary.image requires input of shape [N, H, W, C]",0
false positive for top3_pred[0],0
false negative for label,0
"if evaluating or finetuning, set dropout & BN layers to eval mode",0
"for each label, track 5 most-confident and least-confident examples",0
"even if batch contains sample weights, don't use them",0
we do not track L2-regularization loss in the loss metric,0
This dictionary will get written out at the end of this process; store,0
diagnostic variables here,0
error checking,0
refresh detection cache,0
save log of bad images,0
cache of Detector outputs: dataset name => {img_path => detection_dict},0
img_path: <dataset-name>/<img-filename>,0
get SAS URL for images container,0
strip image paths of dataset name,0
save list of dataset names and task IDs for resuming,0
complete task name: 'detect_for_classifier_caltech_20200722_110816_task01',0
HACK! Sleep for 10s between task submissions in the hopes that it,1
"decreases the chance of backend JSON ""database"" corruption",0
task still running => continue,0
"task finished successfully, save response to disk",0
error checking before we download and crop any images,0
convert from category ID to category name,0
we need the datasets table for getting SAS keys,0
"we already did all error checking above, so we don't do any here",0
get ContainerClient,0
get bounding boxes,0
we must include the dataset <ds> in <crop_path_template> because,0
'{img_path}' actually gets populated with <img_file> in,0
load_and_crop(),0
"get the image, either from disk or from Blob Storage",0
inelegant way to close the container_clients,0
,0
prepare_classification_script_mc.py,0
,0
Notebook-y script used to prepare a series of shell commands to run MegaClassifier,0
on a MegaDetector result set.,0
,0
Differs from prepare_classification_script.py only in the final class mapping step.,0
,0
%% Job options,0
%% Constants,0
"This is just passed along to the metadata in the output file, it has no impact",0
on how the classification scripts run.,0
%% Set up environment,0
commands.append('cd CameraTraps/classification\n'),0
commands.append('conda activate cameratraps-classifier\n'),0
%% Crop images,0
fn = input_files[0],0
%% Run classifier,0
fn = input_files[0],0
%% Remap classifier outputs,0
fn = input_files[0],0
%% Merge classification and detection outputs,0
fn = input_files[0],0
%% Write everything out,0
mean/std values from https://pytorch.org/docs/stable/torchvision/models.html,0
define the transforms,0
resizes smaller edge to img_size,0
weights sums to:,0
- if weight_by_detection_conf: (# images in split - conf delta),0
- otherwise: # images in split,0
for normal (non-weighted) shuffling,0
set all parameters to not require gradients except final FC layer,0
replace final fully-connected layer (which has 1000 ImageNet classes),0
"detect GPU, use all if available",0
input validation,0
set seed,0
create logdir and save params,0
create dataloaders and log the index_to_label mapping,0
create model,0
define loss function and optimizer,0
using EfficientNet training defaults,0
- batch norm momentum: 0.99,0
"- optimizer: RMSProp, decay 0.9 and momentum 0.9",0
- epochs: 350,0
"- learning rate: 0.256, decays by 0.97 every 2.4 epochs",0
- weight decay: 1e-5,0
stop training after 8 epochs without improvement,0
do a complete evaluation run,0
log metrics,0
log confusion matrix,0
log tp/fp/fn images,0
"for every image: undo normalization, clamp to [0, 1], CHW -> HWC",0
"- cannot be in-place, because the HeapItem might be in multiple heaps",0
writer.add_figure() has issues => using add_image() instead,0
"writer.add_figure(f'{label_name}/{tag}', fig, global_step=epoch)",0
false positive for top3_pred[0],0
false negative for label,0
"preds and labels both have shape [N, k]",0
"if evaluating or finetuning, set dropout and BN layers to eval mode",0
"for each label, track k_extreme most-confident and least-confident images",0
"even if batch contains sample weights, don't use them",0
lock before changing ImageFile.LOAD_TRUNCATED_IMAGES,0
input validation,0
use MegaDB to generate list of images,0
only keep images that:,0
"1) end in a supported file extension, and",0
2) actually exist in Azure Blob Storage,0
3) belong to a label with at least min_locs locations,0
write out log of images / labels that were removed,0
"save label counts, pre-subsampling",0
"save label counts, post-subsampling",0
spec_dict['taxa']: list of dict,0
[,0
"{'level': 'family', 'name': 'cervidae', 'datasets': ['idfg']},",0
"{'level': 'genus',  'name': 'meleagris'}",0
],0
"spec_dict['dataset_labels']: dict, dataset => list of dataset_label",0
{,0
"""idfg"": [""deer"", ""elk"", ""prong""],",0
"""idfg_swwlf_2019"": [""elk"", ""muledeer"", ""whitetaileddeer""]",0
},0
"maps output label name to set of (dataset, dataset_label) tuples",0
"because MegaDB is organized by dataset, we do the same",0
ds_to_labels = {,0
'dataset_name': {,0
"'dataset_label': [output_label1, output_label2]",0
},0
},0
we need the datasets table for getting full image paths,0
The line,0
"[img.class[0], seq.class[0]][0] as class",0
selects the image-level class label if available. Otherwise it selects the,0
"sequence-level class label. This line assumes the following conditions,",0
expressed in the WHERE clause:,0
- at least one of the image or sequence class label is given,0
- the image and sequence class labels are arrays with length at most 1,0
- the image class label takes priority over the sequence class label,0
,0
"In Azure Cosmos DB, if a field is not defined, then it is simply excluded",0
"from the result. For example, on the following JSON object,",0
{,0
"""dataset"": ""camera_traps"",",0
"""seq_id"": ""1234"",",0
"""location"": ""A1"",",0
"""images"": [{""file"": ""abcd.jpeg""}],",0
"""class"": [""deer""],",0
},0
"the array [img.class[0], seq.class[0]] just gives ['deer'] because",0
img.class is undefined and therefore excluded.,0
"if no path prefix, set it to the empty string '', because",0
"os.path.join('', x, '') = '{x}/'",0
result keys,0
"- already has: ['dataset', 'location', 'file', 'class', 'bbox']",0
"- add ['label'], remove ['file']",0
"if img is mislabeled, but we don't know the correct class, skip it",0
"otherwise, update the img with the correct class, but skip the",0
img if the correct class is not one we queried for,0
sort keys for determinism,0
we need the datasets table for getting SAS keys,0
strip leading '?' from SAS token,0
only check Azure Blob Storage,0
check local directory first before checking Azure Blob Storage,0
1st pass: populate label_to_locs,0
"label (tuple of str) => set of (dataset, location)",0
2nd pass: eliminate bad images,0
prioritize is a list of prioritization levels,0
number of already matching images,0
main(,0
"label_spec_json_path='idfg_classes.json',",0
"taxonomy_csv_path='../../camera-traps-private/camera_trap_taxonomy_mapping.csv',",0
"output_dir='run_idfg',",0
json_indent=4),0
recursively find all files in cropped_images_dir,0
only find crops of images from detections JSON,0
resizes smaller edge to img_size,0
"Evaluating with accimage is much faster than Pillow or Pillow-SIMD, but accimage",0
is Linux-only.,0
create dataset,0
create model,0
set dropout and BN layers to eval mode,0
load files,0
dataset => set of img_file,0
crop_path: <dataset>/<img_file>___cropXX_mdvY.Y.jpg,0
[----<img_path>----],0
error checking,0
any row with 'correct_class' should be marked 'mislabeled',0
filter to only the mislabeled rows,0
convert '\' to '/',0
verify that overlapping indices are the same,0
"""add"" any new mislabelings",0
write out results,0
error checking,0
load detections JSON,0
get detector version,0
convert from category ID to category name,0
copy keys to modify dict in-place,0
This will be removed later when we filter for animals,0
save log of bad images,0
"True for ground truth, False for MegaDetector",0
always save as .jpg for consistency,0
"we already did all error checking above, so we don't do any here",0
"get the image, either from disk or from Blob Storage",0
inelegant way to close the container_client,0
"crop_path => normalized bbox coordinates [xmin, ymin, width, height]",0
"only ground-truth bboxes do not have a ""confidence"" value",0
try loading image from local directory,0
try to download image from Blob Storage,0
crop the image,0
"expand box width or height to be square, but limit to img size",0
"Image.crop() takes box=[left, upper, right, lower]",0
pad to square using 0s,0
Author: lukemelas (github username),0
Github repo: https://github.com/lukemelas/EfficientNet-PyTorch,0
With adjustments and added comments by workingcoder (github username).,0
Support the construction of 'efficientnet-l2' without pretrained weights,0
Expansion phase (Inverted Bottleneck),0
"image_size = calculate_output_image_size(image_size, 1) <-- this wouldn't modify image_size",0
Depthwise convolution phase,0
"Squeeze and Excitation layer, if desired",0
Pointwise convolution phase,0
Expansion and Depthwise Convolution,0
Squeeze and Excitation,0
Pointwise Convolution,0
Skip connection and drop connect,0
The combination of skip connection and drop connect brings about stochastic depth.,0
Batch norm parameters,0
Get stem static or dynamic convolution depending on image size,0
Stem,0
Build blocks,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
"image_size = calculate_output_image_size(image_size, block_args.stride)  # stride = 1",0
Head,0
Final linear layer,0
Stem,0
Blocks,0
Head,0
Stem,0
Blocks,0
Head,0
Convolution layers,0
Pooling and final linear layer,0
Author: lukemelas (github username),0
Github repo: https://github.com/lukemelas/EfficientNet-PyTorch,0
With adjustments and added comments by workingcoder (github username).,0
###############################################################################,0
## Help functions for model architecture,0
###############################################################################,0
GlobalParams and BlockArgs: Two namedtuples,0
Swish and MemoryEfficientSwish: Two implementations of the method,0
round_filters and round_repeats:,0
Functions to calculate params for scaling model width and depth ! ! !,0
get_width_and_height_from_size and calculate_output_image_size,0
drop_connect: A structural design,0
get_same_padding_conv2d:,0
Conv2dDynamicSamePadding,0
Conv2dStaticSamePadding,0
get_same_padding_maxPool2d:,0
MaxPool2dDynamicSamePadding,0
MaxPool2dStaticSamePadding,0
"It's an additional function, not used in EfficientNet,",0
but can be used in other model (such as EfficientDet).,0
"Parameters for the entire model (stem, all blocks, and head)",0
Parameters for an individual model block,0
Set GlobalParams and BlockArgs's defaults,0
An ordinary implementation of Swish function,0
A memory-efficient implementation of Swish function,0
TODO: modify the params names.,1
"maybe the names (width_divisor,min_width)",0
"are more suitable than (depth_divisor,min_depth).",0
follow the formula transferred from official TensorFlow implementation,0
follow the formula transferred from official TensorFlow implementation,0
"generate binary_tensor mask according to probability (p for 0, 1-p for 1)",0
Note:,0
The following 'SamePadding' functions make output size equal ceil(input size/stride).,0
"Only when stride equals 1, can the output size be the same as input size.",0
Don't be confused by their function names ! ! !,0
Tips for 'SAME' mode padding.,0
Given the following:,0
i: width or height,0
s: stride,0
k: kernel size,0
d: dilation,0
p: padding,0
Output after Conv2d:,0
o = floor((i+p-((k-1)*d+1))/s+1),0
"If o equals i, i = floor((i+p-((k-1)*d+1))/s+1),",0
=> p = (i-1)*s+((k-1)*d+1)-i,0
With the same calculation as Conv2dDynamicSamePadding,0
Calculate padding based on image size and save it,0
Calculate padding based on image size and save it,0
###############################################################################,0
## Helper functions for loading model params,0
###############################################################################,0
BlockDecoder: A Class for encoding and decoding BlockArgs,0
efficientnet_params: A function to query compound coefficient,0
get_model_params and efficientnet:,0
Functions to get BlockArgs and GlobalParams for efficientnet,0
url_map and url_map_advprop: Dicts of url_map for pretrained weights,0
load_pretrained_weights: A function to load pretrained weights,0
Check stride,0
"Coefficients:   width,depth,res,dropout",0
Blocks args for the whole model(efficientnet-b0 by default),0
It will be modified in the construction of EfficientNet Class according to model,0
note: all models have drop connect rate = 0.2,0
ValueError will be raised here if override_params has fields not included in global_params.,0
train with Standard methods,0
check more details in paper(EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks),0
train with Adversarial Examples(AdvProp),0
check more details in paper(Adversarial Examples Improve Image Recognition),0
TODO: add the petrained weights url map of 'efficientnet-l2',1
AutoAugment or Advprop (different preprocessing),0
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils),0
from data_management.megadb.schema import sequences_schema_check,0
resize is for displaying them more quickly,0
dataset and seq_id are required fields,0
sort the images in the sequence,0
"if no class label on the image, show class label on the sequence",0
pool = ThreadPool(),0
"print('len of rendering_info', len(rendering_info))",0
"tqdm(pool.imap_unordered(render_image_info_partial, rendering_info), total=len(rendering_info))",0
options = write_html_image_list(),0
options['headerHtml'],0
print('Checking that the MegaDB entries conform to the schema...'),0
sequences_schema_check.sequences_schema_check(sequences),0
#######,0
,0
visualize_db.py,0
,0
Outputs an HTML page visualizing annotations (class labels and/or bounding boxes),0
on a sample of images in a database in the COCO Camera Traps format,0
,0
#######,0
%% Imports,0
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils),0
Assumes the cameratraps repo root is on the path,0
%% Settings,0
Set to None to visualize all images,0
Target size for rendering; set either dimension to -1 to preserve aspect ratio,0
"These are mutually exclusive; both are category names, not IDs",0
"Special tag used to say ""show me all images with multiple categories""",0
We sometimes flatten image directories by replacing a path separator with,0
another character.  Leave blank for the typical case where this isn't necessary.,0
Control rendering parallelization,0
Process-based parallelization in this function is currently unsupported,0
"due to pickling issues I didn't care to look at, but I'm going to just",0
"flip this with a warning, since I intend to support it in the future.",0
%% Helper functions,0
"Translate the file name in an image entry in the json database to a path, possibly doing",0
some manipulation of path separators,0
%% Core functions,0
"Optionally remove all images without bounding boxes, *before* sampling",0
"Optionally include/remove images with specific labels, *before* sampling",0
Put the annotations in a dataframe so we can select all annotations for a given image,0
Construct label map,0
Take a sample of images,0
Set of dicts representing inputs to render_db_bounding_boxes:,0
,0
"bboxes, boxClasses, image_path",0
All the class labels we've seen for this image (with out without bboxes),0
Iterate over annotations for this image,0
iAnn = 0; anno = annos_i.iloc[iAnn],0
...for each of this image's annotations,0
"We're adding html for an image before we render it, so it's possible this image will",0
fail to render.  For applications where this script is being used to debua a database,0
"(the common case?), this is useful behavior, for other applications, this is annoying.",0
...for each image,0
...def render_image_info,0
def process_images(...),0
%% Command-line driver,0
"Copy all fields from a Namespace (i.e., the output from parse_args) to an object.",0
,0
Skips fields starting with _.  Does not check existence in the target object.,0
Convert to an options object,0
%% Interactive driver(s),0
%%,0
os.startfile(htmlOutputFile),0
%% Constants and imports,0
convert category ID from int to str,0
Retry on blob storage read failures,0
%% Functions,0
PIL.Image.convert() returns a converted copy of this image,0
Alter orientation as needed according to EXIF tag 0x112 (274) for Orientation,0
,0
https://gist.github.com/dangtrinhnt/a577ece4cbe5364aad28,0
https://www.media.mit.edu/pia/Research/deepview/exif.html,0
,0
Null operation,0
Aspect ratio as width over height,0
ar = w / h,0
h = w / ar,0
w = ar * h,0
The following three functions are modified versions of those at:,0
,0
https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py,0
Convert to pixels so we can use the PIL crop() function,0
PIL's crop() does surprising things if you provide values outside of,0
"the image, clip inputs",0
...if this detection is above threshold,0
...for each detection,0
"list of lists, one list of strings for each bounding box (to accommodate multiple labels)",0
for color selection,0
"Always render objects with a confidence of ""None"", this is typically used",0
for ground truth data.,0
"{} is the default, which means ""show labels with no mapping"", so don't use ""if label_map"" here",0
if label_map:,0
"To avoid duplicate colors with detection-only visualization, offset",0
the classification class index by the number of detection classes,0
...for each classification,0
...if we have classification results,0
...if the confidence of this detection is above threshold,0
...for each detection,0
"print('Input must be of size [N, 4], but is ' + str(boxes_shape))",0
Deliberately trimming to the width of the image only in the case where,0
"box expansion is turned on.  There's not an obvious correct behavior here,",0
but the thinking is that if the caller provided an out-of-range bounding,0
"box, they meant to do that, but at least in the eyes of the person writing",0
"this comment, if you expand a box for visualization reasons, you don't want",0
to end up with part of a box.,0
,0
A slightly more sophisticated might check whether it was in fact the expansion,0
"that made this box larger than the image, but this is the case 99.999% of the time",0
"here, so that doesn't seem necessary.",1
...if we need to expand boxes,0
If the total height of the display strings added to the top of the bounding,0
"box exceeds the top of the image, stack the strings below the bounding box",0
instead of above.,0
Each display_str has a top and bottom margin of 0.05x.,0
Reverse list and print from bottom to top.,0
Skip empty strings,0
"list of lists, one list of strings for each bounding box (to accommodate multiple labels)",0
need to be a string here because PIL needs to iterate through chars,0
,0
stacked bar charts are made with each segment starting from a y position,0
"To fit the legend in, shrink current axis by 20%",0
Put a legend to the right of the current axis,0
####,0
,0
visualize_incoming_annotations.py,0
,0
Spot-check the annotations received from iMerit by visualizing annotated bounding,0
boxes on a sample of images and display them in HTML.,0
,0
"Modified in 2021 March to use the new format (iMerit batch 12 onwards), which is a",0
COCO formatted JSON with relative coordinates for the bbox.,0
,0
from data_management.megadb.schema import sequences_schema_check,0
we used frame_num of 1 when sending out images to annotators when it is not explicitly stored (wcs esp.),0
The file_name field in the incoming json looks like,0
alka_squirrels.seq2020_05_07_25C.frame119221.jpg,0
"we need to use the dataset, sequence and frame info to find the actual path in blob storage",0
using the sequences,0
category_id 5 is No Object Visible,0
download the image,0
Write to HTML,0
allow forward references in typing annotations,0
class variables,0
instance variables,0
get path to root,0
taxonomy CSV rows without 'taxonomy_string' entries are excluded,0
"from the taxonomy graph, but can be included in a classification",0
label specification JSON via the 'dataset_labels' key,0
use the lower parent,0
special cases,0
%% Imports,0
%% Taxnomy checking,0
taxonomy CSV rows without 'taxonomy_string' entries are excluded,0
"from the taxonomy graph, but can be included in a classification",0
label specification JSON via the 'dataset_labels' key,0
...for each row in the taxnomy file,0
%% Command-line driver,0
%% Interactive driver,0
%%,0
which datasets are already processed?,0
"sequence-level query should be fairly fast, ~1 sec",0
cases when the class field is on the image level (images in a sequence,0
"that had different class labels, 'caltech' dataset is like this)",0
"this query may take a long time, >1hr",0
"this query should be fairly fast, ~1 sec",0
read species presence info from the JSON files for each dataset,0
has this class name appeared in a previous dataset?,0
columns to populate the spreadsheet,0
sort by descending species count,0
make the spreadsheet,0
hyperlink Bing search URLs,0
hyperlink example image SAS URLs,0
TODO hardcoded columns: change if # of examples or col_order changes,1
,0
map_lila_taxonomy_to_wi_taxonomy.py,0
,0
Loads the LILA category mapping (in which taxonomy information comes from an iNat taxonomy snapshot),0
and tries to map each class to the Wildlife Insights taxonomy.,0
,0
%% Constants and imports,0
Created by get_lila_category_list.py... contains counts for each category,0
This is a manually-curated file used to store mappings that had to be made manually,0
This is the main output file from this whole process,0
%% Load category and taxonomy files,0
%% Pull everything out of pandas,0
%% Cache WI taxonomy lookups,0
This is just a handy lookup table that we'll use to debug mismatches,0
taxon = wi_taxonomy[21653]; print(taxon),0
Look for keywords that don't refer to specific taxa: blank/animal/unknown,0
Do we have a species name?,0
"If 'species' is populated, 'genus' should always be populated; one item currently breaks",0
this rule.,0
...for each taxon,0
%% Find redundant taxa,0
%% Manual review of redundant taxa,0
%% Clean up redundant taxa,0
taxon_name = list(taxon_name_to_preferred_taxon_id.keys())[0],0
"If we've gotten this far, we should be choosing from multiple taxa.",0
,0
"This will become untrue if any of these are resolved later, at which point we shoudl",0
remove them from taxon_name_to_preferred_id,0
Choose the preferred taxa,0
%% Read supplementary mappings,0
"Each line is [lila query],[WI taxon name],[notes]",0
%% Map LILA categories to WI categories,0
Must be ordered from kingdom --> species,0
TODO:,1
"['subspecies','variety']",0
i_taxon = 0; taxon = lila_taxonomy[i_taxon]; print(taxon),0
"Go from kingdom --> species, choosing the lowest-level description as the query",0
"E.g., 'car'",0
"print('Made a supplementary mapping from {} to {}'.format(query,wi_taxon['taxon_name']))",0
print('No match for {}'.format(query)),0
...for each LILA taxon,0
%% Manual mapping,0
%% Build a dictionary from LILA dataset names and categories to LILA taxa,0
i_d = 0; d = lila_taxonomy[i_d],0
"%% Map LILA datasets to WI taxa, and count the number of each taxon available in each dataset",0
dataset_name = list(lila_dataset_to_categories.keys())[0],0
dataset_category = dataset_categories[0],0
"Write out the dataset name, category name, WI GUID, WI scientific name, WI common name,",0
and count,0
...for each category in this dataset,0
...for each dataset,0
...with open(),0
,0
retrieve_sample_image.py,0
,0
"Downloader that retrieves images from Google images, used for verifying taxonomy",0
"lookups and looking for egregious mismappings (e.g., ""snake"" being mapped to a fish called",0
"""snake"").",0
,0
"Simple wrapper around simple_image_download, but I've had to swap in and out the underlying",0
downloader a few times.,0
,0
%% Imports and environment,0
%%,0
%% Main entry point,0
%% Test driver,0
%%,0
,0
"Using the taxonomy .csv file, map all LILA datasets to the standard taxonomy",0
,0
Does not currently produce results; this is just used to confirm that all category names,0
have mappings in the taxonomy file.,0
,0
%% Constants and imports,0
Created by get_lila_category_list.py... contains counts for each category,0
%% Load category and taxonomy files,0
%% Map dataset names and category names to scientific names,0
i_row = 1; row = taxonomy_df.iloc[i_row]; row,0
"%% For each dataset, make sure we can map every category to the taxonomy",0
dataset_name = list(lila_dataset_to_categories.keys())[0],0
c = categories[0],0
,0
"Takes the megadb taxonomy mapping, extracts the rows that are relevant to",0
"LILA, and does some cleanup.",0
,0
%% Constants and imports,0
This is a partially-completed taxonomy file that was created from a different set of,0
"scripts, but covers *most* of LILA as of June 2022",0
Created by get_lila_category_list.py,0
%% Read the input files,0
Get everything out of pandas,0
"%% Find all unique dataset names in the input list, compare them with data names from LILA",0
d = input_taxonomy_rows[0],0
The script that generates this dictionary creates a separate entry for bounding box,0
"metadata files, but those don't represent new dataset names",0
%% Map input columns to output datasets,0
Make sure all of those datasets actually correspond to datasets on LILA,0
%% Re-write the input taxonomy file to refer to LILA datasets,0
Map the string datasetname:token to a taxonomic tree json,0
mapping = input_taxonomy_rows[0],0
Make sure that all occurrences of this mapping_string give us the same output,0
assert taxonomy_string == taxonomy_mappings[mapping_string],0
%% Re-write the input file in the target format,0
mapping_string = list(taxonomy_mappings.keys())[0],0
,0
prepare_lila_taxonomy_release.py,0
,0
"Given the private intermediate taxonomy mapping, prepare the public (release)",0
taxonomy mapping file.,0
,0
%% Imports and constants,0
Created by get_lila_category_list.py... contains counts for each category,0
%% Find out which categories are actually used,0
dataset_name = datasets_to_map[0],0
i_row = 0; row = df.iloc[i_row]; row,0
%% Generate the final output file,0
i_row = 0; row = df.iloc[i_row]; row,0
match_at_level = taxonomic_match[0],0
i_row = 0; row = df.iloc[i_row]; row,0
"E.g.: (43117, 'genus', 'lepus', ['hares and jackrabbits']",0
###############,0
---> CONSTANTS,0
###############,0
max_progressbar = count * (list(range(limit+1))[-1]+1),0
"bar = progressbar.ProgressBar(maxval=max_progressbar,",0
"widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()]).start()",0
bar.update(bar.currval + 1),0
bar.finish(),0
,0
"Given a subset of LILA datasets, find all the categories, and start the taxonomy",0
mapping process.,0
,0
%% Constants and imports,0
Created by get_lila_category_list.py,0
'NACTI',0
'Channel Islands Camera Traps',0
%% Read the list of datasets,0
The script that generates this dictionary creates a separate entry for bounding box,0
"metadata files, but those don't represent new dataset names",0
%% Find all categories,0
dataset_name = datasets_to_map[0],0
%% Initialize taxonomic lookup,0
from taxonomy_mapping.species_lookup import (,0
"get_taxonomic_info, print_taxonomy_matche)",0
%% Manual lookup,0
%%,0
q = 'white-throated monkey',0
raise ValueError(''),0
%% Match every query against our taxonomies,0
mapping_string = category_mappings[1]; print(mapping_string),0
...for each mapping,0
%% Write output rows,0
,0
"Does some consistency-checking on the LILA taxonomy file, and generates",0
an HTML preview page that we can use to determine whether the mappings,0
make sense.,0
,0
%% Imports and constants,0
"lila_taxonomy_file = r""G:\git\agentmorrisprivate\lila-taxonomy\lila-taxonomy-mapping.csv""",0
"lila_taxonomy_file = r""G:\temp\lila\lila_additions_2022.06.29.csv""",0
%% Support functions,0
%% Read the taxonomy mapping file,0
%% Prepare taxonomy lookup,0
from taxonomy_mapping.species_lookup import (,0
"get_taxonomic_info, print_taxonomy_matche)",0
%% Optionally remap all gbif-based mappings to inat (or vice-versa),0
%%,0
i_row = 1; row = df.iloc[i_row]; row,0
This should be zero for the release .csv,0
%%,0
%% Check for mappings that disagree with the taxonomy string,0
Look for internal inconsistency,0
Look for outdated mappings,0
i_row = 0; row = df.iloc[i_row],0
%% List null mappings,0
,0
"These should all be things like ""unidentified"" and ""fire""",0
,0
i_row = 0; row = df.iloc[i_row],0
%% List mappings with scientific names but no common names,0
%% List mappings that map to different things in different data sets,0
x = suppress_multiple_matches[-1],0
...for each row where we saw this query,0
...for each row,0
"%% Verify that nothing ""unidentified"" maps to a species or subspecies",0
"E.g., ""unidentified skunk"" should never map to a specific species of skunk",0
%% Make sure there are valid source and level values for everything with a mapping,0
%% Find WCS mappings that aren't species or aren't the same as the input,0
"WCS used scientific names, so these remappings are slightly more controversial",0
then the standard remappings.,0
row = df.iloc[-500],0
"Anything marked ""species"" or ""unknown"" by definition doesn't map to a species,",0
so ignore these.,0
print('WCS query {} ({}) remapped to {} ({})'.format(,0
"query,common_name,scientific_name,common_names_from_taxonomy))",0
%% Download sample images for all scientific names,0
i_row = 0; row = df.iloc[i_row],0
if s != 'mirafra':,0
continue,0
Check whether we already have enough images for this query,0
"print('Skipping query {}, already have {} images'.format(s,len(sizes_above_threshold)))",0
Check whether we've already run this query for a previous row,0
...for each row in the mapping table,0
%% Rename .jpeg to .jpg,0
"print('Renaming {} to {}'.format(fn,new_fn))",0
%% Choose representative images for each scientific name,0
s = list(scientific_name_to_paths.keys())[0],0
Be suspicious of duplicate sizes,0
...for each scientific name,0
%% Delete unused images,0
%% Produce HTML preview,0
i_row = 2; row = df.iloc[i_row],0
"image_paths = [os.path.relpath(p, output_base) for p in image_paths]",0
...for each row,0
%% Open HTML preview,0
######,0
,0
species_lookup.py,0
,0
Look up species names (common or scientific) in the GBIF and iNaturalist,0
taxonomies.,0
,0
Run initialize_taxonomy_lookup() before calling any other function.,0
,0
######,0
%% Constants and imports,0
As of 2020.05.12:,0
,0
"GBIF: ~777MB zipped, ~1.6GB taxonomy",0
"iNat: ~2.2GB zipped, ~51MB taxonomy",0
These are un-initialized globals that must be initialized by,0
the initialize_taxonomy_lookup() function below.,0
%% Functions,0
Initialization function,0
# Load serialized taxonomy info if we've already saved it,0
"# If we don't have serialized taxonomy info, create it from scratch.",0
Download and unzip taxonomy files,0
taxonomy_name = list(taxonomy_urls.items())[0][0]; zip_url = list(taxonomy_urls.items())[0][1],0
Don't download the zipfile if we've already unzipped what we need,0
Bypasses download if the file exists already,0
Unzip the files we need,0
...for each file that we need from this zipfile,0
Remove the zipfile,0
os.remove(zipfile_path),0
...for each taxonomy,0
"Create dataframes from each of the taxonomy files, and the GBIF common",0
name file,0
Load iNat taxonomy,0
Load GBIF taxonomy,0
Remove questionable rows from the GBIF taxonomy,0
Load GBIF vernacular name mapping,0
Only keep English mappings,0
Convert everything to lowercase,0
"For each taxonomy table, create a mapping from taxon IDs to rows",0
Create name mapping dictionaries,0
Build iNat dictionaries,0
row = inat_taxonomy.iloc[0],0
Build GBIF dictionaries,0
"The canonical name is the Latin name; the ""scientific name""",0
include the taxonomy name.,0
,0
http://globalnames.org/docs/glossary/,0
This only seems to happen for really esoteric species that aren't,0
"likely to apply to our problems, but doing this for completeness.",0
Don't include taxon IDs that were removed from the master table,0
Save everything to file,0
...def initialize_taxonomy_lookup(),0
"list of dicts: {'source': source_name, 'taxonomy': match_details}",0
i_match = 0,0
"list of (taxon_id, taxonRank, scientific name, [vernacular names])",0
corresponding to an exact match and its parents,0
Walk taxonomy hierarchy,0
This can happen because we remove questionable rows from the,0
GBIF taxonomy,0
"print(f'Warning: no row exists for parent_taxon_id {parent_taxon_id},' + \",0
"f'child taxon_id: {taxon_id}, query: {query}')",0
The GBIF taxonomy contains unranked entries,0
...while there is taxonomy left to walk,0
...for each match,0
Remove redundant matches,0
i_tree_a = 0; tree_a = matching_trees[i_tree_a],0
i_tree_b = 1; tree_b = matching_trees[i_tree_b],0
"If tree a's primary taxon ID is inside tree b, discard tree a",0
,0
taxonomy_level_b = tree_b['taxonomy'][0],0
...for each level in taxonomy B,0
...for each tree (inner),0
...for each tree (outer),0
...def traverse_taxonomy(),0
"print(""Finding taxonomy information for: {0}"".format(query))",0
"In GBIF, some queries hit for both common and scientific, make sure we end",0
up with unique inputs,0
"If the species is not found in either taxonomy, return None",0
Both GBIF and iNat have a 1-to-1 mapping between taxon_id and row number,0
Walk both taxonomies,0
...def get_taxonomic_info(),0
m = matches[0],0
"For example: [(9761484, 'species', 'anas platyrhynchos')]",0
...for each taxonomy level,0
...for each match,0
...def print_taxonomy_matches(),0
%% Taxonomy functions that make subjective judgements,0
query = 'person',0
Do we have an iNat match?,0
"print_taxonomy_matches(inat_matches, verbose=True)",0
"print_taxonomy_matches(gbif_matches, verbose=True)",0
print('Warning: multiple iNat matches for {}'.format(query)),0
Prefer chordates... most of the names that aren't what we want,0
"are esoteric insects, like a moth called ""cheetah""",0
,0
"If we can't find a chordate, just take the first match.",0
,0
i_test_match = 0,0
"This is (taxonID, taxonLevel, scientific, [list of common])",0
print(f'Warning: multiple iNat common names for {query}'),0
Default to returning the query,0
"print(f'Matched iNat {query} to {scientific_name},{common_name}')",0
...if we had iNat matches,0
If we either prefer GBIF or didn't have iNat matches,0
,0
Code is deliberately redundant here; I'm expecting some subtleties in how,0
handle GBIF and iNat.,0
print('Warning: multiple GBIF matches for {}'.format(query)),0
Prefer chordates... most of the names that aren't what we want,0
"are esoteric insects, like a moth called ""cheetah""",0
,0
"If we can't find a chordate, just take the first match.",0
,0
i_test_match = 0,0
"This is (taxonID, taxonLevel, scientific, [list of common])",0
print(f'Warning: multiple GBIF common names for {query}'),0
Default to returning the query,0
...if we needed to look in the GBIF taxonomy,0
...def _get_preferred_taxonomic_match(),0
%% Interactive drivers and debug,0
%% Initialization,0
%% Taxonomic lookup,0
query = 'lion',0
print(matches),0
Print the taxonomy in the taxonomy spreadsheet format,0
%% Directly access the taxonomy tables,0
%% Command-line driver,0
Read command line inputs (absolute path),0
Read the tokens from the input text file,0
Loop through each token and get scientific name,0
,0
process_species_by_dataset,0
,0
We generated a list of all the annotations in our universe; this script is,0
used to (interactively) map them onto the GBIF and iNat taxonomies.  Don't,0
"try to run this script from top to bottom; it's used like a notebook, not like",0
"a script, since manual review steps are required.",0
,0
%% Imports,0
%autoreload 0,0
%autoreload -species_lookup,0
%% Constants,0
Input file,0
Output file after automatic remapping,0
File to which we manually copy that file and do all the manual review; this,0
should never be programmatically written to,0
The final output spreadsheet,0
HTML file generated to facilitate the identificaiton of egregious mismappings,0
%% Functions,0
Prefer iNat matches over GBIF matches,0
query = 'person',0
Do we have an iNat match?,0
"print_taxonomy_matches(inat_matches, verbose=True)",0
"print_taxonomy_matches(gbif_matches, verbose=True)",0
print('Warning: multiple iNat matches for {}'.format(query)),0
Prefer chordates... most of the names that aren't what we want,0
"are esoteric insects, like a moth called ""cheetah""",0
,0
"If we can't find a chordate, just take the first match.",0
,0
i_test_match = 0,0
"This is (taxonID, taxonLevel, scientific, [list of common])",0
print(f'Warning: multiple iNat common names for {query}'),0
Default to returning the query,0
"print(f'Matched iNat {query} to {scientific_name},{common_name}')",0
...if we had iNat matches,0
If we either prefer GBIF or didn't have iNat matches,0
,0
Code is deliberately redundant here; I'm expecting some subtleties in how,0
handle GBIF and iNat.,0
print('Warning: multiple GBIF matches for {}'.format(query)),0
Prefer chordates... most of the names that aren't what we want,0
"are esoteric insects, like a moth called ""cheetah""",0
,0
"If we can't find a chordate, just take the first match.",0
,0
i_test_match = 0,0
"This is (taxonID, taxonLevel, scientific, [list of common])",0
print(f'Warning: multiple GBIF common names for {query}'),0
Default to returning the query,0
...if we needed to look in the GBIF taxonomy,0
...def get_preferred_taxonomic_match(),0
%% Initialization,0
%% Test single-query lookup,0
%%,0
%%,0
"q = ""grevy's zebra""",0
%% Read the input data,0
%% Run all our taxonomic lookups,0
i_row = 0; row = df.iloc[i_row],0
query = 'lion',0
...for each query,0
Write to the excel file that we'll use for manual review,0
%% Download preview images for everything we successfully mapped,0
uncomment this to load saved output_file,0
"output_df = pd.read_excel(output_file, keep_default_na=False)",0
i_row = 0; row = output_df.iloc[i_row],0
...for each query,0
%% Write HTML file with representative images to scan for obvious mis-mappings,0
i_row = 0; row = output_df.iloc[i_row],0
...for each row,0
%% Look for redundancy with the master table,0
Note: `master_table_file` is a CSV file that is the concatenation of the,0
"manually-remapped files (""manual_remapped.xlsx""), which are the output of",0
this script run across from different groups of datasets. The concatenation,0
"should be done manually. If `master_table_file` doesn't exist yet, skip this",0
"code cell. Then, after going through the manual steps below, set the final",0
manually-remapped version to be the `master_table_file`.,0
%% Manual review,0
Copy the spreadsheet to another file; you're about to do a ton of manual,0
review work and you don't want that programmatically overwrriten.,0
,0
See manual_review_xlsx above,0
%% Read back the results of the manual review process,0
%% Look for manual mapping errors,0
Manually inspect df for typos in 'dataset_names' and 'taxonomy_level' columns,0
Identify rows where:,0
,0
- 'taxonomy_level' does not match level of 1st element in 'taxonomy_string',0
- 'scientific_name' does not match name of 1st element in 'taxonomy_string',0
,0
...both of which typically represent manual mapping errors.,0
i_row = 0; row = df.iloc[i_row],0
"I'm not sure why both of these checks are necessary, best guess is that",1
the Excel parser was reading blanks as na on one OS/Excel version and as '',0
on another.,0
The taxonomy_string column is a .json-formatted string; expand it into,0
an object via eval(),0
"%% Find scientific names that were added manually, and match them to taxonomies",0
i_row = 0; row = df.iloc[i_row],0
...for each query,0
%% Write out final version,0
,0
"Generate read-only SAS URLs for all LILA containers, to facilitate partial downloads.",0
,0
The results of this script end up here:,0
,0
http://lila.science/wp-content/uploads/2020/03/lila_sas_urls.txt,0
,0
"Update: that file is manually maintained now, it can't be programmatically generated",0
,0
%% Imports,0
Read-only,0
%% Enumerate containers,0
%% Generate SAS tokens,0
%% Generate SAS URLs,0
%% Write to output file,0
--------some stuff needed to get AJAX to work with bottle?--------#,0
-------------------------------------------------------------------------------- #,0
PREPARE TO QUEUE IMAGES FOR LABELING,0
-------------------------------------------------------------------------------- #,0
# Connect as USER to database DB_NAME through peewee and initialize database proxy,0
# Load embedding model,0
---------------------------------------------------------------------- #,0
CREATE QUEUE OF IMAGES TO LABEL,0
---------------------------------------------------------------------- #,0
Use classifier to generate predictions,0
# Update model predicted class in PostgreSQL database,0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?,0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
-------------------------------------------------------------------------------- #,0
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI,0
-------------------------------------------------------------------------------- #,0
"# static routes (to serve CSS, etc.)",0
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps",0
"return bottle.static_file(filename, root='../../../../../../../../../../../.')",0
# dynamic routes,0
if data['display_grayscale']:,0
indices_to_exclude.update(set(color_indices)),0
elif not data['display_grayscale']:,0
indices_to_exclude.update(set(grayscale_indices)),0
data['display_images'] = {},0
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices],0
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices],0
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices],0
data['display_images']['detection_categories'] = [],0
for i in indices:,0
if str(dataset.samples[i][1]) == 'None':,0
data['display_images']['detection_categories'].append('None'),0
else:,0
existing_category_entries = {cat.id: cat.name for cat in Category.select()},0
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()",0
data['display_images']['detection_categories'].append(cat_name),0
Use image ids in images_to_label to get the corresponding dataset indices,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
print(set(dataset.set_indices[4]).update(set(indices_to_label))),0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Use image id images_to_label to get the corresponding dataset index,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?,0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Train on samples that have been labeled so far,0
dataset.set_kind(DetectionKind.UserDetection.value),0
print(y_train),0
Predict on the samples that have not been labeled,0
print(y_pred),0
Update model predicted class in PostgreSQL database,0
timer = time.time(),0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
print('Updating the database took %0.2f seconds'%(time.time() - timer)),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids)),0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
"once the classifier has been trained the first time, switch to AL sampling",0
,0
make_oneclass_json.py,0
,0
"Takes a coco-camera-traps .json database and collapses species classes to binary,",0
optionally removing labels from empty images (to be detector-friendly) (depending on,0
"""experiment_type"").",0
,0
"Assumes that empty images are labeled as ""empty"".",0
,0
%% Imports and environment,0
%% Core conversion function,0
"We're removing empty images from the annotation list, but not from",0
"the ""images"" list; they'll still get used in detector training.",0
print('Ignoring empty annotation'),0
%% Interactive driver,0
%%,0
Load annotations,0
Convert from multi-class to one-class,0
Write out the one-class data,0
%% Command-line driver,0
,0
plot_bounding_boxes.py,0
,0
Takes a .json database containing bounding boxes and renders those boxes on the,0
source images.,0
,0
"This assumes annotations in coco-camera-traps format, with absolute bbox",0
coordinates.,0
,0
%% Imports and environment,0
How many images should we process?  Set to -1 to process all images.,0
Should we randomize the image order?,0
%%  Read database and build up convenience mappings,0
Category ID to category name,0
Image ID to image info,0
Image ID to image path,0
Image ID to all annotations referring to this image,0
"%% Iterate over images, draw bounding boxes, write to file",0
For each image,0
image = images[0],0
Build up a list of bounding boxes to draw on this image,0
Load the image,0
Create figure and axes,0
Display the image,0
ann = imageAnnotations[0],0
"For each annotation associated with this image, render bounding box and label",0
"In the Rectangle() function, the first argument (""location"") is the bottom-left",0
of the rectangle.,0
,0
Origin is the upper-left of the image.,0
Add the patch to the Axes,0
Add a class label,0
This is magic goop that removes whitespace around image plots (sort of),0
Write the output image,0
...for each image,0
,0
plot_imerit_annotations.py,0
,0
Takes a .json file full of bounding box annotations and renders those boxes on the,0
source images.,0
,0
"This assumes annotations in the format we receive them, specifically:",0
,0
1) Relative bbox coordinates,0
"2) A list of .json objects, not a well-formatted .json file",0
,0
"I.e., don't use this on a COCO-style .json file.  See plot_bounding_boxes.py",0
for the same operation performed on a proper COCO-camera-traps database.,0
,0
%% Imports and environment,0
"os.makedirs(outputBase, exist_ok=True)",0
%%  Read all source images and build up a hash table from image name to full path,0
"This spans training and validation directories, so it's not the same as",0
just joining the image name to a base path,0
"%% Iterate over annotations, draw bounding boxes, write to file",0
annData has keys:,0
,0
"annotations, categories, images",0
,0
Each of these are lists of dictionaries,0
%% Render all annotations on each image in the sequence,0
%% Pull out image metadata,0
Build up a list of bounding boxes to draw on this image,0
Pull out just the image name from the filename,0
,0
File names look like:,0
,0
seq6efffac2-5567-11e8-b3fe-dca9047ef277.frame1.img59a94e52-23d2-11e8-a6a3-ec086b02610b.jpg,0
"m = re.findall(r'img(.*\.jpg)$', imgFileName, re.M|re.I)",0
print(m),0
assert(len(m) == 1),0
queryFileName = m[0],0
Map this image back to the original directory,0
"%% Loop over annotations, find annotations that match this image",0
%%,0
"x,y,w,h",0
,0
"x,y is the bottom-left of the rectangle",0
,0
"x,y origin is the upper-left",0
...for each annotation,0
%% Render with PIL (scrap),0
%% Render with Matplotlib,0
Create figure and axes,0
Display the image,0
Location is the bottom-left of the rect,0
,0
Origin is the upper-left,0
Add the patch to the Axes,0
This is magic goop that removes whitespace around image plots (sort of),0
os.startfile(outputFileName),0
%% Showing figures on-screen during debugging,0
plt.show(),0
Various (mostly unsuccessful) approaches to getting the plot window to show up,0
"in the foreground, which is a backend-specific operation...",0
,0
fig.canvas.manager.window.activateWindow(),0
fig.canvas.manager.window.raise_(),0
fm = plt.get_current_fig_manager(),0
"fm.window.attributes('-topmost', 1)",0
"fm.window.attributes('-topmost', 0)",0
,0
# This is the one that I found to be most robust... at like 80% robust.,0
plt.get_current_fig_manager().window.raise_(),0
%%,0
...for each image,0
...for each file,0
,0
convert_imerit_json_to_coco_json.py,0
,0
"Takes a .json file with bounding boxes but no class labels, and a .json file containing the",0
"class labels for those images, and creates a new json file with class labels and bounding",0
boxes.,0
,0
"The bounding box .json file is in the format returned by our annotators, which is not",0
actually a fully-formed .json file; rather it's a series of .json objects,0
,0
"Leaves filenames intact.  Stores imerit ""category IDs"" (empty/human/group/animal) in a new",0
"field called ""annotation_type"".",0
,0
%% Imports and constants,0
%% Configure files and paths,0
"For Snapshot Serengeti, we stored image IDs in our annotation files as:",0
,0
S1_B06_R1_PICT0008,0
,0
...but the corresponding ID in the master database is actually:,0
,0
S1\B06\R1\S1_B06_R1_PICT0008,0
,0
"If this is ""True"", we'll expand the former to the latter",0
Handling a one-off issue in which .'s were mysteriously replaced with -'s,0
"in our annotations.  This will be set dynamically, but I keep it here as",0
a constant to remind me to remove this code when we clean this issue up.,1
Used in the (rare) case where a bounding box was added to an image that was originally,0
annotated as empty,0
Used in the (rare) case where we added bounding boxes to an image with multiple species,0
"Should we include ambiguous bounding boxes (with the ""ambiguous"" category label)",0
in the output file?  Ambiguous boxes are boxes drawn on images with multiple species.,0
"%%  Read metadata from the master database, bounding boxes from the annotations file",0
"The bounding box .json file is in the format returned by our annotators, which is not",0
actually a fully-formed .json file; rather it's a series of .json objects,0
"Each element of annData is a dictionary corresponding to a single sequence, with keys:",0
,0
"annotations, categories, images",0
sequence = annData[0],0
%% Build convenience mappings,0
Image ID to images,0
Category ID to categories (referring to the database categories),0
"Image ID to categories (i.e., species labels)",0
Utility function we'll use to create annotations for images in empty,0
sequences (empty images in non-empty sequences already have annotations),0
"%% Reformat annotations, grabbing category IDs from the master database (prep)",0
iSequence = 0; sequence = annData[0],0
"%% Reformat annotations, grabbing category IDs from the master database (loop)",0
Make a copy here; we're going to manipulate the sequence annotations,0
when we need to add synthetic annotations for empty images,0
im = sequenceImages[0],0
Are there any annotations in this sequence?,0
Which images in this sequence have annotations?,0
For each image in this sequence...,0
imeritImageID = im['id'],0
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG,0
Confirm that the file exists,0
Hande a one-off issue with our annotations,0
datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG,0
,0
...had become:,0
,0
datasetsnapshotserengeti.seqASG000001a-frame0.imgS1_B06_R1_PICT0008.JPG,0
Does it look like we encountered this issue?,0
Convert:,0
,0
S1_B06_R1_PICT0008,0
,0
...to:,0
,0
S1/B06/B06_R1/S1_B06_R1_PICT0008,0
Make sure we only see each image once,0
Create empty annotations for empty images,0
,0
Here we use the *unmodified* file name,0
Create an empty annotation for this image,0
Annotations still use the annotation filename (not database ID) at this point;,0
these will get converted to database IDs below when we process the,0
whole sequence.,0
Sanity-check image size,0
"print('Warning: img {} was listed in DB as {}x{}, annotated as {}x{}, actual size{}x{}'.format(",0
"old_id,new_im['width'],new_im['height'],im['width'],im['height'],imgObj.width,imgObj.height))",0
...for each image in this sequence,0
For each annotation in this sequence...,0
ann = sequenceAnnotations[0],0
Prepare an annotation using the category ID from the database and,0
the bounding box from the annotations file,0
Maintain iMerit's annotation category,0
Generate an (arbitrary) ID for this annotation; the COCO format has a concept,0
"of annotation ID, but our annotation files don't",0
This was a one-off quirk with our file naming,1
We'll do special handling of images with multiple categories later,0
Store the annotation type (group/human/animal/empty),0
This annotation has no bounding box but the image wasn't originally,0
annotated as empty,0
This annotation has a bounding box but the image was originally,0
annotated as empty,0
unnormalize the bbox,0
... for each annotation in this sequence,0
... for each sequence,0
%% Post-processing,0
Count empty images,0
...for each file,0
%% Sanity-check empty images,0
,0
make_ss_annotation_image_folder.py,0
,0
Take a directory full of images with the very long filenames we give annotators:,0
,0
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension,0
,0
"...along with a COCO-camera-traps database referring to those files, and:",0
,0
1) Creates a new COCO-camera-traps database with the original filenames in them,0
(copying the annotations),0
,0
2) Optionally creates a new directory with those images named according to the,0
"Snapshot Serengeti naming convention, including complete relative paths.",0
,0
See convert_imerit_json_to_coco_json to see how we get from the original annotation,0
.json to a COCO-camera-traps database.,0
,0
%% Constants and imports,0
%% Configure files/paths,0
%% Read the annotations (referring to the old filenames),0
"%% Update filenames, optionally copying files",0
im = data['images'][0],0
For each image...,0
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG,0
"Find the image name, e.g. S1_B06_R1_PICT0008",0
Convert:,0
,0
S1_B06_R1_PICT0008.JPG,0
,0
...to:,0
,0
S1/B06/B06_R1/S1_B06_R1_PICT0008.JPG,0
...for each image,0
%% Write the revised database,0
,0
get_annotation_tool_link.py,0
,0
"Takes a COCO-camera-traps-style .json file with URLs already embedded, and prepares",0
a link to the visipedia annotation tool that reviews a subset of those images.,0
,0
,0
create_new_annotation_json.py,0
,0
"Creates a subset of a larger .json database, in this case specifically to pick some images",0
from Snapshot Serengeti.,0
,0
from utils import get_db_dicts,0
for seq in already_annotated:,0
seq_to_ims.pop(seq),0
remove already annotated images,0
add lion images,0
lion_seqs = cat_to_seqs[cat_to_id['lionMale']] + cat_to_seqs[cat_to_id['lionFemale']],0
#print(len(lion_seqs)),0
lion_seqs = [seq for seq in lion_seqs if seq not in already_annotated],0
#print(len(lion_seqs)),0
"lion_seqs_to_annotate.extend(random.sample(lion_seqs, min(len(lion_seqs),num_lions)))",0
print(len(lion_seqs_to_annotate)),0
num_elephants = 1000,0
elephant_seqs = cat_to_seqs[cat_to_id['elephant']],0
#print(len(lion_seqs)),0
elephant_seqs = [seq for seq in elephant_seqs if seq not in already_annotated],0
#print(len(lion_seqs)),0
"elephant_seqs_to_annotate = random.sample(elephant_seqs, num_elephants)",0
num_empty = 10,0
empty_seqs_to_annotate = [],0
for loc in loc_to_seqs:,0
empty_seqs = cats_per_location[loc][cat_to_id['empty']],0
empty_seqs = [seq for seq in empty_seqs if seq not in already_annotated],0
empty_seqs = [seq for seq in empty_seqs if seq_to_season[seq] in seasons_to_keep],0
"empty_seqs_to_annotate.extend(random.sample(empty_seqs, min(len(empty_seqs),num_empty)))",0
ims_to_annotate.extend(empty_ims_to_annotate),0
,0
filter_database.py,0
,0
"Look through a COCO-ct database and find images matching some crtieria, writing",0
a subset of images and annotations to a new file.,0
,0
%% Constants and imports,0
%% Configuration,0
%%  Read database and build up convenience mappings,0
Category ID to category name,0
Image ID to image info,0
Image ID to image path,0
%% Filter,0
ann = annotations[0],0
Is this a tiny box or a group annotation?,0
"x,y,w,h",0
All empty annotations should be classified as either empty or ambiguous,0
,0
"The ambiguous cases are basically minor misses on the annotators' part,",0
where two different small animals were present somewhere.,0
%% Write output file,0
,0
find_images_for_classes.py,0
,0
"Given a .json database, find images that are associated with one or more",0
classes.,0
,0
%% Constants and imports,0
%% Configuration,0
%%  Read database and build up convenience mappings,0
Category ID to category name,0
Image ID to image info,0
Image ID to image path,0
%% Look for target-class annotations,0
ann = annotations[0],0
,0
create_tfrecords_format.py,0
,0
This script creates a tfrecords file from a dataset in VOTT format.,0
%% Imports and environment,0
set up the filenames and annotations,0
This loop reads the bboxes and corresponding labels and assigns them,0
the correct image. Kind of slow at the moment...,0
If needed: merging all classes,0
bbox_labels = ['Animal' for _ in bbox_labels],0
BBox coords are stored in the format,0
"x_min (of width axis) y_min (of height axis), x_max, y_max",0
Coordinate system starts in top left corner,0
"In this framework, we need ('ymin', 'xmin', 'ymax', 'xmax') format",0
print out some stats,0
To make sure we loaded the bboxes correctly:,0
self.validate_bboxes(),0
For each image in the data set...,0
Make sure all are greater equal 0,0
%% Main tfrecord generation function,0
Propagate optional metadata to tfrecords,0
endfor each annotation for the current image,0
endfor each image,0
"Calculate number of shards to get the desired number of images per record,",0
ensure it is evenly divisible by the number of threads,0
%% Command-line driver,0
%% Driver,0
,0
make_tfrecords_cis_trans.py,0
,0
Given a .json file that contains a three-element list (train/val/test) of image IDs and a .json database that contains,0
"those image IDs, generates tfrecords whose filenames include ""train""/""val""/""test""",0
,0
"dataset = json.load(open('/ai4efs/databases/snapshotserengeti/oneclass/SnapshotSerengeti_Seasons_1_to_4_tfrecord_format_valid_ims.json','r'))",0
print('Creating trans_val tfrecords'),0
dataset = [im_id_to_im[idx] for idx in trans_val],0
,0
create_classification_tfrecords_from_json.py,0
,0
Called from make_tfrecords_cis_trans.py if you're running a classification experiment.,0
,0
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/',0
datafile = 'eccv_18_annotation_files_oneclass/CaltechCameraTrapsECCV18',0
image_file_root = datafolder+'eccv_18_all_images/',0
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/',0
datafolder = '/data/iwildcam/',0
datafile = 'combined_iwildcam_annotations_oneclass/eccv_train_and_imerit_2',0
need consecutive category ids,0
old_cat_id_to_new_cat_id = {categories[idx]['id']:idx+1 for idx in range(len(categories))},0
print(old_cat_id_to_new_cat_id),0
remove multiclass images,0
print(images[0]),0
print(vis_data[0]),0
,0
make_tfrecords_from_json.py,0
,0
"Given a coco-camera-traps .json file, creates tfrecords",0
,0
Thin wrapper for create_tfrecords_from_json.,0
,0
%% Constants and imports,0
%% Constants and imports (interactive),0
%%,0
%% Main tfrecord generation function,0
"check whether the input file has already been converted to the tfrecords format,",0
"if not, convert",0
"Calculate number of shards to get the desired number of images per record,",0
ensure it is evenly divisible by the number of threads,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
eMammal_make_splits.py,0
,0
"Based on a tfrecords_format json file of the database, creates 3 splits according to",0
the specified fractions based on location (images from the same location should be in,0
one split) or based on images.,0
,0
"If a previous split is provided (append_to_previous_split is True), the entries in",0
"each split will be preserved, and new entries will be appended, so that new models",0
can warm start with a model trained on the original splits.,0
configurations and paths,0
approximate fraction for the new entries,0
read in the previous splits of image ID or location ID if available,0
"find new locations and assign them to a split, without reassigning any previous locations",0
"find out which images are new, shuffle and split them",0
do NOT sort the IDs to keep the shuffled order,0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
add empty category,0
"add all images that don't have annotations, with cat empty",0
print(detection_results.keys()),0
group the detections and gts by image id:,0
print(image_id),0
print(len(scores)),0
print(len(labels)),0
recall_thresh = 0.9,0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall]),0
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])",0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall]),0
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])",0
exp_name = 'eccv_train',0
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json',0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(len(detection_results['images'])),0
print(len(seqs)),0
"print(len(seqs[0]),len(seqs[",0
print(detection_results.keys()),0
group the detections by image id:,0
group the ground truth annotations by image id:,0
print(seq),0
"for image_id, dets in per_image_detections.iteritems():",0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
print(num_gts),0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
if sum(detected_class_labels)>0:,0
print('detected animal box'),0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])",0
"print(scores, tp_fp_labels)",0
num_total_gts += num_gts,0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
print('valid box'),0
"print(best_im, best_score)",0
exp_name = 'eccv_train',0
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json',0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
need to loop over confidence values,0
"for each value, check if any detections on the image are > conf",0
"If so, that image gets class ""animal""",0
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0",0
"for image_id, dets in per_image_detections.iteritems():",0
"calc prec, rec for this confidence thresh",0
group the ground truth annotations by image id:,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(detection_results.keys()),0
group the detections and gts by image id:,0
print(image_id),0
print(len(scores)),0
print(len(labels)),0
recall_thresh = 0.9,0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall]),0
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])",0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall]),0
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])",0
print(sorted_ap),0
plt.bar(sorted_ap),0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(detection_results.keys()),0
group the detections by image id:,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
print(gts),0
print(num_gts),0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[:2])",0
"print(scores, tp_fp_labels)",0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
print('detected animal box'),0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])",0
"print(scores, tp_fp_labels)",0
num_total_gts += num_gts,0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
print('valid box'),0
best_im = np.argmax(max_im_scores),0
"print(best_im, best_score)",0
"temp_labels = np.zeros(len(im_detection_labels),  dtype=np.int32)",0
"temp_scores = np.zeros(len(im_detection_scores), dtype=np.float32)",0
"for j in range(min(im_num_gts, len(im_detection_labels))):",0
temp_labels[j] = True #TODO: this currently only works for oneclass?,1
temp_scores[j] = best_score,0
im_detection_labels = temp_labels,0
im_detection_scores = temp_scores,0
num_total_gts+=im_num_gts,0
"print(len(detection_scores), len(detection_scores[0]), len(detection_scores[1]))",0
exp_name = 'small_balanced_cct',0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(seq),0
"for image_id, dets in per_image_detections.iteritems():",0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
seq_num_gts.append(num_gts),0
print(num_gts),0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
num_total_gts += 1,0
print('valid box'),0
"print(best_im, best_score)",0
print('no valid box'),0
if sum(seq_num_gts)>0:,0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
need to loop over confidence values,0
"for each value, check if any detections on the image are > conf",0
"If so, that image gets class ""animal""",0
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0",0
"calc prec, rec for this confidence thresh",0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
group the detections by image id:,0
group the ground truth annotations by image id:,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
print('detected animal box'),0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])",0
"print(scores, tp_fp_labels)",0
num_total_gts += num_gts,0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
print('valid box'),0
"print(best_im, best_score)",0
,0
evaluate_detections.py,0
,0
Adapted from analyze_detection.py which is now archived.,0
,0
%% Imports and constants,0
%% Functions,0
"labels input to compute_object_detection_metrics() needs to start at 0, not 1",0
num_detections = len(dets['boxes']),0
to prevent 'Invalid dimensions for box data.' error,0
this box will not match any detections,0
compute one-class precision/recall/average precision (if every box is just of an object class),0
%% Command-line driver,0
,0
detection_eval_utils.py,0
,0
Utility functions used in evaluate_detections.py,0
,0
group the ground truth annotations by image id,0
#####,0
,0
detect_and_predict_image.py,0
,0
"Functions to load a TensorFlow detection and a classification model, run inference,",0
"render bounding boxes on images, and write out the resulting",0
images (with bounding boxes and classes).,0
,0
"See the ""test driver"" cell for example invocation.",0
,0
,0
#####,0
"%% Constants, imports, environment",0
Minimum detection confidence for showing a bounding box on the output image,0
Stick this into filenames before the extension for the rendered result,0
Number of top-scoring classes to show at each bounding box,0
%% Core detection functions,0
Load images if they're not already numpy arrays,0
iImage = 0; image = images[iImage],0
"Load the image as an nparray of size h,w,nChannels",0
"There was a time when I was loading with PIL and switched to mpimg,",0
"but I can't remember why, and converting to RGB is a very good reason",0
"to load with PIL, since mpimg doesn't give any indication of color",0
"order, which basically breaks all .png files.",0
,0
"So if you find a bug related to using PIL, update this comment",0
"to indicate what it was, but also disable .png support.",0
image = mpimg.imread(image),0
This shouldn't be necessarily when loading with PIL and converting to RGB,0
Actual detection,0
...for each image,0
"Currently ""boxes"" is a list of length nImages, where each element is shaped as",0
,0
"1,nDetections,4",0
,0
"This implicitly banks on TF giving us back a fixed number of boxes, let's assert on this",0
to make sure this doesn't silently break in the future.,0
iBox = 0; box = boxes[iBox],0
"""scores"" is a length-nImages list of elements with size 1,nDetections",0
"""classes"" is a length-nImages list of elements with size 1,nDetections",0
,0
"Still as floats, but really representing ints",0
Squeeze out the empty axis,0
boxes is nImages x nDetections x 4,0
scores and classes are both nImages x nDetections,0
Get input and output tensors of classification model,0
"imsize = cur_image['width'], cur_image['height']",0
Select detections with a confidence larger 0.5,0
Get these boxes and convert normalized coordinates to pixel coordinates,0
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes",0
"However, we need to make sure that it box coordinates are still within the image",0
For convenience:,0
Create an array with contains the index of the corresponding crop_box for each selected box,0
i.e. [False False 0 False 1 2 3 False False],0
For each box,0
If this box should be classified,0
Run inference,0
if box should not be classified,0
...for each box,0
species_scores should have shape len(images) x len(boxes) x num_species,0
...for each image,0
...with tf.Session,0
with classification_graph,0
species_scores should have shape len(images) x len(boxes) x num_species,0
%% Rendering functions,0
Display the image,0
plt.show(),0
"top, left, bottom, right",0
,0
"x,y origin is the upper-left",0
Generate bounding box text,0
Choose color based on class,0
Location is the bottom-left of the rect,0
,0
Origin is the upper-left,0
iRight = x + w,0
iTop = y + h,0
Add the patch to the Axes,0
Add class description,0
First determine best location by finding the corner that is closest to the image center,0
relative corner coordinates,0
relative coordinates of image center,0
Compute pair-wise squared distance and get the index of the one with minimal distance,0
Get the corresponding coordinates ...,0
... and alignment for the text box,0
Plot the text box with background,0
...for each box,0
This is magic goop that removes whitespace around image plots (sort of),0
"plt.savefig(outputFileName, bbox_inches='tight', pad_inches=0.0, dpi=dpi, transparent=True)",0
os.startfile(outputFileName),0
...for each image,0
...def render_bounding_boxes,0
Load and run detector on target images,0
Read the name of all classes,0
remove empty lines,0
%% Interactive driver,0
%%,0
%%,0
%% File helper functions,0
%% Command-line driver,0
Hack to avoid running on already-detected images,1
,0
"Script for evaluating a frozen graph given a COCO-style dataset, which was generated by",0
the ../data_management/databases/classification/make_classification_dataset.py script.,0
This is useful for debugging as the accuracy reported by this script should match the,0
accuracy reported by the Tensorflow training.,0
,0
Check that all files exists for easier debugging,0
Load frozen graph,0
Collect tensors for input and output,0
Read image,0
Run inference,0
Check if correct,0
,0
Script for evaluating precision/recall on a two-class problem given,0
"a frozen graph and a COCO-style dataset, which was generated by",0
the ../data_management/databases/classification/make_classification_dataset.py script.,0
We assume the positive class is at index 0 (with a zero-based indexing),0
,0
Check that all files exists for easier debugging,0
Load frozen graph,0
Collect tensors for input and output,0
Read image,0
"with open(image_path, 'rb') as fi:",0
"image =  sess.run(tf.image.decode_jpeg(fi.read(), channels=3))",0
image = image / 255.,0
Run inference,0
predicted_class = np.argmax(predictions),0
Check if correct,0
if coco.imgToAnns[image_id][0]['category_id'] == predicted_class:,0
correct = correct + 1,0
"In matplotlib < 1.5, plt.fill_between does not have a 'step' argument",0
,0
Script for selecting testing images of a COCO-style dataset generated by the script,0
../data_management/databases/classification/make_classification_dataset.py in a consistent,0
manner and predicting the class for it.,0
,0
Mandatory parameters,0
Optional parameters,0
Validate parameters,0
Derived parameters,0
"We assume that the dataset was generated with the make_classification_dataset.py script,",0
hence the images should be located in the same folder as the json,0
Make seletion deterministic,0
Load frozen graph,0
Get dataset information,0
Get classes,0
...and the class list corresponding to the model outputs by assuming,0
that they are in order of their ids,0
Get images of each class,0
Shuffle the image list,0
Start the image sampling,0
"Set of avaiable class IDs, will be filled below",0
If there are still images left for that class,0
"Get image for the sampled class, we already shuffled the class images before so",0
we can simply pop(),0
Start prediction,0
Collect tensors for input and output,0
Read image,0
Run inference,0
Print output to log file,0
#####,0
,0
api_apply_classifier_single_node.py,0
,0
Takes the JSON file produced by the detection API and,0
classifies all boxes above a confidence threshold.,0
,0
#####,0
"%% Constants, imports, environment",0
Assumes that the root of the CameraTraps repo is on the PYTHONPATH,0
Minimum detection confidence for classifying an object,0
Number of top-scoring classes to show at each bounding box,0
Enlargment factor applied to boxes before passing them to the classifier,0
,0
Provides more context and can lead to better results,0
List of detection categories for which we will run the classification,0
,0
"Currently there are {""1"": ""animal"", ""2"": ""person"", ""4"": ""vehicle""}",0
,0
Should be a list of string-formatted ints.,0
Number of significant float digits in JSON output,0
%% Core detection functions,0
Read the name of all classes,0
remove empty lines,0
Create field with name *classification_categories*,0
Add classes using 0-based indexing,0
def add_classification_categories,0
Make sure we have the right json object,0
Get input and output tensors of classification model,0
For each image,0
Read image,0
"Scale pixel values to [0,1]",0
For each box,0
Skip detections with low confidence,0
Skip if detection category is not in whitelist,0
Skip if already classified,0
"Get current box in relative coordinates and format [x_min, y_min, width_of_box, height_of_box]",0
"Convert to [ymin, xmin, ymax, xmax] and store it as 1x4 numpy array so we can",0
re-use the generic multi-box padding code,0
Convert normalized coordinates to pixel coordinates,0
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes.",0
,0
"However, we need to make sure that it box coordinates are still within the image.",0
Get the first (and only) row as our bbox to classify,0
Get the image data for that box,0
Run inference,0
Add an empty list to the json for our predictions,0
Add the *num_annotated_classes* top scoring classes,0
...for each box,0
...for each image,0
...with tf.Session,0
with classification_graph,0
def classify_boxes,0
Load classification model,0
Load detector json,0
Add classes to detector_json,0
"Run classifier on all images, changes will be writting directly to the json",0
Write output json,0
def load_and_run_classifier,0
%% Command-line driver,0
,0
Script for selecting testing images of a COCO-style dataset generated by the script,0
../data_management/databases/classification/make_classification_dataset.py in a consistent,0
manner and predicting the class for it.,0
,0
Assumes the cameratraps repo root is on the path,0
Make seletion deterministic,0
Mandatory parameters,0
Optional parameters,0
Validate parameters,0
Tranfer parameters to post-processing format,0
Load frozen graph,0
Reading image list,0
Reading class list,0
Image sampling,0
Start prediction,0
Collect tensors for input and output,0
Array for collecting infos for rendering the html,0
Read image,0
"Scale pixel values to [0,1]",0
Run inference,0
Add links to all available classes,0
,0
predict_image.py,0
,0
"Given a pointer to a frozen detection graph, runs inference on a single image,",0
printing the top classes to the console,0
,0
%% Imports,0
%% Command-line processing,0
Check that all files exist for easier debugging,0
%% Inference,0
Load frozen graph,0
Load class list,0
Remove empty lines,0
Collect tensors for input and output,0
Read image,0
Run inference,0
Print output,0
,0
Mostly unmodified script for freezing a model,0
Added for convenience and for possible future optimizations,0
,0
Copyright 2015 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
'input_checkpoint' may be a prefix if we're using Saver V2 format,0
Remove all the explicit device specifications for this node. This helps to,0
make the graph more portable.,0
List of all partition variables. Because the condition is heuristic,0
"based, the list could include false positives.",0
This tensor doesn't exist in the graph (for example it's,0
'global_step' or a similar housekeeping element) so skip it.,0
`var_list` is required to be a map of variable names to Variable,0
tensors. Partition variables are Identity tensors that cannot be,0
handled by Saver.,0
Models that have been frozen previously do not contain Variables.,0
optimized_output_graph_def = optimize_for_inference_lib.optimize_for_inference(,0
"output_graph_def,",0
"input_node_names.replace("" "", """").split("",""),",0
"output_node_names.replace("" "", """").split("",""),",0
tf.float32.as_datatype_enum),0
Write GraphDef to file if output path has been given.,0
,0
"Creates a graph description, which is required to create a frozen graph.",0
Adapted from from ./tf-slim/export_inference_graph.py,0
Added preprocessing to the definition for easier handling,0
,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
tf.app.flags.DEFINE_integer(,0
"'batch_size', None,",0
"'Batch size for the exported model. Defaulted to ""None"" so batch size can '",0
'be specified at model runtime.'),0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
#####################,0
Optimization Flags #,0
#####################,0
######################,0
Learning Rate Flags #,0
######################,0
######################,0
Dataset Flags #,0
######################,0
####################,0
Fine-Tuning Flags #,0
####################,0
"Note: when num_clones is > 1, this will actually have each clone to go",0
over each epoch FLAGS.num_epochs_per_decay times. This is different,0
behavior from sync replicas and is expected to produce different results.,0
Warn the user if a checkpoint exists in the train_dir. Then we'll be,0
ignoring the checkpoint anyway.,0
TODO(sguada) variables.filter_variables(),1
######################,0
Config model_deploy #,0
######################,0
Create global_step,0
#####################,0
Select the dataset #,0
#####################,0
#####################,0
Select the network #,0
#####################,0
####################################,0
Select the preprocessing function #,0
####################################,0
#############################################################,0
Create a dataset provider that loads data from the dataset #,0
#############################################################,0
###################,0
Define the model #,0
###################,0
############################,0
Specify the loss function #,0
############################,0
Gather initial summaries.,0
"Gather update_ops from the first clone. These contain, for example,",0
the updates for the batch_norm variables created by network_fn.,0
Add summaries for end_points.,0
Add summaries for losses.,0
Add summaries for variables.,0
################################,0
Configure the moving averages #,0
################################,0
########################################,0
Configure the optimization procedure. #,0
########################################,0
"If sync_replicas is enabled, the averaging will be done in the chief",0
queue runner.,0
Update ops executed locally by trainer.,0
Variables to train.,0
and returns a train_tensor and summary_op,0
Add total_loss to summary.,0
Create gradient updates.,0
Add the summaries from the first clone. These contain the summaries,0
created by model_fn and either optimize_clones() or _gather_clone_loss().,0
Merge all summaries together.,0
##########################,0
Kicks off the training. #,0
##########################,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
#####################,0
Select the dataset #,0
#####################,0
###################,0
Select the model #,0
###################,0
#############################################################,0
Create a dataset provider that loads data from the dataset #,0
#############################################################,0
####################################,0
Select the preprocessing function #,0
####################################,0
###################,0
Define the model #,0
###################,0
Define the metrics:,0
Print the summaries to screen.,0
TODO(sguada) use num_epochs=1,1
This ensures that we make a single pass over all of the data.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Transform the image to floats.,0
"Randomly crop a [height, width] section of the image.",0
Randomly flip the image horizontally.,0
"Because these operations are not commutative, consider randomizing",0
the order their operation.,0
Subtract off the mean and divide by the variance of the pixels.,0
Transform the image to floats.,0
Resize and crop if needed.,0
Subtract off the mean and divide by the variance of the pixels.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Use tf.slice instead of crop_to_bounding box as it accepts tensors to,0
define the crop size.,0
Compute the rank assertions.,0
Create a random bounding box.,0
,0
Use tf.random_uniform and not numpy.random.rand as doing the former would,0
"generate random numbers at graph eval time, unlike the latter which",0
generates random numbers at graph definition time.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Pass the real x only to one of the func calls.,0
The random_* ops do not necessarily clamp.,0
"Each bounding box has shape [1, num_boxes, box coords] and",0
"the coordinates are ordered [ymin, xmin, ymax, xmax].",0
A large fraction of image datasets contain a human-annotated bounding,0
box delineating the region of the image containing the object of interest.,0
We choose to create a new bounding box for the object which is a randomly,0
distorted version of the human-annotated bounding box that obeys an,0
"allowed range of aspect ratios, sizes and overlap with the human-annotated",0
"bounding box. If no box is supplied, then we assume the bounding box is",0
the entire image.,0
Crop the image to the specified bounding box.,0
"Each bounding box has shape [1, num_boxes, box coords] and",0
"the coordinates are ordered [ymin, xmin, ymax, xmax].",0
Restore the shape since the dynamic slice based upon the bbox_size loses,0
the third dimension.,0
This resizing operation may distort the images because the aspect,0
ratio is not respected. We select a resize method in a round robin,0
fashion based on the thread number.,0
Note that ResizeMethod contains 4 enumerated resizing methods.,0
We select only 1 case for fast_mode bilinear.,0
Randomly flip the image horizontally.,0
Randomly distort the colors. There are 1 or 4 ways to do it.,0
Crop the central region of the image with an area containing 87.5% of,0
the original image.,0
Resize the image to the specified height and width.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TODO(sguada) docstring paragraph by (a) motivating the need for the file and,1
(b) defining clones.,0
TODO(sguada) describe the high-level components of model deployment.,1
"E.g. ""each model deployment is composed of several parts: a DeploymentConfig,",0
"which captures A, B and C, an input_fn which loads data.. etc",0
Set up DeploymentConfig,0
Create the global step on the device storing the variables.,0
Define the inputs,0
Define the optimizer.,0
Define the model including the loss.,0
Run training.,0
Namedtuple used to represent a clone during deployment.,0
"Namedtuple used to represent a DeployedModel, returned by deploy().",0
Default parameters for DeploymentConfig,0
Create clones.,0
The return value.,0
Individual components of the loss that will need summaries.,0
Compute and aggregate losses on the clone device.,0
Add the summaries out of the clone device block.,0
Only use regularization_losses for the first clone,0
Compute the total_loss summing all the clones_losses.,0
Sum the gradients across clones.,0
Gather initial summaries.,0
Create Clones.,0
"Gather update_ops from the first clone. These contain, for example,",0
the updates for the batch_norm variables created by model_fn.,0
Place the global step on the device storing the variables.,0
Compute the gradients for the clones.,0
Add summaries to the gradients.,0
Create gradient updates.,0
Only use regularization_losses for the first clone,0
Add the summaries from the first clone. These contain the summaries,0
created by model_fn and either optimize_clones() or _gather_clone_loss().,0
Add total_loss to summary.,0
Merge all summaries together.,0
Note that each grad_and_vars looks like the following:,0
"((grad_var0_clone0, var0), ... (grad_varN_cloneN, varN))",0
!/usr/bin/python,0
Copyright 2016 Google Inc. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Failed to find ""index"" occurrence of item.",0
pylint: disable=broad-except,0
pylint: enable=broad-except,0
Grab the 'index' annotation.,0
Some images contain bounding box annotations that,0
"extend outside of the supplied image. See, e.g.",0
n03127925/n03127925_147.xml,0
"Additionally, for some bounding boxes, the min > max",0
or the box is entirely outside of the image.,0
Example: <...>/n06470073/n00141669_6790.xml,0
Determine if the annotation is from an ImageNet Challenge label.,0
Note: There is a slight bug in the bounding box annotation data.,0
Many of the dog labels have the human label 'Scottish_deerhound',0
instead of the synset ID 'n02092002' in the bbox.label field. As a,0
"simple hack to overcome this issue, we only exclude bbox labels",1
*which are synset ID's* that do not match original synset label for,0
the XML file.,0
Guard against improperly specified boxes.,0
Note bbox.filename occasionally contains '%s' in the name. This is,0
data set noise that is fixed by just using the basename of the XML file.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The URLs where the MNIST data can be downloaded.,0
The names of the classes.,0
"First, process the training data:",0
"Next, process the testing data:",0
"Finally, write the labels file:",0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The URL where the Flowers data can be downloaded.,0
The number of images in the validation set.,0
Seed for repeatability.,0
The number of shards per dataset split.,0
Initializes function that decodes RGB JPEG data.,0
Read the filename:,0
Divide into train and test:,0
"First, convert the training and validation sets.",0
"Finally, write the labels file:",0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TODO(nsilberman): Add tfrecord file type once the script is updated.,1
"If set to false, will not try to set label_to_names in dataset",0
by reading them from labels.txt or github.,0
n01440764,0
n01443537,0
n02119247    black fox,0
n02119359    silver fox,0
pylint: disable=g-line-too-long,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The URL where the CIFAR data can be downloaded.,0
The number of training files.,0
The height and width of each image.,0
The names of the classes.,0
"First, process the training data:",0
"Next, process the testing data:",0
"Finally, write the labels file:",0
Copyright 2016 Google Inc. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The labels file contains a list of valid labels are held in this file.,0
Assumes that the file contains entries as such:,0
n01440764,0
n01443537,0
n01484850,0
where each line corresponds to a label expressed as a synset. We map,0
each synset contained in the file to an integer (based on the alphabetical,0
ordering). See below for details.,0
This file containing mapping from synset to human-readable label.,0
Assumes each line of the file looks like:,0
,0
n02119247    black fox,0
n02119359    silver fox,0
"n02119477    red fox, Vulpes fulva",0
,0
where each line corresponds to a unique mapping. Note that each line is,0
formatted as <synset>\t<human readable label>.,0
This file is the output of process_bounding_box.py,0
Assumes each line of the file looks like:,0
,0
"n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940",0
,0
where each line corresponds to one bounding box annotation associated,0
with an image. Each line can be parsed as:,0
,0
"<JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>",0
,0
Note that there might exist mulitple bounding box annotations associated,0
with an image file.,0
pylint: disable=expression-not-assigned,0
pylint: enable=expression-not-assigned,0
Create a single Session to run all image coding calls.,0
Initializes function that converts PNG to JPEG data.,0
Initializes function that converts CMYK JPEG data to RGB JPEG data.,0
Initializes function that decodes RGB JPEG data.,0
File list from:,0
https://groups.google.com/forum/embed/?place=forum/torch7#!topic/torch7/fOSTXHIESSU,0
File list from:,0
https://github.com/cytsai/ilsvrc-cmyk-image-list,0
Read the image file.,0
Clean the dirty data.,0
1 image is a PNG.,0
22 JPEG images are in CMYK colorspace.,0
Decode the RGB JPEG.,0
Check that image converted to RGB,0
Each thread produces N shards where N = int(num_shards / num_threads).,0
"For instance, if num_shards = 128, and the num_threads = 2, then the first",0
"thread would produce shards [0, 64).",0
"Generate a sharded version of the file name, e.g. 'train-00002-of-00010'",0
"Break all images into batches with a [ranges[i][0], ranges[i][1]].",0
Launch a thread for each batch.,0
Create a mechanism for monitoring when all threads are finished.,0
Create a generic TensorFlow-based utility for converting all image codings.,0
Wait for all the threads to terminate.,0
Leave label index 0 empty as a background class.,0
Construct the list of JPEG files and labels.,0
Shuffle the ordering of all image files in order to guarantee,0
random ordering of the images with respect to label in the,0
saved TFRecord files. Make the randomization repeatable.,0
Build a map from synset to human-readable label.,0
Run it!,0
Allowing None in the signature so that dataset_factory can use the default.,0
!/usr/bin/python,0
Copyright 2016 Google Inc. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Read in the 50000 synsets associated with the validation data set.,0
Make all sub-directories in the validation data dir.,0
Move all of the image to the appropriate sub-directory.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
"The dataset has classes with no images (empty and human), which have ID 0 and 1,",0
so we need to specify 49 here despite having only 47 classes with images,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Final pooling and prediction,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The current_stride variable keeps track of the effective stride of the,0
activations. This allows us to invoke atrous convolution whenever applying,0
the next residual unit would result in the activations having stride larger,0
than the target output_stride.,0
The atrous convolution rate parameter.,0
Move stride from the block's last unit to the end of the block.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Collect activations at the block's end before performing subsampling.,0
Subsampling of the block's output activations.,0
"The following implies padding='SAME' for pool1, which makes feature",0
alignment easier for dense prediction tasks. This is also used in,0
https://github.com/facebook/fb.resnet.torch. However the accompanying,0
code of 'Deep Residual Learning for Image Recognition' uses,0
padding='VALID' for pool1. You can switch to that choice by setting,0
"slim.arg_scope([slim.max_pool2d], padding='VALID').",0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Collect outputs for conv2d, fully_connected and max_pool2d",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"inputs has shape [batch, 224, 224, 3]",0
"inputs has shape [batch, 513, 513, 3]",0
We do not include batch normalization or activation functions in,0
conv1 because the first ResNet unit will perform these. Cf.,0
Appendix of [2].,0
This is needed because the pre-activation variant does not have batch,1
normalization or activation functions in the residual unit output. See,0
Appendix of [2].,0
Convert end_points_collection into a dictionary of end_points.,0
Global average pooling.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
=============================================================================,0
"These parameters come from the online port, which don't necessarily match",0
those in the paper.,0
TODO(nsilberman): confirm these values with Philip.,1
##########,0
Encoder #,0
##########,0
"No normalizer for the first encoder layers as per 'Image-to-Image',",0
Section 5.1.1,0
First layer doesn't use normalizer_fn,0
Last layer doesn't use activation_fn nor normalizer_fn,0
##########,0
Decoder #,0
##########,0
"Dropout is used at both train and test time as per 'Image-to-Image',",0
Section 2.1 (last paragraph).,0
The Relu comes BEFORE the upsample op:,0
Explicitly set the normalizer_fn to None to override any default value,0
"that may come from an arg_scope, such as pix2pix_arg_scope.",0
No normalization on the input layer.,0
Stride 1 on the last layer.,0
"1-dim logits, stride 1, no activation, no normalization.",0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
Alias,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pylint: disable=unused-import,0
pylint: enable=unused-import,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
If we are fine tuning a checkpoint we need to start at a lower learning,0
rate since we are farther along on training.,0
We can start quantizing immediately if we are finetuning.,0
We need to wait for the model to train a bit before we quantize if we are,0
training from scratch.,0
Call rewriter to produce graph with fake quant ops and folded batch norms,0
"quant_delay delays start of quantization till quant_delay steps, allowing",0
for better model accuracy.,0
Configure the learning rate using an exponential decay.,0
"When restoring from a floating point model, the min/max values for",0
quantized weights and activations are not present.,0
We instruct slim to ignore variables that are missing during restoration,0
by setting ignore_missing_vars=True,0
"If we are restoring from a floating point model, we need to initialize",0
the global step to zero for the exponential decay to result in,0
reasonable learning rates.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Orignaly, arg_scope = slim.arg_scope and layers = slim, now switch to more",0
update-to-date tf.contrib.* API.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Turns off fused batch norm.,0
collection containing the moving mean and moving variance.,0
"For inverted pyramid models, we start with gating switched off.",0
batch_size x 32 x 112 x 112 x 64,0
Separable conv is slow when used at first conv layer.,0
batch_size x 32 x 56 x 56 x 64,0
batch_size x 32 x 56 x 56 x 64,0
batch_size x 32 x 56 x 56 x 192,0
batch_size x 32 x 28 x 28 x 192,0
batch_size x 32 x 28 x 28 x 256,0
batch_size x 16 x 14 x 14 x 512,0
batch_size x 16 x 14 x 14 x 512,0
batch_size x 16 x 14 x 14 x 512,0
batch_size x 16 x 14 x 14 x 528,0
batch_size x 16 x 14 x 14 x 832,0
batch_size x 8 x 7 x 7 x 832,0
batch_size x 8 x 7 x 7 x 1024,0
Final pooling and prediction,0
Temporal average pooling.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
299 x 299 x 3,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 160,0
71 x 71 x 192,0
35 x 35 x 384,0
4 x Inception-A blocks,0
35 x 35 x 384,0
Reduction-A block,0
17 x 17 x 1024,0
7 x Inception-B blocks,0
17 x 17 x 1024,0
Reduction-B block,0
8 x 8 x 1536,0
3 x Inception-C blocks,0
Auxiliary Head logits,0
17 x 17 x 1024,0
Final pooling and prediction,0
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which",1
can be set to False to disable pooling here (as in resnet_*()).,0
8 x 8 x 1536,0
1 x 1 x 1536,0
1536,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN,1
setups need the gradient of gradient FusedBatchNormGrad.,0
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN,1
setups need the gradient of gradient FusedBatchNormGrad.,0
First upscaling is different because it takes the input vector.,0
Last layer has different normalizer and activation.,0
Convert to proper channels.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80,0
71 x 71 x 192,0
35 x 35 x 192,0
35 x 35 x 320,0
TODO(alemi): Register intermediate endpoints,1
"17 x 17 x 1088 if output_stride == 8,",0
33 x 33 x 1088 if output_stride == 16,0
TODO(alemi): register intermediate endpoints,1
TODO(gpapan): Properly support output_stride for the rest of the net.,1
8 x 8 x 2080,0
TODO(alemi): register intermediate endpoints,1
8 x 8 x 1536,0
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which",1
can be set to False to disable pooling here (as in resnet_*()).,0
Set weight_decay for weights in conv2d and fully_connected layers.,0
Set activation_fn and parameters for batch_norm.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
collection containing update_ops.,0
use fused batch norm if possible.,0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"inputs has shape [batch, 224, 224, 3]",0
"inputs has shape [batch, 513, 513, 3]",0
Use clip_by_value to simulate bandpass activation.,0
Convert end_points_collection into a dictionary of end_points.,0
Global average pooling.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"end_points will collect relevant activations for external use, for example",0
summaries or losses.,0
299 x 299 x 3,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80.,0
71 x 71 x 192.,0
35 x 35 x 192.,0
Inception blocks,0
mixed: 35 x 35 x 256.,0
mixed_1: 35 x 35 x 288.,0
mixed_2: 35 x 35 x 288.,0
mixed_3: 17 x 17 x 768.,0
mixed4: 17 x 17 x 768.,0
mixed_5: 17 x 17 x 768.,0
mixed_6: 17 x 17 x 768.,0
mixed_7: 17 x 17 x 768.,0
mixed_8: 8 x 8 x 1280.,0
mixed_9: 8 x 8 x 2048.,0
mixed_10: 8 x 8 x 2048.,0
Auxiliary Head logits,0
Shape of feature map before the final layer.,0
Final pooling and prediction,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 2048,0
2048,0
1000,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Reflection pad by 1 in spatial dimensions (axes 1, 2 = h, w) to make a 3x3",0
'valid' convolution produce an output with the same dimension as the,0
input.,0
This corrects 1 pixel offset for images with even width and height.,0
conv2d is left aligned and conv2d_transpose is right aligned for even,0
sized images (while doing 'SAME' padding).,0
Note: This doesn't reflect actual model in paper.,0
Neither dropout nor batch norm -> dont need is_training,0
##########,0
Encoder #,0
##########,0
7x7 input stage,0
##################,0
Residual Blocks #,0
##################,0
##########,0
Decoder #,0
##########,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"end_points will collect relevant activations for external use, for example",0
summaries or losses.,0
Used to find thinned depths for each layer.,0
Note that sizes in the comments below assume an input spatial size of,0
"224x224, however, the inputs can be of any size greater 32x32.",0
224 x 224 x 3,0
depthwise_multiplier here is different from depth_multiplier.,0
depthwise_multiplier determines the output channels of the initial,0
"depthwise conv (see docs for tf.nn.separable_conv2d), while",0
depth_multiplier controls the # channels of the subsequent 1x1,0
convolution. Must have,0
in_channels * depthwise_multipler <= out_channels,0
so that the separable convolution is not overparameterized.,0
Use a normal convolution instead of a separable convolution.,0
112 x 112 x 64,0
56 x 56 x 64,0
56 x 56 x 64,0
56 x 56 x 192,0
28 x 28 x 192,0
Inception module.,0
28 x 28 x 256,0
28 x 28 x 320,0
14 x 14 x 576,0
14 x 14 x 576,0
14 x 14 x 576,0
14 x 14 x 576,0
14 x 14 x 576,0
7 x 7 x 1024,0
7 x 7 x 1024,0
Final pooling and prediction,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 1024,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
=============================================================================,0
Tensorflow mandates these.,0
Conv and DepthSepConv namedtuple define layers of the MobileNet architecture,0
Conv defines 3x3 convolution layers,0
DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.,0
stride is the stride of the convolution,0
depth is the number of channels or filters in a layer,0
MOBILENETV1_CONV_DEFS specifies the MobileNet body,0
Used to find thinned depths for each layer.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
By passing filters=None,0
separable_conv2d produces only a depthwise convolution layer,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 1024,0
Set weight_decay for weights in Conv and DepthSepConv layers.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Turns off fused batch norm.,0
collection containing the moving mean and moving variance.,0
Final pooling and prediction,0
Temporal average pooling.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Orignaly, add_arg_scope = slim.add_arg_scope and layers = slim, now switch to",0
more update-to-date tf.contrib.* API.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
Only insert rate to params if rate > 1.,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Notes for training NASNet Cifar Model,0
-------------------------------------,0
batch_size: 32,0
learning rate: 0.025,0
cosine (single period) learning rate decay,0
auxiliary head loss weighting: 0.4,0
clip global norm of all gradients by 5,0
600 epochs with a batch size of 32,0
This is used for the drop path probabilities since it needs to increase,0
the drop out probability over the course of training.,0
Notes for training large NASNet model on ImageNet,0
-------------------------------------,0
batch size (per replica): 16,0
learning rate: 0.015 * 100,0
learning rate decay factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 100 replicas,0
auxiliary head loss weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Notes for training the mobile NASNet ImageNet model,0
-------------------------------------,0
batch size (per replica): 32,0
learning rate: 0.04 * 50,0
learning rate scaling factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 50 replicas,0
auxiliary head weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Shape of feature map before the final layer.,0
149 x 149 x 32,0
Run the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Find where to place the reduction cells or stride normal cells,0
Setup for building in the auxiliary head.,0
Run the cells,0
true_cell_num accounts for the stem cells,0
Final softmax layer,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Find where to place the reduction cells or stride normal cells,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Setup for building in the auxiliary head.,0
Run the cells,0
true_cell_num accounts for the stem cells,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Final softmax layer,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Calculate the total number of cells in the network.,0
There is no distinction between reduction and normal cells in PNAS so the,0
total number of cells is equal to the number normal cells plus the number,0
of stem cells (two by default).,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Calculate the total number of cells in the network.,0
There is no distinction between reduction and normal cells in PNAS so the,0
total number of cells is equal to the number normal cells plus the number,0
of stem cells (two by default).,0
Configuration for the PNASNet-5 model.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"The cap for tf.clip_by_value, it's hinted from the activation distribution",0
"that the majority of activation values are in the range [-6, 6].",0
Skip path 1,0
Skip path 2,0
"First pad with 0's on the right and bottom, then shift the filter to",0
include those 0's that were added.,0
"If odd number of filters, add an additional one to the second path.",0
Concat and apply BN,0
Set the prev layer to the current layer if it is none,0
Check to be sure prev layer stuff is setup correctly,0
num_or_size_splits=1,0
Apply conv operations,0
Combine hidden states using 'add'.,0
Add hiddenstate to the list of hiddenstates we can choose from,0
Dont stride if this is not one of the original hiddenstates,0
"Check if a stride is needed, then use a strided 1x1 here",0
Determine if a reduction should be applied to make the number of,0
filters match.,0
Return the concat of all the states,0
Scale keep prob by layer number,0
The added 2 is for the reduction cells,0
Decrease the keep probability over time,0
,0
Script for generating a two-class dataset in COCO format for training an obscured image classifier,0
,0
Requires Python >= 3.6 because of the glob ** expression,0
,0
Collect images and labels,0
"Labels: clean = 0, obscured = 1",0
,0
top_folders_to_bottom.py,0
,0
Given a base folder with files like:,0
,0
A/1/2/a.jpg,0
B/3/4/b.jpg,0
,0
"...moves the top-level folders to the bottom in a new output folder, i.e., creates:",0
,0
1/2/A/a.jpg,0
3/4/B/b.jpg,0
,0
"In practice, this is used to make this:",0
,0
animal/camera01/image01.jpg,0
,0
...look like:,0
,0
camera01/animal/image01.jpg,0
,0
%% Constants and imports,0
%% Support functions,0
%% Main functions,0
Find top-level folder,0
Find file/folder names,0
Move or copy,0
...def process_file(),0
Enumerate input folder,0
Convert absolute paths to relative paths,0
Standardize delimiters,0
Make sure each input file maps to a unique output file,0
relative_filename = relative_files[0],0
Loop,0
...def top_folders_to_bottom(),0
%% Interactive driver,0
%%,0
%%,0
%% Command-line driver,0
"python top_folders_to_bottom.py ""g:\temp\separated_images"" ""g:\temp\separated_images_inverted"" --n_threads 100",0
Convert to an options object,0
%% Constants and imports,0
Assumes ai4eutils is on the python path (https://github.com/Microsoft/ai4eutils),0
Assumes the cameratraps repo root is on the path,0
%% Options,0
"Make sure there is no overlap between the two sets, because this will cause",0
issues in the code,0
## Required inputs,0
## Options,0
Can be a folder or a SAS URL,0
# These apply only when we're doing ground-truth comparisons,0
Classes we'll treat as negative,0
,0
"Include the token ""#NO_LABELS#"" to indicate that an image with no annotations",0
should be considered empty.,0
Classes we'll treat as neither positive nor negative,0
"A list of output sets that we should count, but not render images for.",0
,0
"Typically used to preview sets with lots of empties, where you don't want to",0
"subset but also don't want to render 100,000 empty images.",0
,0
"detections, non_detections",0
"detections_animal, detections_person, detections_vehicle",0
"By default, choose a confidence threshold based on the detector version",0
Used for summary statistics only,0
"Number of images to sample, -1 for ""all images""",0
"Random seed for sampling, or None",0
Optionally separate detections into categories (animal/vehicle/human),0
,0
Currently only supported when ground truth is unavailable,0
Optionally replace one or more strings in filenames with other strings;,0
useful for taking a set of results generated for one folder structure,0
and applying them to a slightly different folder structure.,0
Allow bypassing API output loading when operating on previously-loaded,0
results,0
Should we also split out a separate report about the detections that were,0
just below our main confidence threshold?,0
,0
Currently only supported when ground truth is unavailable,0
Control rendering parallelization,0
...PostProcessingOptions,0
#%% Helper classes and functions,0
Anything greater than this isn't clearly positive or negative,0
image has annotations suggesting both negative and positive,0
"image is not annotated or is annotated with 'unknown', 'unlabeled', ETC.",0
image has not yet been assigned a state,0
"In some analyses, we add an additional class that lets us look at",0
detections just below our main confidence threshold,0
count the # of images with each type of DetectionStatus,0
Check whether this image has:,0
- unknown / unassigned-type labels,0
- negative-type labels,0
"- positive labels (i.e., labels that are neither unknown nor negative)",0
"assert has_unknown_labels is False, '{} has unknown labels'.format(annotations)",0
If there are no image annotations...,0
n_negative += 1,0
im['_detection_status'] = DetectionStatus.DS_NEGATIVE,0
"If the image has more than one type of labels, it's ambiguous",0
"note: bools are automatically converted to 0/1, so we can sum",0
"After the check above, we can be sure it's only one of positive,",0
"negative, or unknown.",0
,0
Important: do not merge the following 'unknown' branch with the first,0
"'unknown' branch above, where we tested 'if len(categories) == 0'",0
,0
If the image has only unknown labels,0
If the image has only negative labels,0
If the images has only positive labels,0
"Annotate the category, if it is unambiguous",0
...for each image,0
...mark_detection_status(),0
"Leaving code in place for reading from blob storage, may support this",0
in the future.,0
resize is to display them in this notebook or in the HTML more quickly,0
os.path.isfile() is slow when mounting remote directories; much faster,0
to just try/except on the image open.,0
return '',0
Render images to a flat folder... we can use os.sep here because we've,0
already normalized paths,0
"Create class labels like ""gt_1"" or ""gt_27""",0
"for i_box,box in enumerate(ground_truth_boxes):",0
gt_classes.append('_' + str(box[-1])),0
"errno.ENAMETOOLONG doesn't get thrown properly on Windows, so",0
we awkwardly check against a hard-coded limit,0
Use slashes regardless of os,0
Optionally add links back to the original images,0
...render_bounding_boxes,0
Count items in each category,0
Optionally sort by filename before writing to html,0
Write the individual HTML files,0
...prepare_html_subpages(),0
Get unique categories above the threshold for this image,0
Render an image (with no ground truth information),0
"This is a list of [class,confidence] pairs, sorted by confidence",0
"If we either don't have a confidence threshold, or we've met our",0
confidence threshold,0
...if this detection has classification info,0
...for each detection,0
...def render_image_no_gt(),0
This should already have been normalized to either '/' or '\',0
...def render_image_with_gt(),0
%% Main function,0
#%% Expand some options for convenience,0
#%% Prepare output dir,0
#%% Load ground truth if available,0
Mark images in the ground truth as positive or negative,0
#%% Load detection (and possibly classification) results,0
"If the caller hasn't supplied results, load them",0
Determine confidence thresholds if necessary,0
Remove failed rows,0
Convert keys and values to lowercase,0
"Add column 'pred_detection_label' to indicate predicted detection status,",0
not separating out the classes,0
#%% Pull out descriptive metadata,0
This is rare; it only happens during debugging when the caller,0
is supplying already-loaded API results.,0
"#%% If we have ground truth, remove images we can't match to ground truth",0
#%% Sample images for visualization,0
#%% Fork here depending on whether or not ground truth is available,0
"If we have ground truth, we'll compute precision/recall and sample tp/fp/tn/fn.",0
,0
Otherwise we'll just visualize detections/non-detections.,0
#%% Detection evaluation: compute precision/recall,0
numpy array of detection probabilities,0
"numpy array of bools (0.0/1.0), and -1 as null value",0
Don't include ambiguous/unknown ground truth in precision/recall analysis,0
"For completeness, include the result at a confidence threshold of 1.0",0
Compute and print summary statistics,0
Thresholds go up throughout precisions/recalls/thresholds; find the last,0
value where recall is at or above target.  That's our precision @ target recall.,0
"np.where returns a tuple of arrays, but in this syntax where we're",0
"comparing an array with a scalar, there will only be one element.",0
Convert back to a list,0
Flatten the confusion matrix,0
"#%% Collect classification results, if they exist",0
Mapping of classnames to idx for the confusion matrix.,0
,0
"The lambda is actually kind of a hack, because we use assume that",1
the following code does not reassign classname_to_idx,0
Confusion matrix as defaultdict of defaultdict,0
,0
"Rows / first index is ground truth, columns / second index is predicted category",0
iDetection = 0; fn = detector_files[iDetection]; print(fn),0
"If this image has classification predictions, and an unambiguous class",0
"annotated, and is a positive image...",0
"The unambiguous category, we make this a set for easier handling afterward",0
"Compute the accuracy as intersection of union,",0
i.e. (# of categories in both prediciton and GT),0
divided by (# of categories in either prediction or GT,0
,0
"In case of only one GT category, the result will be 1.0, if",0
prediction is one category and this category matches GT,0
,0
"It is 1.0/(# of predicted top-1 categories), if the GT is",0
one of the predicted top-1 categories.,0
,0
"It is 0.0, if none of the predicted categories is correct",0
Distribute this accuracy across all predicted categories in the,0
confusion matrix,0
...for each file in the detection results,0
If we have classification results,0
Build confusion matrix as array from classifier_cm,0
Print some statistics,0
Prepare confusion matrix output,0
Get confusion matrix as string,0
Get fixed-size classname for each idx,0
Prepend class name on each line and add to the top,0
Print formatted confusion matrix,0
"Actually don't, this gets really messy in all but the widest consoles",0
Plot confusion matrix,0
To manually add more space at bottom: plt.rcParams['figure.subplot.bottom'] = 0.1,0
,0
"Add 0.5 to figsize for every class. For two classes, this will result in",0
"fig = plt.figure(figsize=[4,4])",0
...if we have classification results,0
#%% Render output,0
Write p/r table to .csv file in output directory,0
Write precision/recall plot to .png file in output directory,0
#%% Sampling,0
Sample true/false positives/negatives with correct/incorrect top-1,0
classification and render to html,0
Accumulate html image structs (in the format expected by write_html_image_lists),0
"for each category, e.g. 'tp', 'fp', ..., 'class_bird', ...",0
Add default entries by accessing them for the first time,0
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]",0
"Each element will be a three-tuple with elements file,max_conf,detections",0
"Assemble the information we need for rendering, so we can parallelize without",0
dealing with Pandas,0
i_row = 0; row = images_to_visualize.iloc[0],0
Filenames should already have been normalized to either '/' or '\',0
Map all the rendering results in the list rendering_results into the,0
dictionary images_html,0
Prepare the individual html image files,0
Write index.html,0
Show links to each GT class,0
,0
We could do this without classification results; currently we don't.,0
Add links to all available classes,0
Close body and html tags,0
...for each image,0
"#%% Otherwise, if we don't have ground truth...",0
#%% Sample detections/non-detections,0
Accumulate html image structs (in the format expected by write_html_image_list),0
for each category,0
Add default entries by accessing them for the first time,0
"Maps detection categories - e.g. ""human"" - to result set names, e.g.",0
"""detections_human""",0
Add a set of results for each category and combination of categories,0
Create output directories,0
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]",0
"list of 3-tuples with elements (file, max_conf, detections)",0
"Assemble the information we need for rendering, so we can parallelize without",0
dealing with Pandas,0
i_row = 0; row = images_to_visualize.iloc[0],0
Filenames should already have been normalized to either '/' or '\',0
"render_image_no_gt(file_info,detection_categories_to_results_name,",0
"detection_categories,classification_categories)",0
Map all the rendering results in the list rendering_results into the,0
dictionary images_html,0
Prepare the individual html image files,0
Write index.html,0
"We can't just sum these, because image_counts includes images in both their",0
detection and classification classes,0
total_images = sum(image_counts.values()),0
Don't print classification classes here; we'll do that later with a slightly,0
different structure,0
Add links to all available classes,0
os.startfile(output_html_file),0
...if we do/don't have ground truth,0
...process_batch_results,0
%% Interactive driver(s),0
%%,0
options.unlabeled_classes = ['human'],0
os.start(ppresults.output_html_file),0
%% Command-line driver,0
,0
load_api_results.py,0
,0
Loads the output of the batch processing API (json) into a pandas dataframe.,0
,0
Also functions to group entries by seq_id.,0
,0
Includes the deprecated functions that worked with the old CSV API output format.,0
,0
%% Constants and imports,0
%% Functions for grouping by sequence_id,0
example,0
input 'file': 'SER/S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG',0
output 'id': 'S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG',0
%% Functions for loading the result as a Pandas DataFrame,0
Validate that this is really a detector output file,0
Fields in the API output json other than 'images',0
Normalize paths to simplify comparisons later,0
"image['file'] = image['file'].replace('\\','/')",0
Replace some path tokens to match local paths to original blob structure,0
"If this is a newer file that doesn't include maximum detection confidence values,",0
"add them, because our unofficial internal dataframe format includes this.",0
Pack the json output into a Pandas DataFrame,0
Sanity-check that this is really a detector output file,0
Normalize paths to simplify comparisons later,0
De-serialize detections,0
Optionally replace some path tokens to match local paths to the original blob structure,0
string_to_replace = list(options.detector_output_filename_replacements.keys())[0],0
"TODO: hit some silly issues with vectorized str() and escaped characters, vectorize",1
this later.,0
,0
"detection_results['image_path'].str.replace(string_to_replace,replacement_string)",0
iRow = 0,0
%% Imports,0
Assumes ai4eutils is on the python path (https://github.com/Microsoft/ai4eutils),0
%% Constants and support classes,0
We will confirm that this matches what we load from each file,0
Process-based parallelization isn't supported yet,0
%% Main function,0
"Warn the user if some ""detections"" might not get rendered",0
#%% Validate inputs,0
#%% Load both result sets,0
assert results_a['detection_categories'] == default_detection_categories,0
assert results_b['detection_categories'] == default_detection_categories,0
#%% Make sure they represent the same set of images,0
#%% Find differences,0
"Each of these maps a filename to a two-element list (the image in set A, the image in set B)",0
,0
"Right now, we only handle a very simple notion of class transition, where the detection",0
of maximum confidence changes class *and* both images have an above-threshold detection.,0
fn = filenames_a[0],0
We shouldn't have gotten this far if error_on_non_matching_lists is set,0
det = im_a['detections'][0],0
...for each filename,0
#%% Sample and plot differences,0
"Render two sets of results (i.e., a comparison) for a single",0
image.,0
...def render_image_pair(),0
fn = image_filenames[0],0
...def render_detection_comparisons(),0
"For each category, generate comparison images and the",0
comparison HTML page.,0
,0
category = 'common_detections',0
Choose detection pairs we're going to render for this category,0
...for each category,0
#%% Write the top-level HTML file content,0
...def compare_batch_results(),0
%% Interactive driver,0
%% KRU,0
%% Command-line driver,0
# TODO,1
,0
"Merge high-confidence detections from one results file into another file,",0
when the target file does not detect anything on an image.,0
,0
Does not currently attempt to merge every detection based on whether individual,0
detections are missing; only merges detections into images that would otherwise,0
be considered blank.,0
,0
"If you want to literally merge two .json files, see combine_api_outputs.py.",0
,0
%% Constants and imports,0
%% Structs,0
Don't bother merging into target images where the max detection is already,0
higher than this threshold,0
"If you want to merge only certain categories, specify one",0
(but not both) of these.,0
%% Main function,0
im = output_data['images'][0],0
"Determine whether we should be processing all categories, or just a subset",0
of categories.,0
i_source_file = 0; source_file = source_files[i_source_file],0
source_im = source_data['images'][0],0
detection_category = list(detection_categories)[0],0
"This is already a detection, no need to proceed looking for detections to",0
transfer,0
Boxes are x/y/w/h,0
source_sizes = [det['bbox'][2]*det['bbox'][3] for det in source_detections_this_category_raw],0
Only look at boxes below the size threshold,0
...for each detection category,0
"print('Adding {} detections to image {}'.format(len(detections_to_transfer),image_filename))",0
Update the max_detection_conf field (if present),0
...for each image,0
...for each source file,0
%% Test driver,0
%%,0
%% Command-line driver (TODO),0
,0
separate_detections_into_folders.py,0
,0
## Overview,0
,0
"Given a .json file with batch processing results, separate the files in that",0
"set of results into folders that contain animals/people/vehicles/nothing,",0
according to per-class thresholds.,0
,0
"Image files are copied, not moved.",0
,0
,0
## Output structure,0
,0
Preserves relative paths within each of those folders; cannot be used with .json,0
files that have absolute paths in them.,0
,0
"For example, if your .json file has these images:",0
,0
a/b/c/1.jpg,0
a/b/d/2.jpg,0
a/b/e/3.jpg,0
a/b/f/4.jpg,0
a/x/y/5.jpg,0
,0
And let's say:,0
,0
"* The results say that the first three images are empty/person/vehicle, respectively",0
"* The fourth image is above threshold for ""animal"" and ""person""",0
* The fifth image contains an animal,0
,0
* You specify an output base folder of c:\out,0
,0
You will get the following files:,0
,0
c:\out\empty\a\b\c\1.jpg,0
c:\out\people\a\b\d\2.jpg,0
c:\out\vehicles\a\b\e\3.jpg,0
c:\out\animal_person\a\b\f\4.jpg,0
c:\out\animals\a\x\y\5.jpg,0
,0
## Rendering bounding boxes,0
,0
"By default, images are just copied to the target output folder.  If you specify --render_boxes,",0
bounding boxes will be rendered on the output images.  Because this is no longer strictly,0
"a copy operation, this may result in the loss of metadata.  More accurately, this *may*",0
result in the loss of some EXIF metadata; this *will* result in the loss of IPTC/XMP metadata.,0
,0
Rendering boxes also makes this script a lot slower.,0
,0
## Classification-based separation,0
,0
"If you have a results file with classification data, you can also specify classes to put",0
"in their own folders, within the ""animals"" folder, like this:",0
,0
"--classification_thresholds ""deer=0.75,cow=0.75""",0
,0
"So, e.g., you might get:",0
,0
c:\out\animals\deer\a\x\y\5.jpg,0
,0
"In this scenario, the folders within ""animals"" will be:",0
,0
"deer, cow, multiple, unclassified",0
,0
"""multiple"" in this case only means ""deer and cow""; if an image is classified as containing a",0
"bird and a bear, that would end up in ""unclassified"", since the folder separation is based only",0
on the categories you provide at the command line.,0
,0
"No classification-based separation is done within the animal_person, animal_vehicle, or",0
animal_person_vehicle folders.,0
,0
%% Constants and imports,0
Occasionally we have near-zero confidence detections associated with COCO classes that,0
didn't quite get squeezed out of the model in training.  As long as they're near zero,0
"confidence, we just ignore them.",0
%% Options class,0
"Dictionary mapping categories (plus combinations of categories, and 'empty') to output folders",0
Populated only when using classification results,0
"Originally specified as a string, converted to a dict mapping name:threshold",0
...__init__(),0
...class SeparateDetectionsIntoFoldersOptions,0
%% Support functions,0
Find the maximum confidence for each category,0
,0
det = detections[0],0
"For zero-confidence detections, we occasionally have leftover goop",0
from COCO classes,0
assert det['conf'] < invalid_category_epsilon,0
...for each detection on this image,0
Count the number of thresholds exceeded,0
...for each category,0
If this is above multiple thresholds,0
"TODO: handle species-based separation in, e.g., the animal_person case",1
"Are we making species classification folders, and is this an animal?",0
Do we need to put this into a specific species folder?,0
Find the animal-class detections that are above threshold,0
Count the number of classification categories that are above threshold for at,0
least one detection,0
d = valid_animal_detections[0],0
classification = d['classifications'][0],0
"Do we have a threshold for this category, and if so, is",0
this classification above threshold?,0
...for each classification,0
...for each detection,0
...if we have to deal with classification subfolders,0
...if we have 0/1/more categories above threshold,0
...if this is/isn't a failure case,0
Skip this image if it's empty and we're not processing empty images,0
"At this point, this image is getting copied; we may or may not also need to",0
draw bounding boxes.,0
Do a simple copy operation if we don't need to render any boxes,0
Open the source image,0
"Render bounding boxes for each category separately, beacuse",0
we allow different thresholds for each category.,0
"When we're not using classification folders, remove classification",0
information to maintain standard detection colors.,0
...for each category,0
Read EXIF metadata,0
"Write output with EXIF metadata if available, and quality='keep' if this is a JPEG",0
"image.  Unfortunately, neither parameter likes ""None"", so we get a slightly",0
icky cascade of if's here.,0
Also see:,0
,0
https://discuss.dizzycoding.com/determining-jpg-quality-in-python-pil/,0
,0
...for more ways to preserve jpeg quality if quality='keep' doesn't do the trick.,0
...if we don't/do need to render boxes,0
...def process_detections(),0
%% Main function,0
Create output folder if necessary,0
Load detection results,0
Map class names to output folders,0
Create all combinations of categories,0
category_name = category_names[0],0
Do we have a custom threshold for this category?,0
Create folder mappings for each category,0
Create the actual folders,0
"Handle species classification thresholds, if specified",0
"E.g. deer=0.75,cow=0.75",0
token = tokens[0],0
...for each token,0
...if classification thresholds are still in string format,0
Validate the classes in the threshold list,0
...if we need to deal with classification categories,0
i_image = 14; im = images[i_image]; im,0
...for each image,0
...def separate_detections_into_folders,0
%% Interactive driver,0
%%,0
%%,0
%%,0
%% Testing various command-line invocations,0
"With boxes, no classification",0
"No boxes, no classification (default)",0
"With boxes, with classification",0
"No boxes, with classification",0
%% Command-line driver,0
"python api\batch_processing\postprocessing\separate_detections_into_folders.py ""d:\temp\rspb_mini.json"" ""d:\temp\demo_images\rspb_2018_2019_mini"" ""d:\temp\separation_test"" --nthreads 2",0
Convert to an options object,0
"print('{} {}'.format(v,name))",0
List of category numbers to use in separation; uses all categories if None,0
"Can be ""size"", ""width"", or ""height""",0
For each image...,0
,0
im = images[0],0
d = im['detections'][0],0
Are there really any detections here?,0
Is this a category we're supposed to process?,0
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing#detector-outputs,0
...for each detection,0
...for each image,0
...def categorize_detections_by_size(),0
,0
add_max_conf.py,0
,0
"The MD output format included a ""max_detection_conf"" field with each image",0
up to and including version 1.2; it was removed as of version 1.3 (it's,0
redundant with the individual detection confidence values).,1
,0
"Just in case someone took a dependency on that field, this script allows you",0
to add it back to an existing .json file.,0
,0
%% Imports and constants,0
%% Main function,0
%% Driver,0
#######,0
,0
remove_repeat_detections.py,0
,0
"Used after running find_repeat_detections, then manually filtering the results,",0
to create a final filtered output file.,0
,0
"If you want to use this script, we recommend that you read the user's guide:",0
,0
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms,0
,0
#######,0
%% Constants and imports,0
%% Main function,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
#######,0
,0
repeat_detections_core.py,0
,0
Core utilities shared by find_repeat_detections and remove_repeat_detections.,0
,0
#######,0
%% Imports and environment,0
"from ai4eutils; this is assumed to be on the path, as per repo convention",0
"""PIL cannot read EXIF metainfo for the images""",0
"""Metadata Warning, tag 256 had too many entries: 42, expected 1""",0
%% Constants,0
%% Classes,0
Relevant for rendering the folder of images for filtering,0
,0
"imageBase can also be a SAS URL, in which case some error-checking is",0
disabled.,0
Don't consider detections with confidence lower than this as suspicious,0
Don't consider detections with confidence higher than this as suspicious,0
What's the IOU threshold for considering two boxes the same?,0
How many occurrences of a single location (as defined by the IOU threshold),0
are required before we declare it suspicious?,0
"Ignore ""suspicious"" detections larger than some size; these are often animals",0
taking up the whole image.  This is expressed as a fraction of the image size.,0
"Ignore ""suspicious"" detections smaller than some size",0
Ignore folders with more than this many images in them,0
A list of classes we don't want to treat as suspicious. Each element is an int.,0
Load detections from a filter file rather than finding them from the detector output,0
".json file containing detections, should be called detectionIndex.json in the filtering_* folder",0
produced in the first pass,0
(optional) List of filenames remaining after deletion of identified,0
repeated detections that are actually animals.  This should be a flat,0
"text file, one relative filename per line.  See enumerate_images().",0
Turn on/off optional outputs,0
Determines whether bounding-box rendering errors (typically network errors) should,0
be treated as failures,0
Box rendering options,0
State variables,0
"Replace filename tokens after reading, useful when the directory structure",0
has changed relative to the structure the detector saw,0
How many folders up from the leaf nodes should we be going to aggregate images?,0
An optional function that takes a string (an image file name) and returns,0
"a string (the corresponding  folder ID), typically used when multiple folders",0
actually correspond to the same camera in a manufacturer-specific way (e.g.,0
a/b/c/RECONYX100 and a/b/c/RECONYX101 may really be the same camera).,0
Include/exclude specific folders... only one of these may be,0
"specified; ""including"" folders includes *only* those folders.",0
"Optionally show *other* detections (i.e., detections other than the",0
one the user is evaluating) in a light gray,0
"If bRenderOtherDetections is True, what color should we use to render the",0
(hopefully pretty subtle) non-target detections?,0
,0
"In theory I'd like these ""other detection"" rectangles to be partially",0
"transparent, but this is not straightforward, and the alpha is ignored",0
"here.  But maybe if I leave it here and wish hard enough, someday it",0
will work.,0
,0
otherDetectionsColors = ['dimgray'],0
Sort detections within a directory so nearby detections are adjacent,0
"in the list, for faster review.",0
,0
"Can be None, 'xsort', or 'clustersort'",0
,0
* None sorts detections chronologically by first occurrence,0
* 'xsort' sorts detections from left to right,0
* 'clustersort' clusters detections and sorts by cluster,0
Only relevant if smartSort == 'clustersort',0
"The data table (Pandas DataFrame), as loaded from the input json file via",0
load_api_results(),0
"The other fields in the input json file, loaded via load_api_results()",0
The data table after modification,0
dict mapping folder names to whole rows from the data table,0
dict mapping filenames to rows in the master table,0
"An array of length nDirs, where each element is a list of DetectionLocation",0
objects for that directory that have been flagged as suspicious,0
"This is a bit of a hack right now, but for future-proofing, I don't want to call this",1
"to retrieve anything other than the highest-confidence detection, and I'm assuming this",0
"is already sorted, so assert() that.",0
It's not clear whether it's better to use instances[0].bbox or self.bbox,1
"here... they should be very similar, unless iouThreshold is very low.",0
self.bbox is a better representation of the overal DetectionLocation.,0
%% Helper functions,0
"We store detetions as x/y/w/h, rtree and pyqtree use l/b/r/t",0
"We store detetions as x/y/w/h, rtree and pyqtree use l/b/r/t",0
%% Sort a list of candidate detections to make them visually easier to review,0
Just sort by the X location of each box,0
"Prepare a list of points to represent each box,",0
that's what we'll use for clustering,0
Upper-left,0
"points.append([det.bbox[0],det.bbox[1]])",0
Center,0
"Labels *could* be any unique labels according to the docs, but in practice",0
they are unique integers from 0:nClusters,0
Make sure the labels are unique incrementing integers,0
Store the label assigned to each cluster,0
"Now sort the clusters by their x coordinate, and re-assign labels",0
so the labels are sortable,0
"Compute the centroid for debugging, but we're only going to use the x",0
"coordinate.  This is the centroid of points used to represent detections,",0
which may be box centers or box corners.,0
old_cluster_label_to_new_cluster_label[old_cluster_label] =\,0
new_cluster_labels[old_cluster_label],0
%% Look for matches (one directory),0
List of DetectionLocations,0
candidateDetections = [],0
Create a tree to store candidate detections,0
For each image in this directory,0
,0
iDirectoryRow = 0; row = rows.iloc[iDirectoryRow],0
,0
"iDirectoryRow is a pandas index, so it may not start from zero;",0
"for debugging, we maintain i_iteration as a loop index.",0
print('Searching row {} of {} (index {}) in dir {}'.\,0
"format(i_iteration,len(rows),iDirectoryRow,dirName))",0
Don't bother checking images with no detections above threshold,0
"Array of dicts, where each element is",0
{,0
"'category': '1',  # str value, category ID",0
"'conf': 0.926,  # confidence of this detections",0
,0
"# (x_min, y_min) is upper-left, all in relative coordinates",0
"'bbox': [x_min, y_min, width_of_box, height_of_box]",0
,0
},0
For each detection in this image,0
"This is no longer strictly true; I sometimes run RDE in stages, so",0
some probabilities have already been made negative,0
,0
assert confidence >= 0.0 and confidence <= 1.0,0
Optionally exclude some classes from consideration as suspicious,0
Is this detection too big to be suspicious?,0
print('Illegal zero-size bounding box on image {}'.format(filename)),0
These are relative coordinates,0
print('Ignoring very small detection with area {}'.format(area)),0
print('Ignoring very large detection with area {}'.format(area)),0
This will return candidates of all classes,0
For each detection in our candidate list,0
Don't match across categories,0
Is this a match?,0
"If so, add this example to the list for this detection",0
We *don't* break here; we allow this instance to possibly,0
match multiple candidates.  There isn't an obvious right or,0
wrong here.,0
...for each detection on our candidate list,0
"If we found no matches, add this to the candidate list",0
candidateDetections.append(candidate),0
pyqtree,0
...for each detection,0
...for each row,0
Get all candidate detections,0
print('Found {} candidate detections for folder {}'.format(,0
"len(candidateDetections),dirName))",0
"For debugging only, it's convenient to have these sorted",0
as if they had never gone into a tree structure.  Typically,0
this is in practce a sort by filename.,0
...def find_matches_in_directory(dirName),0
"%% Update the detection table based on suspicious results, write .csv output",0
"An array of length nDirs, where each element is a list of DetectionLocation",0
objects for that directory that have been flagged as suspicious,0
For each directory,0
For each suspicious detection group in this directory,0
For each instance of this suspicious detection,0
This should match the bbox for the detection event,0
The bbox for this instance should be almost the same as the bbox,0
"for this detection group, where ""almost"" is defined by the IOU",0
threshold.,0
if iou < options.iouThreshold:,0
"print('IOU warning: {},{}'.format(iou,options.iouThreshold))",0
Make sure the bounding box matches,0
"Make the probability negative, if it hasn't been switched by",0
another bounding box,0
...for each instance,0
...for each detection,0
...for each directory,0
Update maximum probabilities,0
For each row...,0
No longer strictly true; sometimes I run RDE on RDE output,0
assert maxPOriginal >= 0,0
We should only be making detections *less* likely in this process,0
"If there was a meaningful change, count it",0
"Negative probabilities should be the only reason maxP changed, so",0
we should have found at least one negative value if we reached,0
this point.,0
...if there was a meaningful change to the max probability for this row,0
...for each row,0
If we're also writing output...,0
"...def update_detection_table(RepeatDetectionResults,options)",0
%% Main function,0
#%% Input handling,0
Validate some options,0
Load the filtering file,0
Load the same options we used when finding repeat detections,0
...except for things that explicitly tell this function not to,0
find repeat detections.,0
...if we're loading from an existing filtering file,0
Check early to avoid problems with the output folder,0
"Load file to a pandas dataframe.  Also populates 'max_detection_conf', even if it's",0
not present in the .json file.,0
detectionResults[detectionResults['failure'].notna()],0
"Before doing any real work, make sure we can *probably* access images",0
"This is just a cursory check on the first image, but it heads off most",0
"problems related to incorrect mount points, etc.  Better to do this before",0
spending 20 minutes finding repeat detections.,0
#%% Separate files into directories,0
This will be a map from a directory name to smaller data frames,0
This is a mapping back into the rows of the original table,0
iRow = 0; row = detectionResults.iloc[0],0
Create a new DataFrame with just this row,0
rowsByDirectory[dirName] = pd.DataFrame(row),0
...for each unique detection,0
Convert lists of rows to proper DataFrames,0
#% Look for matches (or load them from file),0
"Are we actually looking for matches, or just loading from a file?",0
length-nDirs list of lists of DetectionLocation objects,0
We're actually looking for matches...,0
"We get slightly nicer progress bar behavior using threads, by passing a pbar",0
object and letting it get updated.  We can't serialize this object across,0
processes.,0
#%% Find suspicious locations based on match results,0
For each directory,0
,0
iDir = 51,0
A list of DetectionLocation objects,0
A list of DetectionLocation objects,0
occurrenceList is a list of file/detection pairs,0
Sort the above-threshold detections for easier review,0
...for each directory,0
If we're just loading detections from a file...,0
"We're skipping detection-finding, but to see which images are actually legit false",0
"positives, we may be looking for physical files or loading from a text file.",0
For each directory,0
iDir = 0; detections = suspiciousDetections[0],0
,0
"suspiciousDetections is an array of DetectionLocation objects,",0
one per directory.,0
For each detection that was present before filtering,0
iDetection = 0; detection = detections[iDetection],0
Are we checking the directory to see whether detections were actually false,0
"positives, or reading from a list?",0
Is the image still there?,0
"If not, remove this from the list of suspicious detections",0
...for each detection,0
...for each directory,0
...if we are/aren't finding detections (vs. loading from file),0
Create filtering directory,0
iDir = 0; suspiciousDetectionsThisDir = suspiciousDetections[iDir],0
suspiciousDetectionsThisDir is a list of DetectionLocation objects,0
iDetection = 0; detection = suspiciousDetectionsThisDir[0],0
Sort instances in descending order by confidence,0
Choose the highest-confidence index,0
Should we render (typically in a very light color) detections,0
*other* than the one we're highlighting here?,0
Render other detections first (typically in a thin+light box),0
Now render the example detection (on top of at least one,0
of the other detections),0
This converts the *first* instance to an API standard detection;,0
"because we just sorted this list in descending order by confidence,",0
this is the highest-confidence detection.,0
...if we are/aren't rendering other bounding boxes,0
...for each detection in this folder,0
...for each folder,0
Write out the detection index,0
...if we're writing filtering info,0
...find_repeat_detections(),0
#######,0
,0
find_repeat_detections.py,0
,0
"If you want to use this script, we recommend that you read the user's guide:",0
,0
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms,0
,0
"Really, don't try to run this script without reading the user's guide, you'll think",0
it's more magical than it is.,0
,0
"This script looks through a sequence of detections in the API output json file, and finds",0
"candidates that might be ""repeated false positives"", i.e. that random branch that the",0
detector thinks is an animal/person/vehicle.,0
,0
"Typically after running this script, you would do a manual step to remove",0
"true positives, then run remove_repeat_detections to produce a final output file.",0
,0
There's no way that statement was self-explanatory; see the user's guide.,0
,0
#######,0
%% Constants and imports,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
Convert to an options object,0
,0
If a request has been sent to AML for batch scoring but the monitoring thread of the API was,0
"interrupted (uncaught exception or having to re-start the API container), we could manually",0
"aggregate results from each shard using this script, assuming all jobs submitted to AML have finished.",0
,0
Need to have set environment variables STORAGE_ACCOUNT_NAME and STORAGE_ACCOUNT_KEY to those of the,0
"storage account backing the API. Also need to adjust the INTERNAL_CONTAINER, AML_CONTAINER and",0
AML_CONFIG fields in api_core/orchestrator_api/api_config.py to match the instance of the API that this,0
request was submitted to.,0
,0
May need to change the import statement in api_core/orchestrator_api/orchestrator.py,0
"""from sas_blob_utils import SasBlob"" to",0
"""from .sas_blob_utils import SasBlob"" to not confuse with the module in AI4Eutils;",0
"and change ""import api_config"" to",0
"""from api.batch_processing.api_core.orchestrator_api import api_config""",0
Execute this script from the root of the repository. You may need to add the repository to PYTHONPATH.,0
"list_jobs_submitted cannot be serialized (""can't pickle _thread.RLock objects ""), but",0
do not need it for aggregating results,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%% helper classes and functions,0
TODO log exception when we have more telemetry,1
TODO check that the expiry date of input_container_sas is at least a month,0
into the future,0
"if no permission specified explicitly but has an access policy, assumes okay",0
TODO - check based on access policy as well,1
return current UTC time as a string in the ISO 8601 format (so we can query by,0
timestamp in the Cosmos DB job status table.,0
example: '2021-02-08T20:02:05.699689Z',0
"image_paths will have length at least 1, otherwise would have ended before this step",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
a job moves from created to running/problem after the Batch Job has been submitted,0
"job_id should be unique across all instances, and is also the partition key",0
TODO do not read the entry first to get the call_params when the Cosmos SDK add a,1
patching functionality:,0
https://feedback.azure.com/forums/263030-azure-cosmos-db/suggestions/6693091-be-able-to-do-partial-updates-on-document,0
need to retain other fields in 'status' to be able to restart monitoring thread,0
retain existing fields; update as needed,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
sentinel should change if new configurations are available,0
configs have not changed,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Gunicorn logger handler will get attached if needed in server.py,0
set for all tasks in the job,0
cannot execute the scoring script that is in the mounted directory; has to be copied to cwd,0
not luck giving the commandline arguments via formatted string - set as env vars instead,0
form shards of images and assign each shard to a Task,0
for persisting stdout and stderr,0
persist stdout and stderr (will be removed when node removed),0
paths are relative to the Task working directory,0
can also just upload on failure,0
first try submitting Tasks,0
retry submitting Tasks,0
Change the Job's on_all_tasks_complete option to 'terminateJob' so the Job's status changes automatically,0
after all submitted tasks are done,0
This is so that we do not take up the quota for active Jobs in the Batch account.,0
return type: TaskAddCollectionResult,0
actually we should probably only re-submit if it's a server_error,0
docs: # https://docs.microsoft.com/en-us/rest/api/batchservice/odata-filters-in-batch#list-tasks,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%% Flask app,0
reference: https://trstringer.com/logging-flask-gunicorn-the-manageable-way/,0
%% Helper classes,0
%% Flask endpoints,0
required params,0
can be an URL to a file not hosted in an Azure blob storage container,0
"if use_url, then images_requested_json_sas is required",0
optional params,0
check model_version is among the available model versions,0
check request_name has only allowed characters,0
optional params for telemetry collection - logged to status table for now as part of call_params,0
All API instances / node pools share a quota on total number of active Jobs;,0
we cannot accept new Job submissions if we are at the quota,0
required fields,0
request_status is either completed or failed,0
the create_batch_job thread will stop when it wakes up the next time,0
"Fix for Zooniverse - deleting any ""-"" characters in the job_id",0
"If the status is running, it could be a Job submitted before the last restart of this",0
"API instance. If that is the case, we should start to monitor its progress again.",0
WARNING model_version could be wrong (a newer version number gets written to the output file) around,0
"the time that  the model is updated, if this request was submitted before the model update",0
and the API restart; this should be quite rare,0
conform to previous schemes,0
%% undocumented endpoints,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Gunicorn logger handler will get attached if needed in server.py,0
request_name and request_submission_timestamp are for appending to,0
output file names,0
image_paths can be a list of strings (Azure blob names or public URLs),0
"or a list of length-2 lists where each is a [image_id, metadata] pair",0
Case 1: listing all images in the container,0
- not possible to have attached metadata if listing images in a blob,0
list all images to process,0
+ 1 so if the number of images listed > MAX_NUMBER_IMAGES_ACCEPTED_PER_JOB,0
we will know and not proceed,0
Case 2: user supplied a list of images to process; can include metadata,0
filter down to those conforming to the provided prefix and accepted suffixes (image file types),0
prefix is case-sensitive; suffix is not,0
"Although urlparse(p).path preserves the extension on local paths, it will not work for",0
"blob file names that contains ""#"", which will be treated as indication of a query.",0
"If the URL is generated via Azure Blob Storage, the ""#"" char will be properly encoded",0
apply the first_n and sample_n filters,0
OK if first_n > total number of images,0
sample by shuffling image paths and take the first sample_n images,0
"upload the image list to the container, which is also mounted on all nodes",0
all sharding and scoring use the uploaded list,0
now request_status moves from created to running,0
an extra field to allow the monitoring thread to restart after an API restart: total number of tasks,0
also record the number of images to process for reporting,0
start the monitor thread with the same name,0
"both succeeded and failed tasks are marked ""completed"" on Batch",0
"preserving format from before, but SAS URL to 'failed_images' and 'images' are no longer provided",0
"failures should be contained in the output entries, indicated by an 'error' field",0
"when people download this, the timestamp will have : replaced by _",0
check if the result blob has already been written (could be another instance of the API / worker thread),0
"and if so, skip aggregating and uploading the results, and just generate the SAS URL, which",0
could be needed still if the previous request_status was `problem`.,0
upload the output JSON to the Job folder,0
%% Helper functions *copied* from ct_utils.py and visualization/visualization_utils.py,0
"Determine the factor, which shifts the decimal point of x",0
just behind the last significant digit,0
"Shift decimal point by multiplicatipon with factor, flooring, and",0
division by factor,0
PIL.Image.convert() returns a converted copy of this image,0
alter orientation as needed according to EXIF tag 0x112 (274) for Orientation,0
https://gist.github.com/dangtrinhnt/a577ece4cbe5364aad28,0
https://www.media.mit.edu/pia/Research/deepview/exif.html,0
"%% TFDetector class, an unmodified *copy* of the class in detection/tf_detector.py,",0
so we do not have to import the packages required by run_detector.py,0
Number of decimal places to round to for confidence and bbox coordinates,0
"MegaDetector was trained with batch size of 1, and the resizing function is a part",0
of the inference graph,0
An enumeration of failure reasons,0
"change from [y1, x1, y2, x2] to [x1, y1, width, height]",0
convert numpy floats to Python floats,0
need to change the above line to the following if supporting a batch size > 1 and resizing to the same size,0
"np_images = [np.asarray(image, np.uint8) for image in images]",0
"images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)",0
performs inference,0
our batch size is 1; need to loop the batch dim if supporting batch size > 1,0
%% Scoring script,0
determine if there is metadata attached to each image_id,0
information to determine input and output locations,0
other parameters for the task,0
test that we can write to output path; also in case there is no image to process,0
list images to process,0
"items in this list can be strings or [image_id, metadata]",0
model path,0
"Path to .pb TensorFlow detector model file, relative to the",0
models/megadetector_copies folder in mounted container,0
score the images,0
,0
manage_video_batch.py,0
,0
Notebook-esque script to manage the process of running a local batch of videos,0
through MD.  Defers most of the heavy lifting to manage_local_batch.py .,0
,0
%% Imports and constants,0
%% Split videos into frames,0
"%% List frame files, break into folders",0
Find unique (relative) folders,0
fn = frame_files[0],0
%% List videos,0
%% Check for videos that are missing entirely,0
list(folder_to_frame_files.keys())[0],0
video_filenames[0],0
fn = video_filenames[0],0
%% Check for videos with very few frames,0
%% Print the list of videos that are problematic,0
%% Process images like we would for any other camera trap job,0
"...typically using manage_local_batch.py, but do this however you like, as long",0
as you get a results file at the end.,0
,0
"If you do RDE, remember to use the second folder from the bottom, rather than the",0
bottom-most folder.,0
%% Convert frame results to video results,0
%% Confirm that the videos in the .json file are what we expect them to be,0
%% Scrap,0
%% Test a possibly-broken video,0
%% List videos in a folder,0
%% Imports,0
%% Constants,0
%% Classes,0
class variables,0
"instance variables, in order of when they are typically set",0
Leaving this commented out to remind us that we don't want this check here; let,0
the API fail on these images.  It's a huge hassle to remove non-image,0
files.,0
,0
for path_or_url in images_list:,0
if not is_image_file_or_url(path_or_url):,0
raise ValueError('{} is not an image'.format(path_or_url)),0
Commented out as a reminder: don't check task status (which is a rest API call),0
in __repr__; require the caller to explicitly request status,0
"status=getattr(self, 'status', None))",0
estimate # of failed images from failed shards,0
Download all three JSON urls to memory,0
Remove files that were submitted but don't appear to be images,0
assert all(is_image_file_or_url(s) for s in submitted_images),0
Diff submitted and processed images,0
Confirm that the procesed images are a subset of the submitted images,0
https://www.geeksforgeeks.org/break-list-chunks-size-n-python/,0
%% Interactive driver,0
%%,0
%%,0
%%,0
%% Imports and constants,0
%% Constants I set per script,0
## Required,0
base_output_folder_name = os.path.expanduser('~/postprocessing/' + organization_name_short),0
Shared Access Signature (SAS) tokens for the Azure Blob Storage container.,0
Leading question mark is optional.,0
,0
The read-only token is used for accessing images; the write-enabled token is,0
used for writing file lists.,0
## Typically left as default,0
"Pre-pended to all folder names/prefixes, if they're defined below",0
"This is how we break the container up into multiple taskgroups, e.g., for",0
separate surveys. The typical case is to do the whole container as a single,0
taskgroup.,0
"If your ""folders"" are really logical folders corresponding to multiple folders,",0
map them here,0
"A list of .json files to load images from, instead of enumerating.  Formatted as a",0
"dictionary, like folder_prefixes.",0
This is only necessary if you will be performing postprocessing steps that,0
"don't yet support SAS URLs, specifically the ""subsetting"" step, or in some",0
cases the splitting of files into multiple output directories for,0
empty/animal/vehicle/people.,0
,0
"For those applications, you will need to mount the container to a local drive.",0
For this case I recommend using rclone whether you are on Windows or Linux;,0
rclone is much easier than blobfuse for transient mounting.,0
,0
"But most of the time, you can ignore this.",0
"Supported model_versions: '4', '3', '4_prelim'",0
,0
Also available at the /supported_model_versions and /default_model_version,0
endpoints,0
,0
"Unless you have any specific reason to set this to a non-default value, leave",0
"it at the default, which as of 2020.04.28 is MegaDetector 4.1",0
,0
"additional_task_args = {""model_version"":""4_prelim""}",0
,0
"file_lists_by_folder will contain a list of local JSON file names,",0
each JSON file contains a list of blob names corresponding to an API taskgroup,0
"%% Derived variables, path setup",0
local folders,0
Turn warnings into errors if more than this many images are missing,0
%% Support functions,0
"scheme, netloc, path, query, fragment",0
%% Read images from lists or enumerate blobs to files,0
folder_name = folder_names[0],0
"Load file lists for this ""folder""",0
,0
file_list = input_file_lists[folder][0],0
Write to file,0
A flat list of blob paths for each folder,0
folder_name = folder_names[0],0
"If we don't/do have multiple prefixes to enumerate for this ""folder""",0
"If this is intended to be a folder, it needs to end in '/', otherwise",0
files that start with the same string will match too,0
...for each prefix,0
Write to file,0
...for each folder,0
%% Some just-to-be-safe double-checking around enumeration,0
Make sure each folder has at least one image matched; the opposite is usually a sign of a copy/paste issue,0
...for each image,0
...for each prefix,0
...for each folder,0
Make sure each image comes from one of our folders; the opposite is usually a sign of a bug up above,0
...for each prefix,0
...for each folder,0
...for each image,0
%% Divide images into chunks for each folder,0
The JSON file at folder_chunks[i][j] corresponds to task j of taskgroup i,0
list_file = file_lists_by_folder[0],0
"%% Create taskgroups and tasks, and upload image lists to blob storage",0
periods not allowed in task names,0
%% Generate API calls for each task,0
clipboard.copy(request_strings[0]),0
clipboard.copy('\n\n'.join(request_strings)),0
%% Run the tasks (don't run this cell unless you are absolutely sure!),0
I really want to make sure I'm sure...,0
%% Estimate total time,0
Around 0.8s/image on 16 GPUs,0
%% Manually create task groups if we ran the tasks manually,0
%%,0
"%% Write task information out to disk, in case we need to resume",0
%% Status check,0
print(task.id),0
%% Resume jobs if this notebook closes,0
%% For multiple tasks (use this only when we're merging with another job),0
%% For just the one task,0
%% Load into separate taskgroups,0
p = task_cache_paths[0],0
%% Typically merge everything into one taskgroup,0
"%% Look for failed shards or missing images, start new tasks if necessary",0
List of lists of paths,0
i_taskgroup = 0; taskgroup = taskgroups[i_taskgroup];,0
"Make a copy, because we append to taskgroup",0
i_task = 0; task = tasks[i_task],0
Each taskgroup corresponds to one of our folders,0
Check that we have (almost) all the images,0
Now look for failed images,0
Write it out as a flat list as well (without explanation of failures),0
...for each task,0
...for each task group,0
%% Pull results,0
i_taskgroup = 0; taskgroup = taskgroups[i_taskgroup]; task = taskgroup[0],0
Each taskgroup corresponds to one of our folders,0
...for each task,0
...for each task group,0
%% Combine results from task groups into final output files,0
Check that we have (almost) all the images,0
The only reason we should ever have a repeated request is the case where an,0
"image was missing and we reprocessed it, or where it failed and later succeeded",0
"There may be non-image files in the request list, ignore those",0
Something has gone bonkers if there are images in the results that,0
aren't in the request,0
...for each folder,0
%% Post-processing (no ground truth),0
i_folder = 0; folder_name_raw = folder_names[i_folder],0
"Omit some pages from the output, useful when animals are rare",0
%% Manual processing follows,0
,0
"Everything after this should be considered mostly manual, and no longer includes",0
looping over folders.,0
,0
%% RDE (sample directory collapsing),0
"In this example, the camera created folders called ""100EK113"", ""101EK113"", etc., for every N images",0
"%% Repeat detection elimination, phase 1",0
"Deliberately leaving these imports here, rather than at the top, because this cell is not",0
typically executed,0
To invoke custom collapsing of folders for a particular manufacturer's naming scheme,0
options.customDirNameFunction = remove_overflow_folders,0
Exclude people and vehicles from RDE,0
"options.excludeClasses = [2,3]",0
%% Manual RDE step,0
# DELETE THE VALID DETECTIONS ##,0
%% Re-filtering,0
%% Post-processing (post-RDE),0
i_folder = 0; folder_name_raw = folder_names[i_folder],0
"Omit some pages from the output, useful when animals are rare",0
"print('No RDE file available for {}, skipping'.format(folder_name))",0
continue,0
%% Create a new category for large boxes,0
%% Subsetting,0
i_folder = 0; folder_name = folders[i_folder],0
%% String replacement,0
%% Folder splitting,0
i_folder = 0; folder_name_raw = folder_names[i_folder],0
api_output_file = folder_name_to_combined_output_file[folder_name],0
%% Imports and constants,0
from ai4eutils,0
To specify a non-default confidence threshold for including detections in the .json file,0
Turn warnings into errors if more than this many images are missing,0
Only relevant when we're using a single GPU,0
"Specify a target image size when running MD... strongly recommended to leave this at ""None""",0
Only relevant when running on CPU,0
OS-specific script line continuation character,0
OS-specific script comment character,0
"Prefer threads on Windows, processes on Linux",0
"This is for things like image rendering, not for MegaDetector",0
Should we use YOLOv5's val.py instead of run_detector_batch.py?,1
Directory in which to run val.py.  Only relevant if use_yolo_inference_scripts is True.,0
Should we remove intermediate files used for running YOLOv5's val.py?,0
,0
Only relevant if use_yolo_inference_scripts is True.,0
Should we apply YOLOv5's augmentation?  Only allowed when use_yolo_inference_scripts,0
is True.,0
%% Constants I set per script,0
Optional descriptor,0
model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v5.0.0/md_v5b.0.0.pt'),0
model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v4.1.0/md_v4.1.0.pb'),0
"Number of jobs to split data into, typically equal to the number of available GPUs",0
Only used to print out a time estimate,0
"%% Derived variables, constant validation, path setup",0
%% Enumerate files,0
%% Load files from prior enumeration,0
%% Divide images into chunks,0
%% Estimate total time,0
%% Write file lists,0
%% Generate commands,0
"A list of the scripts tied to each GPU, as absolute paths.  We'll write this out at",0
the end so each GPU's list of commands can be run at once.  Generally only used when,0
"running lots of small batches via YOLOv5's val.py, which doesn't support checkpointing.",0
i_task = 0; task = task_info[i_task],0
Generate the script to run MD,0
Check whether this output file exists,0
Generate the script to resume from the checkpoint (only supported with MD inference code),0
...for each task,0
Write out a script for each GPU that runs all of the commands associated with,0
that GPU.  Typically only used when running lots of little scripts in lieu,0
of checkpointing.,0
...for each GPU,0
%% Run the tasks,0
%%% Run the tasks (commented out),0
i_task = 0; task = task_info[i_task],0
"This will write absolute paths to the file, we'll fix this later",1
...for each chunk,0
...if False,0
"%% Load results, look for failed or missing images in each task",0
i_task = 0; task = task_info[i_task],0
im = task_results['images'][0],0
...for each task,0
%% Merge results files and make images relative,0
im = combined_results['images'][0],0
%% Post-processing (pre-RDE),0
options.sample_seed = 0,0
"Omit some pages from the output, useful when animals are rare",0
%% RDE (sample directory collapsing),0
"In this example, the camera created folders called ""100EK113"", ""101EK113"", etc., for every N images",0
%%,0
%%,0
%%,0
relativePath = image_filenames[0],0
"%% Repeat detection elimination, phase 1",0
"Deliberately leaving these imports here, rather than at the top, because this",0
cell is not typically executed,0
options.minSuspiciousDetectionSize = 0.05,0
This will cause a very light gray box to get drawn around all the detections,0
we're *not* considering as suspicious.,0
options.lineThickness = 5,0
options.boxExpansion = 8,0
To invoke custom collapsing of folders for a particular manufacturer's naming scheme,0
options.customDirNameFunction = remove_overflow_folders,0
Exclude people and vehicles from RDE,0
"options.excludeClasses = [2,3]",0
options.maxImagesPerFolder = 50000,0
options.includeFolders = ['a/b/c'],0
options.excludeFolder = ['a/b/c'],0
"Can be None, 'xsort', or 'clustersort'",0
import clipboard; clipboard.copy(os.path.dirname(suspiciousDetectionResults.filterFile)),0
path_utils.open_file(os.path.dirname(suspiciousDetectionResults.filterFile)),0
%% Manual RDE step,0
# DELETE THE VALID DETECTIONS ##,0
%% Re-filtering,0
%% Post-processing (post-RDE),0
options.sample_seed = 0,0
"Omit some pages from the output, useful when animals are rare",0
"%% Run MegaClassifier (actually, write out a script that runs MegaClassifier)",0
"This is just passed along to the metadata in the output file, it has no impact",0
on how the classification scripts run.,0
#%% Set up environment,0
commands.append('cd CameraTraps/classification\n'),0
commands.append('conda activate cameratraps-classifier\n'),0
#%% Crop images,0
fn = input_files[0],0
#%% Run classifier,0
fn = input_files[0],0
#%% Remap classifier outputs,0
fn = input_files[0],0
#%% Merge classification and detection outputs,0
fn = input_files[0],0
#%% Write  out classification script,0
"%% Run a non-MegaClassifier classifier (i.e., a classifier with no output mapping)",0
"This is just passed along to the metadata in the output file, it has no impact",0
on how the classification scripts run.,0
#%% Set up environment,0
#%% Crop images,0
fn = input_files[0],0
#%% Run classifier,0
fn = input_files[0],0
#%% Merge classification and detection outputs,0
fn = input_files[0],0
#%% Write everything out,0
%% Run the classifier(s) via the .sh script(s) or batch file(s) we just wrote,0
...,0
%% Within-image classification smoothing,0
,0
Only count detections with a classification confidence threshold above,0
"*classification_confidence_threshold*, which in practice means we're only",0
looking at one category per detection.,0
,0
If an image has at least *min_detections_above_threshold* such detections,0
"in the most common category, and no more than *max_detections_secondary_class*",0
"in the second-most-common category, flip all detections to the most common",0
category.,0
,0
"Optionally treat some classes as particularly unreliable, typically used to overwrite an",0
"""other"" class.",0
,0
This cell also removes everything but the non-dominant classification for each detection.,0
,0
How many detections do we need above the classification threshold to determine a dominant category,0
for an image?,0
"Even if we have a dominant class, if a non-dominant class has at least this many classifications",0
"in an image, leave them alone.",0
"If the dominant class has at least this many classifications, overwrite ""other"" classifications",0
What confidence threshold should we use for assessing the dominant category in an image?,0
Which classifications should we even bother over-writing?,0
Detection confidence threshold for things we count when determining a dominant class,0
Which detections should we even bother over-writing?,0
"Before we do anything else, get rid of everything but the top classification",0
for each detection.,0
...for each detection in this image,0
...for each image,0
im = d['images'][0],0
...for each classification,0
...if there are classifications for this detection,0
...for each detection,0
"Handle a quirky special case: if the most common category is ""other"" and",0
"it's ""tied"" with the second-most-common category, swap them",0
secondary_count = category_to_count[keys[1]],0
The 'secondary count' is the most common non-other class,0
If we have at least *min_detections_to_overwrite_other* in a category that isn't,0
"""other"", change all ""other"" classifications to that category",0
...for each classification,0
...if there are classifications for this detection,0
...for each detection,0
"...if we should overwrite all ""other"" classifications",0
"At this point, we know we have a dominant category; change all other above-threshold",0
"classifications to that category.  That category may have been ""other"", in which",0
case we may have already made the relevant changes.,0
det = detections[0],0
...for each classification,0
...if there are classifications for this detection,0
...for each detection,0
...for each image,0
...for each file we want to smooth,0
"%% Post-processing (post-classification, post-within-image-smoothing)",0
classification_detection_file = classification_detection_files[1],0
%% Read EXIF data from all images,0
%% Prepare COCO-camera-traps-compatible image objects for EXIF results,0
import dateutil,0
"This is a standard format for EXIF datetime, and dateutil.parser",0
doesn't handle it correctly.,0
return dateutil.parser.parse(s),0
exif_result = exif_results[0],0
Currently we assume that each leaf-node folder is a location,0
"We collected this image this century, but not today, make sure the parsed datetime",0
jives with that.,0
,0
The latter check is to make sure we don't repeat a particular pathological approach,0
"to datetime parsing, where dateutil parses time correctly, but swaps in the current",0
date when it's not sure where the date is.,0
...for each exif image result,0
%% Assemble into sequences,0
Make a list of images appearing at each location,0
im = image_info[0],0
%% Load classification results,0
Map each filename to classification results for that file,0
%% Smooth classification results over sequences (prep),0
These are the only classes to which we're going to switch other classifications,0
Only switch classifications to the dominant class if we see the dominant class at least,0
this many times,0
"If we see more than this many of a class that are above threshold, don't switch those",0
classifications to the dominant class.,0
"If the ratio between a dominant class and a secondary class count is greater than this,",0
"regardless of the secondary class count, switch those classificaitons (i.e., ignore",0
max_secondary_class_classifications_above_threshold_for_class_smoothing).,0
,0
"This may be different for different dominant classes, e.g. if we see lots of cows, they really",0
"tend to be cows.  Less so for canids, so we set a higher ""override ratio"" for canids.",0
"If there are at least this many classifications for the dominant class in a sequence,",0
"regardless of what that class is, convert all 'other' classifications (regardless of",0
confidence) to that class.,0
"If there are at least this many classifications for the dominant class in a sequence,",0
"regardless of what that class is, classify all previously-unclassified detections",0
as that class.,0
Only count classifications above this confidence level when determining the dominant,0
"class, and when deciding whether to switch other classifications.",0
Confidence values to use when we change a detection's classification (the,0
original confidence value is irrelevant at that point),0
%% Smooth classification results over sequences (supporting functions),0
im = images_this_sequence[0],0
det = results_this_image['detections'][0],0
Only process animal detections,0
Only process detections with classification information,0
"We only care about top-1 classifications, remove everything else",0
Make sure the list of classifications is already sorted by confidence,0
...and just keep the first one,0
"Confidence values should be sorted within a detection; verify this, and ignore",0
...for each detection in this image,0
...for each image in this sequence,0
...top_classifications_for_sequence(),0
Count above-threshold classifications in this sequence,0
Sort the dictionary in descending order by count,0
"Handle a quirky special case: if the most common category is ""other"" and",0
"it's ""tied"" with the second-most-common category, swap them.",0
...def count_above_threshold_classifications(),0
%% Smooth classifications at the sequence level (main loop),0
Break if this token is contained in a filename (set to None for normal operation),0
i_sequence = 0; seq_id = all_sequences[i_sequence],0
Count top-1 classifications in this sequence (regardless of confidence),0
Handy debugging code for looking at the numbers for a particular sequence,0
Count above-threshold classifications for each category,0
"If our dominant category ID isn't something we want to smooth to, don't mess around with this sequence",0
"# Smooth ""other"" classifications ##",0
"By not re-computing ""max_count"" here, we are making a decision that the count used",0
"to decide whether a class should overwrite another class does not include any ""other""",0
classifications we changed to be the dominant class.  If we wanted to include those...,0
,0
sorted_category_to_count = count_above_threshold_classifications(classifications_this_sequence),0
max_count = get_first_value_from_sorted_dictionary(sorted_category_to_count),0
assert dominant_category_id == get_first_key_from_sorted_dictionary(sorted_category_to_count),0
# Smooth non-dominant classes ##,0
Don't flip classes to the dominant class if they have a large number of classifications,0
"Don't smooth over this class if there are a bunch of them, and the ratio",0
if primary to secondary class count isn't too large,0
Default ratio,0
Does this dominant class have a custom ratio?,0
# Smooth unclassified detections ##,0
...for each sequence,0
%% Write smoothed classification results,0
"%% Post-processing (post-classification, post-within-image-and-within-sequence-smoothing)",0
%% Zip .json files,0
%% 99.9% of jobs end here,0
Everything after this is run ad hoc and/or requires some manual editing.,0
%% Compare results files for different model versions (or before/after RDE),0
Choose all pairwise combinations of the files in [filenames],0
%% Merge in high-confidence detections from another results file,0
%% Create a new category for large boxes,0
"This is a size threshold, not a confidence threshold",0
size_options.categories_to_separate = [3],0
%% Preview large boxes,0
%% .json splitting,0
options.query = None,0
options.replacement = None,0
Reminder: 'n_from_bottom' with a parameter of zero is the same as 'bottom',0
%% Custom splitting/subsetting,0
i_folder = 0; folder_name = folders[i_folder],0
"This doesn't do anything in this case, since we're not splitting folders",0
options.make_folder_relative = True,0
%% String replacement,0
%% Splitting images into folders,0
%% Generate commands for a subset of tasks,0
i_task = 8,0
...for each task,0
%% End notebook: turn this script into a notebook (how meta!),0
Exclude everything before the first cell,0
Remove the first [first_non_empty_lines] from the list,0
Add the last cell,0
,0
xmp_integration.py,0
,0
"Tools for loading MegaDetector batch API results into XMP metadata, specifically",0
for consumption in digiKam:,0
,0
https://cran.r-project.org/web/packages/camtrapR/vignettes/camtrapr2.html,0
,0
%% Imports and constants,0
%% Class definitions,0
Folder where images are stored,0
.json file containing MegaDetector output,0
"String to remove from all path names, typically representing a",0
prefix that was added during MegaDetector processing,0
Optionally *rename* (not copy) all images that have no detections,0
above [rename_conf] for the categories in rename_cats from x.jpg to,0
x.check.jpg,0
"Comma-deleted list of category names (or ""all"") to apply the rename_conf",0
behavior to.,0
"Minimum detection threshold (applies to all classes, defaults to None,",0
i.e. 0.0,0
%% Functions,0
Relative image path,0
Absolute image path,0
List of categories to write to XMP metadata,0
Categories with above-threshold detections present for,0
this image,0
Maximum confidence for each category,0
Have we already added this to the list of categories to,0
write out to this image?,0
If we're supposed to compare to a threshold...,0
Else we treat *any* detection as valid...,0
Keep track of the highest-confidence detection for this class,0
If we're doing the rename/.check behavior...,0
Legacy code to rename files where XMP writing failed,0
%% Interactive/test driver,0
%%,0
%% Command-line driver,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Cosmos DB `batch-api-jobs` table for job status,0
"aggregate the number of images, country and organization names info from each job",0
submitted during yesterday (UTC time),0
create the card,0
,0
api_frontend.py,0
,0
"Defines the Flask app, which takes requests (one or more images) from",0
"remote callers and pushes the images onto the shared Redis queue, to be processed",0
by the main service in api_backend.py .,0
,0
%% Imports,0
%% Initialization,0
%% Support functions,0
Make a dict that the request_processing_function can return to the endpoint,0
function to notify it of an error,0
Verify that the content uploaded is not too big,0
,0
request.content_length is the length of the total payload,0
Verify that the number of images is acceptable,0
...def check_posted_data(request),0
%% Main loop,0
Check whether the request_processing_function had an error,0
Write images to temporary files,0
,0
TODO: read from memory rather than using intermediate files,1
"Submit the image(s) for processing by api_backend.py, who is waiting on this queue",0
TODO: convert to a blocking read and eliminate the sleep() statement in this loop,1
"image = Image.open(os.path.join(temp_direc, image_name))",0
...if we do/don't have a request available on the queue,0
...while(True),0
...def detect_sync(),0
%% Command-line driver,0
use --non-docker if you are testing without Docker,0
,0
python api_frontend.py --non-docker,0
# Camera trap real-time API configuration,0
"Full path to the temporary folder for image storage, only meaningful",0
within the Docker container,0
Upper limit on total content length (all images and parameters),0
Minimum confidence threshold for detections,0
Minimum confidence threshold for showing a bounding box on the output image,0
Use this when testing without Docker,0
,0
api_backend.py,0
,0
"Defines the model execution service, which pulls requests (one or more images)",0
"from the shared Redis queue, and runs them through the TF model.",0
,0
%% Imports,0
%% Initialization,0
%% Main loop,0
TODO: convert to a blocking read and eliminate the sleep() statement in this loop,1
Filter the detections by the confidence threshold,0
,0
"Each result is [ymin, xmin, ymax, xmax, confidence, category]",0
,0
"Coordinates are relative, with the origin in the upper-left",0
...if serialized_entry,0
...while(True),0
...def detect_process(),0
%% Command-line driver,0
use --non-docker if you are testing without Docker,0
,0
python api_frontend.py --non-docker,0
run detections on a test image to load the model,0
%%,0
Importing libraries,0
%%,0
%%,0
%%,0
%%,0
GPU configuration: set up GPUs based on availability and user specification,0
Environment variable setup for numpy multi-threading. It is important to avoid cpu and ram issues.,0
Load and set configurations from the YAML file,0
Set a global seed for reproducibility,0
"If the annotation directory does not have a data split, split the data first",0
Replace annotation dir from config with the directory containing the split files,0
Split the data according to the split type,0
Get the path to the annotation files,0
Crop training data,0
Crop validation data,0
Crop test data (most likely we don't need this),0
Dataset and algorithm loading based on the configuration,0
Logger setup based on the specified logger type,0
Callbacks for model checkpointing and learning rate monitoring,0
Trainer configuration in PyTorch Lightning,0
"Training, validation, or evaluation execution based on the mode",0
%%,0
%%,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%%,0
PyTorch imports,0
"Importing the model, dataset, transformations and utility functions from PytorchWildlife",0
Importing the utility function for saving cropped images,0
Setting the device to use for computations ('cuda' indicates GPU),0
Initializing the MegaDetectorV5 model for image detection,0
Creating a dataset of images with the specified transform,0
Creating a DataLoader for batching and parallel processing of the images,0
Performing batch detection on the images,0
Saving the detected objects as cropped images,0
%%,0
Read the original CSV file,0
Prepare a list to store new records for the new CSV,0
Process the data if the name of the file is in the dataframe,0
Save the crop into a new csv,0
Add record to the new CSV data,0
Create a DataFrame from the new records,0
Define the path for the new CSV file,0
Save the new DataFrame to CSV,0
# DATA SPLITTING,0
Load the data from the csv file,0
Separate the features and the targets,0
First split to separate out the test set,0
Adjust val_size to account for the initial split,0
Second split to separate out the validation set,0
"Combine features, labels, and classification back into dataframes",0
Create the output directory in case that it does not exist,0
Save the splits to new CSV files,0
Return the dataframes,0
Load the data from the csv file,0
Calculate train size based on val and test size,0
Get unique locations,0
Split locations into train and temp (temporary holding for val and test),0
Adjust the proportions for val and test based on the remaining locations,0
"Allocate images to train, validation, and test sets based on their location",0
Save the datasets to CSV files,0
Return the split datasets,0
Load the data from the csv file,0
Convert 'Photo_Time' from string to datetime,0
Calculate train size based on val and test size,0
Sort by 'Photo_Time' to ensure chronological order,0
Group photos into sequences based on a 30-second interval,0
Assign unique sequence IDs to each group,0
Get unique sequence IDs,0
Split sequence IDs into train and temp (temporary holding for val and test),0
Adjust the proportions for val and test based on the remaining sequences,0
"Allocate images to train, validation, and test sets based on their sequence ID",0
Save the datasets to CSV files,0
Return the split datasets,0
Exportable class names for external use,0
Applying the ResNet layers and operations,0
Initialize the network with the specified settings,0
Selecting the appropriate ResNet architecture and pre-trained weights,0
self.pretrained_weights = ResNet18_Weights.IMAGENET1K_V1,0
self.pretrained_weights = ResNet50_Weights.IMAGENET1K_V1,0
Constructing the feature extractor and classifier,0
Criterion for binary classification,0
Load pre-trained weights and adjust for the current model,0
init_weights = self.pretrained_weights.get_state_dict(progress=True),0
Load the weights into the feature extractor,0
Identify missing and unused keys in the loaded weights,0
Import necessary libraries,0
Exportable class names for external use,0
Define normalization mean and standard deviation for image preprocessing,0
Define data transformations for training and validation datasets,0
Load data for prediction,0
Load data for training/validation,0
"Load datasets for different modes (training, validation, testing, prediction)",0
Calculate class counts and label mappings,0
Define parameters for the optimizer,0
Optimizer parameters for feature extraction,0
Optimizer parameters for the classifier,0
Setup optimizer and optimizer scheduler,0
Forward pass,0
Calculate loss,0
Forward pass,0
Forward pass,0
Concatenate outputs from all test steps,0
Calculate the metrics and save the output,0
Forward pass,0
Concatenate outputs from all predict steps,0
Compute the confusion matrix from true labels and predictions,0
Calculate class-wise accuracy (accuracy for each class),0
Calculate micro accuracy (overall accuracy),0
Calculate macro accuracy (mean of class-wise accuracies),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%%,0
Importing necessary basic libraries and modules,0
%%,0
PyTorch imports for tensor operations,0
%%,0
"Importing the models, transformations, and utility functions from PytorchWildlife",0
%%,0
Setting the device to use for computations ('cuda' indicates GPU),0
%%,0
Initializing the MegaDetectorV6 model for image detection,0
Uncomment the following line to use MegaDetectorV5 instead of MegaDetectorV6,0
"detection_model = pw_detection.MegaDetectorV5(device=DEVICE, pretrained=True, version=""a"")",0
%%,0
Initializing the model for image classification,0
%%,0
Initializing a box annotator for visualizing detections,0
Processing the video and saving the result with annotated detections and classifications,0
%%,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Importing necessary basic libraries and modules,0
PyTorch imports,0
"Importing the model, dataset, transformations and utility functions from PytorchWildlife",0
Importing basic libraries,0
"Importing the models, dataset, transformations, and utility functions from PytorchWildlife",0
Setting the device to use for computations ('cuda' indicates GPU),0
Initializing a supervision box annotator for visualizing detections,0
Create a temp folder,0
Initializing the detection and classification models,0
Defining functions for different detection scenarios,0
Create an exception for custom weights,0
"If the detection model is HerdNet, use dot annotator, else use box annotator",0
Herdnet receives both clf and det confidence thresholds,0
Only run classifier when detection class is animal,0
Clean the temp folder if it contains files,0
Check the contents of the extracted folder,0
If the detection model is HerdNet set batch_size to 1,0
Building Gradio UI,0
The timelapse checkbox is only visible when the detection model is not HerdNet,0
Show timelapsed checkbox only when detection model is not HerdNet,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%%,0
Importing necessary basic libraries and modules,0
PyTorch imports,0
%%,0
"Importing the model, dataset, transformations and utility functions from PytorchWildlife",0
%%,0
Setting the device to use for computations ('cuda' indicates GPU),0
%%,0
Initializing the MegaDetectorV6 model for image detection,0
Uncomment the following line to use MegaDetectorV5 instead of MegaDetectorV6,0
"detection_model = pw_detection.MegaDetectorV5(device=DEVICE, pretrained=True, version=""a"")",0
%% Single image detection,0
Specifying the path to the target image TODO: Allow argparsing,0
Performing the detection on the single image,0
Saving the detection results,0
Saving the detected objects as cropped images,0
%% Batch detection,0
Specifying the folder path containing multiple images for batch detection,0
Performing batch detection on the images,0
%% Output to annotated images,0
Saving the batch detection results as annotated images,0
%% Output to cropped images,0
Saving the detected objects as cropped images,0
%% Output to JSON results,0
Saving the detection results in JSON format,0
Saving the detection results in timelapse JSON format,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%%,0
Importing necessary basic libraries and modules,0
PyTorch imports,0
%%,0
"Importing the model, dataset, transformations and utility functions from PytorchWildlife",0
%%,0
Setting the device to use for computations ('cuda' indicates GPU),0
%%,0
Initializing the HerdNet model for image detection,0
"If you want to use ennedi dataset weigths, you can use the following line:",0
"detection_model = pw_detection.HerdNet(device=DEVICE, dataset=""ennedi"")",0
%% Single image detection,0
Performing the detection on the single image,0
%% Output to annotated images,0
Saving the batch detection results as annotated images,0
%% Batch image detection,0
Specifying the folder path containing multiple images for batch detection,0
Performing batch detection on the images,0
%% Output to annotated images,0
Saving the batch detection results as annotated images,0
Saving the detection results in JSON format,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%%,0
Importing necessary basic libraries and modules,0
PyTorch imports,0
%% Argument parsing,0
%%,0
Setting the device to use for computations ('cuda' indicates GPU),0
%%,0
Initializing the MegaDetectorV6 model for image detection,0
Uncomment the following line to use MegaDetectorV5 instead of MegaDetectorV6,0
"detection_model = pw_detection.MegaDetectorV5(device=DEVICE, pretrained=True, version=""a"")",0
%% Batch detection,0
Performing batch detection on the images,0
%% Output to JSON results,0
Saving the detection results in JSON format,0
Separate the positive and negative detections through file copying:,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Making the DetectionImageFolder class available for import from this module,0
Listing and sorting all image files in the specified directory,0
Get image filename and path,0
Load and convert image to RGB,0
Apply transformation if specified,0
Only run recognition on animal detections,0
Get image path and corresponding bbox xyxy for cropping,0
Load and crop image with supervision,0
Apply transformation if specified,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
from yolov5.utils.augmentations import letterbox,0
Making the provided classes available for import from this module,0
Convert PIL Image to Torch Tensor,0
Original shape,0
New shape,0
Scale ratio (new / old) and compute padding,0
Resize image,0
Pad image,0
Convert the image to a PyTorch tensor and normalize it,0
Resize and pad the image using a customized letterbox function.,0
Normalization constants,0
Define the sequence of transformations,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
!!! Output paths need to be optimized !!!,1
!!! Output paths need to be optimized !!!,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%%,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Importing basic libraries,0
Placeholder class-level attributes to be defined in derived classes,0
"If there are size differences in the input images, use a for loop instead of matrix processing for scaling",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Image size for the Opossum classifier,0
Class names for prediction,0
"If pretrained, use the provided URL to fetch the weights",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Making the PlainResNetInference class available for import from this module,0
Following the ResNet structure to extract features,0
Initialize the network and weights,0
... [Missing weight URL definition for ResNet18],0
... [Missing weight URL definition for ResNet50],0
Print missing and unused keys for debugging purposes,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Image size for the Opossum classifier,0
Class names for prediction,0
"If pretrained, use the provided URL to fetch the weights",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Image size for the Opossum classifier,0
Class names for prediction,0
"If pretrained, use the provided URL to fetch the weights",0
Configuration file for the Sphinx documentation builder.,0
,0
"For the full list of built-in configuration values, see the documentation:",0
https://www.sphinx-doc.org/en/master/usage/configuration.html,0
-- Project information -----------------------------------------------------,0
https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information,0
-- General configuration ---------------------------------------------------,0
https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration,0
-- Options for HTML output -------------------------------------------------,0
https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output,0
-- Options for todo extension ----------------------------------------------,1
https://www.sphinx-doc.org/en/master/usage/extensions/todo.html#configuration,0
"Determine the factor, which shifts the decimal point of x",0
just behind the last significant digit,0
"Shift decimal point by multiplicatipon with factor, flooring, and",0
division by factor,0
_ = pretty_print_object(obj),0
Sloppy that I'm making a module-wide change here...,1
Determine the coordinates of the intersection rectangle,0
The intersection of two axis-aligned bounding boxes is always an,0
axis-aligned bounding box,0
Compute the area of both AABBs,0
Compute the intersection over union by taking the intersection,0
area and dividing it by the sum of prediction + ground-truth,0
areas - the intersection area.,0
%% Functions for running commands as subprocesses,0
%%,0
%% Test driver for execute_and_print,0
%% Parallel test driver for execute_command_and_print,0
Should we use threads (vs. processes) for parallelization?,0
"Only relevant if n_workers == 1, i.e. if we're not parallelizing",0
%% Constants and imports,0
%% Classes,0
Collect all names,0
Make names unique and sort,0
cast location to string as the entries in locations are strings,0
Convert classnames to lowercase to simplify comparisons later,0
Normalize paths to simplify comparisons later,0
"Make custom replacements in filenames, typically used to",0
accommodate changes in root paths after DB construction,0
## Build useful mappings to facilitate working with the DB,0
Category ID <--> name,0
Image filename --> ID,0
Image ID --> image object,0
Image ID --> annotations,0
"Each image can potentially multiple annotations, hence using lists",0
...__init__,0
...class IndexedJsonDb,0
%% Functions,0
Find all unique locations,0
i_location = 0; location = locations[i_location],0
Sorting datetimes fails when there are None's in the list.  So instead of sorting datetimes,0
"directly, sort tuples with a boolean for none-ness, then the datetime itself.",0
,0
https://stackoverflow.com/questions/18411560/sort-list-while-pushing-none-values-to-the-end,0
previous_datetime = sorted_images_this_location[0]['datetime'],0
im = sorted_images_this_location[1],0
"Start a new sequence if necessary, including the case where this datetime is invalid",0
"If this was an invalid datetime, this will record the previous datetime",0
"as None, which will force the next image to start a new sequence.",0
...for each image in this location,0
Fill in seq_num_frames,0
...for each location,0
...create_sequences(),0
,0
cct_to_md.py,0
,0
"""Converts"" a COCO Camera Traps file to a MD results file.  Currently ignores",0
"non-bounding-box annotations, and gives all annotations a confidence of 1.0.",0
,0
The only reason to do this is if you are going to add information to an existing,0
"CCT-formatted dataset, and want to do that in Timelapse.",0
,0
"Currently assumes that width and height are present in the input data, does not",0
read them from images.,0
,0
%% Constants and imports,0
%% Functions,0
# Validate input,0
# Read input,0
# Prepare metadata,0
ann = d['annotations'][0],0
# Process images,0
im = d['images'][0],0
"MegaDetector: [x,y,width,height] (normalized, origin upper-left)",0
"CCT: [x,y,width,height] (absolute, origin upper-left)",0
...if there's a bounding box,0
...for each annotation,0
This field is no longer included in MD output files by default,0
im_out['max_detection_conf'] = max_detection_conf,0
...for each image,0
# Write output,0
...cct_to_md(),0
%% Command-line driver,0
TODO,1
%% Interactive driver,0
%%,0
%%,0
,0
cct_json_to_filename_json.py,0
,0
"Given a .json file in COCO Camera Traps format, outputs a .json-formatted list of",0
relative file names present in the CCT file.,0
,0
%% Constants and environment,0
%% Main function,0
"json.dump(s,open(outputFilename,'w'))",0
%% Command-line driver,0
%% Interactive driver,0
%%,0
,0
cct_to_csv.py,0
,0
"""Converts"" a COCO Camera Traps .json file to .csv, in quotes because",0
"all kinds of assumptions are made here, and if you have a particular .csv",0
"format in mind, YMMV.  Most notably, does not include any bounding box information",0
or any non-standard fields that may be present in the .json file.  Does not,0
propagate information about sequence-level vs. image-level annotations.,0
,0
"Does not assume access to the images, therefore does not open .jpg files to find",0
"datetime information if it's not in the metadata, just writes datetime as 'unknown'.",0
,0
%% Imports,0
%% Main function,0
#%% Read input,0
#%% Build internal mappings,0
annotation = annotations[0],0
#%% Write output file,0
im = images[0],0
Write out one line per class:,0
...for each class name,0
...for each image,0
...with open(output_file),0
...def cct_to_csv,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
,0
remove_exif.py,0
,0
"Removes all EXIF/IPTC/XMP metadata from a folder of images, without making",0
"backup copies, using pyexiv2.",0
,0
%% Imports and constants,0
%% List files,0
%% Remove EXIF data (support),0
PYEXIV2 IS NOT THREAD SAFE; DO NOT CALL THIS IN PARALLEL FROM A SINGLE PROCESS,1
data = img.read_exif(); print(data),0
%% Debug,0
%%,0
%%,0
%% Remove EXIF data (execution),0
fn = image_files[0],0
"joblib.Parallel defaults to a process-based backend, but let's be sure",0
"results = Parallel(n_jobs=n_exif_threads,verbose=2,prefer='processes')(delayed(remove_exif)(fn) for fn in image_files[0:10])",0
,0
yolo_to_coco.py,0
,0
Converts a YOLO-formatted dataset to a COCO-formatted dataset.,0
,0
"Currently supports only a single folder (i.e., no recursion).  Treats images without",0
corresponding .txt files as empty.,0
,0
%% Imports and constants,0
from ai4eutils,0
%% Support functions,0
Validate input,0
Class names,0
Blank lines should only appear at the end,0
Enumerate images,0
fn = image_files[0],0
Create the image object for this image,0
Is there an annotation file for this image?,0
"This is an image with no annotations, currently don't do anything special",0
here,0
s = lines[0],0
"COCO: [x_min, y_min, width, height] in absolute coordinates",0
"YOLO: [class, x_center, y_center, width, height] in normalized coordinates",0
...for each annotation,0
...if this image has annotations,0
...for each image,0
...def yolo_to_coco(),0
%% Interactive driver,0
%% Convert YOLO folders to COCO,0
%% Check DB integrity,0
%% Preview some images,0
%% Command-line driver,0
TODO,1
,0
read_exif.py,0
,0
"Given a folder of images, read relevant metadata (EXIF/IPTC/XMP) fields from all images,",0
and write them to  a .json or .csv file.,0
,0
This module can use either PIL (which can only reliably read EXIF data) or exiftool (which,0
can read everything).  The latter approach expects that exiftool is available on the system,0
path.  No attempt is made to be consistent in format across the two approaches.,0
,0
%% Imports and constants,0
From ai4eutils,0
%% Options,0
Number of concurrent workers,0
Should we use threads (vs. processes) for parallelization?,0
,0
Not relevant if n_workers is 1.,0
Should we use exiftool or pil?,0
%% Functions,0
exif_tags = img.info['exif'] if ('exif' in img.info) else None,0
print('Warning: unrecognized EXIF tag: {}'.format(k)),0
"-G means ""Print group name for each tag"", e.g. print:",0
,0
[File]          Bits Per Sample                 : 8,0
,0
...instead of:,0
,0
Bits Per Sample                 : 8,0
A list of three-element lists (type/tag/value),0
line_raw = exif_lines[0],0
A typical line:,0
,0
[ExifTool]      ExifTool Version Number         : 12.13,0
"Split on the first occurrence of "":""",0
...for each output line,0
...which processing library are we using?,0
...read_exif_tags_for_image(),0
...populate_exif_data(),0
Enumerate *relative* paths,0
Find all EXIF tags that exist in any image,0
...for each tag in this image,0
...for each image,0
Write header,0
...for each key that *might* be present in this image,0
...for each image,0
...with open(),0
...if we're writing to .json/.csv,0
https://stackoverflow.com/questions/11210104/check-if-a-program-exists-from-a-python-script,0
%% Interactive driver,0
%%,0
output_file = os.path.expanduser('~/data/test-exif.csv'),0
options.processing_library = 'pil',0
"file_path = os.path.join(input_folder,'KRU_S1_11_R1_IMAG0148.JPG')",0
%%,0
%% Command-line driver,0
,0
"Given a json-formatted list of image filenames, retrieve the width and height of every image.",0
,0
%% Constants and imports,0
%% Processing functions,0
Is this image on disk?,0
"all_results = list(tqdm(pool.imap(process_image, filenames), total=len(filenames)))",0
%% Interactive driver,0
%%,0
List images in a test folder,0
%%,0
"process_list_file(image_list_file,image_size_file,image_prefix=base_dir)",0
%% Command-line driver,0
"python sanity_check_json_db.py ""e:\wildlife_data\wellington_data\wellington_camera_traps.json"" --baseDir ""e:\wildlife_data\wellington_data\images"" --bFindUnusedImages --bCheckImageSizes",0
"python sanity_check_json_db.py ""D:/wildlife_data/mcgill_test/mcgill_test.json"" --baseDir ""D:/wildlife_data/mcgill_test"" --bFindUnusedImages --bCheckImageSizes",0
"Here the '-u' prevents buffering, which makes tee happier",0
,0
python -u sanity_check_json_db.py '/datadrive1/nacti_metadata.json' --baseDir '/datadrive1/nactiUnzip/' --bFindUnusedImages --bCheckImageSizes | tee ~/nactiTest.out,0
,0
Partially-formed stub to get from MegaDetector output files to COCO Camera Traps data.,0
,0
"Was actually written to convert *many* MD .json files to a single CCT file, hence",0
the loop over .json files.,0
,0
"THIS CODE HAS NEVER BEEN RUN, it was added as a demonstration of how to do this.  YMMV.",0
,0
"You may find a more polished, command-line-ready version of this code at:",0
,0
https://github.com/StewartWILDlab/mdtools,0
,0
%% Constants and imports,0
"Images sizes are required to convert between absolute and relative coordinates,",0
so we need to read the images.,0
Only required if you want to write a database preview,0
%% Create CCT dictionaries,0
image_ids_to_images = {},0
Force the empty category to be ID 0,0
Load .json annotations for this data set,0
i_entry = 0; entry = data['images'][i_entry],0
,0
"PERF: Not exactly trivially parallelizable, but about 100% of the",0
time here is spent reading image sizes (which we need to do to get from,0
"absolute to relative coordinates), so worth parallelizing.",0
Generate a unique ID from the path,0
detection = detections[0],0
Have we seen this category before?,0
Create an annotation,0
"MegaDetector: [x,y,width,height] (normalized, origin upper-left)",0
"CCT: [x,y,width,height] (absolute, origin upper-left)",0
...for each detection,0
...for each image,0
Remove non-reviewed images and associated annotations,0
%% Create info struct,0
%% Write .json output,0
%% Clean start,0
## Everything after this should work from a clean start ###,0
%% Validate output,0
%% Preview animal labels,0
%% Preview empty labels,0
"viz_options.classes_to_exclude = ['empty','human']",0
,0
generate_crops_from_cct.py,0
,0
"Given a .json file in COCO Camera Traps format, create a cropped image for",0
each bounding box.,0
,0
%% Imports and constants,0
%% Functions,0
# Read and validate input,0
# Find annotations for each image,0
"This actually maps image IDs to annotations, but only to annotations",0
containing boxes,0
# Generate crops,0
TODO: parallelize this loop,1
im = d['images'][0],0
Load the image,0
Generate crops,0
i_ann = 0; ann = annotations_this_image[i_ann],0
"x/y/w/h, origin at the upper-left",0
...for each box,0
...for each image,0
...generate_crops_from_cct(),0
%% Interactive driver,0
%%,0
%%,0
%%,0
%% Command-line driver,0
TODO,1
%% Scrap,0
%%,0
,0
coco_to_yolo.py,0
,0
Converts a COCO-formatted dataset to a YOLO-formatted dataset.,0
,0
"If the input and output folders are the same, writes .txt files to the input folder,",0
and neither moves nor modifies images.,0
,0
"Currently ignores segmentation masks, and errors if an annotation has a",0
segmentation polygon but no bbox,0
,0
Has only been tested on a handful of COCO Camera Traps data sets; if you,0
"use it for more general COCO conversion, YMMV.",0
,0
%% Imports and constants,0
%% Support functions,0
Validate input,0
Read input data,0
Parse annotations,0
i_ann = 0; ann = data['annotations'][0],0
Make sure no annotations have *only* segmentation data,0
Re-map class IDs to make sure they run from 0...n-classes-1,0
,0
"TODO: this allows unused categories in the output data set, which I *think* is OK,",1
but I'm only 81% sure.,0
Process images (everything but I/O),0
"List of dictionaries with keys 'source_image','dest_image','bboxes','dest_txt'",0
i_image = 0; im = data['images'][i_image],0
"assert os.path.isfile(source_image), 'Could not find image {}'.format(source_image)",0
If this annotation has no bounding boxes...,0
"This is not entirely clear from the COCO spec, but it seems to be consensus",0
"that if you want to specify an image with no objects, you don't include any",0
annotations for that image.,0
We allow empty bbox lists in COCO camera traps; this is typically a negative,0
"example in a dataset that has bounding boxes, and 0 is typically the empty",0
category.,0
...if this is an empty annotation,0
"COCO: [x_min, y_min, width, height] in absolute coordinates",0
"YOLO: [class, x_center, y_center, width, height] in normalized coordinates",0
Convert from COCO coordinates to YOLO coordinates,0
...for each annotation,0
...if this image has annotations,0
...for each image,0
Write output,0
Category IDs should range from 0..N-1,0
TODO: parallelize this loop,1
,0
output_info = images_to_copy[0],0
Only write an annotation file if there are bounding boxes.  Images with,0
"no .txt files are treated as hard negatives, at least by YOLOv5:",0
,0
https://github.com/ultralytics/yolov5/issues/3218,0
,0
"I think this is also true for images with empty annotation files, but",0
"I'm using the convention suggested on that issue, i.e. hard negatives",0
are expressed as images without .txt files.,0
bbox = bboxes[0],0
...for each image,0
...def coco_to_yolo(),0
%% Interactive driver,0
%% CCT data,0
"If preview_export is True, I'm exporting to preview these with BoundingBoxEditor:",0
,0
https://github.com/mfl28/BoundingBoxEditor,0
,0
"This export will be compatible, other than the fact that you need to move",0
"""object.data"" into the ""labels"" folder.",0
,0
"Otherwise I'm exporting for training, in the YOLOv5 flat format.",0
%% Command-line driver,0
TODO,1
,0
cct_to_wi.py,0
,0
Converts COCO Camera Traps .json files to the Wildlife Insights,0
batch upload format,0
,0
Also see:,0
,0
https://github.com/ConservationInternational/Wildlife-Insights----Data-Migration,0
,0
https://data.naturalsciences.org/wildlife-insights/taxonomy/search,0
,0
%% Imports,0
%% Paths,0
A COCO camera traps file with information about this dataset,0
A .json dictionary mapping common names in this dataset to dictionaries with the,0
"WI taxonomy fields: common_name, wi_taxon_id, class, orer, family, genus, species",0
%% Constants,0
%% Project information,0
%% Read templates,0
%% Compare dictionary to template lists,0
Write the header,0
Write values,0
%% Project file,0
%% Camera file,0
%% Deployment file,0
%% Images file,0
Read .json file with image information,0
Read taxonomy dictionary,0
Populate output information,0
df = pd.DataFrame(columns = images_fields),0
annotation = annotations[0],0
im = input_data['images'][0],0
"We don't have counts, but we can differentiate between zero and 1",0
This is the label mapping used for our incoming iMerit annotations,0
"Only used to parse the incoming annotations. In our database, the string name is used to avoid confusion",0
MegaDetector outputs,0
,0
add_bounding_boxes_to_megadb.py,0
,0
"Given COCO-formatted JSONs containing manually labeled bounding box annotations, add them to",0
"MegaDB sequence entries, which can then be ingested into MegaDB.",0
the category map that comes in the COCO JSONs for iMerit batch 12 - to check that each,0
JSON,0
"dataset name : (seq_id, frame_num) : [bbox, bbox]",0
where bbox is a dict with str 'category' and list 'bbox',0
iterate over image_id_to_image rather than image_id_to_annotations so we include,0
the confirmed empty images,0
The file_name field in the incoming json looks like,0
alka_squirrels.seq2020_05_07_25C.frame119221.jpg,0
there seems to be a bug in the annotations where sometimes there's a,0
non-empty label along with a label of category_id 5,0
ignore the empty label (they seem to be actually non-empty),0
,0
add_bounding_boxes_to_json.py,0
,0
This script takes a image database in the COCO Camera Traps format and merges in a set of bounding,0
box annotations in the format that iMerit uses (a .json where actually only each row is a valid json).,0
,0
"If you need to update an existing bbox database, please get all the original annotation files and",0
re-generate from scratch,0
,0
%% Imports,0
%% Configurations and paths,0
images database,0
output bboxes database,0
annotation files (pseudo json) obtained from our annotation vendor that contain annotations for this dataset,0
None or a string or tuple of strings that is the prefix to all file_name of interest / in this dataset in the annotation files,0
functions for mapping the image_id in the annotation files (pseudo jsons) to the image_id used in the image DB,0
our img_id doesn't contain frame info,0
"batch3 - ""file_name"":""ASG0000019_0_S1_B06_R1_PICT0007.JPG""",0
"batch5 and 7 - ""file_name"":""datasetsnapshotserengeti.seqASG000002m-frame0.imgS1_B06_R1_PICT0056.JPG""",0
sometimes - 'datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG',0
"id in DB (old_token): 'S6/J01/J01_R1/S6_J01_R1_IMAG0001', 'S1/B05/B05_R1/S1_B05_R1_PICT0036'",0
specify which one to use for your dataset here,0
%% Load the image database and fill in DB info for the output bbox database,0
load the images database,0
%% Find the height and width of images from the annotation files,0
,0
...if they are not available in the images DB,0
each row in this pseudo-json is a COCO formatted entry for an image sequence,0
%% Other functions required by specific datasets,0
the IDFG image database does not include images from unlabeled folders that were annotated with bounding boxes,0
%% Create the bbox database from all annotation files pertaining to this dataset,0
"for the incoming annotations, look up by category name (common) and convert to the numerical id used in our databases",0
"for each annotation pseudo-json, check that the image it refers to exists in the original database",0
each row in this pseudo-json is a COCO formatted entry for an image sequence,0
check that entry is for this dataset,0
category map for this entry in the annotation file - usually the same across all entries but just in case,0
rspb_add_image_entry(img_id),0
use the image length and width in the image DB,0
"[top left x, top left y, width, height] in relative coordinates",0
"add all images that have been sent to annotation, some of which may be empty of bounding boxes",0
rspb_add_image_entry(db_image_id),0
%% Common queries,0
This query is used when preparing tfrecords for object detector training.,0
We do not want to get the whole seq obj where at least one image has bbox because,0
some images in that sequence will not be bbox labeled so will be confusing.,0
Include images with bbox length 0 - these are confirmed empty by bbox annotators.,0
"If frame_num is not available, it will not be a field in the result iterable.",0
"Note that the seq_id is the Cosmos DB assigned ID for that sequence, not the",0
"seq_id field, which may contain ""/"" characters.",0
"For public datasets to be converted to the CCT format, we get the whole seq object because",0
sequence level attributes need to be included too. megadb/converters/megadb_to_cct.py handles,0
the case of bbox-only JSONs with the flag exclude_species_class,0
Getting all sequences in a dataset - for updating or deleting entries which need the id field,0
%% Parameters,0
Use None if querying across all partitions,0
"The `sequences` table has the `dataset` as the partition key, so if only querying",0
"entries from one dataset, set the dataset name here.",0
"e.g. {'name': '@top_n', 'value': 100} - see query_and_upsert_examples/query_for_data.ipynb",0
Use False if do not want all results stored in a single JSON.,0
%% Script,0
execute the query,0
loop through and save the results,0
MODIFY HERE depending on the query,0
wiitigers Unicode issue - no good mapping from DB file names to file names in blob URL,0
build filename,0
if need to re-download a dataset's images in case of corruption,0
entries_to_download = {,0
"filename: entry for filename, entry in entries_to_download.items()",0
if entry['dataset'] == DATASET,0
},0
input validation,0
"existing files, with paths relative to <store_dir>",0
parse JSON or TXT file,0
"create a new storage container client for this dataset,",0
and cache it,0
schema already checks that the min possible value of frame_num is 0,0
"if there are more than one image item, each needs a frame_num",0
checks across all sequence items,0
per sequence item checks,0
load the schema,0
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory,0
load the schema,0
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory,0
checks across all datasets items,0
check for expiry date of the SAS keys,0
update the sequences,0
"`id` is from the database, as well as all attributes starting with _",0
"if valuable sequence information is available, add them to the image",0
required fields for an image object,0
add seq-level class labels for this image,0
add other sequence-level properties to each image too,0
add other image-level properties,0
... for im in seq['images'],0
... for seq in mega_db,0
consolidate categories,0
some property names have changed in the new schema,0
a dummy sequence ID will be generated if the image entry does not have a seq_id field,0
seq_id only needs to be unique within this dataset; MegaDB does not rely on it as the _id field,0
"""annotations"" fields are opened and have its sub-field surfaced one level up",0
set the `dataset` property on each sequence to the provided dataset_name,0
check that the location field is the same for all images in a sequence,0
check which fields in a CCT image entry are sequence-level,0
image-level properties that really should be sequence-level,0
need to add (misidentified) seq properties not present for each image in a sequence to img_level_properties,0
"(some properties act like flags - all have the same value, but not present on each img)",0
add the sequence-level properties to the sequence objects,0
not every sequence have to have all the seq_level_properties,0
get the value of this sequence-level property from the first image entry,0
check which fields are really dataset-level and should be included in the dataset table instead.,0
delete sequence-level properties that should be dataset-level,0
make all `class` fields lower-case; cast `seq_id` to type string in case they're integers,0
turn all float NaN values into None so it gets converted to null when serialized,0
this was an issue in the Snapshot Safari datasets,0
%% validation,0
"at first a dict of image_id: image_obj with annotations embedded, then it becomes",0
an array of image objects,0
%% integrate the image DB,0
takes in image entries and species and other annotations in the image DB,0
convert the species category to explicit string name,0
there may be other fields in the annotation object,0
these fields should already be gotten from the image object,0
%% integrate the bbox DB,0
add any images that are not in the image DB,0
also add any fields in the image object that are not present already,0
add bbox to the annotations field,0
for any newly added images,0
"'bbox_abs': bbox_anno['bbox'],",0
not keeping height and width,0
,0
jb_csv_to_json.py,0
,0
Convert a particular .csv file to CCT format.  Images were not available at,0
"the time I wrote this script, so this is much shorter than other scripts",0
in this folder.,0
,0
%% Constants and environment,0
%% Read source data,0
%% Confirm filename uniqueness (this data set has one label per image),0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
%% Create CCT dictionaries,0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
imageName = imageFilenames[0],0
"As per above, this is convenient and appears to be true; asserting to be safe",0
"Filenames look like ""290716114012001a1116.jpg""",0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
%% Create info struct,0
%% Write output,0
%% Sanity-check,0
%% Imports and constants,0
%% Load data,0
"with open(image_json,'r') as f:",0
data = json.load(f),0
%% Sanity-check data,0
%% Label previews,0
%% Collect images to annotate,0
%% Sort by sequence and frame,0
%% Copy to a folder by GUID,0
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension,0
im = images_to_annotate[0],0
...for each image,0
%% Write out the annotation list,0
%% Constants and imports,0
%% Enumerate files,0
edited_image_folder = edited_image_folders[0],0
fn = edited_image_files[0],0
%% Read metadata and capture location information,0
i_row = 0; row = df.iloc[i_row],0
Sometimes '2017' was just '17' in the date column,0
%% Read the .json files and build output dictionaries,0
json_fn = json_files[0],0
if 'partial' in json_fn:,0
continue,0
line = lines[0],0
if image_fn == 'SD1_238_6_26_17_16_76.73.jpg':,0
asdfad,0
SD29_079_5_14_2018_17_52.85.jpg,0
Re-write two-digit years as four-digit years,0
Sometimes the year was written with two digits instead of 4,0
assert len(tokens[4]) == 4 and tokens[4].startswith('20'),0
Have we seen this location already?,0
"Not a typo, it's actually ""formateddata""",0
An image shouldn't be annotated as both empty and non-empty,0
An image shouldn't be annotated as both empty and non-empty,0
box = formatteddata[0],0
"open_file(os.path.join(base_folder,jpeg_image_folder,image_fn))",0
...for each box,0
...if there are boxes on this image,0
...for each line,0
...with open(),0
...for each json file,0
%% Prepare the output .json,0
%% Check DB integrity,0
%% Print unique locations,0
SD12_202_6_23_2017_1_31.85.jpg,0
%% Preview some images,0
%% Statistics,0
,0
"Import a Snapshot Safari project (one project, one season)",0
,0
Before running this script:,0
,0
"* Mount the blob container where the images live, or copy the",0
images to local storage,0
,0
What this script does:,0
,0
* Creates a .json file,0
* Creates zip archives of the season without humans.,0
* Copies animals and humans to separate folders,0
,0
After running this script:,0
,0
* Create or update LILA page,0
* Push zipfile and unzipped images to LILA,0
* Push unzipped humans to wildlifeblobssc,0
* Delete images from UMN uplaod storage,0
,0
"Snapshot Serengeti is handled specially, because we're dealing with bounding",0
boxes too.  See snapshot_serengeti_lila.py.,0
,0
%% Imports,0
From ai4eutils,0
From CameraTraps,0
%% Constants,0
project_name = 'XXX'; season_name = 'S1'; project_friendly_name = 'Snapshot Unknown',0
project_name = 'SER'; season_name = 'S1-11'; project_friendly_name = 'Snapshot Serengeti',0
project_name = 'KRU'; season_name = 'S1'; project_friendly_name = 'Snapshot Kruger',0
project_name = 'CDB'; season_name = 'S1'; project_friendly_name = 'Snapshot Camdeboo',0
project_name = 'MTZ'; season_name = 'S1'; project_friendly_name = 'Snapshot Mountain Zebra',0
project_name = 'ENO'; season_name = 'S1'; project_friendly_name = 'Snapshot Enonkishu',0
project_name = 'KAR'; season_name = 'S1'; project_friendly_name = 'Snapshot Karoo',0
project_name = 'KGA'; season_name = 'S1'; project_friendly_name = 'Snapshot Kgalagadi',0
%% Folder/file creation,0
E.g. KRU_S1,0
E.g. Z:\KRU,0
E.g. Z:\KRU\KRU_S1,0
Contains annotations for each capture event (sequence),0
Maps image IDs to filenames; each line looks like:,0
,0
"KRU_S1#1#1#2,3,KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0004.JPG",0
"Total number of each answer to each question, e.g. total number of times each species was identified",0
,0
Not used here,1
Create output folders,0
Images will be placed in a season-specific folder inside this (the source data includes,0
this in path names),0
%% Load metadata files,0
%% Convert to dictionaries (prep),0
%% Convert to dictionaries (loops),0
"TODO: iterrows() is a terrible way to do this, but this is one of those days",1
"where I want to get this done, not get better at Python.",0
irow = 0; row = image_table.iloc[0],0
"Loaded as an int64, converting to int here",0
...for each row in the image table,0
Make sure image IDs are what we think they are,0
...for each row in the annotation table,0
%% Take a look at categories (just sanity-checking),0
print('\nCategories by species:'),0
pp.pprint(categories_by_species),0
%% Fill in some image fields we didn't have when we created the image table,0
"width, height, corrupt, seq_num_frames, location, datetime",0
Every annotation in this list should have the same sequence ID,0
"Figure out ""seq_num_frames"", which really should be done in a separate lopp;",0
there's no reason to do this redundantly for every image,0
Every image in this sequence should point back to the same equence,0
Every annotation in this list should have the same location,0
Every annotation in this list should have the same datetime,0
Is this image on disk?,0
iImage = 0; im = images[0],0
...for each image,0
"images_processed = pool.map(process_image, images)",0
"images_processed = list(tqdm(pool.imap_unordered(process_image, images), total=len(images)))",0
%% Count missing/corrupted images,0
%% Print distribution of sequence lengths (sanity-check),0
%% Replicate annotations across images,0
iAnn = 0; ann = annotations[iAnn],0
%% See what files are on disk but not annotated,0
%% Sanity-check image and annotation uniqueness,0
%% Minor updates to fields,0
%% Write .json file,0
%% Create a list of human files,0
ann = annotations[0],0
%% Create public archive and public/private folders,0
im = images[0],0
E.g. KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0001.JPG,0
Copy to private output folder,0
Add to zipfile,0
Possibly start a new archive,0
Copy to public output folder,0
...for each image,0
%% Sanity-check .json file,0
"This will produce some validation errors, because this zipfile doesn't include humans",0
%% Zip up .json and .csv files,0
%% When I skip to this part (using a pre-rendered .json file),0
%%,0
%%,0
ann = annotations[0],0
%% Summary prep for LILA,0
"%% Generate preview, sanity-check labels",0
"viz_options.classes_to_include = ['jackalblackbacked','bustardkori']",0
%% Scrap,0
%% Find annotations for a particular image,0
%% Write a list of missing images,0
,0
save_the_elephants_survey_A.py,0
,0
Convert the .csv file provided for the Save the Elephants Survey A data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
Handle all unstructured fields in the source data as extra fields in the annotations,0
"photo_type really should be an image property, but there are a few conflicts",0
that forced me to handle it as an annotation proprerty,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
Maps relative paths to row indices in input_metadata,0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
Ignore directories,0
%% Make sure the multiple-annotation cases make sense,0
%%,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
Force the empty category to be ID 0,0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image],0
Example filename:,0
,0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG',0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG',0
i_row = row_indices[0],0
There are a small handful of datetime mismatches across annotations,0
for the same image,0
assert im['datetime'] == timestamp,0
Special cases based on the 'photo type' field,0
Various spellings of 'community',0
Have we seen this category before?,0
Create an annotation,0
fieldname = list(mapped_fields.keys())[0],0
...for each row,0
...for each image,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
%% Scrap,0
%% Find unique photo types,0
##,0
,0
timelapse_csv_set_to_json.py,0
,0
Given a directory full of reasonably-consistent Timelapse-exported,0
".csvs, assemble a CCT .json.",0
,0
"Assumes that you have a list of all files in the directory tree, including",0
image and .csv files.,0
,0
##,0
%% Constants and imports,0
Text file with relative paths to all files (images and .csv files),0
"%% Read file list, make a list of all image files and all .csv files",0
"%% Verify column consistency, create a giant array with all rows from all .csv files",0
i_csv = 0; csv_filename = csv_files[0],0
%% Prepare some data structures we'll need for mapping image rows in .csv files to actual image files,0
Enumerate all folders containing image files,0
"In this data set, a site folder looks like:",0
,0
Processed Images\\site_name,0
%% Map .csv files to candidate camera folders,0
fn = valid_csv_files[0],0
"Some site folders appear as ""XXNNNN"", some appear as ""XXNNNN_complete""",0
...for each .csv file,0
%% Map camera folders to candidate image folders,0
%% Main loop over labels (prep),0
Force the empty category to be ID 0,0
Images that are marked empty and also have a species label,0
%% Main loop over labels (loop),0
i_row = 0; row = input_metadata.iloc[i_row],0
"for i_row,row in input_metadata.iterrows():",0
"Usually this is just a single folder name, sometimes it's a full path,",0
which we don't want,0
Check whether this file exists on disk,0
Retrieve image width and height,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each row in the big table of concatenated .csv files,0
%% Fix cases where an image was annotated as 'unlabeled' and as something else,0
This annotation is 'unlabeled',0
Was there another category associated with this image?,0
%% Check for un-annnotated images,0
Enumerate all images,0
list(relative_path_to_image.keys())[0],0
%% Create info struct,0
%% Write output,0
%% Sanity-check the database's integrity,0
%% Render a bunch of images to make sure the labels got carried along correctly,0
,0
auckland_doc_to_json.py,0
,0
Convert Auckland DOC data set to COCO camera traps format.  This was,0
for a training data set where class names were encoded in path names.,0
,0
%% Constants and imports,0
Filenames will be stored in the output .json relative to this base dir,0
%% Enumerate files,0
%% Assemble dictionaries,0
Force the empty category to be ID 0,0
fn = image_files[0]; print(fn),0
Typically y:\Maukahuka_Auckland_Island\1_Training\Winter_Trial_2019\cat\cat\eat\20190903_IDdY_34_E3_tmp_201908240051.JPG,0
"This data set has two top-level folders, ""1_Training"" (which has class names encoded",0
"in paths) and ""2_Testing"" (which has no class information).",0
...for each image,0
%% Write output .json,0
%% Write train/test .jsons,0
%% Validate .json files,0
%% Preview labels,0
checkpoint,0
,0
awc_to_json.py,0
,0
Convert a particular .csv file to CCT format.,0
,0
%% Constants and environment,0
%% Read source data,0
%% Main loop over labels,0
Force the empty category to be ID 0,0
iRow = 0; row = input_metadata.iloc[iRow],0
"ImageID,FileName,FilePath,SpeciesID,CommonName",0
"This makes an assumption of one annotation per image, which happens to be",0
true in this data set.,0
Retrieve image width and height,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
list(relativePathToImage.keys())[0],0
%% Create info struct,0
%% Write output,0
%% Sanity-check the database's integrity,0
%% Render a bunch of images to make sure the labels got carried along correctly,0
,0
idaho-camera-traps.py,0
,0
Prepare the Idaho Camera Traps dataset for release on LILA.,0
,0
%% Imports and constants,0
Multi-threading for .csv file comparison and image existence validation,0
"We are going to map the original filenames/locations to obfuscated strings, but once",0
"we've done that, we will re-use the mappings every time we run this script.",0
This is the file to which mappings get saved,0
The maximum time (in seconds) between images within which two images are considered the,0
same sequence.,0
"This is a two-column file, where each line is [string in the original metadata],[category name we want to map it to]",0
"The output file, using the original strings",0
"The output file, using obfuscated strings for everything but filenamed",0
"The output file, using obfuscated strings and obfuscated filenames",0
"One time only, I ran MegaDetector on the whole dataset...",0
...then set aside any images that *may* have contained humans that had not already been,0
annotated as such.  Those went in this folder...,0
...and the ones that *actually* had humans (identified via manual review) got,0
copied to this folder...,0
"...which was enumerated to this text file, which is a manually-curated list of",0
images that were flagged as human.,0
Unopinionated .json conversion of the .csv metadata,0
%% List files (images + .csv),0
Ignore .csv files in folders with multiple .csv files,0
...which would require some extra work to decipher.,0
fn = csv_files[0],0
%% Parse each .csv file into sequences (function),0
csv_file = csv_files[-1],0
os.startfile(csv_file_absolute),0
survey = csv_file.split('\\')[0],0
Sample paths from which we need to derive locations:,0
,0
St.Joe_elk\AM99\Trip 1\100RECNX\TimelapseData.csv,0
Beaverhead_elk\AM34\Trip 1\100RECNX\TimelapseData.csv,0
,0
ClearCreek_mustelids\Winter2015-16\FS-001-P\FS-001-P.csv,0
ClearCreek_mustelids\Summer2015\FS-001\FS-001.csv,0
ClearCreek_mustelids\Summer2016\IDFG-016\IDFG-016.csv,0
,0
I:\idfg-images\ClearCreek_mustelids\Summer2016\IDFG-017b,0
I:\idfg-images\ClearCreek_mustelids\Summer2016\IDFG-017a,0
Load .csv file,0
Validate the opstate column,0
# Create datetimes,0
print('Creating datetimes'),0
i_row = 0; row = df.iloc[i_row],0
Make sure data are sorted chronologically,0
,0
"In odd circumstances, they are not... so sort them first, but warn",0
Debugging when I was trying to see what was up with the unsorted dates,0
# Parse into sequences,0
print('Creating sequences'),0
i_row = 0; row = df.iloc[i_row],0
Start a new sequence if:,0
,0
* This image has no timestamp,0
* This image has a frame number of zero,0
* We have no previous image timestamp,0
,0
Start a new sequence if necessary,0
...for each row,0
# Parse labels for each sequence,0
sequence_id = location_sequences[0],0
Row indices in a sequence should be adjacent,0
sequence_df = df[df['seq_id']==sequence_id],0
# Determine what's present,0
Be conservative; assume humans are present in all maintenance images,0
The presence columns are *almost* always identical for all images in a sequence,0
assert single_presence_value,0
print('Warning: presence value for {} is inconsistent for {}'.format(,0
"presence_column,sequence_id))",0
...for each presence column,0
Tally up the standard (survey) species,0
"If no presence columns are marked, all counts should be zero",0
count_column = count_columns[0],0
Occasionally a count gets entered (correctly) without the presence column being marked,0
"assert len(values) == 1 and values[0] == 0, 'Non-zero counts with no presence",0
columns marked for sequence {}'.format(sequence_id),0
"Handle this by virtually checking the ""right"" box",0
Make sure we found a match,0
Handle 'other' tags,0
column_name = otherpresent_columns[0],0
print('Found non-survey counted species column: {}'.format(column_name)),0
...for each non-empty presence column,0
"Very rarely, the ""otherpresent"" column is checked, but no more detail is available",0
...handling non-survey species,0
Build the sequence data,0
i_row = 0; row = sequence_df.iloc[i_row],0
Only one folder used a single .csv file for two subfolders,0
...for each sequence,0
...def csv_to_sequences(),0
%% Parse each .csv file into sequences (loop),0
%%,0
%%,0
i_file = -1; csv_file = csv_files[i_file],0
%% Save sequence data,0
%% Load sequence data,0
%%,0
%% Validate file mapping (based on the existing enumeration),0
sequences = sequences_by_file[0],0
sequence = sequences[0],0
"Actually, one folder has relative paths",0
assert '\\' not in image_file_relative and '/' not in image_file_relative,0
os.startfile(csv_folder),0
assert os.path.isfile(image_file_absolute),0
found_file = os.path.isfile(image_file_absolute),0
...for each image,0
...for each sequence,0
...for each .csv file,0
%% Load manual category mappings,0
The second column is blank when the first column already represents the category name,0
%% Convert to CCT .json (original strings),0
Force the empty category to be ID 0,0
For each .csv file...,0
,0
sequences = sequences_by_file[0],0
For each sequence...,0
,0
sequence = sequences[0],0
Find categories for this image,0
"When 'unknown' is used in combination with another label, use that",0
"label; the ""unknown"" here doesn't mean ""another unknown species"", it means",0
there is some other unknown property about the main species.,0
category_name_string = species_present[0],0
"This piece of text had a lot of complicated syntax in it, and it would have",0
been too complicated to handle in a general way,0
print('Ignoring category {}'.format(category_name_string)),0
Don't process redundant labels,0
category_name = category_names[0],0
If we've seen this category before...,0
If this is a new category...,0
print('Adding new category for {}'.format(category_name)),0
...for each category (inner),0
...for each category (outer),0
...if we do/don't have species in this sequence,0
"We should have at least one category assigned (which may be ""empty"" or ""unknown"")",0
assert len(sequence_category_ids) > 0,0
Was any image in this sequence manually flagged as human?,0
print('Flagging sequence {} as human based on manual review'.format(sequence_id)),0
For each image in this sequence...,0
,0
i_image = 0; im = images[i_image],0
Create annotations for this image,0
...for each image in this sequence,0
...for each sequence,0
...for each .csv file,0
Verify that all images have annotations,0
ann = ict_data['annotations'][0],0
For debugging only,0
%% Create output (original strings),0
%% Validate .json file,0
%% Preview labels,0
%% Look for humans that were found by MegaDetector that haven't already been identified as human,0
This whole step only needed to get run once,0
%%,0
Load MD results,0
Get a list of filenames that MD tagged as human,0
im = md_results['images'][0],0
...for each detection,0
...for each image,0
Map images to annotations in ICT,0
ann = ict_data['annotations'][0],0
For every image,0
im = ict_data['images'][0],0
Does this image already have a human annotation?,0
...for each annotation,0
...for each image,0
%% Copy images for review to a new folder,0
fn = missing_human_images[0],0
%% Manual step...,0
Copy any images from that list that have humans in them to...,0
%% Create a list of the images we just manually flagged,0
fn = human_tagged_filenames[0],0
E.g. '0000_Beaverhead_elk~AM174~Trip 1~100RECNX~IMG_1397.JPG',0
"%% Translate location, image, sequence IDs",0
Load mappings if available,0
Generate mappings,0
If we've seen this location before...,0
Otherwise assign a string-formatted int as the ID,0
If we've seen this sequence before...,0
Otherwise assign a string-formatted int as the ID,0
Assign an image ID,0
...for each image,0
Assign annotation mappings,0
Save mappings,0
"Back this file up, lest we should accidentally re-run this script",0
with force_generate_mappings = True and overwrite the mappings we used.,0
...if we are/aren't re-generating mappings,0
%% Apply mappings,0
"%% Write new dictionaries (modified strings, original files)",0
"%% Validate .json file (modified strings, original files)",0
%% Preview labels (original files),0
"viz_options.classes_to_exclude = ['empty','deer','elk']",0
viz_options.classes_to_include = ['bobcat'],0
%% Copy images to final output folder (prep),0
ann = d['annotations'][0],0
Is this a public or private image?,0
Generate absolute path,0
Copy to output,0
Update the filename reference,0
...def process_image(im),0
%% Copy images to final output folder (execution),0
For each image,0
im = images[0],0
Write output .json,0
%% Make sure the right number of images got there,0
%% Validate .json file (final filenames),0
%% Preview labels (final filenames),0
"viz_options.classes_to_exclude = ['empty','deer','elk']",0
viz_options.classes_to_include = ['horse'],0
viz_options.classes_to_include = [viz_options.multiple_categories_tag],0
"viz_options.classes_to_include = ['human','vehicle','domestic dog']",0
%% Create zipfiles,0
%% List public files,0
%% Find the size of each file,0
fn = all_public_output_files[0],0
%% Split into chunks of approximately-equal size,0
...for each file,0
%% Create a zipfile for each chunk,0
...for each filename,0
with ZipFile(),0
...def create_zipfile(),0
i_file_list = 0; file_list = file_lists[i_file_list],0
"....if __name__ == ""__main__""",0
,0
wellington_to_json.py,0
,0
Convert the .csv file provided for the Wellington data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
%% Read source data,0
"Filenames were provided as *.jpg, but images were *.JPG, converting here",0
"%% Map filenames to rows, verify image existence",0
"Takes ~30 seconds, since it's checking the existence of ~270k images",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
"I didn't expect this to be true a priori, but it appears to be true, and",0
it saves us the trouble of checking consistency across multiple occurrences,0
of an image.,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
imageName = imageFilenames[0],0
"As per above, this is convenient and appears to be true; asserting to be safe",0
"Filenames look like ""290716114012001a1116.jpg""",0
This gets imported as an int64,0
"These appear as ""image1"", ""image2"", etc.",0
"In the form ""001a""",0
Can be in the form '111' or 's46',0
"In the form ""7/29/2016 11:40""",0
Check image height and width,0
"Use 'empty', to be consistent with other data on lila",0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
,0
mcgill_to_json.py,0
,0
Convert the .csv file provided for the McGill test data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
Create an additional column for concatenated filenames,0
Maps relative filenames to rows,0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
row = input_metadata.iloc[0],0
"I didn't expect this to be true a priori, but it appears to be true, and",0
it saves us the trouble of checking consistency across multiple occurrences,0
of an image.,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
row = input_metadata.iloc[0],0
"Filenames look like ""290716114012001a1116.jpg""",0
"In the form ""001a""",0
Can be in the form '111' or 's46',0
"In the form ""7/29/2016 11:40""",0
Check image height and width,0
NaN is the only thing we should see that's not a string,0
NaN is the only thing we should see that's not a string,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
,0
bellevue_to_json.py,0
,0
"""Bellevue Camera Traps"" is the rather unremarkable camera trap data set",0
used by one of the repo's maintainers for testing.  It's organized as:,0
,0
approximate_date/[loose_camera_specifier/]/species,0
,0
E.g.:,0
,0
"""2018.03.30\coyote\DSCF0091.JPG""",0
"""2018.07.18\oldcam\empty\DSCF0001.JPG""",0
,0
%% Constants and imports,0
from the ai4eutils repo,0
Filenames will be stored in the output .json relative to this base dir,0
%% Exif functions,0
"%% Enumerate files, create image/annotation/category info",0
Force the empty category to be ID 0,0
Keep track of unique camera folders,0
Each element will be a dictionary with fields:,0
,0
"relative_path, width, height, datetime",0
fname = image_files[0],0
Corrupt or not an image,0
Store file info,0
E.g. 2018.03.30/coyote/DSCF0091.JPG,0
...for each image file,0
%% Synthesize sequence information,0
Sort images by time within each folder,0
camera_path = camera_folders[0],0
previous_datetime = sorted_images_this_camera[0]['datetime'],0
im = sorted_images_this_camera[1],0
Start a new sequence if necessary,0
...for each image in this camera,0
...for each camera,0
Fill in seq_num_frames,0
%% A little cleanup,0
%% Write output .json,0
%% Sanity-check data,0
%% Label previews,0
,0
snapshot_safar_importer_reprise.py,0
,0
This is a 2023 update to snapshot_safari_importer.py.  We do a bunch of things now that,0
we didn't do the last time we imported Snapshot data (like updating the big taxonomy),0
"file, and we skip a bunch of things now that we used to do (like generating massive",0
"zipfiles).  So, new year, new importer.",0
,0
%% Constants and imports,0
%% List files,0
"Do a one-time enumeration of the entire drive; this will take a long time,",0
but will save a lot of hassle later.,0
%% Create derived lists,0
Takes about 60 seconds,0
CSV files are one of:,0
,0
"_report_lila.csv (this is the one we want to use, with the species/count/etc. for each sequence)",0
_report_lila_image_inventory.csv (maps captures to images),0
_report_lila_overview.csv (distrubution of species),0
%% List project folders,0
Project folders look like one of these:,0
,0
APN,0
Snapshot Cameo/DEB,0
%% Map report and inventory files to codes,0
fn = csv_files[0],0
%% Make sure that every report has a corresponding inventory file,0
%% Count species based on overview and report files,0
%% Print counts,0
%% Make sure that capture IDs in the reports/inventory files match,0
...and that all the images in the inventory tables are actually present on disk.,0
assert image_path_relative in all_files_relative_set,0
Make sure this isn't just a case issue,0
...for each report on this project,0
...for each project,0
"%% For all the files we have on disk, see which are and aren't in the inventory files",0
"There aren't any capital-P .PNG files, but if I don't include that",0
"in this list, I'll look at this in a year and wonder whether I forgot",0
to include it.,0
fn = all_files_relative[0],0
print('Skipping project {}'.format(project_code)),0
,0
plot_wni_giraffes.py,0
,0
Plot keypoints on a random sample of images from the wni-giraffes data set.,0
,0
%% Constants and imports,0
%% Load and select data,0
%% Support functions,0
https://stackoverflow.com/questions/32504246/draw-ellipse-in-python-pil-with-line-thickness,0
Use a single channel image (mode='L') as mask.,0
The size of the mask can be increased relative to the imput image,0
to get smoother looking results.,0
draw outer shape in white (color) and inner shape in black (transparent),0
downsample the mask using PIL.Image.LANCZOS,0
(a high-quality downsampling filter).,0
paste outline color to input image through the mask,0
%% Plot some images,0
ann = annotations_to_plot[0],0
i_tool = 0; tool_name = short_tool_names[i_tool],0
Don't plot tools that don't have a consensus annotation,0
...for each tool,0
...for each annotation,0
,0
idfg_iwildcam_lila_prep.py,0
,0
Adding class labels (from the private test .csv) to the iWildCam 2019 IDFG,0
"test set, in preparation for release on LILA.",0
,0
This version works with the public iWildCam release images.,0
,0
"%% ############ Take one, from iWildCam .json files ############",0
%% Imports and constants,0
%% Read input files,0
Remove the header line,0
%% Parse annotations,0
Lines look like:,0
,0
"b005e5b2-2c0b-11e9-bcad-06f1011196c4,1,Private",0
%% Minor cleanup re: images,0
%% Create annotations,0
%% Prepare info,0
%% Minor adjustments to categories,0
Remove unused categories,0
Name adjustments,0
%% Create output,0
%% Write output,0
%% Validate .json file,0
%% Preview labels,0
viz_options.classes_to_exclude = ['test'],0
"%% ############ Take two, from pre-iWildCam .json files created from IDFG .csv files ############",0
%% Imports and constants,0
%% One-time line break addition,0
%% Read input files,0
%% Prepare info,0
%% Minor adjustments to categories,0
%% Minor adjustments to annotations,0
%% Create output,0
%% Write output,0
%% Validate .json file,0
%% Preview labels,0
viz_options.classes_to_exclude = ['test'],0
,0
missouri_to_json.py,0
,0
Create .json files from the original source files for the Missouri Camera Traps,0
data set.  Metadata was provided here in two formats:,0
,0
"1) In one subset of the data, folder names indicated species names.  In Set 1,",0
there are no empty sequences.  Set 1 has a metadata file to indicate image-level,0
bounding boxes.,0
,0
2) A subset of the data (overlapping with (1)) was annotated with bounding,0
"boxes, specified in a whitespace-delimited text file.  In set 2, there are",0
"some sequences omitted from the metadata file, which implied emptiness.",0
,0
"In the end, set 2 labels were not reliable enough to publish, so LILA includes only set 1.",0
,0
%% Constants and imports,0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Filenames will be stored in the output .json relative to this base dir,0
This will be a list of filenames that need re-annotation due to redundant boxes,0
"%% Enumerate files, read image sizes (both sets)",0
"Takes a few minutes, since we're reading image sizes.",0
Each element will be a list of relative path/full path/width/height,0
"Only process leaf nodes corresponding to sequences, which look like:",0
,0
E:\wildlife_data\missouri_camera_traps\Set1\1.02-Agouti\SEQ75583,0
E:\wildlife_data\missouri_camera_traps\Set2\p1d101,0
,0
assert len(files) <= 2,0
Read the image,0
Not an image...,0
Store file info,0
"...if we didn't hit the max file limit, keep going",0
...for each file,0
%% Add sequence lengths (both sets),0
%% Load the set 1 metadata file,0
"List of lists, length varies according to number of bounding boxes",0
,0
Preserves original ordering,0
iLine = 0; line = metadataSet1Lines[0],0
"Lines should be filename, number of bounding boxes, boxes (four values per box)",0
Make sure we have image info for this image,0
%% Print missing files from Set 1 metadata,0
Manual changes I made to the metadata file:,0
,0
'IMG' --> 'IMG_',0
Red_Brocket_Deer --> Red_Deer,0
European-Hare --> European_Hare,0
Wood-Mouse --> Wood_Mouse,0
Coiban-Agouti --> Coiban_Agouti,0
%% Load the set 2 metadata file,0
"This metadata file contains most (but not all) images, and a class label (person/animal/empty)",0
"for each, plus bounding boxes.",0
"List of lists, length varies according to number of bounding boxes",0
,0
Preserves original ordering,0
"Create class IDs for each *sequence*, which we'll use to attach classes to",0
images for which we don't have metadata,0
,0
This only contains mappings for sequences that appear in the metadata.,0
iLine = 0; line = metadataSet2Lines[0],0
"Lines should be filename, number of bounding boxes, labeled boxes (five values per box)",0
,0
Empty images look like filename\t0\t0,0
E.g. 'Set2\\p1d101\\p1d101s100i10.JPG',0
Make sure we don't have mixed classes within an image,0
"Figure out what class this *sequence* is, so we know how to handle unlabeled",0
images from this sequence,0
Can't un-do a mixed sequence,0
Previously-empty sequences get the image class label,0
"If the sequence has a non-empty class, possibly change it",0
Make sure we have image info for this image,0
...for each line in the set 2 metadata file,0
%% What Set 2 images do I not have metadata for?,0
These are *mostly* empty images,0
iImage = 0; imageID = set2ImageIDs[iImage],0
%% Create categories and annotations for set 1,0
"Though we have no empty sequences, we do have empty images in this set",0
For each image,0
,0
iImage = 0; imageID = set1ImageIDs[iImage],0
E.g. Set1\\1.80-Coiban_Agouti\\SEQ83155\\SEQ83155_IMG_0010.JPG,0
Find the species name,0
This image may still be empty...,0
category['count'] = category['count'] + 1,0
"If we have bounding boxes, create image-level annotations",0
"filename, number of bounding boxes, boxes (four values per box)",0
"Make sure the relative filename matches, allowing for the fact that",0
some of the filenames in the metadata aren't quite right,0
This image is non-empty,0
"Some redundant bounding boxes crept in, don't add them twice",0
Check this bbox against previous bboxes,0
,0
Inefficient?  Yes.  In an important way?  No.,0
"Bounding box values are in absolute coordinates, with the origin",0
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1].",0
,0
"Convert to floats and to x/y/w/h, as per CCT standard",0
...for each box,0
if we do/don't have boxes for this image,0
Else create a sequence-level annotation,0
...for each image,0
%% Write out the list of images with redundant boxes,0
%% Create categories and annotations for set 2,0
For each image,0
,0
iImage = 0; imageID = set2ImageIDs[iImage],0
E.g. 'Set2\\p1d100\\p1d100s10i1.JPG',0
"Find the sequence ID, sanity check filename against what we stored",0
"If we have bounding boxes or an explicit empty label, create image-level annotations",0
"filename, number of bounding boxes, labeled boxes (five values per box)",0
"Make sure the relative filename matches, allowing for the fact that",0
some of the filenames in the metadata aren't quite right,0
"Bounding box values are in absolute coordinates, with the origin",0
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1].",0
,0
Convert to floats and to x/y/w/h,0
...for each box,0
...if we do/don't have boxes for this image,0
Else create a sequence-level annotation,0
...for each image,0
"%% The 'count' field isn't really meaningful, delete it",0
"It's really the count of image-level annotations, not total images assigned to a class",0
%% Write output .json files,0
%% Sanity-check final set 1 .json file,0
"python sanity_check_json_db.py --bCheckImageSizes --baseDir ""E:\wildlife_data\missouri_camera_traps"" ""E:\wildlife_data\missouri_camera_traps\missouri_camera_traps_set1.json""",0
%% Generate previews,0
,0
filenames_to_json.py,0
,0
Take a directory of images in which species labels are encoded by folder,0
"names, and produces a COCO-style .json file",0
,0
%% Constants and imports,0
from the ai4eutils repo,0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Filenames will be stored in the output .json relative to this base dir,0
"rawClassListFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_classes.csv')",0
"classMappingsFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_class_mapping.csv')",0
"%% Enumerate files, read image sizes",0
Each element will be a list of relative path/full path/width/height,0
Read the image,0
Corrupt or not an image,0
Store file info,0
Write to output file,0
...for each image file,0
...csv file output,0
...if the file list is/isn't available,0
%% Enumerate classes,0
Maps classes to counts,0
We like 'empty' to be class 0,0
%% Assemble dictionaries,0
...for each category,0
Each element is a list of relative path/full path/width/height/className,0
...for each image,0
%% External class mapping,0
%% Write raw class table,0
cat = categories[0],0
%% Read the mapped class table,0
"id, source, count, target",0
"%% Make classMappings contain *all* classes, not just remapped classes",0
cat = categories[0],0
%% Create new class list,0
"Start at 1, explicitly assign 0 to ""empty""",0
One-off issue with character encoding,0
%% Re-map annotations,0
ann = annotations[0],0
%% Write output .json,0
%% Utilities,0
%%,0
Find images with a particular tag,0
%% Randomly sample annotations,0
,0
umn_to_json.py,0
,0
Prepare images and metadata for the Orinoqua Camera Traps dataset.,0
,0
%% Imports and constants,0
"For two deployments, we're only processing imagse in the ""detections"" subfolder",0
%% Enumerate deployment folders,0
%% Load ground truth,0
i_row = 0; row = ground_truth_df.iloc[i_row],0
%% Create relative paths for ground truth data,0
"Some deployment folders have no subfolders, e.g. this is a valid file name:",0
,0
M00/01010132.JPG,0
,0
"But some deployment folders have subfolders, e.g. this is also a valid file name:",0
,0
N17/100EK113/07160020.JPG,0
,0
"So we can't find files by just concatenating folder and file names, we have to enumerate and explicitly",0
"map what will appear in the ground truth as ""folder/filename"" to complete relative paths.",0
deployment_name = list(deployment_folders)[0],0
Enumerate all files in this folder,0
"print('Enumerated {} files for deployment {}'.format(len(files),deployment_name))",0
filename = files[100],0
...for each file in this deployment,0
...for each deployment,0
%% Add relative paths to our ground truth table,0
i_row = 0; row = ground_truth_df.iloc[i_row],0
"row['filename'] looks like, e.g. A01/01080001.JPG.  This is not actually a path, it's",0
"just the deployment ID and the image name, separated by a slash.",0
Find the relative path for this image,0
Make sure this image file exists,0
...for each row in the ground truth table,0
%% Take everything out of Pandas,0
%% Convert string timestamps to Python datetimes,0
im = ground_truth_dicts[0],0
"Filenames look like, e.g., N36/100EK113/06040726.JPG",0
%% Synthesize sequence information,0
Sort images by time within each location,0
i_location=0; location = locations[i_location],0
previous_datetime = sorted_images_this_location[0]['datetime'],0
im = sorted_images_this_camera[1],0
"Timestamp for this image, may be None",0
Start a new sequence if:,0
,0
* This image has no timestamp,0
* This iamge has a frame number of zero,0
* We have no previous image timestamp,0
,0
Start a new sequence if necessary,0
...for each image in this location,0
...for each location,0
%% Create category dict and category IDs,0
im = images[0],0
%% Imports and constants (.json generation),0
%% Count frames in each sequence,0
%% Double check images with multiple annotations,0
im = images[0],0
%% Assemble dictionaries,0
Force the empty category to be ID 0,0
input_im = images[0],0
...for each image,0
%% Write output .json,0
%% Validate .json file,0
%% Map relative paths to annotation categories,0
ann = data['annotations'][0],0
%% Copy images to output,0
EXCLUDE HUMAN AND MISSING,0
im = data['images'][0],0
im = images[0],0
%% Preview labels,0
viz_options.classes_to_exclude = ['test'],0
,0
nacti_fieldname_adjustments.py,0
,0
"NACTI metadata was posted with ""filename"" in images instead of ""file_name"", and",0
"used string (rather than int) category IDs (in categories, but not in annotations).",0
,0
This script fixes those issues and rev's the version number.,0
,0
%% Constants and environment,0
%% Read .json file,0
"%% Rev version number, update field names and types",0
%% Write json file,0
,0
cct_field_adjustments.py,0
,0
CCT metadata was posted with int locations instead of strings.,0
,0
This script fixes those issues and rev's the version number.,0
,0
%% Constants and environment,0
%% Read .json file,0
"%% Rev version number, update field names and types",0
%% Write json file,0
%% Check output data file,0
,0
carrizo_shrubfree_2018.py,0
,0
Convert the .csv file provided for the Carrizo Mojave data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
%% Read source data,0
Original .csv file had superfluous spaces in column names,0
"%% Map filenames to rows, verify image existence",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
58 missing files (of 17652),0
%% Check for images that aren't included in the metadata file,0
3012 of 20606 files are not in metadata,0
%% Create CCT dictionaries,0
Map categories to integer IDs,0
,0
The category '0' is reserved for 'empty',0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
Each filename should just match one row,0
Don't include images that don't exist on disk,0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
,0
snapshot_serengeti_lila.py,0
,0
Create zipfiles of Snapshot Serengeti S1-S11.,0
,0
"Create a metadata file for S1-S10, plus separate metadata files",0
"for S1-S11.  At the time this code was written, S11 was under embargo.",0
,0
Create zip archives of each season without humans.,0
,0
Create a human zip archive.,0
,0
%% Constants and imports,0
import sys; sys.path.append(r'c:\git\ai4eutils'),0
import sys; sys.path.append(r'c:\git\cameratraps'),0
assert(os.path.isdir(metadata_base)),0
"There are two redundant categories, and we re-map ""blank"" to ""empty"" as per CCT convention",0
"%% Load metadata files, concatenate into a single table",0
iSeason = 1,0
%% Convert to dictionaries (prep),0
%% Convert to dictionaries (loops),0
"TODO: iterrows() is a terrible way to do this, but this is one of those days",1
"where I want to get this done, not get better at Python.",0
irow = 0; row = image_table.iloc[0],0
"Loaded as an int64, converting to int here",0
...for each row in the image table,0
Make sure image IDs are what we think they are,0
...for each row in the annotation table,0
%% Load previously-saved dictionaries when re-starting mid-script,0
%%,0
%% Take a look at categories (just sanity-checking),0
%%,0
%% Fill in some image fields we didn't have when we created the image table,0
"width, height, corrupt, seq_num_frames, location, datetime",0
Every annotation in this list should have the same sequence ID,0
"Figure out ""seq_num_frames"", which really should be done in a separate lopp;",0
there's no reason to do this redundantly for every image,0
Every image in this sequence should point back to the same equence,0
Every annotation in this list should have the same location,0
Every annotation in this list should have the same datetime,0
Is this image on disk?,0
iImage = 0; im = images[0],0
...for each image,0
"images_processed = pool.map(process_image, images)",0
"images_processed = list(tqdm(pool.imap_unordered(process_image, images), total=len(images)))",0
%%,0
%% Count missing/corrupted images,0
%% Print distribution of sequence lengths (sanity-check),0
%% Replicate annotations across images,0
iAnn = 0; ann = annotations[iAnn],0
%% See what files are on disk but not annotated (~15 mins),0
247370 files not in the database (of 7425810),0
%% Load old image database,0
%% Look for old images not in the new DB and vice-versa,0
"At the time this was written, ""old"" was S1-S6",0
old_im = cct_old['images'][0],0
new_im = images[0],0
4 old images not in new db,0
12 new images not in old db,0
%% Save our work,0
%% Load our work,0
%%,0
%% Examine size mismatches,0
i_mismatch = -1; old_im = size_mismatches[i_mismatch],0
%% Sanity-check image and annotation uniqueness,0
"%% Split data by seasons, create master list for public seasons",0
ann = annotations[0],0
%% Minor updates to fields,0
"%% Write master .json out for S1-10, write individual season .jsons (including S11)",0
"Loop over all seasons, plus one iteration for the ""all public data"" iteration, and",0
"one for the ""all data"" iteration",0
%% Find categories that only exist in S11,0
List of categories in each season,0
Category 55 (fire) only in S11,0
Category 56 (hyenabrown) only in S11,0
Category 57 (wilddog) only in S11,0
Category 58 (kudu) only in S11,0
Category 59 (pangolin) only in S11,0
Category 60 (lioncub) only in S11,0
%% Prepare season-specific .csv files,0
iSeason = 1,0
%% Create a list of human files,0
ann = annotations[0],0
%% Save our work,0
%% Load our work,0
%%,0
"%% Create archives (human, per-season) (prep)",0
im = images[0],0
im = images[0],0
Don't include humans,0
Only include files from this season,0
Possibly start a new archive,0
...for each image,0
i_season = 0,0
"for i_season in range(0,nSeasons):",0
create_season_archive(i_season),0
%% Create archives (loop),0
pool = ThreadPool(nSeasons+1),0
"n_images = pool.map(create_archive, range(-1,nSeasons))",0
"seasons_to_zip = range(-1,nSeasons)",0
...for each season,0
%% Sanity-check .json files,0
%logstart -o r'E:\snapshot_temp\python.txt',0
%% Zip up .json and .csv files,0
pool = ThreadPool(len(files_to_zip)),0
"pool.map(zip_single_file, files_to_zip)",0
%% Super-sanity-check that S11 info isn't leaking,0
im = data_public['images'][0],0
ann = data_public['annotations'][0],0
iRow = 0; row = annotation_df.iloc[iRow],0
iRow = 0; row = image_df.iloc[iRow],0
%% Create bounding box archive,0
i_image = 0; im = data['images'][0],0
i_box = 0; boxann = bbox_data['annotations'][0],0
%% Sanity-check a few files to make sure bounding boxes are still sensible,0
import sys; sys.path.append(r'C:\git\CameraTraps'),0
%% Check categories,0
%% Summary prep for LILA,0
,0
wi_to_json,0
,0
Prepares CCT-formatted metadata based on a Wildlife Insights data export.,0
,0
"Mostly assumes you have the images also, for validation/QA.",0
,0
%% Imports and constants,0
"Because WI filenames are GUIDs, it's not practical to page through sequences in an",0
image viewer.  So we're going to (optionally) create a copy of the data set where,0
images are ordered.,0
%% Load ground truth,0
%% Take everything out of Pandas,0
%% Synthesize common names when they're not available,0
"Blank rows should always have ""Blank"" as the common name",0
"print('Warning: missing common name for row {} ({})'.format(i_row,row['filename']))",0
%% Convert string timestamps to Python datetimes,0
im = ground_truth_dicts[0],0
"The field called ""location"" in the WI .csv file is a URL, we want to reclaim",0
"the ""location"" keyword for CCT output",0
"Filenames look like, e.g., N36/100EK113/06040726.JPG",0
%% Synthesize sequence information,0
Sort images by time within each location,0
i_location=0; location = locations[i_location],0
previous_datetime = sorted_images_this_location[0]['datetime'],0
im = sorted_images_this_camera[1],0
"Timestamp for this image, may be None",0
Start a new sequence if:,0
,0
* This image has no timestamp,0
* This iamge has a frame number of zero,0
* We have no previous image timestamp,0
,0
Start a new sequence if necessary,0
...for each image in this location,0
...for each location,0
%% Create category dict and category IDs,0
im = images[0],0
%% Count frames in each sequence,0
%% Build relative paths,0
im = images[0],0
Sample URL:,0
,0
gs://project-asfasdfd/deployment/21444549/asdfasdfd-616a-4d10-a921-45ac456c568a.jpg',0
%% Double check images with multiple annotations,0
im = images[0],0
%% Assemble dictionaries,0
Force the empty category to be ID 0,0
input_im = images[0],0
"print('Warning: image ID {} ({}) has multiple annotations'.format(im['id'],im['id'].replace('_','/')))",0
...for each image,0
%% Write output .json,0
%% Validate .json file,0
%% Preview labels,0
"open_file(os.path.join(image_base,'2100703/1141a545-88d2-498b-a684-7431f7aeb324.jpg'))",0
%%,0
%% Create ordered dataset,0
"Because WI filenames are GUIDs, it's not practical to page through sequences in an",0
image viewer.  So we're going to create a copy of the data set where images are,0
ordered.,0
im = images_out[0]; im,0
%% Create ordered .json,0
%% Copy files to their new locations,0
im = ordered_images[0],0
im = data_ordered['images'][0],0
%% Preview labels in the ordered dataset,0
"open_file(os.path.join(image_base,'2100703/1141a545-88d2-498b-a684-7431f7aeb324.jpg'))",0
%% Open an ordered filename from the unordered filename,0
,0
rspb_to_json.py,0
,0
Convert the .csv file provided for the RSPB data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
[location] is an obfuscation,0
%% Create info struct,0
%% Read source data,0
metadataTable.columns.values,0
,0
"array(['Project', 'inDir', 'FileName', 'Station', 'Camera',",0
"'StationCameraFileName', 'DateTimeOriginal', 'DateReadable',",0
"'outDir', 'filename_new', 'fileExistsAlready', 'CopyStatus',",0
"'Species'], dtype=object)",0
We'll populate these later,0
keys should be lowercase,0
"%% Enumerate images, confirm filename uniqueness",0
"%% Update metadata filenames to include site and camera folders, check existence",0
,0
Takes ~1min,0
iRow = 0; row = metadataTable.iloc[iRow],0
There's a bug in the metadata; the 'camera' column isn't correct.,0
camera = row['Camera'],0
"These appear as, e.g., '3.22e12'",0
camera = str(int(float(camera))),0
Let's pull this out of the file name instead,0
,0
Filenames look like one of the following:,0
,0
A1__03224850850507__2015-11-28__10-45-04(1).JPG,0
Bayama2PH__C05__NA(NA).JPG,0
assert(os.path.isfile(fullPath)),0
metadataTable.iloc[iRow] = row,0
Re-assemble into an updated table,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
Write to a text file,0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
iRow = 0; row = metadataTable.iloc[iRow],0
A1__03224850850507__2015-11-28__10-45-04(1).JPG,0
'A1\\03224850850507\\A1__03224850850507__2015-11-28__10-45-04(1).JPG',0
Not currently populated,0
"Often -1, sometimes a semi-meaningful int",0
A1,0
03224850850507,0
"In variable form, but sometimes '28/11/2015 10:45'",0
Check image height and width,0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Write output,0
%% Check database integrity,0
%% Preview a few images to make sure labels were passed along sensibly,0
%% One-time processing step: copy images to a flat directory for annotation,0
%%,0
,0
pc_to_json.py,0
,0
Convert a particular collection of .csv files to CCT format.,0
,0
%% Constants and environment,0
%% Read and concatenate source data,0
List files,0
"List of dataframes, one per .csv file; we'll concatenate later",0
i_file = 87; fn = input_files[i_file],0
Concatenate into a giant data frame,0
%% List files,0
%% Main loop over labels (prep),0
Force the empty category to be ID 0,0
%% Main loop over labels (loop),0
iRow = 0; row = input_metadata.iloc[iRow],0
"ImageID,FileName,FilePath,SpeciesID,CommonName",0
assert os.path.isfile(full_path),0
Retrieve image width and height,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
%% See what's up with missing files,0
s = list(image_relative_paths)[0],0
s = missing_files[0],0
%% Check for images that aren't included in the metadata file,0
%% Sample the database,0
%%,0
Collect the images we want,0
%% Create info struct,0
%% Write output,0
%% Sanity-check the database's integrity,0
%% Render a bunch of images to make sure the labels got carried along correctly,0
options.classes_to_exclude = ['unlabeled'],0
%% Write out a list of files to annotate,0
,0
save_the_elephants_survey_B.py,0
,0
Convert the .csv file provided for the Save the Elephants Survey B data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
input_base = r'/mnt/blobfuse/wildlifeblobssc/ste_2019_08_drop',0
output_base = r'/home/gramener/survey_b',0
Handle all unstructured fields in the source data as extra fields in the annotations,0
"photo_type really should be an image property, but there are a few conflicts",0
that forced me to handle it as an annotation proprerty,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
"%% Map filenames to rows, verify image existence",0
Maps relative paths to row indices in input_metadata,0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
Ignore directories,0
%% Make sure the multiple-annotation cases make sense,0
%%,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
Force the empty category to be ID 0,0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image],0
Example filename:,0
,0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG',0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG',0
i_row = row_indices[0],0
timestamp = row['Date'],0
There are a small handful of datetime mismatches across annotations,0
for the same image,0
assert im['datetime'] == timestamp,0
Special cases based on the 'photo type' field,0
Various spellings of 'community',0
Have we seen this category before?,0
Create an annotation,0
fieldname = list(mapped_fields.keys())[0],0
...for each row,0
...for each image,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
%% Scrap,0
%% Find unique photo types,0
,0
ubc_to_json.py,0
,0
Convert the .csv file provided for the UBC data set to a,0
COCO-camera-traps .json file,0
,0
"Images were provided in eight folders, each of which contained a .csv",0
file with annotations.  Those annotations came in two slightly different,0
"formats, the two formats corresponding to folders starting with ""SC_"" and",0
otherwise.,0
,0
%% Constants and environment,0
Map Excel column names - which vary a little across spreadsheets - to a common set of names,0
%% Enumerate images,0
Load from file if we've already enumerated,0
%% Create CCT dictionaries,0
Force the empty category to be ID 0,0
To simplify debugging of the loop below,0
#%% Create CCT dictionaries (loop),0
#%%,0
Read source data for this folder,0
Rename columns,0
Folder name is the first two characters of the filename,0
,0
Create relative path names from the filename itself,0
Folder name is the camera name,0
,0
Create relative path names from camera name and filename,0
Which of our images are in the spreadsheet?,0
i_row = 0; fn = input_metadata['image_relative_path'][i_row],0
#%% Check for images that aren't included in the metadata file,0
Find all the images in this folder,0
Which of these aren't in the spreadsheet?,0
#%% Create entries in CCT dictionaries,0
Only process images we have on disk,0
"This is redundant, but doing this for clarity, at basically no performance",0
cost since we need to *read* the images below to check validity.,0
i_row = row_indices[0],0
"These generally represent zero-byte images in this data set, don't try",0
to find the very small handful that might be other kinds of failures we,0
might want to keep around.,0
print('Error opening image {}'.format(image_relative_path)),0
If we've seen this category before...,0
...make sure it used the same latin --> common mapping,0
,0
"If the previous instance had no mapping, use the new one.",0
assert common_name == category['common_name'],0
Create an annotation,0
...for each annotation we found for this image,0
...for each image,0
...for each dataset,0
Print all of our species mappings,0
"%% Copy images for which we actually have annotations to a new folder, lowercase everything",0
im = images[0],0
%% Create info struct,0
"%% Convert image IDs to lowercase in annotations, tag as sequence level",0
"While there isn't any sequence information, the nature of false positives",0
"here leads me to believe the images were labeled at the sequence level, so",0
we should trust labels more when positives are verified.  Overall false,0
positive rate looks to be between 1% and 5%.,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
,0
helena_to_cct.py,0
,0
Convert the Helena Detections data set to a COCO-camera-traps .json file,0
,0
%% Constants and environment,0
This is one time process,0
%% Create Filenames and timestamps mapping CSV,0
import pdb;pdb.set_trace(),0
%% To create CCT JSON for RSPB dataset,0
%% Read source data,0
Original Excel file had timestamp in different columns,0
"%% Map filenames to rows, verify image existence",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
%% Skipping this check because one image has multiple species,0
assert len(duplicate_rows) == 0,0
%% Check for images that aren't included in the metadata file,0
%% Create CCT dictionaries,0
Map categories to integer IDs,0
,0
The category '0' is reserved for 'empty',0
For each image,0
Don't include images that don't exist on disk,0
Some filenames will match to multiple rows,0
assert(len(rows) == 1),0
iRow = rows[0],0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
ann['datetime'] = row['datetime'],0
ann['site'] = row['site'],0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
%% Imports and constants,0
from github.com/microsoft/ai4eutils,0
from github.com/ecologize/CameraTraps,0
A list of files in the lilablobssc container for this data set,0
The raw detection files provided by NOAA,0
A version of the above with filename columns added,0
%% Read input .csv,0
%% Read list of files,0
%% Convert paths to full paths,0
i_row = 0; row = df.iloc[i_row],0
assert ir_image_path in all_files,0
...for each row,0
%% Write results,0
"%% Load output file, just to be sure",0
%% Render annotations on an image,0
i_image = 2004,0
%% Download the image,0
%% Find all the rows (detections) associated with this image,0
"as l,r,t,b",0
%% Render the detections on the image(s),0
In pixel coordinates,0
In pixel coordinates,0
%% Save images,0
%% Clean up,0
,0
carrizo_trail_cam_2017.py,0
,0
"Convert the .csv files provided for the ""Trail Cam Carrizo"" 2017 data set to",0
a COCO-camera-traps .json file.,0
,0
%% Constants and environment,0
%% Read source data,0
Removing the empty records,0
"%% Map filenames to rows, verify image existence",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
908 missing files (of 60562),0
%% Check for images that aren't included in the metadata file,0
105329 of 164983 files are not in metadata,0
%% Create CCT dictionaries,0
Map categories to integer IDs,0
,0
The category '0' is reserved for 'empty',0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
Each filename should just match one row,0
Don't include images that don't exist on disk,0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
,0
channel_islands_to_cct.py,0
,0
Convert the Channel Islands data set to a COCO-camera-traps .json file,0
,0
"Uses the command-line tool ExifTool (exiftool.org) to pull EXIF tags from images,",0
"because every Python package we tried failed to pull the ""Maker Notes"" field properly.",0
,0
"%% Imports, constants, paths",0
# Imports ##,0
# Constants ##,0
# Paths ##,0
Confirm that exiftool is available,0
"assert which(exiftool_command_name) is not None, 'Could not locate the ExifTool executable'",0
%% Load information from every .json file,0
"Ignore the sample file... actually, first make sure there is a sample file",0
...and now ignore that sample file.,0
json_file = json_files[0],0
ann = annotations[0],0
...for each annotation in this file,0
...for each .json file,0
"%% Confirm URL uniqueness, handle redundant tags",0
Have we already added this image?,0
"One .json file was basically duplicated, but as:",0
,0
Ellie_2016-2017 SC12.json,0
Ellie_2016-2017-SC12.json,0
"If the new image has no output, just leave the old one there",0
"If the old image has no output, and the new one has output, default to the one with output",0
Don't worry about the cases where someone tagged 'fox' and someone tagged 'fox_partial',0
...for each image we've already added,0
...if this URL is/isn't in the list of URLs we've already processed,0
...for each image,0
%% Save progress,0
%%,0
%%,0
%% Download files (functions),0
https://www.quickprogrammingtips.com/python/how-to-download-multiple-files-concurrently-in-python.html,0
"This is returned with a leading slash, remove it",0
%% Download files (execution),0
%% Read required fields from EXIF data (functions),0
"-G means ""Print group name for each tag"", e.g. print:",0
,0
[File]          Bits Per Sample                 : 8,0
,0
...instead of:,0
,0
Bits Per Sample                 : 8,0
"If we don't get any EXIF information, this probably isn't an image",0
line_raw = exif_lines[0],0
"Split on the first occurrence of "":""",0
Typically:,0
,0
"'[MakerNotes]    Sequence                        ', '1 of 3']",0
Not a typo; we are using serial number as a location,0
"If there are multiple timestamps, make sure they're *almost* the same",0
"If there are multiple timestamps, make sure they're *almost* the same",0
...for each line in the exiftool output,0
"This isn't directly related to the lack of maker notes, but it happens that files that are missing",0
maker notes also happen to be missing EXIF date information,0
...process_exif(),0
"This is returned with a leading slash, remove it",0
Ignore non-image files,0
%% Read EXIF data (execution),0
ann = images[0],0
%% Save progress,0
Use default=str to handle datetime objects,0
%%,0
%%,0
"Not deserializing datetimes yet, will do this if I actually need to run this",0
%% Check for EXIF read errors,0
%% Remove junk,0
Ignore non-image files,0
%% Fill in some None values,0
"...so we can sort by datetime later, and let None's be sorted arbitrarily",0
%% Find unique locations,0
%% Synthesize sequence information,0
Sort images by time within each location,0
i_location=0; location = locations[i_location],0
previous_datetime = sorted_images_this_location[0]['datetime'],0
im = sorted_images_this_camera[1],0
"Timestamp for this image, may be None",0
Start a new sequence if:,0
,0
* This image has no timestamp,0
* This image has a frame number of zero,0
* We have no previous image timestamp,0
,0
Start a new sequence if necessary,0
...for each image in this location,0
...for each location,0
%% Count frames in each sequence,0
images_this_sequence = [im for im in images if im['seq_id'] == seq_id],0
"%% Create output filenames for each image, store original filenames",0
i_location = 0; location = locations[i_location],0
i_image = 0; im = sorted_images_this_location[i_image],0
%% Save progress,0
Use default=str to handle datetime objects,0
%%,0
%%,0
%% Copy images to their output files (functions),0
%% Copy images to output files (execution),0
%% Rename the main image list for consistency with other scripts,0
%% Create CCT dictionaries,0
Make sure this is really a box,0
Transform to CCT format,0
Force the empty category to be ID 0,0
i_image = 0; input_im = all_image_info[0],0
"This issue only impacted one image that wasn't a real image, it was just a screenshot",0
"showing ""no images available for this camera""",0
Convert datetime if necessary,0
Process temperature if available,0
Read width and height if necessary,0
I don't know what this field is; confirming that it's always None,0
Process object and bbox,0
os.startfile(output_image_full_path),0
"Zero is hard-coded as the empty category, but check to be safe",0
"I can't figure out the 'index' field, but I'm not losing sleep about it",0
assert input_annotation['index'] == 1+i_ann,0
"Some annotators (but not all) included ""_partial"" when animals were partially obscured",0
"Annotators *mostly* used 'none', but sometimes 'empty'.  'empty' is CCT-correct.",0
If we've seen this category before...,0
If this is a new category...,0
...if this is an empty/non-empty annotation,0
Create an annotation,0
...for each annotation on this image,0
...for each image,0
%% Change *two* annotations on images that I discovered contains a human after running MDv4,0
%% Move human images,0
ann = annotations[0],0
%% Count images by location,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
viz_options.classes_to_exclude = [0],0
,0
ena24_to_json_2017.py,0
,0
Convert the ENA24 data set to a COCO-camera-traps .json file,0
,0
%% Constants and environment,0
Temporary folders for human and non-human images,0
Clean existing output folders/zipfiles,0
%% Support functions,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
Check image height and width,0
"Each row is category, [box coordinates]",0
"If there's just one row, loadtxt reads it as a 1d array; make it a 2d array",0
with one row,0
Each row is a bounding box,0
Have we seen this category before?,0
Create an annotation,0
...for each bounding box,0
"This was here for debugging; nearly every instance is Human+Horse, Human+Vehicle,",0
"or Human+Dog, but there is one Rabbit+Opossium, and a few Deer+Chicken!",0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
%% Create ZIP files for human and non human,0
%% Validate output,0
%% Preview labels,0
%% Imports and constants,0
configurations and paths,0
%% Helper functions,0
"dest_path = copy(source_path, dest_folder)",0
num_workers = multiprocessing.cpu_count(),0
pool = ThreadPool(num_workers),0
"results = pool.starmap(_copy_unzip, zip(sources, itertools.repeat(dest_folder)))",0
,0
print('Waiting for processes to finish...'),0
pool.close(),0
pool.join(),0
sequential,0
%% Command-line driver,0
if the blob container is already mounted on the VM,0
or you can download them using the storage Python SDK,0
store storage account key in environment variable AZ_STORAGE_KEY,0
,0
eMammal_helpers.py,0
,0
Support functions for processing eMammal metadata,0
,0
%% Constants and imports,0
%% Support functions,0
"pad to a total of 3 digits if < 1000, or 4 digits otherwise",0
img_frame is a string from the xml tree,0
"length 4 frame order is returned as is, others are left padded to be 3 digit long",0
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels,0
pad to a total of 4 digits,0
img_frame is a string from the xml tree,0
"length 4 frame order is returned as is, others are left padded to be 3 digit long",0
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels,0
others column,0
summer day hours: 6am - 7pm,0
others day hours: 7am - 6pm,0
,0
make_eMammal_json.py,0
,0
"Produces the COCO-formatted json database for an eMammal dataset, i.e. a",0
"collection of folders, each of which contains a deployment_manifest.xml file.",0
,0
"In this process, each image needs to be loaded to size it.",0
,0
"To add bounding box annotations to the resulting database, use",0
add_annotations_to_eMammal_json.py.,0
,0
%% Constants and imports,0
"Either add the eMammal directory to your path, or run from there",0
os.chdir(r'd:\git\CameraTraps\database_tools\eMammal'),0
import warnings,0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
warnings.filterwarnings('ignore'),0
Should we run the image size retrieval in parallel?,0
%% Support functions,0
%% Main loop (metadata processing; image sizes are retrieved later),0
deployment = folders[0],0
sequence = image_sequences[0],0
get species info for this sequence,0
add each image's info to database,0
img = images[0],0
"some manifests don't have the ImageOrder info, but the info is in the file name",0
full_img_id has no frame info,0
,0
frame number only used in requests to iMerit for ordering,0
...for each image,0
...for each sequence,0
...for each deployment,0
%% Get image sizes,0
"'tasks' is currently a list of 2-tuples, with each entry as [image dictionary,path].",0
,0
"Go through that and copy just the image dictionaries to 'db_images', adding size",0
information to each entry.  Takes a couple hours.,0
opening each image seems too fast for this multi-threaded version to be faster than sequential code.,0
%% Assemble top-level dictionaries,0
%% Write out .json,0
,0
make_full_SS_json.py,0
,0
Create a COCO-camera-traps .json file for Snapshot Serengeti data from,0
the original .csv files provided on Dryad.,0
,0
"This was used for ""version 1.0"" of the public Snapshot Serengeti archive; it's no",0
longer used as of v2.0 (early 2020).  See snapshot_serengeti_lila.py for the updated,0
Snapshot Safari preparation process.,0
,0
%% Imports and constants,0
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays",0
Count the number of images with multiple species,0
"%% Read image .csv file, format into a dictionary mapping images to capture events",0
%% Create CCT-style .json,0
still need image width and height,0
...for each image,0
%% Write output files,0
,0
make_per_season_SS_json.py,0
,0
Create a COCO-camera-traps .json file for each Snapshot Serengeti season from,0
the original .csv files provided on Dryad.,0
,0
"This was used for ""version 1.0"" of the public Snapshot Serengeti archive; it's no",0
longer used as of v2.0 (early 2020).  See snapshot_serengeti_lila.py for the updated,0
Snapshot Safari preparation process.,0
,0
%% Imports and constants,0
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays",0
"%% Read image .csv file, format into a dictionary mapping images to capture events",0
%% Create CCT-style .json,0
still need image width and height,0
...for each image ID,0
%% Write output files,0
...for each season,0
,0
generate_lila_per_image_labels.py,0
,0
"Generate a .csv file with one row per annotation, containing full URLs to every",0
"camera trap image on LILA, with taxonomically expanded labels.",0
,0
"Typically there will be one row per image, though images with multiple annotations",0
will have multiple rows.,0
,0
"Some images may not physically exist, particularly images that are labeled as ""human"".",0
This script does not validate image URLs.,0
,0
Does not include bounding box annotations.,0
,0
%% Constants and imports,0
"We'll write images, metadata downloads, and temporary files here",0
"Some datasets don't have ""sequence_level_annotation"" fields populated, but we know their",0
annotation level,0
%% Download and parse the metadata file,0
To select an individual data set for debugging,0
%% Download and extract metadata for the datasets we're interested in,0
%% Load taxonomy data,0
"%% Build a dictionary that maps each [dataset,query] pair to the full taxonomic label set",0
i_row = 0; row = taxonomy_df.iloc[i_row],0
%% Process annotations for each dataset,0
ds_name = list(metadata_table.keys())[0],0
"Go through annotations, marking each image with the categories that are present",0
,0
ann = annotations[0],0
im = images[10],0
This field name was only used for Caltech Camera Traps,0
raise ValueError('Suspicious date parsing result'),0
Special case we don't want to print a warning about,0
"Location, sequence, and image IDs are only guaranteed to be unique within",0
"a dataset, so for the output .csv file, include both",0
category_name = list(categories_this_image)[0],0
Only print a warning the first time we see an unmapped label,0
...for each category that was applied at least once to this image,0
...for each image in this dataset,0
print('Warning: no date information available for this dataset'),0
print('Warning: no location information available for this dataset'),0
...for each dataset,0
...with open(),0
%% Read the .csv back,0
%% Do some post-hoc integrity checking,0
"-1 isn't *really* valid, but we use it sometimes for sequences of unknown length",0
i_row = 0; row = df.iloc[i_row],0
%% Preview constants,0
%% Choose images to download,0
ds_name = list(metadata_table.keys())[2],0
Find all rows for this dataset,0
...for each dataset,0
%% Download images,0
i_image = 0; image = images_to_download[i_image],0
%% Write preview HTML,0
im = images_to_download[0],0
,0
Common constants and functions related to LILA data management/retrieval.,0
,0
%% Imports and constants,0
LILA camera trap master metadata file,0
"wildlife_insights_taxonomy_url = 'https://api.wildlifeinsights.org/api/v1/taxonomy?fields=class,order,family,genus,species,authority,taxonomyType,uniqueIdentifier,commonNameEnglish&page[size]={}'.format(wildlife_insights_page_size)",0
from ai4eutils,0
%% Common functions,0
"We haven't implemented paging, make sure that's not an issue",0
d['data'] is a list of items that look like:,0
Put the master metadata file in the same folder where we're putting images,0
Read lines from the master metadata file,0
Parse those lines into a table,0
Each line in this file is name/sas_url/json_url/[bbox_json_url],0
Create a separate entry for bounding boxes if they exist,0
Unzip if necessary,0
,0
get_lila_category_list.py,0
,0
Generates a .json-formatted dictionary mapping each LILA dataset to all categories,0
"that exist for that dataset, with counts for the number of occurrences of each category",0
"(the number of *annotations* for each category, not the number of *images*).",0
,0
"Also loads the taxonomy mapping file, to include scientific names for each category.",0
,0
get_lila_category_counts counts the number of *images* for each category in each dataset.,0
,0
%% Constants and imports,0
array to fill for output,0
"We'll write images, metadata downloads, and temporary files here",0
Created by get_lila_category_list.py... contains counts for each category,0
%% Load category and taxonomy files,0
%% Map dataset names and category names to scientific names,0
i_row = 1; row = taxonomy_df.iloc[i_row]; row,0
%% Download and parse the metadata file,0
%% Download and extract metadata for the datasets we're interested in,0
%% Get category names and counts for each dataset,0
ds_name = 'NACTI',0
Open the metadata file,0
Collect list of categories and mappings to category name,0
ann = annotations[0],0
c = categories[0],0
"Don't do taxonomy mapping for bbox data sets, which are sometimes just binary and are",0
always redundant with the class-level data sets.,0
"As of right now, this is the only quirky case",0
...for each dataset,0
%% Save dict,0
%% Print the results,0
ds_name = list(dataset_to_categories.keys())[0],0
...for each dataset,0
,0
download_lila_subset.py,0
,0
"Example of how to download a list of files from LILA, e.g. all the files",0
in a data set corresponding to a particular species.,0
,0
%% Constants and imports,0
LILA camera trap master metadata file,0
"In this example, we're using the Missouri Camera Traps data set and the Caltech Camera Traps dataset",0
All lower-case; we'll convert category names to lower-case when comparing,0
"We'll write images, metadata downloads, and temporary files here",0
"We will demonstrate two approaches to downloading, one that loops over files",0
"and downloads directly in Python, another that uses AzCopy.",0
,0
AzCopy will generally be more performant and supports resuming if the,0
transfers are interrupted.  This script assumes that azcopy is on the system path.,0
Number of concurrent download threads (when not using AzCopy) (AzCopy does its,0
own magical parallelism),0
%% Support functions,0
remove the leading '/',0
%% Download and parse the metadata file,0
Put the master metadata file in the same folder where we're putting images,0
Read lines from the master metadata file,0
Parse those lines into a table,0
Each line in this file is name/base_url/json_url/[box_url],0
%% Download and extract metadata for the datasets we're interested in,0
Unzip if necessary,0
...for each dataset of interest,0
%% List of files we're going to download (for all data sets),0
"Flat list or URLS, for use with direct Python downloads",0
For use with azcopy,0
This may or may not be a SAS URL,0
# Open the metadata file,0
# Build a list of image files (relative path names) that match the target species,0
Retrieve all the images that match that category,0
Retrieve image file names,0
Convert to URLs,0
...for each dataset,0
%% Download those image files,0
ds_name = 'Caltech Camera Traps',0
ds_name = 'SWG Camera Traps',0
We want to use the whole relative path for this script (relative to the base of the container),0
"to build the output filename, to make sure that different data sets end up in different folders.",0
This may or may not be a SAS URL,0
For example:,0
,0
caltech-unzipped/cct_images,0
swg-camera-traps,0
Check whether the URL includes a folder,0
E.g. caltech-unzipped,0
E.g. cct_images,0
E.g. swg-camera-traps,0
"Write out a list of files, and use the azcopy ""list-of-files"" option to download those files.",0
,0
This azcopy feature is unofficially documented at:,0
,0
https://github.com/Azure/azure-storage-azcopy/wiki/Listing-specific-files-to-transfer,0
,0
import clipboard; clipboard.copy(cmd),0
Loop over files,0
,0
get_lila_category_counts.py,0
,0
Count the number of images and bounding boxes with each label in one or more LILA datasets.,0
,0
"This script doesn't write these counts out anywhere other than the console, it's just intended",0
as a template for doing operations like this on LILA data.  get_lila_category_list.py writes,0
"information out to a .json file, but it counts *annotations*, not *images*, for each category.",0
,0
%% Constants and imports,0
"If None, will use all datasets",0
"We'll write images, metadata downloads, and temporary files here",0
%% Download and parse the metadata file,0
%% Download and extract metadata for the datasets we're interested in,0
%% Count categories,0
ds_name = datasets_of_interest[0],0
"Go through annotations, marking each image with the categories that are present",0
,0
ann = annotations[0],0
Now go through images and count categories,0
im = images[0],0
...for each dataset,0
%% Print the results,0
...for each dataset,0
,0
add_url_to_database.py,0
,0
"Adds a ""url"" field to a coco-camera-traps .json database, specifically to allow the db to",0
be reviewed in the Visipedia annotation tool.,0
,0
,0
remove_corrupted_images_from_database.py,0
,0
"Given a coco-camera-traps .json file, checks all images for TF-friendliness and generates",0
a new .json file that only contains the non-corrupted images.,0
,0
%% Imports and constants,0
import multiprocessing,0
"I leave this at an annoying low number, since by definition weird stuff will",0
"be happening in the TF kernel, and it's useful to keep having content in the console.",0
%% Function definitions,0
"I sometimes pass in a list of images, sometimes a dict with a single",0
element mapping a job ID to the list of images,0
"We're about to start a lot of TF sessions, and we don't want gobs",0
of debugging information printing out for every session.,0
At some point we were creating a single session and looping over images,0
"within that session, but the only way I found to reliably not run out",0
of GPU memory was to create a session per image and gc.collect() after,0
each session.,0
Map Image IDs to boolean (should I keep this image?),0
"Convert to lists, append job numbers to the image lists",0
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)",0
Merge results,0
%% Interactive driver,0
%%,0
base_dir = r'D:\temp\snapshot_serengeti_tfrecord_generation',0
Load annotations,0
Check for corruption,0
Write out only the uncorrupted data,0
%% Command-line driver,0
#######,0
,0
integrity_check_json_db.py,0
,0
"Does some integrity-checking and computes basic statistics on a db, specifically:",0
,0
* Verifies that required fields are present and have the right types,0
* Verifies that annotations refer to valid images,0
* Verifies that annotations refer to valid categories,0
"* Verifies that image, category, and annotation IDs are unique",0
,0
* Optionally checks file existence,0
,0
* Finds un-annotated images,0
* Finds unused categories,0
,0
* Prints a list of categories sorted by count,0
,0
#######,0
%% Constants and environment,0
%% Functions,0
"If baseDir is non-empty, checks image existence",0
This is used in a medium-hacky way to share modified options across threads,1
print('Image path {} does not exist'.format(filePath)),0
"#%% Read .json file if necessary, integrity-check fields",0
info = data['info'],0
"#%% Build dictionaries, checking ID uniqueness and internal validity as we go",0
Confirm that required fields are present,0
Confirm ID uniqueness,0
...for each category,0
image = images[0],0
Confirm that required fields are present,0
Confirm ID uniqueness,0
We previously supported ints here; this should be strings now,0
"assert isinstance(image['location'], str) or isinstance(image['location'], int), \",0
'Illegal image location type',0
Are we checking for unused images?,0
Recursively enumerate images,0
print('Image {} is unused'.format(p)),0
Are we checking file existence and/or image size?,0
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)",0
...for each image,0
Confirm that required fields are present,0
Confirm ID uniqueness,0
Confirm validity,0
...for each annotation,0
#%% Print statistics,0
Find un-annotated images and multi-annotation images,0
Find unused categories,0
Prints a list of categories sorted by count,0
,0
https://stackoverflow.com/questions/72899/how-do-i-sort-a-list-of-dictionaries-by-a-value-of-the-dictionary,0
...def integrity_check_json_db(),0
%% Command-line driver,0
%% Interactive driver(s),0
%%,0
Integrity-check .json files for LILA,0
options.iMaxNumImages = 10,0
,0
combine_two_json_files.py,0
,0
"Merges two coco-camera-traps .json files. In particular, categories are combined and re-numbered.",0
,0
Combined Info,0
Combined Images,0
Combined Categories,0
## categories to merge,0
Combined Annotations,0
,0
make_detection_db_for_viewing.py,0
,0
"Given a .json file with ground truth bounding boxes, and a .p file containing detections for the same images,",0
"creates a new .json file with separate classes for ground truth and detection, suitable for viewing in the Visipedia",0
annotation tool.,0
,0
%% Imports and constants,0
%% Main function,0
im_id_to_im = {im['id']:im for im in images},0
make new categories to distinguish between ground truth and detections,0
"update all gt annotations to be class ""gt""",0
collect all detections by image,0
keep any detection with score above det_thresh,0
"need to convert bbox from [x1,y1,x2,y2] to [x,y,w,h]",0
"add ""info"" and ""licenses"" for annotation tools to function",0
create new db,0
%% Command-line handling,0
,0
analyze_json_database.py,0
,0
Plots location/class/etc. distributions for classes in a coco-camera-traps .json file.,0
,0
Currently includes some one-off code for specific species.,0
,0
%% Constants and imports,0
%% Path configuration,0
%% Load source data,0
%% Build image/category dictionaries,0
%% Make plot of category distribution,0
%% make plots of location distribution,0
"plt.title('Number of images per location, by category')",0
"plt.tight_layout(rect=[0,0,1,0.9])",0
#make plot of images per season,0
%% Make plot of lions per location,0
%% Make plot of elephants per location,0
for loc in sorted_by_total[:25]:,0
"print('Location:' + loc[0] +', Lions: ' + str(len(loc_to_lion_ims[loc[0]])) + ', Elephants: ' + str(len(loc_to_elephant_ims[loc[0]])) + ', Total ims: ' + str(len(loc_to_ims[loc[0]])))",0
,0
subset_json_db.py,0
,0
Select a subset of images (and associated annotations) from a .json file,0
in COCO Camera Traps format.,0
,0
"To subset the .json files produced by our batch processing API, see",0
subset_json_detector_output.py,0
,0
Sample invocation:,0
,0
"python subset_json_db.py ""E:\Statewide_wolf_container\idfg_20190409.json"" ""E:\Statewide_wolf_container\idfg_20190409_clearcreek.json"" ""clearcreek"" --ignore_case",0
,0
%% Constants and imports,0
%% Functions,0
Load the input file if necessary,0
Find images matching the query,0
Find annotations referring to those images,0
Write the output file if requested,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
%% Constants and imports,0
%% Merge functions,0
i_input_dict = 0; input_dict = input_dicts[i_input_dict],0
We will prepend an index to every ID to guarantee uniqueness,0
Map detection categories from the original data set into the merged data set,0
...for each category,0
Merge original image list into the merged data set,0
Create a unique ID,0
...for each image,0
Same for annotations,0
...for each annotation,0
"Merge info dicts, don't check completion time fields",0
...for each dictionary,0
Convert merged image dictionaries to a sorted list,0
%% Driver,0
,0
add_width_and_height_to_database.py,0
,0
Grabs width and height from actual image files for a .json database that is missing w/h.,0
,0
Originally used when we created a .json file for snapshot serengeti from .csv.,0
,0
"This file converts the JSON output of the batch processing API to a pickle file, which can be used by the",0
script ./make_classification_dataset.py,0
"Convert boxes from JSON   [x_min, y_min, width_of_box, height_of_box]",0
"to PICKLE [ymin,  xmin,  ymax,         xmax]",0
Write detections to file with pickle,0
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the",0
script ./make_classification_dataset.py,0
Write detections to file with pickle,0
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the",0
script ./make_classification_dataset.py,0
Assumes that the root of the CameraTrap repo is in the PYTHONPATH,0
Minimum threshold to put a detection into the output JSON file,0
Parameter check,0
Load detections from input,0
Load COCO style annotations,0
Build output JSON in format version 1.0,0
Adding the only known metadata info,0
"The pickle file does not contain category information, so we assume the default",0
For each image with detections,0
for each detection,0
"Convert boxes from [ymin, xmin, ymax, xmax] format to",0
"[x_min, y_min, width_of_box, height_of_box]",0
Write output json,0
global flag for whether or not we encounter missing images,0
"- will only print ""missing image"" warning once",0
TFRecords variables,0
1.3 for the cropping during test time and 1.3 for the context that the,0
CNN requires in the left-over image,0
Create output directories,0
Load COCO style annotations from the input dataset,0
"Get all categories, their names, and create updated ID for the json file",0
Prepare the coco-style json files,0
Split the dataset by locations,0
Load detections,0
Write out COCO-style json files to the output directory,0
Write detections to file with pickle,0
## Preparations: get all the output tensors,0
For all images listed in the annotations file,0
Skip the image if it is annotated with more than one category,0
"Get ""old"" and ""new"" category ID and category name for this image.",0
Skip if in excluded categories.,0
get path to image,0
"If we already have detection results, we can use them",0
Otherwise run detector and add detections to the collection,0
Only select detections with confidence larger than DETECTION_THRESHOLD,0
Skip image if no detection selected,0
whether it belongs to a training or testing location,0
Skip images that we do not have available right now,0
- this is useful for processing parts of large datasets,0
Load image,0
Run inference,0
"remove batch dimension, and convert from float32 to appropriate type",0
convert normalized bbox coordinates to pixel coordinates,0
Pad the detected animal to a square box and additionally by,0
"PADDING_FACTOR. The result will be in crop_boxes. However, we need to make",0
sure that its box coordinates are still within the image.,0
"for each bounding box, crop the image to the padded box and save it",0
"Create the file path as it will appear in the annotation json,",0
adding the box number if there are multiple boxes,0
"if the cropped file already exists, verify its size",0
Add annotations to the appropriate json,0
js_keys = ['/'.join(im['file_name'].split('/')[1:])[:-4] for im in js['images']],0
for tk in js_keys:,0
"assert np.isclose(1, np.sum(detections[tk]['detection_scores'] > 0.5))",0
,0
separate_detections_by_size,0
,0
Not-super-well-maintained script to break a list of API output files up,0
based on bounding box size.,0
,0
%% Imports and constants,0
Folder with one or more .json files in it that we want to split up,0
Enumerate .json files,0
Define size thresholds and confidence thresholds,0
"Not used directly in this script, but useful if we want to generate previews",0
%% Split by size,0
For each size threshold...,0
For each file...,0
fn = input_files[0],0
Just double-checking; we already filtered this out above,0
Don't reprocess .json files we generated with this script,0
Load the input file,0
For each image...,0
1.1 is the same as infinity here; no box can be bigger than a whole image,0
What's the smallest detection above threshold?,0
"[x_min, y_min, width_of_box, height_of_box]",0
,0
size = w * h,0
...for each detection,0
Which list do we put this image on?,0
...for each image in this file,0
Make sure the number of images adds up,0
Write out all files,0
...for each size threshold,0
...for each file,0
,0
tile_images.py,0
,0
Split a folder of images into tiles.  Preserves relative folder structure in a,0
"new output folder, with a/b/c/d.jpg becoming, e.g.:",0
,0
a/b/c/d_row_0_col_0.jpg,0
a/b/c/d_row_0_col_1.jpg,0
,0
%% Imports and constants,0
from ai4eutils,0
%% Main function,0
TODO: parallelization,1
,0
i_fn = 2; relative_fn = image_relative_paths[i_fn],0
Can we skip this image because we've already generated all the tiles?,0
TODO: super-sloppy that I'm pasting this code from below,1
From:,0
,0
https://github.com/whiplashoo/split-image/blob/main/src/split_image/split.py,0
i_col = 0; i_row = 1,0
left/top/right/bottom,0
...for each row,0
...for each column,0
...for each image,0
%% Interactive driver,0
%%,0
%%,0
,0
rde_debug.py,0
,0
Some useful cells for comparing the outputs of the repeat detection,0
"elimination process, specifically to make sure that after optimizations,",0
results are the same up to ordering.,0
,0
%% Compare two RDE files,0
i_dir = 0,0
break,0
"Regardless of ordering within a directory, we should have the same",0
number of unique detections,0
Re-sort,0
Make sure that we have the same number of instances for each detection,0
Make sure the box values match,0
,0
aggregate_video.py,0
,0
Aggregate results and render output video for a video that's already been run through MD,0
,0
%% Constants,0
%% Processing,0
im = d['images'][0],0
...for each detection,0
This is no longer included in output files by default,0
# Split into frames,0
# Render output video,0
## Render detections to images,0
## Combine into a video,0
,0
ocr_sandbox.py,0
,0
sandbox for experimenting with using OCR to pull metadata from camera trap images,0
,0
The general approach is:,0
,0
"* Crop a fixed percentage from the top and bottom of an image, slightly larger",0
than the largest examples we've seen of how much space is used for metadata.,0
,0
"* Refine that crop by blurring a little, then looking for huge peaks in the",0
"color histogram suggesting a solid background, then finding rows that are",0
mostly that color.,0
,0
"* Crop to the refined crop, then run pytesseract to extract text",0
,0
"* Use regular expressions to find time and date, in the future can add, e.g.,",0
"temperature (which is often present *only* in the images, unlike time/date which",0
are also usually in EXIF but often wrong or lost in processing),0
,0
"The metadata extraction (EXIF, IPTC) here is just sample code that seemed to",0
belong in this file.,0
,0
Contact: Dan Morris (cameratraps@lila.science),0
,0
%% Constants and imports,0
pip install pytesseract,0
,0
"Also intall tesseract from: https://github.com/UB-Mannheim/tesseract/wiki, and add",0
"the installation dir to your path (on Windows, typically C:\Program Files (x86)\Tesseract-OCR)",0
pip install IPTCInfo3,0
from the ai4eutils repo: https://github.com/Microsoft/ai4eutils,0
,0
"Only used for writing out a summary, not important for the core metadata extraction",0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Using a semi-arbitrary metric of how much it feels like we found the,0
"text-containing region, discard regions that appear to be extraction failures",0
Pad each crop with a few pixels to make tesseract happy,0
Discard text from the top,0
"When we're looking for pixels that match the background color, allow some",0
tolerance around the dominant color,0
We need to see a consistent color in at least this fraction of pixels in our rough,0
crop to believe that we actually found a candidate metadata region.,0
"What fraction of the [top,bottom] of the image should we use for our rough crop?",0
A row is considered a probable metadata row if it contains at least this fraction,0
"of the background color.  This is used only to find the top and bottom of the crop area,",0
"so it's not that *every* row needs to hit this criteria, only the rows that are generally",0
above and below the text.,0
%% Support functions,0
"%% Load some images, pull EXIF and IPTC data for fun",0
%% Rough crop,0
"This will be an nImages x 1 list of 2 x 1 lists (image top, image bottom)",0
image = images[0],0
"l,t,r,b",0
,0
"0,0 is upper-left",0
"%% Close-crop around the text, return a revised image and success metric",0
Did we find a sensible mode that looks like a background value?,0
"This looks very scientific, right?  Definitely a probability?",0
"print('Failed min background fraction test: {} of {}'.format(pBackGroundValue,minBackgroundFraction))",0
"Notes to self, things I tried that didn't really go anywhere...",0
"analysisImage = cv2.blur(analysisImage, (3,3))",0
"analysisImage = cv2.medianBlur(analysisImage,5)",0
"analysisImage = cv2.Canny(analysisImage,100,100)",0
imagePil = Image.fromarray(analysisImage); imagePil,0
Use row heuristics to refine the crop,0
,0
This egregious block of code makes me miss my fluency in Matlab.,1
"print('Cropping to {},{},{},{}'.format(x,y,w,h))",0
Crop the image,0
"For some reason, tesseract doesn't like characters really close to the edge",0
imagePil = Image.fromarray(croppedImage); imagePil,0
%% Go to OCR-town,0
"An nImages x 2 list of strings, extracted from the top and bottom of each image",0
An nImages x 2 list of cropped images,0
iImage = 0; iRegion = 1; regionSet = imageRegions[iImage]; region = regionSet[iRegion],0
text = pytesseract.image_to_string(region),0
pil --> cv2,0
"image = cv2.medianBlur(image, 3)",0
"image = cv2.erode(image, None, iterations=2)",0
"image = cv2.dilate(image, None, iterations=4)",0
"image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]",0
"image = cv2.blur(image, (3,3))",0
"image = cv2.copyMakeBorder(image,10,10,10,10,cv2.BORDER_CONSTANT,value=[0,0,0])",0
"text = pytesseract.image_to_string(imagePil, lang='eng')",0
https://github.com/tesseract-ocr/tesseract/wiki/Command-Line-Usage,0
"psm 6: ""assume a single uniform block of text""",0
,0
...for each cropped region,0
...for each image,0
%% Extract dates and times,0
s = '1:22 pm',0
s = '1:23:44 pm',0
%% Write results to a handy html file,0
Add image name and resized image,0
Add results and individual region images,0
"textStyle = ""font-family:calibri,verdana,arial;font-weight:bold;font-size:150%;text-align:left;margin-left:50px;""",0
%% Scrap,0
Alternative approaches to finding the text/background  region,0
Using findCountours(),0
imagePil = Image.fromarray(analysisImage); imagePil,0
"analysisImage = cv2.erode(analysisImage, None, iterations=3)",0
"analysisImage = cv2.dilate(analysisImage, None, iterations=3)",0
"analysisImage = cv2.threshold(analysisImage, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]",0
Find object with the biggest bounding box,0
Using connectedComponents(),0
analysisImage = image,0
print('Found {} components'.format(nb_components)),0
We just want the *background* image,0
,0
"Takes a .json file with MD results for an individual video, and converts to a .csv that includes",0
frame times.  This is very bespoke to animal detection and does not include other classes.,0
,0
%% Imports and constants,0
Only necessary if you want to extract the sample rate from the video,0
%% Extract the sample rate if necessary,0
%% Load results,0
%% Convert to .csv,0
i_image = 0; im = results['images'][i_image],0
,0
umn-pr-analysis.py,0
,0
Precision/recall analysis for UMN data,0
,0
%% Imports and constants,0
results_file = results_file_filtered,0
"For two deployments, we're only processing imagse in the ""detections"" subfolder",0
String to remove from MegaDetector results,0
%% Enumerate deployment folders,0
%% Load MD results,0
im = md_results['images'][0],0
%% Load ground truth,0
i_row = 0; row = ground_truth_df.iloc[i_row],0
%% Create relative paths for ground truth data,0
"Some deployment folders have no subfolders, e.g. this is a valid file name:",0
,0
M00/01010132.JPG,0
,0
"But some deployment folders have subfolders, e.g. this is also a valid file name:",0
,0
N17/100EK113/07160020.JPG,0
,0
"So we can't find files by just concatenating folder and file names, we have to enumerate and explicitly",0
"map what will appear in the ground truth as ""folder/filename"" to complete relative paths.",0
deployment_name = list(deployment_folders)[0],0
Enumerate all files in this folder,0
"print('Enumerated {} files for deployment {}'.format(len(files),deployment_name))",0
filename = files[100],0
...for each file in this deployment,0
...for each deployment,0
%% Map relative paths to MD results,0
%% Add relative paths to our ground truth table,0
i_row = 0; row = ground_truth_df.iloc[i_row],0
"row['filename'] looks like, e.g. A01/01080001.JPG.  This is not actually a path, it's",0
"just the deployment ID and the image name, separated by a slash.",0
Find the relative path for this image,0
Make sure we have MegaDetector results for this file,0
Make sure this image file exists,0
...for each row in the ground truth table,0
%% Take everything out of Pandas,0
%% Some additional error-checking of the ground truth,0
An early version of the data required consistency between common_name and is_blank,0
%% Combine MD and ground truth results,0
d = ground_truth_dicts[0],0
Find the maximum confidence for each category,0
...for each detection,0
...for each image,0
%% Precision/recall analysis,0
...for each image,0
Thresholds go up throughout precisions/recalls/thresholds; find the last,0
value where recall is at or above target.  That's our precision @ target recall.,0
Flatten the confusion matrix,0
Write precision/recall plot to .png file in output directory,0
pr_figure_relative_filename = 'prec_recall.png',0
"pr_figure_filename = os.path.join(output_dir, pr_figure_relative_filename)",0
plt.show(block=False),0
"precision_recall_analysis(human_gt_labels,human_prediction_probs,'humans')",0
%% Find and manually review all images of humans,0
%%,0
"...if this image is annotated as ""human""",0
...for each image,0
%% Find and manually review all MegaDetector animal misses,0
%%,0
im = merged_images[0],0
GT says this is not an animal,0
GT says this is an animal,0
%% Convert .json to .csv,0
%%,0
,0
kga-pr-analysis.py,0
,0
Precision/recall analysis for KGA data,0
,0
%% Imports and constants,0
%% Load and filter MD results,0
%% Load and filter ground truth,0
%% Map images to image-level results,0
%% Map sequence IDs to images and annotations to images,0
Verify consistency of annotation within a sequence,0
TODO,1
%% Find max confidence values for each category for each sequence,0
seq_id = list(sequence_id_to_images.keys())[1000],0
im = images_this_sequence[0],0
det = md_result['detections'][],0
...for each detection,0
...for each image in this sequence,0
...for each sequence,0
%% Prepare for precision/recall analysis,0
seq_id = list(sequence_id_to_images.keys())[1000],0
cat_id = list(category_ids_this_sequence)[0],0
...for each category in this sequence,0
...for each sequence,0
%% Precision/recall analysis,0
"Confirm that thresholds are increasing, recall is decreasing",0
This is not necessarily true,0
assert np.all(precisions[:-1] <= precisions[1:]),0
Thresholds go up throughout precisions/recalls/thresholds; find the max,0
value where recall is at or above target.  That's our precision @ target recall.,0
"This is very slightly optimistic in its handling of non-monotonic recall curves,",0
but is an easy scheme to deal with.,0
Flatten the confusion matrix,0
Write precision/recall plot to .png file in output directory,0
pr_figure_relative_filename = 'prec_recall.png',0
"pr_figure_filename = os.path.join(output_dir, pr_figure_relative_filename)",0
plt.show(block=False),0
"precision_recall_analysis(human_gt_labels,human_prediction_probs,'humans')",0
%% Scrap,0
%% Find and manually review all sequence-level MegaDetector animal misses,0
input_base = '/media/user/lila-01/lila/snapshot-safari/KGA/KGA_public',0
i_sequence = 0; seq_id = pr_sequence_ids[i_sequence],0
i_seq = 0; seq_id = false_negative_sequences[i_seq],0
"sequence_folder = os.path.join(sequence_preview_dir,'seq_{}'.format(str(i_seq).zfill(3)))",0
fn = image_files[0],0
"print('Copying {} to {}'.format(input_path,output_path))",0
...for each file in this sequence.,0
...for each sequence,0
%% Image-level postprocessing,0
parse arguments,0
check if a GPU is available,0
load a pretrained embedding model,0
setup experiment,0
load the embedding model,0
setup the target dataset,0
setup finetuning criterion,0
setup an active learning environment,0
create a classifier,0
the main active learning loop,0
Active Learning,0
finetune the embedding model and load new embedding values,0
gather labeled pool and train the classifier,0
save a snapshot,0
Load a checkpoint if necessary,0
setup the training dataset and the validation dataset,0
setup data loaders,0
check if a GPU is available,0
create a model,0
setup loss criterion,0
define optimizer,0
load a checkpoint if provided,0
setup a deep learning engine and start running,0
train the model,0
train for one epoch,0
evaluate on validation set,0
save a checkpoint,0
open the file,0
read it,0
do the substitution,0
matplotlib.use('Agg'),0
from UIComponents.DBObjects import *,0
Initialize Database,0
# database connection credentials,0
# try to connect as USER to database DB_NAME through peewee,0
Load the saved embedding model,0
dataset_query = Detection.select().limit(5),0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})",0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getalllabels()) , dataset.getallpaths(), {})",0
Random examples to start,0
"random_ids = np.random.choice(dataset.current_set, 1000, replace=False).tolist()",0
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)",0
print(random_ids),0
Move Records,0
"moveRecords(dataset, DetectionKind.ModelDetection.value, DetectionKind.UserDetection.value, random_ids)",0
#print([len(x) for x in dataset.set_indices]),0
# Finetune the embedding model,0
#dataset.set_kind(DetectionKind.UserDetection.value),0
#dataset.train(),0
"#train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)",0
"#finetune_embedding(model, checkpoint['loss_type'], dataset, 32, 4, 100)",0
#save_checkpoint({,0
"#        'arch': model.arch,",0
"#        'state_dict': model.state_dict(),",0
"#        'optimizer' : optimizer.state_dict(),",0
"#        'loss_type' : loss_type,",0
"#        }, False, ""%s%s_%s_%04d.tar""%('finetuned', loss_type, model.arch, len(dataset.set_indices[DetectionKind.UserDetection.value])))",0
Get indices of samples to get user to label,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
"kwargs[""already_selected""].extend(indices)",0
Train on samples that have been labeled so far,0
Test on the samples that have not been labeled,0
"'optimizer' : optimizer.state_dict(),",0
num_classes= len(train_dataset.getClassesInfo()[0]),0
"print(""Num Classes= ""+str(num_classes))",0
define loss function (criterion) and optimizer,0
"optimizer = torch.optim.SGD(params, momentum = 0.9, lr = args.lr, weight_decay = args.weight_decay)",0
optimizer.load_state_dict(checkpoint['optimizer']),0
train for one epoch,0
"adjust_lr(optimizer,epoch)",0
if epoch % 1 == 0 and epoch > 0:,0
"a, b, c = e.predict(train_embd_loader, load_info = True, dim = args.feat_dim)",0
"plot_embedding(reduce_dimensionality(a), b, c, {})",0
evaluate on validation set,0
--------some stuff needed to get AJAX to work with bottle?--------#,0
-------------------------------------------------------------------------------- #,0
PREPARE TO QUEUE IMAGES FOR LABELING,0
-------------------------------------------------------------------------------- #,0
# Connect as USER to database DB_NAME through peewee and initialize database proxy,0
# Load embedding model,0
---------------------------------------------------------------------- #,0
CREATE QUEUE OF IMAGES TO LABEL,0
---------------------------------------------------------------------- #,0
Use classifier to generate predictions,0
# Update model predicted class in PostgreSQL database,0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?,0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
-------------------------------------------------------------------------------- #,0
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI,0
-------------------------------------------------------------------------------- #,0
"# static routes (to serve CSS, etc.)",0
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps",0
"return bottle.static_file(filename, root='../../../../../../../../../../../.')",0
# dynamic routes,0
if data['display_grayscale']:,0
indices_to_exclude.update(set(color_indices)),0
elif not data['display_grayscale']:,0
indices_to_exclude.update(set(grayscale_indices)),0
data['display_images'] = {},0
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices],0
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices],0
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices],0
data['display_images']['detection_categories'] = [],0
for i in indices:,0
if str(dataset.samples[i][1]) == 'None':,0
data['display_images']['detection_categories'].append('None'),0
else:,0
existing_category_entries = {cat.id: cat.name for cat in Category.select()},0
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()",0
data['display_images']['detection_categories'].append(cat_name),0
Use image ids in images_to_label to get the corresponding dataset indices,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
print(set(dataset.set_indices[4]).update(set(indices_to_label))),0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Use image id images_to_label to get the corresponding dataset index,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?,0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Train on samples that have been labeled so far,0
dataset.set_kind(DetectionKind.UserDetection.value),0
print(y_train),0
Predict on the samples that have not been labeled,0
print(y_pred),0
Update model predicted class in PostgreSQL database,0
timer = time.time(),0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
print('Updating the database took %0.2f seconds'%(time.time() - timer)),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids)),0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
"once the classifier has been trained the first time, switch to AL sampling",0
# sourceMappingURL=bootstrap.bundle.min.js.map,0
"print(ap_distances.size(),an_distances.size())",0
losses = -(((-ap_distances)/128)+1+1e-16).log() - (((-(128-an_distances))/128)+1+1e-16).log(),0
import pdb,0
pdb.set_trace(),0
losses = ap_distances - an_distances + self.margin,0
print(losses.size()),0
from UIComponents.DBObjects import *,0
TODO: should this also change self.kind?,1
get the embedding representations for all samples (i.e. set current_set to all indices),0
"return PILImage.open(os.path.join(self.img_base,path)).convert('RGB')",0
"print(self.labels_set, self.n_classes)",0
from sklearn.manifold import TSNE,0
embedding= TSNE(n_components=2).fit_transform(X),0
embedding= PCA(n_components=2).fit_transform(X),0
return X,0
"print(dir(event), type(sc))",0
"print(label,bgcolor)",0
"patches.append(mpatches.Patch(color=indexcolors[i], label=label))",0
"plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,",0
"ncol=12, mode=""expand"", borderaxespad=0., handles=patches)",0
plt.legend(handles=patches),0
"plt.xlabel('Dim 1', fontsize=12)",0
"plt.ylabel('Dim 2', fontsize=12)",0
plt.grid(True),0
"plt.xlabel('Dim 1', fontsize=12)",0
"plt.ylabel('Dim 2', fontsize=12)",0
plt.grid(True),0
print(thumb),0
"img.thumbnail((16, 12), PILImage.ANTIALIAS)",0
plt.show(),0
Add all negatives for all positive pairs,0
print(triplets.shape[0]),0
compute output,0
measure accuracy,0
compute loss on this batch,0
"train on a batch, record loss, and measure accuracy (if calc_accuracy)",0
compute output,0
measure accuracy and record loss,0
switch to evaluate mode,0
compute output,0
switch to evaluate mode,0
"self.fc13 = nn.Linear(128, 64)",0
self.bn2 = nn.BatchNorm1d(64),0
x = F.relu(self.fc12(x)),0
x = F.relu(self.bn1(self.fc13(x))),0
x = F.relu(self.fc13(x)),0
"x = F.dropout(x, training=self.training)",0
save features last FC layer,0
x = F.relu(x),0
save features last FC layer,0
import matplotlib.pyplot as plt,0
plt.switch_backend('agg'),0
class EmbeddingNet(nn.Module):,0
"def __init__(self, architecture, feat_dim, use_pretrained=False):",0
"super(EmbeddingNet, self).__init__()",0
self.feat_dim= feat_dim,0
self.inner_model = models.__dict__[architecture](pretrained=use_pretrained),0
if architecture.startswith('resnet'):,0
in_feats= self.inner_model.fc.in_features,0
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)",0
elif architecture.startswith('inception'):,0
in_feats= self.inner_model.fc.in_features,0
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)",0
if architecture.startswith('densenet'):,0
in_feats= self.inner_model.classifier.in_features,0
"self.inner_model.classifier = nn.Linear(in_feats, feat_dim)",0
if architecture.startswith('vgg'):,0
in_feats= self.inner_model.classifier._modules['6'].in_features,0
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)",0
if architecture.startswith('alexnet'):,0
in_feats= self.inner_model.classifier._modules['6'].in_features,0
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)",0
"def forward(self, x):",0
return self.inner_model.forward(x),0
class NormalizedEmbeddingNet(EmbeddingNet):,0
"def __init__(self, architecture, feat_dim, use_pretrained=False):",0
"EmbeddingNet.__init__(self, architecture, feat_dim, use_pretrained = use_pretrained)",0
"def forward(self, x):",0
embedding =  F.normalize(self.inner_model.forward(x))*10.0,0
"return embedding, embedding",0
"def get_random_images(num, image_dir, test_transforms):",0
"data = datasets.ImageFolder(image_dir, transform=test_transforms) # slight abuse; this expects subfolders corresponding to classes but we have no classes here",0
indices = list(range(len(data))),0
np.random.shuffle(indices),0
idx = indices[:num],0
from torch.utils.data.sampler import SubsetRandomSampler,0
sampler = SubsetRandomSampler(idx),0
"loader = torch.utils.data.DataLoader(data,",0
"sampler=sampler, batch_size=num)",0
dataiter = iter(loader),0
"images, labels = dataiter.next()",0
"return images, labels",0
"def predict_image(image, model, test_transforms):",0
"device = torch.device(""cuda"" if torch.cuda.is_available()",0
"else ""cpu"")",0
image_tensor = test_transforms(image).float(),0
image_tensor = image_tensor.unsqueeze_(0),0
input = Variable(image_tensor),0
input = input.to(device),0
output = model(input)[0],0
return output.data.cpu().numpy(),0
Connect to database and initialize db_proxy,0
# database connection credentials,0
# load the dataset,0
Load the saved embedding model from the checkpoint,0
# update the dataset embedding,0
# Create a folder for saving embedding visualizations with this model checkpoint,0
model_emb_dirname = os.path.basename(args.base_model).split('.')[0],0
"os.makedirs(model_emb_dirname, exist_ok=True)",0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')",0
dataset.embedding_mode(),0
"assert 2==3, 'break'",0
datasetindices = list(range(len(dataset))),0
np.random.shuffle(datasetindices),0
random_indices = datasetindices[:args.num],0
print(random_indices),0
"selected_sample_features = np.array([]).reshape(0, 256)",0
selected_sample_labels = [],0
for idx in random_indices:,0
"selected_sample_features = np.vstack([selected_sample_features, X_train[idx]])",0
selected_sample_labels.append(y_train[idx]),0
img_path = imagepaths[idx].split('.JPG')[0],0
image = dataset.loader(img_path),0
selected_sample_images.append(image),0
# TRY NEAREST NEIGHBORS WALK THROUGH EMBEDDING,0
nbrs = NearestNeighbors(n_neighbors=args.num).fit(selected_sample_features),0
"distances, indices = nbrs.kneighbors(selected_sample_features)",0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')",0
"idx_w_closest_nbr = np.where(distances[:,1] == min(distances[:,1]))[0][0]",0
order = [idx_w_closest_nbr],0
for ii in range(len(distances)):,0
"distances[ii, 0] = np.inf",0
while len(order)<args.num:,0
curr_idx = order[-1],0
curr_neighbors = indices[curr_idx],0
curr_dists = list(distances[curr_idx]),0
# print(min(curr_dists)),0
next_closest_pos = curr_dists.index(min(curr_dists)),0
next_closest = curr_neighbors[next_closest_pos],0
order.append(next_closest),0
# make sure you can't revisit past nodes,0
for vi in order:,0
vi_pos = list(indices[next_closest]).index(vi),0
"distances[next_closest, vi_pos] = np.inf",0
for ii in range(len(order)):,0
imgidx = order[ii],0
image = selected_sample_images[imgidx],0
"image.save(model_emb_dirname+""/img""+str(ii)+""_""+str(selected_sample_labels[imgidx])+"".png"")",0
# Specify the transformations on the input images before inference,0
"# test_transforms = transforms.Compose([transforms.Resize([224, 224]), transforms.ToTensor()])",0
"test_transforms = transforms.Compose([transforms.Resize([256, 256]), transforms.RandomCrop([224, 224]), transforms.RandomHorizontalFlip(), transforms.ColorJitter(), transforms.ToTensor(), transforms.Normalize([0.407328, 0.407328, 0.407328], [0.118641, 0.118641, 0.118641])])",0
"images, labels = get_random_images(num, image_dir, test_transforms)",0
"all_features = np.array([]).reshape(0, 256)",0
for ii in range(len(images)):,0
image = to_pil(images[ii]),0
"features = predict_image(image, model, test_transforms)",0
"all_features = np.vstack([all_features, features])",0
# for ii in range(len(images)):,0
#     image = to_pil(images[ii]),0
"#     image.save(""img""+str(ii)+"".png"")",0
# TRY CLUSTERING,0
kmeans1 = KMeans(n_clusters=5).fit(StandardScaler().fit_transform(all_features)),0
print(kmeans1.labels_),0
for ii in range(len(images)):,0
image = to_pil(images[ii]),0
"filename = str(kmeans1.labels_[ii])+""/img""+str(ii)+"".png""",0
if not os.path.exists(os.path.dirname(filename)):,0
os.makedirs(os.path.dirname(filename)),0
image.save(filename),0
class SaveFeatures():,0
"def __init__(self, module):",0
self.hook = module.register_forward_hook(self.hook_fn),0
"def hook_fn(self, module, input, output):",0
"self.features = torch.tensor(output, requires_grad=True).cuda()",0
def close(self):,0
self.hook.remove(),0
Load the saved embedding model from the checkpoint,0
"Get a sample from the database, with eval transforms applied, etc.",0
Connect to database and sample a dataset,0
output = model.forward(sample_image.unsqueeze(0)),0
print(output),0
with torch.no_grad():,0
sample_image_input = sample_image.cuda(non_blocking=True),0
"_, output = model(sample_image_input) # compute output",0
print(output),0
sample_image = PILImage.open(sample_image_path).convert('RGB'),0
"sample_image = transforms.Compose([Resize([256, 256]), CenterCrop(([[224,224]])), ToTensor(), Normalize([0.369875, 0.388726, 0.347536], [0.136821, 0.143952, 0.145229])])(sample_image)",0
print(list(model_inner_resnet.children())),0
print(model_inner_resnet.fc),0
print(model_inner_resnet.fc0),0
# print(model_inner_resnet.layer4[0].conv2),0
# print(type(model)),0
# print(len(list(model_inner_resnet.children()))),0
# print(list(model.children())),0
# print(list(list(model.children())[0].children())),0
"img = np.uint8(np.random.uniform(150, 180, (56, 56, 3)))/255",0
"img_tensor = torch.unsqueeze(torch.from_numpy(img), 0)",0
full_out = model_inner_resnet.forward(img_tensor),0
print(full_out),0
model(img_tensor),0
activations = SaveFeatures(model_inner_resnet.layer4[0].conv2),0
print(activations.features),0
print(type(activations.features)),0
activations.close(),0
Connect to database and sample a dataset,0
Load the saved embedding model from the checkpoint,0
Update the dataset embedding,0
Get a random query image,0
# # # IMAGES IN THE SAME SEQUENCE # # # #,0
"assert 2==3, 'break'",0
# # # CLOSEST IN (EMBEDDING) FEATURE SPACE # # # #,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Variables for the hierarchical cluster,0
Variables for the AL algorithm,0
Data variables,0
"connectivity = kneighbors_graph(self.transformed_X,max_features)",0
Fit cluster and update cluster variables,0
The sklearn hierarchical clustering algo numbers leaves which correspond,0
to actual datapoints 0 to n_points - 1 and all internal nodes have,0
ids greater than n_points - 1 with the root having the highest node id,0
"If no labels have been observed, simply return uniform distribution",0
"If no observations, return worst possible upper lower bounds",0
Loop through generations from bottom to top,0
Update admissible labels for node,0
Calculate score,0
Determine if node should be split,0
Make sure label set for node so that we can flow to children,0
if necessary,0
Only split if all ancestors are admissible nodes,0
This is part  of definition of admissible pruning,0
Check that pruning covers all leave nodes,0
Fill in labels,0
Observe labels for previously recommended batches,0
TODO(lishal): implement multiple selection methods,1
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
A list of initialized samplers is allowed as an input because,0
"for AL_methods that search over different mixtures, may want mixtures to",0
have shared AL_methods so that initialization is only performed once for,0
computation intensive methods like HierarchicalClusteringAL and,0
states are shared between mixtures.,0
"If initialized samplers are not provided, initialize them ourselves.",0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copy these objects to make sure they are not modified while simulating,0
trajectories as they are used later by the main run_experiment script.,0
Assumes that model has already by fit using all labeled data so,0
the probabilities can be used immediately to hallucinate labels,0
All models need to have predict_proba method,0
Hallucinate labels for selected datapoints to be label,0
using class probabilities from model,0
"Not saving already_selected here, if saving then should sort",0
only for the input to fit but preserve ordering of indices in,0
already_selected,0
Useful to know how accuracy compares for model trained on hallucinated,0
labels vs trained on true labels.  But can remove this train to speed,0
up simulations.  Won't speed up significantly since many more models,0
are being trained inside the loop above.,0
Save trajectory for reference,0
Delete created copies,0
THE INPUTS CANNOT BE MODIFIED SO WE MAKE COPIES FOR THE CHECK LATER,0
Should check model but kernel_svm does not have coef_ so need better,0
handling here,0
Make sure that model object fed in did not change during simulations,0
Return indices based on return type specified,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Set gamma for gaussian kernel to be equal to 1/n_features,0
kneighbors graph is constructed using k=10,0
"Make connectivity matrix symmetric, if a point is a k nearest neighbor of",0
"another point, make it vice versa",0
Graph edges are weighted by applying gaussian kernel to manhattan dist.,0
"By default, gamma for rbf kernel is equal to 1/n_features but may",0
get better results if gamma is tuned.,0
Define graph density for an observation to be sum of weights for all,0
edges to the node representing the datapoint.  Normalize sum weights,0
by total number of neighbors.,0
"If a neighbor has already been sampled, reduce the graph density",0
for its direct neighbors to promote diversity.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Update observed reward and arm probabilities,0
Sample an arm,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
y only used for determining how many clusters there should be,0
probably not practical to assume we know # of classes before hand,0
should also probably scale with dimensionality of data,0
Probably okay to always use MiniBatchKMeans,0
Should standardize data before clustering,0
Can cluster on standardized data but train on raw features if desired,0
TODO(lishal): have MarginSampler and this share margin function,1
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Probably okay to always use MiniBatchKMeans,0
Should standardize data before clustering,0
Can cluster on standardized data but train on raw features if desired,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Update min_distances for all examples given new cluster center.,0
Assumes that the transform function takes in original data and not,0
flattened data.,0
Initialize centers with a randomly selected datapoint,0
New examples should not be in already selected since those points,0
should have min_distance of zero to a cluster center.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
This is uniform given the remaining pool but biased wrt the entire pool.,0
sample = [i for i in range(self.X.shape[0]) if i not in already_selected],0
return sample[0:N],0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Fields for hierarchical clustering AL,0
Setting parent and storing nodes in dict for fast access,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Connect to database DB_NAME as USER and initialize tables,0
Populate Info table,0
Populate Category table,0
"# For now, we have a predefined list of species we expect to see in the camera trap database (e.g. maybe from a quick look through the images)",0
# TODO: allow user to update the class list through the labeling tool UI as they see different species,1
Populate Image and Detection tables,0
Image entry data,0
Detection entry data,0
Connect to database DB_NAME as USER and initialize tables,0
Populate Info table,0
Populate Category table,0
# Create category records from species present in the COCO camera trap classification dataset (COCO classes minus any excluded classes),0
# TODO: allow user to update the class list through the labeling tool UI as they see different species,1
Populate Image and Detection tables,0
"with open(os.path.join(args.crop_dir,'crops.json'), 'r') as infile:",0
crops_json = json.load(infile),0
counter = 0,0
timer = time.time(),0
num_detections = len(crops_json),0
for detectionid in crops_json:,0
counter += 1,0
detection_data = crops_json[detectionid],0
# Image entry data,0
existing_image_entries = Image.select().where((Image.file_name == detection_data['file_name'])),0
try:,0
existing_image_entry = existing_image_entries.get(),0
except:,0
"image_entry = Image.create(id=detectionid, file_name=detection_data['file_name'], width=detection_data['width'], height=detection_data['height'], grayscale=detection_data['grayscale'],",0
"source_file_name=detection_data['source_file_name'], relative_size=detection_data['relative_size'],",0
"seq_id=detection_data['seq_id'], seq_num_frames=detection_data['seq_num_frames'], frame_num=detection_data['frame_num'])",0
image_entry.save(),0
# Detection entry data,0
"detection_entry = Detection.create(id=detectionid, image=detectionid, bbox_confidence=detection_data['bbox_confidence'],",0
"bbox_X1=detection_data['bbox_X1'], bbox_Y1=detection_data['bbox_Y1'], bbox_X2=detection_data['bbox_X2'], bbox_Y2=detection_data['bbox_Y2'],",0
kind=DetectionKind.ModelDetection.value),0
detection_entry.save(),0
if counter%100 == 0:,0
"print('Updated database with Image and Detection table entries for %d out of %d crops in %0.2f seconds'%(counter, num_detections, time.time() - timer))",0
# data related to original image,0
Get class names from .txt list,0
Initialize Oracle table,0
Map filenames to classes (NOTE: we assume a single image does not contain more than one class),0
"For each detection, use source image path to get class",0
TODO update: Assumes that crops have already,1
been generated for the images using make_active_learning_classification_dataset.py. The created DB contains tables:,0
- info: information about the dataset,0
- image: images present in the dataset,0
- detections: crops of images with detections with confidence greater than a specified threshold,0
Initialize Database,0
# database connection credentials,0
HOST = 'localhost',0
PORT = 5432,0
"# first, make sure the (user, password) has been created",0
"# sudo -u postgres psql -c ""CREATE USER <db_user> WITH PASSWORD <db_password>;""",0
"# sudo -u postgres psql -c ""CREATE DATABASE <db_name> WITH OWNER <db_user> CONNECTION LIMIT -1;""",0
"# sudo -u postgres psql -c ""GRANT CONNECT ON DATABASE <db_name> TO <db_user>;""",0
"# sudo -u postgres psql -d <db_name> -c ""CREATE EXTENSION IF NOT EXISTS \""uuid-ossp\"";""",0
# Try to connect as USER to database DB_NAME through peewee,0
Populate Tables,0
# create Info table,0
# get class names for Category table,0
Faster anD available in Python 3.5 and above,0
# iterate through images in each class folder,0
killing this process after over 38 hours adding over 500k white-tailed deer crops from emammal,0
resuming for remaining classes,0
# get cropped image data for Image table,0
"if mean of each channel is about the same, image is likely grayscale",0
# still have no info on these:,0
seq_id = CharField(null= True)                # sequence identifier for the original image,0
seq_num_frames = IntegerField(null = True)    # number of frames in sequence,0
frame_num = IntegerField(null = True)         # which frame number in sequence,0
location = CharField(null = True)             # location of camera trap,0
datetime = DateTimeField(null = True),0
# store info about the detection corresponding to this image,0
# store info about the true labels for the detection,0
#  - for pretrain dataset this is the same as the detection_category if the detection categories,0
print(classes),0
Connect to database and sample a dataset,0
Load the saved embedding model from the checkpoint,0
Update the dataset embedding,0
save the images,0
save the features,0
"with open(os.path.join(args.output_dir, 'lastlayer_features.mat'), 'wb') as f:",0
"pickle.dump(sample_features, f)",0
"with open(os.path.join(args.output_dir, 'labels.mat'), 'wb') as f:",0
"pickle.dump(sample_labels, f)",0
"parser.add_argument('--db_name', default='missouricameratraps', type=str, help='Name of the training (target) data Postgres DB.')",0
"parser.add_argument('--db_user', default='user', type=str, help='Name of the user accessing the Postgres DB.')",0
"parser.add_argument('--db_password', default='password', type=str, help='Password of the user accessing the Postgres DB.')",0
"parser.add_argument('--base_model', type=str, help='Path to latest embedding model checkpoint.')",0
"parser.add_argument('--output_dir', type=str, help='Output directory for subset of crops')",0
Add json entry for this crop,0
Copy file for this crop to subset dataset crop dir,0
Copy file for its full-size source image to subset dataset image dir,0
Write crops.json to subset dataset crop dir,0
store info about the crops produced in a JSON file,0
------------------------------------------------------------------------------------------------------------#,0
COMMENT OUT IF NOT USING A SPECIFIC PROJECT WITHIN ROBERT LONG EMAMMAL DATASET,0
------------------------------------------------------------------------------------------------------------#,0
get some information about the source image,0
------------------------------------------------------------------------------------------------------------#,0
NOTE: EDIT THIS SECTION BASED ON DATASET SOURCE,0
get info about sequence the source image belongs to from path and directory,0
# missouricameratraps:,0
imgframenum = int(os.path.basename(imgfile).split('.JPG')[0].split('_')[-1]),0
imgseqid = int(os.path.split(os.path.dirname(imgfile))[-1]),0
"imgseqnumframes = len([name for name in os.listdir(os.path.dirname(imgfile)) if os.path.isfile(os.path.join(os.path.dirname(imgfile), name))])",0
# emammal:,0
------------------------------------------------------------------------------------------------------------#,0
utility function,0
compute output,0
measure accuracy and record loss,0
switch to train mode,0
measure accuracy and record loss,0
compute output,0
measure accuracy and record loss,0
switch to evaluate mode,0
compute output,0
switch to evaluate mode,0
compute output,0
switch to evaluate mode,0
"print(self.labels_set, self.n_classes)",0
Add all negatives for all positive pairs,0
print(triplets.shape[0]),0
constructor,0
update embedding values after a finetuning,0
select either the default or active pools,0
gather test set,0
gather train set,0
finetune the embedding model over the labeled pool,0
a utility function for saving the snapshot,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Variables for the hierarchical cluster,0
Variables for the AL algorithm,0
Data variables,0
"connectivity = kneighbors_graph(self.transformed_X,max_features)",0
Fit cluster and update cluster variables,0
The sklearn hierarchical clustering algo numbers leaves which correspond,0
to actual datapoints 0 to n_points - 1 and all internal nodes have,0
ids greater than n_points - 1 with the root having the highest node id,0
"If no labels have been observed, simply return uniform distribution",0
"If no observations, return worst possible upper lower bounds",0
Loop through generations from bottom to top,0
Update admissible labels for node,0
Calculate score,0
Determine if node should be split,0
Make sure label set for node so that we can flow to children,0
if necessary,0
Only split if all ancestors are admissible nodes,0
This is part  of definition of admissible pruning,0
Check that pruning covers all leave nodes,0
Fill in labels,0
Observe labels for previously recommended batches,0
TODO(lishal): implement multiple selection methods,1
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
A list of initialized samplers is allowed as an input because,0
"for AL_methods that search over different mixtures, may want mixtures to",0
have shared AL_methods so that initialization is only performed once for,0
computation intensive methods like HierarchicalClusteringAL and,0
states are shared between mixtures.,0
"If initialized samplers are not provided, initialize them ourselves.",0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copy these objects to make sure they are not modified while simulating,0
trajectories as they are used later by the main run_experiment script.,0
Assumes that model has already by fit using all labeled data so,0
the probabilities can be used immediately to hallucinate labels,0
All models need to have predict_proba method,0
Hallucinate labels for selected datapoints to be label,0
using class probabilities from model,0
"Not saving already_selected here, if saving then should sort",0
only for the input to fit but preserve ordering of indices in,0
already_selected,0
Useful to know how accuracy compares for model trained on hallucinated,0
labels vs trained on true labels.  But can remove this train to speed,0
up simulations.  Won't speed up significantly since many more models,0
are being trained inside the loop above.,0
Save trajectory for reference,0
Delete created copies,0
THE INPUTS CANNOT BE MODIFIED SO WE MAKE COPIES FOR THE CHECK LATER,0
Should check model but kernel_svm does not have coef_ so need better,0
handling here,0
Make sure that model object fed in did not change during simulations,0
Return indices based on return type specified,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Set gamma for gaussian kernel to be equal to 1/n_features,0
kneighbors graph is constructed using k=10,0
"Make connectivity matrix symmetric, if a point is a k nearest neighbor of",0
"another point, make it vice versa",0
Graph edges are weighted by applying gaussian kernel to manhattan dist.,0
"By default, gamma for rbf kernel is equal to 1/n_features but may",0
get better results if gamma is tuned.,0
Define graph density for an observation to be sum of weights for all,0
edges to the node representing the datapoint.  Normalize sum weights,0
by total number of neighbors.,0
"If a neighbor has already been sampled, reduce the graph density",0
for its direct neighbors to promote diversity.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Update observed reward and arm probabilities,0
Sample an arm,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
y only used for determining how many clusters there should be,0
probably not practical to assume we know # of classes before hand,0
should also probably scale with dimensionality of data,0
Probably okay to always use MiniBatchKMeans,0
Should standardize data before clustering,0
Can cluster on standardized data but train on raw features if desired,0
TODO(lishal): have MarginSampler and this share margin function,1
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Probably okay to always use MiniBatchKMeans,0
Should standardize data before clustering,0
Can cluster on standardized data but train on raw features if desired,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Update min_distances for all examples given new cluster center.,0
Assumes that the transform function takes in original data and not,0
flattened data.,0
Initialize centers with a randomly selected datapoint,0
New examples should not be in already selected since those points,0
should have min_distance of zero to a cluster center.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
This is uniform given the remaining pool but biased wrt the entire pool.,0
sample = [i for i in range(self.X.shape[0]) if i not in already_selected],0
return sample[0:N],0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Fields for hierarchical clustering AL,0
Setting parent and storing nodes in dict for fast access,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Faster and available in Python 3.5 and above,0
transform_list.append(CenterCrop((processed_size))),0
transform_list.append(Lambda(lambda X: normalize(X))),0
"print(self.labels_set, self.n_classes)",0
"from PyQt5 import QtCore, QtWidgets,QtGui",0
from collections import deque,0
from peewee import *,0
from UIComponents.Tag import Tag,0
policy.setHeightForWidth(True),0
"print(self.tab1.parentWidget(),self)",0
self.tab4.add.clicked.connect(self.addSpecies),0
self.tab4.update.clicked.connect(self.updateSpecies),0
checkpoint= load_checkpoint('../merge/triplet_model_0054.tar'),0
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))",0
"embedding_net = EmbeddingNet(checkpoint['arch'], checkpoint['feat_dim'])",0
if checkpoint['loss_type'].lower()=='center':,0
"model = torch.nn.DataParallel(ClassificationNet(embedding_net, n_classes=14)).cuda()",0
else:,0
model= torch.nn.DataParallel(embedding_net).cuda(),0
model.load_state_dict(checkpoint['state_dict']),0
self.parentWidget().progressBar.setMaximum(len(run_dataset)//2048),0
"e=Engine(model,None,None, verbose=True,progressBar= self.parentWidget().progressBar)",0
label = [ x[1] for x in run_dataset.samples],0
"print(indices,selected_set)",0
print(query.sql()),0
src.delete().where(src.image_id<<rList)),0
"det= UserDetection.create(category_id=0, id=str(index+label[1][2]),image_id=final[0], bbox_X=label[1][0], bbox_Y=label[1][1], bbox_W=label[1][2], bbox_H=label[1][3])",0
for x in self.tab1.grid.tags:,0
x.delete_instance(),0
db.create_tables([Detection]),0
This is simply to show the bar,0
"p = Process(target=ex.active, args=())",0
p.start(),0
p.join(),0
ex.active(),0
ex.centralWidget().setCurrentIndex(1),0
main(),0
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))",0
print(row),0
"unq_id= ""crops_""+str(uuid.uuid1())",0
"print(line,imageWidth,imageHeight)",0
"print(""%s,%s,%s,%0.6f,%0.6f,%0.6f,%0.6f,%0.6f""%(line[0], line[1],line[2],float(line[3]),topRel,leftRel,bottomRel,rightRel))",0
if not os.path.exists(dest):,0
os.mkdir(dest),0
raise,0
out.close(),0
"print length,(i-1)*length,i*length",0
matplotlib.use('Agg'),0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
"conf.reset()""""""",0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
conf.reset(),0
for clf in classifiers:,0
"clf.fit(X_train, y_train)",0
preds= clf.predict_proba(X),0
uncertainty+= preds.max(axis=1),0
print(uncertainty[ind]),0
print(uncertainty[ind]),0
true_labels = y[unlabeled_indices],0
select up to 5 digit examples that the classifier is most uncertain about,0
remember best acc@1 and save checkpoint,0
"completeClassificationLoop(run_dataset, model,num_classes)",0
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)",0
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd),0
labels = db.labels_,0
"mapp=(find_probablemap(label,labels, K=args.K))",0
"print(""Clusters"")",0
"for i,x in enumerate(labels):",0
labels[i]= mapp[x],0
print(np.sum(labels == label)/labels.size),0
"print(""Confidence Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning(embd, label, idx)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(""Entropy Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning_entropy(embd, label, idx)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(idx,idx.shape)",0
for i in idx:,0
print(paths[i]),0
"print(""Silohette active learning"")",0
"idx= active_learning2(embd, 1000, args.num_clusters)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
embd= reduce_dimensionality(embd)#[0:10000]),0
labels= labels[0:10000],0
label= label[0:10000],0
paths= paths[0:10000],0
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])",0
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])",0
plt.show(),0
"np.save(args.name_prefix+""_embeddings.npy"",embd)",0
"np.save(args.name_prefix+""_labels.npy"",label)",0
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")",0
del output,0
define loss function (criterion) and optimizer,0
"optimizer = torch.optim.Adam(params, lr=args.lr, weight_decay= args.weight_decay)",0
optimizer.load_state_dict(checkpoint['optimizer']),0
train for one epoch,0
evaluate on validation set,0
matplotlib.use('Agg'),0
selected_set.add(rand_ind[i]),0
"embedding_net = EmbeddingNet('resnet50', 256, True)",0
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)",0
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)",0
print('Embedding Done'),0
sys.stdout.flush(),0
Random examples to start,0
"random_ids = noveltySamples(unlabeled_dataset.em, unlabeled_dataset.getIDs(), 1000)",0
print(random_ids),0
Move Records,0
Finetune the embedding model,0
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)",0
unlabeled_dataset.updateEmbedding(model),0
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})",0
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()",0
matplotlib.use('Agg'),0
"uncertainty= np.apply_along_axis(stats.entropy,1,probs) * (1 - probs.max(axis=1))",0
"copy(paths[srt[i]], ""active"")",0
"plot_together( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()), preds, base_ind, dataset.getpaths(), {})",0
"return np.random.choice(range(0,prob_list[0].shape[0]), 100, replace=False).tolist()",0
selected_set.add(rand_ind[i]),0
"query= Detection.update(kind = destKind.value).where(Detection.id.in_(rList), Detection.kind == srcKind.value)",0
query.execute(),0
"embedding_net = EmbeddingNet('resnet50', 256, True)",0
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)",0
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)",0
print('Embedding Done'),0
sys.stdout.flush(),0
"plot_embedding(dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})",0
Random examples to start,0
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)",0
print(random_ids),0
Move Records,0
Finetune the embedding model,0
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)",0
unlabeled_dataset.updateEmbedding(model),0
"print(dataset.em[dataset.current_set].shape, np.asarray(dataset.getlabels()).shape, len(dataset.getpaths()))",0
"plot_embedding( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})",0
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})",0
train_eval_classifier(),0
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()",0
"names = [""Linear SVM"", ""RBF SVM"", ""Random Forest"", ""Neural Net"", ""Naive Bayes""]",0
"classifiers = [SVC(kernel=""linear"", C=0.025, probability= True, class_weight='balanced'),",0
"SVC(gamma=2, C=1, probability= True, class_weight='balanced'),",0
"RandomForestClassifier(max_depth=None, n_estimators=100, class_weight='balanced'),",0
"MLPClassifier(alpha=1),",0
GaussianNB()],0
estimators= [],0
"for name, clf in zip(names, classifiers):",0
"estimators.append((name, clf))",0
"eclf1 = VotingClassifier(estimators= estimators, voting='hard')",0
"eclf2 = VotingClassifier(estimators= estimators, voting='soft')",0
"names.append(""ensemble hard"")",0
classifiers.append(eclf1),0
"names.append(""ensemble soft"")",0
classifiers.append(eclf2),0
dataset.image_mode(),0
dataset.updateEmbedding(model),0
y_pred= clf.predict(X_test),0
"print(confusion_matrix(y_test, y_pred))",0
paths= dataset.getpaths(),0
"for i, (yp, yt) in enumerate(zip(y_pred, y_test)):",0
if yp != yt:,0
"copy(paths[i],""mistakes"")",0
"print(yt, yp, paths[i],i)",0
"clf_output= clf_e.embedding(eval_loader, dim=48)",0
"self.train_transform = transforms.Compose([Resize(raw_size), CenterCrop((processed_size)), ToTensor(), Normalize(mean, std)])",0
"print(self.labels_set, self.n_classes)",0
"print(line,imageWidth,imageHeight)",0
"print length,(i-1)*length,i*length",0
#########################################################,0
## Configuration,0
#########################################################,0
## The actual code,0
Check arguments,0
Create output directories,0
Padding around the detected objects when cropping,0
1.3 for the cropping during test time and 1.3 for,0
the context that the CNN requires in the left-over,0
image,0
Load a (frozen) Tensorflow model into memory.,0
## Preparations: get all the output tensors,0
The following processing is only for single image,0
Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.,0
Follow the convention by adding back the batch dimension,0
For all images in the image directoryig,0
Load image,0
Run inference,0
"all outputs are float32 numpy arrays, so convert types as appropriate",0
Add detections to the collection,0
Get info about the image,0
Select detections with a confidence larger than DETECTION_CONFIDENCE,0
Get these boxes and convert normalized coordinates to pixel coordinates,0
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes",0
"However, we need to make sure that it box coordinates are still within the image",0
"For each detected bounding box with high confidence, we will",0
crop the image to the padded box and save it,0
generate a unique identifier for the detection,0
"bbox is the detected box, crop_box the padded / enlarged box",0
Add numbering to the original file name if there are multiple boxes,0
The absolute file path where we will store the image,0
"if COCO_OUTPUT_DIR is set, then we will only use the shape",0
of cropped_img in the following code. So instead of reading,0
cropped_img = np.array(Image.open(out_file)),0
we can speed everything up by reading only the size of the image,0
matplotlib.use('Agg'),0
conf= ConfusionMatrix(24),0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
"conf.reset()""""""",0
define loss function (criterion) and optimizer,0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
conf.reset(),0
for clf in classifiers:,0
"clf.fit(X_train, y_train)",0
preds= clf.predict_proba(X),0
uncertainty+= preds.max(axis=1),0
print(uncertainty[ind]),0
print(uncertainty[ind]),0
true_labels = y[unlabeled_indices],0
select up to 5 digit examples that the classifier is most uncertain about,0
"print(indices,selected_set)",0
print(query.sql()),0
remember best acc@1 and save checkpoint,0
"completeClassificationLoop(run_dataset, model,num_classes)",0
"embd, label, paths = extract_embeddings(run_loader, model)",0
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)",0
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd),0
labels = db.labels_,0
"mapp=(find_probablemap(label,labels, K=args.K))",0
"print(""Clusters"")",0
"for i,x in enumerate(labels):",0
labels[i]= mapp[x],0
print(np.sum(labels == label)/labels.size),0
"print(""Confidence Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning(embd, label, idx)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(""Entropy Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning_entropy(embd, label, idx)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"new_selected= selectSamples(embd,paths,3000)",0
"print(idx,idx.shape)",0
for i in idx:,0
print(paths[i]),0
"print(""Silohette active learning"")",0
"idx= active_learning2(embd, 1000, args.num_clusters)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(""Random"")",0
"idx = np.random.choice(np.arange(len(paths)), 1000, replace=False)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
embd= reduce_dimensionality(embd)#[0:10000]),0
labels= labels[0:10000],0
label= label[0:10000],0
paths= paths[0:10000],0
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])",0
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])",0
plt.show(),0
"np.save(args.name_prefix+""_embeddings.npy"",embd)",0
"np.save(args.name_prefix+""_labels.npy"",label)",0
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")",0
combo.setEnabled(not finalized),0
print(self.bbox),0
self.addWidget(self.child),0
Moving container with arrows,0
Left - Bottom,0
Right-Bottom,0
Left-Top,0
Right-Top,0
Left - Bottom,0
Right - Bottom,0
Left - Top,0
Right - Top,0
check cursor horizontal position,0
check cursor vertical position,0
self.resizeEvent=self.onResize,0
"print(""Parent"", parent, parent.width(), parent.height())",0
"self.setGeometry(0,0,410,307)",0
"print(w,h,""w,h"")",0
"print(""final"",tag.getFinal())",0
pass,0
"self.tags.append(TContainer(self,Category.get(-1),[0,0,0.1,0.1],True, Qt.red))",0
label= CharField(),0
fullname=str(self.model),0
"self.name= (fullname[fullname.find("":"")+2:fullname.find("">"")].strip()+'_set').lower()",0
"print(""Parent"", self.parentWidget().width(), self.parentWidget().height())",0
"print(self.model,self.name,query.sql())",0
self.tab4.speciesList.setModel(species),0
"self.tab4.speciesList.setRowHidden(len(species.stringList())-1, True)",0
self.speciesList.itemChanged.connect(self.itemChanged),0
#####,0
,0
video_utils.py,0
,0
"Utilities for splitting, rendering, and assembling videos.",0
,0
#####,0
"%% Constants, imports, environment",0
from ai4eutils,0
%% Path utilities,0
%% Function for rendering frames to video and vice-versa,0
http://tsaith.github.io/combine-images-into-a-video-with-python-3-and-opencv-3.html,0
Determine the width and height from the first image,0
Define the codec and create VideoWriter object,0
"If we're not over-writing, check whether all frame images already exist",0
"OpenCV seems to over-report the number of frames by 1 in some cases, or fails",0
"to read the last frame; either way, I'm allowing one missing frame.",0
"print(""Rendering video {}, couldn't find frame {}"".format(",0
"input_video_file,missing_frame_number))",0
...if we need to check whether to skip this video entirely,0
"for frame_number in tqdm(range(0,n_frames)):",0
print('Skipping frame {}'.format(frame_filename)),0
Recursively enumerate video files,0
Create the target output folder,0
Render frames,0
input_video_file = input_fn_absolute; output_folder = output_folder_video,0
For each video,0
,0
input_fn_relative = input_files_relative_paths[0],0
"process_detection_with_options = partial(process_detection, options=options)",0
zero-indexed,0
Load results,0
# Break into videos,0
im = images[0],0
# For each video...,0
video_name = list(video_to_frames.keys())[0],0
frame = frames[0],0
At most one detection for each category for the whole video,0
category_id = list(detection_categories.keys())[0],0
Find the nth-highest-confidence video to choose a confidence value,0
Prepare the output representation for this video,0
'max_detection_conf' is no longer included in output files by default,0
...for each video,0
Write the output file,0
%% Test driver,0
%% Constants,0
%% Split videos into frames,0
"%% List image files, break into folders",0
Find unique folders,0
fn = frame_files[0],0
%% Load detector output,0
%% Render detector frames,0
folder = list(folders)[0],0
d = detection_results_this_folder[0],0
...for each file in this folder,0
...for each folder,0
%% Render output videos,0
folder = list(folders)[0],0
...for each video,0
,0
run_inference_with_yolov5_val.py,0
,0
Runs a folder of images through MegaDetector (or another YOLOv5 model) with YOLOv5's,0
"val.py, converting the output to the standard MD format.  The main goal is to leverage",0
YOLO's test-time augmentation tools.,0
,0
"YOLOv5's val.py uses each file's base name as a unique identifier, which doesn't work",0
when you have typical camera trap images like:,0
,0
a/b/c/RECONYX0001.JPG,0
d/e/f/RECONYX0001.JPG,0
,0
...so this script jumps through a bunch of hoops to put a symlinks in a flat,0
"folder, run YOLOv5 on that folder, and map the results back to the real files.",0
,0
"Currently requires the user to supply the path where a working YOLOv5 install lives,",0
and assumes that the current conda environment is all set up for YOLOv5.,0
,0
TODO:,1
,0
* Figure out what happens when images are corrupted... right now this is the #1,0
"reason not to use this script, it may be the case that corrupted images look the",0
same as empty images.,0
,0
* Multiple GPU support,0
,0
* Checkpointing,0
,0
* Windows support (I have no idea what all the symlink operations will do on Windows),0
,0
"* Support alternative class names at the command line (currently defaults to MD classes,",0
though other class names can be supplied programmatically),0
,0
%% Imports,0
%% Options class,0
# Required ##,0
# Optional ##,0
%% Main function,0
#%% Path handling,0
#%% Enumerate images,0
#%% Create symlinks to give a unique ID to each image,0
i_image = 0; image_fn = image_files_absolute[i_image],0
...for each image,0
#%% Create the dataset file,0
Category IDs need to be continuous integers starting at 0,0
#%% Prepare YOLOv5 command,0
#%% Run YOLOv5 command,0
#%% Convert results to MD format,0
"We'll use the absolute path as a relative path, and pass '/'",0
as the base path in this case.,0
#%% Clean up,0
...def run_inference_with_yolo_val(),0
%% Command-line driver,0
%% Scrap,0
%% Test driver (folder),0
%% Test driver (file),0
%% Preview results,0
options.sample_seed = 0,0
...for each prediction file,0
%% Compare results,0
Choose all pairwise combinations of the files in [filenames],0
"%% Constants, imports, environment",0
from multiprocessing.pool import ThreadPool as workerpool,0
Number of images to pre-fetch,0
Useful hack to force CPU inference.,1
,0
"Need to do this before any PT/TF imports, which happen when we import",0
from run_detector.,0
Numpy FutureWarnings from tensorflow import,0
%% Support functions for multiprocessing,0
TODO,1
,0
The queue system is a little more elegant if we start one thread for reading and one,1
"for processing, and this works fine on Windows, but because we import TF at module load,",0
"CUDA will only work in the main process, so currently the consumer function runs here.",0
,0
"To enable proper multi-GPU support, we may need to move the TF import to a separate module",0
that isn't loaded until very close to where inference actually happens.,0
%% Other support funtions,0
%% Image processing functions,0
%% Main function,0
Handle the case where image_file_names is not yet actually a list,0
"Find the images to score; images can be a directory, may need to recurse",0
A json list of image paths,0
A single image file,0
Load the detector,0
Does not count those already processed,0
Will not add additional entries not in the starter checkpoint,0
Write a checkpoint if necessary,0
"Back up any previous checkpoints, to protect against crashes while we're writing",0
the checkpoint file.,0
Write the new checkpoint,0
Remove the backup checkpoint if it exists,0
...if it's time to make a checkpoint,0
"When using multiprocessing, let the workers load the model",0
"Results may have been modified in place, but we also return it for",0
backwards-compatibility.,0
The typical case: we need to build the 'info' struct,0
"If the caller supplied the entire ""info"" struct",0
"The 'max_detection_conf' field used to be included by default, and it caused all kinds",0
"of headaches, so it's no longer included unless the user explicitly requests it.",0
%% Interactive driver,0
%%,0
%% Command-line driver,0
This is an experimental hack to allow the use of non-MD YOLOv5 models through,1
the same infrastructure; it disables the code that enforces MDv5-like class lists.,0
Load the checkpoint if available,0
,0
Relative file names are only output at the end; all file paths in the checkpoint are,0
still full paths.,0
"Find the images to score; images can be a directory, may need to recurse",0
A json list of image paths,0
A single image file,0
Test that we can write to the output_file's dir if checkpointing requested,0
"Don't overwrite existing checkpoint files, this is a sure-fire way to eventually",0
erase someone's checkpoint.,0
"Commenting this out for now... the scenario where we are resuming from a checkpoint,",0
then immediately overwrite that checkpoint with empty data is higher-risk than the,0
annoyance of crashing a few minutes after starting a job.,0
Confirm that we can write to the checkpoint path; this avoids issues where,0
we crash after several thousand images.,0
%% Imports,0
import pre- and post-processing functions from the YOLOv5 repo,0
scale_coords() became scale_boxes() in later YOLOv5 versions,0
%% Classes,0
padded resize,0
"Image size can be an int (which translates to a square target size) or (h,w)",0
...if the caller has specified an image size,0
NMS,0
"As of v1.13.0.dev20220824, nms is not implemented for MPS.",0
,0
Send predication back to the CPU to fix.,0
format detections/bounding boxes,0
"This is a loop over detection batches, which will always be length 1 in our case,",0
since we're not doing batch inference.,0
Rescale boxes from img_size to im0 size,0
"normalized center-x, center-y, width and height",0
"MegaDetector output format's categories start at 1, but the MD",0
model's categories start at 0.,0
...for each detection in this batch,0
...if this is a non-empty batch,0
...for each detection batch,0
...try,0
for testing,0
"MegaDetector was trained with batch size of 1, and the resizing function is a part",0
of the inference graph,0
"change from [y1, x1, y2, x2] to [x1, y1, width, height]",0
convert numpy floats to Python floats,0
need to change the above line to the following if supporting a batch size > 1 and resizing to the same size,0
"np_images = [np.asarray(image, np.uint8) for image in images]",0
"images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)",0
performs inference,0
our batch size is 1; need to loop the batch dim if supporting batch size > 1,0
"%% Constants, imports, environment",0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Numpy FutureWarnings from tensorflow import,0
Useful hack to force CPU inference,1
os.environ['CUDA_VISIBLE_DEVICES'] = '-1',0
An enumeration of failure reasons,0
Number of decimal places to round to for confidence and bbox coordinates,0
Label mapping for MegaDetector,0
Should we allow classes that don't look anything like the MegaDetector classes?,0
"Each version of the detector is associated with some ""typical"" values",0
"that are included in output files, so that downstream applications can",0
use them as defaults.,0
%% Classes,0
Stick this into filenames before the extension for the rendered result,0
%% Utility functions,0
mps backend only available in torch >= 1.12.0,0
%% Main function,0
Dictionary mapping output file names to a collision-avoidance count.,0
,0
"Since we'll be writing a bunch of files to the same folder, we rename",0
as necessary to avoid collisions.,0
...def input_file_to_detection_file(),0
Image is modified in place,0
...for each image,0
...def load_and_run_detector(),0
%% Command-line driver,0
Must specify either an image file or a directory,0
"but for a single image, args.image_dir is also None",0
%% Interactive driver,0
%%,0
#####,0
,0
process_video.py,0
,0
"Split a video (or folder of videos) into frames, run the frames through run_detector_batch.py,",0
and optionally stitch together results into a new video with detection boxes.,0
,0
TODO: allow video rendering when processing a whole folder,1
TODO: allow video rendering from existing results,1
,0
#####,0
"%% Constants, imports, environment",0
Only relevant if render_output_video is True,0
Folder to use for extracted frames,0
Folder to use for rendered frames (if rendering output video),0
Should we render a video with detection boxes?,0
,0
"Only supported when processing a single video, not a folder.",0
"If we are rendering boxes to a new video, should we keep the temporary",0
rendered frames?,0
Should we keep the extracted frames?,0
%% Main functions,0
"TODO: keep track of whether we created this folder, delete if we're deleting the extracted",0
"frames and we created the folder, and the output files aren't in the same folder.  For now,",0
we're just deleting the extracted frames and leaving the empty folder around in this case.,0
Render detections to images,0
"TODO: keep track of whether we created this folder, delete if we're deleting the rendered",0
"frames and we created the folder, and the output files aren't in the same folder.  For now,",0
we're just deleting the rendered frames and leaving the empty folder around in this case.,0
Combine into a video,0
Delete the temporary directory we used for detection images,0
shutil.rmtree(rendering_output_dir),0
(Optionally) delete the frames on which we ran MegaDetector,0
shutil.rmtree(frame_output_folder),0
...process_video(),0
# Validate options,0
# Split every video into frames,0
# Run MegaDetector on the extracted frames,0
# (Optionally) delete the frames on which we ran MegaDetector,0
shutil.rmtree(frame_output_folder),0
# Convert frame-level results to video-level results,0
...process_video_folder(),0
%% Interactive driver,0
%% Process a folder of videos,0
import clipboard; clipboard.copy(cmd),0
%% Process a single video,0
import clipboard; clipboard.copy(cmd),0
"%% Render a folder of videos, one file at a time",0
import clipboard; clipboard.copy(s),0
%% Command-line driver,0
Lint as: python3,0
Copyright 2020 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TPU is automatically inferred if tpu_name is None and,0
we are running under cloud ai-platform.,0
,0
copy_checkpoints.py,0
,0
Run this script with specified source_dir and target_dir while the model is training to make a copy,0
of every checkpoint (checkpoints are kept once an hour by default and is difficult to adjust),0
,0
do not copy event or evaluation results,0
input validation,0
plot the images,0
adjust the figure,0
"read in dataset CSV and create merged (dataset, location) col",0
map label to label_index,0
load the splits,0
only weight the training set by detection confidence,0
TODO: consider weighting val and test set as well,1
isotonic regression calibration of MegaDetector confidence,0
treat each split separately,0
new_weight[i] = confidence[i] * (n / c) / total_confidence(i's label),0
- n = # examples in split (weighted by confidence); c = # labels,0
- weight allocated to each label is n/c,0
"- within each label, weigh each example proportional to confidence",0
- new weights sum to n,0
error checking,0
"maps output label name to set of (dataset, dataset_label) tuples",0
find which other label (label_b) has intersection,0
input validation,0
create label index JSON,0
look into sklearn.preprocessing.MultiLabelBinarizer,0
Note: JSON always saves keys as strings!,0
"True for ground truth, False for MegaDetector",0
always save as .jpg for consistency,0
get bounding boxes,0
convert from category ID to category name,0
"check if crops are already downloaded, and ignore bboxes below the",0
confidence threshold,0
assign all images without location info to 'unknown_location',0
remove images from labels that have fewer than min_locs locations,0
merge dataset and location into a single string '<dataset>/<location>',0
"create DataFrame of counts. rows = locations, columns = labels",0
label_count: label => number of examples,0
loc_count: label => number of locs containing that label,0
generate a new split,0
score the split,0
SSE for # of images per label (with 2x weight),0
SSE for # of locs per label,0
label => list of datasets to prioritize for test and validation sets,0
"merge dataset and location into a tuple (dataset, location)",0
sorted smallest to largest,0
greedily add to test set until it has >= 15% of images,0
sort the resulting locs,0
"modify loc_to_size in place, so copy its keys before iterating",0
arguments relevant to both creating the dataset CSV and splits.json,0
arguments only relevant for creating the dataset CSV,0
arguments only relevant for creating the splits JSON,0
comment lines starting with '#' are allowed,0
,0
prepare_classification_script.py,0
,0
Notebook-y script used to prepare a series of shell commands to run a classifier,0
(other than MegaClassifier) on a MegaDetector result set.,0
,0
Differs from prepare_classification_script_mc.py only in the final class mapping step.,0
,0
%% Job options,0
%% Constants,0
"This is just passed along to the metadata in the output file, it has no impact",0
on how the classification scripts run.,0
%% Set up environment,0
commands.append('cd CameraTraps/classification\n'),0
commands.append('conda activate cameratraps-classifier\n'),0
%% Crop images,0
fn = input_files[0],0
%% Run classifier,0
fn = input_files[0],0
%% Merge classification and detection outputs,0
fn = input_files[0],0
%% Write everything out,0
input validation,0
"Evaluating with accimage is much faster than Pillow or Pillow-SIMD, but accimage",0
is Linux-only.,0
create output directory,0
override saved params with kwargs,0
"For now, we don't weight crops by detection confidence during",0
evaluation. But consider changing this.,0
"create model, compile with TorchScript if given checkpoint is not compiled",0
"verify that target names matches original ""label names"" from dataset",0
"if the dataset does not already have a 'other' category, then the",0
'other' category must come last in label_names to avoid conflicting,0
with an existing label_id,0
define loss function (criterion),0
"this file ends up being huge, so we GZIP compress it",0
double check that the accuracy metrics are computed properly,0
save the confusion matrices to .npz,0
save per-label statistics,0
set dropout and BN layers to eval mode,0
"even if batch contains sample weights, don't use them",0
Do target mapping on the outputs (unnormalized logits) instead of,0
"the normalized (softmax) probabilities, because the loss function",0
uses unnormalized logits. Summing probabilities is equivalent to,0
log-sum-exp of unnormalized logits.,0
"a confusion matrix C is such that C[i,j] is the # of observations known to",0
be in group i and predicted to be in group j.,0
match pytorch EfficientNet model names,0
images dataset,0
"for smaller disk / memory usage, we cache the raw JPEG bytes instead",0
of the decoded Tensor,0
convert JPEG bytes to a 3D uint8 Tensor,0
"keras EfficientNet already includes normalization from [0, 255] to [0, 1],",0
so we don't need to do that here,0
labels dataset,0
img_files dataset,0
weights dataset,0
define the transforms,0
efficientnet data preprocessing:,0
- train:,0
"1) random crop: aspect_ratio_range=(0.75, 1.33), area_range=(0.08, 1.0)",0
2) bicubic resize to img_size,0
3) random horizontal flip,0
- test:,0
1) center crop,0
2) bicubic resize to img_size,0
weights sums to:,0
- if weight_by_detection_conf: (# images in split - conf delta),0
- otherwise: (# images in split),0
"freeze the base model's weights, including BatchNorm statistics",0
https://www.tensorflow.org/guide/keras/transfer_learning#fine-tuning,0
rebuild output,0
input validation,0
set seed,0
create logdir and save params,0
create dataloaders and log the index_to_label mapping,0
define loss function and optimizer,0
using EfficientNet training defaults,0
- batch norm momentum: 0.99,0
"- optimizer: RMSProp, decay 0.9 and momentum 0.9",0
- epochs: 350,0
"- learning rate: 0.256, decays by 0.97 every 2.4 epochs",0
- weight decay: 1e-5,0
TODO: change weighted to False if oversampling minority classes,1
stop training after 8 epochs without improvement,0
log metrics,0
log confusion matrix,0
log tp/fp/fn images,0
"tf.summary.image requires input of shape [N, H, W, C]",0
false positive for top3_pred[0],0
false negative for label,0
"if evaluating or finetuning, set dropout & BN layers to eval mode",0
"for each label, track 5 most-confident and least-confident examples",0
"even if batch contains sample weights, don't use them",0
we do not track L2-regularization loss in the loss metric,0
This dictionary will get written out at the end of this process; store,0
diagnostic variables here,0
error checking,0
refresh detection cache,0
save log of bad images,0
cache of Detector outputs: dataset name => {img_path => detection_dict},0
img_path: <dataset-name>/<img-filename>,0
get SAS URL for images container,0
strip image paths of dataset name,0
save list of dataset names and task IDs for resuming,0
complete task name: 'detect_for_classifier_caltech_20200722_110816_task01',0
HACK! Sleep for 10s between task submissions in the hopes that it,1
"decreases the chance of backend JSON ""database"" corruption",0
task still running => continue,0
"task finished successfully, save response to disk",0
error checking before we download and crop any images,0
convert from category ID to category name,0
we need the datasets table for getting SAS keys,0
"we already did all error checking above, so we don't do any here",0
get ContainerClient,0
get bounding boxes,0
we must include the dataset <ds> in <crop_path_template> because,0
'{img_path}' actually gets populated with <img_file> in,0
load_and_crop(),0
"get the image, either from disk or from Blob Storage",0
inelegant way to close the container_clients,0
,0
prepare_classification_script_mc.py,0
,0
Notebook-y script used to prepare a series of shell commands to run MegaClassifier,0
on a MegaDetector result set.,0
,0
Differs from prepare_classification_script.py only in the final class mapping step.,0
,0
%% Job options,0
%% Constants,0
"This is just passed along to the metadata in the output file, it has no impact",0
on how the classification scripts run.,0
%% Set up environment,0
commands.append('cd CameraTraps/classification\n'),0
commands.append('conda activate cameratraps-classifier\n'),0
%% Crop images,0
fn = input_files[0],0
%% Run classifier,0
fn = input_files[0],0
%% Remap classifier outputs,0
fn = input_files[0],0
%% Merge classification and detection outputs,0
fn = input_files[0],0
%% Write everything out,0
mean/std values from https://pytorch.org/docs/stable/torchvision/models.html,0
define the transforms,0
resizes smaller edge to img_size,0
weights sums to:,0
- if weight_by_detection_conf: (# images in split - conf delta),0
- otherwise: # images in split,0
for normal (non-weighted) shuffling,0
set all parameters to not require gradients except final FC layer,0
replace final fully-connected layer (which has 1000 ImageNet classes),0
"detect GPU, use all if available",0
input validation,0
set seed,0
create logdir and save params,0
create dataloaders and log the index_to_label mapping,0
create model,0
define loss function and optimizer,0
using EfficientNet training defaults,0
- batch norm momentum: 0.99,0
"- optimizer: RMSProp, decay 0.9 and momentum 0.9",0
- epochs: 350,0
"- learning rate: 0.256, decays by 0.97 every 2.4 epochs",0
- weight decay: 1e-5,0
stop training after 8 epochs without improvement,0
do a complete evaluation run,0
log metrics,0
log confusion matrix,0
log tp/fp/fn images,0
"for every image: undo normalization, clamp to [0, 1], CHW -> HWC",0
"- cannot be in-place, because the HeapItem might be in multiple heaps",0
writer.add_figure() has issues => using add_image() instead,0
"writer.add_figure(f'{label_name}/{tag}', fig, global_step=epoch)",0
false positive for top3_pred[0],0
false negative for label,0
"preds and labels both have shape [N, k]",0
"if evaluating or finetuning, set dropout and BN layers to eval mode",0
"for each label, track k_extreme most-confident and least-confident images",0
"even if batch contains sample weights, don't use them",0
lock before changing ImageFile.LOAD_TRUNCATED_IMAGES,0
input validation,0
use MegaDB to generate list of images,0
only keep images that:,0
"1) end in a supported file extension, and",0
2) actually exist in Azure Blob Storage,0
3) belong to a label with at least min_locs locations,0
write out log of images / labels that were removed,0
"save label counts, pre-subsampling",0
"save label counts, post-subsampling",0
spec_dict['taxa']: list of dict,0
[,0
"{'level': 'family', 'name': 'cervidae', 'datasets': ['idfg']},",0
"{'level': 'genus',  'name': 'meleagris'}",0
],0
"spec_dict['dataset_labels']: dict, dataset => list of dataset_label",0
{,0
"""idfg"": [""deer"", ""elk"", ""prong""],",0
"""idfg_swwlf_2019"": [""elk"", ""muledeer"", ""whitetaileddeer""]",0
},0
"maps output label name to set of (dataset, dataset_label) tuples",0
"because MegaDB is organized by dataset, we do the same",0
ds_to_labels = {,0
'dataset_name': {,0
"'dataset_label': [output_label1, output_label2]",0
},0
},0
we need the datasets table for getting full image paths,0
The line,0
"[img.class[0], seq.class[0]][0] as class",0
selects the image-level class label if available. Otherwise it selects the,0
"sequence-level class label. This line assumes the following conditions,",0
expressed in the WHERE clause:,0
- at least one of the image or sequence class label is given,0
- the image and sequence class labels are arrays with length at most 1,0
- the image class label takes priority over the sequence class label,0
,0
"In Azure Cosmos DB, if a field is not defined, then it is simply excluded",0
"from the result. For example, on the following JSON object,",0
{,0
"""dataset"": ""camera_traps"",",0
"""seq_id"": ""1234"",",0
"""location"": ""A1"",",0
"""images"": [{""file"": ""abcd.jpeg""}],",0
"""class"": [""deer""],",0
},0
"the array [img.class[0], seq.class[0]] just gives ['deer'] because",0
img.class is undefined and therefore excluded.,0
"if no path prefix, set it to the empty string '', because",0
"os.path.join('', x, '') = '{x}/'",0
result keys,0
"- already has: ['dataset', 'location', 'file', 'class', 'bbox']",0
"- add ['label'], remove ['file']",0
"if img is mislabeled, but we don't know the correct class, skip it",0
"otherwise, update the img with the correct class, but skip the",0
img if the correct class is not one we queried for,0
sort keys for determinism,0
we need the datasets table for getting SAS keys,0
strip leading '?' from SAS token,0
only check Azure Blob Storage,0
check local directory first before checking Azure Blob Storage,0
1st pass: populate label_to_locs,0
"label (tuple of str) => set of (dataset, location)",0
2nd pass: eliminate bad images,0
prioritize is a list of prioritization levels,0
number of already matching images,0
main(,0
"label_spec_json_path='idfg_classes.json',",0
"taxonomy_csv_path='../../camera-traps-private/camera_trap_taxonomy_mapping.csv',",0
"output_dir='run_idfg',",0
json_indent=4),0
recursively find all files in cropped_images_dir,0
only find crops of images from detections JSON,0
resizes smaller edge to img_size,0
"Evaluating with accimage is much faster than Pillow or Pillow-SIMD, but accimage",0
is Linux-only.,0
create dataset,0
create model,0
set dropout and BN layers to eval mode,0
load files,0
dataset => set of img_file,0
crop_path: <dataset>/<img_file>___cropXX_mdvY.Y.jpg,0
[----<img_path>----],0
error checking,0
any row with 'correct_class' should be marked 'mislabeled',0
filter to only the mislabeled rows,0
convert '\' to '/',0
verify that overlapping indices are the same,0
"""add"" any new mislabelings",0
write out results,0
error checking,0
load detections JSON,0
get detector version,0
convert from category ID to category name,0
copy keys to modify dict in-place,0
This will be removed later when we filter for animals,0
save log of bad images,0
"True for ground truth, False for MegaDetector",0
always save as .jpg for consistency,0
"we already did all error checking above, so we don't do any here",0
"get the image, either from disk or from Blob Storage",0
inelegant way to close the container_client,0
"crop_path => normalized bbox coordinates [xmin, ymin, width, height]",0
"only ground-truth bboxes do not have a ""confidence"" value",0
try loading image from local directory,0
try to download image from Blob Storage,0
crop the image,0
"expand box width or height to be square, but limit to img size",0
"Image.crop() takes box=[left, upper, right, lower]",0
pad to square using 0s,0
Author: lukemelas (github username),0
Github repo: https://github.com/lukemelas/EfficientNet-PyTorch,0
With adjustments and added comments by workingcoder (github username).,0
Support the construction of 'efficientnet-l2' without pretrained weights,0
Expansion phase (Inverted Bottleneck),0
"image_size = calculate_output_image_size(image_size, 1) <-- this wouldn't modify image_size",0
Depthwise convolution phase,0
"Squeeze and Excitation layer, if desired",0
Pointwise convolution phase,0
Expansion and Depthwise Convolution,0
Squeeze and Excitation,0
Pointwise Convolution,0
Skip connection and drop connect,0
The combination of skip connection and drop connect brings about stochastic depth.,0
Batch norm parameters,0
Get stem static or dynamic convolution depending on image size,0
Stem,0
Build blocks,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
"image_size = calculate_output_image_size(image_size, block_args.stride)  # stride = 1",0
Head,0
Final linear layer,0
Stem,0
Blocks,0
Head,0
Stem,0
Blocks,0
Head,0
Convolution layers,0
Pooling and final linear layer,0
Author: lukemelas (github username),0
Github repo: https://github.com/lukemelas/EfficientNet-PyTorch,0
With adjustments and added comments by workingcoder (github username).,0
###############################################################################,0
## Help functions for model architecture,0
###############################################################################,0
GlobalParams and BlockArgs: Two namedtuples,0
Swish and MemoryEfficientSwish: Two implementations of the method,0
round_filters and round_repeats:,0
Functions to calculate params for scaling model width and depth ! ! !,0
get_width_and_height_from_size and calculate_output_image_size,0
drop_connect: A structural design,0
get_same_padding_conv2d:,0
Conv2dDynamicSamePadding,0
Conv2dStaticSamePadding,0
get_same_padding_maxPool2d:,0
MaxPool2dDynamicSamePadding,0
MaxPool2dStaticSamePadding,0
"It's an additional function, not used in EfficientNet,",0
but can be used in other model (such as EfficientDet).,0
"Parameters for the entire model (stem, all blocks, and head)",0
Parameters for an individual model block,0
Set GlobalParams and BlockArgs's defaults,0
An ordinary implementation of Swish function,0
A memory-efficient implementation of Swish function,0
TODO: modify the params names.,1
"maybe the names (width_divisor,min_width)",0
"are more suitable than (depth_divisor,min_depth).",0
follow the formula transferred from official TensorFlow implementation,0
follow the formula transferred from official TensorFlow implementation,0
"generate binary_tensor mask according to probability (p for 0, 1-p for 1)",0
Note:,0
The following 'SamePadding' functions make output size equal ceil(input size/stride).,0
"Only when stride equals 1, can the output size be the same as input size.",0
Don't be confused by their function names ! ! !,0
Tips for 'SAME' mode padding.,0
Given the following:,0
i: width or height,0
s: stride,0
k: kernel size,0
d: dilation,0
p: padding,0
Output after Conv2d:,0
o = floor((i+p-((k-1)*d+1))/s+1),0
"If o equals i, i = floor((i+p-((k-1)*d+1))/s+1),",0
=> p = (i-1)*s+((k-1)*d+1)-i,0
With the same calculation as Conv2dDynamicSamePadding,0
Calculate padding based on image size and save it,0
Calculate padding based on image size and save it,0
###############################################################################,0
## Helper functions for loading model params,0
###############################################################################,0
BlockDecoder: A Class for encoding and decoding BlockArgs,0
efficientnet_params: A function to query compound coefficient,0
get_model_params and efficientnet:,0
Functions to get BlockArgs and GlobalParams for efficientnet,0
url_map and url_map_advprop: Dicts of url_map for pretrained weights,0
load_pretrained_weights: A function to load pretrained weights,0
Check stride,0
"Coefficients:   width,depth,res,dropout",0
Blocks args for the whole model(efficientnet-b0 by default),0
It will be modified in the construction of EfficientNet Class according to model,0
note: all models have drop connect rate = 0.2,0
ValueError will be raised here if override_params has fields not included in global_params.,0
train with Standard methods,0
check more details in paper(EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks),0
train with Adversarial Examples(AdvProp),0
check more details in paper(Adversarial Examples Improve Image Recognition),0
TODO: add the petrained weights url map of 'efficientnet-l2',1
AutoAugment or Advprop (different preprocessing),0
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils),0
from data_management.megadb.schema import sequences_schema_check,0
resize is for displaying them more quickly,0
dataset and seq_id are required fields,0
sort the images in the sequence,0
"if no class label on the image, show class label on the sequence",0
pool = ThreadPool(),0
"print('len of rendering_info', len(rendering_info))",0
"tqdm(pool.imap_unordered(render_image_info_partial, rendering_info), total=len(rendering_info))",0
options = write_html_image_list(),0
options['headerHtml'],0
print('Checking that the MegaDB entries conform to the schema...'),0
sequences_schema_check.sequences_schema_check(sequences),0
#######,0
,0
visualize_db.py,0
,0
Outputs an HTML page visualizing annotations (class labels and/or bounding boxes),0
on a sample of images in a database in the COCO Camera Traps format,0
,0
#######,0
%% Imports,0
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils),0
Assumes the cameratraps repo root is on the path,0
%% Settings,0
Set to None to visualize all images,0
Target size for rendering; set either dimension to -1 to preserve aspect ratio,0
"These are mutually exclusive; both are category names, not IDs",0
"Special tag used to say ""show me all images with multiple categories""",0
We sometimes flatten image directories by replacing a path separator with,0
another character.  Leave blank for the typical case where this isn't necessary.,0
Control rendering parallelization,0
Process-based parallelization in this function is currently unsupported,0
"due to pickling issues I didn't care to look at, but I'm going to just",0
"flip this with a warning, since I intend to support it in the future.",0
%% Helper functions,0
"Translate the file name in an image entry in the json database to a path, possibly doing",0
some manipulation of path separators,0
%% Core functions,0
"Optionally remove all images without bounding boxes, *before* sampling",0
"Optionally include/remove images with specific labels, *before* sampling",0
Put the annotations in a dataframe so we can select all annotations for a given image,0
Construct label map,0
Take a sample of images,0
Set of dicts representing inputs to render_db_bounding_boxes:,0
,0
"bboxes, boxClasses, image_path",0
All the class labels we've seen for this image (with out without bboxes),0
Iterate over annotations for this image,0
iAnn = 0; anno = annos_i.iloc[iAnn],0
...for each of this image's annotations,0
"We're adding html for an image before we render it, so it's possible this image will",0
fail to render.  For applications where this script is being used to debua a database,0
"(the common case?), this is useful behavior, for other applications, this is annoying.",0
...for each image,0
...def render_image_info,0
def process_images(...),0
%% Command-line driver,0
"Copy all fields from a Namespace (i.e., the output from parse_args) to an object.",0
,0
Skips fields starting with _.  Does not check existence in the target object.,0
Convert to an options object,0
%% Interactive driver(s),0
%%,0
os.startfile(htmlOutputFile),0
%% Constants and imports,0
convert category ID from int to str,0
Retry on blob storage read failures,0
%% Functions,0
PIL.Image.convert() returns a converted copy of this image,0
Alter orientation as needed according to EXIF tag 0x112 (274) for Orientation,0
,0
https://gist.github.com/dangtrinhnt/a577ece4cbe5364aad28,0
https://www.media.mit.edu/pia/Research/deepview/exif.html,0
,0
Null operation,0
Aspect ratio as width over height,0
ar = w / h,0
h = w / ar,0
w = ar * h,0
The following three functions are modified versions of those at:,0
,0
https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py,0
Convert to pixels so we can use the PIL crop() function,0
PIL's crop() does surprising things if you provide values outside of,0
"the image, clip inputs",0
...if this detection is above threshold,0
...for each detection,0
"list of lists, one list of strings for each bounding box (to accommodate multiple labels)",0
for color selection,0
"Always render objects with a confidence of ""None"", this is typically used",0
for ground truth data.,0
"{} is the default, which means ""show labels with no mapping"", so don't use ""if label_map"" here",0
if label_map:,0
"To avoid duplicate colors with detection-only visualization, offset",0
the classification class index by the number of detection classes,0
...for each classification,0
...if we have classification results,0
...if the confidence of this detection is above threshold,0
...for each detection,0
"print('Input must be of size [N, 4], but is ' + str(boxes_shape))",0
Deliberately trimming to the width of the image only in the case where,0
"box expansion is turned on.  There's not an obvious correct behavior here,",0
but the thinking is that if the caller provided an out-of-range bounding,0
"box, they meant to do that, but at least in the eyes of the person writing",0
"this comment, if you expand a box for visualization reasons, you don't want",0
to end up with part of a box.,0
,0
A slightly more sophisticated might check whether it was in fact the expansion,0
"that made this box larger than the image, but this is the case 99.999% of the time",0
"here, so that doesn't seem necessary.",1
...if we need to expand boxes,0
If the total height of the display strings added to the top of the bounding,0
"box exceeds the top of the image, stack the strings below the bounding box",0
instead of above.,0
Each display_str has a top and bottom margin of 0.05x.,0
Reverse list and print from bottom to top.,0
Skip empty strings,0
"list of lists, one list of strings for each bounding box (to accommodate multiple labels)",0
need to be a string here because PIL needs to iterate through chars,0
,0
stacked bar charts are made with each segment starting from a y position,0
"To fit the legend in, shrink current axis by 20%",0
Put a legend to the right of the current axis,0
####,0
,0
visualize_incoming_annotations.py,0
,0
Spot-check the annotations received from iMerit by visualizing annotated bounding,0
boxes on a sample of images and display them in HTML.,0
,0
"Modified in 2021 March to use the new format (iMerit batch 12 onwards), which is a",0
COCO formatted JSON with relative coordinates for the bbox.,0
,0
from data_management.megadb.schema import sequences_schema_check,0
we used frame_num of 1 when sending out images to annotators when it is not explicitly stored (wcs esp.),0
The file_name field in the incoming json looks like,0
alka_squirrels.seq2020_05_07_25C.frame119221.jpg,0
"we need to use the dataset, sequence and frame info to find the actual path in blob storage",0
using the sequences,0
category_id 5 is No Object Visible,0
download the image,0
Write to HTML,0
allow forward references in typing annotations,0
class variables,0
instance variables,0
get path to root,0
taxonomy CSV rows without 'taxonomy_string' entries are excluded,0
"from the taxonomy graph, but can be included in a classification",0
label specification JSON via the 'dataset_labels' key,0
use the lower parent,0
special cases,0
%% Imports,0
%% Taxnomy checking,0
taxonomy CSV rows without 'taxonomy_string' entries are excluded,0
"from the taxonomy graph, but can be included in a classification",0
label specification JSON via the 'dataset_labels' key,0
...for each row in the taxnomy file,0
%% Command-line driver,0
%% Interactive driver,0
%%,0
which datasets are already processed?,0
"sequence-level query should be fairly fast, ~1 sec",0
cases when the class field is on the image level (images in a sequence,0
"that had different class labels, 'caltech' dataset is like this)",0
"this query may take a long time, >1hr",0
"this query should be fairly fast, ~1 sec",0
read species presence info from the JSON files for each dataset,0
has this class name appeared in a previous dataset?,0
columns to populate the spreadsheet,0
sort by descending species count,0
make the spreadsheet,0
hyperlink Bing search URLs,0
hyperlink example image SAS URLs,0
TODO hardcoded columns: change if # of examples or col_order changes,1
,0
map_lila_taxonomy_to_wi_taxonomy.py,0
,0
Loads the LILA category mapping (in which taxonomy information comes from an iNat taxonomy snapshot),0
and tries to map each class to the Wildlife Insights taxonomy.,0
,0
%% Constants and imports,0
Created by get_lila_category_list.py... contains counts for each category,0
This is a manually-curated file used to store mappings that had to be made manually,0
This is the main output file from this whole process,0
%% Load category and taxonomy files,0
%% Pull everything out of pandas,0
%% Cache WI taxonomy lookups,0
This is just a handy lookup table that we'll use to debug mismatches,0
taxon = wi_taxonomy[21653]; print(taxon),0
Look for keywords that don't refer to specific taxa: blank/animal/unknown,0
Do we have a species name?,0
"If 'species' is populated, 'genus' should always be populated; one item currently breaks",0
this rule.,0
...for each taxon,0
%% Find redundant taxa,0
%% Manual review of redundant taxa,0
%% Clean up redundant taxa,0
taxon_name = list(taxon_name_to_preferred_taxon_id.keys())[0],0
"If we've gotten this far, we should be choosing from multiple taxa.",0
,0
"This will become untrue if any of these are resolved later, at which point we shoudl",0
remove them from taxon_name_to_preferred_id,0
Choose the preferred taxa,0
%% Read supplementary mappings,0
"Each line is [lila query],[WI taxon name],[notes]",0
%% Map LILA categories to WI categories,0
Must be ordered from kingdom --> species,0
TODO:,1
"['subspecies','variety']",0
i_taxon = 0; taxon = lila_taxonomy[i_taxon]; print(taxon),0
"Go from kingdom --> species, choosing the lowest-level description as the query",0
"E.g., 'car'",0
"print('Made a supplementary mapping from {} to {}'.format(query,wi_taxon['taxon_name']))",0
print('No match for {}'.format(query)),0
...for each LILA taxon,0
%% Manual mapping,0
%% Build a dictionary from LILA dataset names and categories to LILA taxa,0
i_d = 0; d = lila_taxonomy[i_d],0
"%% Map LILA datasets to WI taxa, and count the number of each taxon available in each dataset",0
dataset_name = list(lila_dataset_to_categories.keys())[0],0
dataset_category = dataset_categories[0],0
"Write out the dataset name, category name, WI GUID, WI scientific name, WI common name,",0
and count,0
...for each category in this dataset,0
...for each dataset,0
...with open(),0
,0
retrieve_sample_image.py,0
,0
"Downloader that retrieves images from Google images, used for verifying taxonomy",0
"lookups and looking for egregious mismappings (e.g., ""snake"" being mapped to a fish called",0
"""snake"").",0
,0
"Simple wrapper around simple_image_download, but I've had to swap in and out the underlying",0
downloader a few times.,0
,0
%% Imports and environment,0
%%,0
%% Main entry point,0
%% Test driver,0
%%,0
,0
"Using the taxonomy .csv file, map all LILA datasets to the standard taxonomy",0
,0
Does not currently produce results; this is just used to confirm that all category names,0
have mappings in the taxonomy file.,0
,0
%% Constants and imports,0
Created by get_lila_category_list.py... contains counts for each category,0
%% Load category and taxonomy files,0
%% Map dataset names and category names to scientific names,0
i_row = 1; row = taxonomy_df.iloc[i_row]; row,0
"%% For each dataset, make sure we can map every category to the taxonomy",0
dataset_name = list(lila_dataset_to_categories.keys())[0],0
c = categories[0],0
,0
"Takes the megadb taxonomy mapping, extracts the rows that are relevant to",0
"LILA, and does some cleanup.",0
,0
%% Constants and imports,0
This is a partially-completed taxonomy file that was created from a different set of,0
"scripts, but covers *most* of LILA as of June 2022",0
Created by get_lila_category_list.py,0
%% Read the input files,0
Get everything out of pandas,0
"%% Find all unique dataset names in the input list, compare them with data names from LILA",0
d = input_taxonomy_rows[0],0
The script that generates this dictionary creates a separate entry for bounding box,0
"metadata files, but those don't represent new dataset names",0
%% Map input columns to output datasets,0
Make sure all of those datasets actually correspond to datasets on LILA,0
%% Re-write the input taxonomy file to refer to LILA datasets,0
Map the string datasetname:token to a taxonomic tree json,0
mapping = input_taxonomy_rows[0],0
Make sure that all occurrences of this mapping_string give us the same output,0
assert taxonomy_string == taxonomy_mappings[mapping_string],0
%% Re-write the input file in the target format,0
mapping_string = list(taxonomy_mappings.keys())[0],0
,0
prepare_lila_taxonomy_release.py,0
,0
"Given the private intermediate taxonomy mapping, prepare the public (release)",0
taxonomy mapping file.,0
,0
%% Imports and constants,0
Created by get_lila_category_list.py... contains counts for each category,0
%% Find out which categories are actually used,0
dataset_name = datasets_to_map[0],0
i_row = 0; row = df.iloc[i_row]; row,0
%% Generate the final output file,0
i_row = 0; row = df.iloc[i_row]; row,0
match_at_level = taxonomic_match[0],0
i_row = 0; row = df.iloc[i_row]; row,0
"E.g.: (43117, 'genus', 'lepus', ['hares and jackrabbits']",0
###############,0
---> CONSTANTS,0
###############,0
max_progressbar = count * (list(range(limit+1))[-1]+1),0
"bar = progressbar.ProgressBar(maxval=max_progressbar,",0
"widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()]).start()",0
bar.update(bar.currval + 1),0
bar.finish(),0
,0
"Given a subset of LILA datasets, find all the categories, and start the taxonomy",0
mapping process.,0
,0
%% Constants and imports,0
Created by get_lila_category_list.py,0
'NACTI',0
'Channel Islands Camera Traps',0
%% Read the list of datasets,0
The script that generates this dictionary creates a separate entry for bounding box,0
"metadata files, but those don't represent new dataset names",0
%% Find all categories,0
dataset_name = datasets_to_map[0],0
%% Initialize taxonomic lookup,0
from taxonomy_mapping.species_lookup import (,0
"get_taxonomic_info, print_taxonomy_matche)",0
%% Manual lookup,0
%%,0
q = 'white-throated monkey',0
raise ValueError(''),0
%% Match every query against our taxonomies,0
mapping_string = category_mappings[1]; print(mapping_string),0
...for each mapping,0
%% Write output rows,0
,0
"Does some consistency-checking on the LILA taxonomy file, and generates",0
an HTML preview page that we can use to determine whether the mappings,0
make sense.,0
,0
%% Imports and constants,0
"lila_taxonomy_file = r""G:\git\agentmorrisprivate\lila-taxonomy\lila-taxonomy-mapping.csv""",0
"lila_taxonomy_file = r""G:\temp\lila\lila_additions_2022.06.29.csv""",0
%% Support functions,0
%% Read the taxonomy mapping file,0
%% Prepare taxonomy lookup,0
from taxonomy_mapping.species_lookup import (,0
"get_taxonomic_info, print_taxonomy_matche)",0
%% Optionally remap all gbif-based mappings to inat (or vice-versa),0
%%,0
i_row = 1; row = df.iloc[i_row]; row,0
This should be zero for the release .csv,0
%%,0
%% Check for mappings that disagree with the taxonomy string,0
Look for internal inconsistency,0
Look for outdated mappings,0
i_row = 0; row = df.iloc[i_row],0
%% List null mappings,0
,0
"These should all be things like ""unidentified"" and ""fire""",0
,0
i_row = 0; row = df.iloc[i_row],0
%% List mappings with scientific names but no common names,0
%% List mappings that map to different things in different data sets,0
x = suppress_multiple_matches[-1],0
...for each row where we saw this query,0
...for each row,0
"%% Verify that nothing ""unidentified"" maps to a species or subspecies",0
"E.g., ""unidentified skunk"" should never map to a specific species of skunk",0
%% Make sure there are valid source and level values for everything with a mapping,0
%% Find WCS mappings that aren't species or aren't the same as the input,0
"WCS used scientific names, so these remappings are slightly more controversial",0
then the standard remappings.,0
row = df.iloc[-500],0
"Anything marked ""species"" or ""unknown"" by definition doesn't map to a species,",0
so ignore these.,0
print('WCS query {} ({}) remapped to {} ({})'.format(,0
"query,common_name,scientific_name,common_names_from_taxonomy))",0
%% Download sample images for all scientific names,0
i_row = 0; row = df.iloc[i_row],0
if s != 'mirafra':,0
continue,0
Check whether we already have enough images for this query,0
"print('Skipping query {}, already have {} images'.format(s,len(sizes_above_threshold)))",0
Check whether we've already run this query for a previous row,0
...for each row in the mapping table,0
%% Rename .jpeg to .jpg,0
"print('Renaming {} to {}'.format(fn,new_fn))",0
%% Choose representative images for each scientific name,0
s = list(scientific_name_to_paths.keys())[0],0
Be suspicious of duplicate sizes,0
...for each scientific name,0
%% Delete unused images,0
%% Produce HTML preview,0
i_row = 2; row = df.iloc[i_row],0
"image_paths = [os.path.relpath(p, output_base) for p in image_paths]",0
...for each row,0
%% Open HTML preview,0
######,0
,0
species_lookup.py,0
,0
Look up species names (common or scientific) in the GBIF and iNaturalist,0
taxonomies.,0
,0
Run initialize_taxonomy_lookup() before calling any other function.,0
,0
######,0
%% Constants and imports,0
As of 2020.05.12:,0
,0
"GBIF: ~777MB zipped, ~1.6GB taxonomy",0
"iNat: ~2.2GB zipped, ~51MB taxonomy",0
These are un-initialized globals that must be initialized by,0
the initialize_taxonomy_lookup() function below.,0
%% Functions,0
Initialization function,0
# Load serialized taxonomy info if we've already saved it,0
"# If we don't have serialized taxonomy info, create it from scratch.",0
Download and unzip taxonomy files,0
taxonomy_name = list(taxonomy_urls.items())[0][0]; zip_url = list(taxonomy_urls.items())[0][1],0
Don't download the zipfile if we've already unzipped what we need,0
Bypasses download if the file exists already,0
Unzip the files we need,0
...for each file that we need from this zipfile,0
Remove the zipfile,0
os.remove(zipfile_path),0
...for each taxonomy,0
"Create dataframes from each of the taxonomy files, and the GBIF common",0
name file,0
Load iNat taxonomy,0
Load GBIF taxonomy,0
Remove questionable rows from the GBIF taxonomy,0
Load GBIF vernacular name mapping,0
Only keep English mappings,0
Convert everything to lowercase,0
"For each taxonomy table, create a mapping from taxon IDs to rows",0
Create name mapping dictionaries,0
Build iNat dictionaries,0
row = inat_taxonomy.iloc[0],0
Build GBIF dictionaries,0
"The canonical name is the Latin name; the ""scientific name""",0
include the taxonomy name.,0
,0
http://globalnames.org/docs/glossary/,0
This only seems to happen for really esoteric species that aren't,0
"likely to apply to our problems, but doing this for completeness.",0
Don't include taxon IDs that were removed from the master table,0
Save everything to file,0
...def initialize_taxonomy_lookup(),0
"list of dicts: {'source': source_name, 'taxonomy': match_details}",0
i_match = 0,0
"list of (taxon_id, taxonRank, scientific name, [vernacular names])",0
corresponding to an exact match and its parents,0
Walk taxonomy hierarchy,0
This can happen because we remove questionable rows from the,0
GBIF taxonomy,0
"print(f'Warning: no row exists for parent_taxon_id {parent_taxon_id},' + \",0
"f'child taxon_id: {taxon_id}, query: {query}')",0
The GBIF taxonomy contains unranked entries,0
...while there is taxonomy left to walk,0
...for each match,0
Remove redundant matches,0
i_tree_a = 0; tree_a = matching_trees[i_tree_a],0
i_tree_b = 1; tree_b = matching_trees[i_tree_b],0
"If tree a's primary taxon ID is inside tree b, discard tree a",0
,0
taxonomy_level_b = tree_b['taxonomy'][0],0
...for each level in taxonomy B,0
...for each tree (inner),0
...for each tree (outer),0
...def traverse_taxonomy(),0
"print(""Finding taxonomy information for: {0}"".format(query))",0
"In GBIF, some queries hit for both common and scientific, make sure we end",0
up with unique inputs,0
"If the species is not found in either taxonomy, return None",0
Both GBIF and iNat have a 1-to-1 mapping between taxon_id and row number,0
Walk both taxonomies,0
...def get_taxonomic_info(),0
m = matches[0],0
"For example: [(9761484, 'species', 'anas platyrhynchos')]",0
...for each taxonomy level,0
...for each match,0
...def print_taxonomy_matches(),0
%% Taxonomy functions that make subjective judgements,0
query = 'person',0
Do we have an iNat match?,0
"print_taxonomy_matches(inat_matches, verbose=True)",0
"print_taxonomy_matches(gbif_matches, verbose=True)",0
print('Warning: multiple iNat matches for {}'.format(query)),0
Prefer chordates... most of the names that aren't what we want,0
"are esoteric insects, like a moth called ""cheetah""",0
,0
"If we can't find a chordate, just take the first match.",0
,0
i_test_match = 0,0
"This is (taxonID, taxonLevel, scientific, [list of common])",0
print(f'Warning: multiple iNat common names for {query}'),0
Default to returning the query,0
"print(f'Matched iNat {query} to {scientific_name},{common_name}')",0
...if we had iNat matches,0
If we either prefer GBIF or didn't have iNat matches,0
,0
Code is deliberately redundant here; I'm expecting some subtleties in how,0
handle GBIF and iNat.,0
print('Warning: multiple GBIF matches for {}'.format(query)),0
Prefer chordates... most of the names that aren't what we want,0
"are esoteric insects, like a moth called ""cheetah""",0
,0
"If we can't find a chordate, just take the first match.",0
,0
i_test_match = 0,0
"This is (taxonID, taxonLevel, scientific, [list of common])",0
print(f'Warning: multiple GBIF common names for {query}'),0
Default to returning the query,0
...if we needed to look in the GBIF taxonomy,0
...def _get_preferred_taxonomic_match(),0
%% Interactive drivers and debug,0
%% Initialization,0
%% Taxonomic lookup,0
query = 'lion',0
print(matches),0
Print the taxonomy in the taxonomy spreadsheet format,0
%% Directly access the taxonomy tables,0
%% Command-line driver,0
Read command line inputs (absolute path),0
Read the tokens from the input text file,0
Loop through each token and get scientific name,0
,0
process_species_by_dataset,0
,0
We generated a list of all the annotations in our universe; this script is,0
used to (interactively) map them onto the GBIF and iNat taxonomies.  Don't,0
"try to run this script from top to bottom; it's used like a notebook, not like",0
"a script, since manual review steps are required.",0
,0
%% Imports,0
%autoreload 0,0
%autoreload -species_lookup,0
%% Constants,0
Input file,0
Output file after automatic remapping,0
File to which we manually copy that file and do all the manual review; this,0
should never be programmatically written to,0
The final output spreadsheet,0
HTML file generated to facilitate the identificaiton of egregious mismappings,0
%% Functions,0
Prefer iNat matches over GBIF matches,0
query = 'person',0
Do we have an iNat match?,0
"print_taxonomy_matches(inat_matches, verbose=True)",0
"print_taxonomy_matches(gbif_matches, verbose=True)",0
print('Warning: multiple iNat matches for {}'.format(query)),0
Prefer chordates... most of the names that aren't what we want,0
"are esoteric insects, like a moth called ""cheetah""",0
,0
"If we can't find a chordate, just take the first match.",0
,0
i_test_match = 0,0
"This is (taxonID, taxonLevel, scientific, [list of common])",0
print(f'Warning: multiple iNat common names for {query}'),0
Default to returning the query,0
"print(f'Matched iNat {query} to {scientific_name},{common_name}')",0
...if we had iNat matches,0
If we either prefer GBIF or didn't have iNat matches,0
,0
Code is deliberately redundant here; I'm expecting some subtleties in how,0
handle GBIF and iNat.,0
print('Warning: multiple GBIF matches for {}'.format(query)),0
Prefer chordates... most of the names that aren't what we want,0
"are esoteric insects, like a moth called ""cheetah""",0
,0
"If we can't find a chordate, just take the first match.",0
,0
i_test_match = 0,0
"This is (taxonID, taxonLevel, scientific, [list of common])",0
print(f'Warning: multiple GBIF common names for {query}'),0
Default to returning the query,0
...if we needed to look in the GBIF taxonomy,0
...def get_preferred_taxonomic_match(),0
%% Initialization,0
%% Test single-query lookup,0
%%,0
%%,0
"q = ""grevy's zebra""",0
%% Read the input data,0
%% Run all our taxonomic lookups,0
i_row = 0; row = df.iloc[i_row],0
query = 'lion',0
...for each query,0
Write to the excel file that we'll use for manual review,0
%% Download preview images for everything we successfully mapped,0
uncomment this to load saved output_file,0
"output_df = pd.read_excel(output_file, keep_default_na=False)",0
i_row = 0; row = output_df.iloc[i_row],0
...for each query,0
%% Write HTML file with representative images to scan for obvious mis-mappings,0
i_row = 0; row = output_df.iloc[i_row],0
...for each row,0
%% Look for redundancy with the master table,0
Note: `master_table_file` is a CSV file that is the concatenation of the,0
"manually-remapped files (""manual_remapped.xlsx""), which are the output of",0
this script run across from different groups of datasets. The concatenation,0
"should be done manually. If `master_table_file` doesn't exist yet, skip this",0
"code cell. Then, after going through the manual steps below, set the final",0
manually-remapped version to be the `master_table_file`.,0
%% Manual review,0
Copy the spreadsheet to another file; you're about to do a ton of manual,0
review work and you don't want that programmatically overwrriten.,0
,0
See manual_review_xlsx above,0
%% Read back the results of the manual review process,0
%% Look for manual mapping errors,0
Manually inspect df for typos in 'dataset_names' and 'taxonomy_level' columns,0
Identify rows where:,0
,0
- 'taxonomy_level' does not match level of 1st element in 'taxonomy_string',0
- 'scientific_name' does not match name of 1st element in 'taxonomy_string',0
,0
...both of which typically represent manual mapping errors.,0
i_row = 0; row = df.iloc[i_row],0
"I'm not sure why both of these checks are necessary, best guess is that",1
the Excel parser was reading blanks as na on one OS/Excel version and as '',0
on another.,0
The taxonomy_string column is a .json-formatted string; expand it into,0
an object via eval(),0
"%% Find scientific names that were added manually, and match them to taxonomies",0
i_row = 0; row = df.iloc[i_row],0
...for each query,0
%% Write out final version,0
,0
"Generate read-only SAS URLs for all LILA containers, to facilitate partial downloads.",0
,0
The results of this script end up here:,0
,0
http://lila.science/wp-content/uploads/2020/03/lila_sas_urls.txt,0
,0
"Update: that file is manually maintained now, it can't be programmatically generated",0
,0
%% Imports,0
Read-only,0
%% Enumerate containers,0
%% Generate SAS tokens,0
%% Generate SAS URLs,0
%% Write to output file,0
--------some stuff needed to get AJAX to work with bottle?--------#,0
-------------------------------------------------------------------------------- #,0
PREPARE TO QUEUE IMAGES FOR LABELING,0
-------------------------------------------------------------------------------- #,0
# Connect as USER to database DB_NAME through peewee and initialize database proxy,0
# Load embedding model,0
---------------------------------------------------------------------- #,0
CREATE QUEUE OF IMAGES TO LABEL,0
---------------------------------------------------------------------- #,0
Use classifier to generate predictions,0
# Update model predicted class in PostgreSQL database,0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?,0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
-------------------------------------------------------------------------------- #,0
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI,0
-------------------------------------------------------------------------------- #,0
"# static routes (to serve CSS, etc.)",0
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps",0
"return bottle.static_file(filename, root='../../../../../../../../../../../.')",0
# dynamic routes,0
if data['display_grayscale']:,0
indices_to_exclude.update(set(color_indices)),0
elif not data['display_grayscale']:,0
indices_to_exclude.update(set(grayscale_indices)),0
data['display_images'] = {},0
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices],0
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices],0
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices],0
data['display_images']['detection_categories'] = [],0
for i in indices:,0
if str(dataset.samples[i][1]) == 'None':,0
data['display_images']['detection_categories'].append('None'),0
else:,0
existing_category_entries = {cat.id: cat.name for cat in Category.select()},0
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()",0
data['display_images']['detection_categories'].append(cat_name),0
Use image ids in images_to_label to get the corresponding dataset indices,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
print(set(dataset.set_indices[4]).update(set(indices_to_label))),0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Use image id images_to_label to get the corresponding dataset index,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?,0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Train on samples that have been labeled so far,0
dataset.set_kind(DetectionKind.UserDetection.value),0
print(y_train),0
Predict on the samples that have not been labeled,0
print(y_pred),0
Update model predicted class in PostgreSQL database,0
timer = time.time(),0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
print('Updating the database took %0.2f seconds'%(time.time() - timer)),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids)),0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
"once the classifier has been trained the first time, switch to AL sampling",0
,0
make_oneclass_json.py,0
,0
"Takes a coco-camera-traps .json database and collapses species classes to binary,",0
optionally removing labels from empty images (to be detector-friendly) (depending on,0
"""experiment_type"").",0
,0
"Assumes that empty images are labeled as ""empty"".",0
,0
%% Imports and environment,0
%% Core conversion function,0
"We're removing empty images from the annotation list, but not from",0
"the ""images"" list; they'll still get used in detector training.",0
print('Ignoring empty annotation'),0
%% Interactive driver,0
%%,0
Load annotations,0
Convert from multi-class to one-class,0
Write out the one-class data,0
%% Command-line driver,0
,0
plot_bounding_boxes.py,0
,0
Takes a .json database containing bounding boxes and renders those boxes on the,0
source images.,0
,0
"This assumes annotations in coco-camera-traps format, with absolute bbox",0
coordinates.,0
,0
%% Imports and environment,0
How many images should we process?  Set to -1 to process all images.,0
Should we randomize the image order?,0
%%  Read database and build up convenience mappings,0
Category ID to category name,0
Image ID to image info,0
Image ID to image path,0
Image ID to all annotations referring to this image,0
"%% Iterate over images, draw bounding boxes, write to file",0
For each image,0
image = images[0],0
Build up a list of bounding boxes to draw on this image,0
Load the image,0
Create figure and axes,0
Display the image,0
ann = imageAnnotations[0],0
"For each annotation associated with this image, render bounding box and label",0
"In the Rectangle() function, the first argument (""location"") is the bottom-left",0
of the rectangle.,0
,0
Origin is the upper-left of the image.,0
Add the patch to the Axes,0
Add a class label,0
This is magic goop that removes whitespace around image plots (sort of),0
Write the output image,0
...for each image,0
,0
plot_imerit_annotations.py,0
,0
Takes a .json file full of bounding box annotations and renders those boxes on the,0
source images.,0
,0
"This assumes annotations in the format we receive them, specifically:",0
,0
1) Relative bbox coordinates,0
"2) A list of .json objects, not a well-formatted .json file",0
,0
"I.e., don't use this on a COCO-style .json file.  See plot_bounding_boxes.py",0
for the same operation performed on a proper COCO-camera-traps database.,0
,0
%% Imports and environment,0
"os.makedirs(outputBase, exist_ok=True)",0
%%  Read all source images and build up a hash table from image name to full path,0
"This spans training and validation directories, so it's not the same as",0
just joining the image name to a base path,0
"%% Iterate over annotations, draw bounding boxes, write to file",0
annData has keys:,0
,0
"annotations, categories, images",0
,0
Each of these are lists of dictionaries,0
%% Render all annotations on each image in the sequence,0
%% Pull out image metadata,0
Build up a list of bounding boxes to draw on this image,0
Pull out just the image name from the filename,0
,0
File names look like:,0
,0
seq6efffac2-5567-11e8-b3fe-dca9047ef277.frame1.img59a94e52-23d2-11e8-a6a3-ec086b02610b.jpg,0
"m = re.findall(r'img(.*\.jpg)$', imgFileName, re.M|re.I)",0
print(m),0
assert(len(m) == 1),0
queryFileName = m[0],0
Map this image back to the original directory,0
"%% Loop over annotations, find annotations that match this image",0
%%,0
"x,y,w,h",0
,0
"x,y is the bottom-left of the rectangle",0
,0
"x,y origin is the upper-left",0
...for each annotation,0
%% Render with PIL (scrap),0
%% Render with Matplotlib,0
Create figure and axes,0
Display the image,0
Location is the bottom-left of the rect,0
,0
Origin is the upper-left,0
Add the patch to the Axes,0
This is magic goop that removes whitespace around image plots (sort of),0
os.startfile(outputFileName),0
%% Showing figures on-screen during debugging,0
plt.show(),0
Various (mostly unsuccessful) approaches to getting the plot window to show up,0
"in the foreground, which is a backend-specific operation...",0
,0
fig.canvas.manager.window.activateWindow(),0
fig.canvas.manager.window.raise_(),0
fm = plt.get_current_fig_manager(),0
"fm.window.attributes('-topmost', 1)",0
"fm.window.attributes('-topmost', 0)",0
,0
# This is the one that I found to be most robust... at like 80% robust.,0
plt.get_current_fig_manager().window.raise_(),0
%%,0
...for each image,0
...for each file,0
,0
convert_imerit_json_to_coco_json.py,0
,0
"Takes a .json file with bounding boxes but no class labels, and a .json file containing the",0
"class labels for those images, and creates a new json file with class labels and bounding",0
boxes.,0
,0
"The bounding box .json file is in the format returned by our annotators, which is not",0
actually a fully-formed .json file; rather it's a series of .json objects,0
,0
"Leaves filenames intact.  Stores imerit ""category IDs"" (empty/human/group/animal) in a new",0
"field called ""annotation_type"".",0
,0
%% Imports and constants,0
%% Configure files and paths,0
"For Snapshot Serengeti, we stored image IDs in our annotation files as:",0
,0
S1_B06_R1_PICT0008,0
,0
...but the corresponding ID in the master database is actually:,0
,0
S1\B06\R1\S1_B06_R1_PICT0008,0
,0
"If this is ""True"", we'll expand the former to the latter",0
Handling a one-off issue in which .'s were mysteriously replaced with -'s,0
"in our annotations.  This will be set dynamically, but I keep it here as",0
a constant to remind me to remove this code when we clean this issue up.,1
Used in the (rare) case where a bounding box was added to an image that was originally,0
annotated as empty,0
Used in the (rare) case where we added bounding boxes to an image with multiple species,0
"Should we include ambiguous bounding boxes (with the ""ambiguous"" category label)",0
in the output file?  Ambiguous boxes are boxes drawn on images with multiple species.,0
"%%  Read metadata from the master database, bounding boxes from the annotations file",0
"The bounding box .json file is in the format returned by our annotators, which is not",0
actually a fully-formed .json file; rather it's a series of .json objects,0
"Each element of annData is a dictionary corresponding to a single sequence, with keys:",0
,0
"annotations, categories, images",0
sequence = annData[0],0
%% Build convenience mappings,0
Image ID to images,0
Category ID to categories (referring to the database categories),0
"Image ID to categories (i.e., species labels)",0
Utility function we'll use to create annotations for images in empty,0
sequences (empty images in non-empty sequences already have annotations),0
"%% Reformat annotations, grabbing category IDs from the master database (prep)",0
iSequence = 0; sequence = annData[0],0
"%% Reformat annotations, grabbing category IDs from the master database (loop)",0
Make a copy here; we're going to manipulate the sequence annotations,0
when we need to add synthetic annotations for empty images,0
im = sequenceImages[0],0
Are there any annotations in this sequence?,0
Which images in this sequence have annotations?,0
For each image in this sequence...,0
imeritImageID = im['id'],0
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG,0
Confirm that the file exists,0
Hande a one-off issue with our annotations,0
datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG,0
,0
...had become:,0
,0
datasetsnapshotserengeti.seqASG000001a-frame0.imgS1_B06_R1_PICT0008.JPG,0
Does it look like we encountered this issue?,0
Convert:,0
,0
S1_B06_R1_PICT0008,0
,0
...to:,0
,0
S1/B06/B06_R1/S1_B06_R1_PICT0008,0
Make sure we only see each image once,0
Create empty annotations for empty images,0
,0
Here we use the *unmodified* file name,0
Create an empty annotation for this image,0
Annotations still use the annotation filename (not database ID) at this point;,0
these will get converted to database IDs below when we process the,0
whole sequence.,0
Sanity-check image size,0
"print('Warning: img {} was listed in DB as {}x{}, annotated as {}x{}, actual size{}x{}'.format(",0
"old_id,new_im['width'],new_im['height'],im['width'],im['height'],imgObj.width,imgObj.height))",0
...for each image in this sequence,0
For each annotation in this sequence...,0
ann = sequenceAnnotations[0],0
Prepare an annotation using the category ID from the database and,0
the bounding box from the annotations file,0
Maintain iMerit's annotation category,0
Generate an (arbitrary) ID for this annotation; the COCO format has a concept,0
"of annotation ID, but our annotation files don't",0
This was a one-off quirk with our file naming,1
We'll do special handling of images with multiple categories later,0
Store the annotation type (group/human/animal/empty),0
This annotation has no bounding box but the image wasn't originally,0
annotated as empty,0
This annotation has a bounding box but the image was originally,0
annotated as empty,0
unnormalize the bbox,0
... for each annotation in this sequence,0
... for each sequence,0
%% Post-processing,0
Count empty images,0
...for each file,0
%% Sanity-check empty images,0
,0
make_ss_annotation_image_folder.py,0
,0
Take a directory full of images with the very long filenames we give annotators:,0
,0
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension,0
,0
"...along with a COCO-camera-traps database referring to those files, and:",0
,0
1) Creates a new COCO-camera-traps database with the original filenames in them,0
(copying the annotations),0
,0
2) Optionally creates a new directory with those images named according to the,0
"Snapshot Serengeti naming convention, including complete relative paths.",0
,0
See convert_imerit_json_to_coco_json to see how we get from the original annotation,0
.json to a COCO-camera-traps database.,0
,0
%% Constants and imports,0
%% Configure files/paths,0
%% Read the annotations (referring to the old filenames),0
"%% Update filenames, optionally copying files",0
im = data['images'][0],0
For each image...,0
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG,0
"Find the image name, e.g. S1_B06_R1_PICT0008",0
Convert:,0
,0
S1_B06_R1_PICT0008.JPG,0
,0
...to:,0
,0
S1/B06/B06_R1/S1_B06_R1_PICT0008.JPG,0
...for each image,0
%% Write the revised database,0
,0
get_annotation_tool_link.py,0
,0
"Takes a COCO-camera-traps-style .json file with URLs already embedded, and prepares",0
a link to the visipedia annotation tool that reviews a subset of those images.,0
,0
,0
create_new_annotation_json.py,0
,0
"Creates a subset of a larger .json database, in this case specifically to pick some images",0
from Snapshot Serengeti.,0
,0
from utils import get_db_dicts,0
for seq in already_annotated:,0
seq_to_ims.pop(seq),0
remove already annotated images,0
add lion images,0
lion_seqs = cat_to_seqs[cat_to_id['lionMale']] + cat_to_seqs[cat_to_id['lionFemale']],0
#print(len(lion_seqs)),0
lion_seqs = [seq for seq in lion_seqs if seq not in already_annotated],0
#print(len(lion_seqs)),0
"lion_seqs_to_annotate.extend(random.sample(lion_seqs, min(len(lion_seqs),num_lions)))",0
print(len(lion_seqs_to_annotate)),0
num_elephants = 1000,0
elephant_seqs = cat_to_seqs[cat_to_id['elephant']],0
#print(len(lion_seqs)),0
elephant_seqs = [seq for seq in elephant_seqs if seq not in already_annotated],0
#print(len(lion_seqs)),0
"elephant_seqs_to_annotate = random.sample(elephant_seqs, num_elephants)",0
num_empty = 10,0
empty_seqs_to_annotate = [],0
for loc in loc_to_seqs:,0
empty_seqs = cats_per_location[loc][cat_to_id['empty']],0
empty_seqs = [seq for seq in empty_seqs if seq not in already_annotated],0
empty_seqs = [seq for seq in empty_seqs if seq_to_season[seq] in seasons_to_keep],0
"empty_seqs_to_annotate.extend(random.sample(empty_seqs, min(len(empty_seqs),num_empty)))",0
ims_to_annotate.extend(empty_ims_to_annotate),0
,0
filter_database.py,0
,0
"Look through a COCO-ct database and find images matching some crtieria, writing",0
a subset of images and annotations to a new file.,0
,0
%% Constants and imports,0
%% Configuration,0
%%  Read database and build up convenience mappings,0
Category ID to category name,0
Image ID to image info,0
Image ID to image path,0
%% Filter,0
ann = annotations[0],0
Is this a tiny box or a group annotation?,0
"x,y,w,h",0
All empty annotations should be classified as either empty or ambiguous,0
,0
"The ambiguous cases are basically minor misses on the annotators' part,",0
where two different small animals were present somewhere.,0
%% Write output file,0
,0
find_images_for_classes.py,0
,0
"Given a .json database, find images that are associated with one or more",0
classes.,0
,0
%% Constants and imports,0
%% Configuration,0
%%  Read database and build up convenience mappings,0
Category ID to category name,0
Image ID to image info,0
Image ID to image path,0
%% Look for target-class annotations,0
ann = annotations[0],0
,0
create_tfrecords_format.py,0
,0
This script creates a tfrecords file from a dataset in VOTT format.,0
%% Imports and environment,0
set up the filenames and annotations,0
This loop reads the bboxes and corresponding labels and assigns them,0
the correct image. Kind of slow at the moment...,0
If needed: merging all classes,0
bbox_labels = ['Animal' for _ in bbox_labels],0
BBox coords are stored in the format,0
"x_min (of width axis) y_min (of height axis), x_max, y_max",0
Coordinate system starts in top left corner,0
"In this framework, we need ('ymin', 'xmin', 'ymax', 'xmax') format",0
print out some stats,0
To make sure we loaded the bboxes correctly:,0
self.validate_bboxes(),0
For each image in the data set...,0
Make sure all are greater equal 0,0
%% Main tfrecord generation function,0
Propagate optional metadata to tfrecords,0
endfor each annotation for the current image,0
endfor each image,0
"Calculate number of shards to get the desired number of images per record,",0
ensure it is evenly divisible by the number of threads,0
%% Command-line driver,0
%% Driver,0
,0
make_tfrecords_cis_trans.py,0
,0
Given a .json file that contains a three-element list (train/val/test) of image IDs and a .json database that contains,0
"those image IDs, generates tfrecords whose filenames include ""train""/""val""/""test""",0
,0
"dataset = json.load(open('/ai4efs/databases/snapshotserengeti/oneclass/SnapshotSerengeti_Seasons_1_to_4_tfrecord_format_valid_ims.json','r'))",0
print('Creating trans_val tfrecords'),0
dataset = [im_id_to_im[idx] for idx in trans_val],0
,0
create_classification_tfrecords_from_json.py,0
,0
Called from make_tfrecords_cis_trans.py if you're running a classification experiment.,0
,0
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/',0
datafile = 'eccv_18_annotation_files_oneclass/CaltechCameraTrapsECCV18',0
image_file_root = datafolder+'eccv_18_all_images/',0
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/',0
datafolder = '/data/iwildcam/',0
datafile = 'combined_iwildcam_annotations_oneclass/eccv_train_and_imerit_2',0
need consecutive category ids,0
old_cat_id_to_new_cat_id = {categories[idx]['id']:idx+1 for idx in range(len(categories))},0
print(old_cat_id_to_new_cat_id),0
remove multiclass images,0
print(images[0]),0
print(vis_data[0]),0
,0
make_tfrecords_from_json.py,0
,0
"Given a coco-camera-traps .json file, creates tfrecords",0
,0
Thin wrapper for create_tfrecords_from_json.,0
,0
%% Constants and imports,0
%% Constants and imports (interactive),0
%%,0
%% Main tfrecord generation function,0
"check whether the input file has already been converted to the tfrecords format,",0
"if not, convert",0
"Calculate number of shards to get the desired number of images per record,",0
ensure it is evenly divisible by the number of threads,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
eMammal_make_splits.py,0
,0
"Based on a tfrecords_format json file of the database, creates 3 splits according to",0
the specified fractions based on location (images from the same location should be in,0
one split) or based on images.,0
,0
"If a previous split is provided (append_to_previous_split is True), the entries in",0
"each split will be preserved, and new entries will be appended, so that new models",0
can warm start with a model trained on the original splits.,0
configurations and paths,0
approximate fraction for the new entries,0
read in the previous splits of image ID or location ID if available,0
"find new locations and assign them to a split, without reassigning any previous locations",0
"find out which images are new, shuffle and split them",0
do NOT sort the IDs to keep the shuffled order,0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
add empty category,0
"add all images that don't have annotations, with cat empty",0
print(detection_results.keys()),0
group the detections and gts by image id:,0
print(image_id),0
print(len(scores)),0
print(len(labels)),0
recall_thresh = 0.9,0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall]),0
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])",0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall]),0
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])",0
exp_name = 'eccv_train',0
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json',0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(len(detection_results['images'])),0
print(len(seqs)),0
"print(len(seqs[0]),len(seqs[",0
print(detection_results.keys()),0
group the detections by image id:,0
group the ground truth annotations by image id:,0
print(seq),0
"for image_id, dets in per_image_detections.iteritems():",0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
print(num_gts),0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
if sum(detected_class_labels)>0:,0
print('detected animal box'),0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])",0
"print(scores, tp_fp_labels)",0
num_total_gts += num_gts,0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
print('valid box'),0
"print(best_im, best_score)",0
exp_name = 'eccv_train',0
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json',0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
need to loop over confidence values,0
"for each value, check if any detections on the image are > conf",0
"If so, that image gets class ""animal""",0
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0",0
"for image_id, dets in per_image_detections.iteritems():",0
"calc prec, rec for this confidence thresh",0
group the ground truth annotations by image id:,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(detection_results.keys()),0
group the detections and gts by image id:,0
print(image_id),0
print(len(scores)),0
print(len(labels)),0
recall_thresh = 0.9,0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall]),0
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])",0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall]),0
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])",0
print(sorted_ap),0
plt.bar(sorted_ap),0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(detection_results.keys()),0
group the detections by image id:,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
print(gts),0
print(num_gts),0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[:2])",0
"print(scores, tp_fp_labels)",0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
print('detected animal box'),0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])",0
"print(scores, tp_fp_labels)",0
num_total_gts += num_gts,0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
print('valid box'),0
best_im = np.argmax(max_im_scores),0
"print(best_im, best_score)",0
"temp_labels = np.zeros(len(im_detection_labels),  dtype=np.int32)",0
"temp_scores = np.zeros(len(im_detection_scores), dtype=np.float32)",0
"for j in range(min(im_num_gts, len(im_detection_labels))):",0
temp_labels[j] = True #TODO: this currently only works for oneclass?,1
temp_scores[j] = best_score,0
im_detection_labels = temp_labels,0
im_detection_scores = temp_scores,0
num_total_gts+=im_num_gts,0
"print(len(detection_scores), len(detection_scores[0]), len(detection_scores[1]))",0
exp_name = 'small_balanced_cct',0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(seq),0
"for image_id, dets in per_image_detections.iteritems():",0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
seq_num_gts.append(num_gts),0
print(num_gts),0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
num_total_gts += 1,0
print('valid box'),0
"print(best_im, best_score)",0
print('no valid box'),0
if sum(seq_num_gts)>0:,0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
need to loop over confidence values,0
"for each value, check if any detections on the image are > conf",0
"If so, that image gets class ""animal""",0
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0",0
"calc prec, rec for this confidence thresh",0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
group the detections by image id:,0
group the ground truth annotations by image id:,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
print('detected animal box'),0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])",0
"print(scores, tp_fp_labels)",0
num_total_gts += num_gts,0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
print('valid box'),0
"print(best_im, best_score)",0
,0
evaluate_detections.py,0
,0
Adapted from analyze_detection.py which is now archived.,0
,0
%% Imports and constants,0
%% Functions,0
"labels input to compute_object_detection_metrics() needs to start at 0, not 1",0
num_detections = len(dets['boxes']),0
to prevent 'Invalid dimensions for box data.' error,0
this box will not match any detections,0
compute one-class precision/recall/average precision (if every box is just of an object class),0
%% Command-line driver,0
,0
detection_eval_utils.py,0
,0
Utility functions used in evaluate_detections.py,0
,0
group the ground truth annotations by image id,0
#####,0
,0
detect_and_predict_image.py,0
,0
"Functions to load a TensorFlow detection and a classification model, run inference,",0
"render bounding boxes on images, and write out the resulting",0
images (with bounding boxes and classes).,0
,0
"See the ""test driver"" cell for example invocation.",0
,0
,0
#####,0
"%% Constants, imports, environment",0
Minimum detection confidence for showing a bounding box on the output image,0
Stick this into filenames before the extension for the rendered result,0
Number of top-scoring classes to show at each bounding box,0
%% Core detection functions,0
Load images if they're not already numpy arrays,0
iImage = 0; image = images[iImage],0
"Load the image as an nparray of size h,w,nChannels",0
"There was a time when I was loading with PIL and switched to mpimg,",0
"but I can't remember why, and converting to RGB is a very good reason",0
"to load with PIL, since mpimg doesn't give any indication of color",0
"order, which basically breaks all .png files.",0
,0
"So if you find a bug related to using PIL, update this comment",0
"to indicate what it was, but also disable .png support.",0
image = mpimg.imread(image),0
This shouldn't be necessarily when loading with PIL and converting to RGB,0
Actual detection,0
...for each image,0
"Currently ""boxes"" is a list of length nImages, where each element is shaped as",0
,0
"1,nDetections,4",0
,0
"This implicitly banks on TF giving us back a fixed number of boxes, let's assert on this",0
to make sure this doesn't silently break in the future.,0
iBox = 0; box = boxes[iBox],0
"""scores"" is a length-nImages list of elements with size 1,nDetections",0
"""classes"" is a length-nImages list of elements with size 1,nDetections",0
,0
"Still as floats, but really representing ints",0
Squeeze out the empty axis,0
boxes is nImages x nDetections x 4,0
scores and classes are both nImages x nDetections,0
Get input and output tensors of classification model,0
"imsize = cur_image['width'], cur_image['height']",0
Select detections with a confidence larger 0.5,0
Get these boxes and convert normalized coordinates to pixel coordinates,0
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes",0
"However, we need to make sure that it box coordinates are still within the image",0
For convenience:,0
Create an array with contains the index of the corresponding crop_box for each selected box,0
i.e. [False False 0 False 1 2 3 False False],0
For each box,0
If this box should be classified,0
Run inference,0
if box should not be classified,0
...for each box,0
species_scores should have shape len(images) x len(boxes) x num_species,0
...for each image,0
...with tf.Session,0
with classification_graph,0
species_scores should have shape len(images) x len(boxes) x num_species,0
%% Rendering functions,0
Display the image,0
plt.show(),0
"top, left, bottom, right",0
,0
"x,y origin is the upper-left",0
Generate bounding box text,0
Choose color based on class,0
Location is the bottom-left of the rect,0
,0
Origin is the upper-left,0
iRight = x + w,0
iTop = y + h,0
Add the patch to the Axes,0
Add class description,0
First determine best location by finding the corner that is closest to the image center,0
relative corner coordinates,0
relative coordinates of image center,0
Compute pair-wise squared distance and get the index of the one with minimal distance,0
Get the corresponding coordinates ...,0
... and alignment for the text box,0
Plot the text box with background,0
...for each box,0
This is magic goop that removes whitespace around image plots (sort of),0
"plt.savefig(outputFileName, bbox_inches='tight', pad_inches=0.0, dpi=dpi, transparent=True)",0
os.startfile(outputFileName),0
...for each image,0
...def render_bounding_boxes,0
Load and run detector on target images,0
Read the name of all classes,0
remove empty lines,0
%% Interactive driver,0
%%,0
%%,0
%% File helper functions,0
%% Command-line driver,0
Hack to avoid running on already-detected images,1
,0
"Script for evaluating a frozen graph given a COCO-style dataset, which was generated by",0
the ../data_management/databases/classification/make_classification_dataset.py script.,0
This is useful for debugging as the accuracy reported by this script should match the,0
accuracy reported by the Tensorflow training.,0
,0
Check that all files exists for easier debugging,0
Load frozen graph,0
Collect tensors for input and output,0
Read image,0
Run inference,0
Check if correct,0
,0
Script for evaluating precision/recall on a two-class problem given,0
"a frozen graph and a COCO-style dataset, which was generated by",0
the ../data_management/databases/classification/make_classification_dataset.py script.,0
We assume the positive class is at index 0 (with a zero-based indexing),0
,0
Check that all files exists for easier debugging,0
Load frozen graph,0
Collect tensors for input and output,0
Read image,0
"with open(image_path, 'rb') as fi:",0
"image =  sess.run(tf.image.decode_jpeg(fi.read(), channels=3))",0
image = image / 255.,0
Run inference,0
predicted_class = np.argmax(predictions),0
Check if correct,0
if coco.imgToAnns[image_id][0]['category_id'] == predicted_class:,0
correct = correct + 1,0
"In matplotlib < 1.5, plt.fill_between does not have a 'step' argument",0
,0
Script for selecting testing images of a COCO-style dataset generated by the script,0
../data_management/databases/classification/make_classification_dataset.py in a consistent,0
manner and predicting the class for it.,0
,0
Mandatory parameters,0
Optional parameters,0
Validate parameters,0
Derived parameters,0
"We assume that the dataset was generated with the make_classification_dataset.py script,",0
hence the images should be located in the same folder as the json,0
Make seletion deterministic,0
Load frozen graph,0
Get dataset information,0
Get classes,0
...and the class list corresponding to the model outputs by assuming,0
that they are in order of their ids,0
Get images of each class,0
Shuffle the image list,0
Start the image sampling,0
"Set of avaiable class IDs, will be filled below",0
If there are still images left for that class,0
"Get image for the sampled class, we already shuffled the class images before so",0
we can simply pop(),0
Start prediction,0
Collect tensors for input and output,0
Read image,0
Run inference,0
Print output to log file,0
#####,0
,0
api_apply_classifier_single_node.py,0
,0
Takes the JSON file produced by the detection API and,0
classifies all boxes above a confidence threshold.,0
,0
#####,0
"%% Constants, imports, environment",0
Assumes that the root of the CameraTraps repo is on the PYTHONPATH,0
Minimum detection confidence for classifying an object,0
Number of top-scoring classes to show at each bounding box,0
Enlargment factor applied to boxes before passing them to the classifier,0
,0
Provides more context and can lead to better results,0
List of detection categories for which we will run the classification,0
,0
"Currently there are {""1"": ""animal"", ""2"": ""person"", ""4"": ""vehicle""}",0
,0
Should be a list of string-formatted ints.,0
Number of significant float digits in JSON output,0
%% Core detection functions,0
Read the name of all classes,0
remove empty lines,0
Create field with name *classification_categories*,0
Add classes using 0-based indexing,0
def add_classification_categories,0
Make sure we have the right json object,0
Get input and output tensors of classification model,0
For each image,0
Read image,0
"Scale pixel values to [0,1]",0
For each box,0
Skip detections with low confidence,0
Skip if detection category is not in whitelist,0
Skip if already classified,0
"Get current box in relative coordinates and format [x_min, y_min, width_of_box, height_of_box]",0
"Convert to [ymin, xmin, ymax, xmax] and store it as 1x4 numpy array so we can",0
re-use the generic multi-box padding code,0
Convert normalized coordinates to pixel coordinates,0
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes.",0
,0
"However, we need to make sure that it box coordinates are still within the image.",0
Get the first (and only) row as our bbox to classify,0
Get the image data for that box,0
Run inference,0
Add an empty list to the json for our predictions,0
Add the *num_annotated_classes* top scoring classes,0
...for each box,0
...for each image,0
...with tf.Session,0
with classification_graph,0
def classify_boxes,0
Load classification model,0
Load detector json,0
Add classes to detector_json,0
"Run classifier on all images, changes will be writting directly to the json",0
Write output json,0
def load_and_run_classifier,0
%% Command-line driver,0
,0
Script for selecting testing images of a COCO-style dataset generated by the script,0
../data_management/databases/classification/make_classification_dataset.py in a consistent,0
manner and predicting the class for it.,0
,0
Assumes the cameratraps repo root is on the path,0
Make seletion deterministic,0
Mandatory parameters,0
Optional parameters,0
Validate parameters,0
Tranfer parameters to post-processing format,0
Load frozen graph,0
Reading image list,0
Reading class list,0
Image sampling,0
Start prediction,0
Collect tensors for input and output,0
Array for collecting infos for rendering the html,0
Read image,0
"Scale pixel values to [0,1]",0
Run inference,0
Add links to all available classes,0
,0
predict_image.py,0
,0
"Given a pointer to a frozen detection graph, runs inference on a single image,",0
printing the top classes to the console,0
,0
%% Imports,0
%% Command-line processing,0
Check that all files exist for easier debugging,0
%% Inference,0
Load frozen graph,0
Load class list,0
Remove empty lines,0
Collect tensors for input and output,0
Read image,0
Run inference,0
Print output,0
,0
Mostly unmodified script for freezing a model,0
Added for convenience and for possible future optimizations,0
,0
Copyright 2015 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
'input_checkpoint' may be a prefix if we're using Saver V2 format,0
Remove all the explicit device specifications for this node. This helps to,0
make the graph more portable.,0
List of all partition variables. Because the condition is heuristic,0
"based, the list could include false positives.",0
This tensor doesn't exist in the graph (for example it's,0
'global_step' or a similar housekeeping element) so skip it.,0
`var_list` is required to be a map of variable names to Variable,0
tensors. Partition variables are Identity tensors that cannot be,0
handled by Saver.,0
Models that have been frozen previously do not contain Variables.,0
optimized_output_graph_def = optimize_for_inference_lib.optimize_for_inference(,0
"output_graph_def,",0
"input_node_names.replace("" "", """").split("",""),",0
"output_node_names.replace("" "", """").split("",""),",0
tf.float32.as_datatype_enum),0
Write GraphDef to file if output path has been given.,0
,0
"Creates a graph description, which is required to create a frozen graph.",0
Adapted from from ./tf-slim/export_inference_graph.py,0
Added preprocessing to the definition for easier handling,0
,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
tf.app.flags.DEFINE_integer(,0
"'batch_size', None,",0
"'Batch size for the exported model. Defaulted to ""None"" so batch size can '",0
'be specified at model runtime.'),0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
#####################,0
Optimization Flags #,0
#####################,0
######################,0
Learning Rate Flags #,0
######################,0
######################,0
Dataset Flags #,0
######################,0
####################,0
Fine-Tuning Flags #,0
####################,0
"Note: when num_clones is > 1, this will actually have each clone to go",0
over each epoch FLAGS.num_epochs_per_decay times. This is different,0
behavior from sync replicas and is expected to produce different results.,0
Warn the user if a checkpoint exists in the train_dir. Then we'll be,0
ignoring the checkpoint anyway.,0
TODO(sguada) variables.filter_variables(),1
######################,0
Config model_deploy #,0
######################,0
Create global_step,0
#####################,0
Select the dataset #,0
#####################,0
#####################,0
Select the network #,0
#####################,0
####################################,0
Select the preprocessing function #,0
####################################,0
#############################################################,0
Create a dataset provider that loads data from the dataset #,0
#############################################################,0
###################,0
Define the model #,0
###################,0
############################,0
Specify the loss function #,0
############################,0
Gather initial summaries.,0
"Gather update_ops from the first clone. These contain, for example,",0
the updates for the batch_norm variables created by network_fn.,0
Add summaries for end_points.,0
Add summaries for losses.,0
Add summaries for variables.,0
################################,0
Configure the moving averages #,0
################################,0
########################################,0
Configure the optimization procedure. #,0
########################################,0
"If sync_replicas is enabled, the averaging will be done in the chief",0
queue runner.,0
Update ops executed locally by trainer.,0
Variables to train.,0
and returns a train_tensor and summary_op,0
Add total_loss to summary.,0
Create gradient updates.,0
Add the summaries from the first clone. These contain the summaries,0
created by model_fn and either optimize_clones() or _gather_clone_loss().,0
Merge all summaries together.,0
##########################,0
Kicks off the training. #,0
##########################,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
#####################,0
Select the dataset #,0
#####################,0
###################,0
Select the model #,0
###################,0
#############################################################,0
Create a dataset provider that loads data from the dataset #,0
#############################################################,0
####################################,0
Select the preprocessing function #,0
####################################,0
###################,0
Define the model #,0
###################,0
Define the metrics:,0
Print the summaries to screen.,0
TODO(sguada) use num_epochs=1,1
This ensures that we make a single pass over all of the data.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Transform the image to floats.,0
"Randomly crop a [height, width] section of the image.",0
Randomly flip the image horizontally.,0
"Because these operations are not commutative, consider randomizing",0
the order their operation.,0
Subtract off the mean and divide by the variance of the pixels.,0
Transform the image to floats.,0
Resize and crop if needed.,0
Subtract off the mean and divide by the variance of the pixels.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Use tf.slice instead of crop_to_bounding box as it accepts tensors to,0
define the crop size.,0
Compute the rank assertions.,0
Create a random bounding box.,0
,0
Use tf.random_uniform and not numpy.random.rand as doing the former would,0
"generate random numbers at graph eval time, unlike the latter which",0
generates random numbers at graph definition time.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Pass the real x only to one of the func calls.,0
The random_* ops do not necessarily clamp.,0
"Each bounding box has shape [1, num_boxes, box coords] and",0
"the coordinates are ordered [ymin, xmin, ymax, xmax].",0
A large fraction of image datasets contain a human-annotated bounding,0
box delineating the region of the image containing the object of interest.,0
We choose to create a new bounding box for the object which is a randomly,0
distorted version of the human-annotated bounding box that obeys an,0
"allowed range of aspect ratios, sizes and overlap with the human-annotated",0
"bounding box. If no box is supplied, then we assume the bounding box is",0
the entire image.,0
Crop the image to the specified bounding box.,0
"Each bounding box has shape [1, num_boxes, box coords] and",0
"the coordinates are ordered [ymin, xmin, ymax, xmax].",0
Restore the shape since the dynamic slice based upon the bbox_size loses,0
the third dimension.,0
This resizing operation may distort the images because the aspect,0
ratio is not respected. We select a resize method in a round robin,0
fashion based on the thread number.,0
Note that ResizeMethod contains 4 enumerated resizing methods.,0
We select only 1 case for fast_mode bilinear.,0
Randomly flip the image horizontally.,0
Randomly distort the colors. There are 1 or 4 ways to do it.,0
Crop the central region of the image with an area containing 87.5% of,0
the original image.,0
Resize the image to the specified height and width.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TODO(sguada) docstring paragraph by (a) motivating the need for the file and,1
(b) defining clones.,0
TODO(sguada) describe the high-level components of model deployment.,1
"E.g. ""each model deployment is composed of several parts: a DeploymentConfig,",0
"which captures A, B and C, an input_fn which loads data.. etc",0
Set up DeploymentConfig,0
Create the global step on the device storing the variables.,0
Define the inputs,0
Define the optimizer.,0
Define the model including the loss.,0
Run training.,0
Namedtuple used to represent a clone during deployment.,0
"Namedtuple used to represent a DeployedModel, returned by deploy().",0
Default parameters for DeploymentConfig,0
Create clones.,0
The return value.,0
Individual components of the loss that will need summaries.,0
Compute and aggregate losses on the clone device.,0
Add the summaries out of the clone device block.,0
Only use regularization_losses for the first clone,0
Compute the total_loss summing all the clones_losses.,0
Sum the gradients across clones.,0
Gather initial summaries.,0
Create Clones.,0
"Gather update_ops from the first clone. These contain, for example,",0
the updates for the batch_norm variables created by model_fn.,0
Place the global step on the device storing the variables.,0
Compute the gradients for the clones.,0
Add summaries to the gradients.,0
Create gradient updates.,0
Only use regularization_losses for the first clone,0
Add the summaries from the first clone. These contain the summaries,0
created by model_fn and either optimize_clones() or _gather_clone_loss().,0
Add total_loss to summary.,0
Merge all summaries together.,0
Note that each grad_and_vars looks like the following:,0
"((grad_var0_clone0, var0), ... (grad_varN_cloneN, varN))",0
!/usr/bin/python,0
Copyright 2016 Google Inc. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Failed to find ""index"" occurrence of item.",0
pylint: disable=broad-except,0
pylint: enable=broad-except,0
Grab the 'index' annotation.,0
Some images contain bounding box annotations that,0
"extend outside of the supplied image. See, e.g.",0
n03127925/n03127925_147.xml,0
"Additionally, for some bounding boxes, the min > max",0
or the box is entirely outside of the image.,0
Example: <...>/n06470073/n00141669_6790.xml,0
Determine if the annotation is from an ImageNet Challenge label.,0
Note: There is a slight bug in the bounding box annotation data.,0
Many of the dog labels have the human label 'Scottish_deerhound',0
instead of the synset ID 'n02092002' in the bbox.label field. As a,0
"simple hack to overcome this issue, we only exclude bbox labels",1
*which are synset ID's* that do not match original synset label for,0
the XML file.,0
Guard against improperly specified boxes.,0
Note bbox.filename occasionally contains '%s' in the name. This is,0
data set noise that is fixed by just using the basename of the XML file.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The URLs where the MNIST data can be downloaded.,0
The names of the classes.,0
"First, process the training data:",0
"Next, process the testing data:",0
"Finally, write the labels file:",0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The URL where the Flowers data can be downloaded.,0
The number of images in the validation set.,0
Seed for repeatability.,0
The number of shards per dataset split.,0
Initializes function that decodes RGB JPEG data.,0
Read the filename:,0
Divide into train and test:,0
"First, convert the training and validation sets.",0
"Finally, write the labels file:",0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TODO(nsilberman): Add tfrecord file type once the script is updated.,1
"If set to false, will not try to set label_to_names in dataset",0
by reading them from labels.txt or github.,0
n01440764,0
n01443537,0
n02119247    black fox,0
n02119359    silver fox,0
pylint: disable=g-line-too-long,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The URL where the CIFAR data can be downloaded.,0
The number of training files.,0
The height and width of each image.,0
The names of the classes.,0
"First, process the training data:",0
"Next, process the testing data:",0
"Finally, write the labels file:",0
Copyright 2016 Google Inc. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The labels file contains a list of valid labels are held in this file.,0
Assumes that the file contains entries as such:,0
n01440764,0
n01443537,0
n01484850,0
where each line corresponds to a label expressed as a synset. We map,0
each synset contained in the file to an integer (based on the alphabetical,0
ordering). See below for details.,0
This file containing mapping from synset to human-readable label.,0
Assumes each line of the file looks like:,0
,0
n02119247    black fox,0
n02119359    silver fox,0
"n02119477    red fox, Vulpes fulva",0
,0
where each line corresponds to a unique mapping. Note that each line is,0
formatted as <synset>\t<human readable label>.,0
This file is the output of process_bounding_box.py,0
Assumes each line of the file looks like:,0
,0
"n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940",0
,0
where each line corresponds to one bounding box annotation associated,0
with an image. Each line can be parsed as:,0
,0
"<JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>",0
,0
Note that there might exist mulitple bounding box annotations associated,0
with an image file.,0
pylint: disable=expression-not-assigned,0
pylint: enable=expression-not-assigned,0
Create a single Session to run all image coding calls.,0
Initializes function that converts PNG to JPEG data.,0
Initializes function that converts CMYK JPEG data to RGB JPEG data.,0
Initializes function that decodes RGB JPEG data.,0
File list from:,0
https://groups.google.com/forum/embed/?place=forum/torch7#!topic/torch7/fOSTXHIESSU,0
File list from:,0
https://github.com/cytsai/ilsvrc-cmyk-image-list,0
Read the image file.,0
Clean the dirty data.,0
1 image is a PNG.,0
22 JPEG images are in CMYK colorspace.,0
Decode the RGB JPEG.,0
Check that image converted to RGB,0
Each thread produces N shards where N = int(num_shards / num_threads).,0
"For instance, if num_shards = 128, and the num_threads = 2, then the first",0
"thread would produce shards [0, 64).",0
"Generate a sharded version of the file name, e.g. 'train-00002-of-00010'",0
"Break all images into batches with a [ranges[i][0], ranges[i][1]].",0
Launch a thread for each batch.,0
Create a mechanism for monitoring when all threads are finished.,0
Create a generic TensorFlow-based utility for converting all image codings.,0
Wait for all the threads to terminate.,0
Leave label index 0 empty as a background class.,0
Construct the list of JPEG files and labels.,0
Shuffle the ordering of all image files in order to guarantee,0
random ordering of the images with respect to label in the,0
saved TFRecord files. Make the randomization repeatable.,0
Build a map from synset to human-readable label.,0
Run it!,0
Allowing None in the signature so that dataset_factory can use the default.,0
!/usr/bin/python,0
Copyright 2016 Google Inc. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Read in the 50000 synsets associated with the validation data set.,0
Make all sub-directories in the validation data dir.,0
Move all of the image to the appropriate sub-directory.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
"The dataset has classes with no images (empty and human), which have ID 0 and 1,",0
so we need to specify 49 here despite having only 47 classes with images,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Final pooling and prediction,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The current_stride variable keeps track of the effective stride of the,0
activations. This allows us to invoke atrous convolution whenever applying,0
the next residual unit would result in the activations having stride larger,0
than the target output_stride.,0
The atrous convolution rate parameter.,0
Move stride from the block's last unit to the end of the block.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Collect activations at the block's end before performing subsampling.,0
Subsampling of the block's output activations.,0
"The following implies padding='SAME' for pool1, which makes feature",0
alignment easier for dense prediction tasks. This is also used in,0
https://github.com/facebook/fb.resnet.torch. However the accompanying,0
code of 'Deep Residual Learning for Image Recognition' uses,0
padding='VALID' for pool1. You can switch to that choice by setting,0
"slim.arg_scope([slim.max_pool2d], padding='VALID').",0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Collect outputs for conv2d, fully_connected and max_pool2d",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"inputs has shape [batch, 224, 224, 3]",0
"inputs has shape [batch, 513, 513, 3]",0
We do not include batch normalization or activation functions in,0
conv1 because the first ResNet unit will perform these. Cf.,0
Appendix of [2].,0
This is needed because the pre-activation variant does not have batch,1
normalization or activation functions in the residual unit output. See,0
Appendix of [2].,0
Convert end_points_collection into a dictionary of end_points.,0
Global average pooling.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
=============================================================================,0
"These parameters come from the online port, which don't necessarily match",0
those in the paper.,0
TODO(nsilberman): confirm these values with Philip.,1
##########,0
Encoder #,0
##########,0
"No normalizer for the first encoder layers as per 'Image-to-Image',",0
Section 5.1.1,0
First layer doesn't use normalizer_fn,0
Last layer doesn't use activation_fn nor normalizer_fn,0
##########,0
Decoder #,0
##########,0
"Dropout is used at both train and test time as per 'Image-to-Image',",0
Section 2.1 (last paragraph).,0
The Relu comes BEFORE the upsample op:,0
Explicitly set the normalizer_fn to None to override any default value,0
"that may come from an arg_scope, such as pix2pix_arg_scope.",0
No normalization on the input layer.,0
Stride 1 on the last layer.,0
"1-dim logits, stride 1, no activation, no normalization.",0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
Alias,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pylint: disable=unused-import,0
pylint: enable=unused-import,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
If we are fine tuning a checkpoint we need to start at a lower learning,0
rate since we are farther along on training.,0
We can start quantizing immediately if we are finetuning.,0
We need to wait for the model to train a bit before we quantize if we are,0
training from scratch.,0
Call rewriter to produce graph with fake quant ops and folded batch norms,0
"quant_delay delays start of quantization till quant_delay steps, allowing",0
for better model accuracy.,0
Configure the learning rate using an exponential decay.,0
"When restoring from a floating point model, the min/max values for",0
quantized weights and activations are not present.,0
We instruct slim to ignore variables that are missing during restoration,0
by setting ignore_missing_vars=True,0
"If we are restoring from a floating point model, we need to initialize",0
the global step to zero for the exponential decay to result in,0
reasonable learning rates.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Orignaly, arg_scope = slim.arg_scope and layers = slim, now switch to more",0
update-to-date tf.contrib.* API.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Turns off fused batch norm.,0
collection containing the moving mean and moving variance.,0
"For inverted pyramid models, we start with gating switched off.",0
batch_size x 32 x 112 x 112 x 64,0
Separable conv is slow when used at first conv layer.,0
batch_size x 32 x 56 x 56 x 64,0
batch_size x 32 x 56 x 56 x 64,0
batch_size x 32 x 56 x 56 x 192,0
batch_size x 32 x 28 x 28 x 192,0
batch_size x 32 x 28 x 28 x 256,0
batch_size x 16 x 14 x 14 x 512,0
batch_size x 16 x 14 x 14 x 512,0
batch_size x 16 x 14 x 14 x 512,0
batch_size x 16 x 14 x 14 x 528,0
batch_size x 16 x 14 x 14 x 832,0
batch_size x 8 x 7 x 7 x 832,0
batch_size x 8 x 7 x 7 x 1024,0
Final pooling and prediction,0
Temporal average pooling.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
299 x 299 x 3,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 160,0
71 x 71 x 192,0
35 x 35 x 384,0
4 x Inception-A blocks,0
35 x 35 x 384,0
Reduction-A block,0
17 x 17 x 1024,0
7 x Inception-B blocks,0
17 x 17 x 1024,0
Reduction-B block,0
8 x 8 x 1536,0
3 x Inception-C blocks,0
Auxiliary Head logits,0
17 x 17 x 1024,0
Final pooling and prediction,0
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which",1
can be set to False to disable pooling here (as in resnet_*()).,0
8 x 8 x 1536,0
1 x 1 x 1536,0
1536,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN,1
setups need the gradient of gradient FusedBatchNormGrad.,0
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN,1
setups need the gradient of gradient FusedBatchNormGrad.,0
First upscaling is different because it takes the input vector.,0
Last layer has different normalizer and activation.,0
Convert to proper channels.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80,0
71 x 71 x 192,0
35 x 35 x 192,0
35 x 35 x 320,0
TODO(alemi): Register intermediate endpoints,1
"17 x 17 x 1088 if output_stride == 8,",0
33 x 33 x 1088 if output_stride == 16,0
TODO(alemi): register intermediate endpoints,1
TODO(gpapan): Properly support output_stride for the rest of the net.,1
8 x 8 x 2080,0
TODO(alemi): register intermediate endpoints,1
8 x 8 x 1536,0
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which",1
can be set to False to disable pooling here (as in resnet_*()).,0
Set weight_decay for weights in conv2d and fully_connected layers.,0
Set activation_fn and parameters for batch_norm.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
collection containing update_ops.,0
use fused batch norm if possible.,0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"inputs has shape [batch, 224, 224, 3]",0
"inputs has shape [batch, 513, 513, 3]",0
Use clip_by_value to simulate bandpass activation.,0
Convert end_points_collection into a dictionary of end_points.,0
Global average pooling.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"end_points will collect relevant activations for external use, for example",0
summaries or losses.,0
299 x 299 x 3,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80.,0
71 x 71 x 192.,0
35 x 35 x 192.,0
Inception blocks,0
mixed: 35 x 35 x 256.,0
mixed_1: 35 x 35 x 288.,0
mixed_2: 35 x 35 x 288.,0
mixed_3: 17 x 17 x 768.,0
mixed4: 17 x 17 x 768.,0
mixed_5: 17 x 17 x 768.,0
mixed_6: 17 x 17 x 768.,0
mixed_7: 17 x 17 x 768.,0
mixed_8: 8 x 8 x 1280.,0
mixed_9: 8 x 8 x 2048.,0
mixed_10: 8 x 8 x 2048.,0
Auxiliary Head logits,0
Shape of feature map before the final layer.,0
Final pooling and prediction,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 2048,0
2048,0
1000,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Reflection pad by 1 in spatial dimensions (axes 1, 2 = h, w) to make a 3x3",0
'valid' convolution produce an output with the same dimension as the,0
input.,0
This corrects 1 pixel offset for images with even width and height.,0
conv2d is left aligned and conv2d_transpose is right aligned for even,0
sized images (while doing 'SAME' padding).,0
Note: This doesn't reflect actual model in paper.,0
Neither dropout nor batch norm -> dont need is_training,0
##########,0
Encoder #,0
##########,0
7x7 input stage,0
##################,0
Residual Blocks #,0
##################,0
##########,0
Decoder #,0
##########,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"end_points will collect relevant activations for external use, for example",0
summaries or losses.,0
Used to find thinned depths for each layer.,0
Note that sizes in the comments below assume an input spatial size of,0
"224x224, however, the inputs can be of any size greater 32x32.",0
224 x 224 x 3,0
depthwise_multiplier here is different from depth_multiplier.,0
depthwise_multiplier determines the output channels of the initial,0
"depthwise conv (see docs for tf.nn.separable_conv2d), while",0
depth_multiplier controls the # channels of the subsequent 1x1,0
convolution. Must have,0
in_channels * depthwise_multipler <= out_channels,0
so that the separable convolution is not overparameterized.,0
Use a normal convolution instead of a separable convolution.,0
112 x 112 x 64,0
56 x 56 x 64,0
56 x 56 x 64,0
56 x 56 x 192,0
28 x 28 x 192,0
Inception module.,0
28 x 28 x 256,0
28 x 28 x 320,0
14 x 14 x 576,0
14 x 14 x 576,0
14 x 14 x 576,0
14 x 14 x 576,0
14 x 14 x 576,0
7 x 7 x 1024,0
7 x 7 x 1024,0
Final pooling and prediction,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 1024,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
=============================================================================,0
Tensorflow mandates these.,0
Conv and DepthSepConv namedtuple define layers of the MobileNet architecture,0
Conv defines 3x3 convolution layers,0
DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.,0
stride is the stride of the convolution,0
depth is the number of channels or filters in a layer,0
MOBILENETV1_CONV_DEFS specifies the MobileNet body,0
Used to find thinned depths for each layer.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
By passing filters=None,0
separable_conv2d produces only a depthwise convolution layer,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 1024,0
Set weight_decay for weights in Conv and DepthSepConv layers.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Turns off fused batch norm.,0
collection containing the moving mean and moving variance.,0
Final pooling and prediction,0
Temporal average pooling.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Orignaly, add_arg_scope = slim.add_arg_scope and layers = slim, now switch to",0
more update-to-date tf.contrib.* API.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
Only insert rate to params if rate > 1.,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Notes for training NASNet Cifar Model,0
-------------------------------------,0
batch_size: 32,0
learning rate: 0.025,0
cosine (single period) learning rate decay,0
auxiliary head loss weighting: 0.4,0
clip global norm of all gradients by 5,0
600 epochs with a batch size of 32,0
This is used for the drop path probabilities since it needs to increase,0
the drop out probability over the course of training.,0
Notes for training large NASNet model on ImageNet,0
-------------------------------------,0
batch size (per replica): 16,0
learning rate: 0.015 * 100,0
learning rate decay factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 100 replicas,0
auxiliary head loss weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Notes for training the mobile NASNet ImageNet model,0
-------------------------------------,0
batch size (per replica): 32,0
learning rate: 0.04 * 50,0
learning rate scaling factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 50 replicas,0
auxiliary head weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Shape of feature map before the final layer.,0
149 x 149 x 32,0
Run the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Find where to place the reduction cells or stride normal cells,0
Setup for building in the auxiliary head.,0
Run the cells,0
true_cell_num accounts for the stem cells,0
Final softmax layer,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Find where to place the reduction cells or stride normal cells,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Setup for building in the auxiliary head.,0
Run the cells,0
true_cell_num accounts for the stem cells,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Final softmax layer,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Calculate the total number of cells in the network.,0
There is no distinction between reduction and normal cells in PNAS so the,0
total number of cells is equal to the number normal cells plus the number,0
of stem cells (two by default).,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Calculate the total number of cells in the network.,0
There is no distinction between reduction and normal cells in PNAS so the,0
total number of cells is equal to the number normal cells plus the number,0
of stem cells (two by default).,0
Configuration for the PNASNet-5 model.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"The cap for tf.clip_by_value, it's hinted from the activation distribution",0
"that the majority of activation values are in the range [-6, 6].",0
Skip path 1,0
Skip path 2,0
"First pad with 0's on the right and bottom, then shift the filter to",0
include those 0's that were added.,0
"If odd number of filters, add an additional one to the second path.",0
Concat and apply BN,0
Set the prev layer to the current layer if it is none,0
Check to be sure prev layer stuff is setup correctly,0
num_or_size_splits=1,0
Apply conv operations,0
Combine hidden states using 'add'.,0
Add hiddenstate to the list of hiddenstates we can choose from,0
Dont stride if this is not one of the original hiddenstates,0
"Check if a stride is needed, then use a strided 1x1 here",0
Determine if a reduction should be applied to make the number of,0
filters match.,0
Return the concat of all the states,0
Scale keep prob by layer number,0
The added 2 is for the reduction cells,0
Decrease the keep probability over time,0
,0
Script for generating a two-class dataset in COCO format for training an obscured image classifier,0
,0
Requires Python >= 3.6 because of the glob ** expression,0
,0
Collect images and labels,0
"Labels: clean = 0, obscured = 1",0
,0
top_folders_to_bottom.py,0
,0
Given a base folder with files like:,0
,0
A/1/2/a.jpg,0
B/3/4/b.jpg,0
,0
"...moves the top-level folders to the bottom in a new output folder, i.e., creates:",0
,0
1/2/A/a.jpg,0
3/4/B/b.jpg,0
,0
"In practice, this is used to make this:",0
,0
animal/camera01/image01.jpg,0
,0
...look like:,0
,0
camera01/animal/image01.jpg,0
,0
%% Constants and imports,0
%% Support functions,0
%% Main functions,0
Find top-level folder,0
Find file/folder names,0
Move or copy,0
...def process_file(),0
Enumerate input folder,0
Convert absolute paths to relative paths,0
Standardize delimiters,0
Make sure each input file maps to a unique output file,0
relative_filename = relative_files[0],0
Loop,0
...def top_folders_to_bottom(),0
%% Interactive driver,0
%%,0
%%,0
%% Command-line driver,0
"python top_folders_to_bottom.py ""g:\temp\separated_images"" ""g:\temp\separated_images_inverted"" --n_threads 100",0
Convert to an options object,0
%% Constants and imports,0
Assumes ai4eutils is on the python path (https://github.com/Microsoft/ai4eutils),0
Assumes the cameratraps repo root is on the path,0
%% Options,0
"Make sure there is no overlap between the two sets, because this will cause",0
issues in the code,0
## Required inputs,0
## Options,0
Can be a folder or a SAS URL,0
# These apply only when we're doing ground-truth comparisons,0
Classes we'll treat as negative,0
,0
"Include the token ""#NO_LABELS#"" to indicate that an image with no annotations",0
should be considered empty.,0
Classes we'll treat as neither positive nor negative,0
"A list of output sets that we should count, but not render images for.",0
,0
"Typically used to preview sets with lots of empties, where you don't want to",0
"subset but also don't want to render 100,000 empty images.",0
,0
"detections, non_detections",0
"detections_animal, detections_person, detections_vehicle",0
"By default, choose a confidence threshold based on the detector version",0
Used for summary statistics only,0
"Number of images to sample, -1 for ""all images""",0
"Random seed for sampling, or None",0
Optionally separate detections into categories (animal/vehicle/human),0
,0
Currently only supported when ground truth is unavailable,0
Optionally replace one or more strings in filenames with other strings;,0
useful for taking a set of results generated for one folder structure,0
and applying them to a slightly different folder structure.,0
Allow bypassing API output loading when operating on previously-loaded,0
results,0
Should we also split out a separate report about the detections that were,0
just below our main confidence threshold?,0
,0
Currently only supported when ground truth is unavailable,0
Control rendering parallelization,0
...PostProcessingOptions,0
#%% Helper classes and functions,0
Anything greater than this isn't clearly positive or negative,0
image has annotations suggesting both negative and positive,0
"image is not annotated or is annotated with 'unknown', 'unlabeled', ETC.",0
image has not yet been assigned a state,0
"In some analyses, we add an additional class that lets us look at",0
detections just below our main confidence threshold,0
count the # of images with each type of DetectionStatus,0
Check whether this image has:,0
- unknown / unassigned-type labels,0
- negative-type labels,0
"- positive labels (i.e., labels that are neither unknown nor negative)",0
"assert has_unknown_labels is False, '{} has unknown labels'.format(annotations)",0
If there are no image annotations...,0
n_negative += 1,0
im['_detection_status'] = DetectionStatus.DS_NEGATIVE,0
"If the image has more than one type of labels, it's ambiguous",0
"note: bools are automatically converted to 0/1, so we can sum",0
"After the check above, we can be sure it's only one of positive,",0
"negative, or unknown.",0
,0
Important: do not merge the following 'unknown' branch with the first,0
"'unknown' branch above, where we tested 'if len(categories) == 0'",0
,0
If the image has only unknown labels,0
If the image has only negative labels,0
If the images has only positive labels,0
"Annotate the category, if it is unambiguous",0
...for each image,0
...mark_detection_status(),0
"Leaving code in place for reading from blob storage, may support this",0
in the future.,0
resize is to display them in this notebook or in the HTML more quickly,0
os.path.isfile() is slow when mounting remote directories; much faster,0
to just try/except on the image open.,0
return '',0
Render images to a flat folder... we can use os.sep here because we've,0
already normalized paths,0
"Create class labels like ""gt_1"" or ""gt_27""",0
"for i_box,box in enumerate(ground_truth_boxes):",0
gt_classes.append('_' + str(box[-1])),0
"errno.ENAMETOOLONG doesn't get thrown properly on Windows, so",0
we awkwardly check against a hard-coded limit,0
Use slashes regardless of os,0
Optionally add links back to the original images,0
...render_bounding_boxes,0
Count items in each category,0
Optionally sort by filename before writing to html,0
Write the individual HTML files,0
...prepare_html_subpages(),0
Get unique categories above the threshold for this image,0
Render an image (with no ground truth information),0
"This is a list of [class,confidence] pairs, sorted by confidence",0
"If we either don't have a confidence threshold, or we've met our",0
confidence threshold,0
...if this detection has classification info,0
...for each detection,0
...def render_image_no_gt(),0
This should already have been normalized to either '/' or '\',0
...def render_image_with_gt(),0
%% Main function,0
#%% Expand some options for convenience,0
#%% Prepare output dir,0
#%% Load ground truth if available,0
Mark images in the ground truth as positive or negative,0
#%% Load detection (and possibly classification) results,0
"If the caller hasn't supplied results, load them",0
Determine confidence thresholds if necessary,0
Remove failed rows,0
Convert keys and values to lowercase,0
"Add column 'pred_detection_label' to indicate predicted detection status,",0
not separating out the classes,0
#%% Pull out descriptive metadata,0
This is rare; it only happens during debugging when the caller,0
is supplying already-loaded API results.,0
"#%% If we have ground truth, remove images we can't match to ground truth",0
#%% Sample images for visualization,0
#%% Fork here depending on whether or not ground truth is available,0
"If we have ground truth, we'll compute precision/recall and sample tp/fp/tn/fn.",0
,0
Otherwise we'll just visualize detections/non-detections.,0
#%% Detection evaluation: compute precision/recall,0
numpy array of detection probabilities,0
"numpy array of bools (0.0/1.0), and -1 as null value",0
Don't include ambiguous/unknown ground truth in precision/recall analysis,0
"For completeness, include the result at a confidence threshold of 1.0",0
Compute and print summary statistics,0
Thresholds go up throughout precisions/recalls/thresholds; find the last,0
value where recall is at or above target.  That's our precision @ target recall.,0
"np.where returns a tuple of arrays, but in this syntax where we're",0
"comparing an array with a scalar, there will only be one element.",0
Convert back to a list,0
Flatten the confusion matrix,0
"#%% Collect classification results, if they exist",0
Mapping of classnames to idx for the confusion matrix.,0
,0
"The lambda is actually kind of a hack, because we use assume that",1
the following code does not reassign classname_to_idx,0
Confusion matrix as defaultdict of defaultdict,0
,0
"Rows / first index is ground truth, columns / second index is predicted category",0
iDetection = 0; fn = detector_files[iDetection]; print(fn),0
"If this image has classification predictions, and an unambiguous class",0
"annotated, and is a positive image...",0
"The unambiguous category, we make this a set for easier handling afterward",0
"Compute the accuracy as intersection of union,",0
i.e. (# of categories in both prediciton and GT),0
divided by (# of categories in either prediction or GT,0
,0
"In case of only one GT category, the result will be 1.0, if",0
prediction is one category and this category matches GT,0
,0
"It is 1.0/(# of predicted top-1 categories), if the GT is",0
one of the predicted top-1 categories.,0
,0
"It is 0.0, if none of the predicted categories is correct",0
Distribute this accuracy across all predicted categories in the,0
confusion matrix,0
...for each file in the detection results,0
If we have classification results,0
Build confusion matrix as array from classifier_cm,0
Print some statistics,0
Prepare confusion matrix output,0
Get confusion matrix as string,0
Get fixed-size classname for each idx,0
Prepend class name on each line and add to the top,0
Print formatted confusion matrix,0
"Actually don't, this gets really messy in all but the widest consoles",0
Plot confusion matrix,0
To manually add more space at bottom: plt.rcParams['figure.subplot.bottom'] = 0.1,0
,0
"Add 0.5 to figsize for every class. For two classes, this will result in",0
"fig = plt.figure(figsize=[4,4])",0
...if we have classification results,0
#%% Render output,0
Write p/r table to .csv file in output directory,0
Write precision/recall plot to .png file in output directory,0
#%% Sampling,0
Sample true/false positives/negatives with correct/incorrect top-1,0
classification and render to html,0
Accumulate html image structs (in the format expected by write_html_image_lists),0
"for each category, e.g. 'tp', 'fp', ..., 'class_bird', ...",0
Add default entries by accessing them for the first time,0
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]",0
"Each element will be a three-tuple with elements file,max_conf,detections",0
"Assemble the information we need for rendering, so we can parallelize without",0
dealing with Pandas,0
i_row = 0; row = images_to_visualize.iloc[0],0
Filenames should already have been normalized to either '/' or '\',0
Map all the rendering results in the list rendering_results into the,0
dictionary images_html,0
Prepare the individual html image files,0
Write index.html,0
Show links to each GT class,0
,0
We could do this without classification results; currently we don't.,0
Add links to all available classes,0
Close body and html tags,0
...for each image,0
"#%% Otherwise, if we don't have ground truth...",0
#%% Sample detections/non-detections,0
Accumulate html image structs (in the format expected by write_html_image_list),0
for each category,0
Add default entries by accessing them for the first time,0
"Maps detection categories - e.g. ""human"" - to result set names, e.g.",0
"""detections_human""",0
Add a set of results for each category and combination of categories,0
Create output directories,0
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]",0
"list of 3-tuples with elements (file, max_conf, detections)",0
"Assemble the information we need for rendering, so we can parallelize without",0
dealing with Pandas,0
i_row = 0; row = images_to_visualize.iloc[0],0
Filenames should already have been normalized to either '/' or '\',0
"render_image_no_gt(file_info,detection_categories_to_results_name,",0
"detection_categories,classification_categories)",0
Map all the rendering results in the list rendering_results into the,0
dictionary images_html,0
Prepare the individual html image files,0
Write index.html,0
"We can't just sum these, because image_counts includes images in both their",0
detection and classification classes,0
total_images = sum(image_counts.values()),0
Don't print classification classes here; we'll do that later with a slightly,0
different structure,0
Add links to all available classes,0
os.startfile(output_html_file),0
...if we do/don't have ground truth,0
...process_batch_results,0
%% Interactive driver(s),0
%%,0
options.unlabeled_classes = ['human'],0
os.start(ppresults.output_html_file),0
%% Command-line driver,0
,0
load_api_results.py,0
,0
Loads the output of the batch processing API (json) into a pandas dataframe.,0
,0
Also functions to group entries by seq_id.,0
,0
Includes the deprecated functions that worked with the old CSV API output format.,0
,0
%% Constants and imports,0
%% Functions for grouping by sequence_id,0
example,0
input 'file': 'SER/S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG',0
output 'id': 'S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG',0
%% Functions for loading the result as a Pandas DataFrame,0
Validate that this is really a detector output file,0
Fields in the API output json other than 'images',0
Normalize paths to simplify comparisons later,0
"image['file'] = image['file'].replace('\\','/')",0
Replace some path tokens to match local paths to original blob structure,0
"If this is a newer file that doesn't include maximum detection confidence values,",0
"add them, because our unofficial internal dataframe format includes this.",0
Pack the json output into a Pandas DataFrame,0
Sanity-check that this is really a detector output file,0
Normalize paths to simplify comparisons later,0
De-serialize detections,0
Optionally replace some path tokens to match local paths to the original blob structure,0
string_to_replace = list(options.detector_output_filename_replacements.keys())[0],0
"TODO: hit some silly issues with vectorized str() and escaped characters, vectorize",1
this later.,0
,0
"detection_results['image_path'].str.replace(string_to_replace,replacement_string)",0
iRow = 0,0
%% Imports,0
Assumes ai4eutils is on the python path (https://github.com/Microsoft/ai4eutils),0
%% Constants and support classes,0
We will confirm that this matches what we load from each file,0
Process-based parallelization isn't supported yet,0
%% Main function,0
"Warn the user if some ""detections"" might not get rendered",0
#%% Validate inputs,0
#%% Load both result sets,0
assert results_a['detection_categories'] == default_detection_categories,0
assert results_b['detection_categories'] == default_detection_categories,0
#%% Make sure they represent the same set of images,0
#%% Find differences,0
"Each of these maps a filename to a two-element list (the image in set A, the image in set B)",0
,0
"Right now, we only handle a very simple notion of class transition, where the detection",0
of maximum confidence changes class *and* both images have an above-threshold detection.,0
fn = filenames_a[0],0
We shouldn't have gotten this far if error_on_non_matching_lists is set,0
det = im_a['detections'][0],0
...for each filename,0
#%% Sample and plot differences,0
"Render two sets of results (i.e., a comparison) for a single",0
image.,0
...def render_image_pair(),0
fn = image_filenames[0],0
...def render_detection_comparisons(),0
"For each category, generate comparison images and the",0
comparison HTML page.,0
,0
category = 'common_detections',0
Choose detection pairs we're going to render for this category,0
...for each category,0
#%% Write the top-level HTML file content,0
...def compare_batch_results(),0
%% Interactive driver,0
%% KRU,0
%% Command-line driver,0
# TODO,1
,0
"Merge high-confidence detections from one results file into another file,",0
when the target file does not detect anything on an image.,0
,0
Does not currently attempt to merge every detection based on whether individual,0
detections are missing; only merges detections into images that would otherwise,0
be considered blank.,0
,0
"If you want to literally merge two .json files, see combine_api_outputs.py.",0
,0
%% Constants and imports,0
%% Structs,0
Don't bother merging into target images where the max detection is already,0
higher than this threshold,0
"If you want to merge only certain categories, specify one",0
(but not both) of these.,0
%% Main function,0
im = output_data['images'][0],0
"Determine whether we should be processing all categories, or just a subset",0
of categories.,0
i_source_file = 0; source_file = source_files[i_source_file],0
source_im = source_data['images'][0],0
detection_category = list(detection_categories)[0],0
"This is already a detection, no need to proceed looking for detections to",0
transfer,0
Boxes are x/y/w/h,0
source_sizes = [det['bbox'][2]*det['bbox'][3] for det in source_detections_this_category_raw],0
Only look at boxes below the size threshold,0
...for each detection category,0
"print('Adding {} detections to image {}'.format(len(detections_to_transfer),image_filename))",0
Update the max_detection_conf field (if present),0
...for each image,0
...for each source file,0
%% Test driver,0
%%,0
%% Command-line driver (TODO),0
,0
separate_detections_into_folders.py,0
,0
## Overview,0
,0
"Given a .json file with batch processing results, separate the files in that",0
"set of results into folders that contain animals/people/vehicles/nothing,",0
according to per-class thresholds.,0
,0
"Image files are copied, not moved.",0
,0
,0
## Output structure,0
,0
Preserves relative paths within each of those folders; cannot be used with .json,0
files that have absolute paths in them.,0
,0
"For example, if your .json file has these images:",0
,0
a/b/c/1.jpg,0
a/b/d/2.jpg,0
a/b/e/3.jpg,0
a/b/f/4.jpg,0
a/x/y/5.jpg,0
,0
And let's say:,0
,0
"* The results say that the first three images are empty/person/vehicle, respectively",0
"* The fourth image is above threshold for ""animal"" and ""person""",0
* The fifth image contains an animal,0
,0
* You specify an output base folder of c:\out,0
,0
You will get the following files:,0
,0
c:\out\empty\a\b\c\1.jpg,0
c:\out\people\a\b\d\2.jpg,0
c:\out\vehicles\a\b\e\3.jpg,0
c:\out\animal_person\a\b\f\4.jpg,0
c:\out\animals\a\x\y\5.jpg,0
,0
## Rendering bounding boxes,0
,0
"By default, images are just copied to the target output folder.  If you specify --render_boxes,",0
bounding boxes will be rendered on the output images.  Because this is no longer strictly,0
"a copy operation, this may result in the loss of metadata.  More accurately, this *may*",0
result in the loss of some EXIF metadata; this *will* result in the loss of IPTC/XMP metadata.,0
,0
Rendering boxes also makes this script a lot slower.,0
,0
## Classification-based separation,0
,0
"If you have a results file with classification data, you can also specify classes to put",0
"in their own folders, within the ""animals"" folder, like this:",0
,0
"--classification_thresholds ""deer=0.75,cow=0.75""",0
,0
"So, e.g., you might get:",0
,0
c:\out\animals\deer\a\x\y\5.jpg,0
,0
"In this scenario, the folders within ""animals"" will be:",0
,0
"deer, cow, multiple, unclassified",0
,0
"""multiple"" in this case only means ""deer and cow""; if an image is classified as containing a",0
"bird and a bear, that would end up in ""unclassified"", since the folder separation is based only",0
on the categories you provide at the command line.,0
,0
"No classification-based separation is done within the animal_person, animal_vehicle, or",0
animal_person_vehicle folders.,0
,0
%% Constants and imports,0
Occasionally we have near-zero confidence detections associated with COCO classes that,0
didn't quite get squeezed out of the model in training.  As long as they're near zero,0
"confidence, we just ignore them.",0
%% Options class,0
"Dictionary mapping categories (plus combinations of categories, and 'empty') to output folders",0
Populated only when using classification results,0
"Originally specified as a string, converted to a dict mapping name:threshold",0
...__init__(),0
...class SeparateDetectionsIntoFoldersOptions,0
%% Support functions,0
Find the maximum confidence for each category,0
,0
det = detections[0],0
"For zero-confidence detections, we occasionally have leftover goop",0
from COCO classes,0
assert det['conf'] < invalid_category_epsilon,0
...for each detection on this image,0
Count the number of thresholds exceeded,0
...for each category,0
If this is above multiple thresholds,0
"TODO: handle species-based separation in, e.g., the animal_person case",1
"Are we making species classification folders, and is this an animal?",0
Do we need to put this into a specific species folder?,0
Find the animal-class detections that are above threshold,0
Count the number of classification categories that are above threshold for at,0
least one detection,0
d = valid_animal_detections[0],0
classification = d['classifications'][0],0
"Do we have a threshold for this category, and if so, is",0
this classification above threshold?,0
...for each classification,0
...for each detection,0
...if we have to deal with classification subfolders,0
...if we have 0/1/more categories above threshold,0
...if this is/isn't a failure case,0
Skip this image if it's empty and we're not processing empty images,0
"At this point, this image is getting copied; we may or may not also need to",0
draw bounding boxes.,0
Do a simple copy operation if we don't need to render any boxes,0
Open the source image,0
"Render bounding boxes for each category separately, beacuse",0
we allow different thresholds for each category.,0
"When we're not using classification folders, remove classification",0
information to maintain standard detection colors.,0
...for each category,0
Read EXIF metadata,0
"Write output with EXIF metadata if available, and quality='keep' if this is a JPEG",0
"image.  Unfortunately, neither parameter likes ""None"", so we get a slightly",0
icky cascade of if's here.,0
Also see:,0
,0
https://discuss.dizzycoding.com/determining-jpg-quality-in-python-pil/,0
,0
...for more ways to preserve jpeg quality if quality='keep' doesn't do the trick.,0
...if we don't/do need to render boxes,0
...def process_detections(),0
%% Main function,0
Create output folder if necessary,0
Load detection results,0
Map class names to output folders,0
Create all combinations of categories,0
category_name = category_names[0],0
Do we have a custom threshold for this category?,0
Create folder mappings for each category,0
Create the actual folders,0
"Handle species classification thresholds, if specified",0
"E.g. deer=0.75,cow=0.75",0
token = tokens[0],0
...for each token,0
...if classification thresholds are still in string format,0
Validate the classes in the threshold list,0
...if we need to deal with classification categories,0
i_image = 14; im = images[i_image]; im,0
...for each image,0
...def separate_detections_into_folders,0
%% Interactive driver,0
%%,0
%%,0
%%,0
%% Testing various command-line invocations,0
"With boxes, no classification",0
"No boxes, no classification (default)",0
"With boxes, with classification",0
"No boxes, with classification",0
%% Command-line driver,0
"python api\batch_processing\postprocessing\separate_detections_into_folders.py ""d:\temp\rspb_mini.json"" ""d:\temp\demo_images\rspb_2018_2019_mini"" ""d:\temp\separation_test"" --nthreads 2",0
Convert to an options object,0
"print('{} {}'.format(v,name))",0
List of category numbers to use in separation; uses all categories if None,0
"Can be ""size"", ""width"", or ""height""",0
For each image...,0
,0
im = images[0],0
d = im['detections'][0],0
Are there really any detections here?,0
Is this a category we're supposed to process?,0
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing#detector-outputs,0
...for each detection,0
...for each image,0
...def categorize_detections_by_size(),0
,0
add_max_conf.py,0
,0
"The MD output format included a ""max_detection_conf"" field with each image",0
up to and including version 1.2; it was removed as of version 1.3 (it's,0
redundant with the individual detection confidence values).,1
,0
"Just in case someone took a dependency on that field, this script allows you",0
to add it back to an existing .json file.,0
,0
%% Imports and constants,0
%% Main function,0
%% Driver,0
#######,0
,0
remove_repeat_detections.py,0
,0
"Used after running find_repeat_detections, then manually filtering the results,",0
to create a final filtered output file.,0
,0
"If you want to use this script, we recommend that you read the user's guide:",0
,0
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms,0
,0
#######,0
%% Constants and imports,0
%% Main function,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
#######,0
,0
repeat_detections_core.py,0
,0
Core utilities shared by find_repeat_detections and remove_repeat_detections.,0
,0
#######,0
%% Imports and environment,0
"from ai4eutils; this is assumed to be on the path, as per repo convention",0
"""PIL cannot read EXIF metainfo for the images""",0
"""Metadata Warning, tag 256 had too many entries: 42, expected 1""",0
%% Constants,0
%% Classes,0
Relevant for rendering the folder of images for filtering,0
,0
"imageBase can also be a SAS URL, in which case some error-checking is",0
disabled.,0
Don't consider detections with confidence lower than this as suspicious,0
Don't consider detections with confidence higher than this as suspicious,0
What's the IOU threshold for considering two boxes the same?,0
How many occurrences of a single location (as defined by the IOU threshold),0
are required before we declare it suspicious?,0
"Ignore ""suspicious"" detections larger than some size; these are often animals",0
taking up the whole image.  This is expressed as a fraction of the image size.,0
"Ignore ""suspicious"" detections smaller than some size",0
Ignore folders with more than this many images in them,0
A list of classes we don't want to treat as suspicious. Each element is an int.,0
Load detections from a filter file rather than finding them from the detector output,0
".json file containing detections, should be called detectionIndex.json in the filtering_* folder",0
produced in the first pass,0
(optional) List of filenames remaining after deletion of identified,0
repeated detections that are actually animals.  This should be a flat,0
"text file, one relative filename per line.  See enumerate_images().",0
Turn on/off optional outputs,0
Determines whether bounding-box rendering errors (typically network errors) should,0
be treated as failures,0
Box rendering options,0
State variables,0
"Replace filename tokens after reading, useful when the directory structure",0
has changed relative to the structure the detector saw,0
How many folders up from the leaf nodes should we be going to aggregate images?,0
An optional function that takes a string (an image file name) and returns,0
"a string (the corresponding  folder ID), typically used when multiple folders",0
actually correspond to the same camera in a manufacturer-specific way (e.g.,0
a/b/c/RECONYX100 and a/b/c/RECONYX101 may really be the same camera).,0
Include/exclude specific folders... only one of these may be,0
"specified; ""including"" folders includes *only* those folders.",0
"Optionally show *other* detections (i.e., detections other than the",0
one the user is evaluating) in a light gray,0
"If bRenderOtherDetections is True, what color should we use to render the",0
(hopefully pretty subtle) non-target detections?,0
,0
"In theory I'd like these ""other detection"" rectangles to be partially",0
"transparent, but this is not straightforward, and the alpha is ignored",0
"here.  But maybe if I leave it here and wish hard enough, someday it",0
will work.,0
,0
otherDetectionsColors = ['dimgray'],0
Sort detections within a directory so nearby detections are adjacent,0
"in the list, for faster review.",0
,0
"Can be None, 'xsort', or 'clustersort'",0
,0
* None sorts detections chronologically by first occurrence,0
* 'xsort' sorts detections from left to right,0
* 'clustersort' clusters detections and sorts by cluster,0
Only relevant if smartSort == 'clustersort',0
"The data table (Pandas DataFrame), as loaded from the input json file via",0
load_api_results(),0
"The other fields in the input json file, loaded via load_api_results()",0
The data table after modification,0
dict mapping folder names to whole rows from the data table,0
dict mapping filenames to rows in the master table,0
"An array of length nDirs, where each element is a list of DetectionLocation",0
objects for that directory that have been flagged as suspicious,0
"This is a bit of a hack right now, but for future-proofing, I don't want to call this",1
"to retrieve anything other than the highest-confidence detection, and I'm assuming this",0
"is already sorted, so assert() that.",0
It's not clear whether it's better to use instances[0].bbox or self.bbox,1
"here... they should be very similar, unless iouThreshold is very low.",0
self.bbox is a better representation of the overal DetectionLocation.,0
%% Helper functions,0
"We store detetions as x/y/w/h, rtree and pyqtree use l/b/r/t",0
"We store detetions as x/y/w/h, rtree and pyqtree use l/b/r/t",0
%% Sort a list of candidate detections to make them visually easier to review,0
Just sort by the X location of each box,0
"Prepare a list of points to represent each box,",0
that's what we'll use for clustering,0
Upper-left,0
"points.append([det.bbox[0],det.bbox[1]])",0
Center,0
"Labels *could* be any unique labels according to the docs, but in practice",0
they are unique integers from 0:nClusters,0
Make sure the labels are unique incrementing integers,0
Store the label assigned to each cluster,0
"Now sort the clusters by their x coordinate, and re-assign labels",0
so the labels are sortable,0
"Compute the centroid for debugging, but we're only going to use the x",0
"coordinate.  This is the centroid of points used to represent detections,",0
which may be box centers or box corners.,0
old_cluster_label_to_new_cluster_label[old_cluster_label] =\,0
new_cluster_labels[old_cluster_label],0
%% Look for matches (one directory),0
List of DetectionLocations,0
candidateDetections = [],0
Create a tree to store candidate detections,0
For each image in this directory,0
,0
iDirectoryRow = 0; row = rows.iloc[iDirectoryRow],0
,0
"iDirectoryRow is a pandas index, so it may not start from zero;",0
"for debugging, we maintain i_iteration as a loop index.",0
print('Searching row {} of {} (index {}) in dir {}'.\,0
"format(i_iteration,len(rows),iDirectoryRow,dirName))",0
Don't bother checking images with no detections above threshold,0
"Array of dicts, where each element is",0
{,0
"'category': '1',  # str value, category ID",0
"'conf': 0.926,  # confidence of this detections",0
,0
"# (x_min, y_min) is upper-left, all in relative coordinates",0
"'bbox': [x_min, y_min, width_of_box, height_of_box]",0
,0
},0
For each detection in this image,0
"This is no longer strictly true; I sometimes run RDE in stages, so",0
some probabilities have already been made negative,0
,0
assert confidence >= 0.0 and confidence <= 1.0,0
Optionally exclude some classes from consideration as suspicious,0
Is this detection too big to be suspicious?,0
print('Illegal zero-size bounding box on image {}'.format(filename)),0
These are relative coordinates,0
print('Ignoring very small detection with area {}'.format(area)),0
print('Ignoring very large detection with area {}'.format(area)),0
This will return candidates of all classes,0
For each detection in our candidate list,0
Don't match across categories,0
Is this a match?,0
"If so, add this example to the list for this detection",0
We *don't* break here; we allow this instance to possibly,0
match multiple candidates.  There isn't an obvious right or,0
wrong here.,0
...for each detection on our candidate list,0
"If we found no matches, add this to the candidate list",0
candidateDetections.append(candidate),0
pyqtree,0
...for each detection,0
...for each row,0
Get all candidate detections,0
print('Found {} candidate detections for folder {}'.format(,0
"len(candidateDetections),dirName))",0
"For debugging only, it's convenient to have these sorted",0
as if they had never gone into a tree structure.  Typically,0
this is in practce a sort by filename.,0
...def find_matches_in_directory(dirName),0
"%% Update the detection table based on suspicious results, write .csv output",0
"An array of length nDirs, where each element is a list of DetectionLocation",0
objects for that directory that have been flagged as suspicious,0
For each directory,0
For each suspicious detection group in this directory,0
For each instance of this suspicious detection,0
This should match the bbox for the detection event,0
The bbox for this instance should be almost the same as the bbox,0
"for this detection group, where ""almost"" is defined by the IOU",0
threshold.,0
if iou < options.iouThreshold:,0
"print('IOU warning: {},{}'.format(iou,options.iouThreshold))",0
Make sure the bounding box matches,0
"Make the probability negative, if it hasn't been switched by",0
another bounding box,0
...for each instance,0
...for each detection,0
...for each directory,0
Update maximum probabilities,0
For each row...,0
No longer strictly true; sometimes I run RDE on RDE output,0
assert maxPOriginal >= 0,0
We should only be making detections *less* likely in this process,0
"If there was a meaningful change, count it",0
"Negative probabilities should be the only reason maxP changed, so",0
we should have found at least one negative value if we reached,0
this point.,0
...if there was a meaningful change to the max probability for this row,0
...for each row,0
If we're also writing output...,0
"...def update_detection_table(RepeatDetectionResults,options)",0
%% Main function,0
#%% Input handling,0
Validate some options,0
Load the filtering file,0
Load the same options we used when finding repeat detections,0
...except for things that explicitly tell this function not to,0
find repeat detections.,0
...if we're loading from an existing filtering file,0
Check early to avoid problems with the output folder,0
"Load file to a pandas dataframe.  Also populates 'max_detection_conf', even if it's",0
not present in the .json file.,0
detectionResults[detectionResults['failure'].notna()],0
"Before doing any real work, make sure we can *probably* access images",0
"This is just a cursory check on the first image, but it heads off most",0
"problems related to incorrect mount points, etc.  Better to do this before",0
spending 20 minutes finding repeat detections.,0
#%% Separate files into directories,0
This will be a map from a directory name to smaller data frames,0
This is a mapping back into the rows of the original table,0
iRow = 0; row = detectionResults.iloc[0],0
Create a new DataFrame with just this row,0
rowsByDirectory[dirName] = pd.DataFrame(row),0
...for each unique detection,0
Convert lists of rows to proper DataFrames,0
#% Look for matches (or load them from file),0
"Are we actually looking for matches, or just loading from a file?",0
length-nDirs list of lists of DetectionLocation objects,0
We're actually looking for matches...,0
"We get slightly nicer progress bar behavior using threads, by passing a pbar",0
object and letting it get updated.  We can't serialize this object across,0
processes.,0
#%% Find suspicious locations based on match results,0
For each directory,0
,0
iDir = 51,0
A list of DetectionLocation objects,0
A list of DetectionLocation objects,0
occurrenceList is a list of file/detection pairs,0
Sort the above-threshold detections for easier review,0
...for each directory,0
If we're just loading detections from a file...,0
"We're skipping detection-finding, but to see which images are actually legit false",0
"positives, we may be looking for physical files or loading from a text file.",0
For each directory,0
iDir = 0; detections = suspiciousDetections[0],0
,0
"suspiciousDetections is an array of DetectionLocation objects,",0
one per directory.,0
For each detection that was present before filtering,0
iDetection = 0; detection = detections[iDetection],0
Are we checking the directory to see whether detections were actually false,0
"positives, or reading from a list?",0
Is the image still there?,0
"If not, remove this from the list of suspicious detections",0
...for each detection,0
...for each directory,0
...if we are/aren't finding detections (vs. loading from file),0
Create filtering directory,0
iDir = 0; suspiciousDetectionsThisDir = suspiciousDetections[iDir],0
suspiciousDetectionsThisDir is a list of DetectionLocation objects,0
iDetection = 0; detection = suspiciousDetectionsThisDir[0],0
Sort instances in descending order by confidence,0
Choose the highest-confidence index,0
Should we render (typically in a very light color) detections,0
*other* than the one we're highlighting here?,0
Render other detections first (typically in a thin+light box),0
Now render the example detection (on top of at least one,0
of the other detections),0
This converts the *first* instance to an API standard detection;,0
"because we just sorted this list in descending order by confidence,",0
this is the highest-confidence detection.,0
...if we are/aren't rendering other bounding boxes,0
...for each detection in this folder,0
...for each folder,0
Write out the detection index,0
...if we're writing filtering info,0
...find_repeat_detections(),0
#######,0
,0
find_repeat_detections.py,0
,0
"If you want to use this script, we recommend that you read the user's guide:",0
,0
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms,0
,0
"Really, don't try to run this script without reading the user's guide, you'll think",0
it's more magical than it is.,0
,0
"This script looks through a sequence of detections in the API output json file, and finds",0
"candidates that might be ""repeated false positives"", i.e. that random branch that the",0
detector thinks is an animal/person/vehicle.,0
,0
"Typically after running this script, you would do a manual step to remove",0
"true positives, then run remove_repeat_detections to produce a final output file.",0
,0
There's no way that statement was self-explanatory; see the user's guide.,0
,0
#######,0
%% Constants and imports,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
Convert to an options object,0
,0
If a request has been sent to AML for batch scoring but the monitoring thread of the API was,0
"interrupted (uncaught exception or having to re-start the API container), we could manually",0
"aggregate results from each shard using this script, assuming all jobs submitted to AML have finished.",0
,0
Need to have set environment variables STORAGE_ACCOUNT_NAME and STORAGE_ACCOUNT_KEY to those of the,0
"storage account backing the API. Also need to adjust the INTERNAL_CONTAINER, AML_CONTAINER and",0
AML_CONFIG fields in api_core/orchestrator_api/api_config.py to match the instance of the API that this,0
request was submitted to.,0
,0
May need to change the import statement in api_core/orchestrator_api/orchestrator.py,0
"""from sas_blob_utils import SasBlob"" to",0
"""from .sas_blob_utils import SasBlob"" to not confuse with the module in AI4Eutils;",0
"and change ""import api_config"" to",0
"""from api.batch_processing.api_core.orchestrator_api import api_config""",0
Execute this script from the root of the repository. You may need to add the repository to PYTHONPATH.,0
"list_jobs_submitted cannot be serialized (""can't pickle _thread.RLock objects ""), but",0
do not need it for aggregating results,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%% helper classes and functions,0
TODO log exception when we have more telemetry,1
TODO check that the expiry date of input_container_sas is at least a month,0
into the future,0
"if no permission specified explicitly but has an access policy, assumes okay",0
TODO - check based on access policy as well,1
return current UTC time as a string in the ISO 8601 format (so we can query by,0
timestamp in the Cosmos DB job status table.,0
example: '2021-02-08T20:02:05.699689Z',0
"image_paths will have length at least 1, otherwise would have ended before this step",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
a job moves from created to running/problem after the Batch Job has been submitted,0
"job_id should be unique across all instances, and is also the partition key",0
TODO do not read the entry first to get the call_params when the Cosmos SDK add a,1
patching functionality:,0
https://feedback.azure.com/forums/263030-azure-cosmos-db/suggestions/6693091-be-able-to-do-partial-updates-on-document,0
need to retain other fields in 'status' to be able to restart monitoring thread,0
retain existing fields; update as needed,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
sentinel should change if new configurations are available,0
configs have not changed,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Gunicorn logger handler will get attached if needed in server.py,0
set for all tasks in the job,0
cannot execute the scoring script that is in the mounted directory; has to be copied to cwd,0
not luck giving the commandline arguments via formatted string - set as env vars instead,0
form shards of images and assign each shard to a Task,0
for persisting stdout and stderr,0
persist stdout and stderr (will be removed when node removed),0
paths are relative to the Task working directory,0
can also just upload on failure,0
first try submitting Tasks,0
retry submitting Tasks,0
Change the Job's on_all_tasks_complete option to 'terminateJob' so the Job's status changes automatically,0
after all submitted tasks are done,0
This is so that we do not take up the quota for active Jobs in the Batch account.,0
return type: TaskAddCollectionResult,0
actually we should probably only re-submit if it's a server_error,0
docs: # https://docs.microsoft.com/en-us/rest/api/batchservice/odata-filters-in-batch#list-tasks,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%% Flask app,0
reference: https://trstringer.com/logging-flask-gunicorn-the-manageable-way/,0
%% Helper classes,0
%% Flask endpoints,0
required params,0
can be an URL to a file not hosted in an Azure blob storage container,0
"if use_url, then images_requested_json_sas is required",0
optional params,0
check model_version is among the available model versions,0
check request_name has only allowed characters,0
optional params for telemetry collection - logged to status table for now as part of call_params,0
All API instances / node pools share a quota on total number of active Jobs;,0
we cannot accept new Job submissions if we are at the quota,0
required fields,0
request_status is either completed or failed,0
the create_batch_job thread will stop when it wakes up the next time,0
"Fix for Zooniverse - deleting any ""-"" characters in the job_id",0
"If the status is running, it could be a Job submitted before the last restart of this",0
"API instance. If that is the case, we should start to monitor its progress again.",0
WARNING model_version could be wrong (a newer version number gets written to the output file) around,0
"the time that  the model is updated, if this request was submitted before the model update",0
and the API restart; this should be quite rare,0
conform to previous schemes,0
%% undocumented endpoints,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Gunicorn logger handler will get attached if needed in server.py,0
request_name and request_submission_timestamp are for appending to,0
output file names,0
image_paths can be a list of strings (Azure blob names or public URLs),0
"or a list of length-2 lists where each is a [image_id, metadata] pair",0
Case 1: listing all images in the container,0
- not possible to have attached metadata if listing images in a blob,0
list all images to process,0
+ 1 so if the number of images listed > MAX_NUMBER_IMAGES_ACCEPTED_PER_JOB,0
we will know and not proceed,0
Case 2: user supplied a list of images to process; can include metadata,0
filter down to those conforming to the provided prefix and accepted suffixes (image file types),0
prefix is case-sensitive; suffix is not,0
"Although urlparse(p).path preserves the extension on local paths, it will not work for",0
"blob file names that contains ""#"", which will be treated as indication of a query.",0
"If the URL is generated via Azure Blob Storage, the ""#"" char will be properly encoded",0
apply the first_n and sample_n filters,0
OK if first_n > total number of images,0
sample by shuffling image paths and take the first sample_n images,0
"upload the image list to the container, which is also mounted on all nodes",0
all sharding and scoring use the uploaded list,0
now request_status moves from created to running,0
an extra field to allow the monitoring thread to restart after an API restart: total number of tasks,0
also record the number of images to process for reporting,0
start the monitor thread with the same name,0
"both succeeded and failed tasks are marked ""completed"" on Batch",0
"preserving format from before, but SAS URL to 'failed_images' and 'images' are no longer provided",0
"failures should be contained in the output entries, indicated by an 'error' field",0
"when people download this, the timestamp will have : replaced by _",0
check if the result blob has already been written (could be another instance of the API / worker thread),0
"and if so, skip aggregating and uploading the results, and just generate the SAS URL, which",0
could be needed still if the previous request_status was `problem`.,0
upload the output JSON to the Job folder,0
%% Helper functions *copied* from ct_utils.py and visualization/visualization_utils.py,0
"Determine the factor, which shifts the decimal point of x",0
just behind the last significant digit,0
"Shift decimal point by multiplicatipon with factor, flooring, and",0
division by factor,0
PIL.Image.convert() returns a converted copy of this image,0
alter orientation as needed according to EXIF tag 0x112 (274) for Orientation,0
https://gist.github.com/dangtrinhnt/a577ece4cbe5364aad28,0
https://www.media.mit.edu/pia/Research/deepview/exif.html,0
"%% TFDetector class, an unmodified *copy* of the class in detection/tf_detector.py,",0
so we do not have to import the packages required by run_detector.py,0
Number of decimal places to round to for confidence and bbox coordinates,0
"MegaDetector was trained with batch size of 1, and the resizing function is a part",0
of the inference graph,0
An enumeration of failure reasons,0
"change from [y1, x1, y2, x2] to [x1, y1, width, height]",0
convert numpy floats to Python floats,0
need to change the above line to the following if supporting a batch size > 1 and resizing to the same size,0
"np_images = [np.asarray(image, np.uint8) for image in images]",0
"images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)",0
performs inference,0
our batch size is 1; need to loop the batch dim if supporting batch size > 1,0
%% Scoring script,0
determine if there is metadata attached to each image_id,0
information to determine input and output locations,0
other parameters for the task,0
test that we can write to output path; also in case there is no image to process,0
list images to process,0
"items in this list can be strings or [image_id, metadata]",0
model path,0
"Path to .pb TensorFlow detector model file, relative to the",0
models/megadetector_copies folder in mounted container,0
score the images,0
,0
manage_video_batch.py,0
,0
Notebook-esque script to manage the process of running a local batch of videos,0
through MD.  Defers most of the heavy lifting to manage_local_batch.py .,0
,0
%% Imports and constants,0
%% Split videos into frames,0
"%% List frame files, break into folders",0
Find unique (relative) folders,0
fn = frame_files[0],0
%% List videos,0
%% Check for videos that are missing entirely,0
list(folder_to_frame_files.keys())[0],0
video_filenames[0],0
fn = video_filenames[0],0
%% Check for videos with very few frames,0
%% Print the list of videos that are problematic,0
%% Process images like we would for any other camera trap job,0
"...typically using manage_local_batch.py, but do this however you like, as long",0
as you get a results file at the end.,0
,0
"If you do RDE, remember to use the second folder from the bottom, rather than the",0
bottom-most folder.,0
%% Convert frame results to video results,0
%% Confirm that the videos in the .json file are what we expect them to be,0
%% Scrap,0
%% Test a possibly-broken video,0
%% List videos in a folder,0
%% Imports,0
%% Constants,0
%% Classes,0
class variables,0
"instance variables, in order of when they are typically set",0
Leaving this commented out to remind us that we don't want this check here; let,0
the API fail on these images.  It's a huge hassle to remove non-image,0
files.,0
,0
for path_or_url in images_list:,0
if not is_image_file_or_url(path_or_url):,0
raise ValueError('{} is not an image'.format(path_or_url)),0
Commented out as a reminder: don't check task status (which is a rest API call),0
in __repr__; require the caller to explicitly request status,0
"status=getattr(self, 'status', None))",0
estimate # of failed images from failed shards,0
Download all three JSON urls to memory,0
Remove files that were submitted but don't appear to be images,0
assert all(is_image_file_or_url(s) for s in submitted_images),0
Diff submitted and processed images,0
Confirm that the procesed images are a subset of the submitted images,0
https://www.geeksforgeeks.org/break-list-chunks-size-n-python/,0
%% Interactive driver,0
%%,0
%%,0
%%,0
%% Imports and constants,0
%% Constants I set per script,0
## Required,0
base_output_folder_name = os.path.expanduser('~/postprocessing/' + organization_name_short),0
Shared Access Signature (SAS) tokens for the Azure Blob Storage container.,0
Leading question mark is optional.,0
,0
The read-only token is used for accessing images; the write-enabled token is,0
used for writing file lists.,0
## Typically left as default,0
"Pre-pended to all folder names/prefixes, if they're defined below",0
"This is how we break the container up into multiple taskgroups, e.g., for",0
separate surveys. The typical case is to do the whole container as a single,0
taskgroup.,0
"If your ""folders"" are really logical folders corresponding to multiple folders,",0
map them here,0
"A list of .json files to load images from, instead of enumerating.  Formatted as a",0
"dictionary, like folder_prefixes.",0
This is only necessary if you will be performing postprocessing steps that,0
"don't yet support SAS URLs, specifically the ""subsetting"" step, or in some",0
cases the splitting of files into multiple output directories for,0
empty/animal/vehicle/people.,0
,0
"For those applications, you will need to mount the container to a local drive.",0
For this case I recommend using rclone whether you are on Windows or Linux;,0
rclone is much easier than blobfuse for transient mounting.,0
,0
"But most of the time, you can ignore this.",0
"Supported model_versions: '4', '3', '4_prelim'",0
,0
Also available at the /supported_model_versions and /default_model_version,0
endpoints,0
,0
"Unless you have any specific reason to set this to a non-default value, leave",0
"it at the default, which as of 2020.04.28 is MegaDetector 4.1",0
,0
"additional_task_args = {""model_version"":""4_prelim""}",0
,0
"file_lists_by_folder will contain a list of local JSON file names,",0
each JSON file contains a list of blob names corresponding to an API taskgroup,0
"%% Derived variables, path setup",0
local folders,0
Turn warnings into errors if more than this many images are missing,0
%% Support functions,0
"scheme, netloc, path, query, fragment",0
%% Read images from lists or enumerate blobs to files,0
folder_name = folder_names[0],0
"Load file lists for this ""folder""",0
,0
file_list = input_file_lists[folder][0],0
Write to file,0
A flat list of blob paths for each folder,0
folder_name = folder_names[0],0
"If we don't/do have multiple prefixes to enumerate for this ""folder""",0
"If this is intended to be a folder, it needs to end in '/', otherwise",0
files that start with the same string will match too,0
...for each prefix,0
Write to file,0
...for each folder,0
%% Some just-to-be-safe double-checking around enumeration,0
Make sure each folder has at least one image matched; the opposite is usually a sign of a copy/paste issue,0
...for each image,0
...for each prefix,0
...for each folder,0
Make sure each image comes from one of our folders; the opposite is usually a sign of a bug up above,0
...for each prefix,0
...for each folder,0
...for each image,0
%% Divide images into chunks for each folder,0
The JSON file at folder_chunks[i][j] corresponds to task j of taskgroup i,0
list_file = file_lists_by_folder[0],0
"%% Create taskgroups and tasks, and upload image lists to blob storage",0
periods not allowed in task names,0
%% Generate API calls for each task,0
clipboard.copy(request_strings[0]),0
clipboard.copy('\n\n'.join(request_strings)),0
%% Run the tasks (don't run this cell unless you are absolutely sure!),0
I really want to make sure I'm sure...,0
%% Estimate total time,0
Around 0.8s/image on 16 GPUs,0
%% Manually create task groups if we ran the tasks manually,0
%%,0
"%% Write task information out to disk, in case we need to resume",0
%% Status check,0
print(task.id),0
%% Resume jobs if this notebook closes,0
%% For multiple tasks (use this only when we're merging with another job),0
%% For just the one task,0
%% Load into separate taskgroups,0
p = task_cache_paths[0],0
%% Typically merge everything into one taskgroup,0
"%% Look for failed shards or missing images, start new tasks if necessary",0
List of lists of paths,0
i_taskgroup = 0; taskgroup = taskgroups[i_taskgroup];,0
"Make a copy, because we append to taskgroup",0
i_task = 0; task = tasks[i_task],0
Each taskgroup corresponds to one of our folders,0
Check that we have (almost) all the images,0
Now look for failed images,0
Write it out as a flat list as well (without explanation of failures),0
...for each task,0
...for each task group,0
%% Pull results,0
i_taskgroup = 0; taskgroup = taskgroups[i_taskgroup]; task = taskgroup[0],0
Each taskgroup corresponds to one of our folders,0
...for each task,0
...for each task group,0
%% Combine results from task groups into final output files,0
Check that we have (almost) all the images,0
The only reason we should ever have a repeated request is the case where an,0
"image was missing and we reprocessed it, or where it failed and later succeeded",0
"There may be non-image files in the request list, ignore those",0
Something has gone bonkers if there are images in the results that,0
aren't in the request,0
...for each folder,0
%% Post-processing (no ground truth),0
i_folder = 0; folder_name_raw = folder_names[i_folder],0
"Omit some pages from the output, useful when animals are rare",0
%% Manual processing follows,0
,0
"Everything after this should be considered mostly manual, and no longer includes",0
looping over folders.,0
,0
%% RDE (sample directory collapsing),0
"In this example, the camera created folders called ""100EK113"", ""101EK113"", etc., for every N images",0
"%% Repeat detection elimination, phase 1",0
"Deliberately leaving these imports here, rather than at the top, because this cell is not",0
typically executed,0
To invoke custom collapsing of folders for a particular manufacturer's naming scheme,0
options.customDirNameFunction = remove_overflow_folders,0
Exclude people and vehicles from RDE,0
"options.excludeClasses = [2,3]",0
%% Manual RDE step,0
# DELETE THE VALID DETECTIONS ##,0
%% Re-filtering,0
%% Post-processing (post-RDE),0
i_folder = 0; folder_name_raw = folder_names[i_folder],0
"Omit some pages from the output, useful when animals are rare",0
"print('No RDE file available for {}, skipping'.format(folder_name))",0
continue,0
%% Create a new category for large boxes,0
%% Subsetting,0
i_folder = 0; folder_name = folders[i_folder],0
%% String replacement,0
%% Folder splitting,0
i_folder = 0; folder_name_raw = folder_names[i_folder],0
api_output_file = folder_name_to_combined_output_file[folder_name],0
%% Imports and constants,0
from ai4eutils,0
To specify a non-default confidence threshold for including detections in the .json file,0
Turn warnings into errors if more than this many images are missing,0
Only relevant when we're using a single GPU,0
"Specify a target image size when running MD... strongly recommended to leave this at ""None""",0
Only relevant when running on CPU,0
OS-specific script line continuation character,0
OS-specific script comment character,0
"Prefer threads on Windows, processes on Linux",0
"This is for things like image rendering, not for MegaDetector",0
Should we use YOLOv5's val.py instead of run_detector_batch.py?,1
Directory in which to run val.py.  Only relevant if use_yolo_inference_scripts is True.,0
Should we remove intermediate files used for running YOLOv5's val.py?,0
,0
Only relevant if use_yolo_inference_scripts is True.,0
Should we apply YOLOv5's augmentation?  Only allowed when use_yolo_inference_scripts,0
is True.,0
%% Constants I set per script,0
Optional descriptor,0
model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v5.0.0/md_v5b.0.0.pt'),0
model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v4.1.0/md_v4.1.0.pb'),0
"Number of jobs to split data into, typically equal to the number of available GPUs",0
Only used to print out a time estimate,0
"%% Derived variables, constant validation, path setup",0
%% Enumerate files,0
%% Load files from prior enumeration,0
%% Divide images into chunks,0
%% Estimate total time,0
%% Write file lists,0
%% Generate commands,0
"A list of the scripts tied to each GPU, as absolute paths.  We'll write this out at",0
the end so each GPU's list of commands can be run at once.  Generally only used when,0
"running lots of small batches via YOLOv5's val.py, which doesn't support checkpointing.",0
i_task = 0; task = task_info[i_task],0
Generate the script to run MD,0
Check whether this output file exists,0
Generate the script to resume from the checkpoint (only supported with MD inference code),0
...for each task,0
Write out a script for each GPU that runs all of the commands associated with,0
that GPU.  Typically only used when running lots of little scripts in lieu,0
of checkpointing.,0
...for each GPU,0
%% Run the tasks,0
%%% Run the tasks (commented out),0
i_task = 0; task = task_info[i_task],0
"This will write absolute paths to the file, we'll fix this later",1
...for each chunk,0
...if False,0
"%% Load results, look for failed or missing images in each task",0
i_task = 0; task = task_info[i_task],0
im = task_results['images'][0],0
...for each task,0
%% Merge results files and make images relative,0
im = combined_results['images'][0],0
%% Post-processing (pre-RDE),0
options.sample_seed = 0,0
"Omit some pages from the output, useful when animals are rare",0
%% RDE (sample directory collapsing),0
"In this example, the camera created folders called ""100EK113"", ""101EK113"", etc., for every N images",0
%%,0
%%,0
%%,0
relativePath = image_filenames[0],0
"%% Repeat detection elimination, phase 1",0
"Deliberately leaving these imports here, rather than at the top, because this",0
cell is not typically executed,0
options.minSuspiciousDetectionSize = 0.05,0
This will cause a very light gray box to get drawn around all the detections,0
we're *not* considering as suspicious.,0
options.lineThickness = 5,0
options.boxExpansion = 8,0
To invoke custom collapsing of folders for a particular manufacturer's naming scheme,0
options.customDirNameFunction = remove_overflow_folders,0
Exclude people and vehicles from RDE,0
"options.excludeClasses = [2,3]",0
options.maxImagesPerFolder = 50000,0
options.includeFolders = ['a/b/c'],0
options.excludeFolder = ['a/b/c'],0
"Can be None, 'xsort', or 'clustersort'",0
import clipboard; clipboard.copy(os.path.dirname(suspiciousDetectionResults.filterFile)),0
path_utils.open_file(os.path.dirname(suspiciousDetectionResults.filterFile)),0
%% Manual RDE step,0
# DELETE THE VALID DETECTIONS ##,0
%% Re-filtering,0
%% Post-processing (post-RDE),0
options.sample_seed = 0,0
"Omit some pages from the output, useful when animals are rare",0
"%% Run MegaClassifier (actually, write out a script that runs MegaClassifier)",0
"This is just passed along to the metadata in the output file, it has no impact",0
on how the classification scripts run.,0
#%% Set up environment,0
commands.append('cd CameraTraps/classification\n'),0
commands.append('conda activate cameratraps-classifier\n'),0
#%% Crop images,0
fn = input_files[0],0
#%% Run classifier,0
fn = input_files[0],0
#%% Remap classifier outputs,0
fn = input_files[0],0
#%% Merge classification and detection outputs,0
fn = input_files[0],0
#%% Write  out classification script,0
"%% Run a non-MegaClassifier classifier (i.e., a classifier with no output mapping)",0
"This is just passed along to the metadata in the output file, it has no impact",0
on how the classification scripts run.,0
#%% Set up environment,0
#%% Crop images,0
fn = input_files[0],0
#%% Run classifier,0
fn = input_files[0],0
#%% Merge classification and detection outputs,0
fn = input_files[0],0
#%% Write everything out,0
%% Run the classifier(s) via the .sh script(s) or batch file(s) we just wrote,0
...,0
%% Within-image classification smoothing,0
,0
Only count detections with a classification confidence threshold above,0
"*classification_confidence_threshold*, which in practice means we're only",0
looking at one category per detection.,0
,0
If an image has at least *min_detections_above_threshold* such detections,0
"in the most common category, and no more than *max_detections_secondary_class*",0
"in the second-most-common category, flip all detections to the most common",0
category.,0
,0
"Optionally treat some classes as particularly unreliable, typically used to overwrite an",0
"""other"" class.",0
,0
This cell also removes everything but the non-dominant classification for each detection.,0
,0
How many detections do we need above the classification threshold to determine a dominant category,0
for an image?,0
"Even if we have a dominant class, if a non-dominant class has at least this many classifications",0
"in an image, leave them alone.",0
"If the dominant class has at least this many classifications, overwrite ""other"" classifications",0
What confidence threshold should we use for assessing the dominant category in an image?,0
Which classifications should we even bother over-writing?,0
Detection confidence threshold for things we count when determining a dominant class,0
Which detections should we even bother over-writing?,0
"Before we do anything else, get rid of everything but the top classification",0
for each detection.,0
...for each detection in this image,0
...for each image,0
im = d['images'][0],0
...for each classification,0
...if there are classifications for this detection,0
...for each detection,0
"Handle a quirky special case: if the most common category is ""other"" and",0
"it's ""tied"" with the second-most-common category, swap them",0
secondary_count = category_to_count[keys[1]],0
The 'secondary count' is the most common non-other class,0
If we have at least *min_detections_to_overwrite_other* in a category that isn't,0
"""other"", change all ""other"" classifications to that category",0
...for each classification,0
...if there are classifications for this detection,0
...for each detection,0
"...if we should overwrite all ""other"" classifications",0
"At this point, we know we have a dominant category; change all other above-threshold",0
"classifications to that category.  That category may have been ""other"", in which",0
case we may have already made the relevant changes.,0
det = detections[0],0
...for each classification,0
...if there are classifications for this detection,0
...for each detection,0
...for each image,0
...for each file we want to smooth,0
"%% Post-processing (post-classification, post-within-image-smoothing)",0
classification_detection_file = classification_detection_files[1],0
%% Read EXIF data from all images,0
%% Prepare COCO-camera-traps-compatible image objects for EXIF results,0
import dateutil,0
"This is a standard format for EXIF datetime, and dateutil.parser",0
doesn't handle it correctly.,0
return dateutil.parser.parse(s),0
exif_result = exif_results[0],0
Currently we assume that each leaf-node folder is a location,0
"We collected this image this century, but not today, make sure the parsed datetime",0
jives with that.,0
,0
The latter check is to make sure we don't repeat a particular pathological approach,0
"to datetime parsing, where dateutil parses time correctly, but swaps in the current",0
date when it's not sure where the date is.,0
...for each exif image result,0
%% Assemble into sequences,0
Make a list of images appearing at each location,0
im = image_info[0],0
%% Load classification results,0
Map each filename to classification results for that file,0
%% Smooth classification results over sequences (prep),0
These are the only classes to which we're going to switch other classifications,0
Only switch classifications to the dominant class if we see the dominant class at least,0
this many times,0
"If we see more than this many of a class that are above threshold, don't switch those",0
classifications to the dominant class.,0
"If the ratio between a dominant class and a secondary class count is greater than this,",0
"regardless of the secondary class count, switch those classificaitons (i.e., ignore",0
max_secondary_class_classifications_above_threshold_for_class_smoothing).,0
,0
"This may be different for different dominant classes, e.g. if we see lots of cows, they really",0
"tend to be cows.  Less so for canids, so we set a higher ""override ratio"" for canids.",0
"If there are at least this many classifications for the dominant class in a sequence,",0
"regardless of what that class is, convert all 'other' classifications (regardless of",0
confidence) to that class.,0
"If there are at least this many classifications for the dominant class in a sequence,",0
"regardless of what that class is, classify all previously-unclassified detections",0
as that class.,0
Only count classifications above this confidence level when determining the dominant,0
"class, and when deciding whether to switch other classifications.",0
Confidence values to use when we change a detection's classification (the,0
original confidence value is irrelevant at that point),0
%% Smooth classification results over sequences (supporting functions),0
im = images_this_sequence[0],0
det = results_this_image['detections'][0],0
Only process animal detections,0
Only process detections with classification information,0
"We only care about top-1 classifications, remove everything else",0
Make sure the list of classifications is already sorted by confidence,0
...and just keep the first one,0
"Confidence values should be sorted within a detection; verify this, and ignore",0
...for each detection in this image,0
...for each image in this sequence,0
...top_classifications_for_sequence(),0
Count above-threshold classifications in this sequence,0
Sort the dictionary in descending order by count,0
"Handle a quirky special case: if the most common category is ""other"" and",0
"it's ""tied"" with the second-most-common category, swap them.",0
...def count_above_threshold_classifications(),0
%% Smooth classifications at the sequence level (main loop),0
Break if this token is contained in a filename (set to None for normal operation),0
i_sequence = 0; seq_id = all_sequences[i_sequence],0
Count top-1 classifications in this sequence (regardless of confidence),0
Handy debugging code for looking at the numbers for a particular sequence,0
Count above-threshold classifications for each category,0
"If our dominant category ID isn't something we want to smooth to, don't mess around with this sequence",0
"# Smooth ""other"" classifications ##",0
"By not re-computing ""max_count"" here, we are making a decision that the count used",0
"to decide whether a class should overwrite another class does not include any ""other""",0
classifications we changed to be the dominant class.  If we wanted to include those...,0
,0
sorted_category_to_count = count_above_threshold_classifications(classifications_this_sequence),0
max_count = get_first_value_from_sorted_dictionary(sorted_category_to_count),0
assert dominant_category_id == get_first_key_from_sorted_dictionary(sorted_category_to_count),0
# Smooth non-dominant classes ##,0
Don't flip classes to the dominant class if they have a large number of classifications,0
"Don't smooth over this class if there are a bunch of them, and the ratio",0
if primary to secondary class count isn't too large,0
Default ratio,0
Does this dominant class have a custom ratio?,0
# Smooth unclassified detections ##,0
...for each sequence,0
%% Write smoothed classification results,0
"%% Post-processing (post-classification, post-within-image-and-within-sequence-smoothing)",0
%% Zip .json files,0
%% 99.9% of jobs end here,0
Everything after this is run ad hoc and/or requires some manual editing.,0
%% Compare results files for different model versions (or before/after RDE),0
Choose all pairwise combinations of the files in [filenames],0
%% Merge in high-confidence detections from another results file,0
%% Create a new category for large boxes,0
"This is a size threshold, not a confidence threshold",0
size_options.categories_to_separate = [3],0
%% Preview large boxes,0
%% .json splitting,0
options.query = None,0
options.replacement = None,0
Reminder: 'n_from_bottom' with a parameter of zero is the same as 'bottom',0
%% Custom splitting/subsetting,0
i_folder = 0; folder_name = folders[i_folder],0
"This doesn't do anything in this case, since we're not splitting folders",0
options.make_folder_relative = True,0
%% String replacement,0
%% Splitting images into folders,0
%% Generate commands for a subset of tasks,0
i_task = 8,0
...for each task,0
%% End notebook: turn this script into a notebook (how meta!),0
Exclude everything before the first cell,0
Remove the first [first_non_empty_lines] from the list,0
Add the last cell,0
,0
xmp_integration.py,0
,0
"Tools for loading MegaDetector batch API results into XMP metadata, specifically",0
for consumption in digiKam:,0
,0
https://cran.r-project.org/web/packages/camtrapR/vignettes/camtrapr2.html,0
,0
%% Imports and constants,0
%% Class definitions,0
Folder where images are stored,0
.json file containing MegaDetector output,0
"String to remove from all path names, typically representing a",0
prefix that was added during MegaDetector processing,0
Optionally *rename* (not copy) all images that have no detections,0
above [rename_conf] for the categories in rename_cats from x.jpg to,0
x.check.jpg,0
"Comma-deleted list of category names (or ""all"") to apply the rename_conf",0
behavior to.,0
"Minimum detection threshold (applies to all classes, defaults to None,",0
i.e. 0.0,0
%% Functions,0
Relative image path,0
Absolute image path,0
List of categories to write to XMP metadata,0
Categories with above-threshold detections present for,0
this image,0
Maximum confidence for each category,0
Have we already added this to the list of categories to,0
write out to this image?,0
If we're supposed to compare to a threshold...,0
Else we treat *any* detection as valid...,0
Keep track of the highest-confidence detection for this class,0
If we're doing the rename/.check behavior...,0
Legacy code to rename files where XMP writing failed,0
%% Interactive/test driver,0
%%,0
%% Command-line driver,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Cosmos DB `batch-api-jobs` table for job status,0
"aggregate the number of images, country and organization names info from each job",0
submitted during yesterday (UTC time),0
create the card,0
,0
api_frontend.py,0
,0
"Defines the Flask app, which takes requests (one or more images) from",0
"remote callers and pushes the images onto the shared Redis queue, to be processed",0
by the main service in api_backend.py .,0
,0
%% Imports,0
%% Initialization,0
%% Support functions,0
Make a dict that the request_processing_function can return to the endpoint,0
function to notify it of an error,0
Verify that the content uploaded is not too big,0
,0
request.content_length is the length of the total payload,0
Verify that the number of images is acceptable,0
...def check_posted_data(request),0
%% Main loop,0
Check whether the request_processing_function had an error,0
Write images to temporary files,0
,0
TODO: read from memory rather than using intermediate files,1
"Submit the image(s) for processing by api_backend.py, who is waiting on this queue",0
TODO: convert to a blocking read and eliminate the sleep() statement in this loop,1
"image = Image.open(os.path.join(temp_direc, image_name))",0
...if we do/don't have a request available on the queue,0
...while(True),0
...def detect_sync(),0
%% Command-line driver,0
use --non-docker if you are testing without Docker,0
,0
python api_frontend.py --non-docker,0
# Camera trap real-time API configuration,0
"Full path to the temporary folder for image storage, only meaningful",0
within the Docker container,0
Upper limit on total content length (all images and parameters),0
Minimum confidence threshold for detections,0
Minimum confidence threshold for showing a bounding box on the output image,0
Use this when testing without Docker,0
,0
api_backend.py,0
,0
"Defines the model execution service, which pulls requests (one or more images)",0
"from the shared Redis queue, and runs them through the TF model.",0
,0
%% Imports,0
%% Initialization,0
%% Main loop,0
TODO: convert to a blocking read and eliminate the sleep() statement in this loop,1
Filter the detections by the confidence threshold,0
,0
"Each result is [ymin, xmin, ymax, xmax, confidence, category]",0
,0
"Coordinates are relative, with the origin in the upper-left",0
...if serialized_entry,0
...while(True),0
...def detect_process(),0
%% Command-line driver,0
use --non-docker if you are testing without Docker,0
,0
python api_frontend.py --non-docker,0
run detections on a test image to load the model,0
%%,0
Importing libraries,0
%%,0
%%,0
%%,0
app = typer.Typer(),0
%%,0
@app.command(),0
GPU configuration: set up GPUs based on availability and user specification,0
Environment variable setup for numpy multi-threading,0
Load and set configurations from the YAML file,0
Set a global seed for reproducibility,0
"If the annotation directory does not have a data split, split the data first",0
Replace annotation dir from config with the directory containing the split files,0
Split the data according to the split type,0
Get the path to the annotation files,0
Split training data,0
Split validation and test data,0
Dataset and algorithm loading based on the configuration,0
Logger setup based on the specified logger type,0
Callbacks for model checkpointing and learning rate monitoring,0
Trainer configuration in PyTorch Lightning,0
"Training, validation, or evaluation execution based on the mode",0
%%,0
%%,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%%,0
PyTorch imports,0
"Importing the model, dataset, transformations and utility functions from PytorchWildlife",0
Importing the utility function for saving cropped images,0
Setting the device to use for computations ('cuda' indicates GPU),0
Initializing the MegaDetectorV5 model for image detection,0
Creating a dataset of images with the specified transform,0
Creating a DataLoader for batching and parallel processing of the images,0
Performing batch detection on the images,0
Saving the detected objects as cropped images,0
%%,0
Read the original CSV file,0
Prepare a list to store new records for the new CSV,0
Process the data if the name of the file is in the dataframe,0
Save the crop into a new csv,0
Add record to the new CSV data,0
Create a DataFrame from the new records,0
Define the path for the new CSV file,0
Save the new DataFrame to CSV,0
# DATA SPLITTING,0
Load the data from the csv file,0
Separate the features and the targets,0
First split to separate out the test set,0
Adjust val_size to account for the initial split,0
Second split to separate out the validation set,0
"Combine features, labels, and classification back into dataframes",0
Create the output directory in case that it does not exist,0
Save the splits to new CSV files,0
Return the dataframes,0
Load the data from the csv file,0
Calculate train size based on val and test size,0
Get unique locations,0
Split locations into train and temp (temporary holding for val and test),0
Adjust the proportions for val and test based on the remaining locations,0
"Allocate images to train, validation, and test sets based on their location",0
Save the datasets to CSV files,0
Return the split datasets,0
Load the data from the csv file,0
Convert 'Photo_Time' from string to datetime,0
Calculate train size based on val and test size,0
Sort by 'Photo_Time' to ensure chronological order,0
Group photos into sequences based on a 30-second interval,0
Assign unique sequence IDs to each group,0
Get unique sequence IDs,0
Split sequence IDs into train and temp (temporary holding for val and test),0
Adjust the proportions for val and test based on the remaining sequences,0
"Allocate images to train, validation, and test sets based on their sequence ID",0
Save the datasets to CSV files,0
Return the split datasets,0
Exportable class names for external use,0
Applying the ResNet layers and operations,0
Initialize the network with the specified settings,0
Selecting the appropriate ResNet architecture and pre-trained weights,0
self.pretrained_weights = ResNet18_Weights.IMAGENET1K_V1,0
self.pretrained_weights = ResNet50_Weights.IMAGENET1K_V1,0
Constructing the feature extractor and classifier,0
Criterion for binary classification,0
Load pre-trained weights and adjust for the current model,0
init_weights = self.pretrained_weights.get_state_dict(progress=True),0
Load the weights into the feature extractor,0
Identify missing and unused keys in the loaded weights,0
Import necessary libraries,0
Exportable class names for external use,0
Define normalization mean and standard deviation for image preprocessing,0
Define data transformations for training and validation datasets,0
Load data for prediction,0
Load data for training/validation,0
"Load datasets for different modes (training, validation, testing, prediction)",0
Calculate class counts and label mappings,0
Define parameters for the optimizer,0
Optimizer parameters for feature extraction,0
Optimizer parameters for the classifier,0
Setup optimizer and optimizer scheduler,0
Forward pass,0
Calculate loss,0
Forward pass,0
Forward pass,0
Concatenate outputs from all test steps,0
Calculate the metrics and save the output,0
Forward pass,0
Concatenate outputs from all predict steps,0
Compute the confusion matrix from true labels and predictions,0
Calculate class-wise accuracy (accuracy for each class),0
Calculate micro accuracy (overall accuracy),0
Calculate macro accuracy (mean of class-wise accuracies),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%%,0
Importing necessary basic libraries and modules,0
%%,0
PyTorch imports for tensor operations,0
%%,0
"Importing the models, transformations, and utility functions from PytorchWildlife",0
%%,0
Setting the device to use for computations ('cuda' indicates GPU),0
%%,0
Initializing the model for image detection,0
%%,0
Initializing the model for image classification,0
%%,0
Defining transformations for detection and classification,0
%%,0
Initializing a box annotator for visualizing detections,0
Processing the video and saving the result with annotated detections and classifications,0
%%,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%%,0
Importing basic libraries,0
%%,0
"Importing the models, dataset, transformations, and utility functions from PytorchWildlife",0
%%,0
Setting the device to use for computations ('cuda' indicates GPU),0
Initializing a supervision box annotator for visualizing detections,0
Create a temp folder,0
Initializing the detection and classification models,0
Defining transformations for detection and classification,0
%% Defining functions for different detection scenarios,0
Only run classifier when detection class is animal,0
Clean the temp folder if it contains files,0
Check the contents of the extracted folder,0
%% Building Gradio UI,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%%,0
Importing necessary basic libraries and modules,0
%%,0
PyTorch imports,0
%%,0
"Importing the model, dataset, transformations and utility functions from PytorchWildlife",0
%%,0
Setting the device to use for computations ('cuda' indicates GPU),0
%%,0
Initializing the MegaDetectorV5 model for image detection,0
%% Single image detection,0
Specifying the path to the target image TODO: Allow argparsing,0
Opening and converting the image to RGB format,0
Initializing the Yolo-specific transform for the image,0
Performing the detection on the single image,0
Saving the detection results,0
%% Batch detection,0
Specifying the folder path containing multiple images for batch detection,0
Creating a dataset of images with the specified transform,0
Creating a DataLoader for batching and parallel processing of the images,0
Performing batch detection on the images,0
%% Output to annotated images,0
Saving the batch detection results as annotated images,0
%% Output to cropped images,0
Saving the detected objects as cropped images,0
%% Output to JSON results,0
Saving the detection results in JSON format,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Making the DetectionImageFolder class available for import from this module,0
Listing and sorting all image files in the specified directory,0
Get image filename and path,0
Load and convert image to RGB,0
Apply transformation if specified,0
Only run recognition on animal detections,0
Get image path and corresponding bbox xyxy for cropping,0
Load and crop image with supervision,0
Apply transformation if specified,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
from yolov5.utils.augmentations import letterbox,0
Making the provided classes available for import from this module,0
Convert PIL Image to Torch Tensor,0
Original shape,0
New shape,0
Scale ratio (new / old) and compute padding,0
Resize image,0
Pad image,0
Convert the image to a PyTorch tensor and normalize it,0
Resize and pad the image using a customized letterbox function.,0
Normalization constants,0
Define the sequence of transformations,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
!!! Output paths need to be optimized !!!,1
!!! Output paths need to be optimized !!!,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%%,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Importing basic libraries,0
Placeholder class-level attributes to be defined in derived classes,0
"If there are size differences in the input images, use a for loop instead of matrix processing for scaling",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Image size for the Opossum classifier,0
Class names for prediction,0
"If pretrained, use the provided URL to fetch the weights",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Making the PlainResNetInference class available for import from this module,0
Following the ResNet structure to extract features,0
Initialize the network and weights,0
... [Missing weight URL definition for ResNet18],0
... [Missing weight URL definition for ResNet50],0
Print missing and unused keys for debugging purposes,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Image size for the Opossum classifier,0
Class names for prediction,0
"If pretrained, use the provided URL to fetch the weights",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Image size for the Opossum classifier,0
Class names for prediction,0
"If pretrained, use the provided URL to fetch the weights",0
Configuration file for the Sphinx documentation builder.,0
,0
"For the full list of built-in configuration values, see the documentation:",0
https://www.sphinx-doc.org/en/master/usage/configuration.html,0
-- Project information -----------------------------------------------------,0
https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information,0
-- General configuration ---------------------------------------------------,0
https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration,0
-- Options for HTML output -------------------------------------------------,0
https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output,0
-- Options for todo extension ----------------------------------------------,1
https://www.sphinx-doc.org/en/master/usage/extensions/todo.html#configuration,0
"Determine the factor, which shifts the decimal point of x",0
just behind the last significant digit,0
"Shift decimal point by multiplicatipon with factor, flooring, and",0
division by factor,0
_ = pretty_print_object(obj),0
Sloppy that I'm making a module-wide change here...,1
Determine the coordinates of the intersection rectangle,0
The intersection of two axis-aligned bounding boxes is always an,0
axis-aligned bounding box,0
Compute the area of both AABBs,0
Compute the intersection over union by taking the intersection,0
area and dividing it by the sum of prediction + ground-truth,0
areas - the intersection area.,0
%% Functions for running commands as subprocesses,0
%%,0
%% Test driver for execute_and_print,0
%% Parallel test driver for execute_command_and_print,0
Should we use threads (vs. processes) for parallelization?,0
"Only relevant if n_workers == 1, i.e. if we're not parallelizing",0
%% Constants and imports,0
%% Classes,0
Collect all names,0
Make names unique and sort,0
cast location to string as the entries in locations are strings,0
Convert classnames to lowercase to simplify comparisons later,0
Normalize paths to simplify comparisons later,0
"Make custom replacements in filenames, typically used to",0
accommodate changes in root paths after DB construction,0
## Build useful mappings to facilitate working with the DB,0
Category ID <--> name,0
Image filename --> ID,0
Image ID --> image object,0
Image ID --> annotations,0
"Each image can potentially multiple annotations, hence using lists",0
...__init__,0
...class IndexedJsonDb,0
%% Functions,0
Find all unique locations,0
i_location = 0; location = locations[i_location],0
Sorting datetimes fails when there are None's in the list.  So instead of sorting datetimes,0
"directly, sort tuples with a boolean for none-ness, then the datetime itself.",0
,0
https://stackoverflow.com/questions/18411560/sort-list-while-pushing-none-values-to-the-end,0
previous_datetime = sorted_images_this_location[0]['datetime'],0
im = sorted_images_this_location[1],0
"Start a new sequence if necessary, including the case where this datetime is invalid",0
"If this was an invalid datetime, this will record the previous datetime",0
"as None, which will force the next image to start a new sequence.",0
...for each image in this location,0
Fill in seq_num_frames,0
...for each location,0
...create_sequences(),0
,0
cct_to_md.py,0
,0
"""Converts"" a COCO Camera Traps file to a MD results file.  Currently ignores",0
"non-bounding-box annotations, and gives all annotations a confidence of 1.0.",0
,0
The only reason to do this is if you are going to add information to an existing,0
"CCT-formatted dataset, and want to do that in Timelapse.",0
,0
"Currently assumes that width and height are present in the input data, does not",0
read them from images.,0
,0
%% Constants and imports,0
%% Functions,0
# Validate input,0
# Read input,0
# Prepare metadata,0
ann = d['annotations'][0],0
# Process images,0
im = d['images'][0],0
"MegaDetector: [x,y,width,height] (normalized, origin upper-left)",0
"CCT: [x,y,width,height] (absolute, origin upper-left)",0
...if there's a bounding box,0
...for each annotation,0
This field is no longer included in MD output files by default,0
im_out['max_detection_conf'] = max_detection_conf,0
...for each image,0
# Write output,0
...cct_to_md(),0
%% Command-line driver,0
TODO,1
%% Interactive driver,0
%%,0
%%,0
,0
cct_json_to_filename_json.py,0
,0
"Given a .json file in COCO Camera Traps format, outputs a .json-formatted list of",0
relative file names present in the CCT file.,0
,0
%% Constants and environment,0
%% Main function,0
"json.dump(s,open(outputFilename,'w'))",0
%% Command-line driver,0
%% Interactive driver,0
%%,0
,0
cct_to_csv.py,0
,0
"""Converts"" a COCO Camera Traps .json file to .csv, in quotes because",0
"all kinds of assumptions are made here, and if you have a particular .csv",0
"format in mind, YMMV.  Most notably, does not include any bounding box information",0
or any non-standard fields that may be present in the .json file.  Does not,0
propagate information about sequence-level vs. image-level annotations.,0
,0
"Does not assume access to the images, therefore does not open .jpg files to find",0
"datetime information if it's not in the metadata, just writes datetime as 'unknown'.",0
,0
%% Imports,0
%% Main function,0
#%% Read input,0
#%% Build internal mappings,0
annotation = annotations[0],0
#%% Write output file,0
im = images[0],0
Write out one line per class:,0
...for each class name,0
...for each image,0
...with open(output_file),0
...def cct_to_csv,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
,0
remove_exif.py,0
,0
"Removes all EXIF/IPTC/XMP metadata from a folder of images, without making",0
"backup copies, using pyexiv2.",0
,0
%% Imports and constants,0
%% List files,0
%% Remove EXIF data (support),0
PYEXIV2 IS NOT THREAD SAFE; DO NOT CALL THIS IN PARALLEL FROM A SINGLE PROCESS,1
data = img.read_exif(); print(data),0
%% Debug,0
%%,0
%%,0
%% Remove EXIF data (execution),0
fn = image_files[0],0
"joblib.Parallel defaults to a process-based backend, but let's be sure",0
"results = Parallel(n_jobs=n_exif_threads,verbose=2,prefer='processes')(delayed(remove_exif)(fn) for fn in image_files[0:10])",0
,0
yolo_to_coco.py,0
,0
Converts a YOLO-formatted dataset to a COCO-formatted dataset.,0
,0
"Currently supports only a single folder (i.e., no recursion).  Treats images without",0
corresponding .txt files as empty.,0
,0
%% Imports and constants,0
from ai4eutils,0
%% Support functions,0
Validate input,0
Class names,0
Blank lines should only appear at the end,0
Enumerate images,0
fn = image_files[0],0
Create the image object for this image,0
Is there an annotation file for this image?,0
"This is an image with no annotations, currently don't do anything special",0
here,0
s = lines[0],0
"COCO: [x_min, y_min, width, height] in absolute coordinates",0
"YOLO: [class, x_center, y_center, width, height] in normalized coordinates",0
...for each annotation,0
...if this image has annotations,0
...for each image,0
...def yolo_to_coco(),0
%% Interactive driver,0
%% Convert YOLO folders to COCO,0
%% Check DB integrity,0
%% Preview some images,0
%% Command-line driver,0
TODO,1
,0
read_exif.py,0
,0
"Given a folder of images, read relevant metadata (EXIF/IPTC/XMP) fields from all images,",0
and write them to  a .json or .csv file.,0
,0
This module can use either PIL (which can only reliably read EXIF data) or exiftool (which,0
can read everything).  The latter approach expects that exiftool is available on the system,0
path.  No attempt is made to be consistent in format across the two approaches.,0
,0
%% Imports and constants,0
From ai4eutils,0
%% Options,0
Number of concurrent workers,0
Should we use threads (vs. processes) for parallelization?,0
,0
Not relevant if n_workers is 1.,0
Should we use exiftool or pil?,0
%% Functions,0
exif_tags = img.info['exif'] if ('exif' in img.info) else None,0
print('Warning: unrecognized EXIF tag: {}'.format(k)),0
"-G means ""Print group name for each tag"", e.g. print:",0
,0
[File]          Bits Per Sample                 : 8,0
,0
...instead of:,0
,0
Bits Per Sample                 : 8,0
A list of three-element lists (type/tag/value),0
line_raw = exif_lines[0],0
A typical line:,0
,0
[ExifTool]      ExifTool Version Number         : 12.13,0
"Split on the first occurrence of "":""",0
...for each output line,0
...which processing library are we using?,0
...read_exif_tags_for_image(),0
...populate_exif_data(),0
Enumerate *relative* paths,0
Find all EXIF tags that exist in any image,0
...for each tag in this image,0
...for each image,0
Write header,0
...for each key that *might* be present in this image,0
...for each image,0
...with open(),0
...if we're writing to .json/.csv,0
https://stackoverflow.com/questions/11210104/check-if-a-program-exists-from-a-python-script,0
%% Interactive driver,0
%%,0
output_file = os.path.expanduser('~/data/test-exif.csv'),0
options.processing_library = 'pil',0
"file_path = os.path.join(input_folder,'KRU_S1_11_R1_IMAG0148.JPG')",0
%%,0
%% Command-line driver,0
,0
"Given a json-formatted list of image filenames, retrieve the width and height of every image.",0
,0
%% Constants and imports,0
%% Processing functions,0
Is this image on disk?,0
"all_results = list(tqdm(pool.imap(process_image, filenames), total=len(filenames)))",0
%% Interactive driver,0
%%,0
List images in a test folder,0
%%,0
"process_list_file(image_list_file,image_size_file,image_prefix=base_dir)",0
%% Command-line driver,0
"python sanity_check_json_db.py ""e:\wildlife_data\wellington_data\wellington_camera_traps.json"" --baseDir ""e:\wildlife_data\wellington_data\images"" --bFindUnusedImages --bCheckImageSizes",0
"python sanity_check_json_db.py ""D:/wildlife_data/mcgill_test/mcgill_test.json"" --baseDir ""D:/wildlife_data/mcgill_test"" --bFindUnusedImages --bCheckImageSizes",0
"Here the '-u' prevents buffering, which makes tee happier",0
,0
python -u sanity_check_json_db.py '/datadrive1/nacti_metadata.json' --baseDir '/datadrive1/nactiUnzip/' --bFindUnusedImages --bCheckImageSizes | tee ~/nactiTest.out,0
,0
Partially-formed stub to get from MegaDetector output files to COCO Camera Traps data.,0
,0
"Was actually written to convert *many* MD .json files to a single CCT file, hence",0
the loop over .json files.,0
,0
"THIS CODE HAS NEVER BEEN RUN, it was added as a demonstration of how to do this.  YMMV.",0
,0
"You may find a more polished, command-line-ready version of this code at:",0
,0
https://github.com/StewartWILDlab/mdtools,0
,0
%% Constants and imports,0
"Images sizes are required to convert between absolute and relative coordinates,",0
so we need to read the images.,0
Only required if you want to write a database preview,0
%% Create CCT dictionaries,0
image_ids_to_images = {},0
Force the empty category to be ID 0,0
Load .json annotations for this data set,0
i_entry = 0; entry = data['images'][i_entry],0
,0
"PERF: Not exactly trivially parallelizable, but about 100% of the",0
time here is spent reading image sizes (which we need to do to get from,0
"absolute to relative coordinates), so worth parallelizing.",0
Generate a unique ID from the path,0
detection = detections[0],0
Have we seen this category before?,0
Create an annotation,0
"MegaDetector: [x,y,width,height] (normalized, origin upper-left)",0
"CCT: [x,y,width,height] (absolute, origin upper-left)",0
...for each detection,0
...for each image,0
Remove non-reviewed images and associated annotations,0
%% Create info struct,0
%% Write .json output,0
%% Clean start,0
## Everything after this should work from a clean start ###,0
%% Validate output,0
%% Preview animal labels,0
%% Preview empty labels,0
"viz_options.classes_to_exclude = ['empty','human']",0
,0
generate_crops_from_cct.py,0
,0
"Given a .json file in COCO Camera Traps format, create a cropped image for",0
each bounding box.,0
,0
%% Imports and constants,0
%% Functions,0
# Read and validate input,0
# Find annotations for each image,0
"This actually maps image IDs to annotations, but only to annotations",0
containing boxes,0
# Generate crops,0
TODO: parallelize this loop,1
im = d['images'][0],0
Load the image,0
Generate crops,0
i_ann = 0; ann = annotations_this_image[i_ann],0
"x/y/w/h, origin at the upper-left",0
...for each box,0
...for each image,0
...generate_crops_from_cct(),0
%% Interactive driver,0
%%,0
%%,0
%%,0
%% Command-line driver,0
TODO,1
%% Scrap,0
%%,0
,0
coco_to_yolo.py,0
,0
Converts a COCO-formatted dataset to a YOLO-formatted dataset.,0
,0
"If the input and output folders are the same, writes .txt files to the input folder,",0
and neither moves nor modifies images.,0
,0
"Currently ignores segmentation masks, and errors if an annotation has a",0
segmentation polygon but no bbox,0
,0
Has only been tested on a handful of COCO Camera Traps data sets; if you,0
"use it for more general COCO conversion, YMMV.",0
,0
%% Imports and constants,0
%% Support functions,0
Validate input,0
Read input data,0
Parse annotations,0
i_ann = 0; ann = data['annotations'][0],0
Make sure no annotations have *only* segmentation data,0
Re-map class IDs to make sure they run from 0...n-classes-1,0
,0
"TODO: this allows unused categories in the output data set, which I *think* is OK,",1
but I'm only 81% sure.,0
Process images (everything but I/O),0
"List of dictionaries with keys 'source_image','dest_image','bboxes','dest_txt'",0
i_image = 0; im = data['images'][i_image],0
"assert os.path.isfile(source_image), 'Could not find image {}'.format(source_image)",0
If this annotation has no bounding boxes...,0
"This is not entirely clear from the COCO spec, but it seems to be consensus",0
"that if you want to specify an image with no objects, you don't include any",0
annotations for that image.,0
We allow empty bbox lists in COCO camera traps; this is typically a negative,0
"example in a dataset that has bounding boxes, and 0 is typically the empty",0
category.,0
...if this is an empty annotation,0
"COCO: [x_min, y_min, width, height] in absolute coordinates",0
"YOLO: [class, x_center, y_center, width, height] in normalized coordinates",0
Convert from COCO coordinates to YOLO coordinates,0
...for each annotation,0
...if this image has annotations,0
...for each image,0
Write output,0
Category IDs should range from 0..N-1,0
TODO: parallelize this loop,1
,0
output_info = images_to_copy[0],0
Only write an annotation file if there are bounding boxes.  Images with,0
"no .txt files are treated as hard negatives, at least by YOLOv5:",0
,0
https://github.com/ultralytics/yolov5/issues/3218,0
,0
"I think this is also true for images with empty annotation files, but",0
"I'm using the convention suggested on that issue, i.e. hard negatives",0
are expressed as images without .txt files.,0
bbox = bboxes[0],0
...for each image,0
...def coco_to_yolo(),0
%% Interactive driver,0
%% CCT data,0
"If preview_export is True, I'm exporting to preview these with BoundingBoxEditor:",0
,0
https://github.com/mfl28/BoundingBoxEditor,0
,0
"This export will be compatible, other than the fact that you need to move",0
"""object.data"" into the ""labels"" folder.",0
,0
"Otherwise I'm exporting for training, in the YOLOv5 flat format.",0
%% Command-line driver,0
TODO,1
,0
cct_to_wi.py,0
,0
Converts COCO Camera Traps .json files to the Wildlife Insights,0
batch upload format,0
,0
Also see:,0
,0
https://github.com/ConservationInternational/Wildlife-Insights----Data-Migration,0
,0
https://data.naturalsciences.org/wildlife-insights/taxonomy/search,0
,0
%% Imports,0
%% Paths,0
A COCO camera traps file with information about this dataset,0
A .json dictionary mapping common names in this dataset to dictionaries with the,0
"WI taxonomy fields: common_name, wi_taxon_id, class, orer, family, genus, species",0
%% Constants,0
%% Project information,0
%% Read templates,0
%% Compare dictionary to template lists,0
Write the header,0
Write values,0
%% Project file,0
%% Camera file,0
%% Deployment file,0
%% Images file,0
Read .json file with image information,0
Read taxonomy dictionary,0
Populate output information,0
df = pd.DataFrame(columns = images_fields),0
annotation = annotations[0],0
im = input_data['images'][0],0
"We don't have counts, but we can differentiate between zero and 1",0
This is the label mapping used for our incoming iMerit annotations,0
"Only used to parse the incoming annotations. In our database, the string name is used to avoid confusion",0
MegaDetector outputs,0
,0
add_bounding_boxes_to_megadb.py,0
,0
"Given COCO-formatted JSONs containing manually labeled bounding box annotations, add them to",0
"MegaDB sequence entries, which can then be ingested into MegaDB.",0
the category map that comes in the COCO JSONs for iMerit batch 12 - to check that each,0
JSON,0
"dataset name : (seq_id, frame_num) : [bbox, bbox]",0
where bbox is a dict with str 'category' and list 'bbox',0
iterate over image_id_to_image rather than image_id_to_annotations so we include,0
the confirmed empty images,0
The file_name field in the incoming json looks like,0
alka_squirrels.seq2020_05_07_25C.frame119221.jpg,0
there seems to be a bug in the annotations where sometimes there's a,0
non-empty label along with a label of category_id 5,0
ignore the empty label (they seem to be actually non-empty),0
,0
add_bounding_boxes_to_json.py,0
,0
This script takes a image database in the COCO Camera Traps format and merges in a set of bounding,0
box annotations in the format that iMerit uses (a .json where actually only each row is a valid json).,0
,0
"If you need to update an existing bbox database, please get all the original annotation files and",0
re-generate from scratch,0
,0
%% Imports,0
%% Configurations and paths,0
images database,0
output bboxes database,0
annotation files (pseudo json) obtained from our annotation vendor that contain annotations for this dataset,0
None or a string or tuple of strings that is the prefix to all file_name of interest / in this dataset in the annotation files,0
functions for mapping the image_id in the annotation files (pseudo jsons) to the image_id used in the image DB,0
our img_id doesn't contain frame info,0
"batch3 - ""file_name"":""ASG0000019_0_S1_B06_R1_PICT0007.JPG""",0
"batch5 and 7 - ""file_name"":""datasetsnapshotserengeti.seqASG000002m-frame0.imgS1_B06_R1_PICT0056.JPG""",0
sometimes - 'datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG',0
"id in DB (old_token): 'S6/J01/J01_R1/S6_J01_R1_IMAG0001', 'S1/B05/B05_R1/S1_B05_R1_PICT0036'",0
specify which one to use for your dataset here,0
%% Load the image database and fill in DB info for the output bbox database,0
load the images database,0
%% Find the height and width of images from the annotation files,0
,0
...if they are not available in the images DB,0
each row in this pseudo-json is a COCO formatted entry for an image sequence,0
%% Other functions required by specific datasets,0
the IDFG image database does not include images from unlabeled folders that were annotated with bounding boxes,0
%% Create the bbox database from all annotation files pertaining to this dataset,0
"for the incoming annotations, look up by category name (common) and convert to the numerical id used in our databases",0
"for each annotation pseudo-json, check that the image it refers to exists in the original database",0
each row in this pseudo-json is a COCO formatted entry for an image sequence,0
check that entry is for this dataset,0
category map for this entry in the annotation file - usually the same across all entries but just in case,0
rspb_add_image_entry(img_id),0
use the image length and width in the image DB,0
"[top left x, top left y, width, height] in relative coordinates",0
"add all images that have been sent to annotation, some of which may be empty of bounding boxes",0
rspb_add_image_entry(db_image_id),0
%% Common queries,0
This query is used when preparing tfrecords for object detector training.,0
We do not want to get the whole seq obj where at least one image has bbox because,0
some images in that sequence will not be bbox labeled so will be confusing.,0
Include images with bbox length 0 - these are confirmed empty by bbox annotators.,0
"If frame_num is not available, it will not be a field in the result iterable.",0
"Note that the seq_id is the Cosmos DB assigned ID for that sequence, not the",0
"seq_id field, which may contain ""/"" characters.",0
"For public datasets to be converted to the CCT format, we get the whole seq object because",0
sequence level attributes need to be included too. megadb/converters/megadb_to_cct.py handles,0
the case of bbox-only JSONs with the flag exclude_species_class,0
Getting all sequences in a dataset - for updating or deleting entries which need the id field,0
%% Parameters,0
Use None if querying across all partitions,0
"The `sequences` table has the `dataset` as the partition key, so if only querying",0
"entries from one dataset, set the dataset name here.",0
"e.g. {'name': '@top_n', 'value': 100} - see query_and_upsert_examples/query_for_data.ipynb",0
Use False if do not want all results stored in a single JSON.,0
%% Script,0
execute the query,0
loop through and save the results,0
MODIFY HERE depending on the query,0
wiitigers Unicode issue - no good mapping from DB file names to file names in blob URL,0
build filename,0
if need to re-download a dataset's images in case of corruption,0
entries_to_download = {,0
"filename: entry for filename, entry in entries_to_download.items()",0
if entry['dataset'] == DATASET,0
},0
input validation,0
"existing files, with paths relative to <store_dir>",0
parse JSON or TXT file,0
"create a new storage container client for this dataset,",0
and cache it,0
schema already checks that the min possible value of frame_num is 0,0
"if there are more than one image item, each needs a frame_num",0
checks across all sequence items,0
per sequence item checks,0
load the schema,0
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory,0
load the schema,0
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory,0
checks across all datasets items,0
check for expiry date of the SAS keys,0
update the sequences,0
"`id` is from the database, as well as all attributes starting with _",0
"if valuable sequence information is available, add them to the image",0
required fields for an image object,0
add seq-level class labels for this image,0
add other sequence-level properties to each image too,0
add other image-level properties,0
... for im in seq['images'],0
... for seq in mega_db,0
consolidate categories,0
some property names have changed in the new schema,0
a dummy sequence ID will be generated if the image entry does not have a seq_id field,0
seq_id only needs to be unique within this dataset; MegaDB does not rely on it as the _id field,0
"""annotations"" fields are opened and have its sub-field surfaced one level up",0
set the `dataset` property on each sequence to the provided dataset_name,0
check that the location field is the same for all images in a sequence,0
check which fields in a CCT image entry are sequence-level,0
image-level properties that really should be sequence-level,0
need to add (misidentified) seq properties not present for each image in a sequence to img_level_properties,0
"(some properties act like flags - all have the same value, but not present on each img)",0
add the sequence-level properties to the sequence objects,0
not every sequence have to have all the seq_level_properties,0
get the value of this sequence-level property from the first image entry,0
check which fields are really dataset-level and should be included in the dataset table instead.,0
delete sequence-level properties that should be dataset-level,0
make all `class` fields lower-case; cast `seq_id` to type string in case they're integers,0
turn all float NaN values into None so it gets converted to null when serialized,0
this was an issue in the Snapshot Safari datasets,0
%% validation,0
"at first a dict of image_id: image_obj with annotations embedded, then it becomes",0
an array of image objects,0
%% integrate the image DB,0
takes in image entries and species and other annotations in the image DB,0
convert the species category to explicit string name,0
there may be other fields in the annotation object,0
these fields should already be gotten from the image object,0
%% integrate the bbox DB,0
add any images that are not in the image DB,0
also add any fields in the image object that are not present already,0
add bbox to the annotations field,0
for any newly added images,0
"'bbox_abs': bbox_anno['bbox'],",0
not keeping height and width,0
,0
jb_csv_to_json.py,0
,0
Convert a particular .csv file to CCT format.  Images were not available at,0
"the time I wrote this script, so this is much shorter than other scripts",0
in this folder.,0
,0
%% Constants and environment,0
%% Read source data,0
%% Confirm filename uniqueness (this data set has one label per image),0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
%% Create CCT dictionaries,0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
imageName = imageFilenames[0],0
"As per above, this is convenient and appears to be true; asserting to be safe",0
"Filenames look like ""290716114012001a1116.jpg""",0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
%% Create info struct,0
%% Write output,0
%% Sanity-check,0
%% Imports and constants,0
%% Load data,0
"with open(image_json,'r') as f:",0
data = json.load(f),0
%% Sanity-check data,0
%% Label previews,0
%% Collect images to annotate,0
%% Sort by sequence and frame,0
%% Copy to a folder by GUID,0
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension,0
im = images_to_annotate[0],0
...for each image,0
%% Write out the annotation list,0
%% Constants and imports,0
%% Enumerate files,0
edited_image_folder = edited_image_folders[0],0
fn = edited_image_files[0],0
%% Read metadata and capture location information,0
i_row = 0; row = df.iloc[i_row],0
Sometimes '2017' was just '17' in the date column,0
%% Read the .json files and build output dictionaries,0
json_fn = json_files[0],0
if 'partial' in json_fn:,0
continue,0
line = lines[0],0
if image_fn == 'SD1_238_6_26_17_16_76.73.jpg':,0
asdfad,0
SD29_079_5_14_2018_17_52.85.jpg,0
Re-write two-digit years as four-digit years,0
Sometimes the year was written with two digits instead of 4,0
assert len(tokens[4]) == 4 and tokens[4].startswith('20'),0
Have we seen this location already?,0
"Not a typo, it's actually ""formateddata""",0
An image shouldn't be annotated as both empty and non-empty,0
An image shouldn't be annotated as both empty and non-empty,0
box = formatteddata[0],0
"open_file(os.path.join(base_folder,jpeg_image_folder,image_fn))",0
...for each box,0
...if there are boxes on this image,0
...for each line,0
...with open(),0
...for each json file,0
%% Prepare the output .json,0
%% Check DB integrity,0
%% Print unique locations,0
SD12_202_6_23_2017_1_31.85.jpg,0
%% Preview some images,0
%% Statistics,0
,0
"Import a Snapshot Safari project (one project, one season)",0
,0
Before running this script:,0
,0
"* Mount the blob container where the images live, or copy the",0
images to local storage,0
,0
What this script does:,0
,0
* Creates a .json file,0
* Creates zip archives of the season without humans.,0
* Copies animals and humans to separate folders,0
,0
After running this script:,0
,0
* Create or update LILA page,0
* Push zipfile and unzipped images to LILA,0
* Push unzipped humans to wildlifeblobssc,0
* Delete images from UMN uplaod storage,0
,0
"Snapshot Serengeti is handled specially, because we're dealing with bounding",0
boxes too.  See snapshot_serengeti_lila.py.,0
,0
%% Imports,0
From ai4eutils,0
From CameraTraps,0
%% Constants,0
project_name = 'XXX'; season_name = 'S1'; project_friendly_name = 'Snapshot Unknown',0
project_name = 'SER'; season_name = 'S1-11'; project_friendly_name = 'Snapshot Serengeti',0
project_name = 'KRU'; season_name = 'S1'; project_friendly_name = 'Snapshot Kruger',0
project_name = 'CDB'; season_name = 'S1'; project_friendly_name = 'Snapshot Camdeboo',0
project_name = 'MTZ'; season_name = 'S1'; project_friendly_name = 'Snapshot Mountain Zebra',0
project_name = 'ENO'; season_name = 'S1'; project_friendly_name = 'Snapshot Enonkishu',0
project_name = 'KAR'; season_name = 'S1'; project_friendly_name = 'Snapshot Karoo',0
project_name = 'KGA'; season_name = 'S1'; project_friendly_name = 'Snapshot Kgalagadi',0
%% Folder/file creation,0
E.g. KRU_S1,0
E.g. Z:\KRU,0
E.g. Z:\KRU\KRU_S1,0
Contains annotations for each capture event (sequence),0
Maps image IDs to filenames; each line looks like:,0
,0
"KRU_S1#1#1#2,3,KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0004.JPG",0
"Total number of each answer to each question, e.g. total number of times each species was identified",0
,0
Not used here,1
Create output folders,0
Images will be placed in a season-specific folder inside this (the source data includes,0
this in path names),0
%% Load metadata files,0
%% Convert to dictionaries (prep),0
%% Convert to dictionaries (loops),0
"TODO: iterrows() is a terrible way to do this, but this is one of those days",1
"where I want to get this done, not get better at Python.",0
irow = 0; row = image_table.iloc[0],0
"Loaded as an int64, converting to int here",0
...for each row in the image table,0
Make sure image IDs are what we think they are,0
...for each row in the annotation table,0
%% Take a look at categories (just sanity-checking),0
print('\nCategories by species:'),0
pp.pprint(categories_by_species),0
%% Fill in some image fields we didn't have when we created the image table,0
"width, height, corrupt, seq_num_frames, location, datetime",0
Every annotation in this list should have the same sequence ID,0
"Figure out ""seq_num_frames"", which really should be done in a separate lopp;",0
there's no reason to do this redundantly for every image,0
Every image in this sequence should point back to the same equence,0
Every annotation in this list should have the same location,0
Every annotation in this list should have the same datetime,0
Is this image on disk?,0
iImage = 0; im = images[0],0
...for each image,0
"images_processed = pool.map(process_image, images)",0
"images_processed = list(tqdm(pool.imap_unordered(process_image, images), total=len(images)))",0
%% Count missing/corrupted images,0
%% Print distribution of sequence lengths (sanity-check),0
%% Replicate annotations across images,0
iAnn = 0; ann = annotations[iAnn],0
%% See what files are on disk but not annotated,0
%% Sanity-check image and annotation uniqueness,0
%% Minor updates to fields,0
%% Write .json file,0
%% Create a list of human files,0
ann = annotations[0],0
%% Create public archive and public/private folders,0
im = images[0],0
E.g. KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0001.JPG,0
Copy to private output folder,0
Add to zipfile,0
Possibly start a new archive,0
Copy to public output folder,0
...for each image,0
%% Sanity-check .json file,0
"This will produce some validation errors, because this zipfile doesn't include humans",0
%% Zip up .json and .csv files,0
%% When I skip to this part (using a pre-rendered .json file),0
%%,0
%%,0
ann = annotations[0],0
%% Summary prep for LILA,0
"%% Generate preview, sanity-check labels",0
"viz_options.classes_to_include = ['jackalblackbacked','bustardkori']",0
%% Scrap,0
%% Find annotations for a particular image,0
%% Write a list of missing images,0
,0
save_the_elephants_survey_A.py,0
,0
Convert the .csv file provided for the Save the Elephants Survey A data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
Handle all unstructured fields in the source data as extra fields in the annotations,0
"photo_type really should be an image property, but there are a few conflicts",0
that forced me to handle it as an annotation proprerty,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
Maps relative paths to row indices in input_metadata,0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
Ignore directories,0
%% Make sure the multiple-annotation cases make sense,0
%%,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
Force the empty category to be ID 0,0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image],0
Example filename:,0
,0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG',0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG',0
i_row = row_indices[0],0
There are a small handful of datetime mismatches across annotations,0
for the same image,0
assert im['datetime'] == timestamp,0
Special cases based on the 'photo type' field,0
Various spellings of 'community',0
Have we seen this category before?,0
Create an annotation,0
fieldname = list(mapped_fields.keys())[0],0
...for each row,0
...for each image,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
%% Scrap,0
%% Find unique photo types,0
##,0
,0
timelapse_csv_set_to_json.py,0
,0
Given a directory full of reasonably-consistent Timelapse-exported,0
".csvs, assemble a CCT .json.",0
,0
"Assumes that you have a list of all files in the directory tree, including",0
image and .csv files.,0
,0
##,0
%% Constants and imports,0
Text file with relative paths to all files (images and .csv files),0
"%% Read file list, make a list of all image files and all .csv files",0
"%% Verify column consistency, create a giant array with all rows from all .csv files",0
i_csv = 0; csv_filename = csv_files[0],0
%% Prepare some data structures we'll need for mapping image rows in .csv files to actual image files,0
Enumerate all folders containing image files,0
"In this data set, a site folder looks like:",0
,0
Processed Images\\site_name,0
%% Map .csv files to candidate camera folders,0
fn = valid_csv_files[0],0
"Some site folders appear as ""XXNNNN"", some appear as ""XXNNNN_complete""",0
...for each .csv file,0
%% Map camera folders to candidate image folders,0
%% Main loop over labels (prep),0
Force the empty category to be ID 0,0
Images that are marked empty and also have a species label,0
%% Main loop over labels (loop),0
i_row = 0; row = input_metadata.iloc[i_row],0
"for i_row,row in input_metadata.iterrows():",0
"Usually this is just a single folder name, sometimes it's a full path,",0
which we don't want,0
Check whether this file exists on disk,0
Retrieve image width and height,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each row in the big table of concatenated .csv files,0
%% Fix cases where an image was annotated as 'unlabeled' and as something else,0
This annotation is 'unlabeled',0
Was there another category associated with this image?,0
%% Check for un-annnotated images,0
Enumerate all images,0
list(relative_path_to_image.keys())[0],0
%% Create info struct,0
%% Write output,0
%% Sanity-check the database's integrity,0
%% Render a bunch of images to make sure the labels got carried along correctly,0
,0
auckland_doc_to_json.py,0
,0
Convert Auckland DOC data set to COCO camera traps format.  This was,0
for a training data set where class names were encoded in path names.,0
,0
%% Constants and imports,0
Filenames will be stored in the output .json relative to this base dir,0
%% Enumerate files,0
%% Assemble dictionaries,0
Force the empty category to be ID 0,0
fn = image_files[0]; print(fn),0
Typically y:\Maukahuka_Auckland_Island\1_Training\Winter_Trial_2019\cat\cat\eat\20190903_IDdY_34_E3_tmp_201908240051.JPG,0
"This data set has two top-level folders, ""1_Training"" (which has class names encoded",0
"in paths) and ""2_Testing"" (which has no class information).",0
...for each image,0
%% Write output .json,0
%% Write train/test .jsons,0
%% Validate .json files,0
%% Preview labels,0
checkpoint,0
,0
awc_to_json.py,0
,0
Convert a particular .csv file to CCT format.,0
,0
%% Constants and environment,0
%% Read source data,0
%% Main loop over labels,0
Force the empty category to be ID 0,0
iRow = 0; row = input_metadata.iloc[iRow],0
"ImageID,FileName,FilePath,SpeciesID,CommonName",0
"This makes an assumption of one annotation per image, which happens to be",0
true in this data set.,0
Retrieve image width and height,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
list(relativePathToImage.keys())[0],0
%% Create info struct,0
%% Write output,0
%% Sanity-check the database's integrity,0
%% Render a bunch of images to make sure the labels got carried along correctly,0
,0
idaho-camera-traps.py,0
,0
Prepare the Idaho Camera Traps dataset for release on LILA.,0
,0
%% Imports and constants,0
Multi-threading for .csv file comparison and image existence validation,0
"We are going to map the original filenames/locations to obfuscated strings, but once",0
"we've done that, we will re-use the mappings every time we run this script.",0
This is the file to which mappings get saved,0
The maximum time (in seconds) between images within which two images are considered the,0
same sequence.,0
"This is a two-column file, where each line is [string in the original metadata],[category name we want to map it to]",0
"The output file, using the original strings",0
"The output file, using obfuscated strings for everything but filenamed",0
"The output file, using obfuscated strings and obfuscated filenames",0
"One time only, I ran MegaDetector on the whole dataset...",0
...then set aside any images that *may* have contained humans that had not already been,0
annotated as such.  Those went in this folder...,0
...and the ones that *actually* had humans (identified via manual review) got,0
copied to this folder...,0
"...which was enumerated to this text file, which is a manually-curated list of",0
images that were flagged as human.,0
Unopinionated .json conversion of the .csv metadata,0
%% List files (images + .csv),0
Ignore .csv files in folders with multiple .csv files,0
...which would require some extra work to decipher.,0
fn = csv_files[0],0
%% Parse each .csv file into sequences (function),0
csv_file = csv_files[-1],0
os.startfile(csv_file_absolute),0
survey = csv_file.split('\\')[0],0
Sample paths from which we need to derive locations:,0
,0
St.Joe_elk\AM99\Trip 1\100RECNX\TimelapseData.csv,0
Beaverhead_elk\AM34\Trip 1\100RECNX\TimelapseData.csv,0
,0
ClearCreek_mustelids\Winter2015-16\FS-001-P\FS-001-P.csv,0
ClearCreek_mustelids\Summer2015\FS-001\FS-001.csv,0
ClearCreek_mustelids\Summer2016\IDFG-016\IDFG-016.csv,0
,0
I:\idfg-images\ClearCreek_mustelids\Summer2016\IDFG-017b,0
I:\idfg-images\ClearCreek_mustelids\Summer2016\IDFG-017a,0
Load .csv file,0
Validate the opstate column,0
# Create datetimes,0
print('Creating datetimes'),0
i_row = 0; row = df.iloc[i_row],0
Make sure data are sorted chronologically,0
,0
"In odd circumstances, they are not... so sort them first, but warn",0
Debugging when I was trying to see what was up with the unsorted dates,0
# Parse into sequences,0
print('Creating sequences'),0
i_row = 0; row = df.iloc[i_row],0
Start a new sequence if:,0
,0
* This image has no timestamp,0
* This image has a frame number of zero,0
* We have no previous image timestamp,0
,0
Start a new sequence if necessary,0
...for each row,0
# Parse labels for each sequence,0
sequence_id = location_sequences[0],0
Row indices in a sequence should be adjacent,0
sequence_df = df[df['seq_id']==sequence_id],0
# Determine what's present,0
Be conservative; assume humans are present in all maintenance images,0
The presence columns are *almost* always identical for all images in a sequence,0
assert single_presence_value,0
print('Warning: presence value for {} is inconsistent for {}'.format(,0
"presence_column,sequence_id))",0
...for each presence column,0
Tally up the standard (survey) species,0
"If no presence columns are marked, all counts should be zero",0
count_column = count_columns[0],0
Occasionally a count gets entered (correctly) without the presence column being marked,0
"assert len(values) == 1 and values[0] == 0, 'Non-zero counts with no presence",0
columns marked for sequence {}'.format(sequence_id),0
"Handle this by virtually checking the ""right"" box",0
Make sure we found a match,0
Handle 'other' tags,0
column_name = otherpresent_columns[0],0
print('Found non-survey counted species column: {}'.format(column_name)),0
...for each non-empty presence column,0
"Very rarely, the ""otherpresent"" column is checked, but no more detail is available",0
...handling non-survey species,0
Build the sequence data,0
i_row = 0; row = sequence_df.iloc[i_row],0
Only one folder used a single .csv file for two subfolders,0
...for each sequence,0
...def csv_to_sequences(),0
%% Parse each .csv file into sequences (loop),0
%%,0
%%,0
i_file = -1; csv_file = csv_files[i_file],0
%% Save sequence data,0
%% Load sequence data,0
%%,0
%% Validate file mapping (based on the existing enumeration),0
sequences = sequences_by_file[0],0
sequence = sequences[0],0
"Actually, one folder has relative paths",0
assert '\\' not in image_file_relative and '/' not in image_file_relative,0
os.startfile(csv_folder),0
assert os.path.isfile(image_file_absolute),0
found_file = os.path.isfile(image_file_absolute),0
...for each image,0
...for each sequence,0
...for each .csv file,0
%% Load manual category mappings,0
The second column is blank when the first column already represents the category name,0
%% Convert to CCT .json (original strings),0
Force the empty category to be ID 0,0
For each .csv file...,0
,0
sequences = sequences_by_file[0],0
For each sequence...,0
,0
sequence = sequences[0],0
Find categories for this image,0
"When 'unknown' is used in combination with another label, use that",0
"label; the ""unknown"" here doesn't mean ""another unknown species"", it means",0
there is some other unknown property about the main species.,0
category_name_string = species_present[0],0
"This piece of text had a lot of complicated syntax in it, and it would have",0
been too complicated to handle in a general way,0
print('Ignoring category {}'.format(category_name_string)),0
Don't process redundant labels,0
category_name = category_names[0],0
If we've seen this category before...,0
If this is a new category...,0
print('Adding new category for {}'.format(category_name)),0
...for each category (inner),0
...for each category (outer),0
...if we do/don't have species in this sequence,0
"We should have at least one category assigned (which may be ""empty"" or ""unknown"")",0
assert len(sequence_category_ids) > 0,0
Was any image in this sequence manually flagged as human?,0
print('Flagging sequence {} as human based on manual review'.format(sequence_id)),0
For each image in this sequence...,0
,0
i_image = 0; im = images[i_image],0
Create annotations for this image,0
...for each image in this sequence,0
...for each sequence,0
...for each .csv file,0
Verify that all images have annotations,0
ann = ict_data['annotations'][0],0
For debugging only,0
%% Create output (original strings),0
%% Validate .json file,0
%% Preview labels,0
%% Look for humans that were found by MegaDetector that haven't already been identified as human,0
This whole step only needed to get run once,0
%%,0
Load MD results,0
Get a list of filenames that MD tagged as human,0
im = md_results['images'][0],0
...for each detection,0
...for each image,0
Map images to annotations in ICT,0
ann = ict_data['annotations'][0],0
For every image,0
im = ict_data['images'][0],0
Does this image already have a human annotation?,0
...for each annotation,0
...for each image,0
%% Copy images for review to a new folder,0
fn = missing_human_images[0],0
%% Manual step...,0
Copy any images from that list that have humans in them to...,0
%% Create a list of the images we just manually flagged,0
fn = human_tagged_filenames[0],0
E.g. '0000_Beaverhead_elk~AM174~Trip 1~100RECNX~IMG_1397.JPG',0
"%% Translate location, image, sequence IDs",0
Load mappings if available,0
Generate mappings,0
If we've seen this location before...,0
Otherwise assign a string-formatted int as the ID,0
If we've seen this sequence before...,0
Otherwise assign a string-formatted int as the ID,0
Assign an image ID,0
...for each image,0
Assign annotation mappings,0
Save mappings,0
"Back this file up, lest we should accidentally re-run this script",0
with force_generate_mappings = True and overwrite the mappings we used.,0
...if we are/aren't re-generating mappings,0
%% Apply mappings,0
"%% Write new dictionaries (modified strings, original files)",0
"%% Validate .json file (modified strings, original files)",0
%% Preview labels (original files),0
"viz_options.classes_to_exclude = ['empty','deer','elk']",0
viz_options.classes_to_include = ['bobcat'],0
%% Copy images to final output folder (prep),0
ann = d['annotations'][0],0
Is this a public or private image?,0
Generate absolute path,0
Copy to output,0
Update the filename reference,0
...def process_image(im),0
%% Copy images to final output folder (execution),0
For each image,0
im = images[0],0
Write output .json,0
%% Make sure the right number of images got there,0
%% Validate .json file (final filenames),0
%% Preview labels (final filenames),0
"viz_options.classes_to_exclude = ['empty','deer','elk']",0
viz_options.classes_to_include = ['horse'],0
viz_options.classes_to_include = [viz_options.multiple_categories_tag],0
"viz_options.classes_to_include = ['human','vehicle','domestic dog']",0
%% Create zipfiles,0
%% List public files,0
%% Find the size of each file,0
fn = all_public_output_files[0],0
%% Split into chunks of approximately-equal size,0
...for each file,0
%% Create a zipfile for each chunk,0
...for each filename,0
with ZipFile(),0
...def create_zipfile(),0
i_file_list = 0; file_list = file_lists[i_file_list],0
"....if __name__ == ""__main__""",0
,0
wellington_to_json.py,0
,0
Convert the .csv file provided for the Wellington data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
%% Read source data,0
"Filenames were provided as *.jpg, but images were *.JPG, converting here",0
"%% Map filenames to rows, verify image existence",0
"Takes ~30 seconds, since it's checking the existence of ~270k images",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
"I didn't expect this to be true a priori, but it appears to be true, and",0
it saves us the trouble of checking consistency across multiple occurrences,0
of an image.,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
imageName = imageFilenames[0],0
"As per above, this is convenient and appears to be true; asserting to be safe",0
"Filenames look like ""290716114012001a1116.jpg""",0
This gets imported as an int64,0
"These appear as ""image1"", ""image2"", etc.",0
"In the form ""001a""",0
Can be in the form '111' or 's46',0
"In the form ""7/29/2016 11:40""",0
Check image height and width,0
"Use 'empty', to be consistent with other data on lila",0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
,0
mcgill_to_json.py,0
,0
Convert the .csv file provided for the McGill test data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
Create an additional column for concatenated filenames,0
Maps relative filenames to rows,0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
row = input_metadata.iloc[0],0
"I didn't expect this to be true a priori, but it appears to be true, and",0
it saves us the trouble of checking consistency across multiple occurrences,0
of an image.,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
row = input_metadata.iloc[0],0
"Filenames look like ""290716114012001a1116.jpg""",0
"In the form ""001a""",0
Can be in the form '111' or 's46',0
"In the form ""7/29/2016 11:40""",0
Check image height and width,0
NaN is the only thing we should see that's not a string,0
NaN is the only thing we should see that's not a string,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
,0
bellevue_to_json.py,0
,0
"""Bellevue Camera Traps"" is the rather unremarkable camera trap data set",0
used by one of the repo's maintainers for testing.  It's organized as:,0
,0
approximate_date/[loose_camera_specifier/]/species,0
,0
E.g.:,0
,0
"""2018.03.30\coyote\DSCF0091.JPG""",0
"""2018.07.18\oldcam\empty\DSCF0001.JPG""",0
,0
%% Constants and imports,0
from the ai4eutils repo,0
Filenames will be stored in the output .json relative to this base dir,0
%% Exif functions,0
"%% Enumerate files, create image/annotation/category info",0
Force the empty category to be ID 0,0
Keep track of unique camera folders,0
Each element will be a dictionary with fields:,0
,0
"relative_path, width, height, datetime",0
fname = image_files[0],0
Corrupt or not an image,0
Store file info,0
E.g. 2018.03.30/coyote/DSCF0091.JPG,0
...for each image file,0
%% Synthesize sequence information,0
Sort images by time within each folder,0
camera_path = camera_folders[0],0
previous_datetime = sorted_images_this_camera[0]['datetime'],0
im = sorted_images_this_camera[1],0
Start a new sequence if necessary,0
...for each image in this camera,0
...for each camera,0
Fill in seq_num_frames,0
%% A little cleanup,0
%% Write output .json,0
%% Sanity-check data,0
%% Label previews,0
,0
snapshot_safar_importer_reprise.py,0
,0
This is a 2023 update to snapshot_safari_importer.py.  We do a bunch of things now that,0
we didn't do the last time we imported Snapshot data (like updating the big taxonomy),0
"file, and we skip a bunch of things now that we used to do (like generating massive",0
"zipfiles).  So, new year, new importer.",0
,0
%% Constants and imports,0
%% List files,0
"Do a one-time enumeration of the entire drive; this will take a long time,",0
but will save a lot of hassle later.,0
%% Create derived lists,0
Takes about 60 seconds,0
CSV files are one of:,0
,0
"_report_lila.csv (this is the one we want to use, with the species/count/etc. for each sequence)",0
_report_lila_image_inventory.csv (maps captures to images),0
_report_lila_overview.csv (distrubution of species),0
%% List project folders,0
Project folders look like one of these:,0
,0
APN,0
Snapshot Cameo/DEB,0
%% Map report and inventory files to codes,0
fn = csv_files[0],0
%% Make sure that every report has a corresponding inventory file,0
%% Count species based on overview and report files,0
%% Print counts,0
%% Make sure that capture IDs in the reports/inventory files match,0
...and that all the images in the inventory tables are actually present on disk.,0
assert image_path_relative in all_files_relative_set,0
Make sure this isn't just a case issue,0
...for each report on this project,0
...for each project,0
"%% For all the files we have on disk, see which are and aren't in the inventory files",0
"There aren't any capital-P .PNG files, but if I don't include that",0
"in this list, I'll look at this in a year and wonder whether I forgot",0
to include it.,0
fn = all_files_relative[0],0
print('Skipping project {}'.format(project_code)),0
,0
plot_wni_giraffes.py,0
,0
Plot keypoints on a random sample of images from the wni-giraffes data set.,0
,0
%% Constants and imports,0
%% Load and select data,0
%% Support functions,0
https://stackoverflow.com/questions/32504246/draw-ellipse-in-python-pil-with-line-thickness,0
Use a single channel image (mode='L') as mask.,0
The size of the mask can be increased relative to the imput image,0
to get smoother looking results.,0
draw outer shape in white (color) and inner shape in black (transparent),0
downsample the mask using PIL.Image.LANCZOS,0
(a high-quality downsampling filter).,0
paste outline color to input image through the mask,0
%% Plot some images,0
ann = annotations_to_plot[0],0
i_tool = 0; tool_name = short_tool_names[i_tool],0
Don't plot tools that don't have a consensus annotation,0
...for each tool,0
...for each annotation,0
,0
idfg_iwildcam_lila_prep.py,0
,0
Adding class labels (from the private test .csv) to the iWildCam 2019 IDFG,0
"test set, in preparation for release on LILA.",0
,0
This version works with the public iWildCam release images.,0
,0
"%% ############ Take one, from iWildCam .json files ############",0
%% Imports and constants,0
%% Read input files,0
Remove the header line,0
%% Parse annotations,0
Lines look like:,0
,0
"b005e5b2-2c0b-11e9-bcad-06f1011196c4,1,Private",0
%% Minor cleanup re: images,0
%% Create annotations,0
%% Prepare info,0
%% Minor adjustments to categories,0
Remove unused categories,0
Name adjustments,0
%% Create output,0
%% Write output,0
%% Validate .json file,0
%% Preview labels,0
viz_options.classes_to_exclude = ['test'],0
"%% ############ Take two, from pre-iWildCam .json files created from IDFG .csv files ############",0
%% Imports and constants,0
%% One-time line break addition,0
%% Read input files,0
%% Prepare info,0
%% Minor adjustments to categories,0
%% Minor adjustments to annotations,0
%% Create output,0
%% Write output,0
%% Validate .json file,0
%% Preview labels,0
viz_options.classes_to_exclude = ['test'],0
,0
missouri_to_json.py,0
,0
Create .json files from the original source files for the Missouri Camera Traps,0
data set.  Metadata was provided here in two formats:,0
,0
"1) In one subset of the data, folder names indicated species names.  In Set 1,",0
there are no empty sequences.  Set 1 has a metadata file to indicate image-level,0
bounding boxes.,0
,0
2) A subset of the data (overlapping with (1)) was annotated with bounding,0
"boxes, specified in a whitespace-delimited text file.  In set 2, there are",0
"some sequences omitted from the metadata file, which implied emptiness.",0
,0
"In the end, set 2 labels were not reliable enough to publish, so LILA includes only set 1.",0
,0
%% Constants and imports,0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Filenames will be stored in the output .json relative to this base dir,0
This will be a list of filenames that need re-annotation due to redundant boxes,0
"%% Enumerate files, read image sizes (both sets)",0
"Takes a few minutes, since we're reading image sizes.",0
Each element will be a list of relative path/full path/width/height,0
"Only process leaf nodes corresponding to sequences, which look like:",0
,0
E:\wildlife_data\missouri_camera_traps\Set1\1.02-Agouti\SEQ75583,0
E:\wildlife_data\missouri_camera_traps\Set2\p1d101,0
,0
assert len(files) <= 2,0
Read the image,0
Not an image...,0
Store file info,0
"...if we didn't hit the max file limit, keep going",0
...for each file,0
%% Add sequence lengths (both sets),0
%% Load the set 1 metadata file,0
"List of lists, length varies according to number of bounding boxes",0
,0
Preserves original ordering,0
iLine = 0; line = metadataSet1Lines[0],0
"Lines should be filename, number of bounding boxes, boxes (four values per box)",0
Make sure we have image info for this image,0
%% Print missing files from Set 1 metadata,0
Manual changes I made to the metadata file:,0
,0
'IMG' --> 'IMG_',0
Red_Brocket_Deer --> Red_Deer,0
European-Hare --> European_Hare,0
Wood-Mouse --> Wood_Mouse,0
Coiban-Agouti --> Coiban_Agouti,0
%% Load the set 2 metadata file,0
"This metadata file contains most (but not all) images, and a class label (person/animal/empty)",0
"for each, plus bounding boxes.",0
"List of lists, length varies according to number of bounding boxes",0
,0
Preserves original ordering,0
"Create class IDs for each *sequence*, which we'll use to attach classes to",0
images for which we don't have metadata,0
,0
This only contains mappings for sequences that appear in the metadata.,0
iLine = 0; line = metadataSet2Lines[0],0
"Lines should be filename, number of bounding boxes, labeled boxes (five values per box)",0
,0
Empty images look like filename\t0\t0,0
E.g. 'Set2\\p1d101\\p1d101s100i10.JPG',0
Make sure we don't have mixed classes within an image,0
"Figure out what class this *sequence* is, so we know how to handle unlabeled",0
images from this sequence,0
Can't un-do a mixed sequence,0
Previously-empty sequences get the image class label,0
"If the sequence has a non-empty class, possibly change it",0
Make sure we have image info for this image,0
...for each line in the set 2 metadata file,0
%% What Set 2 images do I not have metadata for?,0
These are *mostly* empty images,0
iImage = 0; imageID = set2ImageIDs[iImage],0
%% Create categories and annotations for set 1,0
"Though we have no empty sequences, we do have empty images in this set",0
For each image,0
,0
iImage = 0; imageID = set1ImageIDs[iImage],0
E.g. Set1\\1.80-Coiban_Agouti\\SEQ83155\\SEQ83155_IMG_0010.JPG,0
Find the species name,0
This image may still be empty...,0
category['count'] = category['count'] + 1,0
"If we have bounding boxes, create image-level annotations",0
"filename, number of bounding boxes, boxes (four values per box)",0
"Make sure the relative filename matches, allowing for the fact that",0
some of the filenames in the metadata aren't quite right,0
This image is non-empty,0
"Some redundant bounding boxes crept in, don't add them twice",0
Check this bbox against previous bboxes,0
,0
Inefficient?  Yes.  In an important way?  No.,0
"Bounding box values are in absolute coordinates, with the origin",0
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1].",0
,0
"Convert to floats and to x/y/w/h, as per CCT standard",0
...for each box,0
if we do/don't have boxes for this image,0
Else create a sequence-level annotation,0
...for each image,0
%% Write out the list of images with redundant boxes,0
%% Create categories and annotations for set 2,0
For each image,0
,0
iImage = 0; imageID = set2ImageIDs[iImage],0
E.g. 'Set2\\p1d100\\p1d100s10i1.JPG',0
"Find the sequence ID, sanity check filename against what we stored",0
"If we have bounding boxes or an explicit empty label, create image-level annotations",0
"filename, number of bounding boxes, labeled boxes (five values per box)",0
"Make sure the relative filename matches, allowing for the fact that",0
some of the filenames in the metadata aren't quite right,0
"Bounding box values are in absolute coordinates, with the origin",0
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1].",0
,0
Convert to floats and to x/y/w/h,0
...for each box,0
...if we do/don't have boxes for this image,0
Else create a sequence-level annotation,0
...for each image,0
"%% The 'count' field isn't really meaningful, delete it",0
"It's really the count of image-level annotations, not total images assigned to a class",0
%% Write output .json files,0
%% Sanity-check final set 1 .json file,0
"python sanity_check_json_db.py --bCheckImageSizes --baseDir ""E:\wildlife_data\missouri_camera_traps"" ""E:\wildlife_data\missouri_camera_traps\missouri_camera_traps_set1.json""",0
%% Generate previews,0
,0
filenames_to_json.py,0
,0
Take a directory of images in which species labels are encoded by folder,0
"names, and produces a COCO-style .json file",0
,0
%% Constants and imports,0
from the ai4eutils repo,0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Filenames will be stored in the output .json relative to this base dir,0
"rawClassListFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_classes.csv')",0
"classMappingsFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_class_mapping.csv')",0
"%% Enumerate files, read image sizes",0
Each element will be a list of relative path/full path/width/height,0
Read the image,0
Corrupt or not an image,0
Store file info,0
Write to output file,0
...for each image file,0
...csv file output,0
...if the file list is/isn't available,0
%% Enumerate classes,0
Maps classes to counts,0
We like 'empty' to be class 0,0
%% Assemble dictionaries,0
...for each category,0
Each element is a list of relative path/full path/width/height/className,0
...for each image,0
%% External class mapping,0
%% Write raw class table,0
cat = categories[0],0
%% Read the mapped class table,0
"id, source, count, target",0
"%% Make classMappings contain *all* classes, not just remapped classes",0
cat = categories[0],0
%% Create new class list,0
"Start at 1, explicitly assign 0 to ""empty""",0
One-off issue with character encoding,0
%% Re-map annotations,0
ann = annotations[0],0
%% Write output .json,0
%% Utilities,0
%%,0
Find images with a particular tag,0
%% Randomly sample annotations,0
,0
umn_to_json.py,0
,0
Prepare images and metadata for the Orinoqua Camera Traps dataset.,0
,0
%% Imports and constants,0
"For two deployments, we're only processing imagse in the ""detections"" subfolder",0
%% Enumerate deployment folders,0
%% Load ground truth,0
i_row = 0; row = ground_truth_df.iloc[i_row],0
%% Create relative paths for ground truth data,0
"Some deployment folders have no subfolders, e.g. this is a valid file name:",0
,0
M00/01010132.JPG,0
,0
"But some deployment folders have subfolders, e.g. this is also a valid file name:",0
,0
N17/100EK113/07160020.JPG,0
,0
"So we can't find files by just concatenating folder and file names, we have to enumerate and explicitly",0
"map what will appear in the ground truth as ""folder/filename"" to complete relative paths.",0
deployment_name = list(deployment_folders)[0],0
Enumerate all files in this folder,0
"print('Enumerated {} files for deployment {}'.format(len(files),deployment_name))",0
filename = files[100],0
...for each file in this deployment,0
...for each deployment,0
%% Add relative paths to our ground truth table,0
i_row = 0; row = ground_truth_df.iloc[i_row],0
"row['filename'] looks like, e.g. A01/01080001.JPG.  This is not actually a path, it's",0
"just the deployment ID and the image name, separated by a slash.",0
Find the relative path for this image,0
Make sure this image file exists,0
...for each row in the ground truth table,0
%% Take everything out of Pandas,0
%% Convert string timestamps to Python datetimes,0
im = ground_truth_dicts[0],0
"Filenames look like, e.g., N36/100EK113/06040726.JPG",0
%% Synthesize sequence information,0
Sort images by time within each location,0
i_location=0; location = locations[i_location],0
previous_datetime = sorted_images_this_location[0]['datetime'],0
im = sorted_images_this_camera[1],0
"Timestamp for this image, may be None",0
Start a new sequence if:,0
,0
* This image has no timestamp,0
* This iamge has a frame number of zero,0
* We have no previous image timestamp,0
,0
Start a new sequence if necessary,0
...for each image in this location,0
...for each location,0
%% Create category dict and category IDs,0
im = images[0],0
%% Imports and constants (.json generation),0
%% Count frames in each sequence,0
%% Double check images with multiple annotations,0
im = images[0],0
%% Assemble dictionaries,0
Force the empty category to be ID 0,0
input_im = images[0],0
...for each image,0
%% Write output .json,0
%% Validate .json file,0
%% Map relative paths to annotation categories,0
ann = data['annotations'][0],0
%% Copy images to output,0
EXCLUDE HUMAN AND MISSING,0
im = data['images'][0],0
im = images[0],0
%% Preview labels,0
viz_options.classes_to_exclude = ['test'],0
,0
nacti_fieldname_adjustments.py,0
,0
"NACTI metadata was posted with ""filename"" in images instead of ""file_name"", and",0
"used string (rather than int) category IDs (in categories, but not in annotations).",0
,0
This script fixes those issues and rev's the version number.,0
,0
%% Constants and environment,0
%% Read .json file,0
"%% Rev version number, update field names and types",0
%% Write json file,0
,0
cct_field_adjustments.py,0
,0
CCT metadata was posted with int locations instead of strings.,0
,0
This script fixes those issues and rev's the version number.,0
,0
%% Constants and environment,0
%% Read .json file,0
"%% Rev version number, update field names and types",0
%% Write json file,0
%% Check output data file,0
,0
carrizo_shrubfree_2018.py,0
,0
Convert the .csv file provided for the Carrizo Mojave data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
%% Read source data,0
Original .csv file had superfluous spaces in column names,0
"%% Map filenames to rows, verify image existence",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
58 missing files (of 17652),0
%% Check for images that aren't included in the metadata file,0
3012 of 20606 files are not in metadata,0
%% Create CCT dictionaries,0
Map categories to integer IDs,0
,0
The category '0' is reserved for 'empty',0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
Each filename should just match one row,0
Don't include images that don't exist on disk,0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
,0
snapshot_serengeti_lila.py,0
,0
Create zipfiles of Snapshot Serengeti S1-S11.,0
,0
"Create a metadata file for S1-S10, plus separate metadata files",0
"for S1-S11.  At the time this code was written, S11 was under embargo.",0
,0
Create zip archives of each season without humans.,0
,0
Create a human zip archive.,0
,0
%% Constants and imports,0
import sys; sys.path.append(r'c:\git\ai4eutils'),0
import sys; sys.path.append(r'c:\git\cameratraps'),0
assert(os.path.isdir(metadata_base)),0
"There are two redundant categories, and we re-map ""blank"" to ""empty"" as per CCT convention",0
"%% Load metadata files, concatenate into a single table",0
iSeason = 1,0
%% Convert to dictionaries (prep),0
%% Convert to dictionaries (loops),0
"TODO: iterrows() is a terrible way to do this, but this is one of those days",1
"where I want to get this done, not get better at Python.",0
irow = 0; row = image_table.iloc[0],0
"Loaded as an int64, converting to int here",0
...for each row in the image table,0
Make sure image IDs are what we think they are,0
...for each row in the annotation table,0
%% Load previously-saved dictionaries when re-starting mid-script,0
%%,0
%% Take a look at categories (just sanity-checking),0
%%,0
%% Fill in some image fields we didn't have when we created the image table,0
"width, height, corrupt, seq_num_frames, location, datetime",0
Every annotation in this list should have the same sequence ID,0
"Figure out ""seq_num_frames"", which really should be done in a separate lopp;",0
there's no reason to do this redundantly for every image,0
Every image in this sequence should point back to the same equence,0
Every annotation in this list should have the same location,0
Every annotation in this list should have the same datetime,0
Is this image on disk?,0
iImage = 0; im = images[0],0
...for each image,0
"images_processed = pool.map(process_image, images)",0
"images_processed = list(tqdm(pool.imap_unordered(process_image, images), total=len(images)))",0
%%,0
%% Count missing/corrupted images,0
%% Print distribution of sequence lengths (sanity-check),0
%% Replicate annotations across images,0
iAnn = 0; ann = annotations[iAnn],0
%% See what files are on disk but not annotated (~15 mins),0
247370 files not in the database (of 7425810),0
%% Load old image database,0
%% Look for old images not in the new DB and vice-versa,0
"At the time this was written, ""old"" was S1-S6",0
old_im = cct_old['images'][0],0
new_im = images[0],0
4 old images not in new db,0
12 new images not in old db,0
%% Save our work,0
%% Load our work,0
%%,0
%% Examine size mismatches,0
i_mismatch = -1; old_im = size_mismatches[i_mismatch],0
%% Sanity-check image and annotation uniqueness,0
"%% Split data by seasons, create master list for public seasons",0
ann = annotations[0],0
%% Minor updates to fields,0
"%% Write master .json out for S1-10, write individual season .jsons (including S11)",0
"Loop over all seasons, plus one iteration for the ""all public data"" iteration, and",0
"one for the ""all data"" iteration",0
%% Find categories that only exist in S11,0
List of categories in each season,0
Category 55 (fire) only in S11,0
Category 56 (hyenabrown) only in S11,0
Category 57 (wilddog) only in S11,0
Category 58 (kudu) only in S11,0
Category 59 (pangolin) only in S11,0
Category 60 (lioncub) only in S11,0
%% Prepare season-specific .csv files,0
iSeason = 1,0
%% Create a list of human files,0
ann = annotations[0],0
%% Save our work,0
%% Load our work,0
%%,0
"%% Create archives (human, per-season) (prep)",0
im = images[0],0
im = images[0],0
Don't include humans,0
Only include files from this season,0
Possibly start a new archive,0
...for each image,0
i_season = 0,0
"for i_season in range(0,nSeasons):",0
create_season_archive(i_season),0
%% Create archives (loop),0
pool = ThreadPool(nSeasons+1),0
"n_images = pool.map(create_archive, range(-1,nSeasons))",0
"seasons_to_zip = range(-1,nSeasons)",0
...for each season,0
%% Sanity-check .json files,0
%logstart -o r'E:\snapshot_temp\python.txt',0
%% Zip up .json and .csv files,0
pool = ThreadPool(len(files_to_zip)),0
"pool.map(zip_single_file, files_to_zip)",0
%% Super-sanity-check that S11 info isn't leaking,0
im = data_public['images'][0],0
ann = data_public['annotations'][0],0
iRow = 0; row = annotation_df.iloc[iRow],0
iRow = 0; row = image_df.iloc[iRow],0
%% Create bounding box archive,0
i_image = 0; im = data['images'][0],0
i_box = 0; boxann = bbox_data['annotations'][0],0
%% Sanity-check a few files to make sure bounding boxes are still sensible,0
import sys; sys.path.append(r'C:\git\CameraTraps'),0
%% Check categories,0
%% Summary prep for LILA,0
,0
wi_to_json,0
,0
Prepares CCT-formatted metadata based on a Wildlife Insights data export.,0
,0
"Mostly assumes you have the images also, for validation/QA.",0
,0
%% Imports and constants,0
"Because WI filenames are GUIDs, it's not practical to page through sequences in an",0
image viewer.  So we're going to (optionally) create a copy of the data set where,0
images are ordered.,0
%% Load ground truth,0
%% Take everything out of Pandas,0
%% Synthesize common names when they're not available,0
"Blank rows should always have ""Blank"" as the common name",0
"print('Warning: missing common name for row {} ({})'.format(i_row,row['filename']))",0
%% Convert string timestamps to Python datetimes,0
im = ground_truth_dicts[0],0
"The field called ""location"" in the WI .csv file is a URL, we want to reclaim",0
"the ""location"" keyword for CCT output",0
"Filenames look like, e.g., N36/100EK113/06040726.JPG",0
%% Synthesize sequence information,0
Sort images by time within each location,0
i_location=0; location = locations[i_location],0
previous_datetime = sorted_images_this_location[0]['datetime'],0
im = sorted_images_this_camera[1],0
"Timestamp for this image, may be None",0
Start a new sequence if:,0
,0
* This image has no timestamp,0
* This iamge has a frame number of zero,0
* We have no previous image timestamp,0
,0
Start a new sequence if necessary,0
...for each image in this location,0
...for each location,0
%% Create category dict and category IDs,0
im = images[0],0
%% Count frames in each sequence,0
%% Build relative paths,0
im = images[0],0
Sample URL:,0
,0
gs://project-asfasdfd/deployment/21444549/asdfasdfd-616a-4d10-a921-45ac456c568a.jpg',0
%% Double check images with multiple annotations,0
im = images[0],0
%% Assemble dictionaries,0
Force the empty category to be ID 0,0
input_im = images[0],0
"print('Warning: image ID {} ({}) has multiple annotations'.format(im['id'],im['id'].replace('_','/')))",0
...for each image,0
%% Write output .json,0
%% Validate .json file,0
%% Preview labels,0
"open_file(os.path.join(image_base,'2100703/1141a545-88d2-498b-a684-7431f7aeb324.jpg'))",0
%%,0
%% Create ordered dataset,0
"Because WI filenames are GUIDs, it's not practical to page through sequences in an",0
image viewer.  So we're going to create a copy of the data set where images are,0
ordered.,0
im = images_out[0]; im,0
%% Create ordered .json,0
%% Copy files to their new locations,0
im = ordered_images[0],0
im = data_ordered['images'][0],0
%% Preview labels in the ordered dataset,0
"open_file(os.path.join(image_base,'2100703/1141a545-88d2-498b-a684-7431f7aeb324.jpg'))",0
%% Open an ordered filename from the unordered filename,0
,0
rspb_to_json.py,0
,0
Convert the .csv file provided for the RSPB data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
[location] is an obfuscation,0
%% Create info struct,0
%% Read source data,0
metadataTable.columns.values,0
,0
"array(['Project', 'inDir', 'FileName', 'Station', 'Camera',",0
"'StationCameraFileName', 'DateTimeOriginal', 'DateReadable',",0
"'outDir', 'filename_new', 'fileExistsAlready', 'CopyStatus',",0
"'Species'], dtype=object)",0
We'll populate these later,0
keys should be lowercase,0
"%% Enumerate images, confirm filename uniqueness",0
"%% Update metadata filenames to include site and camera folders, check existence",0
,0
Takes ~1min,0
iRow = 0; row = metadataTable.iloc[iRow],0
There's a bug in the metadata; the 'camera' column isn't correct.,0
camera = row['Camera'],0
"These appear as, e.g., '3.22e12'",0
camera = str(int(float(camera))),0
Let's pull this out of the file name instead,0
,0
Filenames look like one of the following:,0
,0
A1__03224850850507__2015-11-28__10-45-04(1).JPG,0
Bayama2PH__C05__NA(NA).JPG,0
assert(os.path.isfile(fullPath)),0
metadataTable.iloc[iRow] = row,0
Re-assemble into an updated table,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
Write to a text file,0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
iRow = 0; row = metadataTable.iloc[iRow],0
A1__03224850850507__2015-11-28__10-45-04(1).JPG,0
'A1\\03224850850507\\A1__03224850850507__2015-11-28__10-45-04(1).JPG',0
Not currently populated,0
"Often -1, sometimes a semi-meaningful int",0
A1,0
03224850850507,0
"In variable form, but sometimes '28/11/2015 10:45'",0
Check image height and width,0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Write output,0
%% Check database integrity,0
%% Preview a few images to make sure labels were passed along sensibly,0
%% One-time processing step: copy images to a flat directory for annotation,0
%%,0
,0
pc_to_json.py,0
,0
Convert a particular collection of .csv files to CCT format.,0
,0
%% Constants and environment,0
%% Read and concatenate source data,0
List files,0
"List of dataframes, one per .csv file; we'll concatenate later",0
i_file = 87; fn = input_files[i_file],0
Concatenate into a giant data frame,0
%% List files,0
%% Main loop over labels (prep),0
Force the empty category to be ID 0,0
%% Main loop over labels (loop),0
iRow = 0; row = input_metadata.iloc[iRow],0
"ImageID,FileName,FilePath,SpeciesID,CommonName",0
assert os.path.isfile(full_path),0
Retrieve image width and height,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
%% See what's up with missing files,0
s = list(image_relative_paths)[0],0
s = missing_files[0],0
%% Check for images that aren't included in the metadata file,0
%% Sample the database,0
%%,0
Collect the images we want,0
%% Create info struct,0
%% Write output,0
%% Sanity-check the database's integrity,0
%% Render a bunch of images to make sure the labels got carried along correctly,0
options.classes_to_exclude = ['unlabeled'],0
%% Write out a list of files to annotate,0
,0
save_the_elephants_survey_B.py,0
,0
Convert the .csv file provided for the Save the Elephants Survey B data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
input_base = r'/mnt/blobfuse/wildlifeblobssc/ste_2019_08_drop',0
output_base = r'/home/gramener/survey_b',0
Handle all unstructured fields in the source data as extra fields in the annotations,0
"photo_type really should be an image property, but there are a few conflicts",0
that forced me to handle it as an annotation proprerty,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
"%% Map filenames to rows, verify image existence",0
Maps relative paths to row indices in input_metadata,0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
Ignore directories,0
%% Make sure the multiple-annotation cases make sense,0
%%,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
Force the empty category to be ID 0,0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image],0
Example filename:,0
,0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG',0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG',0
i_row = row_indices[0],0
timestamp = row['Date'],0
There are a small handful of datetime mismatches across annotations,0
for the same image,0
assert im['datetime'] == timestamp,0
Special cases based on the 'photo type' field,0
Various spellings of 'community',0
Have we seen this category before?,0
Create an annotation,0
fieldname = list(mapped_fields.keys())[0],0
...for each row,0
...for each image,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
%% Scrap,0
%% Find unique photo types,0
,0
ubc_to_json.py,0
,0
Convert the .csv file provided for the UBC data set to a,0
COCO-camera-traps .json file,0
,0
"Images were provided in eight folders, each of which contained a .csv",0
file with annotations.  Those annotations came in two slightly different,0
"formats, the two formats corresponding to folders starting with ""SC_"" and",0
otherwise.,0
,0
%% Constants and environment,0
Map Excel column names - which vary a little across spreadsheets - to a common set of names,0
%% Enumerate images,0
Load from file if we've already enumerated,0
%% Create CCT dictionaries,0
Force the empty category to be ID 0,0
To simplify debugging of the loop below,0
#%% Create CCT dictionaries (loop),0
#%%,0
Read source data for this folder,0
Rename columns,0
Folder name is the first two characters of the filename,0
,0
Create relative path names from the filename itself,0
Folder name is the camera name,0
,0
Create relative path names from camera name and filename,0
Which of our images are in the spreadsheet?,0
i_row = 0; fn = input_metadata['image_relative_path'][i_row],0
#%% Check for images that aren't included in the metadata file,0
Find all the images in this folder,0
Which of these aren't in the spreadsheet?,0
#%% Create entries in CCT dictionaries,0
Only process images we have on disk,0
"This is redundant, but doing this for clarity, at basically no performance",0
cost since we need to *read* the images below to check validity.,0
i_row = row_indices[0],0
"These generally represent zero-byte images in this data set, don't try",0
to find the very small handful that might be other kinds of failures we,0
might want to keep around.,0
print('Error opening image {}'.format(image_relative_path)),0
If we've seen this category before...,0
...make sure it used the same latin --> common mapping,0
,0
"If the previous instance had no mapping, use the new one.",0
assert common_name == category['common_name'],0
Create an annotation,0
...for each annotation we found for this image,0
...for each image,0
...for each dataset,0
Print all of our species mappings,0
"%% Copy images for which we actually have annotations to a new folder, lowercase everything",0
im = images[0],0
%% Create info struct,0
"%% Convert image IDs to lowercase in annotations, tag as sequence level",0
"While there isn't any sequence information, the nature of false positives",0
"here leads me to believe the images were labeled at the sequence level, so",0
we should trust labels more when positives are verified.  Overall false,0
positive rate looks to be between 1% and 5%.,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
,0
helena_to_cct.py,0
,0
Convert the Helena Detections data set to a COCO-camera-traps .json file,0
,0
%% Constants and environment,0
This is one time process,0
%% Create Filenames and timestamps mapping CSV,0
import pdb;pdb.set_trace(),0
%% To create CCT JSON for RSPB dataset,0
%% Read source data,0
Original Excel file had timestamp in different columns,0
"%% Map filenames to rows, verify image existence",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
%% Skipping this check because one image has multiple species,0
assert len(duplicate_rows) == 0,0
%% Check for images that aren't included in the metadata file,0
%% Create CCT dictionaries,0
Map categories to integer IDs,0
,0
The category '0' is reserved for 'empty',0
For each image,0
Don't include images that don't exist on disk,0
Some filenames will match to multiple rows,0
assert(len(rows) == 1),0
iRow = rows[0],0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
ann['datetime'] = row['datetime'],0
ann['site'] = row['site'],0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
%% Imports and constants,0
from github.com/microsoft/ai4eutils,0
from github.com/ecologize/CameraTraps,0
A list of files in the lilablobssc container for this data set,0
The raw detection files provided by NOAA,0
A version of the above with filename columns added,0
%% Read input .csv,0
%% Read list of files,0
%% Convert paths to full paths,0
i_row = 0; row = df.iloc[i_row],0
assert ir_image_path in all_files,0
...for each row,0
%% Write results,0
"%% Load output file, just to be sure",0
%% Render annotations on an image,0
i_image = 2004,0
%% Download the image,0
%% Find all the rows (detections) associated with this image,0
"as l,r,t,b",0
%% Render the detections on the image(s),0
In pixel coordinates,0
In pixel coordinates,0
%% Save images,0
%% Clean up,0
,0
carrizo_trail_cam_2017.py,0
,0
"Convert the .csv files provided for the ""Trail Cam Carrizo"" 2017 data set to",0
a COCO-camera-traps .json file.,0
,0
%% Constants and environment,0
%% Read source data,0
Removing the empty records,0
"%% Map filenames to rows, verify image existence",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
908 missing files (of 60562),0
%% Check for images that aren't included in the metadata file,0
105329 of 164983 files are not in metadata,0
%% Create CCT dictionaries,0
Map categories to integer IDs,0
,0
The category '0' is reserved for 'empty',0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
Each filename should just match one row,0
Don't include images that don't exist on disk,0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
,0
channel_islands_to_cct.py,0
,0
Convert the Channel Islands data set to a COCO-camera-traps .json file,0
,0
"Uses the command-line tool ExifTool (exiftool.org) to pull EXIF tags from images,",0
"because every Python package we tried failed to pull the ""Maker Notes"" field properly.",0
,0
"%% Imports, constants, paths",0
# Imports ##,0
# Constants ##,0
# Paths ##,0
Confirm that exiftool is available,0
"assert which(exiftool_command_name) is not None, 'Could not locate the ExifTool executable'",0
%% Load information from every .json file,0
"Ignore the sample file... actually, first make sure there is a sample file",0
...and now ignore that sample file.,0
json_file = json_files[0],0
ann = annotations[0],0
...for each annotation in this file,0
...for each .json file,0
"%% Confirm URL uniqueness, handle redundant tags",0
Have we already added this image?,0
"One .json file was basically duplicated, but as:",0
,0
Ellie_2016-2017 SC12.json,0
Ellie_2016-2017-SC12.json,0
"If the new image has no output, just leave the old one there",0
"If the old image has no output, and the new one has output, default to the one with output",0
Don't worry about the cases where someone tagged 'fox' and someone tagged 'fox_partial',0
...for each image we've already added,0
...if this URL is/isn't in the list of URLs we've already processed,0
...for each image,0
%% Save progress,0
%%,0
%%,0
%% Download files (functions),0
https://www.quickprogrammingtips.com/python/how-to-download-multiple-files-concurrently-in-python.html,0
"This is returned with a leading slash, remove it",0
%% Download files (execution),0
%% Read required fields from EXIF data (functions),0
"-G means ""Print group name for each tag"", e.g. print:",0
,0
[File]          Bits Per Sample                 : 8,0
,0
...instead of:,0
,0
Bits Per Sample                 : 8,0
"If we don't get any EXIF information, this probably isn't an image",0
line_raw = exif_lines[0],0
"Split on the first occurrence of "":""",0
Typically:,0
,0
"'[MakerNotes]    Sequence                        ', '1 of 3']",0
Not a typo; we are using serial number as a location,0
"If there are multiple timestamps, make sure they're *almost* the same",0
"If there are multiple timestamps, make sure they're *almost* the same",0
...for each line in the exiftool output,0
"This isn't directly related to the lack of maker notes, but it happens that files that are missing",0
maker notes also happen to be missing EXIF date information,0
...process_exif(),0
"This is returned with a leading slash, remove it",0
Ignore non-image files,0
%% Read EXIF data (execution),0
ann = images[0],0
%% Save progress,0
Use default=str to handle datetime objects,0
%%,0
%%,0
"Not deserializing datetimes yet, will do this if I actually need to run this",0
%% Check for EXIF read errors,0
%% Remove junk,0
Ignore non-image files,0
%% Fill in some None values,0
"...so we can sort by datetime later, and let None's be sorted arbitrarily",0
%% Find unique locations,0
%% Synthesize sequence information,0
Sort images by time within each location,0
i_location=0; location = locations[i_location],0
previous_datetime = sorted_images_this_location[0]['datetime'],0
im = sorted_images_this_camera[1],0
"Timestamp for this image, may be None",0
Start a new sequence if:,0
,0
* This image has no timestamp,0
* This image has a frame number of zero,0
* We have no previous image timestamp,0
,0
Start a new sequence if necessary,0
...for each image in this location,0
...for each location,0
%% Count frames in each sequence,0
images_this_sequence = [im for im in images if im['seq_id'] == seq_id],0
"%% Create output filenames for each image, store original filenames",0
i_location = 0; location = locations[i_location],0
i_image = 0; im = sorted_images_this_location[i_image],0
%% Save progress,0
Use default=str to handle datetime objects,0
%%,0
%%,0
%% Copy images to their output files (functions),0
%% Copy images to output files (execution),0
%% Rename the main image list for consistency with other scripts,0
%% Create CCT dictionaries,0
Make sure this is really a box,0
Transform to CCT format,0
Force the empty category to be ID 0,0
i_image = 0; input_im = all_image_info[0],0
"This issue only impacted one image that wasn't a real image, it was just a screenshot",0
"showing ""no images available for this camera""",0
Convert datetime if necessary,0
Process temperature if available,0
Read width and height if necessary,0
I don't know what this field is; confirming that it's always None,0
Process object and bbox,0
os.startfile(output_image_full_path),0
"Zero is hard-coded as the empty category, but check to be safe",0
"I can't figure out the 'index' field, but I'm not losing sleep about it",0
assert input_annotation['index'] == 1+i_ann,0
"Some annotators (but not all) included ""_partial"" when animals were partially obscured",0
"Annotators *mostly* used 'none', but sometimes 'empty'.  'empty' is CCT-correct.",0
If we've seen this category before...,0
If this is a new category...,0
...if this is an empty/non-empty annotation,0
Create an annotation,0
...for each annotation on this image,0
...for each image,0
%% Change *two* annotations on images that I discovered contains a human after running MDv4,0
%% Move human images,0
ann = annotations[0],0
%% Count images by location,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
viz_options.classes_to_exclude = [0],0
,0
ena24_to_json_2017.py,0
,0
Convert the ENA24 data set to a COCO-camera-traps .json file,0
,0
%% Constants and environment,0
Temporary folders for human and non-human images,0
Clean existing output folders/zipfiles,0
%% Support functions,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
Check image height and width,0
"Each row is category, [box coordinates]",0
"If there's just one row, loadtxt reads it as a 1d array; make it a 2d array",0
with one row,0
Each row is a bounding box,0
Have we seen this category before?,0
Create an annotation,0
...for each bounding box,0
"This was here for debugging; nearly every instance is Human+Horse, Human+Vehicle,",0
"or Human+Dog, but there is one Rabbit+Opossium, and a few Deer+Chicken!",0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
%% Create ZIP files for human and non human,0
%% Validate output,0
%% Preview labels,0
%% Imports and constants,0
configurations and paths,0
%% Helper functions,0
"dest_path = copy(source_path, dest_folder)",0
num_workers = multiprocessing.cpu_count(),0
pool = ThreadPool(num_workers),0
"results = pool.starmap(_copy_unzip, zip(sources, itertools.repeat(dest_folder)))",0
,0
print('Waiting for processes to finish...'),0
pool.close(),0
pool.join(),0
sequential,0
%% Command-line driver,0
if the blob container is already mounted on the VM,0
or you can download them using the storage Python SDK,0
store storage account key in environment variable AZ_STORAGE_KEY,0
,0
eMammal_helpers.py,0
,0
Support functions for processing eMammal metadata,0
,0
%% Constants and imports,0
%% Support functions,0
"pad to a total of 3 digits if < 1000, or 4 digits otherwise",0
img_frame is a string from the xml tree,0
"length 4 frame order is returned as is, others are left padded to be 3 digit long",0
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels,0
pad to a total of 4 digits,0
img_frame is a string from the xml tree,0
"length 4 frame order is returned as is, others are left padded to be 3 digit long",0
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels,0
others column,0
summer day hours: 6am - 7pm,0
others day hours: 7am - 6pm,0
,0
make_eMammal_json.py,0
,0
"Produces the COCO-formatted json database for an eMammal dataset, i.e. a",0
"collection of folders, each of which contains a deployment_manifest.xml file.",0
,0
"In this process, each image needs to be loaded to size it.",0
,0
"To add bounding box annotations to the resulting database, use",0
add_annotations_to_eMammal_json.py.,0
,0
%% Constants and imports,0
"Either add the eMammal directory to your path, or run from there",0
os.chdir(r'd:\git\CameraTraps\database_tools\eMammal'),0
import warnings,0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
warnings.filterwarnings('ignore'),0
Should we run the image size retrieval in parallel?,0
%% Support functions,0
%% Main loop (metadata processing; image sizes are retrieved later),0
deployment = folders[0],0
sequence = image_sequences[0],0
get species info for this sequence,0
add each image's info to database,0
img = images[0],0
"some manifests don't have the ImageOrder info, but the info is in the file name",0
full_img_id has no frame info,0
,0
frame number only used in requests to iMerit for ordering,0
...for each image,0
...for each sequence,0
...for each deployment,0
%% Get image sizes,0
"'tasks' is currently a list of 2-tuples, with each entry as [image dictionary,path].",0
,0
"Go through that and copy just the image dictionaries to 'db_images', adding size",0
information to each entry.  Takes a couple hours.,0
opening each image seems too fast for this multi-threaded version to be faster than sequential code.,0
%% Assemble top-level dictionaries,0
%% Write out .json,0
,0
make_full_SS_json.py,0
,0
Create a COCO-camera-traps .json file for Snapshot Serengeti data from,0
the original .csv files provided on Dryad.,0
,0
"This was used for ""version 1.0"" of the public Snapshot Serengeti archive; it's no",0
longer used as of v2.0 (early 2020).  See snapshot_serengeti_lila.py for the updated,0
Snapshot Safari preparation process.,0
,0
%% Imports and constants,0
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays",0
Count the number of images with multiple species,0
"%% Read image .csv file, format into a dictionary mapping images to capture events",0
%% Create CCT-style .json,0
still need image width and height,0
...for each image,0
%% Write output files,0
,0
make_per_season_SS_json.py,0
,0
Create a COCO-camera-traps .json file for each Snapshot Serengeti season from,0
the original .csv files provided on Dryad.,0
,0
"This was used for ""version 1.0"" of the public Snapshot Serengeti archive; it's no",0
longer used as of v2.0 (early 2020).  See snapshot_serengeti_lila.py for the updated,0
Snapshot Safari preparation process.,0
,0
%% Imports and constants,0
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays",0
"%% Read image .csv file, format into a dictionary mapping images to capture events",0
%% Create CCT-style .json,0
still need image width and height,0
...for each image ID,0
%% Write output files,0
...for each season,0
,0
generate_lila_per_image_labels.py,0
,0
"Generate a .csv file with one row per annotation, containing full URLs to every",0
"camera trap image on LILA, with taxonomically expanded labels.",0
,0
"Typically there will be one row per image, though images with multiple annotations",0
will have multiple rows.,0
,0
"Some images may not physically exist, particularly images that are labeled as ""human"".",0
This script does not validate image URLs.,0
,0
Does not include bounding box annotations.,0
,0
%% Constants and imports,0
"We'll write images, metadata downloads, and temporary files here",0
"Some datasets don't have ""sequence_level_annotation"" fields populated, but we know their",0
annotation level,0
%% Download and parse the metadata file,0
To select an individual data set for debugging,0
%% Download and extract metadata for the datasets we're interested in,0
%% Load taxonomy data,0
"%% Build a dictionary that maps each [dataset,query] pair to the full taxonomic label set",0
i_row = 0; row = taxonomy_df.iloc[i_row],0
%% Process annotations for each dataset,0
ds_name = list(metadata_table.keys())[0],0
"Go through annotations, marking each image with the categories that are present",0
,0
ann = annotations[0],0
im = images[10],0
This field name was only used for Caltech Camera Traps,0
raise ValueError('Suspicious date parsing result'),0
Special case we don't want to print a warning about,0
"Location, sequence, and image IDs are only guaranteed to be unique within",0
"a dataset, so for the output .csv file, include both",0
category_name = list(categories_this_image)[0],0
Only print a warning the first time we see an unmapped label,0
...for each category that was applied at least once to this image,0
...for each image in this dataset,0
print('Warning: no date information available for this dataset'),0
print('Warning: no location information available for this dataset'),0
...for each dataset,0
...with open(),0
%% Read the .csv back,0
%% Do some post-hoc integrity checking,0
"-1 isn't *really* valid, but we use it sometimes for sequences of unknown length",0
i_row = 0; row = df.iloc[i_row],0
%% Preview constants,0
%% Choose images to download,0
ds_name = list(metadata_table.keys())[2],0
Find all rows for this dataset,0
...for each dataset,0
%% Download images,0
i_image = 0; image = images_to_download[i_image],0
%% Write preview HTML,0
im = images_to_download[0],0
,0
Common constants and functions related to LILA data management/retrieval.,0
,0
%% Imports and constants,0
LILA camera trap master metadata file,0
"wildlife_insights_taxonomy_url = 'https://api.wildlifeinsights.org/api/v1/taxonomy?fields=class,order,family,genus,species,authority,taxonomyType,uniqueIdentifier,commonNameEnglish&page[size]={}'.format(wildlife_insights_page_size)",0
from ai4eutils,0
%% Common functions,0
"We haven't implemented paging, make sure that's not an issue",0
d['data'] is a list of items that look like:,0
Put the master metadata file in the same folder where we're putting images,0
Read lines from the master metadata file,0
Parse those lines into a table,0
Each line in this file is name/sas_url/json_url/[bbox_json_url],0
Create a separate entry for bounding boxes if they exist,0
Unzip if necessary,0
,0
get_lila_category_list.py,0
,0
Generates a .json-formatted dictionary mapping each LILA dataset to all categories,0
"that exist for that dataset, with counts for the number of occurrences of each category",0
"(the number of *annotations* for each category, not the number of *images*).",0
,0
"Also loads the taxonomy mapping file, to include scientific names for each category.",0
,0
get_lila_category_counts counts the number of *images* for each category in each dataset.,0
,0
%% Constants and imports,0
array to fill for output,0
"We'll write images, metadata downloads, and temporary files here",0
Created by get_lila_category_list.py... contains counts for each category,0
%% Load category and taxonomy files,0
%% Map dataset names and category names to scientific names,0
i_row = 1; row = taxonomy_df.iloc[i_row]; row,0
%% Download and parse the metadata file,0
%% Download and extract metadata for the datasets we're interested in,0
%% Get category names and counts for each dataset,0
ds_name = 'NACTI',0
Open the metadata file,0
Collect list of categories and mappings to category name,0
ann = annotations[0],0
c = categories[0],0
"Don't do taxonomy mapping for bbox data sets, which are sometimes just binary and are",0
always redundant with the class-level data sets.,0
"As of right now, this is the only quirky case",0
...for each dataset,0
%% Save dict,0
%% Print the results,0
ds_name = list(dataset_to_categories.keys())[0],0
...for each dataset,0
,0
download_lila_subset.py,0
,0
"Example of how to download a list of files from LILA, e.g. all the files",0
in a data set corresponding to a particular species.,0
,0
%% Constants and imports,0
LILA camera trap master metadata file,0
"In this example, we're using the Missouri Camera Traps data set and the Caltech Camera Traps dataset",0
All lower-case; we'll convert category names to lower-case when comparing,0
"We'll write images, metadata downloads, and temporary files here",0
"We will demonstrate two approaches to downloading, one that loops over files",0
"and downloads directly in Python, another that uses AzCopy.",0
,0
AzCopy will generally be more performant and supports resuming if the,0
transfers are interrupted.  This script assumes that azcopy is on the system path.,0
Number of concurrent download threads (when not using AzCopy) (AzCopy does its,0
own magical parallelism),0
%% Support functions,0
remove the leading '/',0
%% Download and parse the metadata file,0
Put the master metadata file in the same folder where we're putting images,0
Read lines from the master metadata file,0
Parse those lines into a table,0
Each line in this file is name/base_url/json_url/[box_url],0
%% Download and extract metadata for the datasets we're interested in,0
Unzip if necessary,0
...for each dataset of interest,0
%% List of files we're going to download (for all data sets),0
"Flat list or URLS, for use with direct Python downloads",0
For use with azcopy,0
This may or may not be a SAS URL,0
# Open the metadata file,0
# Build a list of image files (relative path names) that match the target species,0
Retrieve all the images that match that category,0
Retrieve image file names,0
Convert to URLs,0
...for each dataset,0
%% Download those image files,0
ds_name = 'Caltech Camera Traps',0
ds_name = 'SWG Camera Traps',0
We want to use the whole relative path for this script (relative to the base of the container),0
"to build the output filename, to make sure that different data sets end up in different folders.",0
This may or may not be a SAS URL,0
For example:,0
,0
caltech-unzipped/cct_images,0
swg-camera-traps,0
Check whether the URL includes a folder,0
E.g. caltech-unzipped,0
E.g. cct_images,0
E.g. swg-camera-traps,0
"Write out a list of files, and use the azcopy ""list-of-files"" option to download those files.",0
,0
This azcopy feature is unofficially documented at:,0
,0
https://github.com/Azure/azure-storage-azcopy/wiki/Listing-specific-files-to-transfer,0
,0
import clipboard; clipboard.copy(cmd),0
Loop over files,0
,0
get_lila_category_counts.py,0
,0
Count the number of images and bounding boxes with each label in one or more LILA datasets.,0
,0
"This script doesn't write these counts out anywhere other than the console, it's just intended",0
as a template for doing operations like this on LILA data.  get_lila_category_list.py writes,0
"information out to a .json file, but it counts *annotations*, not *images*, for each category.",0
,0
%% Constants and imports,0
"If None, will use all datasets",0
"We'll write images, metadata downloads, and temporary files here",0
%% Download and parse the metadata file,0
%% Download and extract metadata for the datasets we're interested in,0
%% Count categories,0
ds_name = datasets_of_interest[0],0
"Go through annotations, marking each image with the categories that are present",0
,0
ann = annotations[0],0
Now go through images and count categories,0
im = images[0],0
...for each dataset,0
%% Print the results,0
...for each dataset,0
,0
add_url_to_database.py,0
,0
"Adds a ""url"" field to a coco-camera-traps .json database, specifically to allow the db to",0
be reviewed in the Visipedia annotation tool.,0
,0
,0
remove_corrupted_images_from_database.py,0
,0
"Given a coco-camera-traps .json file, checks all images for TF-friendliness and generates",0
a new .json file that only contains the non-corrupted images.,0
,0
%% Imports and constants,0
import multiprocessing,0
"I leave this at an annoying low number, since by definition weird stuff will",0
"be happening in the TF kernel, and it's useful to keep having content in the console.",0
%% Function definitions,0
"I sometimes pass in a list of images, sometimes a dict with a single",0
element mapping a job ID to the list of images,0
"We're about to start a lot of TF sessions, and we don't want gobs",0
of debugging information printing out for every session.,0
At some point we were creating a single session and looping over images,0
"within that session, but the only way I found to reliably not run out",0
of GPU memory was to create a session per image and gc.collect() after,0
each session.,0
Map Image IDs to boolean (should I keep this image?),0
"Convert to lists, append job numbers to the image lists",0
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)",0
Merge results,0
%% Interactive driver,0
%%,0
base_dir = r'D:\temp\snapshot_serengeti_tfrecord_generation',0
Load annotations,0
Check for corruption,0
Write out only the uncorrupted data,0
%% Command-line driver,0
#######,0
,0
integrity_check_json_db.py,0
,0
"Does some integrity-checking and computes basic statistics on a db, specifically:",0
,0
* Verifies that required fields are present and have the right types,0
* Verifies that annotations refer to valid images,0
* Verifies that annotations refer to valid categories,0
"* Verifies that image, category, and annotation IDs are unique",0
,0
* Optionally checks file existence,0
,0
* Finds un-annotated images,0
* Finds unused categories,0
,0
* Prints a list of categories sorted by count,0
,0
#######,0
%% Constants and environment,0
%% Functions,0
"If baseDir is non-empty, checks image existence",0
This is used in a medium-hacky way to share modified options across threads,1
print('Image path {} does not exist'.format(filePath)),0
"#%% Read .json file if necessary, integrity-check fields",0
info = data['info'],0
"#%% Build dictionaries, checking ID uniqueness and internal validity as we go",0
Confirm that required fields are present,0
Confirm ID uniqueness,0
...for each category,0
image = images[0],0
Confirm that required fields are present,0
Confirm ID uniqueness,0
We previously supported ints here; this should be strings now,0
"assert isinstance(image['location'], str) or isinstance(image['location'], int), \",0
'Illegal image location type',0
Are we checking for unused images?,0
Recursively enumerate images,0
print('Image {} is unused'.format(p)),0
Are we checking file existence and/or image size?,0
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)",0
...for each image,0
Confirm that required fields are present,0
Confirm ID uniqueness,0
Confirm validity,0
...for each annotation,0
#%% Print statistics,0
Find un-annotated images and multi-annotation images,0
Find unused categories,0
Prints a list of categories sorted by count,0
,0
https://stackoverflow.com/questions/72899/how-do-i-sort-a-list-of-dictionaries-by-a-value-of-the-dictionary,0
...def integrity_check_json_db(),0
%% Command-line driver,0
%% Interactive driver(s),0
%%,0
Integrity-check .json files for LILA,0
options.iMaxNumImages = 10,0
,0
combine_two_json_files.py,0
,0
"Merges two coco-camera-traps .json files. In particular, categories are combined and re-numbered.",0
,0
Combined Info,0
Combined Images,0
Combined Categories,0
## categories to merge,0
Combined Annotations,0
,0
make_detection_db_for_viewing.py,0
,0
"Given a .json file with ground truth bounding boxes, and a .p file containing detections for the same images,",0
"creates a new .json file with separate classes for ground truth and detection, suitable for viewing in the Visipedia",0
annotation tool.,0
,0
%% Imports and constants,0
%% Main function,0
im_id_to_im = {im['id']:im for im in images},0
make new categories to distinguish between ground truth and detections,0
"update all gt annotations to be class ""gt""",0
collect all detections by image,0
keep any detection with score above det_thresh,0
"need to convert bbox from [x1,y1,x2,y2] to [x,y,w,h]",0
"add ""info"" and ""licenses"" for annotation tools to function",0
create new db,0
%% Command-line handling,0
,0
analyze_json_database.py,0
,0
Plots location/class/etc. distributions for classes in a coco-camera-traps .json file.,0
,0
Currently includes some one-off code for specific species.,0
,0
%% Constants and imports,0
%% Path configuration,0
%% Load source data,0
%% Build image/category dictionaries,0
%% Make plot of category distribution,0
%% make plots of location distribution,0
"plt.title('Number of images per location, by category')",0
"plt.tight_layout(rect=[0,0,1,0.9])",0
#make plot of images per season,0
%% Make plot of lions per location,0
%% Make plot of elephants per location,0
for loc in sorted_by_total[:25]:,0
"print('Location:' + loc[0] +', Lions: ' + str(len(loc_to_lion_ims[loc[0]])) + ', Elephants: ' + str(len(loc_to_elephant_ims[loc[0]])) + ', Total ims: ' + str(len(loc_to_ims[loc[0]])))",0
,0
subset_json_db.py,0
,0
Select a subset of images (and associated annotations) from a .json file,0
in COCO Camera Traps format.,0
,0
"To subset the .json files produced by our batch processing API, see",0
subset_json_detector_output.py,0
,0
Sample invocation:,0
,0
"python subset_json_db.py ""E:\Statewide_wolf_container\idfg_20190409.json"" ""E:\Statewide_wolf_container\idfg_20190409_clearcreek.json"" ""clearcreek"" --ignore_case",0
,0
%% Constants and imports,0
%% Functions,0
Load the input file if necessary,0
Find images matching the query,0
Find annotations referring to those images,0
Write the output file if requested,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
%% Constants and imports,0
%% Merge functions,0
i_input_dict = 0; input_dict = input_dicts[i_input_dict],0
We will prepend an index to every ID to guarantee uniqueness,0
Map detection categories from the original data set into the merged data set,0
...for each category,0
Merge original image list into the merged data set,0
Create a unique ID,0
...for each image,0
Same for annotations,0
...for each annotation,0
"Merge info dicts, don't check completion time fields",0
...for each dictionary,0
Convert merged image dictionaries to a sorted list,0
%% Driver,0
,0
add_width_and_height_to_database.py,0
,0
Grabs width and height from actual image files for a .json database that is missing w/h.,0
,0
Originally used when we created a .json file for snapshot serengeti from .csv.,0
,0
"This file converts the JSON output of the batch processing API to a pickle file, which can be used by the",0
script ./make_classification_dataset.py,0
"Convert boxes from JSON   [x_min, y_min, width_of_box, height_of_box]",0
"to PICKLE [ymin,  xmin,  ymax,         xmax]",0
Write detections to file with pickle,0
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the",0
script ./make_classification_dataset.py,0
Write detections to file with pickle,0
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the",0
script ./make_classification_dataset.py,0
Assumes that the root of the CameraTrap repo is in the PYTHONPATH,0
Minimum threshold to put a detection into the output JSON file,0
Parameter check,0
Load detections from input,0
Load COCO style annotations,0
Build output JSON in format version 1.0,0
Adding the only known metadata info,0
"The pickle file does not contain category information, so we assume the default",0
For each image with detections,0
for each detection,0
"Convert boxes from [ymin, xmin, ymax, xmax] format to",0
"[x_min, y_min, width_of_box, height_of_box]",0
Write output json,0
global flag for whether or not we encounter missing images,0
"- will only print ""missing image"" warning once",0
TFRecords variables,0
1.3 for the cropping during test time and 1.3 for the context that the,0
CNN requires in the left-over image,0
Create output directories,0
Load COCO style annotations from the input dataset,0
"Get all categories, their names, and create updated ID for the json file",0
Prepare the coco-style json files,0
Split the dataset by locations,0
Load detections,0
Write out COCO-style json files to the output directory,0
Write detections to file with pickle,0
## Preparations: get all the output tensors,0
For all images listed in the annotations file,0
Skip the image if it is annotated with more than one category,0
"Get ""old"" and ""new"" category ID and category name for this image.",0
Skip if in excluded categories.,0
get path to image,0
"If we already have detection results, we can use them",0
Otherwise run detector and add detections to the collection,0
Only select detections with confidence larger than DETECTION_THRESHOLD,0
Skip image if no detection selected,0
whether it belongs to a training or testing location,0
Skip images that we do not have available right now,0
- this is useful for processing parts of large datasets,0
Load image,0
Run inference,0
"remove batch dimension, and convert from float32 to appropriate type",0
convert normalized bbox coordinates to pixel coordinates,0
Pad the detected animal to a square box and additionally by,0
"PADDING_FACTOR. The result will be in crop_boxes. However, we need to make",0
sure that its box coordinates are still within the image.,0
"for each bounding box, crop the image to the padded box and save it",0
"Create the file path as it will appear in the annotation json,",0
adding the box number if there are multiple boxes,0
"if the cropped file already exists, verify its size",0
Add annotations to the appropriate json,0
js_keys = ['/'.join(im['file_name'].split('/')[1:])[:-4] for im in js['images']],0
for tk in js_keys:,0
"assert np.isclose(1, np.sum(detections[tk]['detection_scores'] > 0.5))",0
,0
separate_detections_by_size,0
,0
Not-super-well-maintained script to break a list of API output files up,0
based on bounding box size.,0
,0
%% Imports and constants,0
Folder with one or more .json files in it that we want to split up,0
Enumerate .json files,0
Define size thresholds and confidence thresholds,0
"Not used directly in this script, but useful if we want to generate previews",0
%% Split by size,0
For each size threshold...,0
For each file...,0
fn = input_files[0],0
Just double-checking; we already filtered this out above,0
Don't reprocess .json files we generated with this script,0
Load the input file,0
For each image...,0
1.1 is the same as infinity here; no box can be bigger than a whole image,0
What's the smallest detection above threshold?,0
"[x_min, y_min, width_of_box, height_of_box]",0
,0
size = w * h,0
...for each detection,0
Which list do we put this image on?,0
...for each image in this file,0
Make sure the number of images adds up,0
Write out all files,0
...for each size threshold,0
...for each file,0
,0
tile_images.py,0
,0
Split a folder of images into tiles.  Preserves relative folder structure in a,0
"new output folder, with a/b/c/d.jpg becoming, e.g.:",0
,0
a/b/c/d_row_0_col_0.jpg,0
a/b/c/d_row_0_col_1.jpg,0
,0
%% Imports and constants,0
from ai4eutils,0
%% Main function,0
TODO: parallelization,1
,0
i_fn = 2; relative_fn = image_relative_paths[i_fn],0
Can we skip this image because we've already generated all the tiles?,0
TODO: super-sloppy that I'm pasting this code from below,1
From:,0
,0
https://github.com/whiplashoo/split-image/blob/main/src/split_image/split.py,0
i_col = 0; i_row = 1,0
left/top/right/bottom,0
...for each row,0
...for each column,0
...for each image,0
%% Interactive driver,0
%%,0
%%,0
,0
rde_debug.py,0
,0
Some useful cells for comparing the outputs of the repeat detection,0
"elimination process, specifically to make sure that after optimizations,",0
results are the same up to ordering.,0
,0
%% Compare two RDE files,0
i_dir = 0,0
break,0
"Regardless of ordering within a directory, we should have the same",0
number of unique detections,0
Re-sort,0
Make sure that we have the same number of instances for each detection,0
Make sure the box values match,0
,0
aggregate_video.py,0
,0
Aggregate results and render output video for a video that's already been run through MD,0
,0
%% Constants,0
%% Processing,0
im = d['images'][0],0
...for each detection,0
This is no longer included in output files by default,0
# Split into frames,0
# Render output video,0
## Render detections to images,0
## Combine into a video,0
,0
ocr_sandbox.py,0
,0
sandbox for experimenting with using OCR to pull metadata from camera trap images,0
,0
The general approach is:,0
,0
"* Crop a fixed percentage from the top and bottom of an image, slightly larger",0
than the largest examples we've seen of how much space is used for metadata.,0
,0
"* Refine that crop by blurring a little, then looking for huge peaks in the",0
"color histogram suggesting a solid background, then finding rows that are",0
mostly that color.,0
,0
"* Crop to the refined crop, then run pytesseract to extract text",0
,0
"* Use regular expressions to find time and date, in the future can add, e.g.,",0
"temperature (which is often present *only* in the images, unlike time/date which",0
are also usually in EXIF but often wrong or lost in processing),0
,0
"The metadata extraction (EXIF, IPTC) here is just sample code that seemed to",0
belong in this file.,0
,0
Contact: Dan Morris (cameratraps@lila.science),0
,0
%% Constants and imports,0
pip install pytesseract,0
,0
"Also intall tesseract from: https://github.com/UB-Mannheim/tesseract/wiki, and add",0
"the installation dir to your path (on Windows, typically C:\Program Files (x86)\Tesseract-OCR)",0
pip install IPTCInfo3,0
from the ai4eutils repo: https://github.com/Microsoft/ai4eutils,0
,0
"Only used for writing out a summary, not important for the core metadata extraction",0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Using a semi-arbitrary metric of how much it feels like we found the,0
"text-containing region, discard regions that appear to be extraction failures",0
Pad each crop with a few pixels to make tesseract happy,0
Discard text from the top,0
"When we're looking for pixels that match the background color, allow some",0
tolerance around the dominant color,0
We need to see a consistent color in at least this fraction of pixels in our rough,0
crop to believe that we actually found a candidate metadata region.,0
"What fraction of the [top,bottom] of the image should we use for our rough crop?",0
A row is considered a probable metadata row if it contains at least this fraction,0
"of the background color.  This is used only to find the top and bottom of the crop area,",0
"so it's not that *every* row needs to hit this criteria, only the rows that are generally",0
above and below the text.,0
%% Support functions,0
"%% Load some images, pull EXIF and IPTC data for fun",0
%% Rough crop,0
"This will be an nImages x 1 list of 2 x 1 lists (image top, image bottom)",0
image = images[0],0
"l,t,r,b",0
,0
"0,0 is upper-left",0
"%% Close-crop around the text, return a revised image and success metric",0
Did we find a sensible mode that looks like a background value?,0
"This looks very scientific, right?  Definitely a probability?",0
"print('Failed min background fraction test: {} of {}'.format(pBackGroundValue,minBackgroundFraction))",0
"Notes to self, things I tried that didn't really go anywhere...",0
"analysisImage = cv2.blur(analysisImage, (3,3))",0
"analysisImage = cv2.medianBlur(analysisImage,5)",0
"analysisImage = cv2.Canny(analysisImage,100,100)",0
imagePil = Image.fromarray(analysisImage); imagePil,0
Use row heuristics to refine the crop,0
,0
This egregious block of code makes me miss my fluency in Matlab.,1
"print('Cropping to {},{},{},{}'.format(x,y,w,h))",0
Crop the image,0
"For some reason, tesseract doesn't like characters really close to the edge",0
imagePil = Image.fromarray(croppedImage); imagePil,0
%% Go to OCR-town,0
"An nImages x 2 list of strings, extracted from the top and bottom of each image",0
An nImages x 2 list of cropped images,0
iImage = 0; iRegion = 1; regionSet = imageRegions[iImage]; region = regionSet[iRegion],0
text = pytesseract.image_to_string(region),0
pil --> cv2,0
"image = cv2.medianBlur(image, 3)",0
"image = cv2.erode(image, None, iterations=2)",0
"image = cv2.dilate(image, None, iterations=4)",0
"image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]",0
"image = cv2.blur(image, (3,3))",0
"image = cv2.copyMakeBorder(image,10,10,10,10,cv2.BORDER_CONSTANT,value=[0,0,0])",0
"text = pytesseract.image_to_string(imagePil, lang='eng')",0
https://github.com/tesseract-ocr/tesseract/wiki/Command-Line-Usage,0
"psm 6: ""assume a single uniform block of text""",0
,0
...for each cropped region,0
...for each image,0
%% Extract dates and times,0
s = '1:22 pm',0
s = '1:23:44 pm',0
%% Write results to a handy html file,0
Add image name and resized image,0
Add results and individual region images,0
"textStyle = ""font-family:calibri,verdana,arial;font-weight:bold;font-size:150%;text-align:left;margin-left:50px;""",0
%% Scrap,0
Alternative approaches to finding the text/background  region,0
Using findCountours(),0
imagePil = Image.fromarray(analysisImage); imagePil,0
"analysisImage = cv2.erode(analysisImage, None, iterations=3)",0
"analysisImage = cv2.dilate(analysisImage, None, iterations=3)",0
"analysisImage = cv2.threshold(analysisImage, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]",0
Find object with the biggest bounding box,0
Using connectedComponents(),0
analysisImage = image,0
print('Found {} components'.format(nb_components)),0
We just want the *background* image,0
,0
"Takes a .json file with MD results for an individual video, and converts to a .csv that includes",0
frame times.  This is very bespoke to animal detection and does not include other classes.,0
,0
%% Imports and constants,0
Only necessary if you want to extract the sample rate from the video,0
%% Extract the sample rate if necessary,0
%% Load results,0
%% Convert to .csv,0
i_image = 0; im = results['images'][i_image],0
,0
umn-pr-analysis.py,0
,0
Precision/recall analysis for UMN data,0
,0
%% Imports and constants,0
results_file = results_file_filtered,0
"For two deployments, we're only processing imagse in the ""detections"" subfolder",0
String to remove from MegaDetector results,0
%% Enumerate deployment folders,0
%% Load MD results,0
im = md_results['images'][0],0
%% Load ground truth,0
i_row = 0; row = ground_truth_df.iloc[i_row],0
%% Create relative paths for ground truth data,0
"Some deployment folders have no subfolders, e.g. this is a valid file name:",0
,0
M00/01010132.JPG,0
,0
"But some deployment folders have subfolders, e.g. this is also a valid file name:",0
,0
N17/100EK113/07160020.JPG,0
,0
"So we can't find files by just concatenating folder and file names, we have to enumerate and explicitly",0
"map what will appear in the ground truth as ""folder/filename"" to complete relative paths.",0
deployment_name = list(deployment_folders)[0],0
Enumerate all files in this folder,0
"print('Enumerated {} files for deployment {}'.format(len(files),deployment_name))",0
filename = files[100],0
...for each file in this deployment,0
...for each deployment,0
%% Map relative paths to MD results,0
%% Add relative paths to our ground truth table,0
i_row = 0; row = ground_truth_df.iloc[i_row],0
"row['filename'] looks like, e.g. A01/01080001.JPG.  This is not actually a path, it's",0
"just the deployment ID and the image name, separated by a slash.",0
Find the relative path for this image,0
Make sure we have MegaDetector results for this file,0
Make sure this image file exists,0
...for each row in the ground truth table,0
%% Take everything out of Pandas,0
%% Some additional error-checking of the ground truth,0
An early version of the data required consistency between common_name and is_blank,0
%% Combine MD and ground truth results,0
d = ground_truth_dicts[0],0
Find the maximum confidence for each category,0
...for each detection,0
...for each image,0
%% Precision/recall analysis,0
...for each image,0
Thresholds go up throughout precisions/recalls/thresholds; find the last,0
value where recall is at or above target.  That's our precision @ target recall.,0
Flatten the confusion matrix,0
Write precision/recall plot to .png file in output directory,0
pr_figure_relative_filename = 'prec_recall.png',0
"pr_figure_filename = os.path.join(output_dir, pr_figure_relative_filename)",0
plt.show(block=False),0
"precision_recall_analysis(human_gt_labels,human_prediction_probs,'humans')",0
%% Find and manually review all images of humans,0
%%,0
"...if this image is annotated as ""human""",0
...for each image,0
%% Find and manually review all MegaDetector animal misses,0
%%,0
im = merged_images[0],0
GT says this is not an animal,0
GT says this is an animal,0
%% Convert .json to .csv,0
%%,0
,0
kga-pr-analysis.py,0
,0
Precision/recall analysis for KGA data,0
,0
%% Imports and constants,0
%% Load and filter MD results,0
%% Load and filter ground truth,0
%% Map images to image-level results,0
%% Map sequence IDs to images and annotations to images,0
Verify consistency of annotation within a sequence,0
TODO,1
%% Find max confidence values for each category for each sequence,0
seq_id = list(sequence_id_to_images.keys())[1000],0
im = images_this_sequence[0],0
det = md_result['detections'][],0
...for each detection,0
...for each image in this sequence,0
...for each sequence,0
%% Prepare for precision/recall analysis,0
seq_id = list(sequence_id_to_images.keys())[1000],0
cat_id = list(category_ids_this_sequence)[0],0
...for each category in this sequence,0
...for each sequence,0
%% Precision/recall analysis,0
"Confirm that thresholds are increasing, recall is decreasing",0
This is not necessarily true,0
assert np.all(precisions[:-1] <= precisions[1:]),0
Thresholds go up throughout precisions/recalls/thresholds; find the max,0
value where recall is at or above target.  That's our precision @ target recall.,0
"This is very slightly optimistic in its handling of non-monotonic recall curves,",0
but is an easy scheme to deal with.,0
Flatten the confusion matrix,0
Write precision/recall plot to .png file in output directory,0
pr_figure_relative_filename = 'prec_recall.png',0
"pr_figure_filename = os.path.join(output_dir, pr_figure_relative_filename)",0
plt.show(block=False),0
"precision_recall_analysis(human_gt_labels,human_prediction_probs,'humans')",0
%% Scrap,0
%% Find and manually review all sequence-level MegaDetector animal misses,0
input_base = '/media/user/lila-01/lila/snapshot-safari/KGA/KGA_public',0
i_sequence = 0; seq_id = pr_sequence_ids[i_sequence],0
i_seq = 0; seq_id = false_negative_sequences[i_seq],0
"sequence_folder = os.path.join(sequence_preview_dir,'seq_{}'.format(str(i_seq).zfill(3)))",0
fn = image_files[0],0
"print('Copying {} to {}'.format(input_path,output_path))",0
...for each file in this sequence.,0
...for each sequence,0
%% Image-level postprocessing,0
parse arguments,0
check if a GPU is available,0
load a pretrained embedding model,0
setup experiment,0
load the embedding model,0
setup the target dataset,0
setup finetuning criterion,0
setup an active learning environment,0
create a classifier,0
the main active learning loop,0
Active Learning,0
finetune the embedding model and load new embedding values,0
gather labeled pool and train the classifier,0
save a snapshot,0
Load a checkpoint if necessary,0
setup the training dataset and the validation dataset,0
setup data loaders,0
check if a GPU is available,0
create a model,0
setup loss criterion,0
define optimizer,0
load a checkpoint if provided,0
setup a deep learning engine and start running,0
train the model,0
train for one epoch,0
evaluate on validation set,0
save a checkpoint,0
open the file,0
read it,0
do the substitution,0
matplotlib.use('Agg'),0
from UIComponents.DBObjects import *,0
Initialize Database,0
# database connection credentials,0
# try to connect as USER to database DB_NAME through peewee,0
Load the saved embedding model,0
dataset_query = Detection.select().limit(5),0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})",0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getalllabels()) , dataset.getallpaths(), {})",0
Random examples to start,0
"random_ids = np.random.choice(dataset.current_set, 1000, replace=False).tolist()",0
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)",0
print(random_ids),0
Move Records,0
"moveRecords(dataset, DetectionKind.ModelDetection.value, DetectionKind.UserDetection.value, random_ids)",0
#print([len(x) for x in dataset.set_indices]),0
# Finetune the embedding model,0
#dataset.set_kind(DetectionKind.UserDetection.value),0
#dataset.train(),0
"#train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)",0
"#finetune_embedding(model, checkpoint['loss_type'], dataset, 32, 4, 100)",0
#save_checkpoint({,0
"#        'arch': model.arch,",0
"#        'state_dict': model.state_dict(),",0
"#        'optimizer' : optimizer.state_dict(),",0
"#        'loss_type' : loss_type,",0
"#        }, False, ""%s%s_%s_%04d.tar""%('finetuned', loss_type, model.arch, len(dataset.set_indices[DetectionKind.UserDetection.value])))",0
Get indices of samples to get user to label,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
"kwargs[""already_selected""].extend(indices)",0
Train on samples that have been labeled so far,0
Test on the samples that have not been labeled,0
"'optimizer' : optimizer.state_dict(),",0
num_classes= len(train_dataset.getClassesInfo()[0]),0
"print(""Num Classes= ""+str(num_classes))",0
define loss function (criterion) and optimizer,0
"optimizer = torch.optim.SGD(params, momentum = 0.9, lr = args.lr, weight_decay = args.weight_decay)",0
optimizer.load_state_dict(checkpoint['optimizer']),0
train for one epoch,0
"adjust_lr(optimizer,epoch)",0
if epoch % 1 == 0 and epoch > 0:,0
"a, b, c = e.predict(train_embd_loader, load_info = True, dim = args.feat_dim)",0
"plot_embedding(reduce_dimensionality(a), b, c, {})",0
evaluate on validation set,0
--------some stuff needed to get AJAX to work with bottle?--------#,0
-------------------------------------------------------------------------------- #,0
PREPARE TO QUEUE IMAGES FOR LABELING,0
-------------------------------------------------------------------------------- #,0
# Connect as USER to database DB_NAME through peewee and initialize database proxy,0
# Load embedding model,0
---------------------------------------------------------------------- #,0
CREATE QUEUE OF IMAGES TO LABEL,0
---------------------------------------------------------------------- #,0
Use classifier to generate predictions,0
# Update model predicted class in PostgreSQL database,0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?,0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
-------------------------------------------------------------------------------- #,0
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI,0
-------------------------------------------------------------------------------- #,0
"# static routes (to serve CSS, etc.)",0
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps",0
"return bottle.static_file(filename, root='../../../../../../../../../../../.')",0
# dynamic routes,0
if data['display_grayscale']:,0
indices_to_exclude.update(set(color_indices)),0
elif not data['display_grayscale']:,0
indices_to_exclude.update(set(grayscale_indices)),0
data['display_images'] = {},0
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices],0
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices],0
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices],0
data['display_images']['detection_categories'] = [],0
for i in indices:,0
if str(dataset.samples[i][1]) == 'None':,0
data['display_images']['detection_categories'].append('None'),0
else:,0
existing_category_entries = {cat.id: cat.name for cat in Category.select()},0
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()",0
data['display_images']['detection_categories'].append(cat_name),0
Use image ids in images_to_label to get the corresponding dataset indices,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
print(set(dataset.set_indices[4]).update(set(indices_to_label))),0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Use image id images_to_label to get the corresponding dataset index,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?,0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Train on samples that have been labeled so far,0
dataset.set_kind(DetectionKind.UserDetection.value),0
print(y_train),0
Predict on the samples that have not been labeled,0
print(y_pred),0
Update model predicted class in PostgreSQL database,0
timer = time.time(),0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
print('Updating the database took %0.2f seconds'%(time.time() - timer)),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids)),0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
"once the classifier has been trained the first time, switch to AL sampling",0
# sourceMappingURL=bootstrap.bundle.min.js.map,0
"print(ap_distances.size(),an_distances.size())",0
losses = -(((-ap_distances)/128)+1+1e-16).log() - (((-(128-an_distances))/128)+1+1e-16).log(),0
import pdb,0
pdb.set_trace(),0
losses = ap_distances - an_distances + self.margin,0
print(losses.size()),0
from UIComponents.DBObjects import *,0
TODO: should this also change self.kind?,1
get the embedding representations for all samples (i.e. set current_set to all indices),0
"return PILImage.open(os.path.join(self.img_base,path)).convert('RGB')",0
"print(self.labels_set, self.n_classes)",0
from sklearn.manifold import TSNE,0
embedding= TSNE(n_components=2).fit_transform(X),0
embedding= PCA(n_components=2).fit_transform(X),0
return X,0
"print(dir(event), type(sc))",0
"print(label,bgcolor)",0
"patches.append(mpatches.Patch(color=indexcolors[i], label=label))",0
"plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,",0
"ncol=12, mode=""expand"", borderaxespad=0., handles=patches)",0
plt.legend(handles=patches),0
"plt.xlabel('Dim 1', fontsize=12)",0
"plt.ylabel('Dim 2', fontsize=12)",0
plt.grid(True),0
"plt.xlabel('Dim 1', fontsize=12)",0
"plt.ylabel('Dim 2', fontsize=12)",0
plt.grid(True),0
print(thumb),0
"img.thumbnail((16, 12), PILImage.ANTIALIAS)",0
plt.show(),0
Add all negatives for all positive pairs,0
print(triplets.shape[0]),0
compute output,0
measure accuracy,0
compute loss on this batch,0
"train on a batch, record loss, and measure accuracy (if calc_accuracy)",0
compute output,0
measure accuracy and record loss,0
switch to evaluate mode,0
compute output,0
switch to evaluate mode,0
"self.fc13 = nn.Linear(128, 64)",0
self.bn2 = nn.BatchNorm1d(64),0
x = F.relu(self.fc12(x)),0
x = F.relu(self.bn1(self.fc13(x))),0
x = F.relu(self.fc13(x)),0
"x = F.dropout(x, training=self.training)",0
save features last FC layer,0
x = F.relu(x),0
save features last FC layer,0
import matplotlib.pyplot as plt,0
plt.switch_backend('agg'),0
class EmbeddingNet(nn.Module):,0
"def __init__(self, architecture, feat_dim, use_pretrained=False):",0
"super(EmbeddingNet, self).__init__()",0
self.feat_dim= feat_dim,0
self.inner_model = models.__dict__[architecture](pretrained=use_pretrained),0
if architecture.startswith('resnet'):,0
in_feats= self.inner_model.fc.in_features,0
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)",0
elif architecture.startswith('inception'):,0
in_feats= self.inner_model.fc.in_features,0
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)",0
if architecture.startswith('densenet'):,0
in_feats= self.inner_model.classifier.in_features,0
"self.inner_model.classifier = nn.Linear(in_feats, feat_dim)",0
if architecture.startswith('vgg'):,0
in_feats= self.inner_model.classifier._modules['6'].in_features,0
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)",0
if architecture.startswith('alexnet'):,0
in_feats= self.inner_model.classifier._modules['6'].in_features,0
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)",0
"def forward(self, x):",0
return self.inner_model.forward(x),0
class NormalizedEmbeddingNet(EmbeddingNet):,0
"def __init__(self, architecture, feat_dim, use_pretrained=False):",0
"EmbeddingNet.__init__(self, architecture, feat_dim, use_pretrained = use_pretrained)",0
"def forward(self, x):",0
embedding =  F.normalize(self.inner_model.forward(x))*10.0,0
"return embedding, embedding",0
"def get_random_images(num, image_dir, test_transforms):",0
"data = datasets.ImageFolder(image_dir, transform=test_transforms) # slight abuse; this expects subfolders corresponding to classes but we have no classes here",0
indices = list(range(len(data))),0
np.random.shuffle(indices),0
idx = indices[:num],0
from torch.utils.data.sampler import SubsetRandomSampler,0
sampler = SubsetRandomSampler(idx),0
"loader = torch.utils.data.DataLoader(data,",0
"sampler=sampler, batch_size=num)",0
dataiter = iter(loader),0
"images, labels = dataiter.next()",0
"return images, labels",0
"def predict_image(image, model, test_transforms):",0
"device = torch.device(""cuda"" if torch.cuda.is_available()",0
"else ""cpu"")",0
image_tensor = test_transforms(image).float(),0
image_tensor = image_tensor.unsqueeze_(0),0
input = Variable(image_tensor),0
input = input.to(device),0
output = model(input)[0],0
return output.data.cpu().numpy(),0
Connect to database and initialize db_proxy,0
# database connection credentials,0
# load the dataset,0
Load the saved embedding model from the checkpoint,0
# update the dataset embedding,0
# Create a folder for saving embedding visualizations with this model checkpoint,0
model_emb_dirname = os.path.basename(args.base_model).split('.')[0],0
"os.makedirs(model_emb_dirname, exist_ok=True)",0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')",0
dataset.embedding_mode(),0
"assert 2==3, 'break'",0
datasetindices = list(range(len(dataset))),0
np.random.shuffle(datasetindices),0
random_indices = datasetindices[:args.num],0
print(random_indices),0
"selected_sample_features = np.array([]).reshape(0, 256)",0
selected_sample_labels = [],0
for idx in random_indices:,0
"selected_sample_features = np.vstack([selected_sample_features, X_train[idx]])",0
selected_sample_labels.append(y_train[idx]),0
img_path = imagepaths[idx].split('.JPG')[0],0
image = dataset.loader(img_path),0
selected_sample_images.append(image),0
# TRY NEAREST NEIGHBORS WALK THROUGH EMBEDDING,0
nbrs = NearestNeighbors(n_neighbors=args.num).fit(selected_sample_features),0
"distances, indices = nbrs.kneighbors(selected_sample_features)",0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')",0
"idx_w_closest_nbr = np.where(distances[:,1] == min(distances[:,1]))[0][0]",0
order = [idx_w_closest_nbr],0
for ii in range(len(distances)):,0
"distances[ii, 0] = np.inf",0
while len(order)<args.num:,0
curr_idx = order[-1],0
curr_neighbors = indices[curr_idx],0
curr_dists = list(distances[curr_idx]),0
# print(min(curr_dists)),0
next_closest_pos = curr_dists.index(min(curr_dists)),0
next_closest = curr_neighbors[next_closest_pos],0
order.append(next_closest),0
# make sure you can't revisit past nodes,0
for vi in order:,0
vi_pos = list(indices[next_closest]).index(vi),0
"distances[next_closest, vi_pos] = np.inf",0
for ii in range(len(order)):,0
imgidx = order[ii],0
image = selected_sample_images[imgidx],0
"image.save(model_emb_dirname+""/img""+str(ii)+""_""+str(selected_sample_labels[imgidx])+"".png"")",0
# Specify the transformations on the input images before inference,0
"# test_transforms = transforms.Compose([transforms.Resize([224, 224]), transforms.ToTensor()])",0
"test_transforms = transforms.Compose([transforms.Resize([256, 256]), transforms.RandomCrop([224, 224]), transforms.RandomHorizontalFlip(), transforms.ColorJitter(), transforms.ToTensor(), transforms.Normalize([0.407328, 0.407328, 0.407328], [0.118641, 0.118641, 0.118641])])",0
"images, labels = get_random_images(num, image_dir, test_transforms)",0
"all_features = np.array([]).reshape(0, 256)",0
for ii in range(len(images)):,0
image = to_pil(images[ii]),0
"features = predict_image(image, model, test_transforms)",0
"all_features = np.vstack([all_features, features])",0
# for ii in range(len(images)):,0
#     image = to_pil(images[ii]),0
"#     image.save(""img""+str(ii)+"".png"")",0
# TRY CLUSTERING,0
kmeans1 = KMeans(n_clusters=5).fit(StandardScaler().fit_transform(all_features)),0
print(kmeans1.labels_),0
for ii in range(len(images)):,0
image = to_pil(images[ii]),0
"filename = str(kmeans1.labels_[ii])+""/img""+str(ii)+"".png""",0
if not os.path.exists(os.path.dirname(filename)):,0
os.makedirs(os.path.dirname(filename)),0
image.save(filename),0
class SaveFeatures():,0
"def __init__(self, module):",0
self.hook = module.register_forward_hook(self.hook_fn),0
"def hook_fn(self, module, input, output):",0
"self.features = torch.tensor(output, requires_grad=True).cuda()",0
def close(self):,0
self.hook.remove(),0
Load the saved embedding model from the checkpoint,0
"Get a sample from the database, with eval transforms applied, etc.",0
Connect to database and sample a dataset,0
output = model.forward(sample_image.unsqueeze(0)),0
print(output),0
with torch.no_grad():,0
sample_image_input = sample_image.cuda(non_blocking=True),0
"_, output = model(sample_image_input) # compute output",0
print(output),0
sample_image = PILImage.open(sample_image_path).convert('RGB'),0
"sample_image = transforms.Compose([Resize([256, 256]), CenterCrop(([[224,224]])), ToTensor(), Normalize([0.369875, 0.388726, 0.347536], [0.136821, 0.143952, 0.145229])])(sample_image)",0
print(list(model_inner_resnet.children())),0
print(model_inner_resnet.fc),0
print(model_inner_resnet.fc0),0
# print(model_inner_resnet.layer4[0].conv2),0
# print(type(model)),0
# print(len(list(model_inner_resnet.children()))),0
# print(list(model.children())),0
# print(list(list(model.children())[0].children())),0
"img = np.uint8(np.random.uniform(150, 180, (56, 56, 3)))/255",0
"img_tensor = torch.unsqueeze(torch.from_numpy(img), 0)",0
full_out = model_inner_resnet.forward(img_tensor),0
print(full_out),0
model(img_tensor),0
activations = SaveFeatures(model_inner_resnet.layer4[0].conv2),0
print(activations.features),0
print(type(activations.features)),0
activations.close(),0
Connect to database and sample a dataset,0
Load the saved embedding model from the checkpoint,0
Update the dataset embedding,0
Get a random query image,0
# # # IMAGES IN THE SAME SEQUENCE # # # #,0
"assert 2==3, 'break'",0
# # # CLOSEST IN (EMBEDDING) FEATURE SPACE # # # #,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Variables for the hierarchical cluster,0
Variables for the AL algorithm,0
Data variables,0
"connectivity = kneighbors_graph(self.transformed_X,max_features)",0
Fit cluster and update cluster variables,0
The sklearn hierarchical clustering algo numbers leaves which correspond,0
to actual datapoints 0 to n_points - 1 and all internal nodes have,0
ids greater than n_points - 1 with the root having the highest node id,0
"If no labels have been observed, simply return uniform distribution",0
"If no observations, return worst possible upper lower bounds",0
Loop through generations from bottom to top,0
Update admissible labels for node,0
Calculate score,0
Determine if node should be split,0
Make sure label set for node so that we can flow to children,0
if necessary,0
Only split if all ancestors are admissible nodes,0
This is part  of definition of admissible pruning,0
Check that pruning covers all leave nodes,0
Fill in labels,0
Observe labels for previously recommended batches,0
TODO(lishal): implement multiple selection methods,1
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
A list of initialized samplers is allowed as an input because,0
"for AL_methods that search over different mixtures, may want mixtures to",0
have shared AL_methods so that initialization is only performed once for,0
computation intensive methods like HierarchicalClusteringAL and,0
states are shared between mixtures.,0
"If initialized samplers are not provided, initialize them ourselves.",0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copy these objects to make sure they are not modified while simulating,0
trajectories as they are used later by the main run_experiment script.,0
Assumes that model has already by fit using all labeled data so,0
the probabilities can be used immediately to hallucinate labels,0
All models need to have predict_proba method,0
Hallucinate labels for selected datapoints to be label,0
using class probabilities from model,0
"Not saving already_selected here, if saving then should sort",0
only for the input to fit but preserve ordering of indices in,0
already_selected,0
Useful to know how accuracy compares for model trained on hallucinated,0
labels vs trained on true labels.  But can remove this train to speed,0
up simulations.  Won't speed up significantly since many more models,0
are being trained inside the loop above.,0
Save trajectory for reference,0
Delete created copies,0
THE INPUTS CANNOT BE MODIFIED SO WE MAKE COPIES FOR THE CHECK LATER,0
Should check model but kernel_svm does not have coef_ so need better,0
handling here,0
Make sure that model object fed in did not change during simulations,0
Return indices based on return type specified,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Set gamma for gaussian kernel to be equal to 1/n_features,0
kneighbors graph is constructed using k=10,0
"Make connectivity matrix symmetric, if a point is a k nearest neighbor of",0
"another point, make it vice versa",0
Graph edges are weighted by applying gaussian kernel to manhattan dist.,0
"By default, gamma for rbf kernel is equal to 1/n_features but may",0
get better results if gamma is tuned.,0
Define graph density for an observation to be sum of weights for all,0
edges to the node representing the datapoint.  Normalize sum weights,0
by total number of neighbors.,0
"If a neighbor has already been sampled, reduce the graph density",0
for its direct neighbors to promote diversity.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Update observed reward and arm probabilities,0
Sample an arm,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
y only used for determining how many clusters there should be,0
probably not practical to assume we know # of classes before hand,0
should also probably scale with dimensionality of data,0
Probably okay to always use MiniBatchKMeans,0
Should standardize data before clustering,0
Can cluster on standardized data but train on raw features if desired,0
TODO(lishal): have MarginSampler and this share margin function,1
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Probably okay to always use MiniBatchKMeans,0
Should standardize data before clustering,0
Can cluster on standardized data but train on raw features if desired,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Update min_distances for all examples given new cluster center.,0
Assumes that the transform function takes in original data and not,0
flattened data.,0
Initialize centers with a randomly selected datapoint,0
New examples should not be in already selected since those points,0
should have min_distance of zero to a cluster center.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
This is uniform given the remaining pool but biased wrt the entire pool.,0
sample = [i for i in range(self.X.shape[0]) if i not in already_selected],0
return sample[0:N],0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Fields for hierarchical clustering AL,0
Setting parent and storing nodes in dict for fast access,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Connect to database DB_NAME as USER and initialize tables,0
Populate Info table,0
Populate Category table,0
"# For now, we have a predefined list of species we expect to see in the camera trap database (e.g. maybe from a quick look through the images)",0
# TODO: allow user to update the class list through the labeling tool UI as they see different species,1
Populate Image and Detection tables,0
Image entry data,0
Detection entry data,0
Connect to database DB_NAME as USER and initialize tables,0
Populate Info table,0
Populate Category table,0
# Create category records from species present in the COCO camera trap classification dataset (COCO classes minus any excluded classes),0
# TODO: allow user to update the class list through the labeling tool UI as they see different species,1
Populate Image and Detection tables,0
"with open(os.path.join(args.crop_dir,'crops.json'), 'r') as infile:",0
crops_json = json.load(infile),0
counter = 0,0
timer = time.time(),0
num_detections = len(crops_json),0
for detectionid in crops_json:,0
counter += 1,0
detection_data = crops_json[detectionid],0
# Image entry data,0
existing_image_entries = Image.select().where((Image.file_name == detection_data['file_name'])),0
try:,0
existing_image_entry = existing_image_entries.get(),0
except:,0
"image_entry = Image.create(id=detectionid, file_name=detection_data['file_name'], width=detection_data['width'], height=detection_data['height'], grayscale=detection_data['grayscale'],",0
"source_file_name=detection_data['source_file_name'], relative_size=detection_data['relative_size'],",0
"seq_id=detection_data['seq_id'], seq_num_frames=detection_data['seq_num_frames'], frame_num=detection_data['frame_num'])",0
image_entry.save(),0
# Detection entry data,0
"detection_entry = Detection.create(id=detectionid, image=detectionid, bbox_confidence=detection_data['bbox_confidence'],",0
"bbox_X1=detection_data['bbox_X1'], bbox_Y1=detection_data['bbox_Y1'], bbox_X2=detection_data['bbox_X2'], bbox_Y2=detection_data['bbox_Y2'],",0
kind=DetectionKind.ModelDetection.value),0
detection_entry.save(),0
if counter%100 == 0:,0
"print('Updated database with Image and Detection table entries for %d out of %d crops in %0.2f seconds'%(counter, num_detections, time.time() - timer))",0
# data related to original image,0
Get class names from .txt list,0
Initialize Oracle table,0
Map filenames to classes (NOTE: we assume a single image does not contain more than one class),0
"For each detection, use source image path to get class",0
TODO update: Assumes that crops have already,1
been generated for the images using make_active_learning_classification_dataset.py. The created DB contains tables:,0
- info: information about the dataset,0
- image: images present in the dataset,0
- detections: crops of images with detections with confidence greater than a specified threshold,0
Initialize Database,0
# database connection credentials,0
HOST = 'localhost',0
PORT = 5432,0
"# first, make sure the (user, password) has been created",0
"# sudo -u postgres psql -c ""CREATE USER <db_user> WITH PASSWORD <db_password>;""",0
"# sudo -u postgres psql -c ""CREATE DATABASE <db_name> WITH OWNER <db_user> CONNECTION LIMIT -1;""",0
"# sudo -u postgres psql -c ""GRANT CONNECT ON DATABASE <db_name> TO <db_user>;""",0
"# sudo -u postgres psql -d <db_name> -c ""CREATE EXTENSION IF NOT EXISTS \""uuid-ossp\"";""",0
# Try to connect as USER to database DB_NAME through peewee,0
Populate Tables,0
# create Info table,0
# get class names for Category table,0
Faster anD available in Python 3.5 and above,0
# iterate through images in each class folder,0
killing this process after over 38 hours adding over 500k white-tailed deer crops from emammal,0
resuming for remaining classes,0
# get cropped image data for Image table,0
"if mean of each channel is about the same, image is likely grayscale",0
# still have no info on these:,0
seq_id = CharField(null= True)                # sequence identifier for the original image,0
seq_num_frames = IntegerField(null = True)    # number of frames in sequence,0
frame_num = IntegerField(null = True)         # which frame number in sequence,0
location = CharField(null = True)             # location of camera trap,0
datetime = DateTimeField(null = True),0
# store info about the detection corresponding to this image,0
# store info about the true labels for the detection,0
#  - for pretrain dataset this is the same as the detection_category if the detection categories,0
print(classes),0
Connect to database and sample a dataset,0
Load the saved embedding model from the checkpoint,0
Update the dataset embedding,0
save the images,0
save the features,0
"with open(os.path.join(args.output_dir, 'lastlayer_features.mat'), 'wb') as f:",0
"pickle.dump(sample_features, f)",0
"with open(os.path.join(args.output_dir, 'labels.mat'), 'wb') as f:",0
"pickle.dump(sample_labels, f)",0
"parser.add_argument('--db_name', default='missouricameratraps', type=str, help='Name of the training (target) data Postgres DB.')",0
"parser.add_argument('--db_user', default='user', type=str, help='Name of the user accessing the Postgres DB.')",0
"parser.add_argument('--db_password', default='password', type=str, help='Password of the user accessing the Postgres DB.')",0
"parser.add_argument('--base_model', type=str, help='Path to latest embedding model checkpoint.')",0
"parser.add_argument('--output_dir', type=str, help='Output directory for subset of crops')",0
Add json entry for this crop,0
Copy file for this crop to subset dataset crop dir,0
Copy file for its full-size source image to subset dataset image dir,0
Write crops.json to subset dataset crop dir,0
store info about the crops produced in a JSON file,0
------------------------------------------------------------------------------------------------------------#,0
COMMENT OUT IF NOT USING A SPECIFIC PROJECT WITHIN ROBERT LONG EMAMMAL DATASET,0
------------------------------------------------------------------------------------------------------------#,0
get some information about the source image,0
------------------------------------------------------------------------------------------------------------#,0
NOTE: EDIT THIS SECTION BASED ON DATASET SOURCE,0
get info about sequence the source image belongs to from path and directory,0
# missouricameratraps:,0
imgframenum = int(os.path.basename(imgfile).split('.JPG')[0].split('_')[-1]),0
imgseqid = int(os.path.split(os.path.dirname(imgfile))[-1]),0
"imgseqnumframes = len([name for name in os.listdir(os.path.dirname(imgfile)) if os.path.isfile(os.path.join(os.path.dirname(imgfile), name))])",0
# emammal:,0
------------------------------------------------------------------------------------------------------------#,0
utility function,0
compute output,0
measure accuracy and record loss,0
switch to train mode,0
measure accuracy and record loss,0
compute output,0
measure accuracy and record loss,0
switch to evaluate mode,0
compute output,0
switch to evaluate mode,0
compute output,0
switch to evaluate mode,0
"print(self.labels_set, self.n_classes)",0
Add all negatives for all positive pairs,0
print(triplets.shape[0]),0
constructor,0
update embedding values after a finetuning,0
select either the default or active pools,0
gather test set,0
gather train set,0
finetune the embedding model over the labeled pool,0
a utility function for saving the snapshot,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Variables for the hierarchical cluster,0
Variables for the AL algorithm,0
Data variables,0
"connectivity = kneighbors_graph(self.transformed_X,max_features)",0
Fit cluster and update cluster variables,0
The sklearn hierarchical clustering algo numbers leaves which correspond,0
to actual datapoints 0 to n_points - 1 and all internal nodes have,0
ids greater than n_points - 1 with the root having the highest node id,0
"If no labels have been observed, simply return uniform distribution",0
"If no observations, return worst possible upper lower bounds",0
Loop through generations from bottom to top,0
Update admissible labels for node,0
Calculate score,0
Determine if node should be split,0
Make sure label set for node so that we can flow to children,0
if necessary,0
Only split if all ancestors are admissible nodes,0
This is part  of definition of admissible pruning,0
Check that pruning covers all leave nodes,0
Fill in labels,0
Observe labels for previously recommended batches,0
TODO(lishal): implement multiple selection methods,1
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
A list of initialized samplers is allowed as an input because,0
"for AL_methods that search over different mixtures, may want mixtures to",0
have shared AL_methods so that initialization is only performed once for,0
computation intensive methods like HierarchicalClusteringAL and,0
states are shared between mixtures.,0
"If initialized samplers are not provided, initialize them ourselves.",0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copy these objects to make sure they are not modified while simulating,0
trajectories as they are used later by the main run_experiment script.,0
Assumes that model has already by fit using all labeled data so,0
the probabilities can be used immediately to hallucinate labels,0
All models need to have predict_proba method,0
Hallucinate labels for selected datapoints to be label,0
using class probabilities from model,0
"Not saving already_selected here, if saving then should sort",0
only for the input to fit but preserve ordering of indices in,0
already_selected,0
Useful to know how accuracy compares for model trained on hallucinated,0
labels vs trained on true labels.  But can remove this train to speed,0
up simulations.  Won't speed up significantly since many more models,0
are being trained inside the loop above.,0
Save trajectory for reference,0
Delete created copies,0
THE INPUTS CANNOT BE MODIFIED SO WE MAKE COPIES FOR THE CHECK LATER,0
Should check model but kernel_svm does not have coef_ so need better,0
handling here,0
Make sure that model object fed in did not change during simulations,0
Return indices based on return type specified,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Set gamma for gaussian kernel to be equal to 1/n_features,0
kneighbors graph is constructed using k=10,0
"Make connectivity matrix symmetric, if a point is a k nearest neighbor of",0
"another point, make it vice versa",0
Graph edges are weighted by applying gaussian kernel to manhattan dist.,0
"By default, gamma for rbf kernel is equal to 1/n_features but may",0
get better results if gamma is tuned.,0
Define graph density for an observation to be sum of weights for all,0
edges to the node representing the datapoint.  Normalize sum weights,0
by total number of neighbors.,0
"If a neighbor has already been sampled, reduce the graph density",0
for its direct neighbors to promote diversity.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Update observed reward and arm probabilities,0
Sample an arm,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
y only used for determining how many clusters there should be,0
probably not practical to assume we know # of classes before hand,0
should also probably scale with dimensionality of data,0
Probably okay to always use MiniBatchKMeans,0
Should standardize data before clustering,0
Can cluster on standardized data but train on raw features if desired,0
TODO(lishal): have MarginSampler and this share margin function,1
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Probably okay to always use MiniBatchKMeans,0
Should standardize data before clustering,0
Can cluster on standardized data but train on raw features if desired,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Update min_distances for all examples given new cluster center.,0
Assumes that the transform function takes in original data and not,0
flattened data.,0
Initialize centers with a randomly selected datapoint,0
New examples should not be in already selected since those points,0
should have min_distance of zero to a cluster center.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
This is uniform given the remaining pool but biased wrt the entire pool.,0
sample = [i for i in range(self.X.shape[0]) if i not in already_selected],0
return sample[0:N],0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Fields for hierarchical clustering AL,0
Setting parent and storing nodes in dict for fast access,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Faster and available in Python 3.5 and above,0
transform_list.append(CenterCrop((processed_size))),0
transform_list.append(Lambda(lambda X: normalize(X))),0
"print(self.labels_set, self.n_classes)",0
"from PyQt5 import QtCore, QtWidgets,QtGui",0
from collections import deque,0
from peewee import *,0
from UIComponents.Tag import Tag,0
policy.setHeightForWidth(True),0
"print(self.tab1.parentWidget(),self)",0
self.tab4.add.clicked.connect(self.addSpecies),0
self.tab4.update.clicked.connect(self.updateSpecies),0
checkpoint= load_checkpoint('../merge/triplet_model_0054.tar'),0
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))",0
"embedding_net = EmbeddingNet(checkpoint['arch'], checkpoint['feat_dim'])",0
if checkpoint['loss_type'].lower()=='center':,0
"model = torch.nn.DataParallel(ClassificationNet(embedding_net, n_classes=14)).cuda()",0
else:,0
model= torch.nn.DataParallel(embedding_net).cuda(),0
model.load_state_dict(checkpoint['state_dict']),0
self.parentWidget().progressBar.setMaximum(len(run_dataset)//2048),0
"e=Engine(model,None,None, verbose=True,progressBar= self.parentWidget().progressBar)",0
label = [ x[1] for x in run_dataset.samples],0
"print(indices,selected_set)",0
print(query.sql()),0
src.delete().where(src.image_id<<rList)),0
"det= UserDetection.create(category_id=0, id=str(index+label[1][2]),image_id=final[0], bbox_X=label[1][0], bbox_Y=label[1][1], bbox_W=label[1][2], bbox_H=label[1][3])",0
for x in self.tab1.grid.tags:,0
x.delete_instance(),0
db.create_tables([Detection]),0
This is simply to show the bar,0
"p = Process(target=ex.active, args=())",0
p.start(),0
p.join(),0
ex.active(),0
ex.centralWidget().setCurrentIndex(1),0
main(),0
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))",0
print(row),0
"unq_id= ""crops_""+str(uuid.uuid1())",0
"print(line,imageWidth,imageHeight)",0
"print(""%s,%s,%s,%0.6f,%0.6f,%0.6f,%0.6f,%0.6f""%(line[0], line[1],line[2],float(line[3]),topRel,leftRel,bottomRel,rightRel))",0
if not os.path.exists(dest):,0
os.mkdir(dest),0
raise,0
out.close(),0
"print length,(i-1)*length,i*length",0
matplotlib.use('Agg'),0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
"conf.reset()""""""",0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
conf.reset(),0
for clf in classifiers:,0
"clf.fit(X_train, y_train)",0
preds= clf.predict_proba(X),0
uncertainty+= preds.max(axis=1),0
print(uncertainty[ind]),0
print(uncertainty[ind]),0
true_labels = y[unlabeled_indices],0
select up to 5 digit examples that the classifier is most uncertain about,0
remember best acc@1 and save checkpoint,0
"completeClassificationLoop(run_dataset, model,num_classes)",0
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)",0
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd),0
labels = db.labels_,0
"mapp=(find_probablemap(label,labels, K=args.K))",0
"print(""Clusters"")",0
"for i,x in enumerate(labels):",0
labels[i]= mapp[x],0
print(np.sum(labels == label)/labels.size),0
"print(""Confidence Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning(embd, label, idx)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(""Entropy Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning_entropy(embd, label, idx)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(idx,idx.shape)",0
for i in idx:,0
print(paths[i]),0
"print(""Silohette active learning"")",0
"idx= active_learning2(embd, 1000, args.num_clusters)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
embd= reduce_dimensionality(embd)#[0:10000]),0
labels= labels[0:10000],0
label= label[0:10000],0
paths= paths[0:10000],0
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])",0
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])",0
plt.show(),0
"np.save(args.name_prefix+""_embeddings.npy"",embd)",0
"np.save(args.name_prefix+""_labels.npy"",label)",0
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")",0
del output,0
define loss function (criterion) and optimizer,0
"optimizer = torch.optim.Adam(params, lr=args.lr, weight_decay= args.weight_decay)",0
optimizer.load_state_dict(checkpoint['optimizer']),0
train for one epoch,0
evaluate on validation set,0
matplotlib.use('Agg'),0
selected_set.add(rand_ind[i]),0
"embedding_net = EmbeddingNet('resnet50', 256, True)",0
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)",0
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)",0
print('Embedding Done'),0
sys.stdout.flush(),0
Random examples to start,0
"random_ids = noveltySamples(unlabeled_dataset.em, unlabeled_dataset.getIDs(), 1000)",0
print(random_ids),0
Move Records,0
Finetune the embedding model,0
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)",0
unlabeled_dataset.updateEmbedding(model),0
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})",0
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()",0
matplotlib.use('Agg'),0
"uncertainty= np.apply_along_axis(stats.entropy,1,probs) * (1 - probs.max(axis=1))",0
"copy(paths[srt[i]], ""active"")",0
"plot_together( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()), preds, base_ind, dataset.getpaths(), {})",0
"return np.random.choice(range(0,prob_list[0].shape[0]), 100, replace=False).tolist()",0
selected_set.add(rand_ind[i]),0
"query= Detection.update(kind = destKind.value).where(Detection.id.in_(rList), Detection.kind == srcKind.value)",0
query.execute(),0
"embedding_net = EmbeddingNet('resnet50', 256, True)",0
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)",0
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)",0
print('Embedding Done'),0
sys.stdout.flush(),0
"plot_embedding(dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})",0
Random examples to start,0
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)",0
print(random_ids),0
Move Records,0
Finetune the embedding model,0
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)",0
unlabeled_dataset.updateEmbedding(model),0
"print(dataset.em[dataset.current_set].shape, np.asarray(dataset.getlabels()).shape, len(dataset.getpaths()))",0
"plot_embedding( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})",0
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})",0
train_eval_classifier(),0
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()",0
"names = [""Linear SVM"", ""RBF SVM"", ""Random Forest"", ""Neural Net"", ""Naive Bayes""]",0
"classifiers = [SVC(kernel=""linear"", C=0.025, probability= True, class_weight='balanced'),",0
"SVC(gamma=2, C=1, probability= True, class_weight='balanced'),",0
"RandomForestClassifier(max_depth=None, n_estimators=100, class_weight='balanced'),",0
"MLPClassifier(alpha=1),",0
GaussianNB()],0
estimators= [],0
"for name, clf in zip(names, classifiers):",0
"estimators.append((name, clf))",0
"eclf1 = VotingClassifier(estimators= estimators, voting='hard')",0
"eclf2 = VotingClassifier(estimators= estimators, voting='soft')",0
"names.append(""ensemble hard"")",0
classifiers.append(eclf1),0
"names.append(""ensemble soft"")",0
classifiers.append(eclf2),0
dataset.image_mode(),0
dataset.updateEmbedding(model),0
y_pred= clf.predict(X_test),0
"print(confusion_matrix(y_test, y_pred))",0
paths= dataset.getpaths(),0
"for i, (yp, yt) in enumerate(zip(y_pred, y_test)):",0
if yp != yt:,0
"copy(paths[i],""mistakes"")",0
"print(yt, yp, paths[i],i)",0
"clf_output= clf_e.embedding(eval_loader, dim=48)",0
"self.train_transform = transforms.Compose([Resize(raw_size), CenterCrop((processed_size)), ToTensor(), Normalize(mean, std)])",0
"print(self.labels_set, self.n_classes)",0
"print(line,imageWidth,imageHeight)",0
"print length,(i-1)*length,i*length",0
#########################################################,0
## Configuration,0
#########################################################,0
## The actual code,0
Check arguments,0
Create output directories,0
Padding around the detected objects when cropping,0
1.3 for the cropping during test time and 1.3 for,0
the context that the CNN requires in the left-over,0
image,0
Load a (frozen) Tensorflow model into memory.,0
## Preparations: get all the output tensors,0
The following processing is only for single image,0
Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.,0
Follow the convention by adding back the batch dimension,0
For all images in the image directoryig,0
Load image,0
Run inference,0
"all outputs are float32 numpy arrays, so convert types as appropriate",0
Add detections to the collection,0
Get info about the image,0
Select detections with a confidence larger than DETECTION_CONFIDENCE,0
Get these boxes and convert normalized coordinates to pixel coordinates,0
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes",0
"However, we need to make sure that it box coordinates are still within the image",0
"For each detected bounding box with high confidence, we will",0
crop the image to the padded box and save it,0
generate a unique identifier for the detection,0
"bbox is the detected box, crop_box the padded / enlarged box",0
Add numbering to the original file name if there are multiple boxes,0
The absolute file path where we will store the image,0
"if COCO_OUTPUT_DIR is set, then we will only use the shape",0
of cropped_img in the following code. So instead of reading,0
cropped_img = np.array(Image.open(out_file)),0
we can speed everything up by reading only the size of the image,0
matplotlib.use('Agg'),0
conf= ConfusionMatrix(24),0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
"conf.reset()""""""",0
define loss function (criterion) and optimizer,0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
conf.reset(),0
for clf in classifiers:,0
"clf.fit(X_train, y_train)",0
preds= clf.predict_proba(X),0
uncertainty+= preds.max(axis=1),0
print(uncertainty[ind]),0
print(uncertainty[ind]),0
true_labels = y[unlabeled_indices],0
select up to 5 digit examples that the classifier is most uncertain about,0
"print(indices,selected_set)",0
print(query.sql()),0
remember best acc@1 and save checkpoint,0
"completeClassificationLoop(run_dataset, model,num_classes)",0
"embd, label, paths = extract_embeddings(run_loader, model)",0
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)",0
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd),0
labels = db.labels_,0
"mapp=(find_probablemap(label,labels, K=args.K))",0
"print(""Clusters"")",0
"for i,x in enumerate(labels):",0
labels[i]= mapp[x],0
print(np.sum(labels == label)/labels.size),0
"print(""Confidence Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning(embd, label, idx)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(""Entropy Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning_entropy(embd, label, idx)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"new_selected= selectSamples(embd,paths,3000)",0
"print(idx,idx.shape)",0
for i in idx:,0
print(paths[i]),0
"print(""Silohette active learning"")",0
"idx= active_learning2(embd, 1000, args.num_clusters)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(""Random"")",0
"idx = np.random.choice(np.arange(len(paths)), 1000, replace=False)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
embd= reduce_dimensionality(embd)#[0:10000]),0
labels= labels[0:10000],0
label= label[0:10000],0
paths= paths[0:10000],0
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])",0
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])",0
plt.show(),0
"np.save(args.name_prefix+""_embeddings.npy"",embd)",0
"np.save(args.name_prefix+""_labels.npy"",label)",0
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")",0
combo.setEnabled(not finalized),0
print(self.bbox),0
self.addWidget(self.child),0
Moving container with arrows,0
Left - Bottom,0
Right-Bottom,0
Left-Top,0
Right-Top,0
Left - Bottom,0
Right - Bottom,0
Left - Top,0
Right - Top,0
check cursor horizontal position,0
check cursor vertical position,0
self.resizeEvent=self.onResize,0
"print(""Parent"", parent, parent.width(), parent.height())",0
"self.setGeometry(0,0,410,307)",0
"print(w,h,""w,h"")",0
"print(""final"",tag.getFinal())",0
pass,0
"self.tags.append(TContainer(self,Category.get(-1),[0,0,0.1,0.1],True, Qt.red))",0
label= CharField(),0
fullname=str(self.model),0
"self.name= (fullname[fullname.find("":"")+2:fullname.find("">"")].strip()+'_set').lower()",0
"print(""Parent"", self.parentWidget().width(), self.parentWidget().height())",0
"print(self.model,self.name,query.sql())",0
self.tab4.speciesList.setModel(species),0
"self.tab4.speciesList.setRowHidden(len(species.stringList())-1, True)",0
self.speciesList.itemChanged.connect(self.itemChanged),0
#####,0
,0
video_utils.py,0
,0
"Utilities for splitting, rendering, and assembling videos.",0
,0
#####,0
"%% Constants, imports, environment",0
from ai4eutils,0
%% Path utilities,0
%% Function for rendering frames to video and vice-versa,0
http://tsaith.github.io/combine-images-into-a-video-with-python-3-and-opencv-3.html,0
Determine the width and height from the first image,0
Define the codec and create VideoWriter object,0
"If we're not over-writing, check whether all frame images already exist",0
"OpenCV seems to over-report the number of frames by 1 in some cases, or fails",0
"to read the last frame; either way, I'm allowing one missing frame.",0
"print(""Rendering video {}, couldn't find frame {}"".format(",0
"input_video_file,missing_frame_number))",0
...if we need to check whether to skip this video entirely,0
"for frame_number in tqdm(range(0,n_frames)):",0
print('Skipping frame {}'.format(frame_filename)),0
Recursively enumerate video files,0
Create the target output folder,0
Render frames,0
input_video_file = input_fn_absolute; output_folder = output_folder_video,0
For each video,0
,0
input_fn_relative = input_files_relative_paths[0],0
"process_detection_with_options = partial(process_detection, options=options)",0
zero-indexed,0
Load results,0
# Break into videos,0
im = images[0],0
# For each video...,0
video_name = list(video_to_frames.keys())[0],0
frame = frames[0],0
At most one detection for each category for the whole video,0
category_id = list(detection_categories.keys())[0],0
Find the nth-highest-confidence video to choose a confidence value,0
Prepare the output representation for this video,0
'max_detection_conf' is no longer included in output files by default,0
...for each video,0
Write the output file,0
%% Test driver,0
%% Constants,0
%% Split videos into frames,0
"%% List image files, break into folders",0
Find unique folders,0
fn = frame_files[0],0
%% Load detector output,0
%% Render detector frames,0
folder = list(folders)[0],0
d = detection_results_this_folder[0],0
...for each file in this folder,0
...for each folder,0
%% Render output videos,0
folder = list(folders)[0],0
...for each video,0
,0
run_inference_with_yolov5_val.py,0
,0
Runs a folder of images through MegaDetector (or another YOLOv5 model) with YOLOv5's,0
"val.py, converting the output to the standard MD format.  The main goal is to leverage",0
YOLO's test-time augmentation tools.,0
,0
"YOLOv5's val.py uses each file's base name as a unique identifier, which doesn't work",0
when you have typical camera trap images like:,0
,0
a/b/c/RECONYX0001.JPG,0
d/e/f/RECONYX0001.JPG,0
,0
...so this script jumps through a bunch of hoops to put a symlinks in a flat,0
"folder, run YOLOv5 on that folder, and map the results back to the real files.",0
,0
"Currently requires the user to supply the path where a working YOLOv5 install lives,",0
and assumes that the current conda environment is all set up for YOLOv5.,0
,0
TODO:,1
,0
* Figure out what happens when images are corrupted... right now this is the #1,0
"reason not to use this script, it may be the case that corrupted images look the",0
same as empty images.,0
,0
* Multiple GPU support,0
,0
* Checkpointing,0
,0
* Windows support (I have no idea what all the symlink operations will do on Windows),0
,0
"* Support alternative class names at the command line (currently defaults to MD classes,",0
though other class names can be supplied programmatically),0
,0
%% Imports,0
%% Options class,0
# Required ##,0
# Optional ##,0
%% Main function,0
#%% Path handling,0
#%% Enumerate images,0
#%% Create symlinks to give a unique ID to each image,0
i_image = 0; image_fn = image_files_absolute[i_image],0
...for each image,0
#%% Create the dataset file,0
Category IDs need to be continuous integers starting at 0,0
#%% Prepare YOLOv5 command,0
#%% Run YOLOv5 command,0
#%% Convert results to MD format,0
"We'll use the absolute path as a relative path, and pass '/'",0
as the base path in this case.,0
#%% Clean up,0
...def run_inference_with_yolo_val(),0
%% Command-line driver,0
%% Scrap,0
%% Test driver (folder),0
%% Test driver (file),0
%% Preview results,0
options.sample_seed = 0,0
...for each prediction file,0
%% Compare results,0
Choose all pairwise combinations of the files in [filenames],0
"%% Constants, imports, environment",0
from multiprocessing.pool import ThreadPool as workerpool,0
Number of images to pre-fetch,0
Useful hack to force CPU inference.,1
,0
"Need to do this before any PT/TF imports, which happen when we import",0
from run_detector.,0
Numpy FutureWarnings from tensorflow import,0
%% Support functions for multiprocessing,0
TODO,1
,0
The queue system is a little more elegant if we start one thread for reading and one,1
"for processing, and this works fine on Windows, but because we import TF at module load,",0
"CUDA will only work in the main process, so currently the consumer function runs here.",0
,0
"To enable proper multi-GPU support, we may need to move the TF import to a separate module",0
that isn't loaded until very close to where inference actually happens.,0
%% Other support funtions,0
%% Image processing functions,0
%% Main function,0
Handle the case where image_file_names is not yet actually a list,0
"Find the images to score; images can be a directory, may need to recurse",0
A json list of image paths,0
A single image file,0
Load the detector,0
Does not count those already processed,0
Will not add additional entries not in the starter checkpoint,0
Write a checkpoint if necessary,0
"Back up any previous checkpoints, to protect against crashes while we're writing",0
the checkpoint file.,0
Write the new checkpoint,0
Remove the backup checkpoint if it exists,0
...if it's time to make a checkpoint,0
"When using multiprocessing, let the workers load the model",0
"Results may have been modified in place, but we also return it for",0
backwards-compatibility.,0
The typical case: we need to build the 'info' struct,0
"If the caller supplied the entire ""info"" struct",0
"The 'max_detection_conf' field used to be included by default, and it caused all kinds",0
"of headaches, so it's no longer included unless the user explicitly requests it.",0
%% Interactive driver,0
%%,0
%% Command-line driver,0
This is an experimental hack to allow the use of non-MD YOLOv5 models through,1
the same infrastructure; it disables the code that enforces MDv5-like class lists.,0
Load the checkpoint if available,0
,0
Relative file names are only output at the end; all file paths in the checkpoint are,0
still full paths.,0
"Find the images to score; images can be a directory, may need to recurse",0
A json list of image paths,0
A single image file,0
Test that we can write to the output_file's dir if checkpointing requested,0
"Don't overwrite existing checkpoint files, this is a sure-fire way to eventually",0
erase someone's checkpoint.,0
"Commenting this out for now... the scenario where we are resuming from a checkpoint,",0
then immediately overwrite that checkpoint with empty data is higher-risk than the,0
annoyance of crashing a few minutes after starting a job.,0
Confirm that we can write to the checkpoint path; this avoids issues where,0
we crash after several thousand images.,0
%% Imports,0
import pre- and post-processing functions from the YOLOv5 repo,0
scale_coords() became scale_boxes() in later YOLOv5 versions,0
%% Classes,0
padded resize,0
"Image size can be an int (which translates to a square target size) or (h,w)",0
...if the caller has specified an image size,0
NMS,0
"As of v1.13.0.dev20220824, nms is not implemented for MPS.",0
,0
Send predication back to the CPU to fix.,0
format detections/bounding boxes,0
"This is a loop over detection batches, which will always be length 1 in our case,",0
since we're not doing batch inference.,0
Rescale boxes from img_size to im0 size,0
"normalized center-x, center-y, width and height",0
"MegaDetector output format's categories start at 1, but the MD",0
model's categories start at 0.,0
...for each detection in this batch,0
...if this is a non-empty batch,0
...for each detection batch,0
...try,0
for testing,0
"MegaDetector was trained with batch size of 1, and the resizing function is a part",0
of the inference graph,0
"change from [y1, x1, y2, x2] to [x1, y1, width, height]",0
convert numpy floats to Python floats,0
need to change the above line to the following if supporting a batch size > 1 and resizing to the same size,0
"np_images = [np.asarray(image, np.uint8) for image in images]",0
"images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)",0
performs inference,0
our batch size is 1; need to loop the batch dim if supporting batch size > 1,0
"%% Constants, imports, environment",0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Numpy FutureWarnings from tensorflow import,0
Useful hack to force CPU inference,1
os.environ['CUDA_VISIBLE_DEVICES'] = '-1',0
An enumeration of failure reasons,0
Number of decimal places to round to for confidence and bbox coordinates,0
Label mapping for MegaDetector,0
Should we allow classes that don't look anything like the MegaDetector classes?,0
"Each version of the detector is associated with some ""typical"" values",0
"that are included in output files, so that downstream applications can",0
use them as defaults.,0
%% Classes,0
Stick this into filenames before the extension for the rendered result,0
%% Utility functions,0
mps backend only available in torch >= 1.12.0,0
%% Main function,0
Dictionary mapping output file names to a collision-avoidance count.,0
,0
"Since we'll be writing a bunch of files to the same folder, we rename",0
as necessary to avoid collisions.,0
...def input_file_to_detection_file(),0
Image is modified in place,0
...for each image,0
...def load_and_run_detector(),0
%% Command-line driver,0
Must specify either an image file or a directory,0
"but for a single image, args.image_dir is also None",0
%% Interactive driver,0
%%,0
#####,0
,0
process_video.py,0
,0
"Split a video (or folder of videos) into frames, run the frames through run_detector_batch.py,",0
and optionally stitch together results into a new video with detection boxes.,0
,0
TODO: allow video rendering when processing a whole folder,1
TODO: allow video rendering from existing results,1
,0
#####,0
"%% Constants, imports, environment",0
Only relevant if render_output_video is True,0
Folder to use for extracted frames,0
Folder to use for rendered frames (if rendering output video),0
Should we render a video with detection boxes?,0
,0
"Only supported when processing a single video, not a folder.",0
"If we are rendering boxes to a new video, should we keep the temporary",0
rendered frames?,0
Should we keep the extracted frames?,0
%% Main functions,0
"TODO: keep track of whether we created this folder, delete if we're deleting the extracted",0
"frames and we created the folder, and the output files aren't in the same folder.  For now,",0
we're just deleting the extracted frames and leaving the empty folder around in this case.,0
Render detections to images,0
"TODO: keep track of whether we created this folder, delete if we're deleting the rendered",0
"frames and we created the folder, and the output files aren't in the same folder.  For now,",0
we're just deleting the rendered frames and leaving the empty folder around in this case.,0
Combine into a video,0
Delete the temporary directory we used for detection images,0
shutil.rmtree(rendering_output_dir),0
(Optionally) delete the frames on which we ran MegaDetector,0
shutil.rmtree(frame_output_folder),0
...process_video(),0
# Validate options,0
# Split every video into frames,0
# Run MegaDetector on the extracted frames,0
# (Optionally) delete the frames on which we ran MegaDetector,0
shutil.rmtree(frame_output_folder),0
# Convert frame-level results to video-level results,0
...process_video_folder(),0
%% Interactive driver,0
%% Process a folder of videos,0
import clipboard; clipboard.copy(cmd),0
%% Process a single video,0
import clipboard; clipboard.copy(cmd),0
"%% Render a folder of videos, one file at a time",0
import clipboard; clipboard.copy(s),0
%% Command-line driver,0
Lint as: python3,0
Copyright 2020 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TPU is automatically inferred if tpu_name is None and,0
we are running under cloud ai-platform.,0
,0
copy_checkpoints.py,0
,0
Run this script with specified source_dir and target_dir while the model is training to make a copy,0
of every checkpoint (checkpoints are kept once an hour by default and is difficult to adjust),0
,0
do not copy event or evaluation results,0
input validation,0
plot the images,0
adjust the figure,0
"read in dataset CSV and create merged (dataset, location) col",0
map label to label_index,0
load the splits,0
only weight the training set by detection confidence,0
TODO: consider weighting val and test set as well,1
isotonic regression calibration of MegaDetector confidence,0
treat each split separately,0
new_weight[i] = confidence[i] * (n / c) / total_confidence(i's label),0
- n = # examples in split (weighted by confidence); c = # labels,0
- weight allocated to each label is n/c,0
"- within each label, weigh each example proportional to confidence",0
- new weights sum to n,0
error checking,0
"maps output label name to set of (dataset, dataset_label) tuples",0
find which other label (label_b) has intersection,0
input validation,0
create label index JSON,0
look into sklearn.preprocessing.MultiLabelBinarizer,0
Note: JSON always saves keys as strings!,0
"True for ground truth, False for MegaDetector",0
always save as .jpg for consistency,0
get bounding boxes,0
convert from category ID to category name,0
"check if crops are already downloaded, and ignore bboxes below the",0
confidence threshold,0
assign all images without location info to 'unknown_location',0
remove images from labels that have fewer than min_locs locations,0
merge dataset and location into a single string '<dataset>/<location>',0
"create DataFrame of counts. rows = locations, columns = labels",0
label_count: label => number of examples,0
loc_count: label => number of locs containing that label,0
generate a new split,0
score the split,0
SSE for # of images per label (with 2x weight),0
SSE for # of locs per label,0
label => list of datasets to prioritize for test and validation sets,0
"merge dataset and location into a tuple (dataset, location)",0
sorted smallest to largest,0
greedily add to test set until it has >= 15% of images,0
sort the resulting locs,0
"modify loc_to_size in place, so copy its keys before iterating",0
arguments relevant to both creating the dataset CSV and splits.json,0
arguments only relevant for creating the dataset CSV,0
arguments only relevant for creating the splits JSON,0
comment lines starting with '#' are allowed,0
,0
prepare_classification_script.py,0
,0
Notebook-y script used to prepare a series of shell commands to run a classifier,0
(other than MegaClassifier) on a MegaDetector result set.,0
,0
Differs from prepare_classification_script_mc.py only in the final class mapping step.,0
,0
%% Job options,0
%% Constants,0
"This is just passed along to the metadata in the output file, it has no impact",0
on how the classification scripts run.,0
%% Set up environment,0
commands.append('cd CameraTraps/classification\n'),0
commands.append('conda activate cameratraps-classifier\n'),0
%% Crop images,0
fn = input_files[0],0
%% Run classifier,0
fn = input_files[0],0
%% Merge classification and detection outputs,0
fn = input_files[0],0
%% Write everything out,0
input validation,0
"Evaluating with accimage is much faster than Pillow or Pillow-SIMD, but accimage",0
is Linux-only.,0
create output directory,0
override saved params with kwargs,0
"For now, we don't weight crops by detection confidence during",0
evaluation. But consider changing this.,0
"create model, compile with TorchScript if given checkpoint is not compiled",0
"verify that target names matches original ""label names"" from dataset",0
"if the dataset does not already have a 'other' category, then the",0
'other' category must come last in label_names to avoid conflicting,0
with an existing label_id,0
define loss function (criterion),0
"this file ends up being huge, so we GZIP compress it",0
double check that the accuracy metrics are computed properly,0
save the confusion matrices to .npz,0
save per-label statistics,0
set dropout and BN layers to eval mode,0
"even if batch contains sample weights, don't use them",0
Do target mapping on the outputs (unnormalized logits) instead of,0
"the normalized (softmax) probabilities, because the loss function",0
uses unnormalized logits. Summing probabilities is equivalent to,0
log-sum-exp of unnormalized logits.,0
"a confusion matrix C is such that C[i,j] is the # of observations known to",0
be in group i and predicted to be in group j.,0
match pytorch EfficientNet model names,0
images dataset,0
"for smaller disk / memory usage, we cache the raw JPEG bytes instead",0
of the decoded Tensor,0
convert JPEG bytes to a 3D uint8 Tensor,0
"keras EfficientNet already includes normalization from [0, 255] to [0, 1],",0
so we don't need to do that here,0
labels dataset,0
img_files dataset,0
weights dataset,0
define the transforms,0
efficientnet data preprocessing:,0
- train:,0
"1) random crop: aspect_ratio_range=(0.75, 1.33), area_range=(0.08, 1.0)",0
2) bicubic resize to img_size,0
3) random horizontal flip,0
- test:,0
1) center crop,0
2) bicubic resize to img_size,0
weights sums to:,0
- if weight_by_detection_conf: (# images in split - conf delta),0
- otherwise: (# images in split),0
"freeze the base model's weights, including BatchNorm statistics",0
https://www.tensorflow.org/guide/keras/transfer_learning#fine-tuning,0
rebuild output,0
input validation,0
set seed,0
create logdir and save params,0
create dataloaders and log the index_to_label mapping,0
define loss function and optimizer,0
using EfficientNet training defaults,0
- batch norm momentum: 0.99,0
"- optimizer: RMSProp, decay 0.9 and momentum 0.9",0
- epochs: 350,0
"- learning rate: 0.256, decays by 0.97 every 2.4 epochs",0
- weight decay: 1e-5,0
TODO: change weighted to False if oversampling minority classes,1
stop training after 8 epochs without improvement,0
log metrics,0
log confusion matrix,0
log tp/fp/fn images,0
"tf.summary.image requires input of shape [N, H, W, C]",0
false positive for top3_pred[0],0
false negative for label,0
"if evaluating or finetuning, set dropout & BN layers to eval mode",0
"for each label, track 5 most-confident and least-confident examples",0
"even if batch contains sample weights, don't use them",0
we do not track L2-regularization loss in the loss metric,0
This dictionary will get written out at the end of this process; store,0
diagnostic variables here,0
error checking,0
refresh detection cache,0
save log of bad images,0
cache of Detector outputs: dataset name => {img_path => detection_dict},0
img_path: <dataset-name>/<img-filename>,0
get SAS URL for images container,0
strip image paths of dataset name,0
save list of dataset names and task IDs for resuming,0
complete task name: 'detect_for_classifier_caltech_20200722_110816_task01',0
HACK! Sleep for 10s between task submissions in the hopes that it,1
"decreases the chance of backend JSON ""database"" corruption",0
task still running => continue,0
"task finished successfully, save response to disk",0
error checking before we download and crop any images,0
convert from category ID to category name,0
we need the datasets table for getting SAS keys,0
"we already did all error checking above, so we don't do any here",0
get ContainerClient,0
get bounding boxes,0
we must include the dataset <ds> in <crop_path_template> because,0
'{img_path}' actually gets populated with <img_file> in,0
load_and_crop(),0
"get the image, either from disk or from Blob Storage",0
inelegant way to close the container_clients,0
,0
prepare_classification_script_mc.py,0
,0
Notebook-y script used to prepare a series of shell commands to run MegaClassifier,0
on a MegaDetector result set.,0
,0
Differs from prepare_classification_script.py only in the final class mapping step.,0
,0
%% Job options,0
%% Constants,0
"This is just passed along to the metadata in the output file, it has no impact",0
on how the classification scripts run.,0
%% Set up environment,0
commands.append('cd CameraTraps/classification\n'),0
commands.append('conda activate cameratraps-classifier\n'),0
%% Crop images,0
fn = input_files[0],0
%% Run classifier,0
fn = input_files[0],0
%% Remap classifier outputs,0
fn = input_files[0],0
%% Merge classification and detection outputs,0
fn = input_files[0],0
%% Write everything out,0
mean/std values from https://pytorch.org/docs/stable/torchvision/models.html,0
define the transforms,0
resizes smaller edge to img_size,0
weights sums to:,0
- if weight_by_detection_conf: (# images in split - conf delta),0
- otherwise: # images in split,0
for normal (non-weighted) shuffling,0
set all parameters to not require gradients except final FC layer,0
replace final fully-connected layer (which has 1000 ImageNet classes),0
"detect GPU, use all if available",0
input validation,0
set seed,0
create logdir and save params,0
create dataloaders and log the index_to_label mapping,0
create model,0
define loss function and optimizer,0
using EfficientNet training defaults,0
- batch norm momentum: 0.99,0
"- optimizer: RMSProp, decay 0.9 and momentum 0.9",0
- epochs: 350,0
"- learning rate: 0.256, decays by 0.97 every 2.4 epochs",0
- weight decay: 1e-5,0
stop training after 8 epochs without improvement,0
do a complete evaluation run,0
log metrics,0
log confusion matrix,0
log tp/fp/fn images,0
"for every image: undo normalization, clamp to [0, 1], CHW -> HWC",0
"- cannot be in-place, because the HeapItem might be in multiple heaps",0
writer.add_figure() has issues => using add_image() instead,0
"writer.add_figure(f'{label_name}/{tag}', fig, global_step=epoch)",0
false positive for top3_pred[0],0
false negative for label,0
"preds and labels both have shape [N, k]",0
"if evaluating or finetuning, set dropout and BN layers to eval mode",0
"for each label, track k_extreme most-confident and least-confident images",0
"even if batch contains sample weights, don't use them",0
lock before changing ImageFile.LOAD_TRUNCATED_IMAGES,0
input validation,0
use MegaDB to generate list of images,0
only keep images that:,0
"1) end in a supported file extension, and",0
2) actually exist in Azure Blob Storage,0
3) belong to a label with at least min_locs locations,0
write out log of images / labels that were removed,0
"save label counts, pre-subsampling",0
"save label counts, post-subsampling",0
spec_dict['taxa']: list of dict,0
[,0
"{'level': 'family', 'name': 'cervidae', 'datasets': ['idfg']},",0
"{'level': 'genus',  'name': 'meleagris'}",0
],0
"spec_dict['dataset_labels']: dict, dataset => list of dataset_label",0
{,0
"""idfg"": [""deer"", ""elk"", ""prong""],",0
"""idfg_swwlf_2019"": [""elk"", ""muledeer"", ""whitetaileddeer""]",0
},0
"maps output label name to set of (dataset, dataset_label) tuples",0
"because MegaDB is organized by dataset, we do the same",0
ds_to_labels = {,0
'dataset_name': {,0
"'dataset_label': [output_label1, output_label2]",0
},0
},0
we need the datasets table for getting full image paths,0
The line,0
"[img.class[0], seq.class[0]][0] as class",0
selects the image-level class label if available. Otherwise it selects the,0
"sequence-level class label. This line assumes the following conditions,",0
expressed in the WHERE clause:,0
- at least one of the image or sequence class label is given,0
- the image and sequence class labels are arrays with length at most 1,0
- the image class label takes priority over the sequence class label,0
,0
"In Azure Cosmos DB, if a field is not defined, then it is simply excluded",0
"from the result. For example, on the following JSON object,",0
{,0
"""dataset"": ""camera_traps"",",0
"""seq_id"": ""1234"",",0
"""location"": ""A1"",",0
"""images"": [{""file"": ""abcd.jpeg""}],",0
"""class"": [""deer""],",0
},0
"the array [img.class[0], seq.class[0]] just gives ['deer'] because",0
img.class is undefined and therefore excluded.,0
"if no path prefix, set it to the empty string '', because",0
"os.path.join('', x, '') = '{x}/'",0
result keys,0
"- already has: ['dataset', 'location', 'file', 'class', 'bbox']",0
"- add ['label'], remove ['file']",0
"if img is mislabeled, but we don't know the correct class, skip it",0
"otherwise, update the img with the correct class, but skip the",0
img if the correct class is not one we queried for,0
sort keys for determinism,0
we need the datasets table for getting SAS keys,0
strip leading '?' from SAS token,0
only check Azure Blob Storage,0
check local directory first before checking Azure Blob Storage,0
1st pass: populate label_to_locs,0
"label (tuple of str) => set of (dataset, location)",0
2nd pass: eliminate bad images,0
prioritize is a list of prioritization levels,0
number of already matching images,0
main(,0
"label_spec_json_path='idfg_classes.json',",0
"taxonomy_csv_path='../../camera-traps-private/camera_trap_taxonomy_mapping.csv',",0
"output_dir='run_idfg',",0
json_indent=4),0
recursively find all files in cropped_images_dir,0
only find crops of images from detections JSON,0
resizes smaller edge to img_size,0
"Evaluating with accimage is much faster than Pillow or Pillow-SIMD, but accimage",0
is Linux-only.,0
create dataset,0
create model,0
set dropout and BN layers to eval mode,0
load files,0
dataset => set of img_file,0
crop_path: <dataset>/<img_file>___cropXX_mdvY.Y.jpg,0
[----<img_path>----],0
error checking,0
any row with 'correct_class' should be marked 'mislabeled',0
filter to only the mislabeled rows,0
convert '\' to '/',0
verify that overlapping indices are the same,0
"""add"" any new mislabelings",0
write out results,0
error checking,0
load detections JSON,0
get detector version,0
convert from category ID to category name,0
copy keys to modify dict in-place,0
This will be removed later when we filter for animals,0
save log of bad images,0
"True for ground truth, False for MegaDetector",0
always save as .jpg for consistency,0
"we already did all error checking above, so we don't do any here",0
"get the image, either from disk or from Blob Storage",0
inelegant way to close the container_client,0
"crop_path => normalized bbox coordinates [xmin, ymin, width, height]",0
"only ground-truth bboxes do not have a ""confidence"" value",0
try loading image from local directory,0
try to download image from Blob Storage,0
crop the image,0
"expand box width or height to be square, but limit to img size",0
"Image.crop() takes box=[left, upper, right, lower]",0
pad to square using 0s,0
Author: lukemelas (github username),0
Github repo: https://github.com/lukemelas/EfficientNet-PyTorch,0
With adjustments and added comments by workingcoder (github username).,0
Support the construction of 'efficientnet-l2' without pretrained weights,0
Expansion phase (Inverted Bottleneck),0
"image_size = calculate_output_image_size(image_size, 1) <-- this wouldn't modify image_size",0
Depthwise convolution phase,0
"Squeeze and Excitation layer, if desired",0
Pointwise convolution phase,0
Expansion and Depthwise Convolution,0
Squeeze and Excitation,0
Pointwise Convolution,0
Skip connection and drop connect,0
The combination of skip connection and drop connect brings about stochastic depth.,0
Batch norm parameters,0
Get stem static or dynamic convolution depending on image size,0
Stem,0
Build blocks,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
"image_size = calculate_output_image_size(image_size, block_args.stride)  # stride = 1",0
Head,0
Final linear layer,0
Stem,0
Blocks,0
Head,0
Stem,0
Blocks,0
Head,0
Convolution layers,0
Pooling and final linear layer,0
Author: lukemelas (github username),0
Github repo: https://github.com/lukemelas/EfficientNet-PyTorch,0
With adjustments and added comments by workingcoder (github username).,0
###############################################################################,0
## Help functions for model architecture,0
###############################################################################,0
GlobalParams and BlockArgs: Two namedtuples,0
Swish and MemoryEfficientSwish: Two implementations of the method,0
round_filters and round_repeats:,0
Functions to calculate params for scaling model width and depth ! ! !,0
get_width_and_height_from_size and calculate_output_image_size,0
drop_connect: A structural design,0
get_same_padding_conv2d:,0
Conv2dDynamicSamePadding,0
Conv2dStaticSamePadding,0
get_same_padding_maxPool2d:,0
MaxPool2dDynamicSamePadding,0
MaxPool2dStaticSamePadding,0
"It's an additional function, not used in EfficientNet,",0
but can be used in other model (such as EfficientDet).,0
"Parameters for the entire model (stem, all blocks, and head)",0
Parameters for an individual model block,0
Set GlobalParams and BlockArgs's defaults,0
An ordinary implementation of Swish function,0
A memory-efficient implementation of Swish function,0
TODO: modify the params names.,1
"maybe the names (width_divisor,min_width)",0
"are more suitable than (depth_divisor,min_depth).",0
follow the formula transferred from official TensorFlow implementation,0
follow the formula transferred from official TensorFlow implementation,0
"generate binary_tensor mask according to probability (p for 0, 1-p for 1)",0
Note:,0
The following 'SamePadding' functions make output size equal ceil(input size/stride).,0
"Only when stride equals 1, can the output size be the same as input size.",0
Don't be confused by their function names ! ! !,0
Tips for 'SAME' mode padding.,0
Given the following:,0
i: width or height,0
s: stride,0
k: kernel size,0
d: dilation,0
p: padding,0
Output after Conv2d:,0
o = floor((i+p-((k-1)*d+1))/s+1),0
"If o equals i, i = floor((i+p-((k-1)*d+1))/s+1),",0
=> p = (i-1)*s+((k-1)*d+1)-i,0
With the same calculation as Conv2dDynamicSamePadding,0
Calculate padding based on image size and save it,0
Calculate padding based on image size and save it,0
###############################################################################,0
## Helper functions for loading model params,0
###############################################################################,0
BlockDecoder: A Class for encoding and decoding BlockArgs,0
efficientnet_params: A function to query compound coefficient,0
get_model_params and efficientnet:,0
Functions to get BlockArgs and GlobalParams for efficientnet,0
url_map and url_map_advprop: Dicts of url_map for pretrained weights,0
load_pretrained_weights: A function to load pretrained weights,0
Check stride,0
"Coefficients:   width,depth,res,dropout",0
Blocks args for the whole model(efficientnet-b0 by default),0
It will be modified in the construction of EfficientNet Class according to model,0
note: all models have drop connect rate = 0.2,0
ValueError will be raised here if override_params has fields not included in global_params.,0
train with Standard methods,0
check more details in paper(EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks),0
train with Adversarial Examples(AdvProp),0
check more details in paper(Adversarial Examples Improve Image Recognition),0
TODO: add the petrained weights url map of 'efficientnet-l2',1
AutoAugment or Advprop (different preprocessing),0
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils),0
from data_management.megadb.schema import sequences_schema_check,0
resize is for displaying them more quickly,0
dataset and seq_id are required fields,0
sort the images in the sequence,0
"if no class label on the image, show class label on the sequence",0
pool = ThreadPool(),0
"print('len of rendering_info', len(rendering_info))",0
"tqdm(pool.imap_unordered(render_image_info_partial, rendering_info), total=len(rendering_info))",0
options = write_html_image_list(),0
options['headerHtml'],0
print('Checking that the MegaDB entries conform to the schema...'),0
sequences_schema_check.sequences_schema_check(sequences),0
#######,0
,0
visualize_db.py,0
,0
Outputs an HTML page visualizing annotations (class labels and/or bounding boxes),0
on a sample of images in a database in the COCO Camera Traps format,0
,0
#######,0
%% Imports,0
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils),0
Assumes the cameratraps repo root is on the path,0
%% Settings,0
Set to None to visualize all images,0
Target size for rendering; set either dimension to -1 to preserve aspect ratio,0
"These are mutually exclusive; both are category names, not IDs",0
"Special tag used to say ""show me all images with multiple categories""",0
We sometimes flatten image directories by replacing a path separator with,0
another character.  Leave blank for the typical case where this isn't necessary.,0
Control rendering parallelization,0
Process-based parallelization in this function is currently unsupported,0
"due to pickling issues I didn't care to look at, but I'm going to just",0
"flip this with a warning, since I intend to support it in the future.",0
%% Helper functions,0
"Translate the file name in an image entry in the json database to a path, possibly doing",0
some manipulation of path separators,0
%% Core functions,0
"Optionally remove all images without bounding boxes, *before* sampling",0
"Optionally include/remove images with specific labels, *before* sampling",0
Put the annotations in a dataframe so we can select all annotations for a given image,0
Construct label map,0
Take a sample of images,0
Set of dicts representing inputs to render_db_bounding_boxes:,0
,0
"bboxes, boxClasses, image_path",0
All the class labels we've seen for this image (with out without bboxes),0
Iterate over annotations for this image,0
iAnn = 0; anno = annos_i.iloc[iAnn],0
...for each of this image's annotations,0
"We're adding html for an image before we render it, so it's possible this image will",0
fail to render.  For applications where this script is being used to debua a database,0
"(the common case?), this is useful behavior, for other applications, this is annoying.",0
...for each image,0
...def render_image_info,0
def process_images(...),0
%% Command-line driver,0
"Copy all fields from a Namespace (i.e., the output from parse_args) to an object.",0
,0
Skips fields starting with _.  Does not check existence in the target object.,0
Convert to an options object,0
%% Interactive driver(s),0
%%,0
os.startfile(htmlOutputFile),0
%% Constants and imports,0
convert category ID from int to str,0
Retry on blob storage read failures,0
%% Functions,0
PIL.Image.convert() returns a converted copy of this image,0
Alter orientation as needed according to EXIF tag 0x112 (274) for Orientation,0
,0
https://gist.github.com/dangtrinhnt/a577ece4cbe5364aad28,0
https://www.media.mit.edu/pia/Research/deepview/exif.html,0
,0
Null operation,0
Aspect ratio as width over height,0
ar = w / h,0
h = w / ar,0
w = ar * h,0
The following three functions are modified versions of those at:,0
,0
https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py,0
Convert to pixels so we can use the PIL crop() function,0
PIL's crop() does surprising things if you provide values outside of,0
"the image, clip inputs",0
...if this detection is above threshold,0
...for each detection,0
"list of lists, one list of strings for each bounding box (to accommodate multiple labels)",0
for color selection,0
"Always render objects with a confidence of ""None"", this is typically used",0
for ground truth data.,0
"{} is the default, which means ""show labels with no mapping"", so don't use ""if label_map"" here",0
if label_map:,0
"To avoid duplicate colors with detection-only visualization, offset",0
the classification class index by the number of detection classes,0
...for each classification,0
...if we have classification results,0
...if the confidence of this detection is above threshold,0
...for each detection,0
"print('Input must be of size [N, 4], but is ' + str(boxes_shape))",0
Deliberately trimming to the width of the image only in the case where,0
"box expansion is turned on.  There's not an obvious correct behavior here,",0
but the thinking is that if the caller provided an out-of-range bounding,0
"box, they meant to do that, but at least in the eyes of the person writing",0
"this comment, if you expand a box for visualization reasons, you don't want",0
to end up with part of a box.,0
,0
A slightly more sophisticated might check whether it was in fact the expansion,0
"that made this box larger than the image, but this is the case 99.999% of the time",0
"here, so that doesn't seem necessary.",1
...if we need to expand boxes,0
If the total height of the display strings added to the top of the bounding,0
"box exceeds the top of the image, stack the strings below the bounding box",0
instead of above.,0
Each display_str has a top and bottom margin of 0.05x.,0
Reverse list and print from bottom to top.,0
Skip empty strings,0
"list of lists, one list of strings for each bounding box (to accommodate multiple labels)",0
need to be a string here because PIL needs to iterate through chars,0
,0
stacked bar charts are made with each segment starting from a y position,0
"To fit the legend in, shrink current axis by 20%",0
Put a legend to the right of the current axis,0
####,0
,0
visualize_incoming_annotations.py,0
,0
Spot-check the annotations received from iMerit by visualizing annotated bounding,0
boxes on a sample of images and display them in HTML.,0
,0
"Modified in 2021 March to use the new format (iMerit batch 12 onwards), which is a",0
COCO formatted JSON with relative coordinates for the bbox.,0
,0
from data_management.megadb.schema import sequences_schema_check,0
we used frame_num of 1 when sending out images to annotators when it is not explicitly stored (wcs esp.),0
The file_name field in the incoming json looks like,0
alka_squirrels.seq2020_05_07_25C.frame119221.jpg,0
"we need to use the dataset, sequence and frame info to find the actual path in blob storage",0
using the sequences,0
category_id 5 is No Object Visible,0
download the image,0
Write to HTML,0
allow forward references in typing annotations,0
class variables,0
instance variables,0
get path to root,0
taxonomy CSV rows without 'taxonomy_string' entries are excluded,0
"from the taxonomy graph, but can be included in a classification",0
label specification JSON via the 'dataset_labels' key,0
use the lower parent,0
special cases,0
%% Imports,0
%% Taxnomy checking,0
taxonomy CSV rows without 'taxonomy_string' entries are excluded,0
"from the taxonomy graph, but can be included in a classification",0
label specification JSON via the 'dataset_labels' key,0
...for each row in the taxnomy file,0
%% Command-line driver,0
%% Interactive driver,0
%%,0
which datasets are already processed?,0
"sequence-level query should be fairly fast, ~1 sec",0
cases when the class field is on the image level (images in a sequence,0
"that had different class labels, 'caltech' dataset is like this)",0
"this query may take a long time, >1hr",0
"this query should be fairly fast, ~1 sec",0
read species presence info from the JSON files for each dataset,0
has this class name appeared in a previous dataset?,0
columns to populate the spreadsheet,0
sort by descending species count,0
make the spreadsheet,0
hyperlink Bing search URLs,0
hyperlink example image SAS URLs,0
TODO hardcoded columns: change if # of examples or col_order changes,1
,0
map_lila_taxonomy_to_wi_taxonomy.py,0
,0
Loads the LILA category mapping (in which taxonomy information comes from an iNat taxonomy snapshot),0
and tries to map each class to the Wildlife Insights taxonomy.,0
,0
%% Constants and imports,0
Created by get_lila_category_list.py... contains counts for each category,0
This is a manually-curated file used to store mappings that had to be made manually,0
This is the main output file from this whole process,0
%% Load category and taxonomy files,0
%% Pull everything out of pandas,0
%% Cache WI taxonomy lookups,0
This is just a handy lookup table that we'll use to debug mismatches,0
taxon = wi_taxonomy[21653]; print(taxon),0
Look for keywords that don't refer to specific taxa: blank/animal/unknown,0
Do we have a species name?,0
"If 'species' is populated, 'genus' should always be populated; one item currently breaks",0
this rule.,0
...for each taxon,0
%% Find redundant taxa,0
%% Manual review of redundant taxa,0
%% Clean up redundant taxa,0
taxon_name = list(taxon_name_to_preferred_taxon_id.keys())[0],0
"If we've gotten this far, we should be choosing from multiple taxa.",0
,0
"This will become untrue if any of these are resolved later, at which point we shoudl",0
remove them from taxon_name_to_preferred_id,0
Choose the preferred taxa,0
%% Read supplementary mappings,0
"Each line is [lila query],[WI taxon name],[notes]",0
%% Map LILA categories to WI categories,0
Must be ordered from kingdom --> species,0
TODO:,1
"['subspecies','variety']",0
i_taxon = 0; taxon = lila_taxonomy[i_taxon]; print(taxon),0
"Go from kingdom --> species, choosing the lowest-level description as the query",0
"E.g., 'car'",0
"print('Made a supplementary mapping from {} to {}'.format(query,wi_taxon['taxon_name']))",0
print('No match for {}'.format(query)),0
...for each LILA taxon,0
%% Manual mapping,0
%% Build a dictionary from LILA dataset names and categories to LILA taxa,0
i_d = 0; d = lila_taxonomy[i_d],0
"%% Map LILA datasets to WI taxa, and count the number of each taxon available in each dataset",0
dataset_name = list(lila_dataset_to_categories.keys())[0],0
dataset_category = dataset_categories[0],0
"Write out the dataset name, category name, WI GUID, WI scientific name, WI common name,",0
and count,0
...for each category in this dataset,0
...for each dataset,0
...with open(),0
,0
retrieve_sample_image.py,0
,0
"Downloader that retrieves images from Google images, used for verifying taxonomy",0
"lookups and looking for egregious mismappings (e.g., ""snake"" being mapped to a fish called",0
"""snake"").",0
,0
"Simple wrapper around simple_image_download, but I've had to swap in and out the underlying",0
downloader a few times.,0
,0
%% Imports and environment,0
%%,0
%% Main entry point,0
%% Test driver,0
%%,0
,0
"Using the taxonomy .csv file, map all LILA datasets to the standard taxonomy",0
,0
Does not currently produce results; this is just used to confirm that all category names,0
have mappings in the taxonomy file.,0
,0
%% Constants and imports,0
Created by get_lila_category_list.py... contains counts for each category,0
%% Load category and taxonomy files,0
%% Map dataset names and category names to scientific names,0
i_row = 1; row = taxonomy_df.iloc[i_row]; row,0
"%% For each dataset, make sure we can map every category to the taxonomy",0
dataset_name = list(lila_dataset_to_categories.keys())[0],0
c = categories[0],0
,0
"Takes the megadb taxonomy mapping, extracts the rows that are relevant to",0
"LILA, and does some cleanup.",0
,0
%% Constants and imports,0
This is a partially-completed taxonomy file that was created from a different set of,0
"scripts, but covers *most* of LILA as of June 2022",0
Created by get_lila_category_list.py,0
%% Read the input files,0
Get everything out of pandas,0
"%% Find all unique dataset names in the input list, compare them with data names from LILA",0
d = input_taxonomy_rows[0],0
The script that generates this dictionary creates a separate entry for bounding box,0
"metadata files, but those don't represent new dataset names",0
%% Map input columns to output datasets,0
Make sure all of those datasets actually correspond to datasets on LILA,0
%% Re-write the input taxonomy file to refer to LILA datasets,0
Map the string datasetname:token to a taxonomic tree json,0
mapping = input_taxonomy_rows[0],0
Make sure that all occurrences of this mapping_string give us the same output,0
assert taxonomy_string == taxonomy_mappings[mapping_string],0
%% Re-write the input file in the target format,0
mapping_string = list(taxonomy_mappings.keys())[0],0
,0
prepare_lila_taxonomy_release.py,0
,0
"Given the private intermediate taxonomy mapping, prepare the public (release)",0
taxonomy mapping file.,0
,0
%% Imports and constants,0
Created by get_lila_category_list.py... contains counts for each category,0
%% Find out which categories are actually used,0
dataset_name = datasets_to_map[0],0
i_row = 0; row = df.iloc[i_row]; row,0
%% Generate the final output file,0
i_row = 0; row = df.iloc[i_row]; row,0
match_at_level = taxonomic_match[0],0
i_row = 0; row = df.iloc[i_row]; row,0
"E.g.: (43117, 'genus', 'lepus', ['hares and jackrabbits']",0
###############,0
---> CONSTANTS,0
###############,0
max_progressbar = count * (list(range(limit+1))[-1]+1),0
"bar = progressbar.ProgressBar(maxval=max_progressbar,",0
"widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()]).start()",0
bar.update(bar.currval + 1),0
bar.finish(),0
,0
"Given a subset of LILA datasets, find all the categories, and start the taxonomy",0
mapping process.,0
,0
%% Constants and imports,0
Created by get_lila_category_list.py,0
'NACTI',0
'Channel Islands Camera Traps',0
%% Read the list of datasets,0
The script that generates this dictionary creates a separate entry for bounding box,0
"metadata files, but those don't represent new dataset names",0
%% Find all categories,0
dataset_name = datasets_to_map[0],0
%% Initialize taxonomic lookup,0
from taxonomy_mapping.species_lookup import (,0
"get_taxonomic_info, print_taxonomy_matche)",0
%% Manual lookup,0
%%,0
q = 'white-throated monkey',0
raise ValueError(''),0
%% Match every query against our taxonomies,0
mapping_string = category_mappings[1]; print(mapping_string),0
...for each mapping,0
%% Write output rows,0
,0
"Does some consistency-checking on the LILA taxonomy file, and generates",0
an HTML preview page that we can use to determine whether the mappings,0
make sense.,0
,0
%% Imports and constants,0
"lila_taxonomy_file = r""G:\git\agentmorrisprivate\lila-taxonomy\lila-taxonomy-mapping.csv""",0
"lila_taxonomy_file = r""G:\temp\lila\lila_additions_2022.06.29.csv""",0
%% Support functions,0
%% Read the taxonomy mapping file,0
%% Prepare taxonomy lookup,0
from taxonomy_mapping.species_lookup import (,0
"get_taxonomic_info, print_taxonomy_matche)",0
%% Optionally remap all gbif-based mappings to inat (or vice-versa),0
%%,0
i_row = 1; row = df.iloc[i_row]; row,0
This should be zero for the release .csv,0
%%,0
%% Check for mappings that disagree with the taxonomy string,0
Look for internal inconsistency,0
Look for outdated mappings,0
i_row = 0; row = df.iloc[i_row],0
%% List null mappings,0
,0
"These should all be things like ""unidentified"" and ""fire""",0
,0
i_row = 0; row = df.iloc[i_row],0
%% List mappings with scientific names but no common names,0
%% List mappings that map to different things in different data sets,0
x = suppress_multiple_matches[-1],0
...for each row where we saw this query,0
...for each row,0
"%% Verify that nothing ""unidentified"" maps to a species or subspecies",0
"E.g., ""unidentified skunk"" should never map to a specific species of skunk",0
%% Make sure there are valid source and level values for everything with a mapping,0
%% Find WCS mappings that aren't species or aren't the same as the input,0
"WCS used scientific names, so these remappings are slightly more controversial",0
then the standard remappings.,0
row = df.iloc[-500],0
"Anything marked ""species"" or ""unknown"" by definition doesn't map to a species,",0
so ignore these.,0
print('WCS query {} ({}) remapped to {} ({})'.format(,0
"query,common_name,scientific_name,common_names_from_taxonomy))",0
%% Download sample images for all scientific names,0
i_row = 0; row = df.iloc[i_row],0
if s != 'mirafra':,0
continue,0
Check whether we already have enough images for this query,0
"print('Skipping query {}, already have {} images'.format(s,len(sizes_above_threshold)))",0
Check whether we've already run this query for a previous row,0
...for each row in the mapping table,0
%% Rename .jpeg to .jpg,0
"print('Renaming {} to {}'.format(fn,new_fn))",0
%% Choose representative images for each scientific name,0
s = list(scientific_name_to_paths.keys())[0],0
Be suspicious of duplicate sizes,0
...for each scientific name,0
%% Delete unused images,0
%% Produce HTML preview,0
i_row = 2; row = df.iloc[i_row],0
"image_paths = [os.path.relpath(p, output_base) for p in image_paths]",0
...for each row,0
%% Open HTML preview,0
######,0
,0
species_lookup.py,0
,0
Look up species names (common or scientific) in the GBIF and iNaturalist,0
taxonomies.,0
,0
Run initialize_taxonomy_lookup() before calling any other function.,0
,0
######,0
%% Constants and imports,0
As of 2020.05.12:,0
,0
"GBIF: ~777MB zipped, ~1.6GB taxonomy",0
"iNat: ~2.2GB zipped, ~51MB taxonomy",0
These are un-initialized globals that must be initialized by,0
the initialize_taxonomy_lookup() function below.,0
%% Functions,0
Initialization function,0
# Load serialized taxonomy info if we've already saved it,0
"# If we don't have serialized taxonomy info, create it from scratch.",0
Download and unzip taxonomy files,0
taxonomy_name = list(taxonomy_urls.items())[0][0]; zip_url = list(taxonomy_urls.items())[0][1],0
Don't download the zipfile if we've already unzipped what we need,0
Bypasses download if the file exists already,0
Unzip the files we need,0
...for each file that we need from this zipfile,0
Remove the zipfile,0
os.remove(zipfile_path),0
...for each taxonomy,0
"Create dataframes from each of the taxonomy files, and the GBIF common",0
name file,0
Load iNat taxonomy,0
Load GBIF taxonomy,0
Remove questionable rows from the GBIF taxonomy,0
Load GBIF vernacular name mapping,0
Only keep English mappings,0
Convert everything to lowercase,0
"For each taxonomy table, create a mapping from taxon IDs to rows",0
Create name mapping dictionaries,0
Build iNat dictionaries,0
row = inat_taxonomy.iloc[0],0
Build GBIF dictionaries,0
"The canonical name is the Latin name; the ""scientific name""",0
include the taxonomy name.,0
,0
http://globalnames.org/docs/glossary/,0
This only seems to happen for really esoteric species that aren't,0
"likely to apply to our problems, but doing this for completeness.",0
Don't include taxon IDs that were removed from the master table,0
Save everything to file,0
...def initialize_taxonomy_lookup(),0
"list of dicts: {'source': source_name, 'taxonomy': match_details}",0
i_match = 0,0
"list of (taxon_id, taxonRank, scientific name, [vernacular names])",0
corresponding to an exact match and its parents,0
Walk taxonomy hierarchy,0
This can happen because we remove questionable rows from the,0
GBIF taxonomy,0
"print(f'Warning: no row exists for parent_taxon_id {parent_taxon_id},' + \",0
"f'child taxon_id: {taxon_id}, query: {query}')",0
The GBIF taxonomy contains unranked entries,0
...while there is taxonomy left to walk,0
...for each match,0
Remove redundant matches,0
i_tree_a = 0; tree_a = matching_trees[i_tree_a],0
i_tree_b = 1; tree_b = matching_trees[i_tree_b],0
"If tree a's primary taxon ID is inside tree b, discard tree a",0
,0
taxonomy_level_b = tree_b['taxonomy'][0],0
...for each level in taxonomy B,0
...for each tree (inner),0
...for each tree (outer),0
...def traverse_taxonomy(),0
"print(""Finding taxonomy information for: {0}"".format(query))",0
"In GBIF, some queries hit for both common and scientific, make sure we end",0
up with unique inputs,0
"If the species is not found in either taxonomy, return None",0
Both GBIF and iNat have a 1-to-1 mapping between taxon_id and row number,0
Walk both taxonomies,0
...def get_taxonomic_info(),0
m = matches[0],0
"For example: [(9761484, 'species', 'anas platyrhynchos')]",0
...for each taxonomy level,0
...for each match,0
...def print_taxonomy_matches(),0
%% Taxonomy functions that make subjective judgements,0
query = 'person',0
Do we have an iNat match?,0
"print_taxonomy_matches(inat_matches, verbose=True)",0
"print_taxonomy_matches(gbif_matches, verbose=True)",0
print('Warning: multiple iNat matches for {}'.format(query)),0
Prefer chordates... most of the names that aren't what we want,0
"are esoteric insects, like a moth called ""cheetah""",0
,0
"If we can't find a chordate, just take the first match.",0
,0
i_test_match = 0,0
"This is (taxonID, taxonLevel, scientific, [list of common])",0
print(f'Warning: multiple iNat common names for {query}'),0
Default to returning the query,0
"print(f'Matched iNat {query} to {scientific_name},{common_name}')",0
...if we had iNat matches,0
If we either prefer GBIF or didn't have iNat matches,0
,0
Code is deliberately redundant here; I'm expecting some subtleties in how,0
handle GBIF and iNat.,0
print('Warning: multiple GBIF matches for {}'.format(query)),0
Prefer chordates... most of the names that aren't what we want,0
"are esoteric insects, like a moth called ""cheetah""",0
,0
"If we can't find a chordate, just take the first match.",0
,0
i_test_match = 0,0
"This is (taxonID, taxonLevel, scientific, [list of common])",0
print(f'Warning: multiple GBIF common names for {query}'),0
Default to returning the query,0
...if we needed to look in the GBIF taxonomy,0
...def _get_preferred_taxonomic_match(),0
%% Interactive drivers and debug,0
%% Initialization,0
%% Taxonomic lookup,0
query = 'lion',0
print(matches),0
Print the taxonomy in the taxonomy spreadsheet format,0
%% Directly access the taxonomy tables,0
%% Command-line driver,0
Read command line inputs (absolute path),0
Read the tokens from the input text file,0
Loop through each token and get scientific name,0
,0
process_species_by_dataset,0
,0
We generated a list of all the annotations in our universe; this script is,0
used to (interactively) map them onto the GBIF and iNat taxonomies.  Don't,0
"try to run this script from top to bottom; it's used like a notebook, not like",0
"a script, since manual review steps are required.",0
,0
%% Imports,0
%autoreload 0,0
%autoreload -species_lookup,0
%% Constants,0
Input file,0
Output file after automatic remapping,0
File to which we manually copy that file and do all the manual review; this,0
should never be programmatically written to,0
The final output spreadsheet,0
HTML file generated to facilitate the identificaiton of egregious mismappings,0
%% Functions,0
Prefer iNat matches over GBIF matches,0
query = 'person',0
Do we have an iNat match?,0
"print_taxonomy_matches(inat_matches, verbose=True)",0
"print_taxonomy_matches(gbif_matches, verbose=True)",0
print('Warning: multiple iNat matches for {}'.format(query)),0
Prefer chordates... most of the names that aren't what we want,0
"are esoteric insects, like a moth called ""cheetah""",0
,0
"If we can't find a chordate, just take the first match.",0
,0
i_test_match = 0,0
"This is (taxonID, taxonLevel, scientific, [list of common])",0
print(f'Warning: multiple iNat common names for {query}'),0
Default to returning the query,0
"print(f'Matched iNat {query} to {scientific_name},{common_name}')",0
...if we had iNat matches,0
If we either prefer GBIF or didn't have iNat matches,0
,0
Code is deliberately redundant here; I'm expecting some subtleties in how,0
handle GBIF and iNat.,0
print('Warning: multiple GBIF matches for {}'.format(query)),0
Prefer chordates... most of the names that aren't what we want,0
"are esoteric insects, like a moth called ""cheetah""",0
,0
"If we can't find a chordate, just take the first match.",0
,0
i_test_match = 0,0
"This is (taxonID, taxonLevel, scientific, [list of common])",0
print(f'Warning: multiple GBIF common names for {query}'),0
Default to returning the query,0
...if we needed to look in the GBIF taxonomy,0
...def get_preferred_taxonomic_match(),0
%% Initialization,0
%% Test single-query lookup,0
%%,0
%%,0
"q = ""grevy's zebra""",0
%% Read the input data,0
%% Run all our taxonomic lookups,0
i_row = 0; row = df.iloc[i_row],0
query = 'lion',0
...for each query,0
Write to the excel file that we'll use for manual review,0
%% Download preview images for everything we successfully mapped,0
uncomment this to load saved output_file,0
"output_df = pd.read_excel(output_file, keep_default_na=False)",0
i_row = 0; row = output_df.iloc[i_row],0
...for each query,0
%% Write HTML file with representative images to scan for obvious mis-mappings,0
i_row = 0; row = output_df.iloc[i_row],0
...for each row,0
%% Look for redundancy with the master table,0
Note: `master_table_file` is a CSV file that is the concatenation of the,0
"manually-remapped files (""manual_remapped.xlsx""), which are the output of",0
this script run across from different groups of datasets. The concatenation,0
"should be done manually. If `master_table_file` doesn't exist yet, skip this",0
"code cell. Then, after going through the manual steps below, set the final",0
manually-remapped version to be the `master_table_file`.,0
%% Manual review,0
Copy the spreadsheet to another file; you're about to do a ton of manual,0
review work and you don't want that programmatically overwrriten.,0
,0
See manual_review_xlsx above,0
%% Read back the results of the manual review process,0
%% Look for manual mapping errors,0
Manually inspect df for typos in 'dataset_names' and 'taxonomy_level' columns,0
Identify rows where:,0
,0
- 'taxonomy_level' does not match level of 1st element in 'taxonomy_string',0
- 'scientific_name' does not match name of 1st element in 'taxonomy_string',0
,0
...both of which typically represent manual mapping errors.,0
i_row = 0; row = df.iloc[i_row],0
"I'm not sure why both of these checks are necessary, best guess is that",1
the Excel parser was reading blanks as na on one OS/Excel version and as '',0
on another.,0
The taxonomy_string column is a .json-formatted string; expand it into,0
an object via eval(),0
"%% Find scientific names that were added manually, and match them to taxonomies",0
i_row = 0; row = df.iloc[i_row],0
...for each query,0
%% Write out final version,0
,0
"Generate read-only SAS URLs for all LILA containers, to facilitate partial downloads.",0
,0
The results of this script end up here:,0
,0
http://lila.science/wp-content/uploads/2020/03/lila_sas_urls.txt,0
,0
"Update: that file is manually maintained now, it can't be programmatically generated",0
,0
%% Imports,0
Read-only,0
%% Enumerate containers,0
%% Generate SAS tokens,0
%% Generate SAS URLs,0
%% Write to output file,0
--------some stuff needed to get AJAX to work with bottle?--------#,0
-------------------------------------------------------------------------------- #,0
PREPARE TO QUEUE IMAGES FOR LABELING,0
-------------------------------------------------------------------------------- #,0
# Connect as USER to database DB_NAME through peewee and initialize database proxy,0
# Load embedding model,0
---------------------------------------------------------------------- #,0
CREATE QUEUE OF IMAGES TO LABEL,0
---------------------------------------------------------------------- #,0
Use classifier to generate predictions,0
# Update model predicted class in PostgreSQL database,0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?,0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
-------------------------------------------------------------------------------- #,0
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI,0
-------------------------------------------------------------------------------- #,0
"# static routes (to serve CSS, etc.)",0
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps",0
"return bottle.static_file(filename, root='../../../../../../../../../../../.')",0
# dynamic routes,0
if data['display_grayscale']:,0
indices_to_exclude.update(set(color_indices)),0
elif not data['display_grayscale']:,0
indices_to_exclude.update(set(grayscale_indices)),0
data['display_images'] = {},0
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices],0
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices],0
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices],0
data['display_images']['detection_categories'] = [],0
for i in indices:,0
if str(dataset.samples[i][1]) == 'None':,0
data['display_images']['detection_categories'].append('None'),0
else:,0
existing_category_entries = {cat.id: cat.name for cat in Category.select()},0
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()",0
data['display_images']['detection_categories'].append(cat_name),0
Use image ids in images_to_label to get the corresponding dataset indices,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
print(set(dataset.set_indices[4]).update(set(indices_to_label))),0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Use image id images_to_label to get the corresponding dataset index,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?,0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Train on samples that have been labeled so far,0
dataset.set_kind(DetectionKind.UserDetection.value),0
print(y_train),0
Predict on the samples that have not been labeled,0
print(y_pred),0
Update model predicted class in PostgreSQL database,0
timer = time.time(),0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
print('Updating the database took %0.2f seconds'%(time.time() - timer)),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids)),0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
"once the classifier has been trained the first time, switch to AL sampling",0
,0
make_oneclass_json.py,0
,0
"Takes a coco-camera-traps .json database and collapses species classes to binary,",0
optionally removing labels from empty images (to be detector-friendly) (depending on,0
"""experiment_type"").",0
,0
"Assumes that empty images are labeled as ""empty"".",0
,0
%% Imports and environment,0
%% Core conversion function,0
"We're removing empty images from the annotation list, but not from",0
"the ""images"" list; they'll still get used in detector training.",0
print('Ignoring empty annotation'),0
%% Interactive driver,0
%%,0
Load annotations,0
Convert from multi-class to one-class,0
Write out the one-class data,0
%% Command-line driver,0
,0
plot_bounding_boxes.py,0
,0
Takes a .json database containing bounding boxes and renders those boxes on the,0
source images.,0
,0
"This assumes annotations in coco-camera-traps format, with absolute bbox",0
coordinates.,0
,0
%% Imports and environment,0
How many images should we process?  Set to -1 to process all images.,0
Should we randomize the image order?,0
%%  Read database and build up convenience mappings,0
Category ID to category name,0
Image ID to image info,0
Image ID to image path,0
Image ID to all annotations referring to this image,0
"%% Iterate over images, draw bounding boxes, write to file",0
For each image,0
image = images[0],0
Build up a list of bounding boxes to draw on this image,0
Load the image,0
Create figure and axes,0
Display the image,0
ann = imageAnnotations[0],0
"For each annotation associated with this image, render bounding box and label",0
"In the Rectangle() function, the first argument (""location"") is the bottom-left",0
of the rectangle.,0
,0
Origin is the upper-left of the image.,0
Add the patch to the Axes,0
Add a class label,0
This is magic goop that removes whitespace around image plots (sort of),0
Write the output image,0
...for each image,0
,0
plot_imerit_annotations.py,0
,0
Takes a .json file full of bounding box annotations and renders those boxes on the,0
source images.,0
,0
"This assumes annotations in the format we receive them, specifically:",0
,0
1) Relative bbox coordinates,0
"2) A list of .json objects, not a well-formatted .json file",0
,0
"I.e., don't use this on a COCO-style .json file.  See plot_bounding_boxes.py",0
for the same operation performed on a proper COCO-camera-traps database.,0
,0
%% Imports and environment,0
"os.makedirs(outputBase, exist_ok=True)",0
%%  Read all source images and build up a hash table from image name to full path,0
"This spans training and validation directories, so it's not the same as",0
just joining the image name to a base path,0
"%% Iterate over annotations, draw bounding boxes, write to file",0
annData has keys:,0
,0
"annotations, categories, images",0
,0
Each of these are lists of dictionaries,0
%% Render all annotations on each image in the sequence,0
%% Pull out image metadata,0
Build up a list of bounding boxes to draw on this image,0
Pull out just the image name from the filename,0
,0
File names look like:,0
,0
seq6efffac2-5567-11e8-b3fe-dca9047ef277.frame1.img59a94e52-23d2-11e8-a6a3-ec086b02610b.jpg,0
"m = re.findall(r'img(.*\.jpg)$', imgFileName, re.M|re.I)",0
print(m),0
assert(len(m) == 1),0
queryFileName = m[0],0
Map this image back to the original directory,0
"%% Loop over annotations, find annotations that match this image",0
%%,0
"x,y,w,h",0
,0
"x,y is the bottom-left of the rectangle",0
,0
"x,y origin is the upper-left",0
...for each annotation,0
%% Render with PIL (scrap),0
%% Render with Matplotlib,0
Create figure and axes,0
Display the image,0
Location is the bottom-left of the rect,0
,0
Origin is the upper-left,0
Add the patch to the Axes,0
This is magic goop that removes whitespace around image plots (sort of),0
os.startfile(outputFileName),0
%% Showing figures on-screen during debugging,0
plt.show(),0
Various (mostly unsuccessful) approaches to getting the plot window to show up,0
"in the foreground, which is a backend-specific operation...",0
,0
fig.canvas.manager.window.activateWindow(),0
fig.canvas.manager.window.raise_(),0
fm = plt.get_current_fig_manager(),0
"fm.window.attributes('-topmost', 1)",0
"fm.window.attributes('-topmost', 0)",0
,0
# This is the one that I found to be most robust... at like 80% robust.,0
plt.get_current_fig_manager().window.raise_(),0
%%,0
...for each image,0
...for each file,0
,0
convert_imerit_json_to_coco_json.py,0
,0
"Takes a .json file with bounding boxes but no class labels, and a .json file containing the",0
"class labels for those images, and creates a new json file with class labels and bounding",0
boxes.,0
,0
"The bounding box .json file is in the format returned by our annotators, which is not",0
actually a fully-formed .json file; rather it's a series of .json objects,0
,0
"Leaves filenames intact.  Stores imerit ""category IDs"" (empty/human/group/animal) in a new",0
"field called ""annotation_type"".",0
,0
%% Imports and constants,0
%% Configure files and paths,0
"For Snapshot Serengeti, we stored image IDs in our annotation files as:",0
,0
S1_B06_R1_PICT0008,0
,0
...but the corresponding ID in the master database is actually:,0
,0
S1\B06\R1\S1_B06_R1_PICT0008,0
,0
"If this is ""True"", we'll expand the former to the latter",0
Handling a one-off issue in which .'s were mysteriously replaced with -'s,0
"in our annotations.  This will be set dynamically, but I keep it here as",0
a constant to remind me to remove this code when we clean this issue up.,1
Used in the (rare) case where a bounding box was added to an image that was originally,0
annotated as empty,0
Used in the (rare) case where we added bounding boxes to an image with multiple species,0
"Should we include ambiguous bounding boxes (with the ""ambiguous"" category label)",0
in the output file?  Ambiguous boxes are boxes drawn on images with multiple species.,0
"%%  Read metadata from the master database, bounding boxes from the annotations file",0
"The bounding box .json file is in the format returned by our annotators, which is not",0
actually a fully-formed .json file; rather it's a series of .json objects,0
"Each element of annData is a dictionary corresponding to a single sequence, with keys:",0
,0
"annotations, categories, images",0
sequence = annData[0],0
%% Build convenience mappings,0
Image ID to images,0
Category ID to categories (referring to the database categories),0
"Image ID to categories (i.e., species labels)",0
Utility function we'll use to create annotations for images in empty,0
sequences (empty images in non-empty sequences already have annotations),0
"%% Reformat annotations, grabbing category IDs from the master database (prep)",0
iSequence = 0; sequence = annData[0],0
"%% Reformat annotations, grabbing category IDs from the master database (loop)",0
Make a copy here; we're going to manipulate the sequence annotations,0
when we need to add synthetic annotations for empty images,0
im = sequenceImages[0],0
Are there any annotations in this sequence?,0
Which images in this sequence have annotations?,0
For each image in this sequence...,0
imeritImageID = im['id'],0
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG,0
Confirm that the file exists,0
Hande a one-off issue with our annotations,0
datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG,0
,0
...had become:,0
,0
datasetsnapshotserengeti.seqASG000001a-frame0.imgS1_B06_R1_PICT0008.JPG,0
Does it look like we encountered this issue?,0
Convert:,0
,0
S1_B06_R1_PICT0008,0
,0
...to:,0
,0
S1/B06/B06_R1/S1_B06_R1_PICT0008,0
Make sure we only see each image once,0
Create empty annotations for empty images,0
,0
Here we use the *unmodified* file name,0
Create an empty annotation for this image,0
Annotations still use the annotation filename (not database ID) at this point;,0
these will get converted to database IDs below when we process the,0
whole sequence.,0
Sanity-check image size,0
"print('Warning: img {} was listed in DB as {}x{}, annotated as {}x{}, actual size{}x{}'.format(",0
"old_id,new_im['width'],new_im['height'],im['width'],im['height'],imgObj.width,imgObj.height))",0
...for each image in this sequence,0
For each annotation in this sequence...,0
ann = sequenceAnnotations[0],0
Prepare an annotation using the category ID from the database and,0
the bounding box from the annotations file,0
Maintain iMerit's annotation category,0
Generate an (arbitrary) ID for this annotation; the COCO format has a concept,0
"of annotation ID, but our annotation files don't",0
This was a one-off quirk with our file naming,1
We'll do special handling of images with multiple categories later,0
Store the annotation type (group/human/animal/empty),0
This annotation has no bounding box but the image wasn't originally,0
annotated as empty,0
This annotation has a bounding box but the image was originally,0
annotated as empty,0
unnormalize the bbox,0
... for each annotation in this sequence,0
... for each sequence,0
%% Post-processing,0
Count empty images,0
...for each file,0
%% Sanity-check empty images,0
,0
make_ss_annotation_image_folder.py,0
,0
Take a directory full of images with the very long filenames we give annotators:,0
,0
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension,0
,0
"...along with a COCO-camera-traps database referring to those files, and:",0
,0
1) Creates a new COCO-camera-traps database with the original filenames in them,0
(copying the annotations),0
,0
2) Optionally creates a new directory with those images named according to the,0
"Snapshot Serengeti naming convention, including complete relative paths.",0
,0
See convert_imerit_json_to_coco_json to see how we get from the original annotation,0
.json to a COCO-camera-traps database.,0
,0
%% Constants and imports,0
%% Configure files/paths,0
%% Read the annotations (referring to the old filenames),0
"%% Update filenames, optionally copying files",0
im = data['images'][0],0
For each image...,0
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG,0
"Find the image name, e.g. S1_B06_R1_PICT0008",0
Convert:,0
,0
S1_B06_R1_PICT0008.JPG,0
,0
...to:,0
,0
S1/B06/B06_R1/S1_B06_R1_PICT0008.JPG,0
...for each image,0
%% Write the revised database,0
,0
get_annotation_tool_link.py,0
,0
"Takes a COCO-camera-traps-style .json file with URLs already embedded, and prepares",0
a link to the visipedia annotation tool that reviews a subset of those images.,0
,0
,0
create_new_annotation_json.py,0
,0
"Creates a subset of a larger .json database, in this case specifically to pick some images",0
from Snapshot Serengeti.,0
,0
from utils import get_db_dicts,0
for seq in already_annotated:,0
seq_to_ims.pop(seq),0
remove already annotated images,0
add lion images,0
lion_seqs = cat_to_seqs[cat_to_id['lionMale']] + cat_to_seqs[cat_to_id['lionFemale']],0
#print(len(lion_seqs)),0
lion_seqs = [seq for seq in lion_seqs if seq not in already_annotated],0
#print(len(lion_seqs)),0
"lion_seqs_to_annotate.extend(random.sample(lion_seqs, min(len(lion_seqs),num_lions)))",0
print(len(lion_seqs_to_annotate)),0
num_elephants = 1000,0
elephant_seqs = cat_to_seqs[cat_to_id['elephant']],0
#print(len(lion_seqs)),0
elephant_seqs = [seq for seq in elephant_seqs if seq not in already_annotated],0
#print(len(lion_seqs)),0
"elephant_seqs_to_annotate = random.sample(elephant_seqs, num_elephants)",0
num_empty = 10,0
empty_seqs_to_annotate = [],0
for loc in loc_to_seqs:,0
empty_seqs = cats_per_location[loc][cat_to_id['empty']],0
empty_seqs = [seq for seq in empty_seqs if seq not in already_annotated],0
empty_seqs = [seq for seq in empty_seqs if seq_to_season[seq] in seasons_to_keep],0
"empty_seqs_to_annotate.extend(random.sample(empty_seqs, min(len(empty_seqs),num_empty)))",0
ims_to_annotate.extend(empty_ims_to_annotate),0
,0
filter_database.py,0
,0
"Look through a COCO-ct database and find images matching some crtieria, writing",0
a subset of images and annotations to a new file.,0
,0
%% Constants and imports,0
%% Configuration,0
%%  Read database and build up convenience mappings,0
Category ID to category name,0
Image ID to image info,0
Image ID to image path,0
%% Filter,0
ann = annotations[0],0
Is this a tiny box or a group annotation?,0
"x,y,w,h",0
All empty annotations should be classified as either empty or ambiguous,0
,0
"The ambiguous cases are basically minor misses on the annotators' part,",0
where two different small animals were present somewhere.,0
%% Write output file,0
,0
find_images_for_classes.py,0
,0
"Given a .json database, find images that are associated with one or more",0
classes.,0
,0
%% Constants and imports,0
%% Configuration,0
%%  Read database and build up convenience mappings,0
Category ID to category name,0
Image ID to image info,0
Image ID to image path,0
%% Look for target-class annotations,0
ann = annotations[0],0
,0
create_tfrecords_format.py,0
,0
This script creates a tfrecords file from a dataset in VOTT format.,0
%% Imports and environment,0
set up the filenames and annotations,0
This loop reads the bboxes and corresponding labels and assigns them,0
the correct image. Kind of slow at the moment...,0
If needed: merging all classes,0
bbox_labels = ['Animal' for _ in bbox_labels],0
BBox coords are stored in the format,0
"x_min (of width axis) y_min (of height axis), x_max, y_max",0
Coordinate system starts in top left corner,0
"In this framework, we need ('ymin', 'xmin', 'ymax', 'xmax') format",0
print out some stats,0
To make sure we loaded the bboxes correctly:,0
self.validate_bboxes(),0
For each image in the data set...,0
Make sure all are greater equal 0,0
%% Main tfrecord generation function,0
Propagate optional metadata to tfrecords,0
endfor each annotation for the current image,0
endfor each image,0
"Calculate number of shards to get the desired number of images per record,",0
ensure it is evenly divisible by the number of threads,0
%% Command-line driver,0
%% Driver,0
,0
make_tfrecords_cis_trans.py,0
,0
Given a .json file that contains a three-element list (train/val/test) of image IDs and a .json database that contains,0
"those image IDs, generates tfrecords whose filenames include ""train""/""val""/""test""",0
,0
"dataset = json.load(open('/ai4efs/databases/snapshotserengeti/oneclass/SnapshotSerengeti_Seasons_1_to_4_tfrecord_format_valid_ims.json','r'))",0
print('Creating trans_val tfrecords'),0
dataset = [im_id_to_im[idx] for idx in trans_val],0
,0
create_classification_tfrecords_from_json.py,0
,0
Called from make_tfrecords_cis_trans.py if you're running a classification experiment.,0
,0
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/',0
datafile = 'eccv_18_annotation_files_oneclass/CaltechCameraTrapsECCV18',0
image_file_root = datafolder+'eccv_18_all_images/',0
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/',0
datafolder = '/data/iwildcam/',0
datafile = 'combined_iwildcam_annotations_oneclass/eccv_train_and_imerit_2',0
need consecutive category ids,0
old_cat_id_to_new_cat_id = {categories[idx]['id']:idx+1 for idx in range(len(categories))},0
print(old_cat_id_to_new_cat_id),0
remove multiclass images,0
print(images[0]),0
print(vis_data[0]),0
,0
make_tfrecords_from_json.py,0
,0
"Given a coco-camera-traps .json file, creates tfrecords",0
,0
Thin wrapper for create_tfrecords_from_json.,0
,0
%% Constants and imports,0
%% Constants and imports (interactive),0
%%,0
%% Main tfrecord generation function,0
"check whether the input file has already been converted to the tfrecords format,",0
"if not, convert",0
"Calculate number of shards to get the desired number of images per record,",0
ensure it is evenly divisible by the number of threads,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
eMammal_make_splits.py,0
,0
"Based on a tfrecords_format json file of the database, creates 3 splits according to",0
the specified fractions based on location (images from the same location should be in,0
one split) or based on images.,0
,0
"If a previous split is provided (append_to_previous_split is True), the entries in",0
"each split will be preserved, and new entries will be appended, so that new models",0
can warm start with a model trained on the original splits.,0
configurations and paths,0
approximate fraction for the new entries,0
read in the previous splits of image ID or location ID if available,0
"find new locations and assign them to a split, without reassigning any previous locations",0
"find out which images are new, shuffle and split them",0
do NOT sort the IDs to keep the shuffled order,0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
add empty category,0
"add all images that don't have annotations, with cat empty",0
print(detection_results.keys()),0
group the detections and gts by image id:,0
print(image_id),0
print(len(scores)),0
print(len(labels)),0
recall_thresh = 0.9,0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall]),0
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])",0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall]),0
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])",0
exp_name = 'eccv_train',0
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json',0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(len(detection_results['images'])),0
print(len(seqs)),0
"print(len(seqs[0]),len(seqs[",0
print(detection_results.keys()),0
group the detections by image id:,0
group the ground truth annotations by image id:,0
print(seq),0
"for image_id, dets in per_image_detections.iteritems():",0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
print(num_gts),0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
if sum(detected_class_labels)>0:,0
print('detected animal box'),0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])",0
"print(scores, tp_fp_labels)",0
num_total_gts += num_gts,0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
print('valid box'),0
"print(best_im, best_score)",0
exp_name = 'eccv_train',0
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json',0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
need to loop over confidence values,0
"for each value, check if any detections on the image are > conf",0
"If so, that image gets class ""animal""",0
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0",0
"for image_id, dets in per_image_detections.iteritems():",0
"calc prec, rec for this confidence thresh",0
group the ground truth annotations by image id:,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(detection_results.keys()),0
group the detections and gts by image id:,0
print(image_id),0
print(len(scores)),0
print(len(labels)),0
recall_thresh = 0.9,0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall]),0
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])",0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall]),0
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])",0
print(sorted_ap),0
plt.bar(sorted_ap),0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(detection_results.keys()),0
group the detections by image id:,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
print(gts),0
print(num_gts),0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[:2])",0
"print(scores, tp_fp_labels)",0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
print('detected animal box'),0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])",0
"print(scores, tp_fp_labels)",0
num_total_gts += num_gts,0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
print('valid box'),0
best_im = np.argmax(max_im_scores),0
"print(best_im, best_score)",0
"temp_labels = np.zeros(len(im_detection_labels),  dtype=np.int32)",0
"temp_scores = np.zeros(len(im_detection_scores), dtype=np.float32)",0
"for j in range(min(im_num_gts, len(im_detection_labels))):",0
temp_labels[j] = True #TODO: this currently only works for oneclass?,1
temp_scores[j] = best_score,0
im_detection_labels = temp_labels,0
im_detection_scores = temp_scores,0
num_total_gts+=im_num_gts,0
"print(len(detection_scores), len(detection_scores[0]), len(detection_scores[1]))",0
exp_name = 'small_balanced_cct',0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(seq),0
"for image_id, dets in per_image_detections.iteritems():",0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
seq_num_gts.append(num_gts),0
print(num_gts),0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
num_total_gts += 1,0
print('valid box'),0
"print(best_im, best_score)",0
print('no valid box'),0
if sum(seq_num_gts)>0:,0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
need to loop over confidence values,0
"for each value, check if any detections on the image are > conf",0
"If so, that image gets class ""animal""",0
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0",0
"calc prec, rec for this confidence thresh",0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
group the detections by image id:,0
group the ground truth annotations by image id:,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
print('detected animal box'),0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])",0
"print(scores, tp_fp_labels)",0
num_total_gts += num_gts,0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
print('valid box'),0
"print(best_im, best_score)",0
,0
evaluate_detections.py,0
,0
Adapted from analyze_detection.py which is now archived.,0
,0
%% Imports and constants,0
%% Functions,0
"labels input to compute_object_detection_metrics() needs to start at 0, not 1",0
num_detections = len(dets['boxes']),0
to prevent 'Invalid dimensions for box data.' error,0
this box will not match any detections,0
compute one-class precision/recall/average precision (if every box is just of an object class),0
%% Command-line driver,0
,0
detection_eval_utils.py,0
,0
Utility functions used in evaluate_detections.py,0
,0
group the ground truth annotations by image id,0
#####,0
,0
detect_and_predict_image.py,0
,0
"Functions to load a TensorFlow detection and a classification model, run inference,",0
"render bounding boxes on images, and write out the resulting",0
images (with bounding boxes and classes).,0
,0
"See the ""test driver"" cell for example invocation.",0
,0
,0
#####,0
"%% Constants, imports, environment",0
Minimum detection confidence for showing a bounding box on the output image,0
Stick this into filenames before the extension for the rendered result,0
Number of top-scoring classes to show at each bounding box,0
%% Core detection functions,0
Load images if they're not already numpy arrays,0
iImage = 0; image = images[iImage],0
"Load the image as an nparray of size h,w,nChannels",0
"There was a time when I was loading with PIL and switched to mpimg,",0
"but I can't remember why, and converting to RGB is a very good reason",0
"to load with PIL, since mpimg doesn't give any indication of color",0
"order, which basically breaks all .png files.",0
,0
"So if you find a bug related to using PIL, update this comment",0
"to indicate what it was, but also disable .png support.",0
image = mpimg.imread(image),0
This shouldn't be necessarily when loading with PIL and converting to RGB,0
Actual detection,0
...for each image,0
"Currently ""boxes"" is a list of length nImages, where each element is shaped as",0
,0
"1,nDetections,4",0
,0
"This implicitly banks on TF giving us back a fixed number of boxes, let's assert on this",0
to make sure this doesn't silently break in the future.,0
iBox = 0; box = boxes[iBox],0
"""scores"" is a length-nImages list of elements with size 1,nDetections",0
"""classes"" is a length-nImages list of elements with size 1,nDetections",0
,0
"Still as floats, but really representing ints",0
Squeeze out the empty axis,0
boxes is nImages x nDetections x 4,0
scores and classes are both nImages x nDetections,0
Get input and output tensors of classification model,0
"imsize = cur_image['width'], cur_image['height']",0
Select detections with a confidence larger 0.5,0
Get these boxes and convert normalized coordinates to pixel coordinates,0
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes",0
"However, we need to make sure that it box coordinates are still within the image",0
For convenience:,0
Create an array with contains the index of the corresponding crop_box for each selected box,0
i.e. [False False 0 False 1 2 3 False False],0
For each box,0
If this box should be classified,0
Run inference,0
if box should not be classified,0
...for each box,0
species_scores should have shape len(images) x len(boxes) x num_species,0
...for each image,0
...with tf.Session,0
with classification_graph,0
species_scores should have shape len(images) x len(boxes) x num_species,0
%% Rendering functions,0
Display the image,0
plt.show(),0
"top, left, bottom, right",0
,0
"x,y origin is the upper-left",0
Generate bounding box text,0
Choose color based on class,0
Location is the bottom-left of the rect,0
,0
Origin is the upper-left,0
iRight = x + w,0
iTop = y + h,0
Add the patch to the Axes,0
Add class description,0
First determine best location by finding the corner that is closest to the image center,0
relative corner coordinates,0
relative coordinates of image center,0
Compute pair-wise squared distance and get the index of the one with minimal distance,0
Get the corresponding coordinates ...,0
... and alignment for the text box,0
Plot the text box with background,0
...for each box,0
This is magic goop that removes whitespace around image plots (sort of),0
"plt.savefig(outputFileName, bbox_inches='tight', pad_inches=0.0, dpi=dpi, transparent=True)",0
os.startfile(outputFileName),0
...for each image,0
...def render_bounding_boxes,0
Load and run detector on target images,0
Read the name of all classes,0
remove empty lines,0
%% Interactive driver,0
%%,0
%%,0
%% File helper functions,0
%% Command-line driver,0
Hack to avoid running on already-detected images,1
,0
"Script for evaluating a frozen graph given a COCO-style dataset, which was generated by",0
the ../data_management/databases/classification/make_classification_dataset.py script.,0
This is useful for debugging as the accuracy reported by this script should match the,0
accuracy reported by the Tensorflow training.,0
,0
Check that all files exists for easier debugging,0
Load frozen graph,0
Collect tensors for input and output,0
Read image,0
Run inference,0
Check if correct,0
,0
Script for evaluating precision/recall on a two-class problem given,0
"a frozen graph and a COCO-style dataset, which was generated by",0
the ../data_management/databases/classification/make_classification_dataset.py script.,0
We assume the positive class is at index 0 (with a zero-based indexing),0
,0
Check that all files exists for easier debugging,0
Load frozen graph,0
Collect tensors for input and output,0
Read image,0
"with open(image_path, 'rb') as fi:",0
"image =  sess.run(tf.image.decode_jpeg(fi.read(), channels=3))",0
image = image / 255.,0
Run inference,0
predicted_class = np.argmax(predictions),0
Check if correct,0
if coco.imgToAnns[image_id][0]['category_id'] == predicted_class:,0
correct = correct + 1,0
"In matplotlib < 1.5, plt.fill_between does not have a 'step' argument",0
,0
Script for selecting testing images of a COCO-style dataset generated by the script,0
../data_management/databases/classification/make_classification_dataset.py in a consistent,0
manner and predicting the class for it.,0
,0
Mandatory parameters,0
Optional parameters,0
Validate parameters,0
Derived parameters,0
"We assume that the dataset was generated with the make_classification_dataset.py script,",0
hence the images should be located in the same folder as the json,0
Make seletion deterministic,0
Load frozen graph,0
Get dataset information,0
Get classes,0
...and the class list corresponding to the model outputs by assuming,0
that they are in order of their ids,0
Get images of each class,0
Shuffle the image list,0
Start the image sampling,0
"Set of avaiable class IDs, will be filled below",0
If there are still images left for that class,0
"Get image for the sampled class, we already shuffled the class images before so",0
we can simply pop(),0
Start prediction,0
Collect tensors for input and output,0
Read image,0
Run inference,0
Print output to log file,0
#####,0
,0
api_apply_classifier_single_node.py,0
,0
Takes the JSON file produced by the detection API and,0
classifies all boxes above a confidence threshold.,0
,0
#####,0
"%% Constants, imports, environment",0
Assumes that the root of the CameraTraps repo is on the PYTHONPATH,0
Minimum detection confidence for classifying an object,0
Number of top-scoring classes to show at each bounding box,0
Enlargment factor applied to boxes before passing them to the classifier,0
,0
Provides more context and can lead to better results,0
List of detection categories for which we will run the classification,0
,0
"Currently there are {""1"": ""animal"", ""2"": ""person"", ""4"": ""vehicle""}",0
,0
Should be a list of string-formatted ints.,0
Number of significant float digits in JSON output,0
%% Core detection functions,0
Read the name of all classes,0
remove empty lines,0
Create field with name *classification_categories*,0
Add classes using 0-based indexing,0
def add_classification_categories,0
Make sure we have the right json object,0
Get input and output tensors of classification model,0
For each image,0
Read image,0
"Scale pixel values to [0,1]",0
For each box,0
Skip detections with low confidence,0
Skip if detection category is not in whitelist,0
Skip if already classified,0
"Get current box in relative coordinates and format [x_min, y_min, width_of_box, height_of_box]",0
"Convert to [ymin, xmin, ymax, xmax] and store it as 1x4 numpy array so we can",0
re-use the generic multi-box padding code,0
Convert normalized coordinates to pixel coordinates,0
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes.",0
,0
"However, we need to make sure that it box coordinates are still within the image.",0
Get the first (and only) row as our bbox to classify,0
Get the image data for that box,0
Run inference,0
Add an empty list to the json for our predictions,0
Add the *num_annotated_classes* top scoring classes,0
...for each box,0
...for each image,0
...with tf.Session,0
with classification_graph,0
def classify_boxes,0
Load classification model,0
Load detector json,0
Add classes to detector_json,0
"Run classifier on all images, changes will be writting directly to the json",0
Write output json,0
def load_and_run_classifier,0
%% Command-line driver,0
,0
Script for selecting testing images of a COCO-style dataset generated by the script,0
../data_management/databases/classification/make_classification_dataset.py in a consistent,0
manner and predicting the class for it.,0
,0
Assumes the cameratraps repo root is on the path,0
Make seletion deterministic,0
Mandatory parameters,0
Optional parameters,0
Validate parameters,0
Tranfer parameters to post-processing format,0
Load frozen graph,0
Reading image list,0
Reading class list,0
Image sampling,0
Start prediction,0
Collect tensors for input and output,0
Array for collecting infos for rendering the html,0
Read image,0
"Scale pixel values to [0,1]",0
Run inference,0
Add links to all available classes,0
,0
predict_image.py,0
,0
"Given a pointer to a frozen detection graph, runs inference on a single image,",0
printing the top classes to the console,0
,0
%% Imports,0
%% Command-line processing,0
Check that all files exist for easier debugging,0
%% Inference,0
Load frozen graph,0
Load class list,0
Remove empty lines,0
Collect tensors for input and output,0
Read image,0
Run inference,0
Print output,0
,0
Mostly unmodified script for freezing a model,0
Added for convenience and for possible future optimizations,0
,0
Copyright 2015 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
'input_checkpoint' may be a prefix if we're using Saver V2 format,0
Remove all the explicit device specifications for this node. This helps to,0
make the graph more portable.,0
List of all partition variables. Because the condition is heuristic,0
"based, the list could include false positives.",0
This tensor doesn't exist in the graph (for example it's,0
'global_step' or a similar housekeeping element) so skip it.,0
`var_list` is required to be a map of variable names to Variable,0
tensors. Partition variables are Identity tensors that cannot be,0
handled by Saver.,0
Models that have been frozen previously do not contain Variables.,0
optimized_output_graph_def = optimize_for_inference_lib.optimize_for_inference(,0
"output_graph_def,",0
"input_node_names.replace("" "", """").split("",""),",0
"output_node_names.replace("" "", """").split("",""),",0
tf.float32.as_datatype_enum),0
Write GraphDef to file if output path has been given.,0
,0
"Creates a graph description, which is required to create a frozen graph.",0
Adapted from from ./tf-slim/export_inference_graph.py,0
Added preprocessing to the definition for easier handling,0
,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
tf.app.flags.DEFINE_integer(,0
"'batch_size', None,",0
"'Batch size for the exported model. Defaulted to ""None"" so batch size can '",0
'be specified at model runtime.'),0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
#####################,0
Optimization Flags #,0
#####################,0
######################,0
Learning Rate Flags #,0
######################,0
######################,0
Dataset Flags #,0
######################,0
####################,0
Fine-Tuning Flags #,0
####################,0
"Note: when num_clones is > 1, this will actually have each clone to go",0
over each epoch FLAGS.num_epochs_per_decay times. This is different,0
behavior from sync replicas and is expected to produce different results.,0
Warn the user if a checkpoint exists in the train_dir. Then we'll be,0
ignoring the checkpoint anyway.,0
TODO(sguada) variables.filter_variables(),1
######################,0
Config model_deploy #,0
######################,0
Create global_step,0
#####################,0
Select the dataset #,0
#####################,0
#####################,0
Select the network #,0
#####################,0
####################################,0
Select the preprocessing function #,0
####################################,0
#############################################################,0
Create a dataset provider that loads data from the dataset #,0
#############################################################,0
###################,0
Define the model #,0
###################,0
############################,0
Specify the loss function #,0
############################,0
Gather initial summaries.,0
"Gather update_ops from the first clone. These contain, for example,",0
the updates for the batch_norm variables created by network_fn.,0
Add summaries for end_points.,0
Add summaries for losses.,0
Add summaries for variables.,0
################################,0
Configure the moving averages #,0
################################,0
########################################,0
Configure the optimization procedure. #,0
########################################,0
"If sync_replicas is enabled, the averaging will be done in the chief",0
queue runner.,0
Update ops executed locally by trainer.,0
Variables to train.,0
and returns a train_tensor and summary_op,0
Add total_loss to summary.,0
Create gradient updates.,0
Add the summaries from the first clone. These contain the summaries,0
created by model_fn and either optimize_clones() or _gather_clone_loss().,0
Merge all summaries together.,0
##########################,0
Kicks off the training. #,0
##########################,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
#####################,0
Select the dataset #,0
#####################,0
###################,0
Select the model #,0
###################,0
#############################################################,0
Create a dataset provider that loads data from the dataset #,0
#############################################################,0
####################################,0
Select the preprocessing function #,0
####################################,0
###################,0
Define the model #,0
###################,0
Define the metrics:,0
Print the summaries to screen.,0
TODO(sguada) use num_epochs=1,1
This ensures that we make a single pass over all of the data.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Transform the image to floats.,0
"Randomly crop a [height, width] section of the image.",0
Randomly flip the image horizontally.,0
"Because these operations are not commutative, consider randomizing",0
the order their operation.,0
Subtract off the mean and divide by the variance of the pixels.,0
Transform the image to floats.,0
Resize and crop if needed.,0
Subtract off the mean and divide by the variance of the pixels.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Use tf.slice instead of crop_to_bounding box as it accepts tensors to,0
define the crop size.,0
Compute the rank assertions.,0
Create a random bounding box.,0
,0
Use tf.random_uniform and not numpy.random.rand as doing the former would,0
"generate random numbers at graph eval time, unlike the latter which",0
generates random numbers at graph definition time.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Pass the real x only to one of the func calls.,0
The random_* ops do not necessarily clamp.,0
"Each bounding box has shape [1, num_boxes, box coords] and",0
"the coordinates are ordered [ymin, xmin, ymax, xmax].",0
A large fraction of image datasets contain a human-annotated bounding,0
box delineating the region of the image containing the object of interest.,0
We choose to create a new bounding box for the object which is a randomly,0
distorted version of the human-annotated bounding box that obeys an,0
"allowed range of aspect ratios, sizes and overlap with the human-annotated",0
"bounding box. If no box is supplied, then we assume the bounding box is",0
the entire image.,0
Crop the image to the specified bounding box.,0
"Each bounding box has shape [1, num_boxes, box coords] and",0
"the coordinates are ordered [ymin, xmin, ymax, xmax].",0
Restore the shape since the dynamic slice based upon the bbox_size loses,0
the third dimension.,0
This resizing operation may distort the images because the aspect,0
ratio is not respected. We select a resize method in a round robin,0
fashion based on the thread number.,0
Note that ResizeMethod contains 4 enumerated resizing methods.,0
We select only 1 case for fast_mode bilinear.,0
Randomly flip the image horizontally.,0
Randomly distort the colors. There are 1 or 4 ways to do it.,0
Crop the central region of the image with an area containing 87.5% of,0
the original image.,0
Resize the image to the specified height and width.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TODO(sguada) docstring paragraph by (a) motivating the need for the file and,1
(b) defining clones.,0
TODO(sguada) describe the high-level components of model deployment.,1
"E.g. ""each model deployment is composed of several parts: a DeploymentConfig,",0
"which captures A, B and C, an input_fn which loads data.. etc",0
Set up DeploymentConfig,0
Create the global step on the device storing the variables.,0
Define the inputs,0
Define the optimizer.,0
Define the model including the loss.,0
Run training.,0
Namedtuple used to represent a clone during deployment.,0
"Namedtuple used to represent a DeployedModel, returned by deploy().",0
Default parameters for DeploymentConfig,0
Create clones.,0
The return value.,0
Individual components of the loss that will need summaries.,0
Compute and aggregate losses on the clone device.,0
Add the summaries out of the clone device block.,0
Only use regularization_losses for the first clone,0
Compute the total_loss summing all the clones_losses.,0
Sum the gradients across clones.,0
Gather initial summaries.,0
Create Clones.,0
"Gather update_ops from the first clone. These contain, for example,",0
the updates for the batch_norm variables created by model_fn.,0
Place the global step on the device storing the variables.,0
Compute the gradients for the clones.,0
Add summaries to the gradients.,0
Create gradient updates.,0
Only use regularization_losses for the first clone,0
Add the summaries from the first clone. These contain the summaries,0
created by model_fn and either optimize_clones() or _gather_clone_loss().,0
Add total_loss to summary.,0
Merge all summaries together.,0
Note that each grad_and_vars looks like the following:,0
"((grad_var0_clone0, var0), ... (grad_varN_cloneN, varN))",0
!/usr/bin/python,0
Copyright 2016 Google Inc. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Failed to find ""index"" occurrence of item.",0
pylint: disable=broad-except,0
pylint: enable=broad-except,0
Grab the 'index' annotation.,0
Some images contain bounding box annotations that,0
"extend outside of the supplied image. See, e.g.",0
n03127925/n03127925_147.xml,0
"Additionally, for some bounding boxes, the min > max",0
or the box is entirely outside of the image.,0
Example: <...>/n06470073/n00141669_6790.xml,0
Determine if the annotation is from an ImageNet Challenge label.,0
Note: There is a slight bug in the bounding box annotation data.,0
Many of the dog labels have the human label 'Scottish_deerhound',0
instead of the synset ID 'n02092002' in the bbox.label field. As a,0
"simple hack to overcome this issue, we only exclude bbox labels",1
*which are synset ID's* that do not match original synset label for,0
the XML file.,0
Guard against improperly specified boxes.,0
Note bbox.filename occasionally contains '%s' in the name. This is,0
data set noise that is fixed by just using the basename of the XML file.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The URLs where the MNIST data can be downloaded.,0
The names of the classes.,0
"First, process the training data:",0
"Next, process the testing data:",0
"Finally, write the labels file:",0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The URL where the Flowers data can be downloaded.,0
The number of images in the validation set.,0
Seed for repeatability.,0
The number of shards per dataset split.,0
Initializes function that decodes RGB JPEG data.,0
Read the filename:,0
Divide into train and test:,0
"First, convert the training and validation sets.",0
"Finally, write the labels file:",0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TODO(nsilberman): Add tfrecord file type once the script is updated.,1
"If set to false, will not try to set label_to_names in dataset",0
by reading them from labels.txt or github.,0
n01440764,0
n01443537,0
n02119247    black fox,0
n02119359    silver fox,0
pylint: disable=g-line-too-long,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The URL where the CIFAR data can be downloaded.,0
The number of training files.,0
The height and width of each image.,0
The names of the classes.,0
"First, process the training data:",0
"Next, process the testing data:",0
"Finally, write the labels file:",0
Copyright 2016 Google Inc. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The labels file contains a list of valid labels are held in this file.,0
Assumes that the file contains entries as such:,0
n01440764,0
n01443537,0
n01484850,0
where each line corresponds to a label expressed as a synset. We map,0
each synset contained in the file to an integer (based on the alphabetical,0
ordering). See below for details.,0
This file containing mapping from synset to human-readable label.,0
Assumes each line of the file looks like:,0
,0
n02119247    black fox,0
n02119359    silver fox,0
"n02119477    red fox, Vulpes fulva",0
,0
where each line corresponds to a unique mapping. Note that each line is,0
formatted as <synset>\t<human readable label>.,0
This file is the output of process_bounding_box.py,0
Assumes each line of the file looks like:,0
,0
"n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940",0
,0
where each line corresponds to one bounding box annotation associated,0
with an image. Each line can be parsed as:,0
,0
"<JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>",0
,0
Note that there might exist mulitple bounding box annotations associated,0
with an image file.,0
pylint: disable=expression-not-assigned,0
pylint: enable=expression-not-assigned,0
Create a single Session to run all image coding calls.,0
Initializes function that converts PNG to JPEG data.,0
Initializes function that converts CMYK JPEG data to RGB JPEG data.,0
Initializes function that decodes RGB JPEG data.,0
File list from:,0
https://groups.google.com/forum/embed/?place=forum/torch7#!topic/torch7/fOSTXHIESSU,0
File list from:,0
https://github.com/cytsai/ilsvrc-cmyk-image-list,0
Read the image file.,0
Clean the dirty data.,0
1 image is a PNG.,0
22 JPEG images are in CMYK colorspace.,0
Decode the RGB JPEG.,0
Check that image converted to RGB,0
Each thread produces N shards where N = int(num_shards / num_threads).,0
"For instance, if num_shards = 128, and the num_threads = 2, then the first",0
"thread would produce shards [0, 64).",0
"Generate a sharded version of the file name, e.g. 'train-00002-of-00010'",0
"Break all images into batches with a [ranges[i][0], ranges[i][1]].",0
Launch a thread for each batch.,0
Create a mechanism for monitoring when all threads are finished.,0
Create a generic TensorFlow-based utility for converting all image codings.,0
Wait for all the threads to terminate.,0
Leave label index 0 empty as a background class.,0
Construct the list of JPEG files and labels.,0
Shuffle the ordering of all image files in order to guarantee,0
random ordering of the images with respect to label in the,0
saved TFRecord files. Make the randomization repeatable.,0
Build a map from synset to human-readable label.,0
Run it!,0
Allowing None in the signature so that dataset_factory can use the default.,0
!/usr/bin/python,0
Copyright 2016 Google Inc. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Read in the 50000 synsets associated with the validation data set.,0
Make all sub-directories in the validation data dir.,0
Move all of the image to the appropriate sub-directory.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
"The dataset has classes with no images (empty and human), which have ID 0 and 1,",0
so we need to specify 49 here despite having only 47 classes with images,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Final pooling and prediction,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The current_stride variable keeps track of the effective stride of the,0
activations. This allows us to invoke atrous convolution whenever applying,0
the next residual unit would result in the activations having stride larger,0
than the target output_stride.,0
The atrous convolution rate parameter.,0
Move stride from the block's last unit to the end of the block.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Collect activations at the block's end before performing subsampling.,0
Subsampling of the block's output activations.,0
"The following implies padding='SAME' for pool1, which makes feature",0
alignment easier for dense prediction tasks. This is also used in,0
https://github.com/facebook/fb.resnet.torch. However the accompanying,0
code of 'Deep Residual Learning for Image Recognition' uses,0
padding='VALID' for pool1. You can switch to that choice by setting,0
"slim.arg_scope([slim.max_pool2d], padding='VALID').",0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Collect outputs for conv2d, fully_connected and max_pool2d",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"inputs has shape [batch, 224, 224, 3]",0
"inputs has shape [batch, 513, 513, 3]",0
We do not include batch normalization or activation functions in,0
conv1 because the first ResNet unit will perform these. Cf.,0
Appendix of [2].,0
This is needed because the pre-activation variant does not have batch,1
normalization or activation functions in the residual unit output. See,0
Appendix of [2].,0
Convert end_points_collection into a dictionary of end_points.,0
Global average pooling.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
=============================================================================,0
"These parameters come from the online port, which don't necessarily match",0
those in the paper.,0
TODO(nsilberman): confirm these values with Philip.,1
##########,0
Encoder #,0
##########,0
"No normalizer for the first encoder layers as per 'Image-to-Image',",0
Section 5.1.1,0
First layer doesn't use normalizer_fn,0
Last layer doesn't use activation_fn nor normalizer_fn,0
##########,0
Decoder #,0
##########,0
"Dropout is used at both train and test time as per 'Image-to-Image',",0
Section 2.1 (last paragraph).,0
The Relu comes BEFORE the upsample op:,0
Explicitly set the normalizer_fn to None to override any default value,0
"that may come from an arg_scope, such as pix2pix_arg_scope.",0
No normalization on the input layer.,0
Stride 1 on the last layer.,0
"1-dim logits, stride 1, no activation, no normalization.",0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
Alias,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pylint: disable=unused-import,0
pylint: enable=unused-import,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
If we are fine tuning a checkpoint we need to start at a lower learning,0
rate since we are farther along on training.,0
We can start quantizing immediately if we are finetuning.,0
We need to wait for the model to train a bit before we quantize if we are,0
training from scratch.,0
Call rewriter to produce graph with fake quant ops and folded batch norms,0
"quant_delay delays start of quantization till quant_delay steps, allowing",0
for better model accuracy.,0
Configure the learning rate using an exponential decay.,0
"When restoring from a floating point model, the min/max values for",0
quantized weights and activations are not present.,0
We instruct slim to ignore variables that are missing during restoration,0
by setting ignore_missing_vars=True,0
"If we are restoring from a floating point model, we need to initialize",0
the global step to zero for the exponential decay to result in,0
reasonable learning rates.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Orignaly, arg_scope = slim.arg_scope and layers = slim, now switch to more",0
update-to-date tf.contrib.* API.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Turns off fused batch norm.,0
collection containing the moving mean and moving variance.,0
"For inverted pyramid models, we start with gating switched off.",0
batch_size x 32 x 112 x 112 x 64,0
Separable conv is slow when used at first conv layer.,0
batch_size x 32 x 56 x 56 x 64,0
batch_size x 32 x 56 x 56 x 64,0
batch_size x 32 x 56 x 56 x 192,0
batch_size x 32 x 28 x 28 x 192,0
batch_size x 32 x 28 x 28 x 256,0
batch_size x 16 x 14 x 14 x 512,0
batch_size x 16 x 14 x 14 x 512,0
batch_size x 16 x 14 x 14 x 512,0
batch_size x 16 x 14 x 14 x 528,0
batch_size x 16 x 14 x 14 x 832,0
batch_size x 8 x 7 x 7 x 832,0
batch_size x 8 x 7 x 7 x 1024,0
Final pooling and prediction,0
Temporal average pooling.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
299 x 299 x 3,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 160,0
71 x 71 x 192,0
35 x 35 x 384,0
4 x Inception-A blocks,0
35 x 35 x 384,0
Reduction-A block,0
17 x 17 x 1024,0
7 x Inception-B blocks,0
17 x 17 x 1024,0
Reduction-B block,0
8 x 8 x 1536,0
3 x Inception-C blocks,0
Auxiliary Head logits,0
17 x 17 x 1024,0
Final pooling and prediction,0
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which",1
can be set to False to disable pooling here (as in resnet_*()).,0
8 x 8 x 1536,0
1 x 1 x 1536,0
1536,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN,1
setups need the gradient of gradient FusedBatchNormGrad.,0
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN,1
setups need the gradient of gradient FusedBatchNormGrad.,0
First upscaling is different because it takes the input vector.,0
Last layer has different normalizer and activation.,0
Convert to proper channels.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80,0
71 x 71 x 192,0
35 x 35 x 192,0
35 x 35 x 320,0
TODO(alemi): Register intermediate endpoints,1
"17 x 17 x 1088 if output_stride == 8,",0
33 x 33 x 1088 if output_stride == 16,0
TODO(alemi): register intermediate endpoints,1
TODO(gpapan): Properly support output_stride for the rest of the net.,1
8 x 8 x 2080,0
TODO(alemi): register intermediate endpoints,1
8 x 8 x 1536,0
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which",1
can be set to False to disable pooling here (as in resnet_*()).,0
Set weight_decay for weights in conv2d and fully_connected layers.,0
Set activation_fn and parameters for batch_norm.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
collection containing update_ops.,0
use fused batch norm if possible.,0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"inputs has shape [batch, 224, 224, 3]",0
"inputs has shape [batch, 513, 513, 3]",0
Use clip_by_value to simulate bandpass activation.,0
Convert end_points_collection into a dictionary of end_points.,0
Global average pooling.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"end_points will collect relevant activations for external use, for example",0
summaries or losses.,0
299 x 299 x 3,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80.,0
71 x 71 x 192.,0
35 x 35 x 192.,0
Inception blocks,0
mixed: 35 x 35 x 256.,0
mixed_1: 35 x 35 x 288.,0
mixed_2: 35 x 35 x 288.,0
mixed_3: 17 x 17 x 768.,0
mixed4: 17 x 17 x 768.,0
mixed_5: 17 x 17 x 768.,0
mixed_6: 17 x 17 x 768.,0
mixed_7: 17 x 17 x 768.,0
mixed_8: 8 x 8 x 1280.,0
mixed_9: 8 x 8 x 2048.,0
mixed_10: 8 x 8 x 2048.,0
Auxiliary Head logits,0
Shape of feature map before the final layer.,0
Final pooling and prediction,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 2048,0
2048,0
1000,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Reflection pad by 1 in spatial dimensions (axes 1, 2 = h, w) to make a 3x3",0
'valid' convolution produce an output with the same dimension as the,0
input.,0
This corrects 1 pixel offset for images with even width and height.,0
conv2d is left aligned and conv2d_transpose is right aligned for even,0
sized images (while doing 'SAME' padding).,0
Note: This doesn't reflect actual model in paper.,0
Neither dropout nor batch norm -> dont need is_training,0
##########,0
Encoder #,0
##########,0
7x7 input stage,0
##################,0
Residual Blocks #,0
##################,0
##########,0
Decoder #,0
##########,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"end_points will collect relevant activations for external use, for example",0
summaries or losses.,0
Used to find thinned depths for each layer.,0
Note that sizes in the comments below assume an input spatial size of,0
"224x224, however, the inputs can be of any size greater 32x32.",0
224 x 224 x 3,0
depthwise_multiplier here is different from depth_multiplier.,0
depthwise_multiplier determines the output channels of the initial,0
"depthwise conv (see docs for tf.nn.separable_conv2d), while",0
depth_multiplier controls the # channels of the subsequent 1x1,0
convolution. Must have,0
in_channels * depthwise_multipler <= out_channels,0
so that the separable convolution is not overparameterized.,0
Use a normal convolution instead of a separable convolution.,0
112 x 112 x 64,0
56 x 56 x 64,0
56 x 56 x 64,0
56 x 56 x 192,0
28 x 28 x 192,0
Inception module.,0
28 x 28 x 256,0
28 x 28 x 320,0
14 x 14 x 576,0
14 x 14 x 576,0
14 x 14 x 576,0
14 x 14 x 576,0
14 x 14 x 576,0
7 x 7 x 1024,0
7 x 7 x 1024,0
Final pooling and prediction,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 1024,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
=============================================================================,0
Tensorflow mandates these.,0
Conv and DepthSepConv namedtuple define layers of the MobileNet architecture,0
Conv defines 3x3 convolution layers,0
DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.,0
stride is the stride of the convolution,0
depth is the number of channels or filters in a layer,0
MOBILENETV1_CONV_DEFS specifies the MobileNet body,0
Used to find thinned depths for each layer.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
By passing filters=None,0
separable_conv2d produces only a depthwise convolution layer,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 1024,0
Set weight_decay for weights in Conv and DepthSepConv layers.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Turns off fused batch norm.,0
collection containing the moving mean and moving variance.,0
Final pooling and prediction,0
Temporal average pooling.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Orignaly, add_arg_scope = slim.add_arg_scope and layers = slim, now switch to",0
more update-to-date tf.contrib.* API.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
Only insert rate to params if rate > 1.,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Notes for training NASNet Cifar Model,0
-------------------------------------,0
batch_size: 32,0
learning rate: 0.025,0
cosine (single period) learning rate decay,0
auxiliary head loss weighting: 0.4,0
clip global norm of all gradients by 5,0
600 epochs with a batch size of 32,0
This is used for the drop path probabilities since it needs to increase,0
the drop out probability over the course of training.,0
Notes for training large NASNet model on ImageNet,0
-------------------------------------,0
batch size (per replica): 16,0
learning rate: 0.015 * 100,0
learning rate decay factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 100 replicas,0
auxiliary head loss weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Notes for training the mobile NASNet ImageNet model,0
-------------------------------------,0
batch size (per replica): 32,0
learning rate: 0.04 * 50,0
learning rate scaling factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 50 replicas,0
auxiliary head weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Shape of feature map before the final layer.,0
149 x 149 x 32,0
Run the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Find where to place the reduction cells or stride normal cells,0
Setup for building in the auxiliary head.,0
Run the cells,0
true_cell_num accounts for the stem cells,0
Final softmax layer,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Find where to place the reduction cells or stride normal cells,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Setup for building in the auxiliary head.,0
Run the cells,0
true_cell_num accounts for the stem cells,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Final softmax layer,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Calculate the total number of cells in the network.,0
There is no distinction between reduction and normal cells in PNAS so the,0
total number of cells is equal to the number normal cells plus the number,0
of stem cells (two by default).,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Calculate the total number of cells in the network.,0
There is no distinction between reduction and normal cells in PNAS so the,0
total number of cells is equal to the number normal cells plus the number,0
of stem cells (two by default).,0
Configuration for the PNASNet-5 model.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"The cap for tf.clip_by_value, it's hinted from the activation distribution",0
"that the majority of activation values are in the range [-6, 6].",0
Skip path 1,0
Skip path 2,0
"First pad with 0's on the right and bottom, then shift the filter to",0
include those 0's that were added.,0
"If odd number of filters, add an additional one to the second path.",0
Concat and apply BN,0
Set the prev layer to the current layer if it is none,0
Check to be sure prev layer stuff is setup correctly,0
num_or_size_splits=1,0
Apply conv operations,0
Combine hidden states using 'add'.,0
Add hiddenstate to the list of hiddenstates we can choose from,0
Dont stride if this is not one of the original hiddenstates,0
"Check if a stride is needed, then use a strided 1x1 here",0
Determine if a reduction should be applied to make the number of,0
filters match.,0
Return the concat of all the states,0
Scale keep prob by layer number,0
The added 2 is for the reduction cells,0
Decrease the keep probability over time,0
,0
Script for generating a two-class dataset in COCO format for training an obscured image classifier,0
,0
Requires Python >= 3.6 because of the glob ** expression,0
,0
Collect images and labels,0
"Labels: clean = 0, obscured = 1",0
,0
top_folders_to_bottom.py,0
,0
Given a base folder with files like:,0
,0
A/1/2/a.jpg,0
B/3/4/b.jpg,0
,0
"...moves the top-level folders to the bottom in a new output folder, i.e., creates:",0
,0
1/2/A/a.jpg,0
3/4/B/b.jpg,0
,0
"In practice, this is used to make this:",0
,0
animal/camera01/image01.jpg,0
,0
...look like:,0
,0
camera01/animal/image01.jpg,0
,0
%% Constants and imports,0
%% Support functions,0
%% Main functions,0
Find top-level folder,0
Find file/folder names,0
Move or copy,0
...def process_file(),0
Enumerate input folder,0
Convert absolute paths to relative paths,0
Standardize delimiters,0
Make sure each input file maps to a unique output file,0
relative_filename = relative_files[0],0
Loop,0
...def top_folders_to_bottom(),0
%% Interactive driver,0
%%,0
%%,0
%% Command-line driver,0
"python top_folders_to_bottom.py ""g:\temp\separated_images"" ""g:\temp\separated_images_inverted"" --n_threads 100",0
Convert to an options object,0
%% Constants and imports,0
Assumes ai4eutils is on the python path (https://github.com/Microsoft/ai4eutils),0
Assumes the cameratraps repo root is on the path,0
%% Options,0
"Make sure there is no overlap between the two sets, because this will cause",0
issues in the code,0
## Required inputs,0
## Options,0
Can be a folder or a SAS URL,0
# These apply only when we're doing ground-truth comparisons,0
Classes we'll treat as negative,0
,0
"Include the token ""#NO_LABELS#"" to indicate that an image with no annotations",0
should be considered empty.,0
Classes we'll treat as neither positive nor negative,0
"A list of output sets that we should count, but not render images for.",0
,0
"Typically used to preview sets with lots of empties, where you don't want to",0
"subset but also don't want to render 100,000 empty images.",0
,0
"detections, non_detections",0
"detections_animal, detections_person, detections_vehicle",0
"By default, choose a confidence threshold based on the detector version",0
Used for summary statistics only,0
"Number of images to sample, -1 for ""all images""",0
"Random seed for sampling, or None",0
Optionally separate detections into categories (animal/vehicle/human),0
,0
Currently only supported when ground truth is unavailable,0
Optionally replace one or more strings in filenames with other strings;,0
useful for taking a set of results generated for one folder structure,0
and applying them to a slightly different folder structure.,0
Allow bypassing API output loading when operating on previously-loaded,0
results,0
Should we also split out a separate report about the detections that were,0
just below our main confidence threshold?,0
,0
Currently only supported when ground truth is unavailable,0
Control rendering parallelization,0
...PostProcessingOptions,0
#%% Helper classes and functions,0
Anything greater than this isn't clearly positive or negative,0
image has annotations suggesting both negative and positive,0
"image is not annotated or is annotated with 'unknown', 'unlabeled', ETC.",0
image has not yet been assigned a state,0
"In some analyses, we add an additional class that lets us look at",0
detections just below our main confidence threshold,0
count the # of images with each type of DetectionStatus,0
Check whether this image has:,0
- unknown / unassigned-type labels,0
- negative-type labels,0
"- positive labels (i.e., labels that are neither unknown nor negative)",0
"assert has_unknown_labels is False, '{} has unknown labels'.format(annotations)",0
If there are no image annotations...,0
n_negative += 1,0
im['_detection_status'] = DetectionStatus.DS_NEGATIVE,0
"If the image has more than one type of labels, it's ambiguous",0
"note: bools are automatically converted to 0/1, so we can sum",0
"After the check above, we can be sure it's only one of positive,",0
"negative, or unknown.",0
,0
Important: do not merge the following 'unknown' branch with the first,0
"'unknown' branch above, where we tested 'if len(categories) == 0'",0
,0
If the image has only unknown labels,0
If the image has only negative labels,0
If the images has only positive labels,0
"Annotate the category, if it is unambiguous",0
...for each image,0
...mark_detection_status(),0
"Leaving code in place for reading from blob storage, may support this",0
in the future.,0
resize is to display them in this notebook or in the HTML more quickly,0
os.path.isfile() is slow when mounting remote directories; much faster,0
to just try/except on the image open.,0
return '',0
Render images to a flat folder... we can use os.sep here because we've,0
already normalized paths,0
"Create class labels like ""gt_1"" or ""gt_27""",0
"for i_box,box in enumerate(ground_truth_boxes):",0
gt_classes.append('_' + str(box[-1])),0
"errno.ENAMETOOLONG doesn't get thrown properly on Windows, so",0
we awkwardly check against a hard-coded limit,0
Use slashes regardless of os,0
Optionally add links back to the original images,0
...render_bounding_boxes,0
Count items in each category,0
Optionally sort by filename before writing to html,0
Write the individual HTML files,0
...prepare_html_subpages(),0
Get unique categories above the threshold for this image,0
Render an image (with no ground truth information),0
"This is a list of [class,confidence] pairs, sorted by confidence",0
"If we either don't have a confidence threshold, or we've met our",0
confidence threshold,0
...if this detection has classification info,0
...for each detection,0
...def render_image_no_gt(),0
This should already have been normalized to either '/' or '\',0
...def render_image_with_gt(),0
%% Main function,0
#%% Expand some options for convenience,0
#%% Prepare output dir,0
#%% Load ground truth if available,0
Mark images in the ground truth as positive or negative,0
#%% Load detection (and possibly classification) results,0
"If the caller hasn't supplied results, load them",0
Determine confidence thresholds if necessary,0
Remove failed rows,0
Convert keys and values to lowercase,0
"Add column 'pred_detection_label' to indicate predicted detection status,",0
not separating out the classes,0
#%% Pull out descriptive metadata,0
This is rare; it only happens during debugging when the caller,0
is supplying already-loaded API results.,0
"#%% If we have ground truth, remove images we can't match to ground truth",0
#%% Sample images for visualization,0
#%% Fork here depending on whether or not ground truth is available,0
"If we have ground truth, we'll compute precision/recall and sample tp/fp/tn/fn.",0
,0
Otherwise we'll just visualize detections/non-detections.,0
#%% Detection evaluation: compute precision/recall,0
numpy array of detection probabilities,0
"numpy array of bools (0.0/1.0), and -1 as null value",0
Don't include ambiguous/unknown ground truth in precision/recall analysis,0
"For completeness, include the result at a confidence threshold of 1.0",0
Compute and print summary statistics,0
Thresholds go up throughout precisions/recalls/thresholds; find the last,0
value where recall is at or above target.  That's our precision @ target recall.,0
"np.where returns a tuple of arrays, but in this syntax where we're",0
"comparing an array with a scalar, there will only be one element.",0
Convert back to a list,0
Flatten the confusion matrix,0
"#%% Collect classification results, if they exist",0
Mapping of classnames to idx for the confusion matrix.,0
,0
"The lambda is actually kind of a hack, because we use assume that",1
the following code does not reassign classname_to_idx,0
Confusion matrix as defaultdict of defaultdict,0
,0
"Rows / first index is ground truth, columns / second index is predicted category",0
iDetection = 0; fn = detector_files[iDetection]; print(fn),0
"If this image has classification predictions, and an unambiguous class",0
"annotated, and is a positive image...",0
"The unambiguous category, we make this a set for easier handling afterward",0
"Compute the accuracy as intersection of union,",0
i.e. (# of categories in both prediciton and GT),0
divided by (# of categories in either prediction or GT,0
,0
"In case of only one GT category, the result will be 1.0, if",0
prediction is one category and this category matches GT,0
,0
"It is 1.0/(# of predicted top-1 categories), if the GT is",0
one of the predicted top-1 categories.,0
,0
"It is 0.0, if none of the predicted categories is correct",0
Distribute this accuracy across all predicted categories in the,0
confusion matrix,0
...for each file in the detection results,0
If we have classification results,0
Build confusion matrix as array from classifier_cm,0
Print some statistics,0
Prepare confusion matrix output,0
Get confusion matrix as string,0
Get fixed-size classname for each idx,0
Prepend class name on each line and add to the top,0
Print formatted confusion matrix,0
"Actually don't, this gets really messy in all but the widest consoles",0
Plot confusion matrix,0
To manually add more space at bottom: plt.rcParams['figure.subplot.bottom'] = 0.1,0
,0
"Add 0.5 to figsize for every class. For two classes, this will result in",0
"fig = plt.figure(figsize=[4,4])",0
...if we have classification results,0
#%% Render output,0
Write p/r table to .csv file in output directory,0
Write precision/recall plot to .png file in output directory,0
#%% Sampling,0
Sample true/false positives/negatives with correct/incorrect top-1,0
classification and render to html,0
Accumulate html image structs (in the format expected by write_html_image_lists),0
"for each category, e.g. 'tp', 'fp', ..., 'class_bird', ...",0
Add default entries by accessing them for the first time,0
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]",0
"Each element will be a three-tuple with elements file,max_conf,detections",0
"Assemble the information we need for rendering, so we can parallelize without",0
dealing with Pandas,0
i_row = 0; row = images_to_visualize.iloc[0],0
Filenames should already have been normalized to either '/' or '\',0
Map all the rendering results in the list rendering_results into the,0
dictionary images_html,0
Prepare the individual html image files,0
Write index.html,0
Show links to each GT class,0
,0
We could do this without classification results; currently we don't.,0
Add links to all available classes,0
Close body and html tags,0
...for each image,0
"#%% Otherwise, if we don't have ground truth...",0
#%% Sample detections/non-detections,0
Accumulate html image structs (in the format expected by write_html_image_list),0
for each category,0
Add default entries by accessing them for the first time,0
"Maps detection categories - e.g. ""human"" - to result set names, e.g.",0
"""detections_human""",0
Add a set of results for each category and combination of categories,0
Create output directories,0
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]",0
"list of 3-tuples with elements (file, max_conf, detections)",0
"Assemble the information we need for rendering, so we can parallelize without",0
dealing with Pandas,0
i_row = 0; row = images_to_visualize.iloc[0],0
Filenames should already have been normalized to either '/' or '\',0
"render_image_no_gt(file_info,detection_categories_to_results_name,",0
"detection_categories,classification_categories)",0
Map all the rendering results in the list rendering_results into the,0
dictionary images_html,0
Prepare the individual html image files,0
Write index.html,0
"We can't just sum these, because image_counts includes images in both their",0
detection and classification classes,0
total_images = sum(image_counts.values()),0
Don't print classification classes here; we'll do that later with a slightly,0
different structure,0
Add links to all available classes,0
os.startfile(output_html_file),0
...if we do/don't have ground truth,0
...process_batch_results,0
%% Interactive driver(s),0
%%,0
options.unlabeled_classes = ['human'],0
os.start(ppresults.output_html_file),0
%% Command-line driver,0
,0
load_api_results.py,0
,0
Loads the output of the batch processing API (json) into a pandas dataframe.,0
,0
Also functions to group entries by seq_id.,0
,0
Includes the deprecated functions that worked with the old CSV API output format.,0
,0
%% Constants and imports,0
%% Functions for grouping by sequence_id,0
example,0
input 'file': 'SER/S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG',0
output 'id': 'S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG',0
%% Functions for loading the result as a Pandas DataFrame,0
Validate that this is really a detector output file,0
Fields in the API output json other than 'images',0
Normalize paths to simplify comparisons later,0
"image['file'] = image['file'].replace('\\','/')",0
Replace some path tokens to match local paths to original blob structure,0
"If this is a newer file that doesn't include maximum detection confidence values,",0
"add them, because our unofficial internal dataframe format includes this.",0
Pack the json output into a Pandas DataFrame,0
Sanity-check that this is really a detector output file,0
Normalize paths to simplify comparisons later,0
De-serialize detections,0
Optionally replace some path tokens to match local paths to the original blob structure,0
string_to_replace = list(options.detector_output_filename_replacements.keys())[0],0
"TODO: hit some silly issues with vectorized str() and escaped characters, vectorize",1
this later.,0
,0
"detection_results['image_path'].str.replace(string_to_replace,replacement_string)",0
iRow = 0,0
%% Imports,0
Assumes ai4eutils is on the python path (https://github.com/Microsoft/ai4eutils),0
%% Constants and support classes,0
We will confirm that this matches what we load from each file,0
Process-based parallelization isn't supported yet,0
%% Main function,0
"Warn the user if some ""detections"" might not get rendered",0
#%% Validate inputs,0
#%% Load both result sets,0
assert results_a['detection_categories'] == default_detection_categories,0
assert results_b['detection_categories'] == default_detection_categories,0
#%% Make sure they represent the same set of images,0
#%% Find differences,0
"Each of these maps a filename to a two-element list (the image in set A, the image in set B)",0
,0
"Right now, we only handle a very simple notion of class transition, where the detection",0
of maximum confidence changes class *and* both images have an above-threshold detection.,0
fn = filenames_a[0],0
We shouldn't have gotten this far if error_on_non_matching_lists is set,0
det = im_a['detections'][0],0
...for each filename,0
#%% Sample and plot differences,0
"Render two sets of results (i.e., a comparison) for a single",0
image.,0
...def render_image_pair(),0
fn = image_filenames[0],0
...def render_detection_comparisons(),0
"For each category, generate comparison images and the",0
comparison HTML page.,0
,0
category = 'common_detections',0
Choose detection pairs we're going to render for this category,0
...for each category,0
#%% Write the top-level HTML file content,0
...def compare_batch_results(),0
%% Interactive driver,0
%% KRU,0
%% Command-line driver,0
# TODO,1
,0
"Merge high-confidence detections from one results file into another file,",0
when the target file does not detect anything on an image.,0
,0
Does not currently attempt to merge every detection based on whether individual,0
detections are missing; only merges detections into images that would otherwise,0
be considered blank.,0
,0
"If you want to literally merge two .json files, see combine_api_outputs.py.",0
,0
%% Constants and imports,0
%% Structs,0
Don't bother merging into target images where the max detection is already,0
higher than this threshold,0
"If you want to merge only certain categories, specify one",0
(but not both) of these.,0
%% Main function,0
im = output_data['images'][0],0
"Determine whether we should be processing all categories, or just a subset",0
of categories.,0
i_source_file = 0; source_file = source_files[i_source_file],0
source_im = source_data['images'][0],0
detection_category = list(detection_categories)[0],0
"This is already a detection, no need to proceed looking for detections to",0
transfer,0
Boxes are x/y/w/h,0
source_sizes = [det['bbox'][2]*det['bbox'][3] for det in source_detections_this_category_raw],0
Only look at boxes below the size threshold,0
...for each detection category,0
"print('Adding {} detections to image {}'.format(len(detections_to_transfer),image_filename))",0
Update the max_detection_conf field (if present),0
...for each image,0
...for each source file,0
%% Test driver,0
%%,0
%% Command-line driver (TODO),0
,0
separate_detections_into_folders.py,0
,0
## Overview,0
,0
"Given a .json file with batch processing results, separate the files in that",0
"set of results into folders that contain animals/people/vehicles/nothing,",0
according to per-class thresholds.,0
,0
"Image files are copied, not moved.",0
,0
,0
## Output structure,0
,0
Preserves relative paths within each of those folders; cannot be used with .json,0
files that have absolute paths in them.,0
,0
"For example, if your .json file has these images:",0
,0
a/b/c/1.jpg,0
a/b/d/2.jpg,0
a/b/e/3.jpg,0
a/b/f/4.jpg,0
a/x/y/5.jpg,0
,0
And let's say:,0
,0
"* The results say that the first three images are empty/person/vehicle, respectively",0
"* The fourth image is above threshold for ""animal"" and ""person""",0
* The fifth image contains an animal,0
,0
* You specify an output base folder of c:\out,0
,0
You will get the following files:,0
,0
c:\out\empty\a\b\c\1.jpg,0
c:\out\people\a\b\d\2.jpg,0
c:\out\vehicles\a\b\e\3.jpg,0
c:\out\animal_person\a\b\f\4.jpg,0
c:\out\animals\a\x\y\5.jpg,0
,0
## Rendering bounding boxes,0
,0
"By default, images are just copied to the target output folder.  If you specify --render_boxes,",0
bounding boxes will be rendered on the output images.  Because this is no longer strictly,0
"a copy operation, this may result in the loss of metadata.  More accurately, this *may*",0
result in the loss of some EXIF metadata; this *will* result in the loss of IPTC/XMP metadata.,0
,0
Rendering boxes also makes this script a lot slower.,0
,0
## Classification-based separation,0
,0
"If you have a results file with classification data, you can also specify classes to put",0
"in their own folders, within the ""animals"" folder, like this:",0
,0
"--classification_thresholds ""deer=0.75,cow=0.75""",0
,0
"So, e.g., you might get:",0
,0
c:\out\animals\deer\a\x\y\5.jpg,0
,0
"In this scenario, the folders within ""animals"" will be:",0
,0
"deer, cow, multiple, unclassified",0
,0
"""multiple"" in this case only means ""deer and cow""; if an image is classified as containing a",0
"bird and a bear, that would end up in ""unclassified"", since the folder separation is based only",0
on the categories you provide at the command line.,0
,0
"No classification-based separation is done within the animal_person, animal_vehicle, or",0
animal_person_vehicle folders.,0
,0
%% Constants and imports,0
Occasionally we have near-zero confidence detections associated with COCO classes that,0
didn't quite get squeezed out of the model in training.  As long as they're near zero,0
"confidence, we just ignore them.",0
%% Options class,0
"Dictionary mapping categories (plus combinations of categories, and 'empty') to output folders",0
Populated only when using classification results,0
"Originally specified as a string, converted to a dict mapping name:threshold",0
...__init__(),0
...class SeparateDetectionsIntoFoldersOptions,0
%% Support functions,0
Find the maximum confidence for each category,0
,0
det = detections[0],0
"For zero-confidence detections, we occasionally have leftover goop",0
from COCO classes,0
assert det['conf'] < invalid_category_epsilon,0
...for each detection on this image,0
Count the number of thresholds exceeded,0
...for each category,0
If this is above multiple thresholds,0
"TODO: handle species-based separation in, e.g., the animal_person case",1
"Are we making species classification folders, and is this an animal?",0
Do we need to put this into a specific species folder?,0
Find the animal-class detections that are above threshold,0
Count the number of classification categories that are above threshold for at,0
least one detection,0
d = valid_animal_detections[0],0
classification = d['classifications'][0],0
"Do we have a threshold for this category, and if so, is",0
this classification above threshold?,0
...for each classification,0
...for each detection,0
...if we have to deal with classification subfolders,0
...if we have 0/1/more categories above threshold,0
...if this is/isn't a failure case,0
Skip this image if it's empty and we're not processing empty images,0
"At this point, this image is getting copied; we may or may not also need to",0
draw bounding boxes.,0
Do a simple copy operation if we don't need to render any boxes,0
Open the source image,0
"Render bounding boxes for each category separately, beacuse",0
we allow different thresholds for each category.,0
"When we're not using classification folders, remove classification",0
information to maintain standard detection colors.,0
...for each category,0
Read EXIF metadata,0
"Write output with EXIF metadata if available, and quality='keep' if this is a JPEG",0
"image.  Unfortunately, neither parameter likes ""None"", so we get a slightly",0
icky cascade of if's here.,0
Also see:,0
,0
https://discuss.dizzycoding.com/determining-jpg-quality-in-python-pil/,0
,0
...for more ways to preserve jpeg quality if quality='keep' doesn't do the trick.,0
...if we don't/do need to render boxes,0
...def process_detections(),0
%% Main function,0
Create output folder if necessary,0
Load detection results,0
Map class names to output folders,0
Create all combinations of categories,0
category_name = category_names[0],0
Do we have a custom threshold for this category?,0
Create folder mappings for each category,0
Create the actual folders,0
"Handle species classification thresholds, if specified",0
"E.g. deer=0.75,cow=0.75",0
token = tokens[0],0
...for each token,0
...if classification thresholds are still in string format,0
Validate the classes in the threshold list,0
...if we need to deal with classification categories,0
i_image = 14; im = images[i_image]; im,0
...for each image,0
...def separate_detections_into_folders,0
%% Interactive driver,0
%%,0
%%,0
%%,0
%% Testing various command-line invocations,0
"With boxes, no classification",0
"No boxes, no classification (default)",0
"With boxes, with classification",0
"No boxes, with classification",0
%% Command-line driver,0
"python api\batch_processing\postprocessing\separate_detections_into_folders.py ""d:\temp\rspb_mini.json"" ""d:\temp\demo_images\rspb_2018_2019_mini"" ""d:\temp\separation_test"" --nthreads 2",0
Convert to an options object,0
"print('{} {}'.format(v,name))",0
List of category numbers to use in separation; uses all categories if None,0
"Can be ""size"", ""width"", or ""height""",0
For each image...,0
,0
im = images[0],0
d = im['detections'][0],0
Are there really any detections here?,0
Is this a category we're supposed to process?,0
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing#detector-outputs,0
...for each detection,0
...for each image,0
...def categorize_detections_by_size(),0
,0
add_max_conf.py,0
,0
"The MD output format included a ""max_detection_conf"" field with each image",0
up to and including version 1.2; it was removed as of version 1.3 (it's,0
redundant with the individual detection confidence values).,1
,0
"Just in case someone took a dependency on that field, this script allows you",0
to add it back to an existing .json file.,0
,0
%% Imports and constants,0
%% Main function,0
%% Driver,0
#######,0
,0
remove_repeat_detections.py,0
,0
"Used after running find_repeat_detections, then manually filtering the results,",0
to create a final filtered output file.,0
,0
"If you want to use this script, we recommend that you read the user's guide:",0
,0
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms,0
,0
#######,0
%% Constants and imports,0
%% Main function,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
#######,0
,0
repeat_detections_core.py,0
,0
Core utilities shared by find_repeat_detections and remove_repeat_detections.,0
,0
#######,0
%% Imports and environment,0
"from ai4eutils; this is assumed to be on the path, as per repo convention",0
"""PIL cannot read EXIF metainfo for the images""",0
"""Metadata Warning, tag 256 had too many entries: 42, expected 1""",0
%% Constants,0
%% Classes,0
Relevant for rendering the folder of images for filtering,0
,0
"imageBase can also be a SAS URL, in which case some error-checking is",0
disabled.,0
Don't consider detections with confidence lower than this as suspicious,0
Don't consider detections with confidence higher than this as suspicious,0
What's the IOU threshold for considering two boxes the same?,0
How many occurrences of a single location (as defined by the IOU threshold),0
are required before we declare it suspicious?,0
"Ignore ""suspicious"" detections larger than some size; these are often animals",0
taking up the whole image.  This is expressed as a fraction of the image size.,0
"Ignore ""suspicious"" detections smaller than some size",0
Ignore folders with more than this many images in them,0
A list of classes we don't want to treat as suspicious. Each element is an int.,0
Load detections from a filter file rather than finding them from the detector output,0
".json file containing detections, should be called detectionIndex.json in the filtering_* folder",0
produced in the first pass,0
(optional) List of filenames remaining after deletion of identified,0
repeated detections that are actually animals.  This should be a flat,0
"text file, one relative filename per line.  See enumerate_images().",0
Turn on/off optional outputs,0
Determines whether bounding-box rendering errors (typically network errors) should,0
be treated as failures,0
Box rendering options,0
State variables,0
"Replace filename tokens after reading, useful when the directory structure",0
has changed relative to the structure the detector saw,0
How many folders up from the leaf nodes should we be going to aggregate images?,0
An optional function that takes a string (an image file name) and returns,0
"a string (the corresponding  folder ID), typically used when multiple folders",0
actually correspond to the same camera in a manufacturer-specific way (e.g.,0
a/b/c/RECONYX100 and a/b/c/RECONYX101 may really be the same camera).,0
Include/exclude specific folders... only one of these may be,0
"specified; ""including"" folders includes *only* those folders.",0
"Optionally show *other* detections (i.e., detections other than the",0
one the user is evaluating) in a light gray,0
"If bRenderOtherDetections is True, what color should we use to render the",0
(hopefully pretty subtle) non-target detections?,0
,0
"In theory I'd like these ""other detection"" rectangles to be partially",0
"transparent, but this is not straightforward, and the alpha is ignored",0
"here.  But maybe if I leave it here and wish hard enough, someday it",0
will work.,0
,0
otherDetectionsColors = ['dimgray'],0
Sort detections within a directory so nearby detections are adjacent,0
"in the list, for faster review.",0
,0
"Can be None, 'xsort', or 'clustersort'",0
,0
* None sorts detections chronologically by first occurrence,0
* 'xsort' sorts detections from left to right,0
* 'clustersort' clusters detections and sorts by cluster,0
Only relevant if smartSort == 'clustersort',0
"The data table (Pandas DataFrame), as loaded from the input json file via",0
load_api_results(),0
"The other fields in the input json file, loaded via load_api_results()",0
The data table after modification,0
dict mapping folder names to whole rows from the data table,0
dict mapping filenames to rows in the master table,0
"An array of length nDirs, where each element is a list of DetectionLocation",0
objects for that directory that have been flagged as suspicious,0
"This is a bit of a hack right now, but for future-proofing, I don't want to call this",1
"to retrieve anything other than the highest-confidence detection, and I'm assuming this",0
"is already sorted, so assert() that.",0
It's not clear whether it's better to use instances[0].bbox or self.bbox,1
"here... they should be very similar, unless iouThreshold is very low.",0
self.bbox is a better representation of the overal DetectionLocation.,0
%% Helper functions,0
"We store detetions as x/y/w/h, rtree and pyqtree use l/b/r/t",0
"We store detetions as x/y/w/h, rtree and pyqtree use l/b/r/t",0
%% Sort a list of candidate detections to make them visually easier to review,0
Just sort by the X location of each box,0
"Prepare a list of points to represent each box,",0
that's what we'll use for clustering,0
Upper-left,0
"points.append([det.bbox[0],det.bbox[1]])",0
Center,0
"Labels *could* be any unique labels according to the docs, but in practice",0
they are unique integers from 0:nClusters,0
Make sure the labels are unique incrementing integers,0
Store the label assigned to each cluster,0
"Now sort the clusters by their x coordinate, and re-assign labels",0
so the labels are sortable,0
"Compute the centroid for debugging, but we're only going to use the x",0
"coordinate.  This is the centroid of points used to represent detections,",0
which may be box centers or box corners.,0
old_cluster_label_to_new_cluster_label[old_cluster_label] =\,0
new_cluster_labels[old_cluster_label],0
%% Look for matches (one directory),0
List of DetectionLocations,0
candidateDetections = [],0
Create a tree to store candidate detections,0
For each image in this directory,0
,0
iDirectoryRow = 0; row = rows.iloc[iDirectoryRow],0
,0
"iDirectoryRow is a pandas index, so it may not start from zero;",0
"for debugging, we maintain i_iteration as a loop index.",0
print('Searching row {} of {} (index {}) in dir {}'.\,0
"format(i_iteration,len(rows),iDirectoryRow,dirName))",0
Don't bother checking images with no detections above threshold,0
"Array of dicts, where each element is",0
{,0
"'category': '1',  # str value, category ID",0
"'conf': 0.926,  # confidence of this detections",0
,0
"# (x_min, y_min) is upper-left, all in relative coordinates",0
"'bbox': [x_min, y_min, width_of_box, height_of_box]",0
,0
},0
For each detection in this image,0
"This is no longer strictly true; I sometimes run RDE in stages, so",0
some probabilities have already been made negative,0
,0
assert confidence >= 0.0 and confidence <= 1.0,0
Optionally exclude some classes from consideration as suspicious,0
Is this detection too big to be suspicious?,0
print('Illegal zero-size bounding box on image {}'.format(filename)),0
These are relative coordinates,0
print('Ignoring very small detection with area {}'.format(area)),0
print('Ignoring very large detection with area {}'.format(area)),0
This will return candidates of all classes,0
For each detection in our candidate list,0
Don't match across categories,0
Is this a match?,0
"If so, add this example to the list for this detection",0
We *don't* break here; we allow this instance to possibly,0
match multiple candidates.  There isn't an obvious right or,0
wrong here.,0
...for each detection on our candidate list,0
"If we found no matches, add this to the candidate list",0
candidateDetections.append(candidate),0
pyqtree,0
...for each detection,0
...for each row,0
Get all candidate detections,0
print('Found {} candidate detections for folder {}'.format(,0
"len(candidateDetections),dirName))",0
"For debugging only, it's convenient to have these sorted",0
as if they had never gone into a tree structure.  Typically,0
this is in practce a sort by filename.,0
...def find_matches_in_directory(dirName),0
"%% Update the detection table based on suspicious results, write .csv output",0
"An array of length nDirs, where each element is a list of DetectionLocation",0
objects for that directory that have been flagged as suspicious,0
For each directory,0
For each suspicious detection group in this directory,0
For each instance of this suspicious detection,0
This should match the bbox for the detection event,0
The bbox for this instance should be almost the same as the bbox,0
"for this detection group, where ""almost"" is defined by the IOU",0
threshold.,0
if iou < options.iouThreshold:,0
"print('IOU warning: {},{}'.format(iou,options.iouThreshold))",0
Make sure the bounding box matches,0
"Make the probability negative, if it hasn't been switched by",0
another bounding box,0
...for each instance,0
...for each detection,0
...for each directory,0
Update maximum probabilities,0
For each row...,0
No longer strictly true; sometimes I run RDE on RDE output,0
assert maxPOriginal >= 0,0
We should only be making detections *less* likely in this process,0
"If there was a meaningful change, count it",0
"Negative probabilities should be the only reason maxP changed, so",0
we should have found at least one negative value if we reached,0
this point.,0
...if there was a meaningful change to the max probability for this row,0
...for each row,0
If we're also writing output...,0
"...def update_detection_table(RepeatDetectionResults,options)",0
%% Main function,0
#%% Input handling,0
Validate some options,0
Load the filtering file,0
Load the same options we used when finding repeat detections,0
...except for things that explicitly tell this function not to,0
find repeat detections.,0
...if we're loading from an existing filtering file,0
Check early to avoid problems with the output folder,0
"Load file to a pandas dataframe.  Also populates 'max_detection_conf', even if it's",0
not present in the .json file.,0
detectionResults[detectionResults['failure'].notna()],0
"Before doing any real work, make sure we can *probably* access images",0
"This is just a cursory check on the first image, but it heads off most",0
"problems related to incorrect mount points, etc.  Better to do this before",0
spending 20 minutes finding repeat detections.,0
#%% Separate files into directories,0
This will be a map from a directory name to smaller data frames,0
This is a mapping back into the rows of the original table,0
iRow = 0; row = detectionResults.iloc[0],0
Create a new DataFrame with just this row,0
rowsByDirectory[dirName] = pd.DataFrame(row),0
...for each unique detection,0
Convert lists of rows to proper DataFrames,0
#% Look for matches (or load them from file),0
"Are we actually looking for matches, or just loading from a file?",0
length-nDirs list of lists of DetectionLocation objects,0
We're actually looking for matches...,0
"We get slightly nicer progress bar behavior using threads, by passing a pbar",0
object and letting it get updated.  We can't serialize this object across,0
processes.,0
#%% Find suspicious locations based on match results,0
For each directory,0
,0
iDir = 51,0
A list of DetectionLocation objects,0
A list of DetectionLocation objects,0
occurrenceList is a list of file/detection pairs,0
Sort the above-threshold detections for easier review,0
...for each directory,0
If we're just loading detections from a file...,0
"We're skipping detection-finding, but to see which images are actually legit false",0
"positives, we may be looking for physical files or loading from a text file.",0
For each directory,0
iDir = 0; detections = suspiciousDetections[0],0
,0
"suspiciousDetections is an array of DetectionLocation objects,",0
one per directory.,0
For each detection that was present before filtering,0
iDetection = 0; detection = detections[iDetection],0
Are we checking the directory to see whether detections were actually false,0
"positives, or reading from a list?",0
Is the image still there?,0
"If not, remove this from the list of suspicious detections",0
...for each detection,0
...for each directory,0
...if we are/aren't finding detections (vs. loading from file),0
Create filtering directory,0
iDir = 0; suspiciousDetectionsThisDir = suspiciousDetections[iDir],0
suspiciousDetectionsThisDir is a list of DetectionLocation objects,0
iDetection = 0; detection = suspiciousDetectionsThisDir[0],0
Sort instances in descending order by confidence,0
Choose the highest-confidence index,0
Should we render (typically in a very light color) detections,0
*other* than the one we're highlighting here?,0
Render other detections first (typically in a thin+light box),0
Now render the example detection (on top of at least one,0
of the other detections),0
This converts the *first* instance to an API standard detection;,0
"because we just sorted this list in descending order by confidence,",0
this is the highest-confidence detection.,0
...if we are/aren't rendering other bounding boxes,0
...for each detection in this folder,0
...for each folder,0
Write out the detection index,0
...if we're writing filtering info,0
...find_repeat_detections(),0
#######,0
,0
find_repeat_detections.py,0
,0
"If you want to use this script, we recommend that you read the user's guide:",0
,0
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms,0
,0
"Really, don't try to run this script without reading the user's guide, you'll think",0
it's more magical than it is.,0
,0
"This script looks through a sequence of detections in the API output json file, and finds",0
"candidates that might be ""repeated false positives"", i.e. that random branch that the",0
detector thinks is an animal/person/vehicle.,0
,0
"Typically after running this script, you would do a manual step to remove",0
"true positives, then run remove_repeat_detections to produce a final output file.",0
,0
There's no way that statement was self-explanatory; see the user's guide.,0
,0
#######,0
%% Constants and imports,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
Convert to an options object,0
,0
If a request has been sent to AML for batch scoring but the monitoring thread of the API was,0
"interrupted (uncaught exception or having to re-start the API container), we could manually",0
"aggregate results from each shard using this script, assuming all jobs submitted to AML have finished.",0
,0
Need to have set environment variables STORAGE_ACCOUNT_NAME and STORAGE_ACCOUNT_KEY to those of the,0
"storage account backing the API. Also need to adjust the INTERNAL_CONTAINER, AML_CONTAINER and",0
AML_CONFIG fields in api_core/orchestrator_api/api_config.py to match the instance of the API that this,0
request was submitted to.,0
,0
May need to change the import statement in api_core/orchestrator_api/orchestrator.py,0
"""from sas_blob_utils import SasBlob"" to",0
"""from .sas_blob_utils import SasBlob"" to not confuse with the module in AI4Eutils;",0
"and change ""import api_config"" to",0
"""from api.batch_processing.api_core.orchestrator_api import api_config""",0
Execute this script from the root of the repository. You may need to add the repository to PYTHONPATH.,0
"list_jobs_submitted cannot be serialized (""can't pickle _thread.RLock objects ""), but",0
do not need it for aggregating results,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%% helper classes and functions,0
TODO log exception when we have more telemetry,1
TODO check that the expiry date of input_container_sas is at least a month,0
into the future,0
"if no permission specified explicitly but has an access policy, assumes okay",0
TODO - check based on access policy as well,1
return current UTC time as a string in the ISO 8601 format (so we can query by,0
timestamp in the Cosmos DB job status table.,0
example: '2021-02-08T20:02:05.699689Z',0
"image_paths will have length at least 1, otherwise would have ended before this step",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
a job moves from created to running/problem after the Batch Job has been submitted,0
"job_id should be unique across all instances, and is also the partition key",0
TODO do not read the entry first to get the call_params when the Cosmos SDK add a,1
patching functionality:,0
https://feedback.azure.com/forums/263030-azure-cosmos-db/suggestions/6693091-be-able-to-do-partial-updates-on-document,0
need to retain other fields in 'status' to be able to restart monitoring thread,0
retain existing fields; update as needed,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
sentinel should change if new configurations are available,0
configs have not changed,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Gunicorn logger handler will get attached if needed in server.py,0
set for all tasks in the job,0
cannot execute the scoring script that is in the mounted directory; has to be copied to cwd,0
not luck giving the commandline arguments via formatted string - set as env vars instead,0
form shards of images and assign each shard to a Task,0
for persisting stdout and stderr,0
persist stdout and stderr (will be removed when node removed),0
paths are relative to the Task working directory,0
can also just upload on failure,0
first try submitting Tasks,0
retry submitting Tasks,0
Change the Job's on_all_tasks_complete option to 'terminateJob' so the Job's status changes automatically,0
after all submitted tasks are done,0
This is so that we do not take up the quota for active Jobs in the Batch account.,0
return type: TaskAddCollectionResult,0
actually we should probably only re-submit if it's a server_error,0
docs: # https://docs.microsoft.com/en-us/rest/api/batchservice/odata-filters-in-batch#list-tasks,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%% Flask app,0
reference: https://trstringer.com/logging-flask-gunicorn-the-manageable-way/,0
%% Helper classes,0
%% Flask endpoints,0
required params,0
can be an URL to a file not hosted in an Azure blob storage container,0
"if use_url, then images_requested_json_sas is required",0
optional params,0
check model_version is among the available model versions,0
check request_name has only allowed characters,0
optional params for telemetry collection - logged to status table for now as part of call_params,0
All API instances / node pools share a quota on total number of active Jobs;,0
we cannot accept new Job submissions if we are at the quota,0
required fields,0
request_status is either completed or failed,0
the create_batch_job thread will stop when it wakes up the next time,0
"Fix for Zooniverse - deleting any ""-"" characters in the job_id",0
"If the status is running, it could be a Job submitted before the last restart of this",0
"API instance. If that is the case, we should start to monitor its progress again.",0
WARNING model_version could be wrong (a newer version number gets written to the output file) around,0
"the time that  the model is updated, if this request was submitted before the model update",0
and the API restart; this should be quite rare,0
conform to previous schemes,0
%% undocumented endpoints,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Gunicorn logger handler will get attached if needed in server.py,0
request_name and request_submission_timestamp are for appending to,0
output file names,0
image_paths can be a list of strings (Azure blob names or public URLs),0
"or a list of length-2 lists where each is a [image_id, metadata] pair",0
Case 1: listing all images in the container,0
- not possible to have attached metadata if listing images in a blob,0
list all images to process,0
+ 1 so if the number of images listed > MAX_NUMBER_IMAGES_ACCEPTED_PER_JOB,0
we will know and not proceed,0
Case 2: user supplied a list of images to process; can include metadata,0
filter down to those conforming to the provided prefix and accepted suffixes (image file types),0
prefix is case-sensitive; suffix is not,0
"Although urlparse(p).path preserves the extension on local paths, it will not work for",0
"blob file names that contains ""#"", which will be treated as indication of a query.",0
"If the URL is generated via Azure Blob Storage, the ""#"" char will be properly encoded",0
apply the first_n and sample_n filters,0
OK if first_n > total number of images,0
sample by shuffling image paths and take the first sample_n images,0
"upload the image list to the container, which is also mounted on all nodes",0
all sharding and scoring use the uploaded list,0
now request_status moves from created to running,0
an extra field to allow the monitoring thread to restart after an API restart: total number of tasks,0
also record the number of images to process for reporting,0
start the monitor thread with the same name,0
"both succeeded and failed tasks are marked ""completed"" on Batch",0
"preserving format from before, but SAS URL to 'failed_images' and 'images' are no longer provided",0
"failures should be contained in the output entries, indicated by an 'error' field",0
"when people download this, the timestamp will have : replaced by _",0
check if the result blob has already been written (could be another instance of the API / worker thread),0
"and if so, skip aggregating and uploading the results, and just generate the SAS URL, which",0
could be needed still if the previous request_status was `problem`.,0
upload the output JSON to the Job folder,0
%% Helper functions *copied* from ct_utils.py and visualization/visualization_utils.py,0
"Determine the factor, which shifts the decimal point of x",0
just behind the last significant digit,0
"Shift decimal point by multiplicatipon with factor, flooring, and",0
division by factor,0
PIL.Image.convert() returns a converted copy of this image,0
alter orientation as needed according to EXIF tag 0x112 (274) for Orientation,0
https://gist.github.com/dangtrinhnt/a577ece4cbe5364aad28,0
https://www.media.mit.edu/pia/Research/deepview/exif.html,0
"%% TFDetector class, an unmodified *copy* of the class in detection/tf_detector.py,",0
so we do not have to import the packages required by run_detector.py,0
Number of decimal places to round to for confidence and bbox coordinates,0
"MegaDetector was trained with batch size of 1, and the resizing function is a part",0
of the inference graph,0
An enumeration of failure reasons,0
"change from [y1, x1, y2, x2] to [x1, y1, width, height]",0
convert numpy floats to Python floats,0
need to change the above line to the following if supporting a batch size > 1 and resizing to the same size,0
"np_images = [np.asarray(image, np.uint8) for image in images]",0
"images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)",0
performs inference,0
our batch size is 1; need to loop the batch dim if supporting batch size > 1,0
%% Scoring script,0
determine if there is metadata attached to each image_id,0
information to determine input and output locations,0
other parameters for the task,0
test that we can write to output path; also in case there is no image to process,0
list images to process,0
"items in this list can be strings or [image_id, metadata]",0
model path,0
"Path to .pb TensorFlow detector model file, relative to the",0
models/megadetector_copies folder in mounted container,0
score the images,0
,0
manage_video_batch.py,0
,0
Notebook-esque script to manage the process of running a local batch of videos,0
through MD.  Defers most of the heavy lifting to manage_local_batch.py .,0
,0
%% Imports and constants,0
%% Split videos into frames,0
"%% List frame files, break into folders",0
Find unique (relative) folders,0
fn = frame_files[0],0
%% List videos,0
%% Check for videos that are missing entirely,0
list(folder_to_frame_files.keys())[0],0
video_filenames[0],0
fn = video_filenames[0],0
%% Check for videos with very few frames,0
%% Print the list of videos that are problematic,0
%% Process images like we would for any other camera trap job,0
"...typically using manage_local_batch.py, but do this however you like, as long",0
as you get a results file at the end.,0
,0
"If you do RDE, remember to use the second folder from the bottom, rather than the",0
bottom-most folder.,0
%% Convert frame results to video results,0
%% Confirm that the videos in the .json file are what we expect them to be,0
%% Scrap,0
%% Test a possibly-broken video,0
%% List videos in a folder,0
%% Imports,0
%% Constants,0
%% Classes,0
class variables,0
"instance variables, in order of when they are typically set",0
Leaving this commented out to remind us that we don't want this check here; let,0
the API fail on these images.  It's a huge hassle to remove non-image,0
files.,0
,0
for path_or_url in images_list:,0
if not is_image_file_or_url(path_or_url):,0
raise ValueError('{} is not an image'.format(path_or_url)),0
Commented out as a reminder: don't check task status (which is a rest API call),0
in __repr__; require the caller to explicitly request status,0
"status=getattr(self, 'status', None))",0
estimate # of failed images from failed shards,0
Download all three JSON urls to memory,0
Remove files that were submitted but don't appear to be images,0
assert all(is_image_file_or_url(s) for s in submitted_images),0
Diff submitted and processed images,0
Confirm that the procesed images are a subset of the submitted images,0
https://www.geeksforgeeks.org/break-list-chunks-size-n-python/,0
%% Interactive driver,0
%%,0
%%,0
%%,0
%% Imports and constants,0
%% Constants I set per script,0
## Required,0
base_output_folder_name = os.path.expanduser('~/postprocessing/' + organization_name_short),0
Shared Access Signature (SAS) tokens for the Azure Blob Storage container.,0
Leading question mark is optional.,0
,0
The read-only token is used for accessing images; the write-enabled token is,0
used for writing file lists.,0
## Typically left as default,0
"Pre-pended to all folder names/prefixes, if they're defined below",0
"This is how we break the container up into multiple taskgroups, e.g., for",0
separate surveys. The typical case is to do the whole container as a single,0
taskgroup.,0
"If your ""folders"" are really logical folders corresponding to multiple folders,",0
map them here,0
"A list of .json files to load images from, instead of enumerating.  Formatted as a",0
"dictionary, like folder_prefixes.",0
This is only necessary if you will be performing postprocessing steps that,0
"don't yet support SAS URLs, specifically the ""subsetting"" step, or in some",0
cases the splitting of files into multiple output directories for,0
empty/animal/vehicle/people.,0
,0
"For those applications, you will need to mount the container to a local drive.",0
For this case I recommend using rclone whether you are on Windows or Linux;,0
rclone is much easier than blobfuse for transient mounting.,0
,0
"But most of the time, you can ignore this.",0
"Supported model_versions: '4', '3', '4_prelim'",0
,0
Also available at the /supported_model_versions and /default_model_version,0
endpoints,0
,0
"Unless you have any specific reason to set this to a non-default value, leave",0
"it at the default, which as of 2020.04.28 is MegaDetector 4.1",0
,0
"additional_task_args = {""model_version"":""4_prelim""}",0
,0
"file_lists_by_folder will contain a list of local JSON file names,",0
each JSON file contains a list of blob names corresponding to an API taskgroup,0
"%% Derived variables, path setup",0
local folders,0
Turn warnings into errors if more than this many images are missing,0
%% Support functions,0
"scheme, netloc, path, query, fragment",0
%% Read images from lists or enumerate blobs to files,0
folder_name = folder_names[0],0
"Load file lists for this ""folder""",0
,0
file_list = input_file_lists[folder][0],0
Write to file,0
A flat list of blob paths for each folder,0
folder_name = folder_names[0],0
"If we don't/do have multiple prefixes to enumerate for this ""folder""",0
"If this is intended to be a folder, it needs to end in '/', otherwise",0
files that start with the same string will match too,0
...for each prefix,0
Write to file,0
...for each folder,0
%% Some just-to-be-safe double-checking around enumeration,0
Make sure each folder has at least one image matched; the opposite is usually a sign of a copy/paste issue,0
...for each image,0
...for each prefix,0
...for each folder,0
Make sure each image comes from one of our folders; the opposite is usually a sign of a bug up above,0
...for each prefix,0
...for each folder,0
...for each image,0
%% Divide images into chunks for each folder,0
The JSON file at folder_chunks[i][j] corresponds to task j of taskgroup i,0
list_file = file_lists_by_folder[0],0
"%% Create taskgroups and tasks, and upload image lists to blob storage",0
periods not allowed in task names,0
%% Generate API calls for each task,0
clipboard.copy(request_strings[0]),0
clipboard.copy('\n\n'.join(request_strings)),0
%% Run the tasks (don't run this cell unless you are absolutely sure!),0
I really want to make sure I'm sure...,0
%% Estimate total time,0
Around 0.8s/image on 16 GPUs,0
%% Manually create task groups if we ran the tasks manually,0
%%,0
"%% Write task information out to disk, in case we need to resume",0
%% Status check,0
print(task.id),0
%% Resume jobs if this notebook closes,0
%% For multiple tasks (use this only when we're merging with another job),0
%% For just the one task,0
%% Load into separate taskgroups,0
p = task_cache_paths[0],0
%% Typically merge everything into one taskgroup,0
"%% Look for failed shards or missing images, start new tasks if necessary",0
List of lists of paths,0
i_taskgroup = 0; taskgroup = taskgroups[i_taskgroup];,0
"Make a copy, because we append to taskgroup",0
i_task = 0; task = tasks[i_task],0
Each taskgroup corresponds to one of our folders,0
Check that we have (almost) all the images,0
Now look for failed images,0
Write it out as a flat list as well (without explanation of failures),0
...for each task,0
...for each task group,0
%% Pull results,0
i_taskgroup = 0; taskgroup = taskgroups[i_taskgroup]; task = taskgroup[0],0
Each taskgroup corresponds to one of our folders,0
...for each task,0
...for each task group,0
%% Combine results from task groups into final output files,0
Check that we have (almost) all the images,0
The only reason we should ever have a repeated request is the case where an,0
"image was missing and we reprocessed it, or where it failed and later succeeded",0
"There may be non-image files in the request list, ignore those",0
Something has gone bonkers if there are images in the results that,0
aren't in the request,0
...for each folder,0
%% Post-processing (no ground truth),0
i_folder = 0; folder_name_raw = folder_names[i_folder],0
"Omit some pages from the output, useful when animals are rare",0
%% Manual processing follows,0
,0
"Everything after this should be considered mostly manual, and no longer includes",0
looping over folders.,0
,0
%% RDE (sample directory collapsing),0
"In this example, the camera created folders called ""100EK113"", ""101EK113"", etc., for every N images",0
"%% Repeat detection elimination, phase 1",0
"Deliberately leaving these imports here, rather than at the top, because this cell is not",0
typically executed,0
To invoke custom collapsing of folders for a particular manufacturer's naming scheme,0
options.customDirNameFunction = remove_overflow_folders,0
Exclude people and vehicles from RDE,0
"options.excludeClasses = [2,3]",0
%% Manual RDE step,0
# DELETE THE VALID DETECTIONS ##,0
%% Re-filtering,0
%% Post-processing (post-RDE),0
i_folder = 0; folder_name_raw = folder_names[i_folder],0
"Omit some pages from the output, useful when animals are rare",0
"print('No RDE file available for {}, skipping'.format(folder_name))",0
continue,0
%% Create a new category for large boxes,0
%% Subsetting,0
i_folder = 0; folder_name = folders[i_folder],0
%% String replacement,0
%% Folder splitting,0
i_folder = 0; folder_name_raw = folder_names[i_folder],0
api_output_file = folder_name_to_combined_output_file[folder_name],0
%% Imports and constants,0
from ai4eutils,0
To specify a non-default confidence threshold for including detections in the .json file,0
Turn warnings into errors if more than this many images are missing,0
Only relevant when we're using a single GPU,0
"Specify a target image size when running MD... strongly recommended to leave this at ""None""",0
Only relevant when running on CPU,0
OS-specific script line continuation character,0
OS-specific script comment character,0
"Prefer threads on Windows, processes on Linux",0
"This is for things like image rendering, not for MegaDetector",0
Should we use YOLOv5's val.py instead of run_detector_batch.py?,1
Directory in which to run val.py.  Only relevant if use_yolo_inference_scripts is True.,0
Should we remove intermediate files used for running YOLOv5's val.py?,0
,0
Only relevant if use_yolo_inference_scripts is True.,0
Should we apply YOLOv5's augmentation?  Only allowed when use_yolo_inference_scripts,0
is True.,0
%% Constants I set per script,0
Optional descriptor,0
model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v5.0.0/md_v5b.0.0.pt'),0
model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v4.1.0/md_v4.1.0.pb'),0
"Number of jobs to split data into, typically equal to the number of available GPUs",0
Only used to print out a time estimate,0
"%% Derived variables, constant validation, path setup",0
%% Enumerate files,0
%% Load files from prior enumeration,0
%% Divide images into chunks,0
%% Estimate total time,0
%% Write file lists,0
%% Generate commands,0
"A list of the scripts tied to each GPU, as absolute paths.  We'll write this out at",0
the end so each GPU's list of commands can be run at once.  Generally only used when,0
"running lots of small batches via YOLOv5's val.py, which doesn't support checkpointing.",0
i_task = 0; task = task_info[i_task],0
Generate the script to run MD,0
Check whether this output file exists,0
Generate the script to resume from the checkpoint (only supported with MD inference code),0
...for each task,0
Write out a script for each GPU that runs all of the commands associated with,0
that GPU.  Typically only used when running lots of little scripts in lieu,0
of checkpointing.,0
...for each GPU,0
%% Run the tasks,0
%%% Run the tasks (commented out),0
i_task = 0; task = task_info[i_task],0
"This will write absolute paths to the file, we'll fix this later",1
...for each chunk,0
...if False,0
"%% Load results, look for failed or missing images in each task",0
i_task = 0; task = task_info[i_task],0
im = task_results['images'][0],0
...for each task,0
%% Merge results files and make images relative,0
im = combined_results['images'][0],0
%% Post-processing (pre-RDE),0
options.sample_seed = 0,0
"Omit some pages from the output, useful when animals are rare",0
%% RDE (sample directory collapsing),0
"In this example, the camera created folders called ""100EK113"", ""101EK113"", etc., for every N images",0
%%,0
%%,0
%%,0
relativePath = image_filenames[0],0
"%% Repeat detection elimination, phase 1",0
"Deliberately leaving these imports here, rather than at the top, because this",0
cell is not typically executed,0
options.minSuspiciousDetectionSize = 0.05,0
This will cause a very light gray box to get drawn around all the detections,0
we're *not* considering as suspicious.,0
options.lineThickness = 5,0
options.boxExpansion = 8,0
To invoke custom collapsing of folders for a particular manufacturer's naming scheme,0
options.customDirNameFunction = remove_overflow_folders,0
Exclude people and vehicles from RDE,0
"options.excludeClasses = [2,3]",0
options.maxImagesPerFolder = 50000,0
options.includeFolders = ['a/b/c'],0
options.excludeFolder = ['a/b/c'],0
"Can be None, 'xsort', or 'clustersort'",0
import clipboard; clipboard.copy(os.path.dirname(suspiciousDetectionResults.filterFile)),0
path_utils.open_file(os.path.dirname(suspiciousDetectionResults.filterFile)),0
%% Manual RDE step,0
# DELETE THE VALID DETECTIONS ##,0
%% Re-filtering,0
%% Post-processing (post-RDE),0
options.sample_seed = 0,0
"Omit some pages from the output, useful when animals are rare",0
"%% Run MegaClassifier (actually, write out a script that runs MegaClassifier)",0
"This is just passed along to the metadata in the output file, it has no impact",0
on how the classification scripts run.,0
#%% Set up environment,0
commands.append('cd CameraTraps/classification\n'),0
commands.append('conda activate cameratraps-classifier\n'),0
#%% Crop images,0
fn = input_files[0],0
#%% Run classifier,0
fn = input_files[0],0
#%% Remap classifier outputs,0
fn = input_files[0],0
#%% Merge classification and detection outputs,0
fn = input_files[0],0
#%% Write  out classification script,0
"%% Run a non-MegaClassifier classifier (i.e., a classifier with no output mapping)",0
"This is just passed along to the metadata in the output file, it has no impact",0
on how the classification scripts run.,0
#%% Set up environment,0
#%% Crop images,0
fn = input_files[0],0
#%% Run classifier,0
fn = input_files[0],0
#%% Merge classification and detection outputs,0
fn = input_files[0],0
#%% Write everything out,0
%% Run the classifier(s) via the .sh script(s) or batch file(s) we just wrote,0
...,0
%% Within-image classification smoothing,0
,0
Only count detections with a classification confidence threshold above,0
"*classification_confidence_threshold*, which in practice means we're only",0
looking at one category per detection.,0
,0
If an image has at least *min_detections_above_threshold* such detections,0
"in the most common category, and no more than *max_detections_secondary_class*",0
"in the second-most-common category, flip all detections to the most common",0
category.,0
,0
"Optionally treat some classes as particularly unreliable, typically used to overwrite an",0
"""other"" class.",0
,0
This cell also removes everything but the non-dominant classification for each detection.,0
,0
How many detections do we need above the classification threshold to determine a dominant category,0
for an image?,0
"Even if we have a dominant class, if a non-dominant class has at least this many classifications",0
"in an image, leave them alone.",0
"If the dominant class has at least this many classifications, overwrite ""other"" classifications",0
What confidence threshold should we use for assessing the dominant category in an image?,0
Which classifications should we even bother over-writing?,0
Detection confidence threshold for things we count when determining a dominant class,0
Which detections should we even bother over-writing?,0
"Before we do anything else, get rid of everything but the top classification",0
for each detection.,0
...for each detection in this image,0
...for each image,0
im = d['images'][0],0
...for each classification,0
...if there are classifications for this detection,0
...for each detection,0
"Handle a quirky special case: if the most common category is ""other"" and",0
"it's ""tied"" with the second-most-common category, swap them",0
secondary_count = category_to_count[keys[1]],0
The 'secondary count' is the most common non-other class,0
If we have at least *min_detections_to_overwrite_other* in a category that isn't,0
"""other"", change all ""other"" classifications to that category",0
...for each classification,0
...if there are classifications for this detection,0
...for each detection,0
"...if we should overwrite all ""other"" classifications",0
"At this point, we know we have a dominant category; change all other above-threshold",0
"classifications to that category.  That category may have been ""other"", in which",0
case we may have already made the relevant changes.,0
det = detections[0],0
...for each classification,0
...if there are classifications for this detection,0
...for each detection,0
...for each image,0
...for each file we want to smooth,0
"%% Post-processing (post-classification, post-within-image-smoothing)",0
classification_detection_file = classification_detection_files[1],0
%% Read EXIF data from all images,0
%% Prepare COCO-camera-traps-compatible image objects for EXIF results,0
import dateutil,0
"This is a standard format for EXIF datetime, and dateutil.parser",0
doesn't handle it correctly.,0
return dateutil.parser.parse(s),0
exif_result = exif_results[0],0
Currently we assume that each leaf-node folder is a location,0
"We collected this image this century, but not today, make sure the parsed datetime",0
jives with that.,0
,0
The latter check is to make sure we don't repeat a particular pathological approach,0
"to datetime parsing, where dateutil parses time correctly, but swaps in the current",0
date when it's not sure where the date is.,0
...for each exif image result,0
%% Assemble into sequences,0
Make a list of images appearing at each location,0
im = image_info[0],0
%% Load classification results,0
Map each filename to classification results for that file,0
%% Smooth classification results over sequences (prep),0
These are the only classes to which we're going to switch other classifications,0
Only switch classifications to the dominant class if we see the dominant class at least,0
this many times,0
"If we see more than this many of a class that are above threshold, don't switch those",0
classifications to the dominant class.,0
"If the ratio between a dominant class and a secondary class count is greater than this,",0
"regardless of the secondary class count, switch those classificaitons (i.e., ignore",0
max_secondary_class_classifications_above_threshold_for_class_smoothing).,0
,0
"This may be different for different dominant classes, e.g. if we see lots of cows, they really",0
"tend to be cows.  Less so for canids, so we set a higher ""override ratio"" for canids.",0
"If there are at least this many classifications for the dominant class in a sequence,",0
"regardless of what that class is, convert all 'other' classifications (regardless of",0
confidence) to that class.,0
"If there are at least this many classifications for the dominant class in a sequence,",0
"regardless of what that class is, classify all previously-unclassified detections",0
as that class.,0
Only count classifications above this confidence level when determining the dominant,0
"class, and when deciding whether to switch other classifications.",0
Confidence values to use when we change a detection's classification (the,0
original confidence value is irrelevant at that point),0
%% Smooth classification results over sequences (supporting functions),0
im = images_this_sequence[0],0
det = results_this_image['detections'][0],0
Only process animal detections,0
Only process detections with classification information,0
"We only care about top-1 classifications, remove everything else",0
Make sure the list of classifications is already sorted by confidence,0
...and just keep the first one,0
"Confidence values should be sorted within a detection; verify this, and ignore",0
...for each detection in this image,0
...for each image in this sequence,0
...top_classifications_for_sequence(),0
Count above-threshold classifications in this sequence,0
Sort the dictionary in descending order by count,0
"Handle a quirky special case: if the most common category is ""other"" and",0
"it's ""tied"" with the second-most-common category, swap them.",0
...def count_above_threshold_classifications(),0
%% Smooth classifications at the sequence level (main loop),0
Break if this token is contained in a filename (set to None for normal operation),0
i_sequence = 0; seq_id = all_sequences[i_sequence],0
Count top-1 classifications in this sequence (regardless of confidence),0
Handy debugging code for looking at the numbers for a particular sequence,0
Count above-threshold classifications for each category,0
"If our dominant category ID isn't something we want to smooth to, don't mess around with this sequence",0
"# Smooth ""other"" classifications ##",0
"By not re-computing ""max_count"" here, we are making a decision that the count used",0
"to decide whether a class should overwrite another class does not include any ""other""",0
classifications we changed to be the dominant class.  If we wanted to include those...,0
,0
sorted_category_to_count = count_above_threshold_classifications(classifications_this_sequence),0
max_count = get_first_value_from_sorted_dictionary(sorted_category_to_count),0
assert dominant_category_id == get_first_key_from_sorted_dictionary(sorted_category_to_count),0
# Smooth non-dominant classes ##,0
Don't flip classes to the dominant class if they have a large number of classifications,0
"Don't smooth over this class if there are a bunch of them, and the ratio",0
if primary to secondary class count isn't too large,0
Default ratio,0
Does this dominant class have a custom ratio?,0
# Smooth unclassified detections ##,0
...for each sequence,0
%% Write smoothed classification results,0
"%% Post-processing (post-classification, post-within-image-and-within-sequence-smoothing)",0
%% Zip .json files,0
%% 99.9% of jobs end here,0
Everything after this is run ad hoc and/or requires some manual editing.,0
%% Compare results files for different model versions (or before/after RDE),0
Choose all pairwise combinations of the files in [filenames],0
%% Merge in high-confidence detections from another results file,0
%% Create a new category for large boxes,0
"This is a size threshold, not a confidence threshold",0
size_options.categories_to_separate = [3],0
%% Preview large boxes,0
%% .json splitting,0
options.query = None,0
options.replacement = None,0
Reminder: 'n_from_bottom' with a parameter of zero is the same as 'bottom',0
%% Custom splitting/subsetting,0
i_folder = 0; folder_name = folders[i_folder],0
"This doesn't do anything in this case, since we're not splitting folders",0
options.make_folder_relative = True,0
%% String replacement,0
%% Splitting images into folders,0
%% Generate commands for a subset of tasks,0
i_task = 8,0
...for each task,0
%% End notebook: turn this script into a notebook (how meta!),0
Exclude everything before the first cell,0
Remove the first [first_non_empty_lines] from the list,0
Add the last cell,0
,0
xmp_integration.py,0
,0
"Tools for loading MegaDetector batch API results into XMP metadata, specifically",0
for consumption in digiKam:,0
,0
https://cran.r-project.org/web/packages/camtrapR/vignettes/camtrapr2.html,0
,0
%% Imports and constants,0
%% Class definitions,0
Folder where images are stored,0
.json file containing MegaDetector output,0
"String to remove from all path names, typically representing a",0
prefix that was added during MegaDetector processing,0
Optionally *rename* (not copy) all images that have no detections,0
above [rename_conf] for the categories in rename_cats from x.jpg to,0
x.check.jpg,0
"Comma-deleted list of category names (or ""all"") to apply the rename_conf",0
behavior to.,0
"Minimum detection threshold (applies to all classes, defaults to None,",0
i.e. 0.0,0
%% Functions,0
Relative image path,0
Absolute image path,0
List of categories to write to XMP metadata,0
Categories with above-threshold detections present for,0
this image,0
Maximum confidence for each category,0
Have we already added this to the list of categories to,0
write out to this image?,0
If we're supposed to compare to a threshold...,0
Else we treat *any* detection as valid...,0
Keep track of the highest-confidence detection for this class,0
If we're doing the rename/.check behavior...,0
Legacy code to rename files where XMP writing failed,0
%% Interactive/test driver,0
%%,0
%% Command-line driver,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Cosmos DB `batch-api-jobs` table for job status,0
"aggregate the number of images, country and organization names info from each job",0
submitted during yesterday (UTC time),0
create the card,0
,0
api_frontend.py,0
,0
"Defines the Flask app, which takes requests (one or more images) from",0
"remote callers and pushes the images onto the shared Redis queue, to be processed",0
by the main service in api_backend.py .,0
,0
%% Imports,0
%% Initialization,0
%% Support functions,0
Make a dict that the request_processing_function can return to the endpoint,0
function to notify it of an error,0
Verify that the content uploaded is not too big,0
,0
request.content_length is the length of the total payload,0
Verify that the number of images is acceptable,0
...def check_posted_data(request),0
%% Main loop,0
Check whether the request_processing_function had an error,0
Write images to temporary files,0
,0
TODO: read from memory rather than using intermediate files,1
"Submit the image(s) for processing by api_backend.py, who is waiting on this queue",0
TODO: convert to a blocking read and eliminate the sleep() statement in this loop,1
"image = Image.open(os.path.join(temp_direc, image_name))",0
...if we do/don't have a request available on the queue,0
...while(True),0
...def detect_sync(),0
%% Command-line driver,0
use --non-docker if you are testing without Docker,0
,0
python api_frontend.py --non-docker,0
# Camera trap real-time API configuration,0
"Full path to the temporary folder for image storage, only meaningful",0
within the Docker container,0
Upper limit on total content length (all images and parameters),0
Minimum confidence threshold for detections,0
Minimum confidence threshold for showing a bounding box on the output image,0
Use this when testing without Docker,0
,0
api_backend.py,0
,0
"Defines the model execution service, which pulls requests (one or more images)",0
"from the shared Redis queue, and runs them through the TF model.",0
,0
%% Imports,0
%% Initialization,0
%% Main loop,0
TODO: convert to a blocking read and eliminate the sleep() statement in this loop,1
Filter the detections by the confidence threshold,0
,0
"Each result is [ymin, xmin, ymax, xmax, confidence, category]",0
,0
"Coordinates are relative, with the origin in the upper-left",0
...if serialized_entry,0
...while(True),0
...def detect_process(),0
%% Command-line driver,0
use --non-docker if you are testing without Docker,0
,0
python api_frontend.py --non-docker,0
run detections on a test image to load the model,0
%%,0
Importing libraries,0
%%,0
%%,0
%%,0
app = typer.Typer(),0
%%,0
@app.command(),0
GPU configuration: set up GPUs based on availability and user specification,0
Environment variable setup for numpy multi-threading,0
Load and set configurations from the YAML file,0
Set a global seed for reproducibility,0
"If the annotation directory does not have a data split, split the data first",0
Replace annotation dir from config with the directory containing the split files,0
Split the data according to the split type,0
Get the path to the annotation files,0
Split training data,0
Split validation and test data,0
Dataset and algorithm loading based on the configuration,0
Logger setup based on the specified logger type,0
Callbacks for model checkpointing and learning rate monitoring,0
Trainer configuration in PyTorch Lightning,0
"Training, validation, or evaluation execution based on the mode",0
%%,0
%%,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%%,0
PyTorch imports,0
"Importing the model, dataset, transformations and utility functions from PytorchWildlife",0
Importing the utility function for saving cropped images,0
Setting the device to use for computations ('cuda' indicates GPU),0
Initializing the MegaDetectorV5 model for image detection,0
Creating a dataset of images with the specified transform,0
Creating a DataLoader for batching and parallel processing of the images,0
Performing batch detection on the images,0
Saving the detected objects as cropped images,0
%%,0
Read the original CSV file,0
Prepare a list to store new records for the new CSV,0
Process the data if the name of the file is in the dataframe,0
Save the crop into a new csv,0
Add record to the new CSV data,0
Create a DataFrame from the new records,0
Define the path for the new CSV file,0
Save the new DataFrame to CSV,0
# DATA SPLITTING,0
Load the data from the csv file,0
Separate the features and the targets,0
First split to separate out the test set,0
Adjust val_size to account for the initial split,0
Second split to separate out the validation set,0
"Combine features, labels, and classification back into dataframes",0
Create the output directory in case that it does not exist,0
Save the splits to new CSV files,0
Return the dataframes,0
Load the data from the csv file,0
Calculate train size based on val and test size,0
Get unique locations,0
Split locations into train and temp (temporary holding for val and test),0
Adjust the proportions for val and test based on the remaining locations,0
"Allocate images to train, validation, and test sets based on their location",0
Save the datasets to CSV files,0
Return the split datasets,0
Load the data from the csv file,0
Convert 'Photo_Time' from string to datetime,0
Calculate train size based on val and test size,0
Sort by 'Photo_Time' to ensure chronological order,0
Group photos into sequences based on a 30-second interval,0
Assign unique sequence IDs to each group,0
Get unique sequence IDs,0
Split sequence IDs into train and temp (temporary holding for val and test),0
Adjust the proportions for val and test based on the remaining sequences,0
"Allocate images to train, validation, and test sets based on their sequence ID",0
Save the datasets to CSV files,0
Return the split datasets,0
Exportable class names for external use,0
Applying the ResNet layers and operations,0
Initialize the network with the specified settings,0
Selecting the appropriate ResNet architecture and pre-trained weights,0
self.pretrained_weights = ResNet18_Weights.IMAGENET1K_V1,0
self.pretrained_weights = ResNet50_Weights.IMAGENET1K_V1,0
Constructing the feature extractor and classifier,0
Criterion for binary classification,0
Load pre-trained weights and adjust for the current model,0
init_weights = self.pretrained_weights.get_state_dict(progress=True),0
Load the weights into the feature extractor,0
Identify missing and unused keys in the loaded weights,0
Import necessary libraries,0
Exportable class names for external use,0
Define normalization mean and standard deviation for image preprocessing,0
Define data transformations for training and validation datasets,0
Load data for prediction,0
Load data for training/validation,0
"Load datasets for different modes (training, validation, testing, prediction)",0
Calculate class counts and label mappings,0
Define parameters for the optimizer,0
Optimizer parameters for feature extraction,0
Optimizer parameters for the classifier,0
Setup optimizer and optimizer scheduler,0
Forward pass,0
Calculate loss,0
Forward pass,0
Forward pass,0
Concatenate outputs from all test steps,0
Calculate the metrics and save the output,0
Forward pass,0
Concatenate outputs from all predict steps,0
Compute the confusion matrix from true labels and predictions,0
Calculate class-wise accuracy (accuracy for each class),0
Calculate micro accuracy (overall accuracy),0
Calculate macro accuracy (mean of class-wise accuracies),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%%,0
Importing necessary basic libraries and modules,0
%%,0
PyTorch imports for tensor operations,0
%%,0
"Importing the models, transformations, and utility functions from PytorchWildlife",0
%%,0
Setting the device to use for computations ('cuda' indicates GPU),0
%%,0
Initializing the model for image detection,0
%%,0
Initializing the model for image classification,0
%%,0
Defining transformations for detection and classification,0
%%,0
Initializing a box annotator for visualizing detections,0
Processing the video and saving the result with annotated detections and classifications,0
%%,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%%,0
Importing basic libraries,0
%%,0
%%,0
"Importing the models, dataset, transformations, and utility functions from PytorchWildlife",0
%%,0
Setting the device to use for computations ('cuda' indicates GPU),0
Initializing a supervision box annotator for visualizing detections,0
Create a temp folder,0
Initializing the detection and classification models,0
Defining transformations for detection and classification,0
%% Defining functions for different detection scenarios,0
Only run classifier when detection class is animal,0
%% Building Gradio UI,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%%,0
Importing necessary basic libraries and modules,0
%%,0
PyTorch imports,0
%%,0
"Importing the model, dataset, transformations and utility functions from PytorchWildlife",0
%%,0
Setting the device to use for computations ('cuda' indicates GPU),0
%%,0
Initializing the MegaDetectorV5 model for image detection,0
%% Single image detection,0
Specifying the path to the target image TODO: Allow argparsing,0
Opening and converting the image to RGB format,0
Initializing the Yolo-specific transform for the image,0
Performing the detection on the single image,0
Saving the detection results,0
%% Batch detection,0
Specifying the folder path containing multiple images for batch detection,0
Creating a dataset of images with the specified transform,0
Creating a DataLoader for batching and parallel processing of the images,0
Performing batch detection on the images,0
%% Output to annotated images,0
Saving the batch detection results as annotated images,0
%% Output to cropped images,0
Saving the detected objects as cropped images,0
%% Output to JSON results,0
Saving the detection results in JSON format,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Making the DetectionImageFolder class available for import from this module,0
Listing and sorting all image files in the specified directory,0
Get image filename and path,0
Load and convert image to RGB,0
Apply transformation if specified,0
Only run recognition on animal detections,0
Get image path and corresponding bbox xyxy for cropping,0
Load and crop image with supervision,0
Apply transformation if specified,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
from yolov5.utils.augmentations import letterbox,0
Making the provided classes available for import from this module,0
Convert PIL Image to Torch Tensor,0
Original shape,0
New shape,0
Scale ratio (new / old) and compute padding,0
Resize image,0
Pad image,0
Convert the image to a PyTorch tensor and normalize it,0
Resize and pad the image using a customized letterbox function.,0
Normalization constants,0
Define the sequence of transformations,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
!!! Output paths need to be optimized !!!,1
!!! Output paths need to be optimized !!!,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%%,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Importing basic libraries,0
Placeholder class-level attributes to be defined in derived classes,0
"If there are size differences in the input images, use a for loop instead of matrix processing for scaling",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Image size for the Opossum classifier,0
Class names for prediction,0
"If pretrained, use the provided URL to fetch the weights",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Making the PlainResNetInference class available for import from this module,0
Following the ResNet structure to extract features,0
Initialize the network and weights,0
... [Missing weight URL definition for ResNet18],0
... [Missing weight URL definition for ResNet50],0
Print missing and unused keys for debugging purposes,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Image size for the Opossum classifier,0
Class names for prediction,0
"If pretrained, use the provided URL to fetch the weights",0
Configuration file for the Sphinx documentation builder.,0
,0
"For the full list of built-in configuration values, see the documentation:",0
https://www.sphinx-doc.org/en/master/usage/configuration.html,0
-- Project information -----------------------------------------------------,0
https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information,0
-- General configuration ---------------------------------------------------,0
https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration,0
-- Options for HTML output -------------------------------------------------,0
https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output,0
-- Options for todo extension ----------------------------------------------,1
https://www.sphinx-doc.org/en/master/usage/extensions/todo.html#configuration,0
"Determine the factor, which shifts the decimal point of x",0
just behind the last significant digit,0
"Shift decimal point by multiplicatipon with factor, flooring, and",0
division by factor,0
_ = pretty_print_object(obj),0
Sloppy that I'm making a module-wide change here...,1
Determine the coordinates of the intersection rectangle,0
The intersection of two axis-aligned bounding boxes is always an,0
axis-aligned bounding box,0
Compute the area of both AABBs,0
Compute the intersection over union by taking the intersection,0
area and dividing it by the sum of prediction + ground-truth,0
areas - the intersection area.,0
%% Functions for running commands as subprocesses,0
%%,0
%% Test driver for execute_and_print,0
%% Parallel test driver for execute_command_and_print,0
Should we use threads (vs. processes) for parallelization?,0
"Only relevant if n_workers == 1, i.e. if we're not parallelizing",0
%% Constants and imports,0
%% Classes,0
Collect all names,0
Make names unique and sort,0
cast location to string as the entries in locations are strings,0
Convert classnames to lowercase to simplify comparisons later,0
Normalize paths to simplify comparisons later,0
"Make custom replacements in filenames, typically used to",0
accommodate changes in root paths after DB construction,0
## Build useful mappings to facilitate working with the DB,0
Category ID <--> name,0
Image filename --> ID,0
Image ID --> image object,0
Image ID --> annotations,0
"Each image can potentially multiple annotations, hence using lists",0
...__init__,0
...class IndexedJsonDb,0
%% Functions,0
Find all unique locations,0
i_location = 0; location = locations[i_location],0
Sorting datetimes fails when there are None's in the list.  So instead of sorting datetimes,0
"directly, sort tuples with a boolean for none-ness, then the datetime itself.",0
,0
https://stackoverflow.com/questions/18411560/sort-list-while-pushing-none-values-to-the-end,0
previous_datetime = sorted_images_this_location[0]['datetime'],0
im = sorted_images_this_location[1],0
"Start a new sequence if necessary, including the case where this datetime is invalid",0
"If this was an invalid datetime, this will record the previous datetime",0
"as None, which will force the next image to start a new sequence.",0
...for each image in this location,0
Fill in seq_num_frames,0
...for each location,0
...create_sequences(),0
,0
cct_to_md.py,0
,0
"""Converts"" a COCO Camera Traps file to a MD results file.  Currently ignores",0
"non-bounding-box annotations, and gives all annotations a confidence of 1.0.",0
,0
The only reason to do this is if you are going to add information to an existing,0
"CCT-formatted dataset, and want to do that in Timelapse.",0
,0
"Currently assumes that width and height are present in the input data, does not",0
read them from images.,0
,0
%% Constants and imports,0
%% Functions,0
# Validate input,0
# Read input,0
# Prepare metadata,0
ann = d['annotations'][0],0
# Process images,0
im = d['images'][0],0
"MegaDetector: [x,y,width,height] (normalized, origin upper-left)",0
"CCT: [x,y,width,height] (absolute, origin upper-left)",0
...if there's a bounding box,0
...for each annotation,0
This field is no longer included in MD output files by default,0
im_out['max_detection_conf'] = max_detection_conf,0
...for each image,0
# Write output,0
...cct_to_md(),0
%% Command-line driver,0
TODO,1
%% Interactive driver,0
%%,0
%%,0
,0
cct_json_to_filename_json.py,0
,0
"Given a .json file in COCO Camera Traps format, outputs a .json-formatted list of",0
relative file names present in the CCT file.,0
,0
%% Constants and environment,0
%% Main function,0
"json.dump(s,open(outputFilename,'w'))",0
%% Command-line driver,0
%% Interactive driver,0
%%,0
,0
cct_to_csv.py,0
,0
"""Converts"" a COCO Camera Traps .json file to .csv, in quotes because",0
"all kinds of assumptions are made here, and if you have a particular .csv",0
"format in mind, YMMV.  Most notably, does not include any bounding box information",0
or any non-standard fields that may be present in the .json file.  Does not,0
propagate information about sequence-level vs. image-level annotations.,0
,0
"Does not assume access to the images, therefore does not open .jpg files to find",0
"datetime information if it's not in the metadata, just writes datetime as 'unknown'.",0
,0
%% Imports,0
%% Main function,0
#%% Read input,0
#%% Build internal mappings,0
annotation = annotations[0],0
#%% Write output file,0
im = images[0],0
Write out one line per class:,0
...for each class name,0
...for each image,0
...with open(output_file),0
...def cct_to_csv,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
,0
remove_exif.py,0
,0
"Removes all EXIF/IPTC/XMP metadata from a folder of images, without making",0
"backup copies, using pyexiv2.",0
,0
%% Imports and constants,0
%% List files,0
%% Remove EXIF data (support),0
PYEXIV2 IS NOT THREAD SAFE; DO NOT CALL THIS IN PARALLEL FROM A SINGLE PROCESS,1
data = img.read_exif(); print(data),0
%% Debug,0
%%,0
%%,0
%% Remove EXIF data (execution),0
fn = image_files[0],0
"joblib.Parallel defaults to a process-based backend, but let's be sure",0
"results = Parallel(n_jobs=n_exif_threads,verbose=2,prefer='processes')(delayed(remove_exif)(fn) for fn in image_files[0:10])",0
,0
yolo_to_coco.py,0
,0
Converts a YOLO-formatted dataset to a COCO-formatted dataset.,0
,0
"Currently supports only a single folder (i.e., no recursion).  Treats images without",0
corresponding .txt files as empty.,0
,0
%% Imports and constants,0
from ai4eutils,0
%% Support functions,0
Validate input,0
Class names,0
Blank lines should only appear at the end,0
Enumerate images,0
fn = image_files[0],0
Create the image object for this image,0
Is there an annotation file for this image?,0
"This is an image with no annotations, currently don't do anything special",0
here,0
s = lines[0],0
"COCO: [x_min, y_min, width, height] in absolute coordinates",0
"YOLO: [class, x_center, y_center, width, height] in normalized coordinates",0
...for each annotation,0
...if this image has annotations,0
...for each image,0
...def yolo_to_coco(),0
%% Interactive driver,0
%% Convert YOLO folders to COCO,0
%% Check DB integrity,0
%% Preview some images,0
%% Command-line driver,0
TODO,1
,0
read_exif.py,0
,0
"Given a folder of images, read relevant metadata (EXIF/IPTC/XMP) fields from all images,",0
and write them to  a .json or .csv file.,0
,0
This module can use either PIL (which can only reliably read EXIF data) or exiftool (which,0
can read everything).  The latter approach expects that exiftool is available on the system,0
path.  No attempt is made to be consistent in format across the two approaches.,0
,0
%% Imports and constants,0
From ai4eutils,0
%% Options,0
Number of concurrent workers,0
Should we use threads (vs. processes) for parallelization?,0
,0
Not relevant if n_workers is 1.,0
Should we use exiftool or pil?,0
%% Functions,0
exif_tags = img.info['exif'] if ('exif' in img.info) else None,0
print('Warning: unrecognized EXIF tag: {}'.format(k)),0
"-G means ""Print group name for each tag"", e.g. print:",0
,0
[File]          Bits Per Sample                 : 8,0
,0
...instead of:,0
,0
Bits Per Sample                 : 8,0
A list of three-element lists (type/tag/value),0
line_raw = exif_lines[0],0
A typical line:,0
,0
[ExifTool]      ExifTool Version Number         : 12.13,0
"Split on the first occurrence of "":""",0
...for each output line,0
...which processing library are we using?,0
...read_exif_tags_for_image(),0
...populate_exif_data(),0
Enumerate *relative* paths,0
Find all EXIF tags that exist in any image,0
...for each tag in this image,0
...for each image,0
Write header,0
...for each key that *might* be present in this image,0
...for each image,0
...with open(),0
...if we're writing to .json/.csv,0
https://stackoverflow.com/questions/11210104/check-if-a-program-exists-from-a-python-script,0
%% Interactive driver,0
%%,0
output_file = os.path.expanduser('~/data/test-exif.csv'),0
options.processing_library = 'pil',0
"file_path = os.path.join(input_folder,'KRU_S1_11_R1_IMAG0148.JPG')",0
%%,0
%% Command-line driver,0
,0
"Given a json-formatted list of image filenames, retrieve the width and height of every image.",0
,0
%% Constants and imports,0
%% Processing functions,0
Is this image on disk?,0
"all_results = list(tqdm(pool.imap(process_image, filenames), total=len(filenames)))",0
%% Interactive driver,0
%%,0
List images in a test folder,0
%%,0
"process_list_file(image_list_file,image_size_file,image_prefix=base_dir)",0
%% Command-line driver,0
"python sanity_check_json_db.py ""e:\wildlife_data\wellington_data\wellington_camera_traps.json"" --baseDir ""e:\wildlife_data\wellington_data\images"" --bFindUnusedImages --bCheckImageSizes",0
"python sanity_check_json_db.py ""D:/wildlife_data/mcgill_test/mcgill_test.json"" --baseDir ""D:/wildlife_data/mcgill_test"" --bFindUnusedImages --bCheckImageSizes",0
"Here the '-u' prevents buffering, which makes tee happier",0
,0
python -u sanity_check_json_db.py '/datadrive1/nacti_metadata.json' --baseDir '/datadrive1/nactiUnzip/' --bFindUnusedImages --bCheckImageSizes | tee ~/nactiTest.out,0
,0
Partially-formed stub to get from MegaDetector output files to COCO Camera Traps data.,0
,0
"Was actually written to convert *many* MD .json files to a single CCT file, hence",0
the loop over .json files.,0
,0
"THIS CODE HAS NEVER BEEN RUN, it was added as a demonstration of how to do this.  YMMV.",0
,0
"You may find a more polished, command-line-ready version of this code at:",0
,0
https://github.com/StewartWILDlab/mdtools,0
,0
%% Constants and imports,0
"Images sizes are required to convert between absolute and relative coordinates,",0
so we need to read the images.,0
Only required if you want to write a database preview,0
%% Create CCT dictionaries,0
image_ids_to_images = {},0
Force the empty category to be ID 0,0
Load .json annotations for this data set,0
i_entry = 0; entry = data['images'][i_entry],0
,0
"PERF: Not exactly trivially parallelizable, but about 100% of the",0
time here is spent reading image sizes (which we need to do to get from,0
"absolute to relative coordinates), so worth parallelizing.",0
Generate a unique ID from the path,0
detection = detections[0],0
Have we seen this category before?,0
Create an annotation,0
"MegaDetector: [x,y,width,height] (normalized, origin upper-left)",0
"CCT: [x,y,width,height] (absolute, origin upper-left)",0
...for each detection,0
...for each image,0
Remove non-reviewed images and associated annotations,0
%% Create info struct,0
%% Write .json output,0
%% Clean start,0
## Everything after this should work from a clean start ###,0
%% Validate output,0
%% Preview animal labels,0
%% Preview empty labels,0
"viz_options.classes_to_exclude = ['empty','human']",0
,0
generate_crops_from_cct.py,0
,0
"Given a .json file in COCO Camera Traps format, create a cropped image for",0
each bounding box.,0
,0
%% Imports and constants,0
%% Functions,0
# Read and validate input,0
# Find annotations for each image,0
"This actually maps image IDs to annotations, but only to annotations",0
containing boxes,0
# Generate crops,0
TODO: parallelize this loop,1
im = d['images'][0],0
Load the image,0
Generate crops,0
i_ann = 0; ann = annotations_this_image[i_ann],0
"x/y/w/h, origin at the upper-left",0
...for each box,0
...for each image,0
...generate_crops_from_cct(),0
%% Interactive driver,0
%%,0
%%,0
%%,0
%% Command-line driver,0
TODO,1
%% Scrap,0
%%,0
,0
coco_to_yolo.py,0
,0
Converts a COCO-formatted dataset to a YOLO-formatted dataset.,0
,0
"If the input and output folders are the same, writes .txt files to the input folder,",0
and neither moves nor modifies images.,0
,0
"Currently ignores segmentation masks, and errors if an annotation has a",0
segmentation polygon but no bbox,0
,0
Has only been tested on a handful of COCO Camera Traps data sets; if you,0
"use it for more general COCO conversion, YMMV.",0
,0
%% Imports and constants,0
%% Support functions,0
Validate input,0
Read input data,0
Parse annotations,0
i_ann = 0; ann = data['annotations'][0],0
Make sure no annotations have *only* segmentation data,0
Re-map class IDs to make sure they run from 0...n-classes-1,0
,0
"TODO: this allows unused categories in the output data set, which I *think* is OK,",1
but I'm only 81% sure.,0
Process images (everything but I/O),0
"List of dictionaries with keys 'source_image','dest_image','bboxes','dest_txt'",0
i_image = 0; im = data['images'][i_image],0
"assert os.path.isfile(source_image), 'Could not find image {}'.format(source_image)",0
If this annotation has no bounding boxes...,0
"This is not entirely clear from the COCO spec, but it seems to be consensus",0
"that if you want to specify an image with no objects, you don't include any",0
annotations for that image.,0
We allow empty bbox lists in COCO camera traps; this is typically a negative,0
"example in a dataset that has bounding boxes, and 0 is typically the empty",0
category.,0
...if this is an empty annotation,0
"COCO: [x_min, y_min, width, height] in absolute coordinates",0
"YOLO: [class, x_center, y_center, width, height] in normalized coordinates",0
Convert from COCO coordinates to YOLO coordinates,0
...for each annotation,0
...if this image has annotations,0
...for each image,0
Write output,0
Category IDs should range from 0..N-1,0
TODO: parallelize this loop,1
,0
output_info = images_to_copy[0],0
Only write an annotation file if there are bounding boxes.  Images with,0
"no .txt files are treated as hard negatives, at least by YOLOv5:",0
,0
https://github.com/ultralytics/yolov5/issues/3218,0
,0
"I think this is also true for images with empty annotation files, but",0
"I'm using the convention suggested on that issue, i.e. hard negatives",0
are expressed as images without .txt files.,0
bbox = bboxes[0],0
...for each image,0
...def coco_to_yolo(),0
%% Interactive driver,0
%% CCT data,0
"If preview_export is True, I'm exporting to preview these with BoundingBoxEditor:",0
,0
https://github.com/mfl28/BoundingBoxEditor,0
,0
"This export will be compatible, other than the fact that you need to move",0
"""object.data"" into the ""labels"" folder.",0
,0
"Otherwise I'm exporting for training, in the YOLOv5 flat format.",0
%% Command-line driver,0
TODO,1
,0
cct_to_wi.py,0
,0
Converts COCO Camera Traps .json files to the Wildlife Insights,0
batch upload format,0
,0
Also see:,0
,0
https://github.com/ConservationInternational/Wildlife-Insights----Data-Migration,0
,0
https://data.naturalsciences.org/wildlife-insights/taxonomy/search,0
,0
%% Imports,0
%% Paths,0
A COCO camera traps file with information about this dataset,0
A .json dictionary mapping common names in this dataset to dictionaries with the,0
"WI taxonomy fields: common_name, wi_taxon_id, class, orer, family, genus, species",0
%% Constants,0
%% Project information,0
%% Read templates,0
%% Compare dictionary to template lists,0
Write the header,0
Write values,0
%% Project file,0
%% Camera file,0
%% Deployment file,0
%% Images file,0
Read .json file with image information,0
Read taxonomy dictionary,0
Populate output information,0
df = pd.DataFrame(columns = images_fields),0
annotation = annotations[0],0
im = input_data['images'][0],0
"We don't have counts, but we can differentiate between zero and 1",0
This is the label mapping used for our incoming iMerit annotations,0
"Only used to parse the incoming annotations. In our database, the string name is used to avoid confusion",0
MegaDetector outputs,0
,0
add_bounding_boxes_to_megadb.py,0
,0
"Given COCO-formatted JSONs containing manually labeled bounding box annotations, add them to",0
"MegaDB sequence entries, which can then be ingested into MegaDB.",0
the category map that comes in the COCO JSONs for iMerit batch 12 - to check that each,0
JSON,0
"dataset name : (seq_id, frame_num) : [bbox, bbox]",0
where bbox is a dict with str 'category' and list 'bbox',0
iterate over image_id_to_image rather than image_id_to_annotations so we include,0
the confirmed empty images,0
The file_name field in the incoming json looks like,0
alka_squirrels.seq2020_05_07_25C.frame119221.jpg,0
there seems to be a bug in the annotations where sometimes there's a,0
non-empty label along with a label of category_id 5,0
ignore the empty label (they seem to be actually non-empty),0
,0
add_bounding_boxes_to_json.py,0
,0
This script takes a image database in the COCO Camera Traps format and merges in a set of bounding,0
box annotations in the format that iMerit uses (a .json where actually only each row is a valid json).,0
,0
"If you need to update an existing bbox database, please get all the original annotation files and",0
re-generate from scratch,0
,0
%% Imports,0
%% Configurations and paths,0
images database,0
output bboxes database,0
annotation files (pseudo json) obtained from our annotation vendor that contain annotations for this dataset,0
None or a string or tuple of strings that is the prefix to all file_name of interest / in this dataset in the annotation files,0
functions for mapping the image_id in the annotation files (pseudo jsons) to the image_id used in the image DB,0
our img_id doesn't contain frame info,0
"batch3 - ""file_name"":""ASG0000019_0_S1_B06_R1_PICT0007.JPG""",0
"batch5 and 7 - ""file_name"":""datasetsnapshotserengeti.seqASG000002m-frame0.imgS1_B06_R1_PICT0056.JPG""",0
sometimes - 'datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG',0
"id in DB (old_token): 'S6/J01/J01_R1/S6_J01_R1_IMAG0001', 'S1/B05/B05_R1/S1_B05_R1_PICT0036'",0
specify which one to use for your dataset here,0
%% Load the image database and fill in DB info for the output bbox database,0
load the images database,0
%% Find the height and width of images from the annotation files,0
,0
...if they are not available in the images DB,0
each row in this pseudo-json is a COCO formatted entry for an image sequence,0
%% Other functions required by specific datasets,0
the IDFG image database does not include images from unlabeled folders that were annotated with bounding boxes,0
%% Create the bbox database from all annotation files pertaining to this dataset,0
"for the incoming annotations, look up by category name (common) and convert to the numerical id used in our databases",0
"for each annotation pseudo-json, check that the image it refers to exists in the original database",0
each row in this pseudo-json is a COCO formatted entry for an image sequence,0
check that entry is for this dataset,0
category map for this entry in the annotation file - usually the same across all entries but just in case,0
rspb_add_image_entry(img_id),0
use the image length and width in the image DB,0
"[top left x, top left y, width, height] in relative coordinates",0
"add all images that have been sent to annotation, some of which may be empty of bounding boxes",0
rspb_add_image_entry(db_image_id),0
%% Common queries,0
This query is used when preparing tfrecords for object detector training.,0
We do not want to get the whole seq obj where at least one image has bbox because,0
some images in that sequence will not be bbox labeled so will be confusing.,0
Include images with bbox length 0 - these are confirmed empty by bbox annotators.,0
"If frame_num is not available, it will not be a field in the result iterable.",0
"Note that the seq_id is the Cosmos DB assigned ID for that sequence, not the",0
"seq_id field, which may contain ""/"" characters.",0
"For public datasets to be converted to the CCT format, we get the whole seq object because",0
sequence level attributes need to be included too. megadb/converters/megadb_to_cct.py handles,0
the case of bbox-only JSONs with the flag exclude_species_class,0
Getting all sequences in a dataset - for updating or deleting entries which need the id field,0
%% Parameters,0
Use None if querying across all partitions,0
"The `sequences` table has the `dataset` as the partition key, so if only querying",0
"entries from one dataset, set the dataset name here.",0
"e.g. {'name': '@top_n', 'value': 100} - see query_and_upsert_examples/query_for_data.ipynb",0
Use False if do not want all results stored in a single JSON.,0
%% Script,0
execute the query,0
loop through and save the results,0
MODIFY HERE depending on the query,0
wiitigers Unicode issue - no good mapping from DB file names to file names in blob URL,0
build filename,0
if need to re-download a dataset's images in case of corruption,0
entries_to_download = {,0
"filename: entry for filename, entry in entries_to_download.items()",0
if entry['dataset'] == DATASET,0
},0
input validation,0
"existing files, with paths relative to <store_dir>",0
parse JSON or TXT file,0
"create a new storage container client for this dataset,",0
and cache it,0
schema already checks that the min possible value of frame_num is 0,0
"if there are more than one image item, each needs a frame_num",0
checks across all sequence items,0
per sequence item checks,0
load the schema,0
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory,0
load the schema,0
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory,0
checks across all datasets items,0
check for expiry date of the SAS keys,0
update the sequences,0
"`id` is from the database, as well as all attributes starting with _",0
"if valuable sequence information is available, add them to the image",0
required fields for an image object,0
add seq-level class labels for this image,0
add other sequence-level properties to each image too,0
add other image-level properties,0
... for im in seq['images'],0
... for seq in mega_db,0
consolidate categories,0
some property names have changed in the new schema,0
a dummy sequence ID will be generated if the image entry does not have a seq_id field,0
seq_id only needs to be unique within this dataset; MegaDB does not rely on it as the _id field,0
"""annotations"" fields are opened and have its sub-field surfaced one level up",0
set the `dataset` property on each sequence to the provided dataset_name,0
check that the location field is the same for all images in a sequence,0
check which fields in a CCT image entry are sequence-level,0
image-level properties that really should be sequence-level,0
need to add (misidentified) seq properties not present for each image in a sequence to img_level_properties,0
"(some properties act like flags - all have the same value, but not present on each img)",0
add the sequence-level properties to the sequence objects,0
not every sequence have to have all the seq_level_properties,0
get the value of this sequence-level property from the first image entry,0
check which fields are really dataset-level and should be included in the dataset table instead.,0
delete sequence-level properties that should be dataset-level,0
make all `class` fields lower-case; cast `seq_id` to type string in case they're integers,0
turn all float NaN values into None so it gets converted to null when serialized,0
this was an issue in the Snapshot Safari datasets,0
%% validation,0
"at first a dict of image_id: image_obj with annotations embedded, then it becomes",0
an array of image objects,0
%% integrate the image DB,0
takes in image entries and species and other annotations in the image DB,0
convert the species category to explicit string name,0
there may be other fields in the annotation object,0
these fields should already be gotten from the image object,0
%% integrate the bbox DB,0
add any images that are not in the image DB,0
also add any fields in the image object that are not present already,0
add bbox to the annotations field,0
for any newly added images,0
"'bbox_abs': bbox_anno['bbox'],",0
not keeping height and width,0
,0
jb_csv_to_json.py,0
,0
Convert a particular .csv file to CCT format.  Images were not available at,0
"the time I wrote this script, so this is much shorter than other scripts",0
in this folder.,0
,0
%% Constants and environment,0
%% Read source data,0
%% Confirm filename uniqueness (this data set has one label per image),0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
%% Create CCT dictionaries,0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
imageName = imageFilenames[0],0
"As per above, this is convenient and appears to be true; asserting to be safe",0
"Filenames look like ""290716114012001a1116.jpg""",0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
%% Create info struct,0
%% Write output,0
%% Sanity-check,0
%% Imports and constants,0
%% Load data,0
"with open(image_json,'r') as f:",0
data = json.load(f),0
%% Sanity-check data,0
%% Label previews,0
%% Collect images to annotate,0
%% Sort by sequence and frame,0
%% Copy to a folder by GUID,0
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension,0
im = images_to_annotate[0],0
...for each image,0
%% Write out the annotation list,0
%% Constants and imports,0
%% Enumerate files,0
edited_image_folder = edited_image_folders[0],0
fn = edited_image_files[0],0
%% Read metadata and capture location information,0
i_row = 0; row = df.iloc[i_row],0
Sometimes '2017' was just '17' in the date column,0
%% Read the .json files and build output dictionaries,0
json_fn = json_files[0],0
if 'partial' in json_fn:,0
continue,0
line = lines[0],0
if image_fn == 'SD1_238_6_26_17_16_76.73.jpg':,0
asdfad,0
SD29_079_5_14_2018_17_52.85.jpg,0
Re-write two-digit years as four-digit years,0
Sometimes the year was written with two digits instead of 4,0
assert len(tokens[4]) == 4 and tokens[4].startswith('20'),0
Have we seen this location already?,0
"Not a typo, it's actually ""formateddata""",0
An image shouldn't be annotated as both empty and non-empty,0
An image shouldn't be annotated as both empty and non-empty,0
box = formatteddata[0],0
"open_file(os.path.join(base_folder,jpeg_image_folder,image_fn))",0
...for each box,0
...if there are boxes on this image,0
...for each line,0
...with open(),0
...for each json file,0
%% Prepare the output .json,0
%% Check DB integrity,0
%% Print unique locations,0
SD12_202_6_23_2017_1_31.85.jpg,0
%% Preview some images,0
%% Statistics,0
,0
"Import a Snapshot Safari project (one project, one season)",0
,0
Before running this script:,0
,0
"* Mount the blob container where the images live, or copy the",0
images to local storage,0
,0
What this script does:,0
,0
* Creates a .json file,0
* Creates zip archives of the season without humans.,0
* Copies animals and humans to separate folders,0
,0
After running this script:,0
,0
* Create or update LILA page,0
* Push zipfile and unzipped images to LILA,0
* Push unzipped humans to wildlifeblobssc,0
* Delete images from UMN uplaod storage,0
,0
"Snapshot Serengeti is handled specially, because we're dealing with bounding",0
boxes too.  See snapshot_serengeti_lila.py.,0
,0
%% Imports,0
From ai4eutils,0
From CameraTraps,0
%% Constants,0
project_name = 'XXX'; season_name = 'S1'; project_friendly_name = 'Snapshot Unknown',0
project_name = 'SER'; season_name = 'S1-11'; project_friendly_name = 'Snapshot Serengeti',0
project_name = 'KRU'; season_name = 'S1'; project_friendly_name = 'Snapshot Kruger',0
project_name = 'CDB'; season_name = 'S1'; project_friendly_name = 'Snapshot Camdeboo',0
project_name = 'MTZ'; season_name = 'S1'; project_friendly_name = 'Snapshot Mountain Zebra',0
project_name = 'ENO'; season_name = 'S1'; project_friendly_name = 'Snapshot Enonkishu',0
project_name = 'KAR'; season_name = 'S1'; project_friendly_name = 'Snapshot Karoo',0
project_name = 'KGA'; season_name = 'S1'; project_friendly_name = 'Snapshot Kgalagadi',0
%% Folder/file creation,0
E.g. KRU_S1,0
E.g. Z:\KRU,0
E.g. Z:\KRU\KRU_S1,0
Contains annotations for each capture event (sequence),0
Maps image IDs to filenames; each line looks like:,0
,0
"KRU_S1#1#1#2,3,KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0004.JPG",0
"Total number of each answer to each question, e.g. total number of times each species was identified",0
,0
Not used here,1
Create output folders,0
Images will be placed in a season-specific folder inside this (the source data includes,0
this in path names),0
%% Load metadata files,0
%% Convert to dictionaries (prep),0
%% Convert to dictionaries (loops),0
"TODO: iterrows() is a terrible way to do this, but this is one of those days",1
"where I want to get this done, not get better at Python.",0
irow = 0; row = image_table.iloc[0],0
"Loaded as an int64, converting to int here",0
...for each row in the image table,0
Make sure image IDs are what we think they are,0
...for each row in the annotation table,0
%% Take a look at categories (just sanity-checking),0
print('\nCategories by species:'),0
pp.pprint(categories_by_species),0
%% Fill in some image fields we didn't have when we created the image table,0
"width, height, corrupt, seq_num_frames, location, datetime",0
Every annotation in this list should have the same sequence ID,0
"Figure out ""seq_num_frames"", which really should be done in a separate lopp;",0
there's no reason to do this redundantly for every image,0
Every image in this sequence should point back to the same equence,0
Every annotation in this list should have the same location,0
Every annotation in this list should have the same datetime,0
Is this image on disk?,0
iImage = 0; im = images[0],0
...for each image,0
"images_processed = pool.map(process_image, images)",0
"images_processed = list(tqdm(pool.imap_unordered(process_image, images), total=len(images)))",0
%% Count missing/corrupted images,0
%% Print distribution of sequence lengths (sanity-check),0
%% Replicate annotations across images,0
iAnn = 0; ann = annotations[iAnn],0
%% See what files are on disk but not annotated,0
%% Sanity-check image and annotation uniqueness,0
%% Minor updates to fields,0
%% Write .json file,0
%% Create a list of human files,0
ann = annotations[0],0
%% Create public archive and public/private folders,0
im = images[0],0
E.g. KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0001.JPG,0
Copy to private output folder,0
Add to zipfile,0
Possibly start a new archive,0
Copy to public output folder,0
...for each image,0
%% Sanity-check .json file,0
"This will produce some validation errors, because this zipfile doesn't include humans",0
%% Zip up .json and .csv files,0
%% When I skip to this part (using a pre-rendered .json file),0
%%,0
%%,0
ann = annotations[0],0
%% Summary prep for LILA,0
"%% Generate preview, sanity-check labels",0
"viz_options.classes_to_include = ['jackalblackbacked','bustardkori']",0
%% Scrap,0
%% Find annotations for a particular image,0
%% Write a list of missing images,0
,0
save_the_elephants_survey_A.py,0
,0
Convert the .csv file provided for the Save the Elephants Survey A data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
Handle all unstructured fields in the source data as extra fields in the annotations,0
"photo_type really should be an image property, but there are a few conflicts",0
that forced me to handle it as an annotation proprerty,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
Maps relative paths to row indices in input_metadata,0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
Ignore directories,0
%% Make sure the multiple-annotation cases make sense,0
%%,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
Force the empty category to be ID 0,0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image],0
Example filename:,0
,0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG',0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG',0
i_row = row_indices[0],0
There are a small handful of datetime mismatches across annotations,0
for the same image,0
assert im['datetime'] == timestamp,0
Special cases based on the 'photo type' field,0
Various spellings of 'community',0
Have we seen this category before?,0
Create an annotation,0
fieldname = list(mapped_fields.keys())[0],0
...for each row,0
...for each image,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
%% Scrap,0
%% Find unique photo types,0
##,0
,0
timelapse_csv_set_to_json.py,0
,0
Given a directory full of reasonably-consistent Timelapse-exported,0
".csvs, assemble a CCT .json.",0
,0
"Assumes that you have a list of all files in the directory tree, including",0
image and .csv files.,0
,0
##,0
%% Constants and imports,0
Text file with relative paths to all files (images and .csv files),0
"%% Read file list, make a list of all image files and all .csv files",0
"%% Verify column consistency, create a giant array with all rows from all .csv files",0
i_csv = 0; csv_filename = csv_files[0],0
%% Prepare some data structures we'll need for mapping image rows in .csv files to actual image files,0
Enumerate all folders containing image files,0
"In this data set, a site folder looks like:",0
,0
Processed Images\\site_name,0
%% Map .csv files to candidate camera folders,0
fn = valid_csv_files[0],0
"Some site folders appear as ""XXNNNN"", some appear as ""XXNNNN_complete""",0
...for each .csv file,0
%% Map camera folders to candidate image folders,0
%% Main loop over labels (prep),0
Force the empty category to be ID 0,0
Images that are marked empty and also have a species label,0
%% Main loop over labels (loop),0
i_row = 0; row = input_metadata.iloc[i_row],0
"for i_row,row in input_metadata.iterrows():",0
"Usually this is just a single folder name, sometimes it's a full path,",0
which we don't want,0
Check whether this file exists on disk,0
Retrieve image width and height,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each row in the big table of concatenated .csv files,0
%% Fix cases where an image was annotated as 'unlabeled' and as something else,0
This annotation is 'unlabeled',0
Was there another category associated with this image?,0
%% Check for un-annnotated images,0
Enumerate all images,0
list(relative_path_to_image.keys())[0],0
%% Create info struct,0
%% Write output,0
%% Sanity-check the database's integrity,0
%% Render a bunch of images to make sure the labels got carried along correctly,0
,0
auckland_doc_to_json.py,0
,0
Convert Auckland DOC data set to COCO camera traps format.  This was,0
for a training data set where class names were encoded in path names.,0
,0
%% Constants and imports,0
Filenames will be stored in the output .json relative to this base dir,0
%% Enumerate files,0
%% Assemble dictionaries,0
Force the empty category to be ID 0,0
fn = image_files[0]; print(fn),0
Typically y:\Maukahuka_Auckland_Island\1_Training\Winter_Trial_2019\cat\cat\eat\20190903_IDdY_34_E3_tmp_201908240051.JPG,0
"This data set has two top-level folders, ""1_Training"" (which has class names encoded",0
"in paths) and ""2_Testing"" (which has no class information).",0
...for each image,0
%% Write output .json,0
%% Write train/test .jsons,0
%% Validate .json files,0
%% Preview labels,0
checkpoint,0
,0
awc_to_json.py,0
,0
Convert a particular .csv file to CCT format.,0
,0
%% Constants and environment,0
%% Read source data,0
%% Main loop over labels,0
Force the empty category to be ID 0,0
iRow = 0; row = input_metadata.iloc[iRow],0
"ImageID,FileName,FilePath,SpeciesID,CommonName",0
"This makes an assumption of one annotation per image, which happens to be",0
true in this data set.,0
Retrieve image width and height,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
list(relativePathToImage.keys())[0],0
%% Create info struct,0
%% Write output,0
%% Sanity-check the database's integrity,0
%% Render a bunch of images to make sure the labels got carried along correctly,0
,0
idaho-camera-traps.py,0
,0
Prepare the Idaho Camera Traps dataset for release on LILA.,0
,0
%% Imports and constants,0
Multi-threading for .csv file comparison and image existence validation,0
"We are going to map the original filenames/locations to obfuscated strings, but once",0
"we've done that, we will re-use the mappings every time we run this script.",0
This is the file to which mappings get saved,0
The maximum time (in seconds) between images within which two images are considered the,0
same sequence.,0
"This is a two-column file, where each line is [string in the original metadata],[category name we want to map it to]",0
"The output file, using the original strings",0
"The output file, using obfuscated strings for everything but filenamed",0
"The output file, using obfuscated strings and obfuscated filenames",0
"One time only, I ran MegaDetector on the whole dataset...",0
...then set aside any images that *may* have contained humans that had not already been,0
annotated as such.  Those went in this folder...,0
...and the ones that *actually* had humans (identified via manual review) got,0
copied to this folder...,0
"...which was enumerated to this text file, which is a manually-curated list of",0
images that were flagged as human.,0
Unopinionated .json conversion of the .csv metadata,0
%% List files (images + .csv),0
Ignore .csv files in folders with multiple .csv files,0
...which would require some extra work to decipher.,0
fn = csv_files[0],0
%% Parse each .csv file into sequences (function),0
csv_file = csv_files[-1],0
os.startfile(csv_file_absolute),0
survey = csv_file.split('\\')[0],0
Sample paths from which we need to derive locations:,0
,0
St.Joe_elk\AM99\Trip 1\100RECNX\TimelapseData.csv,0
Beaverhead_elk\AM34\Trip 1\100RECNX\TimelapseData.csv,0
,0
ClearCreek_mustelids\Winter2015-16\FS-001-P\FS-001-P.csv,0
ClearCreek_mustelids\Summer2015\FS-001\FS-001.csv,0
ClearCreek_mustelids\Summer2016\IDFG-016\IDFG-016.csv,0
,0
I:\idfg-images\ClearCreek_mustelids\Summer2016\IDFG-017b,0
I:\idfg-images\ClearCreek_mustelids\Summer2016\IDFG-017a,0
Load .csv file,0
Validate the opstate column,0
# Create datetimes,0
print('Creating datetimes'),0
i_row = 0; row = df.iloc[i_row],0
Make sure data are sorted chronologically,0
,0
"In odd circumstances, they are not... so sort them first, but warn",0
Debugging when I was trying to see what was up with the unsorted dates,0
# Parse into sequences,0
print('Creating sequences'),0
i_row = 0; row = df.iloc[i_row],0
Start a new sequence if:,0
,0
* This image has no timestamp,0
* This image has a frame number of zero,0
* We have no previous image timestamp,0
,0
Start a new sequence if necessary,0
...for each row,0
# Parse labels for each sequence,0
sequence_id = location_sequences[0],0
Row indices in a sequence should be adjacent,0
sequence_df = df[df['seq_id']==sequence_id],0
# Determine what's present,0
Be conservative; assume humans are present in all maintenance images,0
The presence columns are *almost* always identical for all images in a sequence,0
assert single_presence_value,0
print('Warning: presence value for {} is inconsistent for {}'.format(,0
"presence_column,sequence_id))",0
...for each presence column,0
Tally up the standard (survey) species,0
"If no presence columns are marked, all counts should be zero",0
count_column = count_columns[0],0
Occasionally a count gets entered (correctly) without the presence column being marked,0
"assert len(values) == 1 and values[0] == 0, 'Non-zero counts with no presence",0
columns marked for sequence {}'.format(sequence_id),0
"Handle this by virtually checking the ""right"" box",0
Make sure we found a match,0
Handle 'other' tags,0
column_name = otherpresent_columns[0],0
print('Found non-survey counted species column: {}'.format(column_name)),0
...for each non-empty presence column,0
"Very rarely, the ""otherpresent"" column is checked, but no more detail is available",0
...handling non-survey species,0
Build the sequence data,0
i_row = 0; row = sequence_df.iloc[i_row],0
Only one folder used a single .csv file for two subfolders,0
...for each sequence,0
...def csv_to_sequences(),0
%% Parse each .csv file into sequences (loop),0
%%,0
%%,0
i_file = -1; csv_file = csv_files[i_file],0
%% Save sequence data,0
%% Load sequence data,0
%%,0
%% Validate file mapping (based on the existing enumeration),0
sequences = sequences_by_file[0],0
sequence = sequences[0],0
"Actually, one folder has relative paths",0
assert '\\' not in image_file_relative and '/' not in image_file_relative,0
os.startfile(csv_folder),0
assert os.path.isfile(image_file_absolute),0
found_file = os.path.isfile(image_file_absolute),0
...for each image,0
...for each sequence,0
...for each .csv file,0
%% Load manual category mappings,0
The second column is blank when the first column already represents the category name,0
%% Convert to CCT .json (original strings),0
Force the empty category to be ID 0,0
For each .csv file...,0
,0
sequences = sequences_by_file[0],0
For each sequence...,0
,0
sequence = sequences[0],0
Find categories for this image,0
"When 'unknown' is used in combination with another label, use that",0
"label; the ""unknown"" here doesn't mean ""another unknown species"", it means",0
there is some other unknown property about the main species.,0
category_name_string = species_present[0],0
"This piece of text had a lot of complicated syntax in it, and it would have",0
been too complicated to handle in a general way,0
print('Ignoring category {}'.format(category_name_string)),0
Don't process redundant labels,0
category_name = category_names[0],0
If we've seen this category before...,0
If this is a new category...,0
print('Adding new category for {}'.format(category_name)),0
...for each category (inner),0
...for each category (outer),0
...if we do/don't have species in this sequence,0
"We should have at least one category assigned (which may be ""empty"" or ""unknown"")",0
assert len(sequence_category_ids) > 0,0
Was any image in this sequence manually flagged as human?,0
print('Flagging sequence {} as human based on manual review'.format(sequence_id)),0
For each image in this sequence...,0
,0
i_image = 0; im = images[i_image],0
Create annotations for this image,0
...for each image in this sequence,0
...for each sequence,0
...for each .csv file,0
Verify that all images have annotations,0
ann = ict_data['annotations'][0],0
For debugging only,0
%% Create output (original strings),0
%% Validate .json file,0
%% Preview labels,0
%% Look for humans that were found by MegaDetector that haven't already been identified as human,0
This whole step only needed to get run once,0
%%,0
Load MD results,0
Get a list of filenames that MD tagged as human,0
im = md_results['images'][0],0
...for each detection,0
...for each image,0
Map images to annotations in ICT,0
ann = ict_data['annotations'][0],0
For every image,0
im = ict_data['images'][0],0
Does this image already have a human annotation?,0
...for each annotation,0
...for each image,0
%% Copy images for review to a new folder,0
fn = missing_human_images[0],0
%% Manual step...,0
Copy any images from that list that have humans in them to...,0
%% Create a list of the images we just manually flagged,0
fn = human_tagged_filenames[0],0
E.g. '0000_Beaverhead_elk~AM174~Trip 1~100RECNX~IMG_1397.JPG',0
"%% Translate location, image, sequence IDs",0
Load mappings if available,0
Generate mappings,0
If we've seen this location before...,0
Otherwise assign a string-formatted int as the ID,0
If we've seen this sequence before...,0
Otherwise assign a string-formatted int as the ID,0
Assign an image ID,0
...for each image,0
Assign annotation mappings,0
Save mappings,0
"Back this file up, lest we should accidentally re-run this script",0
with force_generate_mappings = True and overwrite the mappings we used.,0
...if we are/aren't re-generating mappings,0
%% Apply mappings,0
"%% Write new dictionaries (modified strings, original files)",0
"%% Validate .json file (modified strings, original files)",0
%% Preview labels (original files),0
"viz_options.classes_to_exclude = ['empty','deer','elk']",0
viz_options.classes_to_include = ['bobcat'],0
%% Copy images to final output folder (prep),0
ann = d['annotations'][0],0
Is this a public or private image?,0
Generate absolute path,0
Copy to output,0
Update the filename reference,0
...def process_image(im),0
%% Copy images to final output folder (execution),0
For each image,0
im = images[0],0
Write output .json,0
%% Make sure the right number of images got there,0
%% Validate .json file (final filenames),0
%% Preview labels (final filenames),0
"viz_options.classes_to_exclude = ['empty','deer','elk']",0
viz_options.classes_to_include = ['horse'],0
viz_options.classes_to_include = [viz_options.multiple_categories_tag],0
"viz_options.classes_to_include = ['human','vehicle','domestic dog']",0
%% Create zipfiles,0
%% List public files,0
%% Find the size of each file,0
fn = all_public_output_files[0],0
%% Split into chunks of approximately-equal size,0
...for each file,0
%% Create a zipfile for each chunk,0
...for each filename,0
with ZipFile(),0
...def create_zipfile(),0
i_file_list = 0; file_list = file_lists[i_file_list],0
"....if __name__ == ""__main__""",0
,0
wellington_to_json.py,0
,0
Convert the .csv file provided for the Wellington data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
%% Read source data,0
"Filenames were provided as *.jpg, but images were *.JPG, converting here",0
"%% Map filenames to rows, verify image existence",0
"Takes ~30 seconds, since it's checking the existence of ~270k images",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
"I didn't expect this to be true a priori, but it appears to be true, and",0
it saves us the trouble of checking consistency across multiple occurrences,0
of an image.,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
imageName = imageFilenames[0],0
"As per above, this is convenient and appears to be true; asserting to be safe",0
"Filenames look like ""290716114012001a1116.jpg""",0
This gets imported as an int64,0
"These appear as ""image1"", ""image2"", etc.",0
"In the form ""001a""",0
Can be in the form '111' or 's46',0
"In the form ""7/29/2016 11:40""",0
Check image height and width,0
"Use 'empty', to be consistent with other data on lila",0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
,0
mcgill_to_json.py,0
,0
Convert the .csv file provided for the McGill test data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
Create an additional column for concatenated filenames,0
Maps relative filenames to rows,0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
row = input_metadata.iloc[0],0
"I didn't expect this to be true a priori, but it appears to be true, and",0
it saves us the trouble of checking consistency across multiple occurrences,0
of an image.,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
row = input_metadata.iloc[0],0
"Filenames look like ""290716114012001a1116.jpg""",0
"In the form ""001a""",0
Can be in the form '111' or 's46',0
"In the form ""7/29/2016 11:40""",0
Check image height and width,0
NaN is the only thing we should see that's not a string,0
NaN is the only thing we should see that's not a string,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
,0
bellevue_to_json.py,0
,0
"""Bellevue Camera Traps"" is the rather unremarkable camera trap data set",0
used by one of the repo's maintainers for testing.  It's organized as:,0
,0
approximate_date/[loose_camera_specifier/]/species,0
,0
E.g.:,0
,0
"""2018.03.30\coyote\DSCF0091.JPG""",0
"""2018.07.18\oldcam\empty\DSCF0001.JPG""",0
,0
%% Constants and imports,0
from the ai4eutils repo,0
Filenames will be stored in the output .json relative to this base dir,0
%% Exif functions,0
"%% Enumerate files, create image/annotation/category info",0
Force the empty category to be ID 0,0
Keep track of unique camera folders,0
Each element will be a dictionary with fields:,0
,0
"relative_path, width, height, datetime",0
fname = image_files[0],0
Corrupt or not an image,0
Store file info,0
E.g. 2018.03.30/coyote/DSCF0091.JPG,0
...for each image file,0
%% Synthesize sequence information,0
Sort images by time within each folder,0
camera_path = camera_folders[0],0
previous_datetime = sorted_images_this_camera[0]['datetime'],0
im = sorted_images_this_camera[1],0
Start a new sequence if necessary,0
...for each image in this camera,0
...for each camera,0
Fill in seq_num_frames,0
%% A little cleanup,0
%% Write output .json,0
%% Sanity-check data,0
%% Label previews,0
,0
snapshot_safar_importer_reprise.py,0
,0
This is a 2023 update to snapshot_safari_importer.py.  We do a bunch of things now that,0
we didn't do the last time we imported Snapshot data (like updating the big taxonomy),0
"file, and we skip a bunch of things now that we used to do (like generating massive",0
"zipfiles).  So, new year, new importer.",0
,0
%% Constants and imports,0
%% List files,0
"Do a one-time enumeration of the entire drive; this will take a long time,",0
but will save a lot of hassle later.,0
%% Create derived lists,0
Takes about 60 seconds,0
CSV files are one of:,0
,0
"_report_lila.csv (this is the one we want to use, with the species/count/etc. for each sequence)",0
_report_lila_image_inventory.csv (maps captures to images),0
_report_lila_overview.csv (distrubution of species),0
%% List project folders,0
Project folders look like one of these:,0
,0
APN,0
Snapshot Cameo/DEB,0
%% Map report and inventory files to codes,0
fn = csv_files[0],0
%% Make sure that every report has a corresponding inventory file,0
%% Count species based on overview and report files,0
%% Print counts,0
%% Make sure that capture IDs in the reports/inventory files match,0
...and that all the images in the inventory tables are actually present on disk.,0
assert image_path_relative in all_files_relative_set,0
Make sure this isn't just a case issue,0
...for each report on this project,0
...for each project,0
"%% For all the files we have on disk, see which are and aren't in the inventory files",0
"There aren't any capital-P .PNG files, but if I don't include that",0
"in this list, I'll look at this in a year and wonder whether I forgot",0
to include it.,0
fn = all_files_relative[0],0
print('Skipping project {}'.format(project_code)),0
,0
plot_wni_giraffes.py,0
,0
Plot keypoints on a random sample of images from the wni-giraffes data set.,0
,0
%% Constants and imports,0
%% Load and select data,0
%% Support functions,0
https://stackoverflow.com/questions/32504246/draw-ellipse-in-python-pil-with-line-thickness,0
Use a single channel image (mode='L') as mask.,0
The size of the mask can be increased relative to the imput image,0
to get smoother looking results.,0
draw outer shape in white (color) and inner shape in black (transparent),0
downsample the mask using PIL.Image.LANCZOS,0
(a high-quality downsampling filter).,0
paste outline color to input image through the mask,0
%% Plot some images,0
ann = annotations_to_plot[0],0
i_tool = 0; tool_name = short_tool_names[i_tool],0
Don't plot tools that don't have a consensus annotation,0
...for each tool,0
...for each annotation,0
,0
idfg_iwildcam_lila_prep.py,0
,0
Adding class labels (from the private test .csv) to the iWildCam 2019 IDFG,0
"test set, in preparation for release on LILA.",0
,0
This version works with the public iWildCam release images.,0
,0
"%% ############ Take one, from iWildCam .json files ############",0
%% Imports and constants,0
%% Read input files,0
Remove the header line,0
%% Parse annotations,0
Lines look like:,0
,0
"b005e5b2-2c0b-11e9-bcad-06f1011196c4,1,Private",0
%% Minor cleanup re: images,0
%% Create annotations,0
%% Prepare info,0
%% Minor adjustments to categories,0
Remove unused categories,0
Name adjustments,0
%% Create output,0
%% Write output,0
%% Validate .json file,0
%% Preview labels,0
viz_options.classes_to_exclude = ['test'],0
"%% ############ Take two, from pre-iWildCam .json files created from IDFG .csv files ############",0
%% Imports and constants,0
%% One-time line break addition,0
%% Read input files,0
%% Prepare info,0
%% Minor adjustments to categories,0
%% Minor adjustments to annotations,0
%% Create output,0
%% Write output,0
%% Validate .json file,0
%% Preview labels,0
viz_options.classes_to_exclude = ['test'],0
,0
missouri_to_json.py,0
,0
Create .json files from the original source files for the Missouri Camera Traps,0
data set.  Metadata was provided here in two formats:,0
,0
"1) In one subset of the data, folder names indicated species names.  In Set 1,",0
there are no empty sequences.  Set 1 has a metadata file to indicate image-level,0
bounding boxes.,0
,0
2) A subset of the data (overlapping with (1)) was annotated with bounding,0
"boxes, specified in a whitespace-delimited text file.  In set 2, there are",0
"some sequences omitted from the metadata file, which implied emptiness.",0
,0
"In the end, set 2 labels were not reliable enough to publish, so LILA includes only set 1.",0
,0
%% Constants and imports,0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Filenames will be stored in the output .json relative to this base dir,0
This will be a list of filenames that need re-annotation due to redundant boxes,0
"%% Enumerate files, read image sizes (both sets)",0
"Takes a few minutes, since we're reading image sizes.",0
Each element will be a list of relative path/full path/width/height,0
"Only process leaf nodes corresponding to sequences, which look like:",0
,0
E:\wildlife_data\missouri_camera_traps\Set1\1.02-Agouti\SEQ75583,0
E:\wildlife_data\missouri_camera_traps\Set2\p1d101,0
,0
assert len(files) <= 2,0
Read the image,0
Not an image...,0
Store file info,0
"...if we didn't hit the max file limit, keep going",0
...for each file,0
%% Add sequence lengths (both sets),0
%% Load the set 1 metadata file,0
"List of lists, length varies according to number of bounding boxes",0
,0
Preserves original ordering,0
iLine = 0; line = metadataSet1Lines[0],0
"Lines should be filename, number of bounding boxes, boxes (four values per box)",0
Make sure we have image info for this image,0
%% Print missing files from Set 1 metadata,0
Manual changes I made to the metadata file:,0
,0
'IMG' --> 'IMG_',0
Red_Brocket_Deer --> Red_Deer,0
European-Hare --> European_Hare,0
Wood-Mouse --> Wood_Mouse,0
Coiban-Agouti --> Coiban_Agouti,0
%% Load the set 2 metadata file,0
"This metadata file contains most (but not all) images, and a class label (person/animal/empty)",0
"for each, plus bounding boxes.",0
"List of lists, length varies according to number of bounding boxes",0
,0
Preserves original ordering,0
"Create class IDs for each *sequence*, which we'll use to attach classes to",0
images for which we don't have metadata,0
,0
This only contains mappings for sequences that appear in the metadata.,0
iLine = 0; line = metadataSet2Lines[0],0
"Lines should be filename, number of bounding boxes, labeled boxes (five values per box)",0
,0
Empty images look like filename\t0\t0,0
E.g. 'Set2\\p1d101\\p1d101s100i10.JPG',0
Make sure we don't have mixed classes within an image,0
"Figure out what class this *sequence* is, so we know how to handle unlabeled",0
images from this sequence,0
Can't un-do a mixed sequence,0
Previously-empty sequences get the image class label,0
"If the sequence has a non-empty class, possibly change it",0
Make sure we have image info for this image,0
...for each line in the set 2 metadata file,0
%% What Set 2 images do I not have metadata for?,0
These are *mostly* empty images,0
iImage = 0; imageID = set2ImageIDs[iImage],0
%% Create categories and annotations for set 1,0
"Though we have no empty sequences, we do have empty images in this set",0
For each image,0
,0
iImage = 0; imageID = set1ImageIDs[iImage],0
E.g. Set1\\1.80-Coiban_Agouti\\SEQ83155\\SEQ83155_IMG_0010.JPG,0
Find the species name,0
This image may still be empty...,0
category['count'] = category['count'] + 1,0
"If we have bounding boxes, create image-level annotations",0
"filename, number of bounding boxes, boxes (four values per box)",0
"Make sure the relative filename matches, allowing for the fact that",0
some of the filenames in the metadata aren't quite right,0
This image is non-empty,0
"Some redundant bounding boxes crept in, don't add them twice",0
Check this bbox against previous bboxes,0
,0
Inefficient?  Yes.  In an important way?  No.,0
"Bounding box values are in absolute coordinates, with the origin",0
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1].",0
,0
"Convert to floats and to x/y/w/h, as per CCT standard",0
...for each box,0
if we do/don't have boxes for this image,0
Else create a sequence-level annotation,0
...for each image,0
%% Write out the list of images with redundant boxes,0
%% Create categories and annotations for set 2,0
For each image,0
,0
iImage = 0; imageID = set2ImageIDs[iImage],0
E.g. 'Set2\\p1d100\\p1d100s10i1.JPG',0
"Find the sequence ID, sanity check filename against what we stored",0
"If we have bounding boxes or an explicit empty label, create image-level annotations",0
"filename, number of bounding boxes, labeled boxes (five values per box)",0
"Make sure the relative filename matches, allowing for the fact that",0
some of the filenames in the metadata aren't quite right,0
"Bounding box values are in absolute coordinates, with the origin",0
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1].",0
,0
Convert to floats and to x/y/w/h,0
...for each box,0
...if we do/don't have boxes for this image,0
Else create a sequence-level annotation,0
...for each image,0
"%% The 'count' field isn't really meaningful, delete it",0
"It's really the count of image-level annotations, not total images assigned to a class",0
%% Write output .json files,0
%% Sanity-check final set 1 .json file,0
"python sanity_check_json_db.py --bCheckImageSizes --baseDir ""E:\wildlife_data\missouri_camera_traps"" ""E:\wildlife_data\missouri_camera_traps\missouri_camera_traps_set1.json""",0
%% Generate previews,0
,0
filenames_to_json.py,0
,0
Take a directory of images in which species labels are encoded by folder,0
"names, and produces a COCO-style .json file",0
,0
%% Constants and imports,0
from the ai4eutils repo,0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Filenames will be stored in the output .json relative to this base dir,0
"rawClassListFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_classes.csv')",0
"classMappingsFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_class_mapping.csv')",0
"%% Enumerate files, read image sizes",0
Each element will be a list of relative path/full path/width/height,0
Read the image,0
Corrupt or not an image,0
Store file info,0
Write to output file,0
...for each image file,0
...csv file output,0
...if the file list is/isn't available,0
%% Enumerate classes,0
Maps classes to counts,0
We like 'empty' to be class 0,0
%% Assemble dictionaries,0
...for each category,0
Each element is a list of relative path/full path/width/height/className,0
...for each image,0
%% External class mapping,0
%% Write raw class table,0
cat = categories[0],0
%% Read the mapped class table,0
"id, source, count, target",0
"%% Make classMappings contain *all* classes, not just remapped classes",0
cat = categories[0],0
%% Create new class list,0
"Start at 1, explicitly assign 0 to ""empty""",0
One-off issue with character encoding,0
%% Re-map annotations,0
ann = annotations[0],0
%% Write output .json,0
%% Utilities,0
%%,0
Find images with a particular tag,0
%% Randomly sample annotations,0
,0
umn_to_json.py,0
,0
Prepare images and metadata for the Orinoqua Camera Traps dataset.,0
,0
%% Imports and constants,0
"For two deployments, we're only processing imagse in the ""detections"" subfolder",0
%% Enumerate deployment folders,0
%% Load ground truth,0
i_row = 0; row = ground_truth_df.iloc[i_row],0
%% Create relative paths for ground truth data,0
"Some deployment folders have no subfolders, e.g. this is a valid file name:",0
,0
M00/01010132.JPG,0
,0
"But some deployment folders have subfolders, e.g. this is also a valid file name:",0
,0
N17/100EK113/07160020.JPG,0
,0
"So we can't find files by just concatenating folder and file names, we have to enumerate and explicitly",0
"map what will appear in the ground truth as ""folder/filename"" to complete relative paths.",0
deployment_name = list(deployment_folders)[0],0
Enumerate all files in this folder,0
"print('Enumerated {} files for deployment {}'.format(len(files),deployment_name))",0
filename = files[100],0
...for each file in this deployment,0
...for each deployment,0
%% Add relative paths to our ground truth table,0
i_row = 0; row = ground_truth_df.iloc[i_row],0
"row['filename'] looks like, e.g. A01/01080001.JPG.  This is not actually a path, it's",0
"just the deployment ID and the image name, separated by a slash.",0
Find the relative path for this image,0
Make sure this image file exists,0
...for each row in the ground truth table,0
%% Take everything out of Pandas,0
%% Convert string timestamps to Python datetimes,0
im = ground_truth_dicts[0],0
"Filenames look like, e.g., N36/100EK113/06040726.JPG",0
%% Synthesize sequence information,0
Sort images by time within each location,0
i_location=0; location = locations[i_location],0
previous_datetime = sorted_images_this_location[0]['datetime'],0
im = sorted_images_this_camera[1],0
"Timestamp for this image, may be None",0
Start a new sequence if:,0
,0
* This image has no timestamp,0
* This iamge has a frame number of zero,0
* We have no previous image timestamp,0
,0
Start a new sequence if necessary,0
...for each image in this location,0
...for each location,0
%% Create category dict and category IDs,0
im = images[0],0
%% Imports and constants (.json generation),0
%% Count frames in each sequence,0
%% Double check images with multiple annotations,0
im = images[0],0
%% Assemble dictionaries,0
Force the empty category to be ID 0,0
input_im = images[0],0
...for each image,0
%% Write output .json,0
%% Validate .json file,0
%% Map relative paths to annotation categories,0
ann = data['annotations'][0],0
%% Copy images to output,0
EXCLUDE HUMAN AND MISSING,0
im = data['images'][0],0
im = images[0],0
%% Preview labels,0
viz_options.classes_to_exclude = ['test'],0
,0
nacti_fieldname_adjustments.py,0
,0
"NACTI metadata was posted with ""filename"" in images instead of ""file_name"", and",0
"used string (rather than int) category IDs (in categories, but not in annotations).",0
,0
This script fixes those issues and rev's the version number.,0
,0
%% Constants and environment,0
%% Read .json file,0
"%% Rev version number, update field names and types",0
%% Write json file,0
,0
cct_field_adjustments.py,0
,0
CCT metadata was posted with int locations instead of strings.,0
,0
This script fixes those issues and rev's the version number.,0
,0
%% Constants and environment,0
%% Read .json file,0
"%% Rev version number, update field names and types",0
%% Write json file,0
%% Check output data file,0
,0
carrizo_shrubfree_2018.py,0
,0
Convert the .csv file provided for the Carrizo Mojave data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
%% Read source data,0
Original .csv file had superfluous spaces in column names,0
"%% Map filenames to rows, verify image existence",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
58 missing files (of 17652),0
%% Check for images that aren't included in the metadata file,0
3012 of 20606 files are not in metadata,0
%% Create CCT dictionaries,0
Map categories to integer IDs,0
,0
The category '0' is reserved for 'empty',0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
Each filename should just match one row,0
Don't include images that don't exist on disk,0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
,0
snapshot_serengeti_lila.py,0
,0
Create zipfiles of Snapshot Serengeti S1-S11.,0
,0
"Create a metadata file for S1-S10, plus separate metadata files",0
"for S1-S11.  At the time this code was written, S11 was under embargo.",0
,0
Create zip archives of each season without humans.,0
,0
Create a human zip archive.,0
,0
%% Constants and imports,0
import sys; sys.path.append(r'c:\git\ai4eutils'),0
import sys; sys.path.append(r'c:\git\cameratraps'),0
assert(os.path.isdir(metadata_base)),0
"There are two redundant categories, and we re-map ""blank"" to ""empty"" as per CCT convention",0
"%% Load metadata files, concatenate into a single table",0
iSeason = 1,0
%% Convert to dictionaries (prep),0
%% Convert to dictionaries (loops),0
"TODO: iterrows() is a terrible way to do this, but this is one of those days",1
"where I want to get this done, not get better at Python.",0
irow = 0; row = image_table.iloc[0],0
"Loaded as an int64, converting to int here",0
...for each row in the image table,0
Make sure image IDs are what we think they are,0
...for each row in the annotation table,0
%% Load previously-saved dictionaries when re-starting mid-script,0
%%,0
%% Take a look at categories (just sanity-checking),0
%%,0
%% Fill in some image fields we didn't have when we created the image table,0
"width, height, corrupt, seq_num_frames, location, datetime",0
Every annotation in this list should have the same sequence ID,0
"Figure out ""seq_num_frames"", which really should be done in a separate lopp;",0
there's no reason to do this redundantly for every image,0
Every image in this sequence should point back to the same equence,0
Every annotation in this list should have the same location,0
Every annotation in this list should have the same datetime,0
Is this image on disk?,0
iImage = 0; im = images[0],0
...for each image,0
"images_processed = pool.map(process_image, images)",0
"images_processed = list(tqdm(pool.imap_unordered(process_image, images), total=len(images)))",0
%%,0
%% Count missing/corrupted images,0
%% Print distribution of sequence lengths (sanity-check),0
%% Replicate annotations across images,0
iAnn = 0; ann = annotations[iAnn],0
%% See what files are on disk but not annotated (~15 mins),0
247370 files not in the database (of 7425810),0
%% Load old image database,0
%% Look for old images not in the new DB and vice-versa,0
"At the time this was written, ""old"" was S1-S6",0
old_im = cct_old['images'][0],0
new_im = images[0],0
4 old images not in new db,0
12 new images not in old db,0
%% Save our work,0
%% Load our work,0
%%,0
%% Examine size mismatches,0
i_mismatch = -1; old_im = size_mismatches[i_mismatch],0
%% Sanity-check image and annotation uniqueness,0
"%% Split data by seasons, create master list for public seasons",0
ann = annotations[0],0
%% Minor updates to fields,0
"%% Write master .json out for S1-10, write individual season .jsons (including S11)",0
"Loop over all seasons, plus one iteration for the ""all public data"" iteration, and",0
"one for the ""all data"" iteration",0
%% Find categories that only exist in S11,0
List of categories in each season,0
Category 55 (fire) only in S11,0
Category 56 (hyenabrown) only in S11,0
Category 57 (wilddog) only in S11,0
Category 58 (kudu) only in S11,0
Category 59 (pangolin) only in S11,0
Category 60 (lioncub) only in S11,0
%% Prepare season-specific .csv files,0
iSeason = 1,0
%% Create a list of human files,0
ann = annotations[0],0
%% Save our work,0
%% Load our work,0
%%,0
"%% Create archives (human, per-season) (prep)",0
im = images[0],0
im = images[0],0
Don't include humans,0
Only include files from this season,0
Possibly start a new archive,0
...for each image,0
i_season = 0,0
"for i_season in range(0,nSeasons):",0
create_season_archive(i_season),0
%% Create archives (loop),0
pool = ThreadPool(nSeasons+1),0
"n_images = pool.map(create_archive, range(-1,nSeasons))",0
"seasons_to_zip = range(-1,nSeasons)",0
...for each season,0
%% Sanity-check .json files,0
%logstart -o r'E:\snapshot_temp\python.txt',0
%% Zip up .json and .csv files,0
pool = ThreadPool(len(files_to_zip)),0
"pool.map(zip_single_file, files_to_zip)",0
%% Super-sanity-check that S11 info isn't leaking,0
im = data_public['images'][0],0
ann = data_public['annotations'][0],0
iRow = 0; row = annotation_df.iloc[iRow],0
iRow = 0; row = image_df.iloc[iRow],0
%% Create bounding box archive,0
i_image = 0; im = data['images'][0],0
i_box = 0; boxann = bbox_data['annotations'][0],0
%% Sanity-check a few files to make sure bounding boxes are still sensible,0
import sys; sys.path.append(r'C:\git\CameraTraps'),0
%% Check categories,0
%% Summary prep for LILA,0
,0
wi_to_json,0
,0
Prepares CCT-formatted metadata based on a Wildlife Insights data export.,0
,0
"Mostly assumes you have the images also, for validation/QA.",0
,0
%% Imports and constants,0
"Because WI filenames are GUIDs, it's not practical to page through sequences in an",0
image viewer.  So we're going to (optionally) create a copy of the data set where,0
images are ordered.,0
%% Load ground truth,0
%% Take everything out of Pandas,0
%% Synthesize common names when they're not available,0
"Blank rows should always have ""Blank"" as the common name",0
"print('Warning: missing common name for row {} ({})'.format(i_row,row['filename']))",0
%% Convert string timestamps to Python datetimes,0
im = ground_truth_dicts[0],0
"The field called ""location"" in the WI .csv file is a URL, we want to reclaim",0
"the ""location"" keyword for CCT output",0
"Filenames look like, e.g., N36/100EK113/06040726.JPG",0
%% Synthesize sequence information,0
Sort images by time within each location,0
i_location=0; location = locations[i_location],0
previous_datetime = sorted_images_this_location[0]['datetime'],0
im = sorted_images_this_camera[1],0
"Timestamp for this image, may be None",0
Start a new sequence if:,0
,0
* This image has no timestamp,0
* This iamge has a frame number of zero,0
* We have no previous image timestamp,0
,0
Start a new sequence if necessary,0
...for each image in this location,0
...for each location,0
%% Create category dict and category IDs,0
im = images[0],0
%% Count frames in each sequence,0
%% Build relative paths,0
im = images[0],0
Sample URL:,0
,0
gs://project-asfasdfd/deployment/21444549/asdfasdfd-616a-4d10-a921-45ac456c568a.jpg',0
%% Double check images with multiple annotations,0
im = images[0],0
%% Assemble dictionaries,0
Force the empty category to be ID 0,0
input_im = images[0],0
"print('Warning: image ID {} ({}) has multiple annotations'.format(im['id'],im['id'].replace('_','/')))",0
...for each image,0
%% Write output .json,0
%% Validate .json file,0
%% Preview labels,0
"open_file(os.path.join(image_base,'2100703/1141a545-88d2-498b-a684-7431f7aeb324.jpg'))",0
%%,0
%% Create ordered dataset,0
"Because WI filenames are GUIDs, it's not practical to page through sequences in an",0
image viewer.  So we're going to create a copy of the data set where images are,0
ordered.,0
im = images_out[0]; im,0
%% Create ordered .json,0
%% Copy files to their new locations,0
im = ordered_images[0],0
im = data_ordered['images'][0],0
%% Preview labels in the ordered dataset,0
"open_file(os.path.join(image_base,'2100703/1141a545-88d2-498b-a684-7431f7aeb324.jpg'))",0
%% Open an ordered filename from the unordered filename,0
,0
rspb_to_json.py,0
,0
Convert the .csv file provided for the RSPB data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
[location] is an obfuscation,0
%% Create info struct,0
%% Read source data,0
metadataTable.columns.values,0
,0
"array(['Project', 'inDir', 'FileName', 'Station', 'Camera',",0
"'StationCameraFileName', 'DateTimeOriginal', 'DateReadable',",0
"'outDir', 'filename_new', 'fileExistsAlready', 'CopyStatus',",0
"'Species'], dtype=object)",0
We'll populate these later,0
keys should be lowercase,0
"%% Enumerate images, confirm filename uniqueness",0
"%% Update metadata filenames to include site and camera folders, check existence",0
,0
Takes ~1min,0
iRow = 0; row = metadataTable.iloc[iRow],0
There's a bug in the metadata; the 'camera' column isn't correct.,0
camera = row['Camera'],0
"These appear as, e.g., '3.22e12'",0
camera = str(int(float(camera))),0
Let's pull this out of the file name instead,0
,0
Filenames look like one of the following:,0
,0
A1__03224850850507__2015-11-28__10-45-04(1).JPG,0
Bayama2PH__C05__NA(NA).JPG,0
assert(os.path.isfile(fullPath)),0
metadataTable.iloc[iRow] = row,0
Re-assemble into an updated table,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
Write to a text file,0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
iRow = 0; row = metadataTable.iloc[iRow],0
A1__03224850850507__2015-11-28__10-45-04(1).JPG,0
'A1\\03224850850507\\A1__03224850850507__2015-11-28__10-45-04(1).JPG',0
Not currently populated,0
"Often -1, sometimes a semi-meaningful int",0
A1,0
03224850850507,0
"In variable form, but sometimes '28/11/2015 10:45'",0
Check image height and width,0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Write output,0
%% Check database integrity,0
%% Preview a few images to make sure labels were passed along sensibly,0
%% One-time processing step: copy images to a flat directory for annotation,0
%%,0
,0
pc_to_json.py,0
,0
Convert a particular collection of .csv files to CCT format.,0
,0
%% Constants and environment,0
%% Read and concatenate source data,0
List files,0
"List of dataframes, one per .csv file; we'll concatenate later",0
i_file = 87; fn = input_files[i_file],0
Concatenate into a giant data frame,0
%% List files,0
%% Main loop over labels (prep),0
Force the empty category to be ID 0,0
%% Main loop over labels (loop),0
iRow = 0; row = input_metadata.iloc[iRow],0
"ImageID,FileName,FilePath,SpeciesID,CommonName",0
assert os.path.isfile(full_path),0
Retrieve image width and height,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
%% See what's up with missing files,0
s = list(image_relative_paths)[0],0
s = missing_files[0],0
%% Check for images that aren't included in the metadata file,0
%% Sample the database,0
%%,0
Collect the images we want,0
%% Create info struct,0
%% Write output,0
%% Sanity-check the database's integrity,0
%% Render a bunch of images to make sure the labels got carried along correctly,0
options.classes_to_exclude = ['unlabeled'],0
%% Write out a list of files to annotate,0
,0
save_the_elephants_survey_B.py,0
,0
Convert the .csv file provided for the Save the Elephants Survey B data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
input_base = r'/mnt/blobfuse/wildlifeblobssc/ste_2019_08_drop',0
output_base = r'/home/gramener/survey_b',0
Handle all unstructured fields in the source data as extra fields in the annotations,0
"photo_type really should be an image property, but there are a few conflicts",0
that forced me to handle it as an annotation proprerty,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
"%% Map filenames to rows, verify image existence",0
Maps relative paths to row indices in input_metadata,0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
Ignore directories,0
%% Make sure the multiple-annotation cases make sense,0
%%,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
Force the empty category to be ID 0,0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image],0
Example filename:,0
,0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG',0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG',0
i_row = row_indices[0],0
timestamp = row['Date'],0
There are a small handful of datetime mismatches across annotations,0
for the same image,0
assert im['datetime'] == timestamp,0
Special cases based on the 'photo type' field,0
Various spellings of 'community',0
Have we seen this category before?,0
Create an annotation,0
fieldname = list(mapped_fields.keys())[0],0
...for each row,0
...for each image,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
%% Scrap,0
%% Find unique photo types,0
,0
ubc_to_json.py,0
,0
Convert the .csv file provided for the UBC data set to a,0
COCO-camera-traps .json file,0
,0
"Images were provided in eight folders, each of which contained a .csv",0
file with annotations.  Those annotations came in two slightly different,0
"formats, the two formats corresponding to folders starting with ""SC_"" and",0
otherwise.,0
,0
%% Constants and environment,0
Map Excel column names - which vary a little across spreadsheets - to a common set of names,0
%% Enumerate images,0
Load from file if we've already enumerated,0
%% Create CCT dictionaries,0
Force the empty category to be ID 0,0
To simplify debugging of the loop below,0
#%% Create CCT dictionaries (loop),0
#%%,0
Read source data for this folder,0
Rename columns,0
Folder name is the first two characters of the filename,0
,0
Create relative path names from the filename itself,0
Folder name is the camera name,0
,0
Create relative path names from camera name and filename,0
Which of our images are in the spreadsheet?,0
i_row = 0; fn = input_metadata['image_relative_path'][i_row],0
#%% Check for images that aren't included in the metadata file,0
Find all the images in this folder,0
Which of these aren't in the spreadsheet?,0
#%% Create entries in CCT dictionaries,0
Only process images we have on disk,0
"This is redundant, but doing this for clarity, at basically no performance",0
cost since we need to *read* the images below to check validity.,0
i_row = row_indices[0],0
"These generally represent zero-byte images in this data set, don't try",0
to find the very small handful that might be other kinds of failures we,0
might want to keep around.,0
print('Error opening image {}'.format(image_relative_path)),0
If we've seen this category before...,0
...make sure it used the same latin --> common mapping,0
,0
"If the previous instance had no mapping, use the new one.",0
assert common_name == category['common_name'],0
Create an annotation,0
...for each annotation we found for this image,0
...for each image,0
...for each dataset,0
Print all of our species mappings,0
"%% Copy images for which we actually have annotations to a new folder, lowercase everything",0
im = images[0],0
%% Create info struct,0
"%% Convert image IDs to lowercase in annotations, tag as sequence level",0
"While there isn't any sequence information, the nature of false positives",0
"here leads me to believe the images were labeled at the sequence level, so",0
we should trust labels more when positives are verified.  Overall false,0
positive rate looks to be between 1% and 5%.,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
,0
helena_to_cct.py,0
,0
Convert the Helena Detections data set to a COCO-camera-traps .json file,0
,0
%% Constants and environment,0
This is one time process,0
%% Create Filenames and timestamps mapping CSV,0
import pdb;pdb.set_trace(),0
%% To create CCT JSON for RSPB dataset,0
%% Read source data,0
Original Excel file had timestamp in different columns,0
"%% Map filenames to rows, verify image existence",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
%% Skipping this check because one image has multiple species,0
assert len(duplicate_rows) == 0,0
%% Check for images that aren't included in the metadata file,0
%% Create CCT dictionaries,0
Map categories to integer IDs,0
,0
The category '0' is reserved for 'empty',0
For each image,0
Don't include images that don't exist on disk,0
Some filenames will match to multiple rows,0
assert(len(rows) == 1),0
iRow = rows[0],0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
ann['datetime'] = row['datetime'],0
ann['site'] = row['site'],0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
%% Imports and constants,0
from github.com/microsoft/ai4eutils,0
from github.com/ecologize/CameraTraps,0
A list of files in the lilablobssc container for this data set,0
The raw detection files provided by NOAA,0
A version of the above with filename columns added,0
%% Read input .csv,0
%% Read list of files,0
%% Convert paths to full paths,0
i_row = 0; row = df.iloc[i_row],0
assert ir_image_path in all_files,0
...for each row,0
%% Write results,0
"%% Load output file, just to be sure",0
%% Render annotations on an image,0
i_image = 2004,0
%% Download the image,0
%% Find all the rows (detections) associated with this image,0
"as l,r,t,b",0
%% Render the detections on the image(s),0
In pixel coordinates,0
In pixel coordinates,0
%% Save images,0
%% Clean up,0
,0
carrizo_trail_cam_2017.py,0
,0
"Convert the .csv files provided for the ""Trail Cam Carrizo"" 2017 data set to",0
a COCO-camera-traps .json file.,0
,0
%% Constants and environment,0
%% Read source data,0
Removing the empty records,0
"%% Map filenames to rows, verify image existence",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
908 missing files (of 60562),0
%% Check for images that aren't included in the metadata file,0
105329 of 164983 files are not in metadata,0
%% Create CCT dictionaries,0
Map categories to integer IDs,0
,0
The category '0' is reserved for 'empty',0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
Each filename should just match one row,0
Don't include images that don't exist on disk,0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
,0
channel_islands_to_cct.py,0
,0
Convert the Channel Islands data set to a COCO-camera-traps .json file,0
,0
"Uses the command-line tool ExifTool (exiftool.org) to pull EXIF tags from images,",0
"because every Python package we tried failed to pull the ""Maker Notes"" field properly.",0
,0
"%% Imports, constants, paths",0
# Imports ##,0
# Constants ##,0
# Paths ##,0
Confirm that exiftool is available,0
"assert which(exiftool_command_name) is not None, 'Could not locate the ExifTool executable'",0
%% Load information from every .json file,0
"Ignore the sample file... actually, first make sure there is a sample file",0
...and now ignore that sample file.,0
json_file = json_files[0],0
ann = annotations[0],0
...for each annotation in this file,0
...for each .json file,0
"%% Confirm URL uniqueness, handle redundant tags",0
Have we already added this image?,0
"One .json file was basically duplicated, but as:",0
,0
Ellie_2016-2017 SC12.json,0
Ellie_2016-2017-SC12.json,0
"If the new image has no output, just leave the old one there",0
"If the old image has no output, and the new one has output, default to the one with output",0
Don't worry about the cases where someone tagged 'fox' and someone tagged 'fox_partial',0
...for each image we've already added,0
...if this URL is/isn't in the list of URLs we've already processed,0
...for each image,0
%% Save progress,0
%%,0
%%,0
%% Download files (functions),0
https://www.quickprogrammingtips.com/python/how-to-download-multiple-files-concurrently-in-python.html,0
"This is returned with a leading slash, remove it",0
%% Download files (execution),0
%% Read required fields from EXIF data (functions),0
"-G means ""Print group name for each tag"", e.g. print:",0
,0
[File]          Bits Per Sample                 : 8,0
,0
...instead of:,0
,0
Bits Per Sample                 : 8,0
"If we don't get any EXIF information, this probably isn't an image",0
line_raw = exif_lines[0],0
"Split on the first occurrence of "":""",0
Typically:,0
,0
"'[MakerNotes]    Sequence                        ', '1 of 3']",0
Not a typo; we are using serial number as a location,0
"If there are multiple timestamps, make sure they're *almost* the same",0
"If there are multiple timestamps, make sure they're *almost* the same",0
...for each line in the exiftool output,0
"This isn't directly related to the lack of maker notes, but it happens that files that are missing",0
maker notes also happen to be missing EXIF date information,0
...process_exif(),0
"This is returned with a leading slash, remove it",0
Ignore non-image files,0
%% Read EXIF data (execution),0
ann = images[0],0
%% Save progress,0
Use default=str to handle datetime objects,0
%%,0
%%,0
"Not deserializing datetimes yet, will do this if I actually need to run this",0
%% Check for EXIF read errors,0
%% Remove junk,0
Ignore non-image files,0
%% Fill in some None values,0
"...so we can sort by datetime later, and let None's be sorted arbitrarily",0
%% Find unique locations,0
%% Synthesize sequence information,0
Sort images by time within each location,0
i_location=0; location = locations[i_location],0
previous_datetime = sorted_images_this_location[0]['datetime'],0
im = sorted_images_this_camera[1],0
"Timestamp for this image, may be None",0
Start a new sequence if:,0
,0
* This image has no timestamp,0
* This image has a frame number of zero,0
* We have no previous image timestamp,0
,0
Start a new sequence if necessary,0
...for each image in this location,0
...for each location,0
%% Count frames in each sequence,0
images_this_sequence = [im for im in images if im['seq_id'] == seq_id],0
"%% Create output filenames for each image, store original filenames",0
i_location = 0; location = locations[i_location],0
i_image = 0; im = sorted_images_this_location[i_image],0
%% Save progress,0
Use default=str to handle datetime objects,0
%%,0
%%,0
%% Copy images to their output files (functions),0
%% Copy images to output files (execution),0
%% Rename the main image list for consistency with other scripts,0
%% Create CCT dictionaries,0
Make sure this is really a box,0
Transform to CCT format,0
Force the empty category to be ID 0,0
i_image = 0; input_im = all_image_info[0],0
"This issue only impacted one image that wasn't a real image, it was just a screenshot",0
"showing ""no images available for this camera""",0
Convert datetime if necessary,0
Process temperature if available,0
Read width and height if necessary,0
I don't know what this field is; confirming that it's always None,0
Process object and bbox,0
os.startfile(output_image_full_path),0
"Zero is hard-coded as the empty category, but check to be safe",0
"I can't figure out the 'index' field, but I'm not losing sleep about it",0
assert input_annotation['index'] == 1+i_ann,0
"Some annotators (but not all) included ""_partial"" when animals were partially obscured",0
"Annotators *mostly* used 'none', but sometimes 'empty'.  'empty' is CCT-correct.",0
If we've seen this category before...,0
If this is a new category...,0
...if this is an empty/non-empty annotation,0
Create an annotation,0
...for each annotation on this image,0
...for each image,0
%% Change *two* annotations on images that I discovered contains a human after running MDv4,0
%% Move human images,0
ann = annotations[0],0
%% Count images by location,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
viz_options.classes_to_exclude = [0],0
,0
ena24_to_json_2017.py,0
,0
Convert the ENA24 data set to a COCO-camera-traps .json file,0
,0
%% Constants and environment,0
Temporary folders for human and non-human images,0
Clean existing output folders/zipfiles,0
%% Support functions,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
Check image height and width,0
"Each row is category, [box coordinates]",0
"If there's just one row, loadtxt reads it as a 1d array; make it a 2d array",0
with one row,0
Each row is a bounding box,0
Have we seen this category before?,0
Create an annotation,0
...for each bounding box,0
"This was here for debugging; nearly every instance is Human+Horse, Human+Vehicle,",0
"or Human+Dog, but there is one Rabbit+Opossium, and a few Deer+Chicken!",0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
%% Create ZIP files for human and non human,0
%% Validate output,0
%% Preview labels,0
%% Imports and constants,0
configurations and paths,0
%% Helper functions,0
"dest_path = copy(source_path, dest_folder)",0
num_workers = multiprocessing.cpu_count(),0
pool = ThreadPool(num_workers),0
"results = pool.starmap(_copy_unzip, zip(sources, itertools.repeat(dest_folder)))",0
,0
print('Waiting for processes to finish...'),0
pool.close(),0
pool.join(),0
sequential,0
%% Command-line driver,0
if the blob container is already mounted on the VM,0
or you can download them using the storage Python SDK,0
store storage account key in environment variable AZ_STORAGE_KEY,0
,0
eMammal_helpers.py,0
,0
Support functions for processing eMammal metadata,0
,0
%% Constants and imports,0
%% Support functions,0
"pad to a total of 3 digits if < 1000, or 4 digits otherwise",0
img_frame is a string from the xml tree,0
"length 4 frame order is returned as is, others are left padded to be 3 digit long",0
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels,0
pad to a total of 4 digits,0
img_frame is a string from the xml tree,0
"length 4 frame order is returned as is, others are left padded to be 3 digit long",0
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels,0
others column,0
summer day hours: 6am - 7pm,0
others day hours: 7am - 6pm,0
,0
make_eMammal_json.py,0
,0
"Produces the COCO-formatted json database for an eMammal dataset, i.e. a",0
"collection of folders, each of which contains a deployment_manifest.xml file.",0
,0
"In this process, each image needs to be loaded to size it.",0
,0
"To add bounding box annotations to the resulting database, use",0
add_annotations_to_eMammal_json.py.,0
,0
%% Constants and imports,0
"Either add the eMammal directory to your path, or run from there",0
os.chdir(r'd:\git\CameraTraps\database_tools\eMammal'),0
import warnings,0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
warnings.filterwarnings('ignore'),0
Should we run the image size retrieval in parallel?,0
%% Support functions,0
%% Main loop (metadata processing; image sizes are retrieved later),0
deployment = folders[0],0
sequence = image_sequences[0],0
get species info for this sequence,0
add each image's info to database,0
img = images[0],0
"some manifests don't have the ImageOrder info, but the info is in the file name",0
full_img_id has no frame info,0
,0
frame number only used in requests to iMerit for ordering,0
...for each image,0
...for each sequence,0
...for each deployment,0
%% Get image sizes,0
"'tasks' is currently a list of 2-tuples, with each entry as [image dictionary,path].",0
,0
"Go through that and copy just the image dictionaries to 'db_images', adding size",0
information to each entry.  Takes a couple hours.,0
opening each image seems too fast for this multi-threaded version to be faster than sequential code.,0
%% Assemble top-level dictionaries,0
%% Write out .json,0
,0
make_full_SS_json.py,0
,0
Create a COCO-camera-traps .json file for Snapshot Serengeti data from,0
the original .csv files provided on Dryad.,0
,0
"This was used for ""version 1.0"" of the public Snapshot Serengeti archive; it's no",0
longer used as of v2.0 (early 2020).  See snapshot_serengeti_lila.py for the updated,0
Snapshot Safari preparation process.,0
,0
%% Imports and constants,0
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays",0
Count the number of images with multiple species,0
"%% Read image .csv file, format into a dictionary mapping images to capture events",0
%% Create CCT-style .json,0
still need image width and height,0
...for each image,0
%% Write output files,0
,0
make_per_season_SS_json.py,0
,0
Create a COCO-camera-traps .json file for each Snapshot Serengeti season from,0
the original .csv files provided on Dryad.,0
,0
"This was used for ""version 1.0"" of the public Snapshot Serengeti archive; it's no",0
longer used as of v2.0 (early 2020).  See snapshot_serengeti_lila.py for the updated,0
Snapshot Safari preparation process.,0
,0
%% Imports and constants,0
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays",0
"%% Read image .csv file, format into a dictionary mapping images to capture events",0
%% Create CCT-style .json,0
still need image width and height,0
...for each image ID,0
%% Write output files,0
...for each season,0
,0
generate_lila_per_image_labels.py,0
,0
"Generate a .csv file with one row per annotation, containing full URLs to every",0
"camera trap image on LILA, with taxonomically expanded labels.",0
,0
"Typically there will be one row per image, though images with multiple annotations",0
will have multiple rows.,0
,0
"Some images may not physically exist, particularly images that are labeled as ""human"".",0
This script does not validate image URLs.,0
,0
Does not include bounding box annotations.,0
,0
%% Constants and imports,0
"We'll write images, metadata downloads, and temporary files here",0
"Some datasets don't have ""sequence_level_annotation"" fields populated, but we know their",0
annotation level,0
%% Download and parse the metadata file,0
To select an individual data set for debugging,0
%% Download and extract metadata for the datasets we're interested in,0
%% Load taxonomy data,0
"%% Build a dictionary that maps each [dataset,query] pair to the full taxonomic label set",0
i_row = 0; row = taxonomy_df.iloc[i_row],0
%% Process annotations for each dataset,0
ds_name = list(metadata_table.keys())[0],0
"Go through annotations, marking each image with the categories that are present",0
,0
ann = annotations[0],0
im = images[10],0
This field name was only used for Caltech Camera Traps,0
raise ValueError('Suspicious date parsing result'),0
Special case we don't want to print a warning about,0
"Location, sequence, and image IDs are only guaranteed to be unique within",0
"a dataset, so for the output .csv file, include both",0
category_name = list(categories_this_image)[0],0
Only print a warning the first time we see an unmapped label,0
...for each category that was applied at least once to this image,0
...for each image in this dataset,0
print('Warning: no date information available for this dataset'),0
print('Warning: no location information available for this dataset'),0
...for each dataset,0
...with open(),0
%% Read the .csv back,0
%% Do some post-hoc integrity checking,0
"-1 isn't *really* valid, but we use it sometimes for sequences of unknown length",0
i_row = 0; row = df.iloc[i_row],0
%% Preview constants,0
%% Choose images to download,0
ds_name = list(metadata_table.keys())[2],0
Find all rows for this dataset,0
...for each dataset,0
%% Download images,0
i_image = 0; image = images_to_download[i_image],0
%% Write preview HTML,0
im = images_to_download[0],0
,0
Common constants and functions related to LILA data management/retrieval.,0
,0
%% Imports and constants,0
LILA camera trap master metadata file,0
"wildlife_insights_taxonomy_url = 'https://api.wildlifeinsights.org/api/v1/taxonomy?fields=class,order,family,genus,species,authority,taxonomyType,uniqueIdentifier,commonNameEnglish&page[size]={}'.format(wildlife_insights_page_size)",0
from ai4eutils,0
%% Common functions,0
"We haven't implemented paging, make sure that's not an issue",0
d['data'] is a list of items that look like:,0
Put the master metadata file in the same folder where we're putting images,0
Read lines from the master metadata file,0
Parse those lines into a table,0
Each line in this file is name/sas_url/json_url/[bbox_json_url],0
Create a separate entry for bounding boxes if they exist,0
Unzip if necessary,0
,0
get_lila_category_list.py,0
,0
Generates a .json-formatted dictionary mapping each LILA dataset to all categories,0
"that exist for that dataset, with counts for the number of occurrences of each category",0
"(the number of *annotations* for each category, not the number of *images*).",0
,0
"Also loads the taxonomy mapping file, to include scientific names for each category.",0
,0
get_lila_category_counts counts the number of *images* for each category in each dataset.,0
,0
%% Constants and imports,0
array to fill for output,0
"We'll write images, metadata downloads, and temporary files here",0
Created by get_lila_category_list.py... contains counts for each category,0
%% Load category and taxonomy files,0
%% Map dataset names and category names to scientific names,0
i_row = 1; row = taxonomy_df.iloc[i_row]; row,0
%% Download and parse the metadata file,0
%% Download and extract metadata for the datasets we're interested in,0
%% Get category names and counts for each dataset,0
ds_name = 'NACTI',0
Open the metadata file,0
Collect list of categories and mappings to category name,0
ann = annotations[0],0
c = categories[0],0
"Don't do taxonomy mapping for bbox data sets, which are sometimes just binary and are",0
always redundant with the class-level data sets.,0
"As of right now, this is the only quirky case",0
...for each dataset,0
%% Save dict,0
%% Print the results,0
ds_name = list(dataset_to_categories.keys())[0],0
...for each dataset,0
,0
download_lila_subset.py,0
,0
"Example of how to download a list of files from LILA, e.g. all the files",0
in a data set corresponding to a particular species.,0
,0
%% Constants and imports,0
LILA camera trap master metadata file,0
"In this example, we're using the Missouri Camera Traps data set and the Caltech Camera Traps dataset",0
All lower-case; we'll convert category names to lower-case when comparing,0
"We'll write images, metadata downloads, and temporary files here",0
"We will demonstrate two approaches to downloading, one that loops over files",0
"and downloads directly in Python, another that uses AzCopy.",0
,0
AzCopy will generally be more performant and supports resuming if the,0
transfers are interrupted.  This script assumes that azcopy is on the system path.,0
Number of concurrent download threads (when not using AzCopy) (AzCopy does its,0
own magical parallelism),0
%% Support functions,0
remove the leading '/',0
%% Download and parse the metadata file,0
Put the master metadata file in the same folder where we're putting images,0
Read lines from the master metadata file,0
Parse those lines into a table,0
Each line in this file is name/base_url/json_url/[box_url],0
%% Download and extract metadata for the datasets we're interested in,0
Unzip if necessary,0
...for each dataset of interest,0
%% List of files we're going to download (for all data sets),0
"Flat list or URLS, for use with direct Python downloads",0
For use with azcopy,0
This may or may not be a SAS URL,0
# Open the metadata file,0
# Build a list of image files (relative path names) that match the target species,0
Retrieve all the images that match that category,0
Retrieve image file names,0
Convert to URLs,0
...for each dataset,0
%% Download those image files,0
ds_name = 'Caltech Camera Traps',0
ds_name = 'SWG Camera Traps',0
We want to use the whole relative path for this script (relative to the base of the container),0
"to build the output filename, to make sure that different data sets end up in different folders.",0
This may or may not be a SAS URL,0
For example:,0
,0
caltech-unzipped/cct_images,0
swg-camera-traps,0
Check whether the URL includes a folder,0
E.g. caltech-unzipped,0
E.g. cct_images,0
E.g. swg-camera-traps,0
"Write out a list of files, and use the azcopy ""list-of-files"" option to download those files.",0
,0
This azcopy feature is unofficially documented at:,0
,0
https://github.com/Azure/azure-storage-azcopy/wiki/Listing-specific-files-to-transfer,0
,0
import clipboard; clipboard.copy(cmd),0
Loop over files,0
,0
get_lila_category_counts.py,0
,0
Count the number of images and bounding boxes with each label in one or more LILA datasets.,0
,0
"This script doesn't write these counts out anywhere other than the console, it's just intended",0
as a template for doing operations like this on LILA data.  get_lila_category_list.py writes,0
"information out to a .json file, but it counts *annotations*, not *images*, for each category.",0
,0
%% Constants and imports,0
"If None, will use all datasets",0
"We'll write images, metadata downloads, and temporary files here",0
%% Download and parse the metadata file,0
%% Download and extract metadata for the datasets we're interested in,0
%% Count categories,0
ds_name = datasets_of_interest[0],0
"Go through annotations, marking each image with the categories that are present",0
,0
ann = annotations[0],0
Now go through images and count categories,0
im = images[0],0
...for each dataset,0
%% Print the results,0
...for each dataset,0
,0
add_url_to_database.py,0
,0
"Adds a ""url"" field to a coco-camera-traps .json database, specifically to allow the db to",0
be reviewed in the Visipedia annotation tool.,0
,0
,0
remove_corrupted_images_from_database.py,0
,0
"Given a coco-camera-traps .json file, checks all images for TF-friendliness and generates",0
a new .json file that only contains the non-corrupted images.,0
,0
%% Imports and constants,0
import multiprocessing,0
"I leave this at an annoying low number, since by definition weird stuff will",0
"be happening in the TF kernel, and it's useful to keep having content in the console.",0
%% Function definitions,0
"I sometimes pass in a list of images, sometimes a dict with a single",0
element mapping a job ID to the list of images,0
"We're about to start a lot of TF sessions, and we don't want gobs",0
of debugging information printing out for every session.,0
At some point we were creating a single session and looping over images,0
"within that session, but the only way I found to reliably not run out",0
of GPU memory was to create a session per image and gc.collect() after,0
each session.,0
Map Image IDs to boolean (should I keep this image?),0
"Convert to lists, append job numbers to the image lists",0
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)",0
Merge results,0
%% Interactive driver,0
%%,0
base_dir = r'D:\temp\snapshot_serengeti_tfrecord_generation',0
Load annotations,0
Check for corruption,0
Write out only the uncorrupted data,0
%% Command-line driver,0
#######,0
,0
integrity_check_json_db.py,0
,0
"Does some integrity-checking and computes basic statistics on a db, specifically:",0
,0
* Verifies that required fields are present and have the right types,0
* Verifies that annotations refer to valid images,0
* Verifies that annotations refer to valid categories,0
"* Verifies that image, category, and annotation IDs are unique",0
,0
* Optionally checks file existence,0
,0
* Finds un-annotated images,0
* Finds unused categories,0
,0
* Prints a list of categories sorted by count,0
,0
#######,0
%% Constants and environment,0
%% Functions,0
"If baseDir is non-empty, checks image existence",0
This is used in a medium-hacky way to share modified options across threads,1
print('Image path {} does not exist'.format(filePath)),0
"#%% Read .json file if necessary, integrity-check fields",0
info = data['info'],0
"#%% Build dictionaries, checking ID uniqueness and internal validity as we go",0
Confirm that required fields are present,0
Confirm ID uniqueness,0
...for each category,0
image = images[0],0
Confirm that required fields are present,0
Confirm ID uniqueness,0
We previously supported ints here; this should be strings now,0
"assert isinstance(image['location'], str) or isinstance(image['location'], int), \",0
'Illegal image location type',0
Are we checking for unused images?,0
Recursively enumerate images,0
print('Image {} is unused'.format(p)),0
Are we checking file existence and/or image size?,0
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)",0
...for each image,0
Confirm that required fields are present,0
Confirm ID uniqueness,0
Confirm validity,0
...for each annotation,0
#%% Print statistics,0
Find un-annotated images and multi-annotation images,0
Find unused categories,0
Prints a list of categories sorted by count,0
,0
https://stackoverflow.com/questions/72899/how-do-i-sort-a-list-of-dictionaries-by-a-value-of-the-dictionary,0
...def integrity_check_json_db(),0
%% Command-line driver,0
%% Interactive driver(s),0
%%,0
Integrity-check .json files for LILA,0
options.iMaxNumImages = 10,0
,0
combine_two_json_files.py,0
,0
"Merges two coco-camera-traps .json files. In particular, categories are combined and re-numbered.",0
,0
Combined Info,0
Combined Images,0
Combined Categories,0
## categories to merge,0
Combined Annotations,0
,0
make_detection_db_for_viewing.py,0
,0
"Given a .json file with ground truth bounding boxes, and a .p file containing detections for the same images,",0
"creates a new .json file with separate classes for ground truth and detection, suitable for viewing in the Visipedia",0
annotation tool.,0
,0
%% Imports and constants,0
%% Main function,0
im_id_to_im = {im['id']:im for im in images},0
make new categories to distinguish between ground truth and detections,0
"update all gt annotations to be class ""gt""",0
collect all detections by image,0
keep any detection with score above det_thresh,0
"need to convert bbox from [x1,y1,x2,y2] to [x,y,w,h]",0
"add ""info"" and ""licenses"" for annotation tools to function",0
create new db,0
%% Command-line handling,0
,0
analyze_json_database.py,0
,0
Plots location/class/etc. distributions for classes in a coco-camera-traps .json file.,0
,0
Currently includes some one-off code for specific species.,0
,0
%% Constants and imports,0
%% Path configuration,0
%% Load source data,0
%% Build image/category dictionaries,0
%% Make plot of category distribution,0
%% make plots of location distribution,0
"plt.title('Number of images per location, by category')",0
"plt.tight_layout(rect=[0,0,1,0.9])",0
#make plot of images per season,0
%% Make plot of lions per location,0
%% Make plot of elephants per location,0
for loc in sorted_by_total[:25]:,0
"print('Location:' + loc[0] +', Lions: ' + str(len(loc_to_lion_ims[loc[0]])) + ', Elephants: ' + str(len(loc_to_elephant_ims[loc[0]])) + ', Total ims: ' + str(len(loc_to_ims[loc[0]])))",0
,0
subset_json_db.py,0
,0
Select a subset of images (and associated annotations) from a .json file,0
in COCO Camera Traps format.,0
,0
"To subset the .json files produced by our batch processing API, see",0
subset_json_detector_output.py,0
,0
Sample invocation:,0
,0
"python subset_json_db.py ""E:\Statewide_wolf_container\idfg_20190409.json"" ""E:\Statewide_wolf_container\idfg_20190409_clearcreek.json"" ""clearcreek"" --ignore_case",0
,0
%% Constants and imports,0
%% Functions,0
Load the input file if necessary,0
Find images matching the query,0
Find annotations referring to those images,0
Write the output file if requested,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
%% Constants and imports,0
%% Merge functions,0
i_input_dict = 0; input_dict = input_dicts[i_input_dict],0
We will prepend an index to every ID to guarantee uniqueness,0
Map detection categories from the original data set into the merged data set,0
...for each category,0
Merge original image list into the merged data set,0
Create a unique ID,0
...for each image,0
Same for annotations,0
...for each annotation,0
"Merge info dicts, don't check completion time fields",0
...for each dictionary,0
Convert merged image dictionaries to a sorted list,0
%% Driver,0
,0
add_width_and_height_to_database.py,0
,0
Grabs width and height from actual image files for a .json database that is missing w/h.,0
,0
Originally used when we created a .json file for snapshot serengeti from .csv.,0
,0
"This file converts the JSON output of the batch processing API to a pickle file, which can be used by the",0
script ./make_classification_dataset.py,0
"Convert boxes from JSON   [x_min, y_min, width_of_box, height_of_box]",0
"to PICKLE [ymin,  xmin,  ymax,         xmax]",0
Write detections to file with pickle,0
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the",0
script ./make_classification_dataset.py,0
Write detections to file with pickle,0
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the",0
script ./make_classification_dataset.py,0
Assumes that the root of the CameraTrap repo is in the PYTHONPATH,0
Minimum threshold to put a detection into the output JSON file,0
Parameter check,0
Load detections from input,0
Load COCO style annotations,0
Build output JSON in format version 1.0,0
Adding the only known metadata info,0
"The pickle file does not contain category information, so we assume the default",0
For each image with detections,0
for each detection,0
"Convert boxes from [ymin, xmin, ymax, xmax] format to",0
"[x_min, y_min, width_of_box, height_of_box]",0
Write output json,0
global flag for whether or not we encounter missing images,0
"- will only print ""missing image"" warning once",0
TFRecords variables,0
1.3 for the cropping during test time and 1.3 for the context that the,0
CNN requires in the left-over image,0
Create output directories,0
Load COCO style annotations from the input dataset,0
"Get all categories, their names, and create updated ID for the json file",0
Prepare the coco-style json files,0
Split the dataset by locations,0
Load detections,0
Write out COCO-style json files to the output directory,0
Write detections to file with pickle,0
## Preparations: get all the output tensors,0
For all images listed in the annotations file,0
Skip the image if it is annotated with more than one category,0
"Get ""old"" and ""new"" category ID and category name for this image.",0
Skip if in excluded categories.,0
get path to image,0
"If we already have detection results, we can use them",0
Otherwise run detector and add detections to the collection,0
Only select detections with confidence larger than DETECTION_THRESHOLD,0
Skip image if no detection selected,0
whether it belongs to a training or testing location,0
Skip images that we do not have available right now,0
- this is useful for processing parts of large datasets,0
Load image,0
Run inference,0
"remove batch dimension, and convert from float32 to appropriate type",0
convert normalized bbox coordinates to pixel coordinates,0
Pad the detected animal to a square box and additionally by,0
"PADDING_FACTOR. The result will be in crop_boxes. However, we need to make",0
sure that its box coordinates are still within the image.,0
"for each bounding box, crop the image to the padded box and save it",0
"Create the file path as it will appear in the annotation json,",0
adding the box number if there are multiple boxes,0
"if the cropped file already exists, verify its size",0
Add annotations to the appropriate json,0
js_keys = ['/'.join(im['file_name'].split('/')[1:])[:-4] for im in js['images']],0
for tk in js_keys:,0
"assert np.isclose(1, np.sum(detections[tk]['detection_scores'] > 0.5))",0
,0
separate_detections_by_size,0
,0
Not-super-well-maintained script to break a list of API output files up,0
based on bounding box size.,0
,0
%% Imports and constants,0
Folder with one or more .json files in it that we want to split up,0
Enumerate .json files,0
Define size thresholds and confidence thresholds,0
"Not used directly in this script, but useful if we want to generate previews",0
%% Split by size,0
For each size threshold...,0
For each file...,0
fn = input_files[0],0
Just double-checking; we already filtered this out above,0
Don't reprocess .json files we generated with this script,0
Load the input file,0
For each image...,0
1.1 is the same as infinity here; no box can be bigger than a whole image,0
What's the smallest detection above threshold?,0
"[x_min, y_min, width_of_box, height_of_box]",0
,0
size = w * h,0
...for each detection,0
Which list do we put this image on?,0
...for each image in this file,0
Make sure the number of images adds up,0
Write out all files,0
...for each size threshold,0
...for each file,0
,0
tile_images.py,0
,0
Split a folder of images into tiles.  Preserves relative folder structure in a,0
"new output folder, with a/b/c/d.jpg becoming, e.g.:",0
,0
a/b/c/d_row_0_col_0.jpg,0
a/b/c/d_row_0_col_1.jpg,0
,0
%% Imports and constants,0
from ai4eutils,0
%% Main function,0
TODO: parallelization,1
,0
i_fn = 2; relative_fn = image_relative_paths[i_fn],0
Can we skip this image because we've already generated all the tiles?,0
TODO: super-sloppy that I'm pasting this code from below,1
From:,0
,0
https://github.com/whiplashoo/split-image/blob/main/src/split_image/split.py,0
i_col = 0; i_row = 1,0
left/top/right/bottom,0
...for each row,0
...for each column,0
...for each image,0
%% Interactive driver,0
%%,0
%%,0
,0
rde_debug.py,0
,0
Some useful cells for comparing the outputs of the repeat detection,0
"elimination process, specifically to make sure that after optimizations,",0
results are the same up to ordering.,0
,0
%% Compare two RDE files,0
i_dir = 0,0
break,0
"Regardless of ordering within a directory, we should have the same",0
number of unique detections,0
Re-sort,0
Make sure that we have the same number of instances for each detection,0
Make sure the box values match,0
,0
aggregate_video.py,0
,0
Aggregate results and render output video for a video that's already been run through MD,0
,0
%% Constants,0
%% Processing,0
im = d['images'][0],0
...for each detection,0
This is no longer included in output files by default,0
# Split into frames,0
# Render output video,0
## Render detections to images,0
## Combine into a video,0
,0
ocr_sandbox.py,0
,0
sandbox for experimenting with using OCR to pull metadata from camera trap images,0
,0
The general approach is:,0
,0
"* Crop a fixed percentage from the top and bottom of an image, slightly larger",0
than the largest examples we've seen of how much space is used for metadata.,0
,0
"* Refine that crop by blurring a little, then looking for huge peaks in the",0
"color histogram suggesting a solid background, then finding rows that are",0
mostly that color.,0
,0
"* Crop to the refined crop, then run pytesseract to extract text",0
,0
"* Use regular expressions to find time and date, in the future can add, e.g.,",0
"temperature (which is often present *only* in the images, unlike time/date which",0
are also usually in EXIF but often wrong or lost in processing),0
,0
"The metadata extraction (EXIF, IPTC) here is just sample code that seemed to",0
belong in this file.,0
,0
Contact: Dan Morris (cameratraps@lila.science),0
,0
%% Constants and imports,0
pip install pytesseract,0
,0
"Also intall tesseract from: https://github.com/UB-Mannheim/tesseract/wiki, and add",0
"the installation dir to your path (on Windows, typically C:\Program Files (x86)\Tesseract-OCR)",0
pip install IPTCInfo3,0
from the ai4eutils repo: https://github.com/Microsoft/ai4eutils,0
,0
"Only used for writing out a summary, not important for the core metadata extraction",0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Using a semi-arbitrary metric of how much it feels like we found the,0
"text-containing region, discard regions that appear to be extraction failures",0
Pad each crop with a few pixels to make tesseract happy,0
Discard text from the top,0
"When we're looking for pixels that match the background color, allow some",0
tolerance around the dominant color,0
We need to see a consistent color in at least this fraction of pixels in our rough,0
crop to believe that we actually found a candidate metadata region.,0
"What fraction of the [top,bottom] of the image should we use for our rough crop?",0
A row is considered a probable metadata row if it contains at least this fraction,0
"of the background color.  This is used only to find the top and bottom of the crop area,",0
"so it's not that *every* row needs to hit this criteria, only the rows that are generally",0
above and below the text.,0
%% Support functions,0
"%% Load some images, pull EXIF and IPTC data for fun",0
%% Rough crop,0
"This will be an nImages x 1 list of 2 x 1 lists (image top, image bottom)",0
image = images[0],0
"l,t,r,b",0
,0
"0,0 is upper-left",0
"%% Close-crop around the text, return a revised image and success metric",0
Did we find a sensible mode that looks like a background value?,0
"This looks very scientific, right?  Definitely a probability?",0
"print('Failed min background fraction test: {} of {}'.format(pBackGroundValue,minBackgroundFraction))",0
"Notes to self, things I tried that didn't really go anywhere...",0
"analysisImage = cv2.blur(analysisImage, (3,3))",0
"analysisImage = cv2.medianBlur(analysisImage,5)",0
"analysisImage = cv2.Canny(analysisImage,100,100)",0
imagePil = Image.fromarray(analysisImage); imagePil,0
Use row heuristics to refine the crop,0
,0
This egregious block of code makes me miss my fluency in Matlab.,1
"print('Cropping to {},{},{},{}'.format(x,y,w,h))",0
Crop the image,0
"For some reason, tesseract doesn't like characters really close to the edge",0
imagePil = Image.fromarray(croppedImage); imagePil,0
%% Go to OCR-town,0
"An nImages x 2 list of strings, extracted from the top and bottom of each image",0
An nImages x 2 list of cropped images,0
iImage = 0; iRegion = 1; regionSet = imageRegions[iImage]; region = regionSet[iRegion],0
text = pytesseract.image_to_string(region),0
pil --> cv2,0
"image = cv2.medianBlur(image, 3)",0
"image = cv2.erode(image, None, iterations=2)",0
"image = cv2.dilate(image, None, iterations=4)",0
"image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]",0
"image = cv2.blur(image, (3,3))",0
"image = cv2.copyMakeBorder(image,10,10,10,10,cv2.BORDER_CONSTANT,value=[0,0,0])",0
"text = pytesseract.image_to_string(imagePil, lang='eng')",0
https://github.com/tesseract-ocr/tesseract/wiki/Command-Line-Usage,0
"psm 6: ""assume a single uniform block of text""",0
,0
...for each cropped region,0
...for each image,0
%% Extract dates and times,0
s = '1:22 pm',0
s = '1:23:44 pm',0
%% Write results to a handy html file,0
Add image name and resized image,0
Add results and individual region images,0
"textStyle = ""font-family:calibri,verdana,arial;font-weight:bold;font-size:150%;text-align:left;margin-left:50px;""",0
%% Scrap,0
Alternative approaches to finding the text/background  region,0
Using findCountours(),0
imagePil = Image.fromarray(analysisImage); imagePil,0
"analysisImage = cv2.erode(analysisImage, None, iterations=3)",0
"analysisImage = cv2.dilate(analysisImage, None, iterations=3)",0
"analysisImage = cv2.threshold(analysisImage, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]",0
Find object with the biggest bounding box,0
Using connectedComponents(),0
analysisImage = image,0
print('Found {} components'.format(nb_components)),0
We just want the *background* image,0
,0
"Takes a .json file with MD results for an individual video, and converts to a .csv that includes",0
frame times.  This is very bespoke to animal detection and does not include other classes.,0
,0
%% Imports and constants,0
Only necessary if you want to extract the sample rate from the video,0
%% Extract the sample rate if necessary,0
%% Load results,0
%% Convert to .csv,0
i_image = 0; im = results['images'][i_image],0
,0
umn-pr-analysis.py,0
,0
Precision/recall analysis for UMN data,0
,0
%% Imports and constants,0
results_file = results_file_filtered,0
"For two deployments, we're only processing imagse in the ""detections"" subfolder",0
String to remove from MegaDetector results,0
%% Enumerate deployment folders,0
%% Load MD results,0
im = md_results['images'][0],0
%% Load ground truth,0
i_row = 0; row = ground_truth_df.iloc[i_row],0
%% Create relative paths for ground truth data,0
"Some deployment folders have no subfolders, e.g. this is a valid file name:",0
,0
M00/01010132.JPG,0
,0
"But some deployment folders have subfolders, e.g. this is also a valid file name:",0
,0
N17/100EK113/07160020.JPG,0
,0
"So we can't find files by just concatenating folder and file names, we have to enumerate and explicitly",0
"map what will appear in the ground truth as ""folder/filename"" to complete relative paths.",0
deployment_name = list(deployment_folders)[0],0
Enumerate all files in this folder,0
"print('Enumerated {} files for deployment {}'.format(len(files),deployment_name))",0
filename = files[100],0
...for each file in this deployment,0
...for each deployment,0
%% Map relative paths to MD results,0
%% Add relative paths to our ground truth table,0
i_row = 0; row = ground_truth_df.iloc[i_row],0
"row['filename'] looks like, e.g. A01/01080001.JPG.  This is not actually a path, it's",0
"just the deployment ID and the image name, separated by a slash.",0
Find the relative path for this image,0
Make sure we have MegaDetector results for this file,0
Make sure this image file exists,0
...for each row in the ground truth table,0
%% Take everything out of Pandas,0
%% Some additional error-checking of the ground truth,0
An early version of the data required consistency between common_name and is_blank,0
%% Combine MD and ground truth results,0
d = ground_truth_dicts[0],0
Find the maximum confidence for each category,0
...for each detection,0
...for each image,0
%% Precision/recall analysis,0
...for each image,0
Thresholds go up throughout precisions/recalls/thresholds; find the last,0
value where recall is at or above target.  That's our precision @ target recall.,0
Flatten the confusion matrix,0
Write precision/recall plot to .png file in output directory,0
pr_figure_relative_filename = 'prec_recall.png',0
"pr_figure_filename = os.path.join(output_dir, pr_figure_relative_filename)",0
plt.show(block=False),0
"precision_recall_analysis(human_gt_labels,human_prediction_probs,'humans')",0
%% Find and manually review all images of humans,0
%%,0
"...if this image is annotated as ""human""",0
...for each image,0
%% Find and manually review all MegaDetector animal misses,0
%%,0
im = merged_images[0],0
GT says this is not an animal,0
GT says this is an animal,0
%% Convert .json to .csv,0
%%,0
,0
kga-pr-analysis.py,0
,0
Precision/recall analysis for KGA data,0
,0
%% Imports and constants,0
%% Load and filter MD results,0
%% Load and filter ground truth,0
%% Map images to image-level results,0
%% Map sequence IDs to images and annotations to images,0
Verify consistency of annotation within a sequence,0
TODO,1
%% Find max confidence values for each category for each sequence,0
seq_id = list(sequence_id_to_images.keys())[1000],0
im = images_this_sequence[0],0
det = md_result['detections'][],0
...for each detection,0
...for each image in this sequence,0
...for each sequence,0
%% Prepare for precision/recall analysis,0
seq_id = list(sequence_id_to_images.keys())[1000],0
cat_id = list(category_ids_this_sequence)[0],0
...for each category in this sequence,0
...for each sequence,0
%% Precision/recall analysis,0
"Confirm that thresholds are increasing, recall is decreasing",0
This is not necessarily true,0
assert np.all(precisions[:-1] <= precisions[1:]),0
Thresholds go up throughout precisions/recalls/thresholds; find the max,0
value where recall is at or above target.  That's our precision @ target recall.,0
"This is very slightly optimistic in its handling of non-monotonic recall curves,",0
but is an easy scheme to deal with.,0
Flatten the confusion matrix,0
Write precision/recall plot to .png file in output directory,0
pr_figure_relative_filename = 'prec_recall.png',0
"pr_figure_filename = os.path.join(output_dir, pr_figure_relative_filename)",0
plt.show(block=False),0
"precision_recall_analysis(human_gt_labels,human_prediction_probs,'humans')",0
%% Scrap,0
%% Find and manually review all sequence-level MegaDetector animal misses,0
input_base = '/media/user/lila-01/lila/snapshot-safari/KGA/KGA_public',0
i_sequence = 0; seq_id = pr_sequence_ids[i_sequence],0
i_seq = 0; seq_id = false_negative_sequences[i_seq],0
"sequence_folder = os.path.join(sequence_preview_dir,'seq_{}'.format(str(i_seq).zfill(3)))",0
fn = image_files[0],0
"print('Copying {} to {}'.format(input_path,output_path))",0
...for each file in this sequence.,0
...for each sequence,0
%% Image-level postprocessing,0
parse arguments,0
check if a GPU is available,0
load a pretrained embedding model,0
setup experiment,0
load the embedding model,0
setup the target dataset,0
setup finetuning criterion,0
setup an active learning environment,0
create a classifier,0
the main active learning loop,0
Active Learning,0
finetune the embedding model and load new embedding values,0
gather labeled pool and train the classifier,0
save a snapshot,0
Load a checkpoint if necessary,0
setup the training dataset and the validation dataset,0
setup data loaders,0
check if a GPU is available,0
create a model,0
setup loss criterion,0
define optimizer,0
load a checkpoint if provided,0
setup a deep learning engine and start running,0
train the model,0
train for one epoch,0
evaluate on validation set,0
save a checkpoint,0
open the file,0
read it,0
do the substitution,0
matplotlib.use('Agg'),0
from UIComponents.DBObjects import *,0
Initialize Database,0
# database connection credentials,0
# try to connect as USER to database DB_NAME through peewee,0
Load the saved embedding model,0
dataset_query = Detection.select().limit(5),0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})",0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getalllabels()) , dataset.getallpaths(), {})",0
Random examples to start,0
"random_ids = np.random.choice(dataset.current_set, 1000, replace=False).tolist()",0
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)",0
print(random_ids),0
Move Records,0
"moveRecords(dataset, DetectionKind.ModelDetection.value, DetectionKind.UserDetection.value, random_ids)",0
#print([len(x) for x in dataset.set_indices]),0
# Finetune the embedding model,0
#dataset.set_kind(DetectionKind.UserDetection.value),0
#dataset.train(),0
"#train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)",0
"#finetune_embedding(model, checkpoint['loss_type'], dataset, 32, 4, 100)",0
#save_checkpoint({,0
"#        'arch': model.arch,",0
"#        'state_dict': model.state_dict(),",0
"#        'optimizer' : optimizer.state_dict(),",0
"#        'loss_type' : loss_type,",0
"#        }, False, ""%s%s_%s_%04d.tar""%('finetuned', loss_type, model.arch, len(dataset.set_indices[DetectionKind.UserDetection.value])))",0
Get indices of samples to get user to label,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
"kwargs[""already_selected""].extend(indices)",0
Train on samples that have been labeled so far,0
Test on the samples that have not been labeled,0
"'optimizer' : optimizer.state_dict(),",0
num_classes= len(train_dataset.getClassesInfo()[0]),0
"print(""Num Classes= ""+str(num_classes))",0
define loss function (criterion) and optimizer,0
"optimizer = torch.optim.SGD(params, momentum = 0.9, lr = args.lr, weight_decay = args.weight_decay)",0
optimizer.load_state_dict(checkpoint['optimizer']),0
train for one epoch,0
"adjust_lr(optimizer,epoch)",0
if epoch % 1 == 0 and epoch > 0:,0
"a, b, c = e.predict(train_embd_loader, load_info = True, dim = args.feat_dim)",0
"plot_embedding(reduce_dimensionality(a), b, c, {})",0
evaluate on validation set,0
--------some stuff needed to get AJAX to work with bottle?--------#,0
-------------------------------------------------------------------------------- #,0
PREPARE TO QUEUE IMAGES FOR LABELING,0
-------------------------------------------------------------------------------- #,0
# Connect as USER to database DB_NAME through peewee and initialize database proxy,0
# Load embedding model,0
---------------------------------------------------------------------- #,0
CREATE QUEUE OF IMAGES TO LABEL,0
---------------------------------------------------------------------- #,0
Use classifier to generate predictions,0
# Update model predicted class in PostgreSQL database,0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?,0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
-------------------------------------------------------------------------------- #,0
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI,0
-------------------------------------------------------------------------------- #,0
"# static routes (to serve CSS, etc.)",0
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps",0
"return bottle.static_file(filename, root='../../../../../../../../../../../.')",0
# dynamic routes,0
if data['display_grayscale']:,0
indices_to_exclude.update(set(color_indices)),0
elif not data['display_grayscale']:,0
indices_to_exclude.update(set(grayscale_indices)),0
data['display_images'] = {},0
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices],0
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices],0
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices],0
data['display_images']['detection_categories'] = [],0
for i in indices:,0
if str(dataset.samples[i][1]) == 'None':,0
data['display_images']['detection_categories'].append('None'),0
else:,0
existing_category_entries = {cat.id: cat.name for cat in Category.select()},0
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()",0
data['display_images']['detection_categories'].append(cat_name),0
Use image ids in images_to_label to get the corresponding dataset indices,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
print(set(dataset.set_indices[4]).update(set(indices_to_label))),0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Use image id images_to_label to get the corresponding dataset index,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?,0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Train on samples that have been labeled so far,0
dataset.set_kind(DetectionKind.UserDetection.value),0
print(y_train),0
Predict on the samples that have not been labeled,0
print(y_pred),0
Update model predicted class in PostgreSQL database,0
timer = time.time(),0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
print('Updating the database took %0.2f seconds'%(time.time() - timer)),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids)),0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
"once the classifier has been trained the first time, switch to AL sampling",0
# sourceMappingURL=bootstrap.bundle.min.js.map,0
"print(ap_distances.size(),an_distances.size())",0
losses = -(((-ap_distances)/128)+1+1e-16).log() - (((-(128-an_distances))/128)+1+1e-16).log(),0
import pdb,0
pdb.set_trace(),0
losses = ap_distances - an_distances + self.margin,0
print(losses.size()),0
from UIComponents.DBObjects import *,0
TODO: should this also change self.kind?,1
get the embedding representations for all samples (i.e. set current_set to all indices),0
"return PILImage.open(os.path.join(self.img_base,path)).convert('RGB')",0
"print(self.labels_set, self.n_classes)",0
from sklearn.manifold import TSNE,0
embedding= TSNE(n_components=2).fit_transform(X),0
embedding= PCA(n_components=2).fit_transform(X),0
return X,0
"print(dir(event), type(sc))",0
"print(label,bgcolor)",0
"patches.append(mpatches.Patch(color=indexcolors[i], label=label))",0
"plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,",0
"ncol=12, mode=""expand"", borderaxespad=0., handles=patches)",0
plt.legend(handles=patches),0
"plt.xlabel('Dim 1', fontsize=12)",0
"plt.ylabel('Dim 2', fontsize=12)",0
plt.grid(True),0
"plt.xlabel('Dim 1', fontsize=12)",0
"plt.ylabel('Dim 2', fontsize=12)",0
plt.grid(True),0
print(thumb),0
"img.thumbnail((16, 12), PILImage.ANTIALIAS)",0
plt.show(),0
Add all negatives for all positive pairs,0
print(triplets.shape[0]),0
compute output,0
measure accuracy,0
compute loss on this batch,0
"train on a batch, record loss, and measure accuracy (if calc_accuracy)",0
compute output,0
measure accuracy and record loss,0
switch to evaluate mode,0
compute output,0
switch to evaluate mode,0
"self.fc13 = nn.Linear(128, 64)",0
self.bn2 = nn.BatchNorm1d(64),0
x = F.relu(self.fc12(x)),0
x = F.relu(self.bn1(self.fc13(x))),0
x = F.relu(self.fc13(x)),0
"x = F.dropout(x, training=self.training)",0
save features last FC layer,0
x = F.relu(x),0
save features last FC layer,0
import matplotlib.pyplot as plt,0
plt.switch_backend('agg'),0
class EmbeddingNet(nn.Module):,0
"def __init__(self, architecture, feat_dim, use_pretrained=False):",0
"super(EmbeddingNet, self).__init__()",0
self.feat_dim= feat_dim,0
self.inner_model = models.__dict__[architecture](pretrained=use_pretrained),0
if architecture.startswith('resnet'):,0
in_feats= self.inner_model.fc.in_features,0
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)",0
elif architecture.startswith('inception'):,0
in_feats= self.inner_model.fc.in_features,0
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)",0
if architecture.startswith('densenet'):,0
in_feats= self.inner_model.classifier.in_features,0
"self.inner_model.classifier = nn.Linear(in_feats, feat_dim)",0
if architecture.startswith('vgg'):,0
in_feats= self.inner_model.classifier._modules['6'].in_features,0
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)",0
if architecture.startswith('alexnet'):,0
in_feats= self.inner_model.classifier._modules['6'].in_features,0
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)",0
"def forward(self, x):",0
return self.inner_model.forward(x),0
class NormalizedEmbeddingNet(EmbeddingNet):,0
"def __init__(self, architecture, feat_dim, use_pretrained=False):",0
"EmbeddingNet.__init__(self, architecture, feat_dim, use_pretrained = use_pretrained)",0
"def forward(self, x):",0
embedding =  F.normalize(self.inner_model.forward(x))*10.0,0
"return embedding, embedding",0
"def get_random_images(num, image_dir, test_transforms):",0
"data = datasets.ImageFolder(image_dir, transform=test_transforms) # slight abuse; this expects subfolders corresponding to classes but we have no classes here",0
indices = list(range(len(data))),0
np.random.shuffle(indices),0
idx = indices[:num],0
from torch.utils.data.sampler import SubsetRandomSampler,0
sampler = SubsetRandomSampler(idx),0
"loader = torch.utils.data.DataLoader(data,",0
"sampler=sampler, batch_size=num)",0
dataiter = iter(loader),0
"images, labels = dataiter.next()",0
"return images, labels",0
"def predict_image(image, model, test_transforms):",0
"device = torch.device(""cuda"" if torch.cuda.is_available()",0
"else ""cpu"")",0
image_tensor = test_transforms(image).float(),0
image_tensor = image_tensor.unsqueeze_(0),0
input = Variable(image_tensor),0
input = input.to(device),0
output = model(input)[0],0
return output.data.cpu().numpy(),0
Connect to database and initialize db_proxy,0
# database connection credentials,0
# load the dataset,0
Load the saved embedding model from the checkpoint,0
# update the dataset embedding,0
# Create a folder for saving embedding visualizations with this model checkpoint,0
model_emb_dirname = os.path.basename(args.base_model).split('.')[0],0
"os.makedirs(model_emb_dirname, exist_ok=True)",0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')",0
dataset.embedding_mode(),0
"assert 2==3, 'break'",0
datasetindices = list(range(len(dataset))),0
np.random.shuffle(datasetindices),0
random_indices = datasetindices[:args.num],0
print(random_indices),0
"selected_sample_features = np.array([]).reshape(0, 256)",0
selected_sample_labels = [],0
for idx in random_indices:,0
"selected_sample_features = np.vstack([selected_sample_features, X_train[idx]])",0
selected_sample_labels.append(y_train[idx]),0
img_path = imagepaths[idx].split('.JPG')[0],0
image = dataset.loader(img_path),0
selected_sample_images.append(image),0
# TRY NEAREST NEIGHBORS WALK THROUGH EMBEDDING,0
nbrs = NearestNeighbors(n_neighbors=args.num).fit(selected_sample_features),0
"distances, indices = nbrs.kneighbors(selected_sample_features)",0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')",0
"idx_w_closest_nbr = np.where(distances[:,1] == min(distances[:,1]))[0][0]",0
order = [idx_w_closest_nbr],0
for ii in range(len(distances)):,0
"distances[ii, 0] = np.inf",0
while len(order)<args.num:,0
curr_idx = order[-1],0
curr_neighbors = indices[curr_idx],0
curr_dists = list(distances[curr_idx]),0
# print(min(curr_dists)),0
next_closest_pos = curr_dists.index(min(curr_dists)),0
next_closest = curr_neighbors[next_closest_pos],0
order.append(next_closest),0
# make sure you can't revisit past nodes,0
for vi in order:,0
vi_pos = list(indices[next_closest]).index(vi),0
"distances[next_closest, vi_pos] = np.inf",0
for ii in range(len(order)):,0
imgidx = order[ii],0
image = selected_sample_images[imgidx],0
"image.save(model_emb_dirname+""/img""+str(ii)+""_""+str(selected_sample_labels[imgidx])+"".png"")",0
# Specify the transformations on the input images before inference,0
"# test_transforms = transforms.Compose([transforms.Resize([224, 224]), transforms.ToTensor()])",0
"test_transforms = transforms.Compose([transforms.Resize([256, 256]), transforms.RandomCrop([224, 224]), transforms.RandomHorizontalFlip(), transforms.ColorJitter(), transforms.ToTensor(), transforms.Normalize([0.407328, 0.407328, 0.407328], [0.118641, 0.118641, 0.118641])])",0
"images, labels = get_random_images(num, image_dir, test_transforms)",0
"all_features = np.array([]).reshape(0, 256)",0
for ii in range(len(images)):,0
image = to_pil(images[ii]),0
"features = predict_image(image, model, test_transforms)",0
"all_features = np.vstack([all_features, features])",0
# for ii in range(len(images)):,0
#     image = to_pil(images[ii]),0
"#     image.save(""img""+str(ii)+"".png"")",0
# TRY CLUSTERING,0
kmeans1 = KMeans(n_clusters=5).fit(StandardScaler().fit_transform(all_features)),0
print(kmeans1.labels_),0
for ii in range(len(images)):,0
image = to_pil(images[ii]),0
"filename = str(kmeans1.labels_[ii])+""/img""+str(ii)+"".png""",0
if not os.path.exists(os.path.dirname(filename)):,0
os.makedirs(os.path.dirname(filename)),0
image.save(filename),0
class SaveFeatures():,0
"def __init__(self, module):",0
self.hook = module.register_forward_hook(self.hook_fn),0
"def hook_fn(self, module, input, output):",0
"self.features = torch.tensor(output, requires_grad=True).cuda()",0
def close(self):,0
self.hook.remove(),0
Load the saved embedding model from the checkpoint,0
"Get a sample from the database, with eval transforms applied, etc.",0
Connect to database and sample a dataset,0
output = model.forward(sample_image.unsqueeze(0)),0
print(output),0
with torch.no_grad():,0
sample_image_input = sample_image.cuda(non_blocking=True),0
"_, output = model(sample_image_input) # compute output",0
print(output),0
sample_image = PILImage.open(sample_image_path).convert('RGB'),0
"sample_image = transforms.Compose([Resize([256, 256]), CenterCrop(([[224,224]])), ToTensor(), Normalize([0.369875, 0.388726, 0.347536], [0.136821, 0.143952, 0.145229])])(sample_image)",0
print(list(model_inner_resnet.children())),0
print(model_inner_resnet.fc),0
print(model_inner_resnet.fc0),0
# print(model_inner_resnet.layer4[0].conv2),0
# print(type(model)),0
# print(len(list(model_inner_resnet.children()))),0
# print(list(model.children())),0
# print(list(list(model.children())[0].children())),0
"img = np.uint8(np.random.uniform(150, 180, (56, 56, 3)))/255",0
"img_tensor = torch.unsqueeze(torch.from_numpy(img), 0)",0
full_out = model_inner_resnet.forward(img_tensor),0
print(full_out),0
model(img_tensor),0
activations = SaveFeatures(model_inner_resnet.layer4[0].conv2),0
print(activations.features),0
print(type(activations.features)),0
activations.close(),0
Connect to database and sample a dataset,0
Load the saved embedding model from the checkpoint,0
Update the dataset embedding,0
Get a random query image,0
# # # IMAGES IN THE SAME SEQUENCE # # # #,0
"assert 2==3, 'break'",0
# # # CLOSEST IN (EMBEDDING) FEATURE SPACE # # # #,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Variables for the hierarchical cluster,0
Variables for the AL algorithm,0
Data variables,0
"connectivity = kneighbors_graph(self.transformed_X,max_features)",0
Fit cluster and update cluster variables,0
The sklearn hierarchical clustering algo numbers leaves which correspond,0
to actual datapoints 0 to n_points - 1 and all internal nodes have,0
ids greater than n_points - 1 with the root having the highest node id,0
"If no labels have been observed, simply return uniform distribution",0
"If no observations, return worst possible upper lower bounds",0
Loop through generations from bottom to top,0
Update admissible labels for node,0
Calculate score,0
Determine if node should be split,0
Make sure label set for node so that we can flow to children,0
if necessary,0
Only split if all ancestors are admissible nodes,0
This is part  of definition of admissible pruning,0
Check that pruning covers all leave nodes,0
Fill in labels,0
Observe labels for previously recommended batches,0
TODO(lishal): implement multiple selection methods,1
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
A list of initialized samplers is allowed as an input because,0
"for AL_methods that search over different mixtures, may want mixtures to",0
have shared AL_methods so that initialization is only performed once for,0
computation intensive methods like HierarchicalClusteringAL and,0
states are shared between mixtures.,0
"If initialized samplers are not provided, initialize them ourselves.",0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copy these objects to make sure they are not modified while simulating,0
trajectories as they are used later by the main run_experiment script.,0
Assumes that model has already by fit using all labeled data so,0
the probabilities can be used immediately to hallucinate labels,0
All models need to have predict_proba method,0
Hallucinate labels for selected datapoints to be label,0
using class probabilities from model,0
"Not saving already_selected here, if saving then should sort",0
only for the input to fit but preserve ordering of indices in,0
already_selected,0
Useful to know how accuracy compares for model trained on hallucinated,0
labels vs trained on true labels.  But can remove this train to speed,0
up simulations.  Won't speed up significantly since many more models,0
are being trained inside the loop above.,0
Save trajectory for reference,0
Delete created copies,0
THE INPUTS CANNOT BE MODIFIED SO WE MAKE COPIES FOR THE CHECK LATER,0
Should check model but kernel_svm does not have coef_ so need better,0
handling here,0
Make sure that model object fed in did not change during simulations,0
Return indices based on return type specified,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Set gamma for gaussian kernel to be equal to 1/n_features,0
kneighbors graph is constructed using k=10,0
"Make connectivity matrix symmetric, if a point is a k nearest neighbor of",0
"another point, make it vice versa",0
Graph edges are weighted by applying gaussian kernel to manhattan dist.,0
"By default, gamma for rbf kernel is equal to 1/n_features but may",0
get better results if gamma is tuned.,0
Define graph density for an observation to be sum of weights for all,0
edges to the node representing the datapoint.  Normalize sum weights,0
by total number of neighbors.,0
"If a neighbor has already been sampled, reduce the graph density",0
for its direct neighbors to promote diversity.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Update observed reward and arm probabilities,0
Sample an arm,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
y only used for determining how many clusters there should be,0
probably not practical to assume we know # of classes before hand,0
should also probably scale with dimensionality of data,0
Probably okay to always use MiniBatchKMeans,0
Should standardize data before clustering,0
Can cluster on standardized data but train on raw features if desired,0
TODO(lishal): have MarginSampler and this share margin function,1
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Probably okay to always use MiniBatchKMeans,0
Should standardize data before clustering,0
Can cluster on standardized data but train on raw features if desired,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Update min_distances for all examples given new cluster center.,0
Assumes that the transform function takes in original data and not,0
flattened data.,0
Initialize centers with a randomly selected datapoint,0
New examples should not be in already selected since those points,0
should have min_distance of zero to a cluster center.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
This is uniform given the remaining pool but biased wrt the entire pool.,0
sample = [i for i in range(self.X.shape[0]) if i not in already_selected],0
return sample[0:N],0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Fields for hierarchical clustering AL,0
Setting parent and storing nodes in dict for fast access,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Connect to database DB_NAME as USER and initialize tables,0
Populate Info table,0
Populate Category table,0
"# For now, we have a predefined list of species we expect to see in the camera trap database (e.g. maybe from a quick look through the images)",0
# TODO: allow user to update the class list through the labeling tool UI as they see different species,1
Populate Image and Detection tables,0
Image entry data,0
Detection entry data,0
Connect to database DB_NAME as USER and initialize tables,0
Populate Info table,0
Populate Category table,0
# Create category records from species present in the COCO camera trap classification dataset (COCO classes minus any excluded classes),0
# TODO: allow user to update the class list through the labeling tool UI as they see different species,1
Populate Image and Detection tables,0
"with open(os.path.join(args.crop_dir,'crops.json'), 'r') as infile:",0
crops_json = json.load(infile),0
counter = 0,0
timer = time.time(),0
num_detections = len(crops_json),0
for detectionid in crops_json:,0
counter += 1,0
detection_data = crops_json[detectionid],0
# Image entry data,0
existing_image_entries = Image.select().where((Image.file_name == detection_data['file_name'])),0
try:,0
existing_image_entry = existing_image_entries.get(),0
except:,0
"image_entry = Image.create(id=detectionid, file_name=detection_data['file_name'], width=detection_data['width'], height=detection_data['height'], grayscale=detection_data['grayscale'],",0
"source_file_name=detection_data['source_file_name'], relative_size=detection_data['relative_size'],",0
"seq_id=detection_data['seq_id'], seq_num_frames=detection_data['seq_num_frames'], frame_num=detection_data['frame_num'])",0
image_entry.save(),0
# Detection entry data,0
"detection_entry = Detection.create(id=detectionid, image=detectionid, bbox_confidence=detection_data['bbox_confidence'],",0
"bbox_X1=detection_data['bbox_X1'], bbox_Y1=detection_data['bbox_Y1'], bbox_X2=detection_data['bbox_X2'], bbox_Y2=detection_data['bbox_Y2'],",0
kind=DetectionKind.ModelDetection.value),0
detection_entry.save(),0
if counter%100 == 0:,0
"print('Updated database with Image and Detection table entries for %d out of %d crops in %0.2f seconds'%(counter, num_detections, time.time() - timer))",0
# data related to original image,0
Get class names from .txt list,0
Initialize Oracle table,0
Map filenames to classes (NOTE: we assume a single image does not contain more than one class),0
"For each detection, use source image path to get class",0
TODO update: Assumes that crops have already,1
been generated for the images using make_active_learning_classification_dataset.py. The created DB contains tables:,0
- info: information about the dataset,0
- image: images present in the dataset,0
- detections: crops of images with detections with confidence greater than a specified threshold,0
Initialize Database,0
# database connection credentials,0
HOST = 'localhost',0
PORT = 5432,0
"# first, make sure the (user, password) has been created",0
"# sudo -u postgres psql -c ""CREATE USER <db_user> WITH PASSWORD <db_password>;""",0
"# sudo -u postgres psql -c ""CREATE DATABASE <db_name> WITH OWNER <db_user> CONNECTION LIMIT -1;""",0
"# sudo -u postgres psql -c ""GRANT CONNECT ON DATABASE <db_name> TO <db_user>;""",0
"# sudo -u postgres psql -d <db_name> -c ""CREATE EXTENSION IF NOT EXISTS \""uuid-ossp\"";""",0
# Try to connect as USER to database DB_NAME through peewee,0
Populate Tables,0
# create Info table,0
# get class names for Category table,0
Faster anD available in Python 3.5 and above,0
# iterate through images in each class folder,0
killing this process after over 38 hours adding over 500k white-tailed deer crops from emammal,0
resuming for remaining classes,0
# get cropped image data for Image table,0
"if mean of each channel is about the same, image is likely grayscale",0
# still have no info on these:,0
seq_id = CharField(null= True)                # sequence identifier for the original image,0
seq_num_frames = IntegerField(null = True)    # number of frames in sequence,0
frame_num = IntegerField(null = True)         # which frame number in sequence,0
location = CharField(null = True)             # location of camera trap,0
datetime = DateTimeField(null = True),0
# store info about the detection corresponding to this image,0
# store info about the true labels for the detection,0
#  - for pretrain dataset this is the same as the detection_category if the detection categories,0
print(classes),0
Connect to database and sample a dataset,0
Load the saved embedding model from the checkpoint,0
Update the dataset embedding,0
save the images,0
save the features,0
"with open(os.path.join(args.output_dir, 'lastlayer_features.mat'), 'wb') as f:",0
"pickle.dump(sample_features, f)",0
"with open(os.path.join(args.output_dir, 'labels.mat'), 'wb') as f:",0
"pickle.dump(sample_labels, f)",0
"parser.add_argument('--db_name', default='missouricameratraps', type=str, help='Name of the training (target) data Postgres DB.')",0
"parser.add_argument('--db_user', default='user', type=str, help='Name of the user accessing the Postgres DB.')",0
"parser.add_argument('--db_password', default='password', type=str, help='Password of the user accessing the Postgres DB.')",0
"parser.add_argument('--base_model', type=str, help='Path to latest embedding model checkpoint.')",0
"parser.add_argument('--output_dir', type=str, help='Output directory for subset of crops')",0
Add json entry for this crop,0
Copy file for this crop to subset dataset crop dir,0
Copy file for its full-size source image to subset dataset image dir,0
Write crops.json to subset dataset crop dir,0
store info about the crops produced in a JSON file,0
------------------------------------------------------------------------------------------------------------#,0
COMMENT OUT IF NOT USING A SPECIFIC PROJECT WITHIN ROBERT LONG EMAMMAL DATASET,0
------------------------------------------------------------------------------------------------------------#,0
get some information about the source image,0
------------------------------------------------------------------------------------------------------------#,0
NOTE: EDIT THIS SECTION BASED ON DATASET SOURCE,0
get info about sequence the source image belongs to from path and directory,0
# missouricameratraps:,0
imgframenum = int(os.path.basename(imgfile).split('.JPG')[0].split('_')[-1]),0
imgseqid = int(os.path.split(os.path.dirname(imgfile))[-1]),0
"imgseqnumframes = len([name for name in os.listdir(os.path.dirname(imgfile)) if os.path.isfile(os.path.join(os.path.dirname(imgfile), name))])",0
# emammal:,0
------------------------------------------------------------------------------------------------------------#,0
utility function,0
compute output,0
measure accuracy and record loss,0
switch to train mode,0
measure accuracy and record loss,0
compute output,0
measure accuracy and record loss,0
switch to evaluate mode,0
compute output,0
switch to evaluate mode,0
compute output,0
switch to evaluate mode,0
"print(self.labels_set, self.n_classes)",0
Add all negatives for all positive pairs,0
print(triplets.shape[0]),0
constructor,0
update embedding values after a finetuning,0
select either the default or active pools,0
gather test set,0
gather train set,0
finetune the embedding model over the labeled pool,0
a utility function for saving the snapshot,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Variables for the hierarchical cluster,0
Variables for the AL algorithm,0
Data variables,0
"connectivity = kneighbors_graph(self.transformed_X,max_features)",0
Fit cluster and update cluster variables,0
The sklearn hierarchical clustering algo numbers leaves which correspond,0
to actual datapoints 0 to n_points - 1 and all internal nodes have,0
ids greater than n_points - 1 with the root having the highest node id,0
"If no labels have been observed, simply return uniform distribution",0
"If no observations, return worst possible upper lower bounds",0
Loop through generations from bottom to top,0
Update admissible labels for node,0
Calculate score,0
Determine if node should be split,0
Make sure label set for node so that we can flow to children,0
if necessary,0
Only split if all ancestors are admissible nodes,0
This is part  of definition of admissible pruning,0
Check that pruning covers all leave nodes,0
Fill in labels,0
Observe labels for previously recommended batches,0
TODO(lishal): implement multiple selection methods,1
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
A list of initialized samplers is allowed as an input because,0
"for AL_methods that search over different mixtures, may want mixtures to",0
have shared AL_methods so that initialization is only performed once for,0
computation intensive methods like HierarchicalClusteringAL and,0
states are shared between mixtures.,0
"If initialized samplers are not provided, initialize them ourselves.",0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copy these objects to make sure they are not modified while simulating,0
trajectories as they are used later by the main run_experiment script.,0
Assumes that model has already by fit using all labeled data so,0
the probabilities can be used immediately to hallucinate labels,0
All models need to have predict_proba method,0
Hallucinate labels for selected datapoints to be label,0
using class probabilities from model,0
"Not saving already_selected here, if saving then should sort",0
only for the input to fit but preserve ordering of indices in,0
already_selected,0
Useful to know how accuracy compares for model trained on hallucinated,0
labels vs trained on true labels.  But can remove this train to speed,0
up simulations.  Won't speed up significantly since many more models,0
are being trained inside the loop above.,0
Save trajectory for reference,0
Delete created copies,0
THE INPUTS CANNOT BE MODIFIED SO WE MAKE COPIES FOR THE CHECK LATER,0
Should check model but kernel_svm does not have coef_ so need better,0
handling here,0
Make sure that model object fed in did not change during simulations,0
Return indices based on return type specified,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Set gamma for gaussian kernel to be equal to 1/n_features,0
kneighbors graph is constructed using k=10,0
"Make connectivity matrix symmetric, if a point is a k nearest neighbor of",0
"another point, make it vice versa",0
Graph edges are weighted by applying gaussian kernel to manhattan dist.,0
"By default, gamma for rbf kernel is equal to 1/n_features but may",0
get better results if gamma is tuned.,0
Define graph density for an observation to be sum of weights for all,0
edges to the node representing the datapoint.  Normalize sum weights,0
by total number of neighbors.,0
"If a neighbor has already been sampled, reduce the graph density",0
for its direct neighbors to promote diversity.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Update observed reward and arm probabilities,0
Sample an arm,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
y only used for determining how many clusters there should be,0
probably not practical to assume we know # of classes before hand,0
should also probably scale with dimensionality of data,0
Probably okay to always use MiniBatchKMeans,0
Should standardize data before clustering,0
Can cluster on standardized data but train on raw features if desired,0
TODO(lishal): have MarginSampler and this share margin function,1
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Probably okay to always use MiniBatchKMeans,0
Should standardize data before clustering,0
Can cluster on standardized data but train on raw features if desired,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Update min_distances for all examples given new cluster center.,0
Assumes that the transform function takes in original data and not,0
flattened data.,0
Initialize centers with a randomly selected datapoint,0
New examples should not be in already selected since those points,0
should have min_distance of zero to a cluster center.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
This is uniform given the remaining pool but biased wrt the entire pool.,0
sample = [i for i in range(self.X.shape[0]) if i not in already_selected],0
return sample[0:N],0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Fields for hierarchical clustering AL,0
Setting parent and storing nodes in dict for fast access,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Faster and available in Python 3.5 and above,0
transform_list.append(CenterCrop((processed_size))),0
transform_list.append(Lambda(lambda X: normalize(X))),0
"print(self.labels_set, self.n_classes)",0
"from PyQt5 import QtCore, QtWidgets,QtGui",0
from collections import deque,0
from peewee import *,0
from UIComponents.Tag import Tag,0
policy.setHeightForWidth(True),0
"print(self.tab1.parentWidget(),self)",0
self.tab4.add.clicked.connect(self.addSpecies),0
self.tab4.update.clicked.connect(self.updateSpecies),0
checkpoint= load_checkpoint('../merge/triplet_model_0054.tar'),0
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))",0
"embedding_net = EmbeddingNet(checkpoint['arch'], checkpoint['feat_dim'])",0
if checkpoint['loss_type'].lower()=='center':,0
"model = torch.nn.DataParallel(ClassificationNet(embedding_net, n_classes=14)).cuda()",0
else:,0
model= torch.nn.DataParallel(embedding_net).cuda(),0
model.load_state_dict(checkpoint['state_dict']),0
self.parentWidget().progressBar.setMaximum(len(run_dataset)//2048),0
"e=Engine(model,None,None, verbose=True,progressBar= self.parentWidget().progressBar)",0
label = [ x[1] for x in run_dataset.samples],0
"print(indices,selected_set)",0
print(query.sql()),0
src.delete().where(src.image_id<<rList)),0
"det= UserDetection.create(category_id=0, id=str(index+label[1][2]),image_id=final[0], bbox_X=label[1][0], bbox_Y=label[1][1], bbox_W=label[1][2], bbox_H=label[1][3])",0
for x in self.tab1.grid.tags:,0
x.delete_instance(),0
db.create_tables([Detection]),0
This is simply to show the bar,0
"p = Process(target=ex.active, args=())",0
p.start(),0
p.join(),0
ex.active(),0
ex.centralWidget().setCurrentIndex(1),0
main(),0
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))",0
print(row),0
"unq_id= ""crops_""+str(uuid.uuid1())",0
"print(line,imageWidth,imageHeight)",0
"print(""%s,%s,%s,%0.6f,%0.6f,%0.6f,%0.6f,%0.6f""%(line[0], line[1],line[2],float(line[3]),topRel,leftRel,bottomRel,rightRel))",0
if not os.path.exists(dest):,0
os.mkdir(dest),0
raise,0
out.close(),0
"print length,(i-1)*length,i*length",0
matplotlib.use('Agg'),0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
"conf.reset()""""""",0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
conf.reset(),0
for clf in classifiers:,0
"clf.fit(X_train, y_train)",0
preds= clf.predict_proba(X),0
uncertainty+= preds.max(axis=1),0
print(uncertainty[ind]),0
print(uncertainty[ind]),0
true_labels = y[unlabeled_indices],0
select up to 5 digit examples that the classifier is most uncertain about,0
remember best acc@1 and save checkpoint,0
"completeClassificationLoop(run_dataset, model,num_classes)",0
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)",0
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd),0
labels = db.labels_,0
"mapp=(find_probablemap(label,labels, K=args.K))",0
"print(""Clusters"")",0
"for i,x in enumerate(labels):",0
labels[i]= mapp[x],0
print(np.sum(labels == label)/labels.size),0
"print(""Confidence Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning(embd, label, idx)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(""Entropy Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning_entropy(embd, label, idx)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(idx,idx.shape)",0
for i in idx:,0
print(paths[i]),0
"print(""Silohette active learning"")",0
"idx= active_learning2(embd, 1000, args.num_clusters)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
embd= reduce_dimensionality(embd)#[0:10000]),0
labels= labels[0:10000],0
label= label[0:10000],0
paths= paths[0:10000],0
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])",0
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])",0
plt.show(),0
"np.save(args.name_prefix+""_embeddings.npy"",embd)",0
"np.save(args.name_prefix+""_labels.npy"",label)",0
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")",0
del output,0
define loss function (criterion) and optimizer,0
"optimizer = torch.optim.Adam(params, lr=args.lr, weight_decay= args.weight_decay)",0
optimizer.load_state_dict(checkpoint['optimizer']),0
train for one epoch,0
evaluate on validation set,0
matplotlib.use('Agg'),0
selected_set.add(rand_ind[i]),0
"embedding_net = EmbeddingNet('resnet50', 256, True)",0
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)",0
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)",0
print('Embedding Done'),0
sys.stdout.flush(),0
Random examples to start,0
"random_ids = noveltySamples(unlabeled_dataset.em, unlabeled_dataset.getIDs(), 1000)",0
print(random_ids),0
Move Records,0
Finetune the embedding model,0
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)",0
unlabeled_dataset.updateEmbedding(model),0
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})",0
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()",0
matplotlib.use('Agg'),0
"uncertainty= np.apply_along_axis(stats.entropy,1,probs) * (1 - probs.max(axis=1))",0
"copy(paths[srt[i]], ""active"")",0
"plot_together( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()), preds, base_ind, dataset.getpaths(), {})",0
"return np.random.choice(range(0,prob_list[0].shape[0]), 100, replace=False).tolist()",0
selected_set.add(rand_ind[i]),0
"query= Detection.update(kind = destKind.value).where(Detection.id.in_(rList), Detection.kind == srcKind.value)",0
query.execute(),0
"embedding_net = EmbeddingNet('resnet50', 256, True)",0
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)",0
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)",0
print('Embedding Done'),0
sys.stdout.flush(),0
"plot_embedding(dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})",0
Random examples to start,0
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)",0
print(random_ids),0
Move Records,0
Finetune the embedding model,0
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)",0
unlabeled_dataset.updateEmbedding(model),0
"print(dataset.em[dataset.current_set].shape, np.asarray(dataset.getlabels()).shape, len(dataset.getpaths()))",0
"plot_embedding( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})",0
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})",0
train_eval_classifier(),0
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()",0
"names = [""Linear SVM"", ""RBF SVM"", ""Random Forest"", ""Neural Net"", ""Naive Bayes""]",0
"classifiers = [SVC(kernel=""linear"", C=0.025, probability= True, class_weight='balanced'),",0
"SVC(gamma=2, C=1, probability= True, class_weight='balanced'),",0
"RandomForestClassifier(max_depth=None, n_estimators=100, class_weight='balanced'),",0
"MLPClassifier(alpha=1),",0
GaussianNB()],0
estimators= [],0
"for name, clf in zip(names, classifiers):",0
"estimators.append((name, clf))",0
"eclf1 = VotingClassifier(estimators= estimators, voting='hard')",0
"eclf2 = VotingClassifier(estimators= estimators, voting='soft')",0
"names.append(""ensemble hard"")",0
classifiers.append(eclf1),0
"names.append(""ensemble soft"")",0
classifiers.append(eclf2),0
dataset.image_mode(),0
dataset.updateEmbedding(model),0
y_pred= clf.predict(X_test),0
"print(confusion_matrix(y_test, y_pred))",0
paths= dataset.getpaths(),0
"for i, (yp, yt) in enumerate(zip(y_pred, y_test)):",0
if yp != yt:,0
"copy(paths[i],""mistakes"")",0
"print(yt, yp, paths[i],i)",0
"clf_output= clf_e.embedding(eval_loader, dim=48)",0
"self.train_transform = transforms.Compose([Resize(raw_size), CenterCrop((processed_size)), ToTensor(), Normalize(mean, std)])",0
"print(self.labels_set, self.n_classes)",0
"print(line,imageWidth,imageHeight)",0
"print length,(i-1)*length,i*length",0
#########################################################,0
## Configuration,0
#########################################################,0
## The actual code,0
Check arguments,0
Create output directories,0
Padding around the detected objects when cropping,0
1.3 for the cropping during test time and 1.3 for,0
the context that the CNN requires in the left-over,0
image,0
Load a (frozen) Tensorflow model into memory.,0
## Preparations: get all the output tensors,0
The following processing is only for single image,0
Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.,0
Follow the convention by adding back the batch dimension,0
For all images in the image directoryig,0
Load image,0
Run inference,0
"all outputs are float32 numpy arrays, so convert types as appropriate",0
Add detections to the collection,0
Get info about the image,0
Select detections with a confidence larger than DETECTION_CONFIDENCE,0
Get these boxes and convert normalized coordinates to pixel coordinates,0
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes",0
"However, we need to make sure that it box coordinates are still within the image",0
"For each detected bounding box with high confidence, we will",0
crop the image to the padded box and save it,0
generate a unique identifier for the detection,0
"bbox is the detected box, crop_box the padded / enlarged box",0
Add numbering to the original file name if there are multiple boxes,0
The absolute file path where we will store the image,0
"if COCO_OUTPUT_DIR is set, then we will only use the shape",0
of cropped_img in the following code. So instead of reading,0
cropped_img = np.array(Image.open(out_file)),0
we can speed everything up by reading only the size of the image,0
matplotlib.use('Agg'),0
conf= ConfusionMatrix(24),0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
"conf.reset()""""""",0
define loss function (criterion) and optimizer,0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
conf.reset(),0
for clf in classifiers:,0
"clf.fit(X_train, y_train)",0
preds= clf.predict_proba(X),0
uncertainty+= preds.max(axis=1),0
print(uncertainty[ind]),0
print(uncertainty[ind]),0
true_labels = y[unlabeled_indices],0
select up to 5 digit examples that the classifier is most uncertain about,0
"print(indices,selected_set)",0
print(query.sql()),0
remember best acc@1 and save checkpoint,0
"completeClassificationLoop(run_dataset, model,num_classes)",0
"embd, label, paths = extract_embeddings(run_loader, model)",0
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)",0
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd),0
labels = db.labels_,0
"mapp=(find_probablemap(label,labels, K=args.K))",0
"print(""Clusters"")",0
"for i,x in enumerate(labels):",0
labels[i]= mapp[x],0
print(np.sum(labels == label)/labels.size),0
"print(""Confidence Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning(embd, label, idx)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(""Entropy Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning_entropy(embd, label, idx)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"new_selected= selectSamples(embd,paths,3000)",0
"print(idx,idx.shape)",0
for i in idx:,0
print(paths[i]),0
"print(""Silohette active learning"")",0
"idx= active_learning2(embd, 1000, args.num_clusters)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(""Random"")",0
"idx = np.random.choice(np.arange(len(paths)), 1000, replace=False)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
embd= reduce_dimensionality(embd)#[0:10000]),0
labels= labels[0:10000],0
label= label[0:10000],0
paths= paths[0:10000],0
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])",0
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])",0
plt.show(),0
"np.save(args.name_prefix+""_embeddings.npy"",embd)",0
"np.save(args.name_prefix+""_labels.npy"",label)",0
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")",0
combo.setEnabled(not finalized),0
print(self.bbox),0
self.addWidget(self.child),0
Moving container with arrows,0
Left - Bottom,0
Right-Bottom,0
Left-Top,0
Right-Top,0
Left - Bottom,0
Right - Bottom,0
Left - Top,0
Right - Top,0
check cursor horizontal position,0
check cursor vertical position,0
self.resizeEvent=self.onResize,0
"print(""Parent"", parent, parent.width(), parent.height())",0
"self.setGeometry(0,0,410,307)",0
"print(w,h,""w,h"")",0
"print(""final"",tag.getFinal())",0
pass,0
"self.tags.append(TContainer(self,Category.get(-1),[0,0,0.1,0.1],True, Qt.red))",0
label= CharField(),0
fullname=str(self.model),0
"self.name= (fullname[fullname.find("":"")+2:fullname.find("">"")].strip()+'_set').lower()",0
"print(""Parent"", self.parentWidget().width(), self.parentWidget().height())",0
"print(self.model,self.name,query.sql())",0
self.tab4.speciesList.setModel(species),0
"self.tab4.speciesList.setRowHidden(len(species.stringList())-1, True)",0
self.speciesList.itemChanged.connect(self.itemChanged),0
#####,0
,0
video_utils.py,0
,0
"Utilities for splitting, rendering, and assembling videos.",0
,0
#####,0
"%% Constants, imports, environment",0
from ai4eutils,0
%% Path utilities,0
%% Function for rendering frames to video and vice-versa,0
http://tsaith.github.io/combine-images-into-a-video-with-python-3-and-opencv-3.html,0
Determine the width and height from the first image,0
Define the codec and create VideoWriter object,0
"If we're not over-writing, check whether all frame images already exist",0
"OpenCV seems to over-report the number of frames by 1 in some cases, or fails",0
"to read the last frame; either way, I'm allowing one missing frame.",0
"print(""Rendering video {}, couldn't find frame {}"".format(",0
"input_video_file,missing_frame_number))",0
...if we need to check whether to skip this video entirely,0
"for frame_number in tqdm(range(0,n_frames)):",0
print('Skipping frame {}'.format(frame_filename)),0
Recursively enumerate video files,0
Create the target output folder,0
Render frames,0
input_video_file = input_fn_absolute; output_folder = output_folder_video,0
For each video,0
,0
input_fn_relative = input_files_relative_paths[0],0
"process_detection_with_options = partial(process_detection, options=options)",0
zero-indexed,0
Load results,0
# Break into videos,0
im = images[0],0
# For each video...,0
video_name = list(video_to_frames.keys())[0],0
frame = frames[0],0
At most one detection for each category for the whole video,0
category_id = list(detection_categories.keys())[0],0
Find the nth-highest-confidence video to choose a confidence value,0
Prepare the output representation for this video,0
'max_detection_conf' is no longer included in output files by default,0
...for each video,0
Write the output file,0
%% Test driver,0
%% Constants,0
%% Split videos into frames,0
"%% List image files, break into folders",0
Find unique folders,0
fn = frame_files[0],0
%% Load detector output,0
%% Render detector frames,0
folder = list(folders)[0],0
d = detection_results_this_folder[0],0
...for each file in this folder,0
...for each folder,0
%% Render output videos,0
folder = list(folders)[0],0
...for each video,0
,0
run_inference_with_yolov5_val.py,0
,0
Runs a folder of images through MegaDetector (or another YOLOv5 model) with YOLOv5's,0
"val.py, converting the output to the standard MD format.  The main goal is to leverage",0
YOLO's test-time augmentation tools.,0
,0
"YOLOv5's val.py uses each file's base name as a unique identifier, which doesn't work",0
when you have typical camera trap images like:,0
,0
a/b/c/RECONYX0001.JPG,0
d/e/f/RECONYX0001.JPG,0
,0
...so this script jumps through a bunch of hoops to put a symlinks in a flat,0
"folder, run YOLOv5 on that folder, and map the results back to the real files.",0
,0
"Currently requires the user to supply the path where a working YOLOv5 install lives,",0
and assumes that the current conda environment is all set up for YOLOv5.,0
,0
TODO:,1
,0
* Figure out what happens when images are corrupted... right now this is the #1,0
"reason not to use this script, it may be the case that corrupted images look the",0
same as empty images.,0
,0
* Multiple GPU support,0
,0
* Checkpointing,0
,0
* Windows support (I have no idea what all the symlink operations will do on Windows),0
,0
"* Support alternative class names at the command line (currently defaults to MD classes,",0
though other class names can be supplied programmatically),0
,0
%% Imports,0
%% Options class,0
# Required ##,0
# Optional ##,0
%% Main function,0
#%% Path handling,0
#%% Enumerate images,0
#%% Create symlinks to give a unique ID to each image,0
i_image = 0; image_fn = image_files_absolute[i_image],0
...for each image,0
#%% Create the dataset file,0
Category IDs need to be continuous integers starting at 0,0
#%% Prepare YOLOv5 command,0
#%% Run YOLOv5 command,0
#%% Convert results to MD format,0
"We'll use the absolute path as a relative path, and pass '/'",0
as the base path in this case.,0
#%% Clean up,0
...def run_inference_with_yolo_val(),0
%% Command-line driver,0
%% Scrap,0
%% Test driver (folder),0
%% Test driver (file),0
%% Preview results,0
options.sample_seed = 0,0
...for each prediction file,0
%% Compare results,0
Choose all pairwise combinations of the files in [filenames],0
"%% Constants, imports, environment",0
from multiprocessing.pool import ThreadPool as workerpool,0
Number of images to pre-fetch,0
Useful hack to force CPU inference.,1
,0
"Need to do this before any PT/TF imports, which happen when we import",0
from run_detector.,0
Numpy FutureWarnings from tensorflow import,0
%% Support functions for multiprocessing,0
TODO,1
,0
The queue system is a little more elegant if we start one thread for reading and one,1
"for processing, and this works fine on Windows, but because we import TF at module load,",0
"CUDA will only work in the main process, so currently the consumer function runs here.",0
,0
"To enable proper multi-GPU support, we may need to move the TF import to a separate module",0
that isn't loaded until very close to where inference actually happens.,0
%% Other support funtions,0
%% Image processing functions,0
%% Main function,0
Handle the case where image_file_names is not yet actually a list,0
"Find the images to score; images can be a directory, may need to recurse",0
A json list of image paths,0
A single image file,0
Load the detector,0
Does not count those already processed,0
Will not add additional entries not in the starter checkpoint,0
Write a checkpoint if necessary,0
"Back up any previous checkpoints, to protect against crashes while we're writing",0
the checkpoint file.,0
Write the new checkpoint,0
Remove the backup checkpoint if it exists,0
...if it's time to make a checkpoint,0
"When using multiprocessing, let the workers load the model",0
"Results may have been modified in place, but we also return it for",0
backwards-compatibility.,0
The typical case: we need to build the 'info' struct,0
"If the caller supplied the entire ""info"" struct",0
"The 'max_detection_conf' field used to be included by default, and it caused all kinds",0
"of headaches, so it's no longer included unless the user explicitly requests it.",0
%% Interactive driver,0
%%,0
%% Command-line driver,0
This is an experimental hack to allow the use of non-MD YOLOv5 models through,1
the same infrastructure; it disables the code that enforces MDv5-like class lists.,0
Load the checkpoint if available,0
,0
Relative file names are only output at the end; all file paths in the checkpoint are,0
still full paths.,0
"Find the images to score; images can be a directory, may need to recurse",0
A json list of image paths,0
A single image file,0
Test that we can write to the output_file's dir if checkpointing requested,0
"Don't overwrite existing checkpoint files, this is a sure-fire way to eventually",0
erase someone's checkpoint.,0
"Commenting this out for now... the scenario where we are resuming from a checkpoint,",0
then immediately overwrite that checkpoint with empty data is higher-risk than the,0
annoyance of crashing a few minutes after starting a job.,0
Confirm that we can write to the checkpoint path; this avoids issues where,0
we crash after several thousand images.,0
%% Imports,0
import pre- and post-processing functions from the YOLOv5 repo,0
scale_coords() became scale_boxes() in later YOLOv5 versions,0
%% Classes,0
padded resize,0
"Image size can be an int (which translates to a square target size) or (h,w)",0
...if the caller has specified an image size,0
NMS,0
"As of v1.13.0.dev20220824, nms is not implemented for MPS.",0
,0
Send predication back to the CPU to fix.,0
format detections/bounding boxes,0
"This is a loop over detection batches, which will always be length 1 in our case,",0
since we're not doing batch inference.,0
Rescale boxes from img_size to im0 size,0
"normalized center-x, center-y, width and height",0
"MegaDetector output format's categories start at 1, but the MD",0
model's categories start at 0.,0
...for each detection in this batch,0
...if this is a non-empty batch,0
...for each detection batch,0
...try,0
for testing,0
"MegaDetector was trained with batch size of 1, and the resizing function is a part",0
of the inference graph,0
"change from [y1, x1, y2, x2] to [x1, y1, width, height]",0
convert numpy floats to Python floats,0
need to change the above line to the following if supporting a batch size > 1 and resizing to the same size,0
"np_images = [np.asarray(image, np.uint8) for image in images]",0
"images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)",0
performs inference,0
our batch size is 1; need to loop the batch dim if supporting batch size > 1,0
"%% Constants, imports, environment",0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Numpy FutureWarnings from tensorflow import,0
Useful hack to force CPU inference,1
os.environ['CUDA_VISIBLE_DEVICES'] = '-1',0
An enumeration of failure reasons,0
Number of decimal places to round to for confidence and bbox coordinates,0
Label mapping for MegaDetector,0
Should we allow classes that don't look anything like the MegaDetector classes?,0
"Each version of the detector is associated with some ""typical"" values",0
"that are included in output files, so that downstream applications can",0
use them as defaults.,0
%% Classes,0
Stick this into filenames before the extension for the rendered result,0
%% Utility functions,0
mps backend only available in torch >= 1.12.0,0
%% Main function,0
Dictionary mapping output file names to a collision-avoidance count.,0
,0
"Since we'll be writing a bunch of files to the same folder, we rename",0
as necessary to avoid collisions.,0
...def input_file_to_detection_file(),0
Image is modified in place,0
...for each image,0
...def load_and_run_detector(),0
%% Command-line driver,0
Must specify either an image file or a directory,0
"but for a single image, args.image_dir is also None",0
%% Interactive driver,0
%%,0
#####,0
,0
process_video.py,0
,0
"Split a video (or folder of videos) into frames, run the frames through run_detector_batch.py,",0
and optionally stitch together results into a new video with detection boxes.,0
,0
TODO: allow video rendering when processing a whole folder,1
TODO: allow video rendering from existing results,1
,0
#####,0
"%% Constants, imports, environment",0
Only relevant if render_output_video is True,0
Folder to use for extracted frames,0
Folder to use for rendered frames (if rendering output video),0
Should we render a video with detection boxes?,0
,0
"Only supported when processing a single video, not a folder.",0
"If we are rendering boxes to a new video, should we keep the temporary",0
rendered frames?,0
Should we keep the extracted frames?,0
%% Main functions,0
"TODO: keep track of whether we created this folder, delete if we're deleting the extracted",0
"frames and we created the folder, and the output files aren't in the same folder.  For now,",0
we're just deleting the extracted frames and leaving the empty folder around in this case.,0
Render detections to images,0
"TODO: keep track of whether we created this folder, delete if we're deleting the rendered",0
"frames and we created the folder, and the output files aren't in the same folder.  For now,",0
we're just deleting the rendered frames and leaving the empty folder around in this case.,0
Combine into a video,0
Delete the temporary directory we used for detection images,0
shutil.rmtree(rendering_output_dir),0
(Optionally) delete the frames on which we ran MegaDetector,0
shutil.rmtree(frame_output_folder),0
...process_video(),0
# Validate options,0
# Split every video into frames,0
# Run MegaDetector on the extracted frames,0
# (Optionally) delete the frames on which we ran MegaDetector,0
shutil.rmtree(frame_output_folder),0
# Convert frame-level results to video-level results,0
...process_video_folder(),0
%% Interactive driver,0
%% Process a folder of videos,0
import clipboard; clipboard.copy(cmd),0
%% Process a single video,0
import clipboard; clipboard.copy(cmd),0
"%% Render a folder of videos, one file at a time",0
import clipboard; clipboard.copy(s),0
%% Command-line driver,0
Lint as: python3,0
Copyright 2020 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TPU is automatically inferred if tpu_name is None and,0
we are running under cloud ai-platform.,0
,0
copy_checkpoints.py,0
,0
Run this script with specified source_dir and target_dir while the model is training to make a copy,0
of every checkpoint (checkpoints are kept once an hour by default and is difficult to adjust),0
,0
do not copy event or evaluation results,0
input validation,0
plot the images,0
adjust the figure,0
"read in dataset CSV and create merged (dataset, location) col",0
map label to label_index,0
load the splits,0
only weight the training set by detection confidence,0
TODO: consider weighting val and test set as well,1
isotonic regression calibration of MegaDetector confidence,0
treat each split separately,0
new_weight[i] = confidence[i] * (n / c) / total_confidence(i's label),0
- n = # examples in split (weighted by confidence); c = # labels,0
- weight allocated to each label is n/c,0
"- within each label, weigh each example proportional to confidence",0
- new weights sum to n,0
error checking,0
"maps output label name to set of (dataset, dataset_label) tuples",0
find which other label (label_b) has intersection,0
input validation,0
create label index JSON,0
look into sklearn.preprocessing.MultiLabelBinarizer,0
Note: JSON always saves keys as strings!,0
"True for ground truth, False for MegaDetector",0
always save as .jpg for consistency,0
get bounding boxes,0
convert from category ID to category name,0
"check if crops are already downloaded, and ignore bboxes below the",0
confidence threshold,0
assign all images without location info to 'unknown_location',0
remove images from labels that have fewer than min_locs locations,0
merge dataset and location into a single string '<dataset>/<location>',0
"create DataFrame of counts. rows = locations, columns = labels",0
label_count: label => number of examples,0
loc_count: label => number of locs containing that label,0
generate a new split,0
score the split,0
SSE for # of images per label (with 2x weight),0
SSE for # of locs per label,0
label => list of datasets to prioritize for test and validation sets,0
"merge dataset and location into a tuple (dataset, location)",0
sorted smallest to largest,0
greedily add to test set until it has >= 15% of images,0
sort the resulting locs,0
"modify loc_to_size in place, so copy its keys before iterating",0
arguments relevant to both creating the dataset CSV and splits.json,0
arguments only relevant for creating the dataset CSV,0
arguments only relevant for creating the splits JSON,0
comment lines starting with '#' are allowed,0
,0
prepare_classification_script.py,0
,0
Notebook-y script used to prepare a series of shell commands to run a classifier,0
(other than MegaClassifier) on a MegaDetector result set.,0
,0
Differs from prepare_classification_script_mc.py only in the final class mapping step.,0
,0
%% Job options,0
%% Constants,0
"This is just passed along to the metadata in the output file, it has no impact",0
on how the classification scripts run.,0
%% Set up environment,0
commands.append('cd CameraTraps/classification\n'),0
commands.append('conda activate cameratraps-classifier\n'),0
%% Crop images,0
fn = input_files[0],0
%% Run classifier,0
fn = input_files[0],0
%% Merge classification and detection outputs,0
fn = input_files[0],0
%% Write everything out,0
input validation,0
"Evaluating with accimage is much faster than Pillow or Pillow-SIMD, but accimage",0
is Linux-only.,0
create output directory,0
override saved params with kwargs,0
"For now, we don't weight crops by detection confidence during",0
evaluation. But consider changing this.,0
"create model, compile with TorchScript if given checkpoint is not compiled",0
"verify that target names matches original ""label names"" from dataset",0
"if the dataset does not already have a 'other' category, then the",0
'other' category must come last in label_names to avoid conflicting,0
with an existing label_id,0
define loss function (criterion),0
"this file ends up being huge, so we GZIP compress it",0
double check that the accuracy metrics are computed properly,0
save the confusion matrices to .npz,0
save per-label statistics,0
set dropout and BN layers to eval mode,0
"even if batch contains sample weights, don't use them",0
Do target mapping on the outputs (unnormalized logits) instead of,0
"the normalized (softmax) probabilities, because the loss function",0
uses unnormalized logits. Summing probabilities is equivalent to,0
log-sum-exp of unnormalized logits.,0
"a confusion matrix C is such that C[i,j] is the # of observations known to",0
be in group i and predicted to be in group j.,0
match pytorch EfficientNet model names,0
images dataset,0
"for smaller disk / memory usage, we cache the raw JPEG bytes instead",0
of the decoded Tensor,0
convert JPEG bytes to a 3D uint8 Tensor,0
"keras EfficientNet already includes normalization from [0, 255] to [0, 1],",0
so we don't need to do that here,0
labels dataset,0
img_files dataset,0
weights dataset,0
define the transforms,0
efficientnet data preprocessing:,0
- train:,0
"1) random crop: aspect_ratio_range=(0.75, 1.33), area_range=(0.08, 1.0)",0
2) bicubic resize to img_size,0
3) random horizontal flip,0
- test:,0
1) center crop,0
2) bicubic resize to img_size,0
weights sums to:,0
- if weight_by_detection_conf: (# images in split - conf delta),0
- otherwise: (# images in split),0
"freeze the base model's weights, including BatchNorm statistics",0
https://www.tensorflow.org/guide/keras/transfer_learning#fine-tuning,0
rebuild output,0
input validation,0
set seed,0
create logdir and save params,0
create dataloaders and log the index_to_label mapping,0
define loss function and optimizer,0
using EfficientNet training defaults,0
- batch norm momentum: 0.99,0
"- optimizer: RMSProp, decay 0.9 and momentum 0.9",0
- epochs: 350,0
"- learning rate: 0.256, decays by 0.97 every 2.4 epochs",0
- weight decay: 1e-5,0
TODO: change weighted to False if oversampling minority classes,1
stop training after 8 epochs without improvement,0
log metrics,0
log confusion matrix,0
log tp/fp/fn images,0
"tf.summary.image requires input of shape [N, H, W, C]",0
false positive for top3_pred[0],0
false negative for label,0
"if evaluating or finetuning, set dropout & BN layers to eval mode",0
"for each label, track 5 most-confident and least-confident examples",0
"even if batch contains sample weights, don't use them",0
we do not track L2-regularization loss in the loss metric,0
This dictionary will get written out at the end of this process; store,0
diagnostic variables here,0
error checking,0
refresh detection cache,0
save log of bad images,0
cache of Detector outputs: dataset name => {img_path => detection_dict},0
img_path: <dataset-name>/<img-filename>,0
get SAS URL for images container,0
strip image paths of dataset name,0
save list of dataset names and task IDs for resuming,0
complete task name: 'detect_for_classifier_caltech_20200722_110816_task01',0
HACK! Sleep for 10s between task submissions in the hopes that it,1
"decreases the chance of backend JSON ""database"" corruption",0
task still running => continue,0
"task finished successfully, save response to disk",0
error checking before we download and crop any images,0
convert from category ID to category name,0
we need the datasets table for getting SAS keys,0
"we already did all error checking above, so we don't do any here",0
get ContainerClient,0
get bounding boxes,0
we must include the dataset <ds> in <crop_path_template> because,0
'{img_path}' actually gets populated with <img_file> in,0
load_and_crop(),0
"get the image, either from disk or from Blob Storage",0
inelegant way to close the container_clients,0
,0
prepare_classification_script_mc.py,0
,0
Notebook-y script used to prepare a series of shell commands to run MegaClassifier,0
on a MegaDetector result set.,0
,0
Differs from prepare_classification_script.py only in the final class mapping step.,0
,0
%% Job options,0
%% Constants,0
"This is just passed along to the metadata in the output file, it has no impact",0
on how the classification scripts run.,0
%% Set up environment,0
commands.append('cd CameraTraps/classification\n'),0
commands.append('conda activate cameratraps-classifier\n'),0
%% Crop images,0
fn = input_files[0],0
%% Run classifier,0
fn = input_files[0],0
%% Remap classifier outputs,0
fn = input_files[0],0
%% Merge classification and detection outputs,0
fn = input_files[0],0
%% Write everything out,0
mean/std values from https://pytorch.org/docs/stable/torchvision/models.html,0
define the transforms,0
resizes smaller edge to img_size,0
weights sums to:,0
- if weight_by_detection_conf: (# images in split - conf delta),0
- otherwise: # images in split,0
for normal (non-weighted) shuffling,0
set all parameters to not require gradients except final FC layer,0
replace final fully-connected layer (which has 1000 ImageNet classes),0
"detect GPU, use all if available",0
input validation,0
set seed,0
create logdir and save params,0
create dataloaders and log the index_to_label mapping,0
create model,0
define loss function and optimizer,0
using EfficientNet training defaults,0
- batch norm momentum: 0.99,0
"- optimizer: RMSProp, decay 0.9 and momentum 0.9",0
- epochs: 350,0
"- learning rate: 0.256, decays by 0.97 every 2.4 epochs",0
- weight decay: 1e-5,0
stop training after 8 epochs without improvement,0
do a complete evaluation run,0
log metrics,0
log confusion matrix,0
log tp/fp/fn images,0
"for every image: undo normalization, clamp to [0, 1], CHW -> HWC",0
"- cannot be in-place, because the HeapItem might be in multiple heaps",0
writer.add_figure() has issues => using add_image() instead,0
"writer.add_figure(f'{label_name}/{tag}', fig, global_step=epoch)",0
false positive for top3_pred[0],0
false negative for label,0
"preds and labels both have shape [N, k]",0
"if evaluating or finetuning, set dropout and BN layers to eval mode",0
"for each label, track k_extreme most-confident and least-confident images",0
"even if batch contains sample weights, don't use them",0
lock before changing ImageFile.LOAD_TRUNCATED_IMAGES,0
input validation,0
use MegaDB to generate list of images,0
only keep images that:,0
"1) end in a supported file extension, and",0
2) actually exist in Azure Blob Storage,0
3) belong to a label with at least min_locs locations,0
write out log of images / labels that were removed,0
"save label counts, pre-subsampling",0
"save label counts, post-subsampling",0
spec_dict['taxa']: list of dict,0
[,0
"{'level': 'family', 'name': 'cervidae', 'datasets': ['idfg']},",0
"{'level': 'genus',  'name': 'meleagris'}",0
],0
"spec_dict['dataset_labels']: dict, dataset => list of dataset_label",0
{,0
"""idfg"": [""deer"", ""elk"", ""prong""],",0
"""idfg_swwlf_2019"": [""elk"", ""muledeer"", ""whitetaileddeer""]",0
},0
"maps output label name to set of (dataset, dataset_label) tuples",0
"because MegaDB is organized by dataset, we do the same",0
ds_to_labels = {,0
'dataset_name': {,0
"'dataset_label': [output_label1, output_label2]",0
},0
},0
we need the datasets table for getting full image paths,0
The line,0
"[img.class[0], seq.class[0]][0] as class",0
selects the image-level class label if available. Otherwise it selects the,0
"sequence-level class label. This line assumes the following conditions,",0
expressed in the WHERE clause:,0
- at least one of the image or sequence class label is given,0
- the image and sequence class labels are arrays with length at most 1,0
- the image class label takes priority over the sequence class label,0
,0
"In Azure Cosmos DB, if a field is not defined, then it is simply excluded",0
"from the result. For example, on the following JSON object,",0
{,0
"""dataset"": ""camera_traps"",",0
"""seq_id"": ""1234"",",0
"""location"": ""A1"",",0
"""images"": [{""file"": ""abcd.jpeg""}],",0
"""class"": [""deer""],",0
},0
"the array [img.class[0], seq.class[0]] just gives ['deer'] because",0
img.class is undefined and therefore excluded.,0
"if no path prefix, set it to the empty string '', because",0
"os.path.join('', x, '') = '{x}/'",0
result keys,0
"- already has: ['dataset', 'location', 'file', 'class', 'bbox']",0
"- add ['label'], remove ['file']",0
"if img is mislabeled, but we don't know the correct class, skip it",0
"otherwise, update the img with the correct class, but skip the",0
img if the correct class is not one we queried for,0
sort keys for determinism,0
we need the datasets table for getting SAS keys,0
strip leading '?' from SAS token,0
only check Azure Blob Storage,0
check local directory first before checking Azure Blob Storage,0
1st pass: populate label_to_locs,0
"label (tuple of str) => set of (dataset, location)",0
2nd pass: eliminate bad images,0
prioritize is a list of prioritization levels,0
number of already matching images,0
main(,0
"label_spec_json_path='idfg_classes.json',",0
"taxonomy_csv_path='../../camera-traps-private/camera_trap_taxonomy_mapping.csv',",0
"output_dir='run_idfg',",0
json_indent=4),0
recursively find all files in cropped_images_dir,0
only find crops of images from detections JSON,0
resizes smaller edge to img_size,0
"Evaluating with accimage is much faster than Pillow or Pillow-SIMD, but accimage",0
is Linux-only.,0
create dataset,0
create model,0
set dropout and BN layers to eval mode,0
load files,0
dataset => set of img_file,0
crop_path: <dataset>/<img_file>___cropXX_mdvY.Y.jpg,0
[----<img_path>----],0
error checking,0
any row with 'correct_class' should be marked 'mislabeled',0
filter to only the mislabeled rows,0
convert '\' to '/',0
verify that overlapping indices are the same,0
"""add"" any new mislabelings",0
write out results,0
error checking,0
load detections JSON,0
get detector version,0
convert from category ID to category name,0
copy keys to modify dict in-place,0
This will be removed later when we filter for animals,0
save log of bad images,0
"True for ground truth, False for MegaDetector",0
always save as .jpg for consistency,0
"we already did all error checking above, so we don't do any here",0
"get the image, either from disk or from Blob Storage",0
inelegant way to close the container_client,0
"crop_path => normalized bbox coordinates [xmin, ymin, width, height]",0
"only ground-truth bboxes do not have a ""confidence"" value",0
try loading image from local directory,0
try to download image from Blob Storage,0
crop the image,0
"expand box width or height to be square, but limit to img size",0
"Image.crop() takes box=[left, upper, right, lower]",0
pad to square using 0s,0
Author: lukemelas (github username),0
Github repo: https://github.com/lukemelas/EfficientNet-PyTorch,0
With adjustments and added comments by workingcoder (github username).,0
Support the construction of 'efficientnet-l2' without pretrained weights,0
Expansion phase (Inverted Bottleneck),0
"image_size = calculate_output_image_size(image_size, 1) <-- this wouldn't modify image_size",0
Depthwise convolution phase,0
"Squeeze and Excitation layer, if desired",0
Pointwise convolution phase,0
Expansion and Depthwise Convolution,0
Squeeze and Excitation,0
Pointwise Convolution,0
Skip connection and drop connect,0
The combination of skip connection and drop connect brings about stochastic depth.,0
Batch norm parameters,0
Get stem static or dynamic convolution depending on image size,0
Stem,0
Build blocks,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
"image_size = calculate_output_image_size(image_size, block_args.stride)  # stride = 1",0
Head,0
Final linear layer,0
Stem,0
Blocks,0
Head,0
Stem,0
Blocks,0
Head,0
Convolution layers,0
Pooling and final linear layer,0
Author: lukemelas (github username),0
Github repo: https://github.com/lukemelas/EfficientNet-PyTorch,0
With adjustments and added comments by workingcoder (github username).,0
###############################################################################,0
## Help functions for model architecture,0
###############################################################################,0
GlobalParams and BlockArgs: Two namedtuples,0
Swish and MemoryEfficientSwish: Two implementations of the method,0
round_filters and round_repeats:,0
Functions to calculate params for scaling model width and depth ! ! !,0
get_width_and_height_from_size and calculate_output_image_size,0
drop_connect: A structural design,0
get_same_padding_conv2d:,0
Conv2dDynamicSamePadding,0
Conv2dStaticSamePadding,0
get_same_padding_maxPool2d:,0
MaxPool2dDynamicSamePadding,0
MaxPool2dStaticSamePadding,0
"It's an additional function, not used in EfficientNet,",0
but can be used in other model (such as EfficientDet).,0
"Parameters for the entire model (stem, all blocks, and head)",0
Parameters for an individual model block,0
Set GlobalParams and BlockArgs's defaults,0
An ordinary implementation of Swish function,0
A memory-efficient implementation of Swish function,0
TODO: modify the params names.,1
"maybe the names (width_divisor,min_width)",0
"are more suitable than (depth_divisor,min_depth).",0
follow the formula transferred from official TensorFlow implementation,0
follow the formula transferred from official TensorFlow implementation,0
"generate binary_tensor mask according to probability (p for 0, 1-p for 1)",0
Note:,0
The following 'SamePadding' functions make output size equal ceil(input size/stride).,0
"Only when stride equals 1, can the output size be the same as input size.",0
Don't be confused by their function names ! ! !,0
Tips for 'SAME' mode padding.,0
Given the following:,0
i: width or height,0
s: stride,0
k: kernel size,0
d: dilation,0
p: padding,0
Output after Conv2d:,0
o = floor((i+p-((k-1)*d+1))/s+1),0
"If o equals i, i = floor((i+p-((k-1)*d+1))/s+1),",0
=> p = (i-1)*s+((k-1)*d+1)-i,0
With the same calculation as Conv2dDynamicSamePadding,0
Calculate padding based on image size and save it,0
Calculate padding based on image size and save it,0
###############################################################################,0
## Helper functions for loading model params,0
###############################################################################,0
BlockDecoder: A Class for encoding and decoding BlockArgs,0
efficientnet_params: A function to query compound coefficient,0
get_model_params and efficientnet:,0
Functions to get BlockArgs and GlobalParams for efficientnet,0
url_map and url_map_advprop: Dicts of url_map for pretrained weights,0
load_pretrained_weights: A function to load pretrained weights,0
Check stride,0
"Coefficients:   width,depth,res,dropout",0
Blocks args for the whole model(efficientnet-b0 by default),0
It will be modified in the construction of EfficientNet Class according to model,0
note: all models have drop connect rate = 0.2,0
ValueError will be raised here if override_params has fields not included in global_params.,0
train with Standard methods,0
check more details in paper(EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks),0
train with Adversarial Examples(AdvProp),0
check more details in paper(Adversarial Examples Improve Image Recognition),0
TODO: add the petrained weights url map of 'efficientnet-l2',1
AutoAugment or Advprop (different preprocessing),0
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils),0
from data_management.megadb.schema import sequences_schema_check,0
resize is for displaying them more quickly,0
dataset and seq_id are required fields,0
sort the images in the sequence,0
"if no class label on the image, show class label on the sequence",0
pool = ThreadPool(),0
"print('len of rendering_info', len(rendering_info))",0
"tqdm(pool.imap_unordered(render_image_info_partial, rendering_info), total=len(rendering_info))",0
options = write_html_image_list(),0
options['headerHtml'],0
print('Checking that the MegaDB entries conform to the schema...'),0
sequences_schema_check.sequences_schema_check(sequences),0
#######,0
,0
visualize_db.py,0
,0
Outputs an HTML page visualizing annotations (class labels and/or bounding boxes),0
on a sample of images in a database in the COCO Camera Traps format,0
,0
#######,0
%% Imports,0
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils),0
Assumes the cameratraps repo root is on the path,0
%% Settings,0
Set to None to visualize all images,0
Target size for rendering; set either dimension to -1 to preserve aspect ratio,0
"These are mutually exclusive; both are category names, not IDs",0
"Special tag used to say ""show me all images with multiple categories""",0
We sometimes flatten image directories by replacing a path separator with,0
another character.  Leave blank for the typical case where this isn't necessary.,0
Control rendering parallelization,0
Process-based parallelization in this function is currently unsupported,0
"due to pickling issues I didn't care to look at, but I'm going to just",0
"flip this with a warning, since I intend to support it in the future.",0
%% Helper functions,0
"Translate the file name in an image entry in the json database to a path, possibly doing",0
some manipulation of path separators,0
%% Core functions,0
"Optionally remove all images without bounding boxes, *before* sampling",0
"Optionally include/remove images with specific labels, *before* sampling",0
Put the annotations in a dataframe so we can select all annotations for a given image,0
Construct label map,0
Take a sample of images,0
Set of dicts representing inputs to render_db_bounding_boxes:,0
,0
"bboxes, boxClasses, image_path",0
All the class labels we've seen for this image (with out without bboxes),0
Iterate over annotations for this image,0
iAnn = 0; anno = annos_i.iloc[iAnn],0
...for each of this image's annotations,0
"We're adding html for an image before we render it, so it's possible this image will",0
fail to render.  For applications where this script is being used to debua a database,0
"(the common case?), this is useful behavior, for other applications, this is annoying.",0
...for each image,0
...def render_image_info,0
def process_images(...),0
%% Command-line driver,0
"Copy all fields from a Namespace (i.e., the output from parse_args) to an object.",0
,0
Skips fields starting with _.  Does not check existence in the target object.,0
Convert to an options object,0
%% Interactive driver(s),0
%%,0
os.startfile(htmlOutputFile),0
%% Constants and imports,0
convert category ID from int to str,0
Retry on blob storage read failures,0
%% Functions,0
PIL.Image.convert() returns a converted copy of this image,0
Alter orientation as needed according to EXIF tag 0x112 (274) for Orientation,0
,0
https://gist.github.com/dangtrinhnt/a577ece4cbe5364aad28,0
https://www.media.mit.edu/pia/Research/deepview/exif.html,0
,0
Null operation,0
Aspect ratio as width over height,0
ar = w / h,0
h = w / ar,0
w = ar * h,0
The following three functions are modified versions of those at:,0
,0
https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py,0
Convert to pixels so we can use the PIL crop() function,0
PIL's crop() does surprising things if you provide values outside of,0
"the image, clip inputs",0
...if this detection is above threshold,0
...for each detection,0
"list of lists, one list of strings for each bounding box (to accommodate multiple labels)",0
for color selection,0
"Always render objects with a confidence of ""None"", this is typically used",0
for ground truth data.,0
"{} is the default, which means ""show labels with no mapping"", so don't use ""if label_map"" here",0
if label_map:,0
"To avoid duplicate colors with detection-only visualization, offset",0
the classification class index by the number of detection classes,0
...for each classification,0
...if we have classification results,0
...if the confidence of this detection is above threshold,0
...for each detection,0
"print('Input must be of size [N, 4], but is ' + str(boxes_shape))",0
Deliberately trimming to the width of the image only in the case where,0
"box expansion is turned on.  There's not an obvious correct behavior here,",0
but the thinking is that if the caller provided an out-of-range bounding,0
"box, they meant to do that, but at least in the eyes of the person writing",0
"this comment, if you expand a box for visualization reasons, you don't want",0
to end up with part of a box.,0
,0
A slightly more sophisticated might check whether it was in fact the expansion,0
"that made this box larger than the image, but this is the case 99.999% of the time",0
"here, so that doesn't seem necessary.",1
...if we need to expand boxes,0
If the total height of the display strings added to the top of the bounding,0
"box exceeds the top of the image, stack the strings below the bounding box",0
instead of above.,0
Each display_str has a top and bottom margin of 0.05x.,0
Reverse list and print from bottom to top.,0
Skip empty strings,0
"list of lists, one list of strings for each bounding box (to accommodate multiple labels)",0
need to be a string here because PIL needs to iterate through chars,0
,0
stacked bar charts are made with each segment starting from a y position,0
"To fit the legend in, shrink current axis by 20%",0
Put a legend to the right of the current axis,0
####,0
,0
visualize_incoming_annotations.py,0
,0
Spot-check the annotations received from iMerit by visualizing annotated bounding,0
boxes on a sample of images and display them in HTML.,0
,0
"Modified in 2021 March to use the new format (iMerit batch 12 onwards), which is a",0
COCO formatted JSON with relative coordinates for the bbox.,0
,0
from data_management.megadb.schema import sequences_schema_check,0
we used frame_num of 1 when sending out images to annotators when it is not explicitly stored (wcs esp.),0
The file_name field in the incoming json looks like,0
alka_squirrels.seq2020_05_07_25C.frame119221.jpg,0
"we need to use the dataset, sequence and frame info to find the actual path in blob storage",0
using the sequences,0
category_id 5 is No Object Visible,0
download the image,0
Write to HTML,0
allow forward references in typing annotations,0
class variables,0
instance variables,0
get path to root,0
taxonomy CSV rows without 'taxonomy_string' entries are excluded,0
"from the taxonomy graph, but can be included in a classification",0
label specification JSON via the 'dataset_labels' key,0
use the lower parent,0
special cases,0
%% Imports,0
%% Taxnomy checking,0
taxonomy CSV rows without 'taxonomy_string' entries are excluded,0
"from the taxonomy graph, but can be included in a classification",0
label specification JSON via the 'dataset_labels' key,0
...for each row in the taxnomy file,0
%% Command-line driver,0
%% Interactive driver,0
%%,0
which datasets are already processed?,0
"sequence-level query should be fairly fast, ~1 sec",0
cases when the class field is on the image level (images in a sequence,0
"that had different class labels, 'caltech' dataset is like this)",0
"this query may take a long time, >1hr",0
"this query should be fairly fast, ~1 sec",0
read species presence info from the JSON files for each dataset,0
has this class name appeared in a previous dataset?,0
columns to populate the spreadsheet,0
sort by descending species count,0
make the spreadsheet,0
hyperlink Bing search URLs,0
hyperlink example image SAS URLs,0
TODO hardcoded columns: change if # of examples or col_order changes,1
,0
map_lila_taxonomy_to_wi_taxonomy.py,0
,0
Loads the LILA category mapping (in which taxonomy information comes from an iNat taxonomy snapshot),0
and tries to map each class to the Wildlife Insights taxonomy.,0
,0
%% Constants and imports,0
Created by get_lila_category_list.py... contains counts for each category,0
This is a manually-curated file used to store mappings that had to be made manually,0
This is the main output file from this whole process,0
%% Load category and taxonomy files,0
%% Pull everything out of pandas,0
%% Cache WI taxonomy lookups,0
This is just a handy lookup table that we'll use to debug mismatches,0
taxon = wi_taxonomy[21653]; print(taxon),0
Look for keywords that don't refer to specific taxa: blank/animal/unknown,0
Do we have a species name?,0
"If 'species' is populated, 'genus' should always be populated; one item currently breaks",0
this rule.,0
...for each taxon,0
%% Find redundant taxa,0
%% Manual review of redundant taxa,0
%% Clean up redundant taxa,0
taxon_name = list(taxon_name_to_preferred_taxon_id.keys())[0],0
"If we've gotten this far, we should be choosing from multiple taxa.",0
,0
"This will become untrue if any of these are resolved later, at which point we shoudl",0
remove them from taxon_name_to_preferred_id,0
Choose the preferred taxa,0
%% Read supplementary mappings,0
"Each line is [lila query],[WI taxon name],[notes]",0
%% Map LILA categories to WI categories,0
Must be ordered from kingdom --> species,0
TODO:,1
"['subspecies','variety']",0
i_taxon = 0; taxon = lila_taxonomy[i_taxon]; print(taxon),0
"Go from kingdom --> species, choosing the lowest-level description as the query",0
"E.g., 'car'",0
"print('Made a supplementary mapping from {} to {}'.format(query,wi_taxon['taxon_name']))",0
print('No match for {}'.format(query)),0
...for each LILA taxon,0
%% Manual mapping,0
%% Build a dictionary from LILA dataset names and categories to LILA taxa,0
i_d = 0; d = lila_taxonomy[i_d],0
"%% Map LILA datasets to WI taxa, and count the number of each taxon available in each dataset",0
dataset_name = list(lila_dataset_to_categories.keys())[0],0
dataset_category = dataset_categories[0],0
"Write out the dataset name, category name, WI GUID, WI scientific name, WI common name,",0
and count,0
...for each category in this dataset,0
...for each dataset,0
...with open(),0
,0
retrieve_sample_image.py,0
,0
"Downloader that retrieves images from Google images, used for verifying taxonomy",0
"lookups and looking for egregious mismappings (e.g., ""snake"" being mapped to a fish called",0
"""snake"").",0
,0
"Simple wrapper around simple_image_download, but I've had to swap in and out the underlying",0
downloader a few times.,0
,0
%% Imports and environment,0
%%,0
%% Main entry point,0
%% Test driver,0
%%,0
,0
"Using the taxonomy .csv file, map all LILA datasets to the standard taxonomy",0
,0
Does not currently produce results; this is just used to confirm that all category names,0
have mappings in the taxonomy file.,0
,0
%% Constants and imports,0
Created by get_lila_category_list.py... contains counts for each category,0
%% Load category and taxonomy files,0
%% Map dataset names and category names to scientific names,0
i_row = 1; row = taxonomy_df.iloc[i_row]; row,0
"%% For each dataset, make sure we can map every category to the taxonomy",0
dataset_name = list(lila_dataset_to_categories.keys())[0],0
c = categories[0],0
,0
"Takes the megadb taxonomy mapping, extracts the rows that are relevant to",0
"LILA, and does some cleanup.",0
,0
%% Constants and imports,0
This is a partially-completed taxonomy file that was created from a different set of,0
"scripts, but covers *most* of LILA as of June 2022",0
Created by get_lila_category_list.py,0
%% Read the input files,0
Get everything out of pandas,0
"%% Find all unique dataset names in the input list, compare them with data names from LILA",0
d = input_taxonomy_rows[0],0
The script that generates this dictionary creates a separate entry for bounding box,0
"metadata files, but those don't represent new dataset names",0
%% Map input columns to output datasets,0
Make sure all of those datasets actually correspond to datasets on LILA,0
%% Re-write the input taxonomy file to refer to LILA datasets,0
Map the string datasetname:token to a taxonomic tree json,0
mapping = input_taxonomy_rows[0],0
Make sure that all occurrences of this mapping_string give us the same output,0
assert taxonomy_string == taxonomy_mappings[mapping_string],0
%% Re-write the input file in the target format,0
mapping_string = list(taxonomy_mappings.keys())[0],0
,0
prepare_lila_taxonomy_release.py,0
,0
"Given the private intermediate taxonomy mapping, prepare the public (release)",0
taxonomy mapping file.,0
,0
%% Imports and constants,0
Created by get_lila_category_list.py... contains counts for each category,0
%% Find out which categories are actually used,0
dataset_name = datasets_to_map[0],0
i_row = 0; row = df.iloc[i_row]; row,0
%% Generate the final output file,0
i_row = 0; row = df.iloc[i_row]; row,0
match_at_level = taxonomic_match[0],0
i_row = 0; row = df.iloc[i_row]; row,0
"E.g.: (43117, 'genus', 'lepus', ['hares and jackrabbits']",0
###############,0
---> CONSTANTS,0
###############,0
max_progressbar = count * (list(range(limit+1))[-1]+1),0
"bar = progressbar.ProgressBar(maxval=max_progressbar,",0
"widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()]).start()",0
bar.update(bar.currval + 1),0
bar.finish(),0
,0
"Given a subset of LILA datasets, find all the categories, and start the taxonomy",0
mapping process.,0
,0
%% Constants and imports,0
Created by get_lila_category_list.py,0
'NACTI',0
'Channel Islands Camera Traps',0
%% Read the list of datasets,0
The script that generates this dictionary creates a separate entry for bounding box,0
"metadata files, but those don't represent new dataset names",0
%% Find all categories,0
dataset_name = datasets_to_map[0],0
%% Initialize taxonomic lookup,0
from taxonomy_mapping.species_lookup import (,0
"get_taxonomic_info, print_taxonomy_matche)",0
%% Manual lookup,0
%%,0
q = 'white-throated monkey',0
raise ValueError(''),0
%% Match every query against our taxonomies,0
mapping_string = category_mappings[1]; print(mapping_string),0
...for each mapping,0
%% Write output rows,0
,0
"Does some consistency-checking on the LILA taxonomy file, and generates",0
an HTML preview page that we can use to determine whether the mappings,0
make sense.,0
,0
%% Imports and constants,0
"lila_taxonomy_file = r""G:\git\agentmorrisprivate\lila-taxonomy\lila-taxonomy-mapping.csv""",0
"lila_taxonomy_file = r""G:\temp\lila\lila_additions_2022.06.29.csv""",0
%% Support functions,0
%% Read the taxonomy mapping file,0
%% Prepare taxonomy lookup,0
from taxonomy_mapping.species_lookup import (,0
"get_taxonomic_info, print_taxonomy_matche)",0
%% Optionally remap all gbif-based mappings to inat (or vice-versa),0
%%,0
i_row = 1; row = df.iloc[i_row]; row,0
This should be zero for the release .csv,0
%%,0
%% Check for mappings that disagree with the taxonomy string,0
Look for internal inconsistency,0
Look for outdated mappings,0
i_row = 0; row = df.iloc[i_row],0
%% List null mappings,0
,0
"These should all be things like ""unidentified"" and ""fire""",0
,0
i_row = 0; row = df.iloc[i_row],0
%% List mappings with scientific names but no common names,0
%% List mappings that map to different things in different data sets,0
x = suppress_multiple_matches[-1],0
...for each row where we saw this query,0
...for each row,0
"%% Verify that nothing ""unidentified"" maps to a species or subspecies",0
"E.g., ""unidentified skunk"" should never map to a specific species of skunk",0
%% Make sure there are valid source and level values for everything with a mapping,0
%% Find WCS mappings that aren't species or aren't the same as the input,0
"WCS used scientific names, so these remappings are slightly more controversial",0
then the standard remappings.,0
row = df.iloc[-500],0
"Anything marked ""species"" or ""unknown"" by definition doesn't map to a species,",0
so ignore these.,0
print('WCS query {} ({}) remapped to {} ({})'.format(,0
"query,common_name,scientific_name,common_names_from_taxonomy))",0
%% Download sample images for all scientific names,0
i_row = 0; row = df.iloc[i_row],0
if s != 'mirafra':,0
continue,0
Check whether we already have enough images for this query,0
"print('Skipping query {}, already have {} images'.format(s,len(sizes_above_threshold)))",0
Check whether we've already run this query for a previous row,0
...for each row in the mapping table,0
%% Rename .jpeg to .jpg,0
"print('Renaming {} to {}'.format(fn,new_fn))",0
%% Choose representative images for each scientific name,0
s = list(scientific_name_to_paths.keys())[0],0
Be suspicious of duplicate sizes,0
...for each scientific name,0
%% Delete unused images,0
%% Produce HTML preview,0
i_row = 2; row = df.iloc[i_row],0
"image_paths = [os.path.relpath(p, output_base) for p in image_paths]",0
...for each row,0
%% Open HTML preview,0
######,0
,0
species_lookup.py,0
,0
Look up species names (common or scientific) in the GBIF and iNaturalist,0
taxonomies.,0
,0
Run initialize_taxonomy_lookup() before calling any other function.,0
,0
######,0
%% Constants and imports,0
As of 2020.05.12:,0
,0
"GBIF: ~777MB zipped, ~1.6GB taxonomy",0
"iNat: ~2.2GB zipped, ~51MB taxonomy",0
These are un-initialized globals that must be initialized by,0
the initialize_taxonomy_lookup() function below.,0
%% Functions,0
Initialization function,0
# Load serialized taxonomy info if we've already saved it,0
"# If we don't have serialized taxonomy info, create it from scratch.",0
Download and unzip taxonomy files,0
taxonomy_name = list(taxonomy_urls.items())[0][0]; zip_url = list(taxonomy_urls.items())[0][1],0
Don't download the zipfile if we've already unzipped what we need,0
Bypasses download if the file exists already,0
Unzip the files we need,0
...for each file that we need from this zipfile,0
Remove the zipfile,0
os.remove(zipfile_path),0
...for each taxonomy,0
"Create dataframes from each of the taxonomy files, and the GBIF common",0
name file,0
Load iNat taxonomy,0
Load GBIF taxonomy,0
Remove questionable rows from the GBIF taxonomy,0
Load GBIF vernacular name mapping,0
Only keep English mappings,0
Convert everything to lowercase,0
"For each taxonomy table, create a mapping from taxon IDs to rows",0
Create name mapping dictionaries,0
Build iNat dictionaries,0
row = inat_taxonomy.iloc[0],0
Build GBIF dictionaries,0
"The canonical name is the Latin name; the ""scientific name""",0
include the taxonomy name.,0
,0
http://globalnames.org/docs/glossary/,0
This only seems to happen for really esoteric species that aren't,0
"likely to apply to our problems, but doing this for completeness.",0
Don't include taxon IDs that were removed from the master table,0
Save everything to file,0
...def initialize_taxonomy_lookup(),0
"list of dicts: {'source': source_name, 'taxonomy': match_details}",0
i_match = 0,0
"list of (taxon_id, taxonRank, scientific name, [vernacular names])",0
corresponding to an exact match and its parents,0
Walk taxonomy hierarchy,0
This can happen because we remove questionable rows from the,0
GBIF taxonomy,0
"print(f'Warning: no row exists for parent_taxon_id {parent_taxon_id},' + \",0
"f'child taxon_id: {taxon_id}, query: {query}')",0
The GBIF taxonomy contains unranked entries,0
...while there is taxonomy left to walk,0
...for each match,0
Remove redundant matches,0
i_tree_a = 0; tree_a = matching_trees[i_tree_a],0
i_tree_b = 1; tree_b = matching_trees[i_tree_b],0
"If tree a's primary taxon ID is inside tree b, discard tree a",0
,0
taxonomy_level_b = tree_b['taxonomy'][0],0
...for each level in taxonomy B,0
...for each tree (inner),0
...for each tree (outer),0
...def traverse_taxonomy(),0
"print(""Finding taxonomy information for: {0}"".format(query))",0
"In GBIF, some queries hit for both common and scientific, make sure we end",0
up with unique inputs,0
"If the species is not found in either taxonomy, return None",0
Both GBIF and iNat have a 1-to-1 mapping between taxon_id and row number,0
Walk both taxonomies,0
...def get_taxonomic_info(),0
m = matches[0],0
"For example: [(9761484, 'species', 'anas platyrhynchos')]",0
...for each taxonomy level,0
...for each match,0
...def print_taxonomy_matches(),0
%% Taxonomy functions that make subjective judgements,0
query = 'person',0
Do we have an iNat match?,0
"print_taxonomy_matches(inat_matches, verbose=True)",0
"print_taxonomy_matches(gbif_matches, verbose=True)",0
print('Warning: multiple iNat matches for {}'.format(query)),0
Prefer chordates... most of the names that aren't what we want,0
"are esoteric insects, like a moth called ""cheetah""",0
,0
"If we can't find a chordate, just take the first match.",0
,0
i_test_match = 0,0
"This is (taxonID, taxonLevel, scientific, [list of common])",0
print(f'Warning: multiple iNat common names for {query}'),0
Default to returning the query,0
"print(f'Matched iNat {query} to {scientific_name},{common_name}')",0
...if we had iNat matches,0
If we either prefer GBIF or didn't have iNat matches,0
,0
Code is deliberately redundant here; I'm expecting some subtleties in how,0
handle GBIF and iNat.,0
print('Warning: multiple GBIF matches for {}'.format(query)),0
Prefer chordates... most of the names that aren't what we want,0
"are esoteric insects, like a moth called ""cheetah""",0
,0
"If we can't find a chordate, just take the first match.",0
,0
i_test_match = 0,0
"This is (taxonID, taxonLevel, scientific, [list of common])",0
print(f'Warning: multiple GBIF common names for {query}'),0
Default to returning the query,0
...if we needed to look in the GBIF taxonomy,0
...def _get_preferred_taxonomic_match(),0
%% Interactive drivers and debug,0
%% Initialization,0
%% Taxonomic lookup,0
query = 'lion',0
print(matches),0
Print the taxonomy in the taxonomy spreadsheet format,0
%% Directly access the taxonomy tables,0
%% Command-line driver,0
Read command line inputs (absolute path),0
Read the tokens from the input text file,0
Loop through each token and get scientific name,0
,0
process_species_by_dataset,0
,0
We generated a list of all the annotations in our universe; this script is,0
used to (interactively) map them onto the GBIF and iNat taxonomies.  Don't,0
"try to run this script from top to bottom; it's used like a notebook, not like",0
"a script, since manual review steps are required.",0
,0
%% Imports,0
%autoreload 0,0
%autoreload -species_lookup,0
%% Constants,0
Input file,0
Output file after automatic remapping,0
File to which we manually copy that file and do all the manual review; this,0
should never be programmatically written to,0
The final output spreadsheet,0
HTML file generated to facilitate the identificaiton of egregious mismappings,0
%% Functions,0
Prefer iNat matches over GBIF matches,0
query = 'person',0
Do we have an iNat match?,0
"print_taxonomy_matches(inat_matches, verbose=True)",0
"print_taxonomy_matches(gbif_matches, verbose=True)",0
print('Warning: multiple iNat matches for {}'.format(query)),0
Prefer chordates... most of the names that aren't what we want,0
"are esoteric insects, like a moth called ""cheetah""",0
,0
"If we can't find a chordate, just take the first match.",0
,0
i_test_match = 0,0
"This is (taxonID, taxonLevel, scientific, [list of common])",0
print(f'Warning: multiple iNat common names for {query}'),0
Default to returning the query,0
"print(f'Matched iNat {query} to {scientific_name},{common_name}')",0
...if we had iNat matches,0
If we either prefer GBIF or didn't have iNat matches,0
,0
Code is deliberately redundant here; I'm expecting some subtleties in how,0
handle GBIF and iNat.,0
print('Warning: multiple GBIF matches for {}'.format(query)),0
Prefer chordates... most of the names that aren't what we want,0
"are esoteric insects, like a moth called ""cheetah""",0
,0
"If we can't find a chordate, just take the first match.",0
,0
i_test_match = 0,0
"This is (taxonID, taxonLevel, scientific, [list of common])",0
print(f'Warning: multiple GBIF common names for {query}'),0
Default to returning the query,0
...if we needed to look in the GBIF taxonomy,0
...def get_preferred_taxonomic_match(),0
%% Initialization,0
%% Test single-query lookup,0
%%,0
%%,0
"q = ""grevy's zebra""",0
%% Read the input data,0
%% Run all our taxonomic lookups,0
i_row = 0; row = df.iloc[i_row],0
query = 'lion',0
...for each query,0
Write to the excel file that we'll use for manual review,0
%% Download preview images for everything we successfully mapped,0
uncomment this to load saved output_file,0
"output_df = pd.read_excel(output_file, keep_default_na=False)",0
i_row = 0; row = output_df.iloc[i_row],0
...for each query,0
%% Write HTML file with representative images to scan for obvious mis-mappings,0
i_row = 0; row = output_df.iloc[i_row],0
...for each row,0
%% Look for redundancy with the master table,0
Note: `master_table_file` is a CSV file that is the concatenation of the,0
"manually-remapped files (""manual_remapped.xlsx""), which are the output of",0
this script run across from different groups of datasets. The concatenation,0
"should be done manually. If `master_table_file` doesn't exist yet, skip this",0
"code cell. Then, after going through the manual steps below, set the final",0
manually-remapped version to be the `master_table_file`.,0
%% Manual review,0
Copy the spreadsheet to another file; you're about to do a ton of manual,0
review work and you don't want that programmatically overwrriten.,0
,0
See manual_review_xlsx above,0
%% Read back the results of the manual review process,0
%% Look for manual mapping errors,0
Manually inspect df for typos in 'dataset_names' and 'taxonomy_level' columns,0
Identify rows where:,0
,0
- 'taxonomy_level' does not match level of 1st element in 'taxonomy_string',0
- 'scientific_name' does not match name of 1st element in 'taxonomy_string',0
,0
...both of which typically represent manual mapping errors.,0
i_row = 0; row = df.iloc[i_row],0
"I'm not sure why both of these checks are necessary, best guess is that",1
the Excel parser was reading blanks as na on one OS/Excel version and as '',0
on another.,0
The taxonomy_string column is a .json-formatted string; expand it into,0
an object via eval(),0
"%% Find scientific names that were added manually, and match them to taxonomies",0
i_row = 0; row = df.iloc[i_row],0
...for each query,0
%% Write out final version,0
,0
"Generate read-only SAS URLs for all LILA containers, to facilitate partial downloads.",0
,0
The results of this script end up here:,0
,0
http://lila.science/wp-content/uploads/2020/03/lila_sas_urls.txt,0
,0
"Update: that file is manually maintained now, it can't be programmatically generated",0
,0
%% Imports,0
Read-only,0
%% Enumerate containers,0
%% Generate SAS tokens,0
%% Generate SAS URLs,0
%% Write to output file,0
--------some stuff needed to get AJAX to work with bottle?--------#,0
-------------------------------------------------------------------------------- #,0
PREPARE TO QUEUE IMAGES FOR LABELING,0
-------------------------------------------------------------------------------- #,0
# Connect as USER to database DB_NAME through peewee and initialize database proxy,0
# Load embedding model,0
---------------------------------------------------------------------- #,0
CREATE QUEUE OF IMAGES TO LABEL,0
---------------------------------------------------------------------- #,0
Use classifier to generate predictions,0
# Update model predicted class in PostgreSQL database,0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?,0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
-------------------------------------------------------------------------------- #,0
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI,0
-------------------------------------------------------------------------------- #,0
"# static routes (to serve CSS, etc.)",0
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps",0
"return bottle.static_file(filename, root='../../../../../../../../../../../.')",0
# dynamic routes,0
if data['display_grayscale']:,0
indices_to_exclude.update(set(color_indices)),0
elif not data['display_grayscale']:,0
indices_to_exclude.update(set(grayscale_indices)),0
data['display_images'] = {},0
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices],0
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices],0
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices],0
data['display_images']['detection_categories'] = [],0
for i in indices:,0
if str(dataset.samples[i][1]) == 'None':,0
data['display_images']['detection_categories'].append('None'),0
else:,0
existing_category_entries = {cat.id: cat.name for cat in Category.select()},0
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()",0
data['display_images']['detection_categories'].append(cat_name),0
Use image ids in images_to_label to get the corresponding dataset indices,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
print(set(dataset.set_indices[4]).update(set(indices_to_label))),0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Use image id images_to_label to get the corresponding dataset index,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?,0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Train on samples that have been labeled so far,0
dataset.set_kind(DetectionKind.UserDetection.value),0
print(y_train),0
Predict on the samples that have not been labeled,0
print(y_pred),0
Update model predicted class in PostgreSQL database,0
timer = time.time(),0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
print('Updating the database took %0.2f seconds'%(time.time() - timer)),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids)),0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
"once the classifier has been trained the first time, switch to AL sampling",0
,0
make_oneclass_json.py,0
,0
"Takes a coco-camera-traps .json database and collapses species classes to binary,",0
optionally removing labels from empty images (to be detector-friendly) (depending on,0
"""experiment_type"").",0
,0
"Assumes that empty images are labeled as ""empty"".",0
,0
%% Imports and environment,0
%% Core conversion function,0
"We're removing empty images from the annotation list, but not from",0
"the ""images"" list; they'll still get used in detector training.",0
print('Ignoring empty annotation'),0
%% Interactive driver,0
%%,0
Load annotations,0
Convert from multi-class to one-class,0
Write out the one-class data,0
%% Command-line driver,0
,0
plot_bounding_boxes.py,0
,0
Takes a .json database containing bounding boxes and renders those boxes on the,0
source images.,0
,0
"This assumes annotations in coco-camera-traps format, with absolute bbox",0
coordinates.,0
,0
%% Imports and environment,0
How many images should we process?  Set to -1 to process all images.,0
Should we randomize the image order?,0
%%  Read database and build up convenience mappings,0
Category ID to category name,0
Image ID to image info,0
Image ID to image path,0
Image ID to all annotations referring to this image,0
"%% Iterate over images, draw bounding boxes, write to file",0
For each image,0
image = images[0],0
Build up a list of bounding boxes to draw on this image,0
Load the image,0
Create figure and axes,0
Display the image,0
ann = imageAnnotations[0],0
"For each annotation associated with this image, render bounding box and label",0
"In the Rectangle() function, the first argument (""location"") is the bottom-left",0
of the rectangle.,0
,0
Origin is the upper-left of the image.,0
Add the patch to the Axes,0
Add a class label,0
This is magic goop that removes whitespace around image plots (sort of),0
Write the output image,0
...for each image,0
,0
plot_imerit_annotations.py,0
,0
Takes a .json file full of bounding box annotations and renders those boxes on the,0
source images.,0
,0
"This assumes annotations in the format we receive them, specifically:",0
,0
1) Relative bbox coordinates,0
"2) A list of .json objects, not a well-formatted .json file",0
,0
"I.e., don't use this on a COCO-style .json file.  See plot_bounding_boxes.py",0
for the same operation performed on a proper COCO-camera-traps database.,0
,0
%% Imports and environment,0
"os.makedirs(outputBase, exist_ok=True)",0
%%  Read all source images and build up a hash table from image name to full path,0
"This spans training and validation directories, so it's not the same as",0
just joining the image name to a base path,0
"%% Iterate over annotations, draw bounding boxes, write to file",0
annData has keys:,0
,0
"annotations, categories, images",0
,0
Each of these are lists of dictionaries,0
%% Render all annotations on each image in the sequence,0
%% Pull out image metadata,0
Build up a list of bounding boxes to draw on this image,0
Pull out just the image name from the filename,0
,0
File names look like:,0
,0
seq6efffac2-5567-11e8-b3fe-dca9047ef277.frame1.img59a94e52-23d2-11e8-a6a3-ec086b02610b.jpg,0
"m = re.findall(r'img(.*\.jpg)$', imgFileName, re.M|re.I)",0
print(m),0
assert(len(m) == 1),0
queryFileName = m[0],0
Map this image back to the original directory,0
"%% Loop over annotations, find annotations that match this image",0
%%,0
"x,y,w,h",0
,0
"x,y is the bottom-left of the rectangle",0
,0
"x,y origin is the upper-left",0
...for each annotation,0
%% Render with PIL (scrap),0
%% Render with Matplotlib,0
Create figure and axes,0
Display the image,0
Location is the bottom-left of the rect,0
,0
Origin is the upper-left,0
Add the patch to the Axes,0
This is magic goop that removes whitespace around image plots (sort of),0
os.startfile(outputFileName),0
%% Showing figures on-screen during debugging,0
plt.show(),0
Various (mostly unsuccessful) approaches to getting the plot window to show up,0
"in the foreground, which is a backend-specific operation...",0
,0
fig.canvas.manager.window.activateWindow(),0
fig.canvas.manager.window.raise_(),0
fm = plt.get_current_fig_manager(),0
"fm.window.attributes('-topmost', 1)",0
"fm.window.attributes('-topmost', 0)",0
,0
# This is the one that I found to be most robust... at like 80% robust.,0
plt.get_current_fig_manager().window.raise_(),0
%%,0
...for each image,0
...for each file,0
,0
convert_imerit_json_to_coco_json.py,0
,0
"Takes a .json file with bounding boxes but no class labels, and a .json file containing the",0
"class labels for those images, and creates a new json file with class labels and bounding",0
boxes.,0
,0
"The bounding box .json file is in the format returned by our annotators, which is not",0
actually a fully-formed .json file; rather it's a series of .json objects,0
,0
"Leaves filenames intact.  Stores imerit ""category IDs"" (empty/human/group/animal) in a new",0
"field called ""annotation_type"".",0
,0
%% Imports and constants,0
%% Configure files and paths,0
"For Snapshot Serengeti, we stored image IDs in our annotation files as:",0
,0
S1_B06_R1_PICT0008,0
,0
...but the corresponding ID in the master database is actually:,0
,0
S1\B06\R1\S1_B06_R1_PICT0008,0
,0
"If this is ""True"", we'll expand the former to the latter",0
Handling a one-off issue in which .'s were mysteriously replaced with -'s,0
"in our annotations.  This will be set dynamically, but I keep it here as",0
a constant to remind me to remove this code when we clean this issue up.,1
Used in the (rare) case where a bounding box was added to an image that was originally,0
annotated as empty,0
Used in the (rare) case where we added bounding boxes to an image with multiple species,0
"Should we include ambiguous bounding boxes (with the ""ambiguous"" category label)",0
in the output file?  Ambiguous boxes are boxes drawn on images with multiple species.,0
"%%  Read metadata from the master database, bounding boxes from the annotations file",0
"The bounding box .json file is in the format returned by our annotators, which is not",0
actually a fully-formed .json file; rather it's a series of .json objects,0
"Each element of annData is a dictionary corresponding to a single sequence, with keys:",0
,0
"annotations, categories, images",0
sequence = annData[0],0
%% Build convenience mappings,0
Image ID to images,0
Category ID to categories (referring to the database categories),0
"Image ID to categories (i.e., species labels)",0
Utility function we'll use to create annotations for images in empty,0
sequences (empty images in non-empty sequences already have annotations),0
"%% Reformat annotations, grabbing category IDs from the master database (prep)",0
iSequence = 0; sequence = annData[0],0
"%% Reformat annotations, grabbing category IDs from the master database (loop)",0
Make a copy here; we're going to manipulate the sequence annotations,0
when we need to add synthetic annotations for empty images,0
im = sequenceImages[0],0
Are there any annotations in this sequence?,0
Which images in this sequence have annotations?,0
For each image in this sequence...,0
imeritImageID = im['id'],0
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG,0
Confirm that the file exists,0
Hande a one-off issue with our annotations,0
datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG,0
,0
...had become:,0
,0
datasetsnapshotserengeti.seqASG000001a-frame0.imgS1_B06_R1_PICT0008.JPG,0
Does it look like we encountered this issue?,0
Convert:,0
,0
S1_B06_R1_PICT0008,0
,0
...to:,0
,0
S1/B06/B06_R1/S1_B06_R1_PICT0008,0
Make sure we only see each image once,0
Create empty annotations for empty images,0
,0
Here we use the *unmodified* file name,0
Create an empty annotation for this image,0
Annotations still use the annotation filename (not database ID) at this point;,0
these will get converted to database IDs below when we process the,0
whole sequence.,0
Sanity-check image size,0
"print('Warning: img {} was listed in DB as {}x{}, annotated as {}x{}, actual size{}x{}'.format(",0
"old_id,new_im['width'],new_im['height'],im['width'],im['height'],imgObj.width,imgObj.height))",0
...for each image in this sequence,0
For each annotation in this sequence...,0
ann = sequenceAnnotations[0],0
Prepare an annotation using the category ID from the database and,0
the bounding box from the annotations file,0
Maintain iMerit's annotation category,0
Generate an (arbitrary) ID for this annotation; the COCO format has a concept,0
"of annotation ID, but our annotation files don't",0
This was a one-off quirk with our file naming,1
We'll do special handling of images with multiple categories later,0
Store the annotation type (group/human/animal/empty),0
This annotation has no bounding box but the image wasn't originally,0
annotated as empty,0
This annotation has a bounding box but the image was originally,0
annotated as empty,0
unnormalize the bbox,0
... for each annotation in this sequence,0
... for each sequence,0
%% Post-processing,0
Count empty images,0
...for each file,0
%% Sanity-check empty images,0
,0
make_ss_annotation_image_folder.py,0
,0
Take a directory full of images with the very long filenames we give annotators:,0
,0
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension,0
,0
"...along with a COCO-camera-traps database referring to those files, and:",0
,0
1) Creates a new COCO-camera-traps database with the original filenames in them,0
(copying the annotations),0
,0
2) Optionally creates a new directory with those images named according to the,0
"Snapshot Serengeti naming convention, including complete relative paths.",0
,0
See convert_imerit_json_to_coco_json to see how we get from the original annotation,0
.json to a COCO-camera-traps database.,0
,0
%% Constants and imports,0
%% Configure files/paths,0
%% Read the annotations (referring to the old filenames),0
"%% Update filenames, optionally copying files",0
im = data['images'][0],0
For each image...,0
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG,0
"Find the image name, e.g. S1_B06_R1_PICT0008",0
Convert:,0
,0
S1_B06_R1_PICT0008.JPG,0
,0
...to:,0
,0
S1/B06/B06_R1/S1_B06_R1_PICT0008.JPG,0
...for each image,0
%% Write the revised database,0
,0
get_annotation_tool_link.py,0
,0
"Takes a COCO-camera-traps-style .json file with URLs already embedded, and prepares",0
a link to the visipedia annotation tool that reviews a subset of those images.,0
,0
,0
create_new_annotation_json.py,0
,0
"Creates a subset of a larger .json database, in this case specifically to pick some images",0
from Snapshot Serengeti.,0
,0
from utils import get_db_dicts,0
for seq in already_annotated:,0
seq_to_ims.pop(seq),0
remove already annotated images,0
add lion images,0
lion_seqs = cat_to_seqs[cat_to_id['lionMale']] + cat_to_seqs[cat_to_id['lionFemale']],0
#print(len(lion_seqs)),0
lion_seqs = [seq for seq in lion_seqs if seq not in already_annotated],0
#print(len(lion_seqs)),0
"lion_seqs_to_annotate.extend(random.sample(lion_seqs, min(len(lion_seqs),num_lions)))",0
print(len(lion_seqs_to_annotate)),0
num_elephants = 1000,0
elephant_seqs = cat_to_seqs[cat_to_id['elephant']],0
#print(len(lion_seqs)),0
elephant_seqs = [seq for seq in elephant_seqs if seq not in already_annotated],0
#print(len(lion_seqs)),0
"elephant_seqs_to_annotate = random.sample(elephant_seqs, num_elephants)",0
num_empty = 10,0
empty_seqs_to_annotate = [],0
for loc in loc_to_seqs:,0
empty_seqs = cats_per_location[loc][cat_to_id['empty']],0
empty_seqs = [seq for seq in empty_seqs if seq not in already_annotated],0
empty_seqs = [seq for seq in empty_seqs if seq_to_season[seq] in seasons_to_keep],0
"empty_seqs_to_annotate.extend(random.sample(empty_seqs, min(len(empty_seqs),num_empty)))",0
ims_to_annotate.extend(empty_ims_to_annotate),0
,0
filter_database.py,0
,0
"Look through a COCO-ct database and find images matching some crtieria, writing",0
a subset of images and annotations to a new file.,0
,0
%% Constants and imports,0
%% Configuration,0
%%  Read database and build up convenience mappings,0
Category ID to category name,0
Image ID to image info,0
Image ID to image path,0
%% Filter,0
ann = annotations[0],0
Is this a tiny box or a group annotation?,0
"x,y,w,h",0
All empty annotations should be classified as either empty or ambiguous,0
,0
"The ambiguous cases are basically minor misses on the annotators' part,",0
where two different small animals were present somewhere.,0
%% Write output file,0
,0
find_images_for_classes.py,0
,0
"Given a .json database, find images that are associated with one or more",0
classes.,0
,0
%% Constants and imports,0
%% Configuration,0
%%  Read database and build up convenience mappings,0
Category ID to category name,0
Image ID to image info,0
Image ID to image path,0
%% Look for target-class annotations,0
ann = annotations[0],0
,0
create_tfrecords_format.py,0
,0
This script creates a tfrecords file from a dataset in VOTT format.,0
%% Imports and environment,0
set up the filenames and annotations,0
This loop reads the bboxes and corresponding labels and assigns them,0
the correct image. Kind of slow at the moment...,0
If needed: merging all classes,0
bbox_labels = ['Animal' for _ in bbox_labels],0
BBox coords are stored in the format,0
"x_min (of width axis) y_min (of height axis), x_max, y_max",0
Coordinate system starts in top left corner,0
"In this framework, we need ('ymin', 'xmin', 'ymax', 'xmax') format",0
print out some stats,0
To make sure we loaded the bboxes correctly:,0
self.validate_bboxes(),0
For each image in the data set...,0
Make sure all are greater equal 0,0
%% Main tfrecord generation function,0
Propagate optional metadata to tfrecords,0
endfor each annotation for the current image,0
endfor each image,0
"Calculate number of shards to get the desired number of images per record,",0
ensure it is evenly divisible by the number of threads,0
%% Command-line driver,0
%% Driver,0
,0
make_tfrecords_cis_trans.py,0
,0
Given a .json file that contains a three-element list (train/val/test) of image IDs and a .json database that contains,0
"those image IDs, generates tfrecords whose filenames include ""train""/""val""/""test""",0
,0
"dataset = json.load(open('/ai4efs/databases/snapshotserengeti/oneclass/SnapshotSerengeti_Seasons_1_to_4_tfrecord_format_valid_ims.json','r'))",0
print('Creating trans_val tfrecords'),0
dataset = [im_id_to_im[idx] for idx in trans_val],0
,0
create_classification_tfrecords_from_json.py,0
,0
Called from make_tfrecords_cis_trans.py if you're running a classification experiment.,0
,0
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/',0
datafile = 'eccv_18_annotation_files_oneclass/CaltechCameraTrapsECCV18',0
image_file_root = datafolder+'eccv_18_all_images/',0
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/',0
datafolder = '/data/iwildcam/',0
datafile = 'combined_iwildcam_annotations_oneclass/eccv_train_and_imerit_2',0
need consecutive category ids,0
old_cat_id_to_new_cat_id = {categories[idx]['id']:idx+1 for idx in range(len(categories))},0
print(old_cat_id_to_new_cat_id),0
remove multiclass images,0
print(images[0]),0
print(vis_data[0]),0
,0
make_tfrecords_from_json.py,0
,0
"Given a coco-camera-traps .json file, creates tfrecords",0
,0
Thin wrapper for create_tfrecords_from_json.,0
,0
%% Constants and imports,0
%% Constants and imports (interactive),0
%%,0
%% Main tfrecord generation function,0
"check whether the input file has already been converted to the tfrecords format,",0
"if not, convert",0
"Calculate number of shards to get the desired number of images per record,",0
ensure it is evenly divisible by the number of threads,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
eMammal_make_splits.py,0
,0
"Based on a tfrecords_format json file of the database, creates 3 splits according to",0
the specified fractions based on location (images from the same location should be in,0
one split) or based on images.,0
,0
"If a previous split is provided (append_to_previous_split is True), the entries in",0
"each split will be preserved, and new entries will be appended, so that new models",0
can warm start with a model trained on the original splits.,0
configurations and paths,0
approximate fraction for the new entries,0
read in the previous splits of image ID or location ID if available,0
"find new locations and assign them to a split, without reassigning any previous locations",0
"find out which images are new, shuffle and split them",0
do NOT sort the IDs to keep the shuffled order,0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
add empty category,0
"add all images that don't have annotations, with cat empty",0
print(detection_results.keys()),0
group the detections and gts by image id:,0
print(image_id),0
print(len(scores)),0
print(len(labels)),0
recall_thresh = 0.9,0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall]),0
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])",0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall]),0
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])",0
exp_name = 'eccv_train',0
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json',0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(len(detection_results['images'])),0
print(len(seqs)),0
"print(len(seqs[0]),len(seqs[",0
print(detection_results.keys()),0
group the detections by image id:,0
group the ground truth annotations by image id:,0
print(seq),0
"for image_id, dets in per_image_detections.iteritems():",0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
print(num_gts),0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
if sum(detected_class_labels)>0:,0
print('detected animal box'),0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])",0
"print(scores, tp_fp_labels)",0
num_total_gts += num_gts,0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
print('valid box'),0
"print(best_im, best_score)",0
exp_name = 'eccv_train',0
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json',0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
need to loop over confidence values,0
"for each value, check if any detections on the image are > conf",0
"If so, that image gets class ""animal""",0
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0",0
"for image_id, dets in per_image_detections.iteritems():",0
"calc prec, rec for this confidence thresh",0
group the ground truth annotations by image id:,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(detection_results.keys()),0
group the detections and gts by image id:,0
print(image_id),0
print(len(scores)),0
print(len(labels)),0
recall_thresh = 0.9,0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall]),0
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])",0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall]),0
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])",0
print(sorted_ap),0
plt.bar(sorted_ap),0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(detection_results.keys()),0
group the detections by image id:,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
print(gts),0
print(num_gts),0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[:2])",0
"print(scores, tp_fp_labels)",0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
print('detected animal box'),0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])",0
"print(scores, tp_fp_labels)",0
num_total_gts += num_gts,0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
print('valid box'),0
best_im = np.argmax(max_im_scores),0
"print(best_im, best_score)",0
"temp_labels = np.zeros(len(im_detection_labels),  dtype=np.int32)",0
"temp_scores = np.zeros(len(im_detection_scores), dtype=np.float32)",0
"for j in range(min(im_num_gts, len(im_detection_labels))):",0
temp_labels[j] = True #TODO: this currently only works for oneclass?,1
temp_scores[j] = best_score,0
im_detection_labels = temp_labels,0
im_detection_scores = temp_scores,0
num_total_gts+=im_num_gts,0
"print(len(detection_scores), len(detection_scores[0]), len(detection_scores[1]))",0
exp_name = 'small_balanced_cct',0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(seq),0
"for image_id, dets in per_image_detections.iteritems():",0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
seq_num_gts.append(num_gts),0
print(num_gts),0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
num_total_gts += 1,0
print('valid box'),0
"print(best_im, best_score)",0
print('no valid box'),0
if sum(seq_num_gts)>0:,0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
need to loop over confidence values,0
"for each value, check if any detections on the image are > conf",0
"If so, that image gets class ""animal""",0
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0",0
"calc prec, rec for this confidence thresh",0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
group the detections by image id:,0
group the ground truth annotations by image id:,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
print('detected animal box'),0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])",0
"print(scores, tp_fp_labels)",0
num_total_gts += num_gts,0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
print('valid box'),0
"print(best_im, best_score)",0
,0
evaluate_detections.py,0
,0
Adapted from analyze_detection.py which is now archived.,0
,0
%% Imports and constants,0
%% Functions,0
"labels input to compute_object_detection_metrics() needs to start at 0, not 1",0
num_detections = len(dets['boxes']),0
to prevent 'Invalid dimensions for box data.' error,0
this box will not match any detections,0
compute one-class precision/recall/average precision (if every box is just of an object class),0
%% Command-line driver,0
,0
detection_eval_utils.py,0
,0
Utility functions used in evaluate_detections.py,0
,0
group the ground truth annotations by image id,0
#####,0
,0
detect_and_predict_image.py,0
,0
"Functions to load a TensorFlow detection and a classification model, run inference,",0
"render bounding boxes on images, and write out the resulting",0
images (with bounding boxes and classes).,0
,0
"See the ""test driver"" cell for example invocation.",0
,0
,0
#####,0
"%% Constants, imports, environment",0
Minimum detection confidence for showing a bounding box on the output image,0
Stick this into filenames before the extension for the rendered result,0
Number of top-scoring classes to show at each bounding box,0
%% Core detection functions,0
Load images if they're not already numpy arrays,0
iImage = 0; image = images[iImage],0
"Load the image as an nparray of size h,w,nChannels",0
"There was a time when I was loading with PIL and switched to mpimg,",0
"but I can't remember why, and converting to RGB is a very good reason",0
"to load with PIL, since mpimg doesn't give any indication of color",0
"order, which basically breaks all .png files.",0
,0
"So if you find a bug related to using PIL, update this comment",0
"to indicate what it was, but also disable .png support.",0
image = mpimg.imread(image),0
This shouldn't be necessarily when loading with PIL and converting to RGB,0
Actual detection,0
...for each image,0
"Currently ""boxes"" is a list of length nImages, where each element is shaped as",0
,0
"1,nDetections,4",0
,0
"This implicitly banks on TF giving us back a fixed number of boxes, let's assert on this",0
to make sure this doesn't silently break in the future.,0
iBox = 0; box = boxes[iBox],0
"""scores"" is a length-nImages list of elements with size 1,nDetections",0
"""classes"" is a length-nImages list of elements with size 1,nDetections",0
,0
"Still as floats, but really representing ints",0
Squeeze out the empty axis,0
boxes is nImages x nDetections x 4,0
scores and classes are both nImages x nDetections,0
Get input and output tensors of classification model,0
"imsize = cur_image['width'], cur_image['height']",0
Select detections with a confidence larger 0.5,0
Get these boxes and convert normalized coordinates to pixel coordinates,0
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes",0
"However, we need to make sure that it box coordinates are still within the image",0
For convenience:,0
Create an array with contains the index of the corresponding crop_box for each selected box,0
i.e. [False False 0 False 1 2 3 False False],0
For each box,0
If this box should be classified,0
Run inference,0
if box should not be classified,0
...for each box,0
species_scores should have shape len(images) x len(boxes) x num_species,0
...for each image,0
...with tf.Session,0
with classification_graph,0
species_scores should have shape len(images) x len(boxes) x num_species,0
%% Rendering functions,0
Display the image,0
plt.show(),0
"top, left, bottom, right",0
,0
"x,y origin is the upper-left",0
Generate bounding box text,0
Choose color based on class,0
Location is the bottom-left of the rect,0
,0
Origin is the upper-left,0
iRight = x + w,0
iTop = y + h,0
Add the patch to the Axes,0
Add class description,0
First determine best location by finding the corner that is closest to the image center,0
relative corner coordinates,0
relative coordinates of image center,0
Compute pair-wise squared distance and get the index of the one with minimal distance,0
Get the corresponding coordinates ...,0
... and alignment for the text box,0
Plot the text box with background,0
...for each box,0
This is magic goop that removes whitespace around image plots (sort of),0
"plt.savefig(outputFileName, bbox_inches='tight', pad_inches=0.0, dpi=dpi, transparent=True)",0
os.startfile(outputFileName),0
...for each image,0
...def render_bounding_boxes,0
Load and run detector on target images,0
Read the name of all classes,0
remove empty lines,0
%% Interactive driver,0
%%,0
%%,0
%% File helper functions,0
%% Command-line driver,0
Hack to avoid running on already-detected images,1
,0
"Script for evaluating a frozen graph given a COCO-style dataset, which was generated by",0
the ../data_management/databases/classification/make_classification_dataset.py script.,0
This is useful for debugging as the accuracy reported by this script should match the,0
accuracy reported by the Tensorflow training.,0
,0
Check that all files exists for easier debugging,0
Load frozen graph,0
Collect tensors for input and output,0
Read image,0
Run inference,0
Check if correct,0
,0
Script for evaluating precision/recall on a two-class problem given,0
"a frozen graph and a COCO-style dataset, which was generated by",0
the ../data_management/databases/classification/make_classification_dataset.py script.,0
We assume the positive class is at index 0 (with a zero-based indexing),0
,0
Check that all files exists for easier debugging,0
Load frozen graph,0
Collect tensors for input and output,0
Read image,0
"with open(image_path, 'rb') as fi:",0
"image =  sess.run(tf.image.decode_jpeg(fi.read(), channels=3))",0
image = image / 255.,0
Run inference,0
predicted_class = np.argmax(predictions),0
Check if correct,0
if coco.imgToAnns[image_id][0]['category_id'] == predicted_class:,0
correct = correct + 1,0
"In matplotlib < 1.5, plt.fill_between does not have a 'step' argument",0
,0
Script for selecting testing images of a COCO-style dataset generated by the script,0
../data_management/databases/classification/make_classification_dataset.py in a consistent,0
manner and predicting the class for it.,0
,0
Mandatory parameters,0
Optional parameters,0
Validate parameters,0
Derived parameters,0
"We assume that the dataset was generated with the make_classification_dataset.py script,",0
hence the images should be located in the same folder as the json,0
Make seletion deterministic,0
Load frozen graph,0
Get dataset information,0
Get classes,0
...and the class list corresponding to the model outputs by assuming,0
that they are in order of their ids,0
Get images of each class,0
Shuffle the image list,0
Start the image sampling,0
"Set of avaiable class IDs, will be filled below",0
If there are still images left for that class,0
"Get image for the sampled class, we already shuffled the class images before so",0
we can simply pop(),0
Start prediction,0
Collect tensors for input and output,0
Read image,0
Run inference,0
Print output to log file,0
#####,0
,0
api_apply_classifier_single_node.py,0
,0
Takes the JSON file produced by the detection API and,0
classifies all boxes above a confidence threshold.,0
,0
#####,0
"%% Constants, imports, environment",0
Assumes that the root of the CameraTraps repo is on the PYTHONPATH,0
Minimum detection confidence for classifying an object,0
Number of top-scoring classes to show at each bounding box,0
Enlargment factor applied to boxes before passing them to the classifier,0
,0
Provides more context and can lead to better results,0
List of detection categories for which we will run the classification,0
,0
"Currently there are {""1"": ""animal"", ""2"": ""person"", ""4"": ""vehicle""}",0
,0
Should be a list of string-formatted ints.,0
Number of significant float digits in JSON output,0
%% Core detection functions,0
Read the name of all classes,0
remove empty lines,0
Create field with name *classification_categories*,0
Add classes using 0-based indexing,0
def add_classification_categories,0
Make sure we have the right json object,0
Get input and output tensors of classification model,0
For each image,0
Read image,0
"Scale pixel values to [0,1]",0
For each box,0
Skip detections with low confidence,0
Skip if detection category is not in whitelist,0
Skip if already classified,0
"Get current box in relative coordinates and format [x_min, y_min, width_of_box, height_of_box]",0
"Convert to [ymin, xmin, ymax, xmax] and store it as 1x4 numpy array so we can",0
re-use the generic multi-box padding code,0
Convert normalized coordinates to pixel coordinates,0
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes.",0
,0
"However, we need to make sure that it box coordinates are still within the image.",0
Get the first (and only) row as our bbox to classify,0
Get the image data for that box,0
Run inference,0
Add an empty list to the json for our predictions,0
Add the *num_annotated_classes* top scoring classes,0
...for each box,0
...for each image,0
...with tf.Session,0
with classification_graph,0
def classify_boxes,0
Load classification model,0
Load detector json,0
Add classes to detector_json,0
"Run classifier on all images, changes will be writting directly to the json",0
Write output json,0
def load_and_run_classifier,0
%% Command-line driver,0
,0
Script for selecting testing images of a COCO-style dataset generated by the script,0
../data_management/databases/classification/make_classification_dataset.py in a consistent,0
manner and predicting the class for it.,0
,0
Assumes the cameratraps repo root is on the path,0
Make seletion deterministic,0
Mandatory parameters,0
Optional parameters,0
Validate parameters,0
Tranfer parameters to post-processing format,0
Load frozen graph,0
Reading image list,0
Reading class list,0
Image sampling,0
Start prediction,0
Collect tensors for input and output,0
Array for collecting infos for rendering the html,0
Read image,0
"Scale pixel values to [0,1]",0
Run inference,0
Add links to all available classes,0
,0
predict_image.py,0
,0
"Given a pointer to a frozen detection graph, runs inference on a single image,",0
printing the top classes to the console,0
,0
%% Imports,0
%% Command-line processing,0
Check that all files exist for easier debugging,0
%% Inference,0
Load frozen graph,0
Load class list,0
Remove empty lines,0
Collect tensors for input and output,0
Read image,0
Run inference,0
Print output,0
,0
Mostly unmodified script for freezing a model,0
Added for convenience and for possible future optimizations,0
,0
Copyright 2015 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
'input_checkpoint' may be a prefix if we're using Saver V2 format,0
Remove all the explicit device specifications for this node. This helps to,0
make the graph more portable.,0
List of all partition variables. Because the condition is heuristic,0
"based, the list could include false positives.",0
This tensor doesn't exist in the graph (for example it's,0
'global_step' or a similar housekeeping element) so skip it.,0
`var_list` is required to be a map of variable names to Variable,0
tensors. Partition variables are Identity tensors that cannot be,0
handled by Saver.,0
Models that have been frozen previously do not contain Variables.,0
optimized_output_graph_def = optimize_for_inference_lib.optimize_for_inference(,0
"output_graph_def,",0
"input_node_names.replace("" "", """").split("",""),",0
"output_node_names.replace("" "", """").split("",""),",0
tf.float32.as_datatype_enum),0
Write GraphDef to file if output path has been given.,0
,0
"Creates a graph description, which is required to create a frozen graph.",0
Adapted from from ./tf-slim/export_inference_graph.py,0
Added preprocessing to the definition for easier handling,0
,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
tf.app.flags.DEFINE_integer(,0
"'batch_size', None,",0
"'Batch size for the exported model. Defaulted to ""None"" so batch size can '",0
'be specified at model runtime.'),0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
#####################,0
Optimization Flags #,0
#####################,0
######################,0
Learning Rate Flags #,0
######################,0
######################,0
Dataset Flags #,0
######################,0
####################,0
Fine-Tuning Flags #,0
####################,0
"Note: when num_clones is > 1, this will actually have each clone to go",0
over each epoch FLAGS.num_epochs_per_decay times. This is different,0
behavior from sync replicas and is expected to produce different results.,0
Warn the user if a checkpoint exists in the train_dir. Then we'll be,0
ignoring the checkpoint anyway.,0
TODO(sguada) variables.filter_variables(),1
######################,0
Config model_deploy #,0
######################,0
Create global_step,0
#####################,0
Select the dataset #,0
#####################,0
#####################,0
Select the network #,0
#####################,0
####################################,0
Select the preprocessing function #,0
####################################,0
#############################################################,0
Create a dataset provider that loads data from the dataset #,0
#############################################################,0
###################,0
Define the model #,0
###################,0
############################,0
Specify the loss function #,0
############################,0
Gather initial summaries.,0
"Gather update_ops from the first clone. These contain, for example,",0
the updates for the batch_norm variables created by network_fn.,0
Add summaries for end_points.,0
Add summaries for losses.,0
Add summaries for variables.,0
################################,0
Configure the moving averages #,0
################################,0
########################################,0
Configure the optimization procedure. #,0
########################################,0
"If sync_replicas is enabled, the averaging will be done in the chief",0
queue runner.,0
Update ops executed locally by trainer.,0
Variables to train.,0
and returns a train_tensor and summary_op,0
Add total_loss to summary.,0
Create gradient updates.,0
Add the summaries from the first clone. These contain the summaries,0
created by model_fn and either optimize_clones() or _gather_clone_loss().,0
Merge all summaries together.,0
##########################,0
Kicks off the training. #,0
##########################,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
#####################,0
Select the dataset #,0
#####################,0
###################,0
Select the model #,0
###################,0
#############################################################,0
Create a dataset provider that loads data from the dataset #,0
#############################################################,0
####################################,0
Select the preprocessing function #,0
####################################,0
###################,0
Define the model #,0
###################,0
Define the metrics:,0
Print the summaries to screen.,0
TODO(sguada) use num_epochs=1,1
This ensures that we make a single pass over all of the data.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Transform the image to floats.,0
"Randomly crop a [height, width] section of the image.",0
Randomly flip the image horizontally.,0
"Because these operations are not commutative, consider randomizing",0
the order their operation.,0
Subtract off the mean and divide by the variance of the pixels.,0
Transform the image to floats.,0
Resize and crop if needed.,0
Subtract off the mean and divide by the variance of the pixels.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Use tf.slice instead of crop_to_bounding box as it accepts tensors to,0
define the crop size.,0
Compute the rank assertions.,0
Create a random bounding box.,0
,0
Use tf.random_uniform and not numpy.random.rand as doing the former would,0
"generate random numbers at graph eval time, unlike the latter which",0
generates random numbers at graph definition time.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Pass the real x only to one of the func calls.,0
The random_* ops do not necessarily clamp.,0
"Each bounding box has shape [1, num_boxes, box coords] and",0
"the coordinates are ordered [ymin, xmin, ymax, xmax].",0
A large fraction of image datasets contain a human-annotated bounding,0
box delineating the region of the image containing the object of interest.,0
We choose to create a new bounding box for the object which is a randomly,0
distorted version of the human-annotated bounding box that obeys an,0
"allowed range of aspect ratios, sizes and overlap with the human-annotated",0
"bounding box. If no box is supplied, then we assume the bounding box is",0
the entire image.,0
Crop the image to the specified bounding box.,0
"Each bounding box has shape [1, num_boxes, box coords] and",0
"the coordinates are ordered [ymin, xmin, ymax, xmax].",0
Restore the shape since the dynamic slice based upon the bbox_size loses,0
the third dimension.,0
This resizing operation may distort the images because the aspect,0
ratio is not respected. We select a resize method in a round robin,0
fashion based on the thread number.,0
Note that ResizeMethod contains 4 enumerated resizing methods.,0
We select only 1 case for fast_mode bilinear.,0
Randomly flip the image horizontally.,0
Randomly distort the colors. There are 1 or 4 ways to do it.,0
Crop the central region of the image with an area containing 87.5% of,0
the original image.,0
Resize the image to the specified height and width.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TODO(sguada) docstring paragraph by (a) motivating the need for the file and,1
(b) defining clones.,0
TODO(sguada) describe the high-level components of model deployment.,1
"E.g. ""each model deployment is composed of several parts: a DeploymentConfig,",0
"which captures A, B and C, an input_fn which loads data.. etc",0
Set up DeploymentConfig,0
Create the global step on the device storing the variables.,0
Define the inputs,0
Define the optimizer.,0
Define the model including the loss.,0
Run training.,0
Namedtuple used to represent a clone during deployment.,0
"Namedtuple used to represent a DeployedModel, returned by deploy().",0
Default parameters for DeploymentConfig,0
Create clones.,0
The return value.,0
Individual components of the loss that will need summaries.,0
Compute and aggregate losses on the clone device.,0
Add the summaries out of the clone device block.,0
Only use regularization_losses for the first clone,0
Compute the total_loss summing all the clones_losses.,0
Sum the gradients across clones.,0
Gather initial summaries.,0
Create Clones.,0
"Gather update_ops from the first clone. These contain, for example,",0
the updates for the batch_norm variables created by model_fn.,0
Place the global step on the device storing the variables.,0
Compute the gradients for the clones.,0
Add summaries to the gradients.,0
Create gradient updates.,0
Only use regularization_losses for the first clone,0
Add the summaries from the first clone. These contain the summaries,0
created by model_fn and either optimize_clones() or _gather_clone_loss().,0
Add total_loss to summary.,0
Merge all summaries together.,0
Note that each grad_and_vars looks like the following:,0
"((grad_var0_clone0, var0), ... (grad_varN_cloneN, varN))",0
!/usr/bin/python,0
Copyright 2016 Google Inc. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Failed to find ""index"" occurrence of item.",0
pylint: disable=broad-except,0
pylint: enable=broad-except,0
Grab the 'index' annotation.,0
Some images contain bounding box annotations that,0
"extend outside of the supplied image. See, e.g.",0
n03127925/n03127925_147.xml,0
"Additionally, for some bounding boxes, the min > max",0
or the box is entirely outside of the image.,0
Example: <...>/n06470073/n00141669_6790.xml,0
Determine if the annotation is from an ImageNet Challenge label.,0
Note: There is a slight bug in the bounding box annotation data.,0
Many of the dog labels have the human label 'Scottish_deerhound',0
instead of the synset ID 'n02092002' in the bbox.label field. As a,0
"simple hack to overcome this issue, we only exclude bbox labels",1
*which are synset ID's* that do not match original synset label for,0
the XML file.,0
Guard against improperly specified boxes.,0
Note bbox.filename occasionally contains '%s' in the name. This is,0
data set noise that is fixed by just using the basename of the XML file.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The URLs where the MNIST data can be downloaded.,0
The names of the classes.,0
"First, process the training data:",0
"Next, process the testing data:",0
"Finally, write the labels file:",0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The URL where the Flowers data can be downloaded.,0
The number of images in the validation set.,0
Seed for repeatability.,0
The number of shards per dataset split.,0
Initializes function that decodes RGB JPEG data.,0
Read the filename:,0
Divide into train and test:,0
"First, convert the training and validation sets.",0
"Finally, write the labels file:",0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TODO(nsilberman): Add tfrecord file type once the script is updated.,1
"If set to false, will not try to set label_to_names in dataset",0
by reading them from labels.txt or github.,0
n01440764,0
n01443537,0
n02119247    black fox,0
n02119359    silver fox,0
pylint: disable=g-line-too-long,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The URL where the CIFAR data can be downloaded.,0
The number of training files.,0
The height and width of each image.,0
The names of the classes.,0
"First, process the training data:",0
"Next, process the testing data:",0
"Finally, write the labels file:",0
Copyright 2016 Google Inc. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The labels file contains a list of valid labels are held in this file.,0
Assumes that the file contains entries as such:,0
n01440764,0
n01443537,0
n01484850,0
where each line corresponds to a label expressed as a synset. We map,0
each synset contained in the file to an integer (based on the alphabetical,0
ordering). See below for details.,0
This file containing mapping from synset to human-readable label.,0
Assumes each line of the file looks like:,0
,0
n02119247    black fox,0
n02119359    silver fox,0
"n02119477    red fox, Vulpes fulva",0
,0
where each line corresponds to a unique mapping. Note that each line is,0
formatted as <synset>\t<human readable label>.,0
This file is the output of process_bounding_box.py,0
Assumes each line of the file looks like:,0
,0
"n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940",0
,0
where each line corresponds to one bounding box annotation associated,0
with an image. Each line can be parsed as:,0
,0
"<JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>",0
,0
Note that there might exist mulitple bounding box annotations associated,0
with an image file.,0
pylint: disable=expression-not-assigned,0
pylint: enable=expression-not-assigned,0
Create a single Session to run all image coding calls.,0
Initializes function that converts PNG to JPEG data.,0
Initializes function that converts CMYK JPEG data to RGB JPEG data.,0
Initializes function that decodes RGB JPEG data.,0
File list from:,0
https://groups.google.com/forum/embed/?place=forum/torch7#!topic/torch7/fOSTXHIESSU,0
File list from:,0
https://github.com/cytsai/ilsvrc-cmyk-image-list,0
Read the image file.,0
Clean the dirty data.,0
1 image is a PNG.,0
22 JPEG images are in CMYK colorspace.,0
Decode the RGB JPEG.,0
Check that image converted to RGB,0
Each thread produces N shards where N = int(num_shards / num_threads).,0
"For instance, if num_shards = 128, and the num_threads = 2, then the first",0
"thread would produce shards [0, 64).",0
"Generate a sharded version of the file name, e.g. 'train-00002-of-00010'",0
"Break all images into batches with a [ranges[i][0], ranges[i][1]].",0
Launch a thread for each batch.,0
Create a mechanism for monitoring when all threads are finished.,0
Create a generic TensorFlow-based utility for converting all image codings.,0
Wait for all the threads to terminate.,0
Leave label index 0 empty as a background class.,0
Construct the list of JPEG files and labels.,0
Shuffle the ordering of all image files in order to guarantee,0
random ordering of the images with respect to label in the,0
saved TFRecord files. Make the randomization repeatable.,0
Build a map from synset to human-readable label.,0
Run it!,0
Allowing None in the signature so that dataset_factory can use the default.,0
!/usr/bin/python,0
Copyright 2016 Google Inc. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Read in the 50000 synsets associated with the validation data set.,0
Make all sub-directories in the validation data dir.,0
Move all of the image to the appropriate sub-directory.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
"The dataset has classes with no images (empty and human), which have ID 0 and 1,",0
so we need to specify 49 here despite having only 47 classes with images,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Final pooling and prediction,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The current_stride variable keeps track of the effective stride of the,0
activations. This allows us to invoke atrous convolution whenever applying,0
the next residual unit would result in the activations having stride larger,0
than the target output_stride.,0
The atrous convolution rate parameter.,0
Move stride from the block's last unit to the end of the block.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Collect activations at the block's end before performing subsampling.,0
Subsampling of the block's output activations.,0
"The following implies padding='SAME' for pool1, which makes feature",0
alignment easier for dense prediction tasks. This is also used in,0
https://github.com/facebook/fb.resnet.torch. However the accompanying,0
code of 'Deep Residual Learning for Image Recognition' uses,0
padding='VALID' for pool1. You can switch to that choice by setting,0
"slim.arg_scope([slim.max_pool2d], padding='VALID').",0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Collect outputs for conv2d, fully_connected and max_pool2d",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"inputs has shape [batch, 224, 224, 3]",0
"inputs has shape [batch, 513, 513, 3]",0
We do not include batch normalization or activation functions in,0
conv1 because the first ResNet unit will perform these. Cf.,0
Appendix of [2].,0
This is needed because the pre-activation variant does not have batch,1
normalization or activation functions in the residual unit output. See,0
Appendix of [2].,0
Convert end_points_collection into a dictionary of end_points.,0
Global average pooling.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
=============================================================================,0
"These parameters come from the online port, which don't necessarily match",0
those in the paper.,0
TODO(nsilberman): confirm these values with Philip.,1
##########,0
Encoder #,0
##########,0
"No normalizer for the first encoder layers as per 'Image-to-Image',",0
Section 5.1.1,0
First layer doesn't use normalizer_fn,0
Last layer doesn't use activation_fn nor normalizer_fn,0
##########,0
Decoder #,0
##########,0
"Dropout is used at both train and test time as per 'Image-to-Image',",0
Section 2.1 (last paragraph).,0
The Relu comes BEFORE the upsample op:,0
Explicitly set the normalizer_fn to None to override any default value,0
"that may come from an arg_scope, such as pix2pix_arg_scope.",0
No normalization on the input layer.,0
Stride 1 on the last layer.,0
"1-dim logits, stride 1, no activation, no normalization.",0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
Alias,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pylint: disable=unused-import,0
pylint: enable=unused-import,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
If we are fine tuning a checkpoint we need to start at a lower learning,0
rate since we are farther along on training.,0
We can start quantizing immediately if we are finetuning.,0
We need to wait for the model to train a bit before we quantize if we are,0
training from scratch.,0
Call rewriter to produce graph with fake quant ops and folded batch norms,0
"quant_delay delays start of quantization till quant_delay steps, allowing",0
for better model accuracy.,0
Configure the learning rate using an exponential decay.,0
"When restoring from a floating point model, the min/max values for",0
quantized weights and activations are not present.,0
We instruct slim to ignore variables that are missing during restoration,0
by setting ignore_missing_vars=True,0
"If we are restoring from a floating point model, we need to initialize",0
the global step to zero for the exponential decay to result in,0
reasonable learning rates.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Orignaly, arg_scope = slim.arg_scope and layers = slim, now switch to more",0
update-to-date tf.contrib.* API.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Turns off fused batch norm.,0
collection containing the moving mean and moving variance.,0
"For inverted pyramid models, we start with gating switched off.",0
batch_size x 32 x 112 x 112 x 64,0
Separable conv is slow when used at first conv layer.,0
batch_size x 32 x 56 x 56 x 64,0
batch_size x 32 x 56 x 56 x 64,0
batch_size x 32 x 56 x 56 x 192,0
batch_size x 32 x 28 x 28 x 192,0
batch_size x 32 x 28 x 28 x 256,0
batch_size x 16 x 14 x 14 x 512,0
batch_size x 16 x 14 x 14 x 512,0
batch_size x 16 x 14 x 14 x 512,0
batch_size x 16 x 14 x 14 x 528,0
batch_size x 16 x 14 x 14 x 832,0
batch_size x 8 x 7 x 7 x 832,0
batch_size x 8 x 7 x 7 x 1024,0
Final pooling and prediction,0
Temporal average pooling.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
299 x 299 x 3,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 160,0
71 x 71 x 192,0
35 x 35 x 384,0
4 x Inception-A blocks,0
35 x 35 x 384,0
Reduction-A block,0
17 x 17 x 1024,0
7 x Inception-B blocks,0
17 x 17 x 1024,0
Reduction-B block,0
8 x 8 x 1536,0
3 x Inception-C blocks,0
Auxiliary Head logits,0
17 x 17 x 1024,0
Final pooling and prediction,0
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which",1
can be set to False to disable pooling here (as in resnet_*()).,0
8 x 8 x 1536,0
1 x 1 x 1536,0
1536,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN,1
setups need the gradient of gradient FusedBatchNormGrad.,0
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN,1
setups need the gradient of gradient FusedBatchNormGrad.,0
First upscaling is different because it takes the input vector.,0
Last layer has different normalizer and activation.,0
Convert to proper channels.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80,0
71 x 71 x 192,0
35 x 35 x 192,0
35 x 35 x 320,0
TODO(alemi): Register intermediate endpoints,1
"17 x 17 x 1088 if output_stride == 8,",0
33 x 33 x 1088 if output_stride == 16,0
TODO(alemi): register intermediate endpoints,1
TODO(gpapan): Properly support output_stride for the rest of the net.,1
8 x 8 x 2080,0
TODO(alemi): register intermediate endpoints,1
8 x 8 x 1536,0
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which",1
can be set to False to disable pooling here (as in resnet_*()).,0
Set weight_decay for weights in conv2d and fully_connected layers.,0
Set activation_fn and parameters for batch_norm.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
collection containing update_ops.,0
use fused batch norm if possible.,0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"inputs has shape [batch, 224, 224, 3]",0
"inputs has shape [batch, 513, 513, 3]",0
Use clip_by_value to simulate bandpass activation.,0
Convert end_points_collection into a dictionary of end_points.,0
Global average pooling.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"end_points will collect relevant activations for external use, for example",0
summaries or losses.,0
299 x 299 x 3,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80.,0
71 x 71 x 192.,0
35 x 35 x 192.,0
Inception blocks,0
mixed: 35 x 35 x 256.,0
mixed_1: 35 x 35 x 288.,0
mixed_2: 35 x 35 x 288.,0
mixed_3: 17 x 17 x 768.,0
mixed4: 17 x 17 x 768.,0
mixed_5: 17 x 17 x 768.,0
mixed_6: 17 x 17 x 768.,0
mixed_7: 17 x 17 x 768.,0
mixed_8: 8 x 8 x 1280.,0
mixed_9: 8 x 8 x 2048.,0
mixed_10: 8 x 8 x 2048.,0
Auxiliary Head logits,0
Shape of feature map before the final layer.,0
Final pooling and prediction,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 2048,0
2048,0
1000,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Reflection pad by 1 in spatial dimensions (axes 1, 2 = h, w) to make a 3x3",0
'valid' convolution produce an output with the same dimension as the,0
input.,0
This corrects 1 pixel offset for images with even width and height.,0
conv2d is left aligned and conv2d_transpose is right aligned for even,0
sized images (while doing 'SAME' padding).,0
Note: This doesn't reflect actual model in paper.,0
Neither dropout nor batch norm -> dont need is_training,0
##########,0
Encoder #,0
##########,0
7x7 input stage,0
##################,0
Residual Blocks #,0
##################,0
##########,0
Decoder #,0
##########,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"end_points will collect relevant activations for external use, for example",0
summaries or losses.,0
Used to find thinned depths for each layer.,0
Note that sizes in the comments below assume an input spatial size of,0
"224x224, however, the inputs can be of any size greater 32x32.",0
224 x 224 x 3,0
depthwise_multiplier here is different from depth_multiplier.,0
depthwise_multiplier determines the output channels of the initial,0
"depthwise conv (see docs for tf.nn.separable_conv2d), while",0
depth_multiplier controls the # channels of the subsequent 1x1,0
convolution. Must have,0
in_channels * depthwise_multipler <= out_channels,0
so that the separable convolution is not overparameterized.,0
Use a normal convolution instead of a separable convolution.,0
112 x 112 x 64,0
56 x 56 x 64,0
56 x 56 x 64,0
56 x 56 x 192,0
28 x 28 x 192,0
Inception module.,0
28 x 28 x 256,0
28 x 28 x 320,0
14 x 14 x 576,0
14 x 14 x 576,0
14 x 14 x 576,0
14 x 14 x 576,0
14 x 14 x 576,0
7 x 7 x 1024,0
7 x 7 x 1024,0
Final pooling and prediction,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 1024,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
=============================================================================,0
Tensorflow mandates these.,0
Conv and DepthSepConv namedtuple define layers of the MobileNet architecture,0
Conv defines 3x3 convolution layers,0
DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.,0
stride is the stride of the convolution,0
depth is the number of channels or filters in a layer,0
MOBILENETV1_CONV_DEFS specifies the MobileNet body,0
Used to find thinned depths for each layer.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
By passing filters=None,0
separable_conv2d produces only a depthwise convolution layer,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 1024,0
Set weight_decay for weights in Conv and DepthSepConv layers.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Turns off fused batch norm.,0
collection containing the moving mean and moving variance.,0
Final pooling and prediction,0
Temporal average pooling.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Orignaly, add_arg_scope = slim.add_arg_scope and layers = slim, now switch to",0
more update-to-date tf.contrib.* API.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
Only insert rate to params if rate > 1.,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Notes for training NASNet Cifar Model,0
-------------------------------------,0
batch_size: 32,0
learning rate: 0.025,0
cosine (single period) learning rate decay,0
auxiliary head loss weighting: 0.4,0
clip global norm of all gradients by 5,0
600 epochs with a batch size of 32,0
This is used for the drop path probabilities since it needs to increase,0
the drop out probability over the course of training.,0
Notes for training large NASNet model on ImageNet,0
-------------------------------------,0
batch size (per replica): 16,0
learning rate: 0.015 * 100,0
learning rate decay factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 100 replicas,0
auxiliary head loss weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Notes for training the mobile NASNet ImageNet model,0
-------------------------------------,0
batch size (per replica): 32,0
learning rate: 0.04 * 50,0
learning rate scaling factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 50 replicas,0
auxiliary head weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Shape of feature map before the final layer.,0
149 x 149 x 32,0
Run the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Find where to place the reduction cells or stride normal cells,0
Setup for building in the auxiliary head.,0
Run the cells,0
true_cell_num accounts for the stem cells,0
Final softmax layer,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Find where to place the reduction cells or stride normal cells,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Setup for building in the auxiliary head.,0
Run the cells,0
true_cell_num accounts for the stem cells,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Final softmax layer,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Calculate the total number of cells in the network.,0
There is no distinction between reduction and normal cells in PNAS so the,0
total number of cells is equal to the number normal cells plus the number,0
of stem cells (two by default).,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Calculate the total number of cells in the network.,0
There is no distinction between reduction and normal cells in PNAS so the,0
total number of cells is equal to the number normal cells plus the number,0
of stem cells (two by default).,0
Configuration for the PNASNet-5 model.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"The cap for tf.clip_by_value, it's hinted from the activation distribution",0
"that the majority of activation values are in the range [-6, 6].",0
Skip path 1,0
Skip path 2,0
"First pad with 0's on the right and bottom, then shift the filter to",0
include those 0's that were added.,0
"If odd number of filters, add an additional one to the second path.",0
Concat and apply BN,0
Set the prev layer to the current layer if it is none,0
Check to be sure prev layer stuff is setup correctly,0
num_or_size_splits=1,0
Apply conv operations,0
Combine hidden states using 'add'.,0
Add hiddenstate to the list of hiddenstates we can choose from,0
Dont stride if this is not one of the original hiddenstates,0
"Check if a stride is needed, then use a strided 1x1 here",0
Determine if a reduction should be applied to make the number of,0
filters match.,0
Return the concat of all the states,0
Scale keep prob by layer number,0
The added 2 is for the reduction cells,0
Decrease the keep probability over time,0
,0
Script for generating a two-class dataset in COCO format for training an obscured image classifier,0
,0
Requires Python >= 3.6 because of the glob ** expression,0
,0
Collect images and labels,0
"Labels: clean = 0, obscured = 1",0
,0
top_folders_to_bottom.py,0
,0
Given a base folder with files like:,0
,0
A/1/2/a.jpg,0
B/3/4/b.jpg,0
,0
"...moves the top-level folders to the bottom in a new output folder, i.e., creates:",0
,0
1/2/A/a.jpg,0
3/4/B/b.jpg,0
,0
"In practice, this is used to make this:",0
,0
animal/camera01/image01.jpg,0
,0
...look like:,0
,0
camera01/animal/image01.jpg,0
,0
%% Constants and imports,0
%% Support functions,0
%% Main functions,0
Find top-level folder,0
Find file/folder names,0
Move or copy,0
...def process_file(),0
Enumerate input folder,0
Convert absolute paths to relative paths,0
Standardize delimiters,0
Make sure each input file maps to a unique output file,0
relative_filename = relative_files[0],0
Loop,0
...def top_folders_to_bottom(),0
%% Interactive driver,0
%%,0
%%,0
%% Command-line driver,0
"python top_folders_to_bottom.py ""g:\temp\separated_images"" ""g:\temp\separated_images_inverted"" --n_threads 100",0
Convert to an options object,0
%% Constants and imports,0
Assumes ai4eutils is on the python path (https://github.com/Microsoft/ai4eutils),0
Assumes the cameratraps repo root is on the path,0
%% Options,0
"Make sure there is no overlap between the two sets, because this will cause",0
issues in the code,0
## Required inputs,0
## Options,0
Can be a folder or a SAS URL,0
# These apply only when we're doing ground-truth comparisons,0
Classes we'll treat as negative,0
,0
"Include the token ""#NO_LABELS#"" to indicate that an image with no annotations",0
should be considered empty.,0
Classes we'll treat as neither positive nor negative,0
"A list of output sets that we should count, but not render images for.",0
,0
"Typically used to preview sets with lots of empties, where you don't want to",0
"subset but also don't want to render 100,000 empty images.",0
,0
"detections, non_detections",0
"detections_animal, detections_person, detections_vehicle",0
"By default, choose a confidence threshold based on the detector version",0
Used for summary statistics only,0
"Number of images to sample, -1 for ""all images""",0
"Random seed for sampling, or None",0
Optionally separate detections into categories (animal/vehicle/human),0
,0
Currently only supported when ground truth is unavailable,0
Optionally replace one or more strings in filenames with other strings;,0
useful for taking a set of results generated for one folder structure,0
and applying them to a slightly different folder structure.,0
Allow bypassing API output loading when operating on previously-loaded,0
results,0
Should we also split out a separate report about the detections that were,0
just below our main confidence threshold?,0
,0
Currently only supported when ground truth is unavailable,0
Control rendering parallelization,0
...PostProcessingOptions,0
#%% Helper classes and functions,0
Anything greater than this isn't clearly positive or negative,0
image has annotations suggesting both negative and positive,0
"image is not annotated or is annotated with 'unknown', 'unlabeled', ETC.",0
image has not yet been assigned a state,0
"In some analyses, we add an additional class that lets us look at",0
detections just below our main confidence threshold,0
count the # of images with each type of DetectionStatus,0
Check whether this image has:,0
- unknown / unassigned-type labels,0
- negative-type labels,0
"- positive labels (i.e., labels that are neither unknown nor negative)",0
"assert has_unknown_labels is False, '{} has unknown labels'.format(annotations)",0
If there are no image annotations...,0
n_negative += 1,0
im['_detection_status'] = DetectionStatus.DS_NEGATIVE,0
"If the image has more than one type of labels, it's ambiguous",0
"note: bools are automatically converted to 0/1, so we can sum",0
"After the check above, we can be sure it's only one of positive,",0
"negative, or unknown.",0
,0
Important: do not merge the following 'unknown' branch with the first,0
"'unknown' branch above, where we tested 'if len(categories) == 0'",0
,0
If the image has only unknown labels,0
If the image has only negative labels,0
If the images has only positive labels,0
"Annotate the category, if it is unambiguous",0
...for each image,0
...mark_detection_status(),0
"Leaving code in place for reading from blob storage, may support this",0
in the future.,0
resize is to display them in this notebook or in the HTML more quickly,0
os.path.isfile() is slow when mounting remote directories; much faster,0
to just try/except on the image open.,0
return '',0
Render images to a flat folder... we can use os.sep here because we've,0
already normalized paths,0
"Create class labels like ""gt_1"" or ""gt_27""",0
"for i_box,box in enumerate(ground_truth_boxes):",0
gt_classes.append('_' + str(box[-1])),0
"errno.ENAMETOOLONG doesn't get thrown properly on Windows, so",0
we awkwardly check against a hard-coded limit,0
Use slashes regardless of os,0
Optionally add links back to the original images,0
...render_bounding_boxes,0
Count items in each category,0
Optionally sort by filename before writing to html,0
Write the individual HTML files,0
...prepare_html_subpages(),0
Get unique categories above the threshold for this image,0
Render an image (with no ground truth information),0
"This is a list of [class,confidence] pairs, sorted by confidence",0
"If we either don't have a confidence threshold, or we've met our",0
confidence threshold,0
...if this detection has classification info,0
...for each detection,0
...def render_image_no_gt(),0
This should already have been normalized to either '/' or '\',0
...def render_image_with_gt(),0
%% Main function,0
#%% Expand some options for convenience,0
#%% Prepare output dir,0
#%% Load ground truth if available,0
Mark images in the ground truth as positive or negative,0
#%% Load detection (and possibly classification) results,0
"If the caller hasn't supplied results, load them",0
Determine confidence thresholds if necessary,0
Remove failed rows,0
Convert keys and values to lowercase,0
"Add column 'pred_detection_label' to indicate predicted detection status,",0
not separating out the classes,0
#%% Pull out descriptive metadata,0
This is rare; it only happens during debugging when the caller,0
is supplying already-loaded API results.,0
"#%% If we have ground truth, remove images we can't match to ground truth",0
#%% Sample images for visualization,0
#%% Fork here depending on whether or not ground truth is available,0
"If we have ground truth, we'll compute precision/recall and sample tp/fp/tn/fn.",0
,0
Otherwise we'll just visualize detections/non-detections.,0
#%% Detection evaluation: compute precision/recall,0
numpy array of detection probabilities,0
"numpy array of bools (0.0/1.0), and -1 as null value",0
Don't include ambiguous/unknown ground truth in precision/recall analysis,0
"For completeness, include the result at a confidence threshold of 1.0",0
Compute and print summary statistics,0
Thresholds go up throughout precisions/recalls/thresholds; find the last,0
value where recall is at or above target.  That's our precision @ target recall.,0
"np.where returns a tuple of arrays, but in this syntax where we're",0
"comparing an array with a scalar, there will only be one element.",0
Convert back to a list,0
Flatten the confusion matrix,0
"#%% Collect classification results, if they exist",0
Mapping of classnames to idx for the confusion matrix.,0
,0
"The lambda is actually kind of a hack, because we use assume that",1
the following code does not reassign classname_to_idx,0
Confusion matrix as defaultdict of defaultdict,0
,0
"Rows / first index is ground truth, columns / second index is predicted category",0
iDetection = 0; fn = detector_files[iDetection]; print(fn),0
"If this image has classification predictions, and an unambiguous class",0
"annotated, and is a positive image...",0
"The unambiguous category, we make this a set for easier handling afterward",0
"Compute the accuracy as intersection of union,",0
i.e. (# of categories in both prediciton and GT),0
divided by (# of categories in either prediction or GT,0
,0
"In case of only one GT category, the result will be 1.0, if",0
prediction is one category and this category matches GT,0
,0
"It is 1.0/(# of predicted top-1 categories), if the GT is",0
one of the predicted top-1 categories.,0
,0
"It is 0.0, if none of the predicted categories is correct",0
Distribute this accuracy across all predicted categories in the,0
confusion matrix,0
...for each file in the detection results,0
If we have classification results,0
Build confusion matrix as array from classifier_cm,0
Print some statistics,0
Prepare confusion matrix output,0
Get confusion matrix as string,0
Get fixed-size classname for each idx,0
Prepend class name on each line and add to the top,0
Print formatted confusion matrix,0
"Actually don't, this gets really messy in all but the widest consoles",0
Plot confusion matrix,0
To manually add more space at bottom: plt.rcParams['figure.subplot.bottom'] = 0.1,0
,0
"Add 0.5 to figsize for every class. For two classes, this will result in",0
"fig = plt.figure(figsize=[4,4])",0
...if we have classification results,0
#%% Render output,0
Write p/r table to .csv file in output directory,0
Write precision/recall plot to .png file in output directory,0
#%% Sampling,0
Sample true/false positives/negatives with correct/incorrect top-1,0
classification and render to html,0
Accumulate html image structs (in the format expected by write_html_image_lists),0
"for each category, e.g. 'tp', 'fp', ..., 'class_bird', ...",0
Add default entries by accessing them for the first time,0
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]",0
"Each element will be a three-tuple with elements file,max_conf,detections",0
"Assemble the information we need for rendering, so we can parallelize without",0
dealing with Pandas,0
i_row = 0; row = images_to_visualize.iloc[0],0
Filenames should already have been normalized to either '/' or '\',0
Map all the rendering results in the list rendering_results into the,0
dictionary images_html,0
Prepare the individual html image files,0
Write index.html,0
Show links to each GT class,0
,0
We could do this without classification results; currently we don't.,0
Add links to all available classes,0
Close body and html tags,0
...for each image,0
"#%% Otherwise, if we don't have ground truth...",0
#%% Sample detections/non-detections,0
Accumulate html image structs (in the format expected by write_html_image_list),0
for each category,0
Add default entries by accessing them for the first time,0
"Maps detection categories - e.g. ""human"" - to result set names, e.g.",0
"""detections_human""",0
Add a set of results for each category and combination of categories,0
Create output directories,0
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]",0
"list of 3-tuples with elements (file, max_conf, detections)",0
"Assemble the information we need for rendering, so we can parallelize without",0
dealing with Pandas,0
i_row = 0; row = images_to_visualize.iloc[0],0
Filenames should already have been normalized to either '/' or '\',0
"render_image_no_gt(file_info,detection_categories_to_results_name,",0
"detection_categories,classification_categories)",0
Map all the rendering results in the list rendering_results into the,0
dictionary images_html,0
Prepare the individual html image files,0
Write index.html,0
"We can't just sum these, because image_counts includes images in both their",0
detection and classification classes,0
total_images = sum(image_counts.values()),0
Don't print classification classes here; we'll do that later with a slightly,0
different structure,0
Add links to all available classes,0
os.startfile(output_html_file),0
...if we do/don't have ground truth,0
...process_batch_results,0
%% Interactive driver(s),0
%%,0
options.unlabeled_classes = ['human'],0
os.start(ppresults.output_html_file),0
%% Command-line driver,0
,0
load_api_results.py,0
,0
Loads the output of the batch processing API (json) into a pandas dataframe.,0
,0
Also functions to group entries by seq_id.,0
,0
Includes the deprecated functions that worked with the old CSV API output format.,0
,0
%% Constants and imports,0
%% Functions for grouping by sequence_id,0
example,0
input 'file': 'SER/S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG',0
output 'id': 'S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG',0
%% Functions for loading the result as a Pandas DataFrame,0
Validate that this is really a detector output file,0
Fields in the API output json other than 'images',0
Normalize paths to simplify comparisons later,0
"image['file'] = image['file'].replace('\\','/')",0
Replace some path tokens to match local paths to original blob structure,0
"If this is a newer file that doesn't include maximum detection confidence values,",0
"add them, because our unofficial internal dataframe format includes this.",0
Pack the json output into a Pandas DataFrame,0
Sanity-check that this is really a detector output file,0
Normalize paths to simplify comparisons later,0
De-serialize detections,0
Optionally replace some path tokens to match local paths to the original blob structure,0
string_to_replace = list(options.detector_output_filename_replacements.keys())[0],0
"TODO: hit some silly issues with vectorized str() and escaped characters, vectorize",1
this later.,0
,0
"detection_results['image_path'].str.replace(string_to_replace,replacement_string)",0
iRow = 0,0
%% Imports,0
Assumes ai4eutils is on the python path (https://github.com/Microsoft/ai4eutils),0
%% Constants and support classes,0
We will confirm that this matches what we load from each file,0
Process-based parallelization isn't supported yet,0
%% Main function,0
"Warn the user if some ""detections"" might not get rendered",0
#%% Validate inputs,0
#%% Load both result sets,0
assert results_a['detection_categories'] == default_detection_categories,0
assert results_b['detection_categories'] == default_detection_categories,0
#%% Make sure they represent the same set of images,0
#%% Find differences,0
"Each of these maps a filename to a two-element list (the image in set A, the image in set B)",0
,0
"Right now, we only handle a very simple notion of class transition, where the detection",0
of maximum confidence changes class *and* both images have an above-threshold detection.,0
fn = filenames_a[0],0
We shouldn't have gotten this far if error_on_non_matching_lists is set,0
det = im_a['detections'][0],0
...for each filename,0
#%% Sample and plot differences,0
"Render two sets of results (i.e., a comparison) for a single",0
image.,0
...def render_image_pair(),0
fn = image_filenames[0],0
...def render_detection_comparisons(),0
"For each category, generate comparison images and the",0
comparison HTML page.,0
,0
category = 'common_detections',0
Choose detection pairs we're going to render for this category,0
...for each category,0
#%% Write the top-level HTML file content,0
...def compare_batch_results(),0
%% Interactive driver,0
%% KRU,0
%% Command-line driver,0
# TODO,1
,0
"Merge high-confidence detections from one results file into another file,",0
when the target file does not detect anything on an image.,0
,0
Does not currently attempt to merge every detection based on whether individual,0
detections are missing; only merges detections into images that would otherwise,0
be considered blank.,0
,0
"If you want to literally merge two .json files, see combine_api_outputs.py.",0
,0
%% Constants and imports,0
%% Structs,0
Don't bother merging into target images where the max detection is already,0
higher than this threshold,0
"If you want to merge only certain categories, specify one",0
(but not both) of these.,0
%% Main function,0
im = output_data['images'][0],0
"Determine whether we should be processing all categories, or just a subset",0
of categories.,0
i_source_file = 0; source_file = source_files[i_source_file],0
source_im = source_data['images'][0],0
detection_category = list(detection_categories)[0],0
"This is already a detection, no need to proceed looking for detections to",0
transfer,0
Boxes are x/y/w/h,0
source_sizes = [det['bbox'][2]*det['bbox'][3] for det in source_detections_this_category_raw],0
Only look at boxes below the size threshold,0
...for each detection category,0
"print('Adding {} detections to image {}'.format(len(detections_to_transfer),image_filename))",0
Update the max_detection_conf field (if present),0
...for each image,0
...for each source file,0
%% Test driver,0
%%,0
%% Command-line driver (TODO),0
,0
separate_detections_into_folders.py,0
,0
## Overview,0
,0
"Given a .json file with batch processing results, separate the files in that",0
"set of results into folders that contain animals/people/vehicles/nothing,",0
according to per-class thresholds.,0
,0
"Image files are copied, not moved.",0
,0
,0
## Output structure,0
,0
Preserves relative paths within each of those folders; cannot be used with .json,0
files that have absolute paths in them.,0
,0
"For example, if your .json file has these images:",0
,0
a/b/c/1.jpg,0
a/b/d/2.jpg,0
a/b/e/3.jpg,0
a/b/f/4.jpg,0
a/x/y/5.jpg,0
,0
And let's say:,0
,0
"* The results say that the first three images are empty/person/vehicle, respectively",0
"* The fourth image is above threshold for ""animal"" and ""person""",0
* The fifth image contains an animal,0
,0
* You specify an output base folder of c:\out,0
,0
You will get the following files:,0
,0
c:\out\empty\a\b\c\1.jpg,0
c:\out\people\a\b\d\2.jpg,0
c:\out\vehicles\a\b\e\3.jpg,0
c:\out\animal_person\a\b\f\4.jpg,0
c:\out\animals\a\x\y\5.jpg,0
,0
## Rendering bounding boxes,0
,0
"By default, images are just copied to the target output folder.  If you specify --render_boxes,",0
bounding boxes will be rendered on the output images.  Because this is no longer strictly,0
"a copy operation, this may result in the loss of metadata.  More accurately, this *may*",0
result in the loss of some EXIF metadata; this *will* result in the loss of IPTC/XMP metadata.,0
,0
Rendering boxes also makes this script a lot slower.,0
,0
## Classification-based separation,0
,0
"If you have a results file with classification data, you can also specify classes to put",0
"in their own folders, within the ""animals"" folder, like this:",0
,0
"--classification_thresholds ""deer=0.75,cow=0.75""",0
,0
"So, e.g., you might get:",0
,0
c:\out\animals\deer\a\x\y\5.jpg,0
,0
"In this scenario, the folders within ""animals"" will be:",0
,0
"deer, cow, multiple, unclassified",0
,0
"""multiple"" in this case only means ""deer and cow""; if an image is classified as containing a",0
"bird and a bear, that would end up in ""unclassified"", since the folder separation is based only",0
on the categories you provide at the command line.,0
,0
"No classification-based separation is done within the animal_person, animal_vehicle, or",0
animal_person_vehicle folders.,0
,0
%% Constants and imports,0
Occasionally we have near-zero confidence detections associated with COCO classes that,0
didn't quite get squeezed out of the model in training.  As long as they're near zero,0
"confidence, we just ignore them.",0
%% Options class,0
"Dictionary mapping categories (plus combinations of categories, and 'empty') to output folders",0
Populated only when using classification results,0
"Originally specified as a string, converted to a dict mapping name:threshold",0
...__init__(),0
...class SeparateDetectionsIntoFoldersOptions,0
%% Support functions,0
Find the maximum confidence for each category,0
,0
det = detections[0],0
"For zero-confidence detections, we occasionally have leftover goop",0
from COCO classes,0
assert det['conf'] < invalid_category_epsilon,0
...for each detection on this image,0
Count the number of thresholds exceeded,0
...for each category,0
If this is above multiple thresholds,0
"TODO: handle species-based separation in, e.g., the animal_person case",1
"Are we making species classification folders, and is this an animal?",0
Do we need to put this into a specific species folder?,0
Find the animal-class detections that are above threshold,0
Count the number of classification categories that are above threshold for at,0
least one detection,0
d = valid_animal_detections[0],0
classification = d['classifications'][0],0
"Do we have a threshold for this category, and if so, is",0
this classification above threshold?,0
...for each classification,0
...for each detection,0
...if we have to deal with classification subfolders,0
...if we have 0/1/more categories above threshold,0
...if this is/isn't a failure case,0
Skip this image if it's empty and we're not processing empty images,0
"At this point, this image is getting copied; we may or may not also need to",0
draw bounding boxes.,0
Do a simple copy operation if we don't need to render any boxes,0
Open the source image,0
"Render bounding boxes for each category separately, beacuse",0
we allow different thresholds for each category.,0
"When we're not using classification folders, remove classification",0
information to maintain standard detection colors.,0
...for each category,0
Read EXIF metadata,0
"Write output with EXIF metadata if available, and quality='keep' if this is a JPEG",0
"image.  Unfortunately, neither parameter likes ""None"", so we get a slightly",0
icky cascade of if's here.,0
Also see:,0
,0
https://discuss.dizzycoding.com/determining-jpg-quality-in-python-pil/,0
,0
...for more ways to preserve jpeg quality if quality='keep' doesn't do the trick.,0
...if we don't/do need to render boxes,0
...def process_detections(),0
%% Main function,0
Create output folder if necessary,0
Load detection results,0
Map class names to output folders,0
Create all combinations of categories,0
category_name = category_names[0],0
Do we have a custom threshold for this category?,0
Create folder mappings for each category,0
Create the actual folders,0
"Handle species classification thresholds, if specified",0
"E.g. deer=0.75,cow=0.75",0
token = tokens[0],0
...for each token,0
...if classification thresholds are still in string format,0
Validate the classes in the threshold list,0
...if we need to deal with classification categories,0
i_image = 14; im = images[i_image]; im,0
...for each image,0
...def separate_detections_into_folders,0
%% Interactive driver,0
%%,0
%%,0
%%,0
%% Testing various command-line invocations,0
"With boxes, no classification",0
"No boxes, no classification (default)",0
"With boxes, with classification",0
"No boxes, with classification",0
%% Command-line driver,0
"python api\batch_processing\postprocessing\separate_detections_into_folders.py ""d:\temp\rspb_mini.json"" ""d:\temp\demo_images\rspb_2018_2019_mini"" ""d:\temp\separation_test"" --nthreads 2",0
Convert to an options object,0
"print('{} {}'.format(v,name))",0
List of category numbers to use in separation; uses all categories if None,0
"Can be ""size"", ""width"", or ""height""",0
For each image...,0
,0
im = images[0],0
d = im['detections'][0],0
Are there really any detections here?,0
Is this a category we're supposed to process?,0
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing#detector-outputs,0
...for each detection,0
...for each image,0
...def categorize_detections_by_size(),0
,0
add_max_conf.py,0
,0
"The MD output format included a ""max_detection_conf"" field with each image",0
up to and including version 1.2; it was removed as of version 1.3 (it's,0
redundant with the individual detection confidence values).,1
,0
"Just in case someone took a dependency on that field, this script allows you",0
to add it back to an existing .json file.,0
,0
%% Imports and constants,0
%% Main function,0
%% Driver,0
#######,0
,0
remove_repeat_detections.py,0
,0
"Used after running find_repeat_detections, then manually filtering the results,",0
to create a final filtered output file.,0
,0
"If you want to use this script, we recommend that you read the user's guide:",0
,0
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms,0
,0
#######,0
%% Constants and imports,0
%% Main function,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
#######,0
,0
repeat_detections_core.py,0
,0
Core utilities shared by find_repeat_detections and remove_repeat_detections.,0
,0
#######,0
%% Imports and environment,0
"from ai4eutils; this is assumed to be on the path, as per repo convention",0
"""PIL cannot read EXIF metainfo for the images""",0
"""Metadata Warning, tag 256 had too many entries: 42, expected 1""",0
%% Constants,0
%% Classes,0
Relevant for rendering the folder of images for filtering,0
,0
"imageBase can also be a SAS URL, in which case some error-checking is",0
disabled.,0
Don't consider detections with confidence lower than this as suspicious,0
Don't consider detections with confidence higher than this as suspicious,0
What's the IOU threshold for considering two boxes the same?,0
How many occurrences of a single location (as defined by the IOU threshold),0
are required before we declare it suspicious?,0
"Ignore ""suspicious"" detections larger than some size; these are often animals",0
taking up the whole image.  This is expressed as a fraction of the image size.,0
"Ignore ""suspicious"" detections smaller than some size",0
Ignore folders with more than this many images in them,0
A list of classes we don't want to treat as suspicious. Each element is an int.,0
Load detections from a filter file rather than finding them from the detector output,0
".json file containing detections, should be called detectionIndex.json in the filtering_* folder",0
produced in the first pass,0
(optional) List of filenames remaining after deletion of identified,0
repeated detections that are actually animals.  This should be a flat,0
"text file, one relative filename per line.  See enumerate_images().",0
Turn on/off optional outputs,0
Determines whether bounding-box rendering errors (typically network errors) should,0
be treated as failures,0
Box rendering options,0
State variables,0
"Replace filename tokens after reading, useful when the directory structure",0
has changed relative to the structure the detector saw,0
How many folders up from the leaf nodes should we be going to aggregate images?,0
An optional function that takes a string (an image file name) and returns,0
"a string (the corresponding  folder ID), typically used when multiple folders",0
actually correspond to the same camera in a manufacturer-specific way (e.g.,0
a/b/c/RECONYX100 and a/b/c/RECONYX101 may really be the same camera).,0
Include/exclude specific folders... only one of these may be,0
"specified; ""including"" folders includes *only* those folders.",0
"Optionally show *other* detections (i.e., detections other than the",0
one the user is evaluating) in a light gray,0
"If bRenderOtherDetections is True, what color should we use to render the",0
(hopefully pretty subtle) non-target detections?,0
,0
"In theory I'd like these ""other detection"" rectangles to be partially",0
"transparent, but this is not straightforward, and the alpha is ignored",0
"here.  But maybe if I leave it here and wish hard enough, someday it",0
will work.,0
,0
otherDetectionsColors = ['dimgray'],0
Sort detections within a directory so nearby detections are adjacent,0
"in the list, for faster review.",0
,0
"Can be None, 'xsort', or 'clustersort'",0
,0
* None sorts detections chronologically by first occurrence,0
* 'xsort' sorts detections from left to right,0
* 'clustersort' clusters detections and sorts by cluster,0
Only relevant if smartSort == 'clustersort',0
"The data table (Pandas DataFrame), as loaded from the input json file via",0
load_api_results(),0
"The other fields in the input json file, loaded via load_api_results()",0
The data table after modification,0
dict mapping folder names to whole rows from the data table,0
dict mapping filenames to rows in the master table,0
"An array of length nDirs, where each element is a list of DetectionLocation",0
objects for that directory that have been flagged as suspicious,0
"This is a bit of a hack right now, but for future-proofing, I don't want to call this",1
"to retrieve anything other than the highest-confidence detection, and I'm assuming this",0
"is already sorted, so assert() that.",0
It's not clear whether it's better to use instances[0].bbox or self.bbox,1
"here... they should be very similar, unless iouThreshold is very low.",0
self.bbox is a better representation of the overal DetectionLocation.,0
%% Helper functions,0
"We store detetions as x/y/w/h, rtree and pyqtree use l/b/r/t",0
"We store detetions as x/y/w/h, rtree and pyqtree use l/b/r/t",0
%% Sort a list of candidate detections to make them visually easier to review,0
Just sort by the X location of each box,0
"Prepare a list of points to represent each box,",0
that's what we'll use for clustering,0
Upper-left,0
"points.append([det.bbox[0],det.bbox[1]])",0
Center,0
"Labels *could* be any unique labels according to the docs, but in practice",0
they are unique integers from 0:nClusters,0
Make sure the labels are unique incrementing integers,0
Store the label assigned to each cluster,0
"Now sort the clusters by their x coordinate, and re-assign labels",0
so the labels are sortable,0
"Compute the centroid for debugging, but we're only going to use the x",0
"coordinate.  This is the centroid of points used to represent detections,",0
which may be box centers or box corners.,0
old_cluster_label_to_new_cluster_label[old_cluster_label] =\,0
new_cluster_labels[old_cluster_label],0
%% Look for matches (one directory),0
List of DetectionLocations,0
candidateDetections = [],0
Create a tree to store candidate detections,0
For each image in this directory,0
,0
iDirectoryRow = 0; row = rows.iloc[iDirectoryRow],0
,0
"iDirectoryRow is a pandas index, so it may not start from zero;",0
"for debugging, we maintain i_iteration as a loop index.",0
print('Searching row {} of {} (index {}) in dir {}'.\,0
"format(i_iteration,len(rows),iDirectoryRow,dirName))",0
Don't bother checking images with no detections above threshold,0
"Array of dicts, where each element is",0
{,0
"'category': '1',  # str value, category ID",0
"'conf': 0.926,  # confidence of this detections",0
,0
"# (x_min, y_min) is upper-left, all in relative coordinates",0
"'bbox': [x_min, y_min, width_of_box, height_of_box]",0
,0
},0
For each detection in this image,0
"This is no longer strictly true; I sometimes run RDE in stages, so",0
some probabilities have already been made negative,0
,0
assert confidence >= 0.0 and confidence <= 1.0,0
Optionally exclude some classes from consideration as suspicious,0
Is this detection too big to be suspicious?,0
print('Illegal zero-size bounding box on image {}'.format(filename)),0
These are relative coordinates,0
print('Ignoring very small detection with area {}'.format(area)),0
print('Ignoring very large detection with area {}'.format(area)),0
This will return candidates of all classes,0
For each detection in our candidate list,0
Don't match across categories,0
Is this a match?,0
"If so, add this example to the list for this detection",0
We *don't* break here; we allow this instance to possibly,0
match multiple candidates.  There isn't an obvious right or,0
wrong here.,0
...for each detection on our candidate list,0
"If we found no matches, add this to the candidate list",0
candidateDetections.append(candidate),0
pyqtree,0
...for each detection,0
...for each row,0
Get all candidate detections,0
print('Found {} candidate detections for folder {}'.format(,0
"len(candidateDetections),dirName))",0
"For debugging only, it's convenient to have these sorted",0
as if they had never gone into a tree structure.  Typically,0
this is in practce a sort by filename.,0
...def find_matches_in_directory(dirName),0
"%% Update the detection table based on suspicious results, write .csv output",0
"An array of length nDirs, where each element is a list of DetectionLocation",0
objects for that directory that have been flagged as suspicious,0
For each directory,0
For each suspicious detection group in this directory,0
For each instance of this suspicious detection,0
This should match the bbox for the detection event,0
The bbox for this instance should be almost the same as the bbox,0
"for this detection group, where ""almost"" is defined by the IOU",0
threshold.,0
if iou < options.iouThreshold:,0
"print('IOU warning: {},{}'.format(iou,options.iouThreshold))",0
Make sure the bounding box matches,0
"Make the probability negative, if it hasn't been switched by",0
another bounding box,0
...for each instance,0
...for each detection,0
...for each directory,0
Update maximum probabilities,0
For each row...,0
No longer strictly true; sometimes I run RDE on RDE output,0
assert maxPOriginal >= 0,0
We should only be making detections *less* likely in this process,0
"If there was a meaningful change, count it",0
"Negative probabilities should be the only reason maxP changed, so",0
we should have found at least one negative value if we reached,0
this point.,0
...if there was a meaningful change to the max probability for this row,0
...for each row,0
If we're also writing output...,0
"...def update_detection_table(RepeatDetectionResults,options)",0
%% Main function,0
#%% Input handling,0
Validate some options,0
Load the filtering file,0
Load the same options we used when finding repeat detections,0
...except for things that explicitly tell this function not to,0
find repeat detections.,0
...if we're loading from an existing filtering file,0
Check early to avoid problems with the output folder,0
"Load file to a pandas dataframe.  Also populates 'max_detection_conf', even if it's",0
not present in the .json file.,0
detectionResults[detectionResults['failure'].notna()],0
"Before doing any real work, make sure we can *probably* access images",0
"This is just a cursory check on the first image, but it heads off most",0
"problems related to incorrect mount points, etc.  Better to do this before",0
spending 20 minutes finding repeat detections.,0
#%% Separate files into directories,0
This will be a map from a directory name to smaller data frames,0
This is a mapping back into the rows of the original table,0
iRow = 0; row = detectionResults.iloc[0],0
Create a new DataFrame with just this row,0
rowsByDirectory[dirName] = pd.DataFrame(row),0
...for each unique detection,0
Convert lists of rows to proper DataFrames,0
#% Look for matches (or load them from file),0
"Are we actually looking for matches, or just loading from a file?",0
length-nDirs list of lists of DetectionLocation objects,0
We're actually looking for matches...,0
"We get slightly nicer progress bar behavior using threads, by passing a pbar",0
object and letting it get updated.  We can't serialize this object across,0
processes.,0
#%% Find suspicious locations based on match results,0
For each directory,0
,0
iDir = 51,0
A list of DetectionLocation objects,0
A list of DetectionLocation objects,0
occurrenceList is a list of file/detection pairs,0
Sort the above-threshold detections for easier review,0
...for each directory,0
If we're just loading detections from a file...,0
"We're skipping detection-finding, but to see which images are actually legit false",0
"positives, we may be looking for physical files or loading from a text file.",0
For each directory,0
iDir = 0; detections = suspiciousDetections[0],0
,0
"suspiciousDetections is an array of DetectionLocation objects,",0
one per directory.,0
For each detection that was present before filtering,0
iDetection = 0; detection = detections[iDetection],0
Are we checking the directory to see whether detections were actually false,0
"positives, or reading from a list?",0
Is the image still there?,0
"If not, remove this from the list of suspicious detections",0
...for each detection,0
...for each directory,0
...if we are/aren't finding detections (vs. loading from file),0
Create filtering directory,0
iDir = 0; suspiciousDetectionsThisDir = suspiciousDetections[iDir],0
suspiciousDetectionsThisDir is a list of DetectionLocation objects,0
iDetection = 0; detection = suspiciousDetectionsThisDir[0],0
Sort instances in descending order by confidence,0
Choose the highest-confidence index,0
Should we render (typically in a very light color) detections,0
*other* than the one we're highlighting here?,0
Render other detections first (typically in a thin+light box),0
Now render the example detection (on top of at least one,0
of the other detections),0
This converts the *first* instance to an API standard detection;,0
"because we just sorted this list in descending order by confidence,",0
this is the highest-confidence detection.,0
...if we are/aren't rendering other bounding boxes,0
...for each detection in this folder,0
...for each folder,0
Write out the detection index,0
...if we're writing filtering info,0
...find_repeat_detections(),0
#######,0
,0
find_repeat_detections.py,0
,0
"If you want to use this script, we recommend that you read the user's guide:",0
,0
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms,0
,0
"Really, don't try to run this script without reading the user's guide, you'll think",0
it's more magical than it is.,0
,0
"This script looks through a sequence of detections in the API output json file, and finds",0
"candidates that might be ""repeated false positives"", i.e. that random branch that the",0
detector thinks is an animal/person/vehicle.,0
,0
"Typically after running this script, you would do a manual step to remove",0
"true positives, then run remove_repeat_detections to produce a final output file.",0
,0
There's no way that statement was self-explanatory; see the user's guide.,0
,0
#######,0
%% Constants and imports,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
Convert to an options object,0
,0
If a request has been sent to AML for batch scoring but the monitoring thread of the API was,0
"interrupted (uncaught exception or having to re-start the API container), we could manually",0
"aggregate results from each shard using this script, assuming all jobs submitted to AML have finished.",0
,0
Need to have set environment variables STORAGE_ACCOUNT_NAME and STORAGE_ACCOUNT_KEY to those of the,0
"storage account backing the API. Also need to adjust the INTERNAL_CONTAINER, AML_CONTAINER and",0
AML_CONFIG fields in api_core/orchestrator_api/api_config.py to match the instance of the API that this,0
request was submitted to.,0
,0
May need to change the import statement in api_core/orchestrator_api/orchestrator.py,0
"""from sas_blob_utils import SasBlob"" to",0
"""from .sas_blob_utils import SasBlob"" to not confuse with the module in AI4Eutils;",0
"and change ""import api_config"" to",0
"""from api.batch_processing.api_core.orchestrator_api import api_config""",0
Execute this script from the root of the repository. You may need to add the repository to PYTHONPATH.,0
"list_jobs_submitted cannot be serialized (""can't pickle _thread.RLock objects ""), but",0
do not need it for aggregating results,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%% helper classes and functions,0
TODO log exception when we have more telemetry,1
TODO check that the expiry date of input_container_sas is at least a month,0
into the future,0
"if no permission specified explicitly but has an access policy, assumes okay",0
TODO - check based on access policy as well,1
return current UTC time as a string in the ISO 8601 format (so we can query by,0
timestamp in the Cosmos DB job status table.,0
example: '2021-02-08T20:02:05.699689Z',0
"image_paths will have length at least 1, otherwise would have ended before this step",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
a job moves from created to running/problem after the Batch Job has been submitted,0
"job_id should be unique across all instances, and is also the partition key",0
TODO do not read the entry first to get the call_params when the Cosmos SDK add a,1
patching functionality:,0
https://feedback.azure.com/forums/263030-azure-cosmos-db/suggestions/6693091-be-able-to-do-partial-updates-on-document,0
need to retain other fields in 'status' to be able to restart monitoring thread,0
retain existing fields; update as needed,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
sentinel should change if new configurations are available,0
configs have not changed,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Gunicorn logger handler will get attached if needed in server.py,0
set for all tasks in the job,0
cannot execute the scoring script that is in the mounted directory; has to be copied to cwd,0
not luck giving the commandline arguments via formatted string - set as env vars instead,0
form shards of images and assign each shard to a Task,0
for persisting stdout and stderr,0
persist stdout and stderr (will be removed when node removed),0
paths are relative to the Task working directory,0
can also just upload on failure,0
first try submitting Tasks,0
retry submitting Tasks,0
Change the Job's on_all_tasks_complete option to 'terminateJob' so the Job's status changes automatically,0
after all submitted tasks are done,0
This is so that we do not take up the quota for active Jobs in the Batch account.,0
return type: TaskAddCollectionResult,0
actually we should probably only re-submit if it's a server_error,0
docs: # https://docs.microsoft.com/en-us/rest/api/batchservice/odata-filters-in-batch#list-tasks,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%% Flask app,0
reference: https://trstringer.com/logging-flask-gunicorn-the-manageable-way/,0
%% Helper classes,0
%% Flask endpoints,0
required params,0
can be an URL to a file not hosted in an Azure blob storage container,0
"if use_url, then images_requested_json_sas is required",0
optional params,0
check model_version is among the available model versions,0
check request_name has only allowed characters,0
optional params for telemetry collection - logged to status table for now as part of call_params,0
All API instances / node pools share a quota on total number of active Jobs;,0
we cannot accept new Job submissions if we are at the quota,0
required fields,0
request_status is either completed or failed,0
the create_batch_job thread will stop when it wakes up the next time,0
"Fix for Zooniverse - deleting any ""-"" characters in the job_id",0
"If the status is running, it could be a Job submitted before the last restart of this",0
"API instance. If that is the case, we should start to monitor its progress again.",0
WARNING model_version could be wrong (a newer version number gets written to the output file) around,0
"the time that  the model is updated, if this request was submitted before the model update",0
and the API restart; this should be quite rare,0
conform to previous schemes,0
%% undocumented endpoints,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Gunicorn logger handler will get attached if needed in server.py,0
request_name and request_submission_timestamp are for appending to,0
output file names,0
image_paths can be a list of strings (Azure blob names or public URLs),0
"or a list of length-2 lists where each is a [image_id, metadata] pair",0
Case 1: listing all images in the container,0
- not possible to have attached metadata if listing images in a blob,0
list all images to process,0
+ 1 so if the number of images listed > MAX_NUMBER_IMAGES_ACCEPTED_PER_JOB,0
we will know and not proceed,0
Case 2: user supplied a list of images to process; can include metadata,0
filter down to those conforming to the provided prefix and accepted suffixes (image file types),0
prefix is case-sensitive; suffix is not,0
"Although urlparse(p).path preserves the extension on local paths, it will not work for",0
"blob file names that contains ""#"", which will be treated as indication of a query.",0
"If the URL is generated via Azure Blob Storage, the ""#"" char will be properly encoded",0
apply the first_n and sample_n filters,0
OK if first_n > total number of images,0
sample by shuffling image paths and take the first sample_n images,0
"upload the image list to the container, which is also mounted on all nodes",0
all sharding and scoring use the uploaded list,0
now request_status moves from created to running,0
an extra field to allow the monitoring thread to restart after an API restart: total number of tasks,0
also record the number of images to process for reporting,0
start the monitor thread with the same name,0
"both succeeded and failed tasks are marked ""completed"" on Batch",0
"preserving format from before, but SAS URL to 'failed_images' and 'images' are no longer provided",0
"failures should be contained in the output entries, indicated by an 'error' field",0
"when people download this, the timestamp will have : replaced by _",0
check if the result blob has already been written (could be another instance of the API / worker thread),0
"and if so, skip aggregating and uploading the results, and just generate the SAS URL, which",0
could be needed still if the previous request_status was `problem`.,0
upload the output JSON to the Job folder,0
%% Helper functions *copied* from ct_utils.py and visualization/visualization_utils.py,0
"Determine the factor, which shifts the decimal point of x",0
just behind the last significant digit,0
"Shift decimal point by multiplicatipon with factor, flooring, and",0
division by factor,0
PIL.Image.convert() returns a converted copy of this image,0
alter orientation as needed according to EXIF tag 0x112 (274) for Orientation,0
https://gist.github.com/dangtrinhnt/a577ece4cbe5364aad28,0
https://www.media.mit.edu/pia/Research/deepview/exif.html,0
"%% TFDetector class, an unmodified *copy* of the class in detection/tf_detector.py,",0
so we do not have to import the packages required by run_detector.py,0
Number of decimal places to round to for confidence and bbox coordinates,0
"MegaDetector was trained with batch size of 1, and the resizing function is a part",0
of the inference graph,0
An enumeration of failure reasons,0
"change from [y1, x1, y2, x2] to [x1, y1, width, height]",0
convert numpy floats to Python floats,0
need to change the above line to the following if supporting a batch size > 1 and resizing to the same size,0
"np_images = [np.asarray(image, np.uint8) for image in images]",0
"images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)",0
performs inference,0
our batch size is 1; need to loop the batch dim if supporting batch size > 1,0
%% Scoring script,0
determine if there is metadata attached to each image_id,0
information to determine input and output locations,0
other parameters for the task,0
test that we can write to output path; also in case there is no image to process,0
list images to process,0
"items in this list can be strings or [image_id, metadata]",0
model path,0
"Path to .pb TensorFlow detector model file, relative to the",0
models/megadetector_copies folder in mounted container,0
score the images,0
,0
manage_video_batch.py,0
,0
Notebook-esque script to manage the process of running a local batch of videos,0
through MD.  Defers most of the heavy lifting to manage_local_batch.py .,0
,0
%% Imports and constants,0
%% Split videos into frames,0
"%% List frame files, break into folders",0
Find unique (relative) folders,0
fn = frame_files[0],0
%% List videos,0
%% Check for videos that are missing entirely,0
list(folder_to_frame_files.keys())[0],0
video_filenames[0],0
fn = video_filenames[0],0
%% Check for videos with very few frames,0
%% Print the list of videos that are problematic,0
%% Process images like we would for any other camera trap job,0
"...typically using manage_local_batch.py, but do this however you like, as long",0
as you get a results file at the end.,0
,0
"If you do RDE, remember to use the second folder from the bottom, rather than the",0
bottom-most folder.,0
%% Convert frame results to video results,0
%% Confirm that the videos in the .json file are what we expect them to be,0
%% Scrap,0
%% Test a possibly-broken video,0
%% List videos in a folder,0
%% Imports,0
%% Constants,0
%% Classes,0
class variables,0
"instance variables, in order of when they are typically set",0
Leaving this commented out to remind us that we don't want this check here; let,0
the API fail on these images.  It's a huge hassle to remove non-image,0
files.,0
,0
for path_or_url in images_list:,0
if not is_image_file_or_url(path_or_url):,0
raise ValueError('{} is not an image'.format(path_or_url)),0
Commented out as a reminder: don't check task status (which is a rest API call),0
in __repr__; require the caller to explicitly request status,0
"status=getattr(self, 'status', None))",0
estimate # of failed images from failed shards,0
Download all three JSON urls to memory,0
Remove files that were submitted but don't appear to be images,0
assert all(is_image_file_or_url(s) for s in submitted_images),0
Diff submitted and processed images,0
Confirm that the procesed images are a subset of the submitted images,0
https://www.geeksforgeeks.org/break-list-chunks-size-n-python/,0
%% Interactive driver,0
%%,0
%%,0
%%,0
%% Imports and constants,0
%% Constants I set per script,0
## Required,0
base_output_folder_name = os.path.expanduser('~/postprocessing/' + organization_name_short),0
Shared Access Signature (SAS) tokens for the Azure Blob Storage container.,0
Leading question mark is optional.,0
,0
The read-only token is used for accessing images; the write-enabled token is,0
used for writing file lists.,0
## Typically left as default,0
"Pre-pended to all folder names/prefixes, if they're defined below",0
"This is how we break the container up into multiple taskgroups, e.g., for",0
separate surveys. The typical case is to do the whole container as a single,0
taskgroup.,0
"If your ""folders"" are really logical folders corresponding to multiple folders,",0
map them here,0
"A list of .json files to load images from, instead of enumerating.  Formatted as a",0
"dictionary, like folder_prefixes.",0
This is only necessary if you will be performing postprocessing steps that,0
"don't yet support SAS URLs, specifically the ""subsetting"" step, or in some",0
cases the splitting of files into multiple output directories for,0
empty/animal/vehicle/people.,0
,0
"For those applications, you will need to mount the container to a local drive.",0
For this case I recommend using rclone whether you are on Windows or Linux;,0
rclone is much easier than blobfuse for transient mounting.,0
,0
"But most of the time, you can ignore this.",0
"Supported model_versions: '4', '3', '4_prelim'",0
,0
Also available at the /supported_model_versions and /default_model_version,0
endpoints,0
,0
"Unless you have any specific reason to set this to a non-default value, leave",0
"it at the default, which as of 2020.04.28 is MegaDetector 4.1",0
,0
"additional_task_args = {""model_version"":""4_prelim""}",0
,0
"file_lists_by_folder will contain a list of local JSON file names,",0
each JSON file contains a list of blob names corresponding to an API taskgroup,0
"%% Derived variables, path setup",0
local folders,0
Turn warnings into errors if more than this many images are missing,0
%% Support functions,0
"scheme, netloc, path, query, fragment",0
%% Read images from lists or enumerate blobs to files,0
folder_name = folder_names[0],0
"Load file lists for this ""folder""",0
,0
file_list = input_file_lists[folder][0],0
Write to file,0
A flat list of blob paths for each folder,0
folder_name = folder_names[0],0
"If we don't/do have multiple prefixes to enumerate for this ""folder""",0
"If this is intended to be a folder, it needs to end in '/', otherwise",0
files that start with the same string will match too,0
...for each prefix,0
Write to file,0
...for each folder,0
%% Some just-to-be-safe double-checking around enumeration,0
Make sure each folder has at least one image matched; the opposite is usually a sign of a copy/paste issue,0
...for each image,0
...for each prefix,0
...for each folder,0
Make sure each image comes from one of our folders; the opposite is usually a sign of a bug up above,0
...for each prefix,0
...for each folder,0
...for each image,0
%% Divide images into chunks for each folder,0
The JSON file at folder_chunks[i][j] corresponds to task j of taskgroup i,0
list_file = file_lists_by_folder[0],0
"%% Create taskgroups and tasks, and upload image lists to blob storage",0
periods not allowed in task names,0
%% Generate API calls for each task,0
clipboard.copy(request_strings[0]),0
clipboard.copy('\n\n'.join(request_strings)),0
%% Run the tasks (don't run this cell unless you are absolutely sure!),0
I really want to make sure I'm sure...,0
%% Estimate total time,0
Around 0.8s/image on 16 GPUs,0
%% Manually create task groups if we ran the tasks manually,0
%%,0
"%% Write task information out to disk, in case we need to resume",0
%% Status check,0
print(task.id),0
%% Resume jobs if this notebook closes,0
%% For multiple tasks (use this only when we're merging with another job),0
%% For just the one task,0
%% Load into separate taskgroups,0
p = task_cache_paths[0],0
%% Typically merge everything into one taskgroup,0
"%% Look for failed shards or missing images, start new tasks if necessary",0
List of lists of paths,0
i_taskgroup = 0; taskgroup = taskgroups[i_taskgroup];,0
"Make a copy, because we append to taskgroup",0
i_task = 0; task = tasks[i_task],0
Each taskgroup corresponds to one of our folders,0
Check that we have (almost) all the images,0
Now look for failed images,0
Write it out as a flat list as well (without explanation of failures),0
...for each task,0
...for each task group,0
%% Pull results,0
i_taskgroup = 0; taskgroup = taskgroups[i_taskgroup]; task = taskgroup[0],0
Each taskgroup corresponds to one of our folders,0
...for each task,0
...for each task group,0
%% Combine results from task groups into final output files,0
Check that we have (almost) all the images,0
The only reason we should ever have a repeated request is the case where an,0
"image was missing and we reprocessed it, or where it failed and later succeeded",0
"There may be non-image files in the request list, ignore those",0
Something has gone bonkers if there are images in the results that,0
aren't in the request,0
...for each folder,0
%% Post-processing (no ground truth),0
i_folder = 0; folder_name_raw = folder_names[i_folder],0
"Omit some pages from the output, useful when animals are rare",0
%% Manual processing follows,0
,0
"Everything after this should be considered mostly manual, and no longer includes",0
looping over folders.,0
,0
%% RDE (sample directory collapsing),0
"In this example, the camera created folders called ""100EK113"", ""101EK113"", etc., for every N images",0
"%% Repeat detection elimination, phase 1",0
"Deliberately leaving these imports here, rather than at the top, because this cell is not",0
typically executed,0
To invoke custom collapsing of folders for a particular manufacturer's naming scheme,0
options.customDirNameFunction = remove_overflow_folders,0
Exclude people and vehicles from RDE,0
"options.excludeClasses = [2,3]",0
%% Manual RDE step,0
# DELETE THE VALID DETECTIONS ##,0
%% Re-filtering,0
%% Post-processing (post-RDE),0
i_folder = 0; folder_name_raw = folder_names[i_folder],0
"Omit some pages from the output, useful when animals are rare",0
"print('No RDE file available for {}, skipping'.format(folder_name))",0
continue,0
%% Create a new category for large boxes,0
%% Subsetting,0
i_folder = 0; folder_name = folders[i_folder],0
%% String replacement,0
%% Folder splitting,0
i_folder = 0; folder_name_raw = folder_names[i_folder],0
api_output_file = folder_name_to_combined_output_file[folder_name],0
%% Imports and constants,0
from ai4eutils,0
To specify a non-default confidence threshold for including detections in the .json file,0
Turn warnings into errors if more than this many images are missing,0
Only relevant when we're using a single GPU,0
"Specify a target image size when running MD... strongly recommended to leave this at ""None""",0
Only relevant when running on CPU,0
OS-specific script line continuation character,0
OS-specific script comment character,0
"Prefer threads on Windows, processes on Linux",0
"This is for things like image rendering, not for MegaDetector",0
Should we use YOLOv5's val.py instead of run_detector_batch.py?,1
Directory in which to run val.py.  Only relevant if use_yolo_inference_scripts is True.,0
Should we remove intermediate files used for running YOLOv5's val.py?,0
,0
Only relevant if use_yolo_inference_scripts is True.,0
Should we apply YOLOv5's augmentation?  Only allowed when use_yolo_inference_scripts,0
is True.,0
%% Constants I set per script,0
Optional descriptor,0
model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v5.0.0/md_v5b.0.0.pt'),0
model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v4.1.0/md_v4.1.0.pb'),0
"Number of jobs to split data into, typically equal to the number of available GPUs",0
Only used to print out a time estimate,0
"%% Derived variables, constant validation, path setup",0
%% Enumerate files,0
%% Load files from prior enumeration,0
%% Divide images into chunks,0
%% Estimate total time,0
%% Write file lists,0
%% Generate commands,0
"A list of the scripts tied to each GPU, as absolute paths.  We'll write this out at",0
the end so each GPU's list of commands can be run at once.  Generally only used when,0
"running lots of small batches via YOLOv5's val.py, which doesn't support checkpointing.",0
i_task = 0; task = task_info[i_task],0
Generate the script to run MD,0
Check whether this output file exists,0
Generate the script to resume from the checkpoint (only supported with MD inference code),0
...for each task,0
Write out a script for each GPU that runs all of the commands associated with,0
that GPU.  Typically only used when running lots of little scripts in lieu,0
of checkpointing.,0
...for each GPU,0
%% Run the tasks,0
%%% Run the tasks (commented out),0
i_task = 0; task = task_info[i_task],0
"This will write absolute paths to the file, we'll fix this later",1
...for each chunk,0
...if False,0
"%% Load results, look for failed or missing images in each task",0
i_task = 0; task = task_info[i_task],0
im = task_results['images'][0],0
...for each task,0
%% Merge results files and make images relative,0
im = combined_results['images'][0],0
%% Post-processing (pre-RDE),0
options.sample_seed = 0,0
"Omit some pages from the output, useful when animals are rare",0
%% RDE (sample directory collapsing),0
"In this example, the camera created folders called ""100EK113"", ""101EK113"", etc., for every N images",0
%%,0
%%,0
%%,0
relativePath = image_filenames[0],0
"%% Repeat detection elimination, phase 1",0
"Deliberately leaving these imports here, rather than at the top, because this",0
cell is not typically executed,0
options.minSuspiciousDetectionSize = 0.05,0
This will cause a very light gray box to get drawn around all the detections,0
we're *not* considering as suspicious.,0
options.lineThickness = 5,0
options.boxExpansion = 8,0
To invoke custom collapsing of folders for a particular manufacturer's naming scheme,0
options.customDirNameFunction = remove_overflow_folders,0
Exclude people and vehicles from RDE,0
"options.excludeClasses = [2,3]",0
options.maxImagesPerFolder = 50000,0
options.includeFolders = ['a/b/c'],0
options.excludeFolder = ['a/b/c'],0
"Can be None, 'xsort', or 'clustersort'",0
import clipboard; clipboard.copy(os.path.dirname(suspiciousDetectionResults.filterFile)),0
path_utils.open_file(os.path.dirname(suspiciousDetectionResults.filterFile)),0
%% Manual RDE step,0
# DELETE THE VALID DETECTIONS ##,0
%% Re-filtering,0
%% Post-processing (post-RDE),0
options.sample_seed = 0,0
"Omit some pages from the output, useful when animals are rare",0
"%% Run MegaClassifier (actually, write out a script that runs MegaClassifier)",0
"This is just passed along to the metadata in the output file, it has no impact",0
on how the classification scripts run.,0
#%% Set up environment,0
commands.append('cd CameraTraps/classification\n'),0
commands.append('conda activate cameratraps-classifier\n'),0
#%% Crop images,0
fn = input_files[0],0
#%% Run classifier,0
fn = input_files[0],0
#%% Remap classifier outputs,0
fn = input_files[0],0
#%% Merge classification and detection outputs,0
fn = input_files[0],0
#%% Write  out classification script,0
"%% Run a non-MegaClassifier classifier (i.e., a classifier with no output mapping)",0
"This is just passed along to the metadata in the output file, it has no impact",0
on how the classification scripts run.,0
#%% Set up environment,0
#%% Crop images,0
fn = input_files[0],0
#%% Run classifier,0
fn = input_files[0],0
#%% Merge classification and detection outputs,0
fn = input_files[0],0
#%% Write everything out,0
%% Run the classifier(s) via the .sh script(s) or batch file(s) we just wrote,0
...,0
%% Within-image classification smoothing,0
,0
Only count detections with a classification confidence threshold above,0
"*classification_confidence_threshold*, which in practice means we're only",0
looking at one category per detection.,0
,0
If an image has at least *min_detections_above_threshold* such detections,0
"in the most common category, and no more than *max_detections_secondary_class*",0
"in the second-most-common category, flip all detections to the most common",0
category.,0
,0
"Optionally treat some classes as particularly unreliable, typically used to overwrite an",0
"""other"" class.",0
,0
This cell also removes everything but the non-dominant classification for each detection.,0
,0
How many detections do we need above the classification threshold to determine a dominant category,0
for an image?,0
"Even if we have a dominant class, if a non-dominant class has at least this many classifications",0
"in an image, leave them alone.",0
"If the dominant class has at least this many classifications, overwrite ""other"" classifications",0
What confidence threshold should we use for assessing the dominant category in an image?,0
Which classifications should we even bother over-writing?,0
Detection confidence threshold for things we count when determining a dominant class,0
Which detections should we even bother over-writing?,0
"Before we do anything else, get rid of everything but the top classification",0
for each detection.,0
...for each detection in this image,0
...for each image,0
im = d['images'][0],0
...for each classification,0
...if there are classifications for this detection,0
...for each detection,0
"Handle a quirky special case: if the most common category is ""other"" and",0
"it's ""tied"" with the second-most-common category, swap them",0
secondary_count = category_to_count[keys[1]],0
The 'secondary count' is the most common non-other class,0
If we have at least *min_detections_to_overwrite_other* in a category that isn't,0
"""other"", change all ""other"" classifications to that category",0
...for each classification,0
...if there are classifications for this detection,0
...for each detection,0
"...if we should overwrite all ""other"" classifications",0
"At this point, we know we have a dominant category; change all other above-threshold",0
"classifications to that category.  That category may have been ""other"", in which",0
case we may have already made the relevant changes.,0
det = detections[0],0
...for each classification,0
...if there are classifications for this detection,0
...for each detection,0
...for each image,0
...for each file we want to smooth,0
"%% Post-processing (post-classification, post-within-image-smoothing)",0
classification_detection_file = classification_detection_files[1],0
%% Read EXIF data from all images,0
%% Prepare COCO-camera-traps-compatible image objects for EXIF results,0
import dateutil,0
"This is a standard format for EXIF datetime, and dateutil.parser",0
doesn't handle it correctly.,0
return dateutil.parser.parse(s),0
exif_result = exif_results[0],0
Currently we assume that each leaf-node folder is a location,0
"We collected this image this century, but not today, make sure the parsed datetime",0
jives with that.,0
,0
The latter check is to make sure we don't repeat a particular pathological approach,0
"to datetime parsing, where dateutil parses time correctly, but swaps in the current",0
date when it's not sure where the date is.,0
...for each exif image result,0
%% Assemble into sequences,0
Make a list of images appearing at each location,0
im = image_info[0],0
%% Load classification results,0
Map each filename to classification results for that file,0
%% Smooth classification results over sequences (prep),0
These are the only classes to which we're going to switch other classifications,0
Only switch classifications to the dominant class if we see the dominant class at least,0
this many times,0
"If we see more than this many of a class that are above threshold, don't switch those",0
classifications to the dominant class.,0
"If the ratio between a dominant class and a secondary class count is greater than this,",0
"regardless of the secondary class count, switch those classificaitons (i.e., ignore",0
max_secondary_class_classifications_above_threshold_for_class_smoothing).,0
,0
"This may be different for different dominant classes, e.g. if we see lots of cows, they really",0
"tend to be cows.  Less so for canids, so we set a higher ""override ratio"" for canids.",0
"If there are at least this many classifications for the dominant class in a sequence,",0
"regardless of what that class is, convert all 'other' classifications (regardless of",0
confidence) to that class.,0
"If there are at least this many classifications for the dominant class in a sequence,",0
"regardless of what that class is, classify all previously-unclassified detections",0
as that class.,0
Only count classifications above this confidence level when determining the dominant,0
"class, and when deciding whether to switch other classifications.",0
Confidence values to use when we change a detection's classification (the,0
original confidence value is irrelevant at that point),0
%% Smooth classification results over sequences (supporting functions),0
im = images_this_sequence[0],0
det = results_this_image['detections'][0],0
Only process animal detections,0
Only process detections with classification information,0
"We only care about top-1 classifications, remove everything else",0
Make sure the list of classifications is already sorted by confidence,0
...and just keep the first one,0
"Confidence values should be sorted within a detection; verify this, and ignore",0
...for each detection in this image,0
...for each image in this sequence,0
...top_classifications_for_sequence(),0
Count above-threshold classifications in this sequence,0
Sort the dictionary in descending order by count,0
"Handle a quirky special case: if the most common category is ""other"" and",0
"it's ""tied"" with the second-most-common category, swap them.",0
...def count_above_threshold_classifications(),0
%% Smooth classifications at the sequence level (main loop),0
Break if this token is contained in a filename (set to None for normal operation),0
i_sequence = 0; seq_id = all_sequences[i_sequence],0
Count top-1 classifications in this sequence (regardless of confidence),0
Handy debugging code for looking at the numbers for a particular sequence,0
Count above-threshold classifications for each category,0
"If our dominant category ID isn't something we want to smooth to, don't mess around with this sequence",0
"# Smooth ""other"" classifications ##",0
"By not re-computing ""max_count"" here, we are making a decision that the count used",0
"to decide whether a class should overwrite another class does not include any ""other""",0
classifications we changed to be the dominant class.  If we wanted to include those...,0
,0
sorted_category_to_count = count_above_threshold_classifications(classifications_this_sequence),0
max_count = get_first_value_from_sorted_dictionary(sorted_category_to_count),0
assert dominant_category_id == get_first_key_from_sorted_dictionary(sorted_category_to_count),0
# Smooth non-dominant classes ##,0
Don't flip classes to the dominant class if they have a large number of classifications,0
"Don't smooth over this class if there are a bunch of them, and the ratio",0
if primary to secondary class count isn't too large,0
Default ratio,0
Does this dominant class have a custom ratio?,0
# Smooth unclassified detections ##,0
...for each sequence,0
%% Write smoothed classification results,0
"%% Post-processing (post-classification, post-within-image-and-within-sequence-smoothing)",0
%% Zip .json files,0
%% 99.9% of jobs end here,0
Everything after this is run ad hoc and/or requires some manual editing.,0
%% Compare results files for different model versions (or before/after RDE),0
Choose all pairwise combinations of the files in [filenames],0
%% Merge in high-confidence detections from another results file,0
%% Create a new category for large boxes,0
"This is a size threshold, not a confidence threshold",0
size_options.categories_to_separate = [3],0
%% Preview large boxes,0
%% .json splitting,0
options.query = None,0
options.replacement = None,0
Reminder: 'n_from_bottom' with a parameter of zero is the same as 'bottom',0
%% Custom splitting/subsetting,0
i_folder = 0; folder_name = folders[i_folder],0
"This doesn't do anything in this case, since we're not splitting folders",0
options.make_folder_relative = True,0
%% String replacement,0
%% Splitting images into folders,0
%% Generate commands for a subset of tasks,0
i_task = 8,0
...for each task,0
%% End notebook: turn this script into a notebook (how meta!),0
Exclude everything before the first cell,0
Remove the first [first_non_empty_lines] from the list,0
Add the last cell,0
,0
xmp_integration.py,0
,0
"Tools for loading MegaDetector batch API results into XMP metadata, specifically",0
for consumption in digiKam:,0
,0
https://cran.r-project.org/web/packages/camtrapR/vignettes/camtrapr2.html,0
,0
%% Imports and constants,0
%% Class definitions,0
Folder where images are stored,0
.json file containing MegaDetector output,0
"String to remove from all path names, typically representing a",0
prefix that was added during MegaDetector processing,0
Optionally *rename* (not copy) all images that have no detections,0
above [rename_conf] for the categories in rename_cats from x.jpg to,0
x.check.jpg,0
"Comma-deleted list of category names (or ""all"") to apply the rename_conf",0
behavior to.,0
"Minimum detection threshold (applies to all classes, defaults to None,",0
i.e. 0.0,0
%% Functions,0
Relative image path,0
Absolute image path,0
List of categories to write to XMP metadata,0
Categories with above-threshold detections present for,0
this image,0
Maximum confidence for each category,0
Have we already added this to the list of categories to,0
write out to this image?,0
If we're supposed to compare to a threshold...,0
Else we treat *any* detection as valid...,0
Keep track of the highest-confidence detection for this class,0
If we're doing the rename/.check behavior...,0
Legacy code to rename files where XMP writing failed,0
%% Interactive/test driver,0
%%,0
%% Command-line driver,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Cosmos DB `batch-api-jobs` table for job status,0
"aggregate the number of images, country and organization names info from each job",0
submitted during yesterday (UTC time),0
create the card,0
,0
api_frontend.py,0
,0
"Defines the Flask app, which takes requests (one or more images) from",0
"remote callers and pushes the images onto the shared Redis queue, to be processed",0
by the main service in api_backend.py .,0
,0
%% Imports,0
%% Initialization,0
%% Support functions,0
Make a dict that the request_processing_function can return to the endpoint,0
function to notify it of an error,0
Verify that the content uploaded is not too big,0
,0
request.content_length is the length of the total payload,0
Verify that the number of images is acceptable,0
...def check_posted_data(request),0
%% Main loop,0
Check whether the request_processing_function had an error,0
Write images to temporary files,0
,0
TODO: read from memory rather than using intermediate files,1
"Submit the image(s) for processing by api_backend.py, who is waiting on this queue",0
TODO: convert to a blocking read and eliminate the sleep() statement in this loop,1
"image = Image.open(os.path.join(temp_direc, image_name))",0
...if we do/don't have a request available on the queue,0
...while(True),0
...def detect_sync(),0
%% Command-line driver,0
use --non-docker if you are testing without Docker,0
,0
python api_frontend.py --non-docker,0
# Camera trap real-time API configuration,0
"Full path to the temporary folder for image storage, only meaningful",0
within the Docker container,0
Upper limit on total content length (all images and parameters),0
Minimum confidence threshold for detections,0
Minimum confidence threshold for showing a bounding box on the output image,0
Use this when testing without Docker,0
,0
api_backend.py,0
,0
"Defines the model execution service, which pulls requests (one or more images)",0
"from the shared Redis queue, and runs them through the TF model.",0
,0
%% Imports,0
%% Initialization,0
%% Main loop,0
TODO: convert to a blocking read and eliminate the sleep() statement in this loop,1
Filter the detections by the confidence threshold,0
,0
"Each result is [ymin, xmin, ymax, xmax, confidence, category]",0
,0
"Coordinates are relative, with the origin in the upper-left",0
...if serialized_entry,0
...while(True),0
...def detect_process(),0
%% Command-line driver,0
use --non-docker if you are testing without Docker,0
,0
python api_frontend.py --non-docker,0
run detections on a test image to load the model,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%%,0
Importing necessary basic libraries and modules,0
%%,0
PyTorch imports for tensor operations,0
%%,0
"Importing the models, transformations, and utility functions from PytorchWildlife",0
%%,0
Setting the device to use for computations ('cuda' indicates GPU),0
%%,0
Initializing the model for image detection,0
%%,0
Initializing the model for image classification,0
%%,0
Defining transformations for detection and classification,0
%%,0
Initializing a box annotator for visualizing detections,0
Processing the video and saving the result with annotated detections and classifications,0
%%,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%%,0
Importing basic libraries,0
%%,0
%%,0
"Importing the models, dataset, transformations, and utility functions from PytorchWildlife",0
%%,0
Setting the device to use for computations ('cuda' indicates GPU),0
Initializing a supervision box annotator for visualizing detections,0
Create a temp folder,0
Initializing the detection and classification models,0
Defining transformations for detection and classification,0
%% Defining functions for different detection scenarios,0
Only run classifier when detection class is animal,0
%% Building Gradio UI,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%%,0
Importing necessary basic libraries and modules,0
%%,0
PyTorch imports,0
%%,0
"Importing the model, dataset, transformations and utility functions from PytorchWildlife",0
%%,0
Setting the device to use for computations ('cuda' indicates GPU),0
%%,0
Initializing the MegaDetectorV5 model for image detection,0
%% Single image detection,0
Specifying the path to the target image TODO: Allow argparsing,0
Opening and converting the image to RGB format,0
Initializing the Yolo-specific transform for the image,0
Performing the detection on the single image,0
Saving the detection results,0
%% Batch detection,0
Specifying the folder path containing multiple images for batch detection,0
Creating a dataset of images with the specified transform,0
Creating a DataLoader for batching and parallel processing of the images,0
Performing batch detection on the images,0
%% Output to annotated images,0
Saving the batch detection results as annotated images,0
%% Output to cropped images,0
Saving the detected objects as cropped images,0
%% Output to JSON results,0
Saving the detection results in JSON format,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Making the DetectionImageFolder class available for import from this module,0
Listing and sorting all image files in the specified directory,0
Get image filename and path,0
Load and convert image to RGB,0
Apply transformation if specified,0
Only run recognition on animal detections,0
Get image path and corresponding bbox xyxy for cropping,0
Load and crop image with supervision,0
Apply transformation if specified,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Making the provided classes available for import from this module,0
Resize and pad the image using the letterbox function,0
Transpose and convert image to PyTorch tensor,0
Normalization constants,0
Define the sequence of transformations,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
!!! Output paths need to be optimized !!!,1
!!! Output paths need to be optimized !!!,1
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%%,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Importing basic libraries,0
Placeholder class-level attributes to be defined in derived classes,0
"If there are size differences in the input images, use a for loop instead of matrix processing for scaling",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Image size for the Opossum classifier,0
Class names for prediction,0
"If pretrained, use the provided URL to fetch the weights",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Making the PlainResNetInference class available for import from this module,0
Following the ResNet structure to extract features,0
Initialize the network and weights,0
... [Missing weight URL definition for ResNet18],0
... [Missing weight URL definition for ResNet50],0
Print missing and unused keys for debugging purposes,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Image size for the Opossum classifier,0
Class names for prediction,0
"If pretrained, use the provided URL to fetch the weights",0
Configuration file for the Sphinx documentation builder.,0
,0
"For the full list of built-in configuration values, see the documentation:",0
https://www.sphinx-doc.org/en/master/usage/configuration.html,0
-- Project information -----------------------------------------------------,0
https://www.sphinx-doc.org/en/master/usage/configuration.html#project-information,0
-- General configuration ---------------------------------------------------,0
https://www.sphinx-doc.org/en/master/usage/configuration.html#general-configuration,0
-- Options for HTML output -------------------------------------------------,0
https://www.sphinx-doc.org/en/master/usage/configuration.html#options-for-html-output,0
-- Options for todo extension ----------------------------------------------,1
https://www.sphinx-doc.org/en/master/usage/extensions/todo.html#configuration,0
"Determine the factor, which shifts the decimal point of x",0
just behind the last significant digit,0
"Shift decimal point by multiplicatipon with factor, flooring, and",0
division by factor,0
_ = pretty_print_object(obj),0
Sloppy that I'm making a module-wide change here...,1
Determine the coordinates of the intersection rectangle,0
The intersection of two axis-aligned bounding boxes is always an,0
axis-aligned bounding box,0
Compute the area of both AABBs,0
Compute the intersection over union by taking the intersection,0
area and dividing it by the sum of prediction + ground-truth,0
areas - the intersection area.,0
%% Functions for running commands as subprocesses,0
%%,0
%% Test driver for execute_and_print,0
%% Parallel test driver for execute_command_and_print,0
Should we use threads (vs. processes) for parallelization?,0
"Only relevant if n_workers == 1, i.e. if we're not parallelizing",0
%% Constants and imports,0
%% Classes,0
Collect all names,0
Make names unique and sort,0
cast location to string as the entries in locations are strings,0
Convert classnames to lowercase to simplify comparisons later,0
Normalize paths to simplify comparisons later,0
"Make custom replacements in filenames, typically used to",0
accommodate changes in root paths after DB construction,0
## Build useful mappings to facilitate working with the DB,0
Category ID <--> name,0
Image filename --> ID,0
Image ID --> image object,0
Image ID --> annotations,0
"Each image can potentially multiple annotations, hence using lists",0
...__init__,0
...class IndexedJsonDb,0
%% Functions,0
Find all unique locations,0
i_location = 0; location = locations[i_location],0
Sorting datetimes fails when there are None's in the list.  So instead of sorting datetimes,0
"directly, sort tuples with a boolean for none-ness, then the datetime itself.",0
,0
https://stackoverflow.com/questions/18411560/sort-list-while-pushing-none-values-to-the-end,0
previous_datetime = sorted_images_this_location[0]['datetime'],0
im = sorted_images_this_location[1],0
"Start a new sequence if necessary, including the case where this datetime is invalid",0
"If this was an invalid datetime, this will record the previous datetime",0
"as None, which will force the next image to start a new sequence.",0
...for each image in this location,0
Fill in seq_num_frames,0
...for each location,0
...create_sequences(),0
,0
cct_to_md.py,0
,0
"""Converts"" a COCO Camera Traps file to a MD results file.  Currently ignores",0
"non-bounding-box annotations, and gives all annotations a confidence of 1.0.",0
,0
The only reason to do this is if you are going to add information to an existing,0
"CCT-formatted dataset, and want to do that in Timelapse.",0
,0
"Currently assumes that width and height are present in the input data, does not",0
read them from images.,0
,0
%% Constants and imports,0
%% Functions,0
# Validate input,0
# Read input,0
# Prepare metadata,0
ann = d['annotations'][0],0
# Process images,0
im = d['images'][0],0
"MegaDetector: [x,y,width,height] (normalized, origin upper-left)",0
"CCT: [x,y,width,height] (absolute, origin upper-left)",0
...if there's a bounding box,0
...for each annotation,0
This field is no longer included in MD output files by default,0
im_out['max_detection_conf'] = max_detection_conf,0
...for each image,0
# Write output,0
...cct_to_md(),0
%% Command-line driver,0
TODO,1
%% Interactive driver,0
%%,0
%%,0
,0
cct_json_to_filename_json.py,0
,0
"Given a .json file in COCO Camera Traps format, outputs a .json-formatted list of",0
relative file names present in the CCT file.,0
,0
%% Constants and environment,0
%% Main function,0
"json.dump(s,open(outputFilename,'w'))",0
%% Command-line driver,0
%% Interactive driver,0
%%,0
,0
cct_to_csv.py,0
,0
"""Converts"" a COCO Camera Traps .json file to .csv, in quotes because",0
"all kinds of assumptions are made here, and if you have a particular .csv",0
"format in mind, YMMV.  Most notably, does not include any bounding box information",0
or any non-standard fields that may be present in the .json file.  Does not,0
propagate information about sequence-level vs. image-level annotations.,0
,0
"Does not assume access to the images, therefore does not open .jpg files to find",0
"datetime information if it's not in the metadata, just writes datetime as 'unknown'.",0
,0
%% Imports,0
%% Main function,0
#%% Read input,0
#%% Build internal mappings,0
annotation = annotations[0],0
#%% Write output file,0
im = images[0],0
Write out one line per class:,0
...for each class name,0
...for each image,0
...with open(output_file),0
...def cct_to_csv,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
,0
remove_exif.py,0
,0
"Removes all EXIF/IPTC/XMP metadata from a folder of images, without making",0
"backup copies, using pyexiv2.",0
,0
%% Imports and constants,0
%% List files,0
%% Remove EXIF data (support),0
PYEXIV2 IS NOT THREAD SAFE; DO NOT CALL THIS IN PARALLEL FROM A SINGLE PROCESS,1
data = img.read_exif(); print(data),0
%% Debug,0
%%,0
%%,0
%% Remove EXIF data (execution),0
fn = image_files[0],0
"joblib.Parallel defaults to a process-based backend, but let's be sure",0
"results = Parallel(n_jobs=n_exif_threads,verbose=2,prefer='processes')(delayed(remove_exif)(fn) for fn in image_files[0:10])",0
,0
yolo_to_coco.py,0
,0
Converts a YOLO-formatted dataset to a COCO-formatted dataset.,0
,0
"Currently supports only a single folder (i.e., no recursion).  Treats images without",0
corresponding .txt files as empty.,0
,0
%% Imports and constants,0
from ai4eutils,0
%% Support functions,0
Validate input,0
Class names,0
Blank lines should only appear at the end,0
Enumerate images,0
fn = image_files[0],0
Create the image object for this image,0
Is there an annotation file for this image?,0
"This is an image with no annotations, currently don't do anything special",0
here,0
s = lines[0],0
"COCO: [x_min, y_min, width, height] in absolute coordinates",0
"YOLO: [class, x_center, y_center, width, height] in normalized coordinates",0
...for each annotation,0
...if this image has annotations,0
...for each image,0
...def yolo_to_coco(),0
%% Interactive driver,0
%% Convert YOLO folders to COCO,0
%% Check DB integrity,0
%% Preview some images,0
%% Command-line driver,0
TODO,1
,0
read_exif.py,0
,0
"Given a folder of images, read relevant metadata (EXIF/IPTC/XMP) fields from all images,",0
and write them to  a .json or .csv file.,0
,0
This module can use either PIL (which can only reliably read EXIF data) or exiftool (which,0
can read everything).  The latter approach expects that exiftool is available on the system,0
path.  No attempt is made to be consistent in format across the two approaches.,0
,0
%% Imports and constants,0
From ai4eutils,0
%% Options,0
Number of concurrent workers,0
Should we use threads (vs. processes) for parallelization?,0
,0
Not relevant if n_workers is 1.,0
Should we use exiftool or pil?,0
%% Functions,0
exif_tags = img.info['exif'] if ('exif' in img.info) else None,0
print('Warning: unrecognized EXIF tag: {}'.format(k)),0
"-G means ""Print group name for each tag"", e.g. print:",0
,0
[File]          Bits Per Sample                 : 8,0
,0
...instead of:,0
,0
Bits Per Sample                 : 8,0
A list of three-element lists (type/tag/value),0
line_raw = exif_lines[0],0
A typical line:,0
,0
[ExifTool]      ExifTool Version Number         : 12.13,0
"Split on the first occurrence of "":""",0
...for each output line,0
...which processing library are we using?,0
...read_exif_tags_for_image(),0
...populate_exif_data(),0
Enumerate *relative* paths,0
Find all EXIF tags that exist in any image,0
...for each tag in this image,0
...for each image,0
Write header,0
...for each key that *might* be present in this image,0
...for each image,0
...with open(),0
...if we're writing to .json/.csv,0
https://stackoverflow.com/questions/11210104/check-if-a-program-exists-from-a-python-script,0
%% Interactive driver,0
%%,0
output_file = os.path.expanduser('~/data/test-exif.csv'),0
options.processing_library = 'pil',0
"file_path = os.path.join(input_folder,'KRU_S1_11_R1_IMAG0148.JPG')",0
%%,0
%% Command-line driver,0
,0
"Given a json-formatted list of image filenames, retrieve the width and height of every image.",0
,0
%% Constants and imports,0
%% Processing functions,0
Is this image on disk?,0
"all_results = list(tqdm(pool.imap(process_image, filenames), total=len(filenames)))",0
%% Interactive driver,0
%%,0
List images in a test folder,0
%%,0
"process_list_file(image_list_file,image_size_file,image_prefix=base_dir)",0
%% Command-line driver,0
"python sanity_check_json_db.py ""e:\wildlife_data\wellington_data\wellington_camera_traps.json"" --baseDir ""e:\wildlife_data\wellington_data\images"" --bFindUnusedImages --bCheckImageSizes",0
"python sanity_check_json_db.py ""D:/wildlife_data/mcgill_test/mcgill_test.json"" --baseDir ""D:/wildlife_data/mcgill_test"" --bFindUnusedImages --bCheckImageSizes",0
"Here the '-u' prevents buffering, which makes tee happier",0
,0
python -u sanity_check_json_db.py '/datadrive1/nacti_metadata.json' --baseDir '/datadrive1/nactiUnzip/' --bFindUnusedImages --bCheckImageSizes | tee ~/nactiTest.out,0
,0
Partially-formed stub to get from MegaDetector output files to COCO Camera Traps data.,0
,0
"Was actually written to convert *many* MD .json files to a single CCT file, hence",0
the loop over .json files.,0
,0
"THIS CODE HAS NEVER BEEN RUN, it was added as a demonstration of how to do this.  YMMV.",0
,0
"You may find a more polished, command-line-ready version of this code at:",0
,0
https://github.com/StewartWILDlab/mdtools,0
,0
%% Constants and imports,0
"Images sizes are required to convert between absolute and relative coordinates,",0
so we need to read the images.,0
Only required if you want to write a database preview,0
%% Create CCT dictionaries,0
image_ids_to_images = {},0
Force the empty category to be ID 0,0
Load .json annotations for this data set,0
i_entry = 0; entry = data['images'][i_entry],0
,0
"PERF: Not exactly trivially parallelizable, but about 100% of the",0
time here is spent reading image sizes (which we need to do to get from,0
"absolute to relative coordinates), so worth parallelizing.",0
Generate a unique ID from the path,0
detection = detections[0],0
Have we seen this category before?,0
Create an annotation,0
"MegaDetector: [x,y,width,height] (normalized, origin upper-left)",0
"CCT: [x,y,width,height] (absolute, origin upper-left)",0
...for each detection,0
...for each image,0
Remove non-reviewed images and associated annotations,0
%% Create info struct,0
%% Write .json output,0
%% Clean start,0
## Everything after this should work from a clean start ###,0
%% Validate output,0
%% Preview animal labels,0
%% Preview empty labels,0
"viz_options.classes_to_exclude = ['empty','human']",0
,0
generate_crops_from_cct.py,0
,0
"Given a .json file in COCO Camera Traps format, create a cropped image for",0
each bounding box.,0
,0
%% Imports and constants,0
%% Functions,0
# Read and validate input,0
# Find annotations for each image,0
"This actually maps image IDs to annotations, but only to annotations",0
containing boxes,0
# Generate crops,0
TODO: parallelize this loop,1
im = d['images'][0],0
Load the image,0
Generate crops,0
i_ann = 0; ann = annotations_this_image[i_ann],0
"x/y/w/h, origin at the upper-left",0
...for each box,0
...for each image,0
...generate_crops_from_cct(),0
%% Interactive driver,0
%%,0
%%,0
%%,0
%% Command-line driver,0
TODO,1
%% Scrap,0
%%,0
,0
coco_to_yolo.py,0
,0
Converts a COCO-formatted dataset to a YOLO-formatted dataset.,0
,0
"If the input and output folders are the same, writes .txt files to the input folder,",0
and neither moves nor modifies images.,0
,0
"Currently ignores segmentation masks, and errors if an annotation has a",0
segmentation polygon but no bbox,0
,0
Has only been tested on a handful of COCO Camera Traps data sets; if you,0
"use it for more general COCO conversion, YMMV.",0
,0
%% Imports and constants,0
%% Support functions,0
Validate input,0
Read input data,0
Parse annotations,0
i_ann = 0; ann = data['annotations'][0],0
Make sure no annotations have *only* segmentation data,0
Re-map class IDs to make sure they run from 0...n-classes-1,0
,0
"TODO: this allows unused categories in the output data set, which I *think* is OK,",1
but I'm only 81% sure.,0
Process images (everything but I/O),0
"List of dictionaries with keys 'source_image','dest_image','bboxes','dest_txt'",0
i_image = 0; im = data['images'][i_image],0
"assert os.path.isfile(source_image), 'Could not find image {}'.format(source_image)",0
If this annotation has no bounding boxes...,0
"This is not entirely clear from the COCO spec, but it seems to be consensus",0
"that if you want to specify an image with no objects, you don't include any",0
annotations for that image.,0
We allow empty bbox lists in COCO camera traps; this is typically a negative,0
"example in a dataset that has bounding boxes, and 0 is typically the empty",0
category.,0
...if this is an empty annotation,0
"COCO: [x_min, y_min, width, height] in absolute coordinates",0
"YOLO: [class, x_center, y_center, width, height] in normalized coordinates",0
Convert from COCO coordinates to YOLO coordinates,0
...for each annotation,0
...if this image has annotations,0
...for each image,0
Write output,0
Category IDs should range from 0..N-1,0
TODO: parallelize this loop,1
,0
output_info = images_to_copy[0],0
Only write an annotation file if there are bounding boxes.  Images with,0
"no .txt files are treated as hard negatives, at least by YOLOv5:",0
,0
https://github.com/ultralytics/yolov5/issues/3218,0
,0
"I think this is also true for images with empty annotation files, but",0
"I'm using the convention suggested on that issue, i.e. hard negatives",0
are expressed as images without .txt files.,0
bbox = bboxes[0],0
...for each image,0
...def coco_to_yolo(),0
%% Interactive driver,0
%% CCT data,0
"If preview_export is True, I'm exporting to preview these with BoundingBoxEditor:",0
,0
https://github.com/mfl28/BoundingBoxEditor,0
,0
"This export will be compatible, other than the fact that you need to move",0
"""object.data"" into the ""labels"" folder.",0
,0
"Otherwise I'm exporting for training, in the YOLOv5 flat format.",0
%% Command-line driver,0
TODO,1
,0
cct_to_wi.py,0
,0
Converts COCO Camera Traps .json files to the Wildlife Insights,0
batch upload format,0
,0
Also see:,0
,0
https://github.com/ConservationInternational/Wildlife-Insights----Data-Migration,0
,0
https://data.naturalsciences.org/wildlife-insights/taxonomy/search,0
,0
%% Imports,0
%% Paths,0
A COCO camera traps file with information about this dataset,0
A .json dictionary mapping common names in this dataset to dictionaries with the,0
"WI taxonomy fields: common_name, wi_taxon_id, class, orer, family, genus, species",0
%% Constants,0
%% Project information,0
%% Read templates,0
%% Compare dictionary to template lists,0
Write the header,0
Write values,0
%% Project file,0
%% Camera file,0
%% Deployment file,0
%% Images file,0
Read .json file with image information,0
Read taxonomy dictionary,0
Populate output information,0
df = pd.DataFrame(columns = images_fields),0
annotation = annotations[0],0
im = input_data['images'][0],0
"We don't have counts, but we can differentiate between zero and 1",0
This is the label mapping used for our incoming iMerit annotations,0
"Only used to parse the incoming annotations. In our database, the string name is used to avoid confusion",0
MegaDetector outputs,0
,0
add_bounding_boxes_to_megadb.py,0
,0
"Given COCO-formatted JSONs containing manually labeled bounding box annotations, add them to",0
"MegaDB sequence entries, which can then be ingested into MegaDB.",0
the category map that comes in the COCO JSONs for iMerit batch 12 - to check that each,0
JSON,0
"dataset name : (seq_id, frame_num) : [bbox, bbox]",0
where bbox is a dict with str 'category' and list 'bbox',0
iterate over image_id_to_image rather than image_id_to_annotations so we include,0
the confirmed empty images,0
The file_name field in the incoming json looks like,0
alka_squirrels.seq2020_05_07_25C.frame119221.jpg,0
there seems to be a bug in the annotations where sometimes there's a,0
non-empty label along with a label of category_id 5,0
ignore the empty label (they seem to be actually non-empty),0
,0
add_bounding_boxes_to_json.py,0
,0
This script takes a image database in the COCO Camera Traps format and merges in a set of bounding,0
box annotations in the format that iMerit uses (a .json where actually only each row is a valid json).,0
,0
"If you need to update an existing bbox database, please get all the original annotation files and",0
re-generate from scratch,0
,0
%% Imports,0
%% Configurations and paths,0
images database,0
output bboxes database,0
annotation files (pseudo json) obtained from our annotation vendor that contain annotations for this dataset,0
None or a string or tuple of strings that is the prefix to all file_name of interest / in this dataset in the annotation files,0
functions for mapping the image_id in the annotation files (pseudo jsons) to the image_id used in the image DB,0
our img_id doesn't contain frame info,0
"batch3 - ""file_name"":""ASG0000019_0_S1_B06_R1_PICT0007.JPG""",0
"batch5 and 7 - ""file_name"":""datasetsnapshotserengeti.seqASG000002m-frame0.imgS1_B06_R1_PICT0056.JPG""",0
sometimes - 'datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG',0
"id in DB (old_token): 'S6/J01/J01_R1/S6_J01_R1_IMAG0001', 'S1/B05/B05_R1/S1_B05_R1_PICT0036'",0
specify which one to use for your dataset here,0
%% Load the image database and fill in DB info for the output bbox database,0
load the images database,0
%% Find the height and width of images from the annotation files,0
,0
...if they are not available in the images DB,0
each row in this pseudo-json is a COCO formatted entry for an image sequence,0
%% Other functions required by specific datasets,0
the IDFG image database does not include images from unlabeled folders that were annotated with bounding boxes,0
%% Create the bbox database from all annotation files pertaining to this dataset,0
"for the incoming annotations, look up by category name (common) and convert to the numerical id used in our databases",0
"for each annotation pseudo-json, check that the image it refers to exists in the original database",0
each row in this pseudo-json is a COCO formatted entry for an image sequence,0
check that entry is for this dataset,0
category map for this entry in the annotation file - usually the same across all entries but just in case,0
rspb_add_image_entry(img_id),0
use the image length and width in the image DB,0
"[top left x, top left y, width, height] in relative coordinates",0
"add all images that have been sent to annotation, some of which may be empty of bounding boxes",0
rspb_add_image_entry(db_image_id),0
%% Common queries,0
This query is used when preparing tfrecords for object detector training.,0
We do not want to get the whole seq obj where at least one image has bbox because,0
some images in that sequence will not be bbox labeled so will be confusing.,0
Include images with bbox length 0 - these are confirmed empty by bbox annotators.,0
"If frame_num is not available, it will not be a field in the result iterable.",0
"Note that the seq_id is the Cosmos DB assigned ID for that sequence, not the",0
"seq_id field, which may contain ""/"" characters.",0
"For public datasets to be converted to the CCT format, we get the whole seq object because",0
sequence level attributes need to be included too. megadb/converters/megadb_to_cct.py handles,0
the case of bbox-only JSONs with the flag exclude_species_class,0
Getting all sequences in a dataset - for updating or deleting entries which need the id field,0
%% Parameters,0
Use None if querying across all partitions,0
"The `sequences` table has the `dataset` as the partition key, so if only querying",0
"entries from one dataset, set the dataset name here.",0
"e.g. {'name': '@top_n', 'value': 100} - see query_and_upsert_examples/query_for_data.ipynb",0
Use False if do not want all results stored in a single JSON.,0
%% Script,0
execute the query,0
loop through and save the results,0
MODIFY HERE depending on the query,0
wiitigers Unicode issue - no good mapping from DB file names to file names in blob URL,0
build filename,0
if need to re-download a dataset's images in case of corruption,0
entries_to_download = {,0
"filename: entry for filename, entry in entries_to_download.items()",0
if entry['dataset'] == DATASET,0
},0
input validation,0
"existing files, with paths relative to <store_dir>",0
parse JSON or TXT file,0
"create a new storage container client for this dataset,",0
and cache it,0
schema already checks that the min possible value of frame_num is 0,0
"if there are more than one image item, each needs a frame_num",0
checks across all sequence items,0
per sequence item checks,0
load the schema,0
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory,0
load the schema,0
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory,0
checks across all datasets items,0
check for expiry date of the SAS keys,0
update the sequences,0
"`id` is from the database, as well as all attributes starting with _",0
"if valuable sequence information is available, add them to the image",0
required fields for an image object,0
add seq-level class labels for this image,0
add other sequence-level properties to each image too,0
add other image-level properties,0
... for im in seq['images'],0
... for seq in mega_db,0
consolidate categories,0
some property names have changed in the new schema,0
a dummy sequence ID will be generated if the image entry does not have a seq_id field,0
seq_id only needs to be unique within this dataset; MegaDB does not rely on it as the _id field,0
"""annotations"" fields are opened and have its sub-field surfaced one level up",0
set the `dataset` property on each sequence to the provided dataset_name,0
check that the location field is the same for all images in a sequence,0
check which fields in a CCT image entry are sequence-level,0
image-level properties that really should be sequence-level,0
need to add (misidentified) seq properties not present for each image in a sequence to img_level_properties,0
"(some properties act like flags - all have the same value, but not present on each img)",0
add the sequence-level properties to the sequence objects,0
not every sequence have to have all the seq_level_properties,0
get the value of this sequence-level property from the first image entry,0
check which fields are really dataset-level and should be included in the dataset table instead.,0
delete sequence-level properties that should be dataset-level,0
make all `class` fields lower-case; cast `seq_id` to type string in case they're integers,0
turn all float NaN values into None so it gets converted to null when serialized,0
this was an issue in the Snapshot Safari datasets,0
%% validation,0
"at first a dict of image_id: image_obj with annotations embedded, then it becomes",0
an array of image objects,0
%% integrate the image DB,0
takes in image entries and species and other annotations in the image DB,0
convert the species category to explicit string name,0
there may be other fields in the annotation object,0
these fields should already be gotten from the image object,0
%% integrate the bbox DB,0
add any images that are not in the image DB,0
also add any fields in the image object that are not present already,0
add bbox to the annotations field,0
for any newly added images,0
"'bbox_abs': bbox_anno['bbox'],",0
not keeping height and width,0
,0
jb_csv_to_json.py,0
,0
Convert a particular .csv file to CCT format.  Images were not available at,0
"the time I wrote this script, so this is much shorter than other scripts",0
in this folder.,0
,0
%% Constants and environment,0
%% Read source data,0
%% Confirm filename uniqueness (this data set has one label per image),0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
%% Create CCT dictionaries,0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
imageName = imageFilenames[0],0
"As per above, this is convenient and appears to be true; asserting to be safe",0
"Filenames look like ""290716114012001a1116.jpg""",0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
%% Create info struct,0
%% Write output,0
%% Sanity-check,0
%% Imports and constants,0
%% Load data,0
"with open(image_json,'r') as f:",0
data = json.load(f),0
%% Sanity-check data,0
%% Label previews,0
%% Collect images to annotate,0
%% Sort by sequence and frame,0
%% Copy to a folder by GUID,0
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension,0
im = images_to_annotate[0],0
...for each image,0
%% Write out the annotation list,0
%% Constants and imports,0
%% Enumerate files,0
edited_image_folder = edited_image_folders[0],0
fn = edited_image_files[0],0
%% Read metadata and capture location information,0
i_row = 0; row = df.iloc[i_row],0
Sometimes '2017' was just '17' in the date column,0
%% Read the .json files and build output dictionaries,0
json_fn = json_files[0],0
if 'partial' in json_fn:,0
continue,0
line = lines[0],0
if image_fn == 'SD1_238_6_26_17_16_76.73.jpg':,0
asdfad,0
SD29_079_5_14_2018_17_52.85.jpg,0
Re-write two-digit years as four-digit years,0
Sometimes the year was written with two digits instead of 4,0
assert len(tokens[4]) == 4 and tokens[4].startswith('20'),0
Have we seen this location already?,0
"Not a typo, it's actually ""formateddata""",0
An image shouldn't be annotated as both empty and non-empty,0
An image shouldn't be annotated as both empty and non-empty,0
box = formatteddata[0],0
"open_file(os.path.join(base_folder,jpeg_image_folder,image_fn))",0
...for each box,0
...if there are boxes on this image,0
...for each line,0
...with open(),0
...for each json file,0
%% Prepare the output .json,0
%% Check DB integrity,0
%% Print unique locations,0
SD12_202_6_23_2017_1_31.85.jpg,0
%% Preview some images,0
%% Statistics,0
,0
"Import a Snapshot Safari project (one project, one season)",0
,0
Before running this script:,0
,0
"* Mount the blob container where the images live, or copy the",0
images to local storage,0
,0
What this script does:,0
,0
* Creates a .json file,0
* Creates zip archives of the season without humans.,0
* Copies animals and humans to separate folders,0
,0
After running this script:,0
,0
* Create or update LILA page,0
* Push zipfile and unzipped images to LILA,0
* Push unzipped humans to wildlifeblobssc,0
* Delete images from UMN uplaod storage,0
,0
"Snapshot Serengeti is handled specially, because we're dealing with bounding",0
boxes too.  See snapshot_serengeti_lila.py.,0
,0
%% Imports,0
From ai4eutils,0
From CameraTraps,0
%% Constants,0
project_name = 'XXX'; season_name = 'S1'; project_friendly_name = 'Snapshot Unknown',0
project_name = 'SER'; season_name = 'S1-11'; project_friendly_name = 'Snapshot Serengeti',0
project_name = 'KRU'; season_name = 'S1'; project_friendly_name = 'Snapshot Kruger',0
project_name = 'CDB'; season_name = 'S1'; project_friendly_name = 'Snapshot Camdeboo',0
project_name = 'MTZ'; season_name = 'S1'; project_friendly_name = 'Snapshot Mountain Zebra',0
project_name = 'ENO'; season_name = 'S1'; project_friendly_name = 'Snapshot Enonkishu',0
project_name = 'KAR'; season_name = 'S1'; project_friendly_name = 'Snapshot Karoo',0
project_name = 'KGA'; season_name = 'S1'; project_friendly_name = 'Snapshot Kgalagadi',0
%% Folder/file creation,0
E.g. KRU_S1,0
E.g. Z:\KRU,0
E.g. Z:\KRU\KRU_S1,0
Contains annotations for each capture event (sequence),0
Maps image IDs to filenames; each line looks like:,0
,0
"KRU_S1#1#1#2,3,KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0004.JPG",0
"Total number of each answer to each question, e.g. total number of times each species was identified",0
,0
Not used here,1
Create output folders,0
Images will be placed in a season-specific folder inside this (the source data includes,0
this in path names),0
%% Load metadata files,0
%% Convert to dictionaries (prep),0
%% Convert to dictionaries (loops),0
"TODO: iterrows() is a terrible way to do this, but this is one of those days",1
"where I want to get this done, not get better at Python.",0
irow = 0; row = image_table.iloc[0],0
"Loaded as an int64, converting to int here",0
...for each row in the image table,0
Make sure image IDs are what we think they are,0
...for each row in the annotation table,0
%% Take a look at categories (just sanity-checking),0
print('\nCategories by species:'),0
pp.pprint(categories_by_species),0
%% Fill in some image fields we didn't have when we created the image table,0
"width, height, corrupt, seq_num_frames, location, datetime",0
Every annotation in this list should have the same sequence ID,0
"Figure out ""seq_num_frames"", which really should be done in a separate lopp;",0
there's no reason to do this redundantly for every image,0
Every image in this sequence should point back to the same equence,0
Every annotation in this list should have the same location,0
Every annotation in this list should have the same datetime,0
Is this image on disk?,0
iImage = 0; im = images[0],0
...for each image,0
"images_processed = pool.map(process_image, images)",0
"images_processed = list(tqdm(pool.imap_unordered(process_image, images), total=len(images)))",0
%% Count missing/corrupted images,0
%% Print distribution of sequence lengths (sanity-check),0
%% Replicate annotations across images,0
iAnn = 0; ann = annotations[iAnn],0
%% See what files are on disk but not annotated,0
%% Sanity-check image and annotation uniqueness,0
%% Minor updates to fields,0
%% Write .json file,0
%% Create a list of human files,0
ann = annotations[0],0
%% Create public archive and public/private folders,0
im = images[0],0
E.g. KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0001.JPG,0
Copy to private output folder,0
Add to zipfile,0
Possibly start a new archive,0
Copy to public output folder,0
...for each image,0
%% Sanity-check .json file,0
"This will produce some validation errors, because this zipfile doesn't include humans",0
%% Zip up .json and .csv files,0
%% When I skip to this part (using a pre-rendered .json file),0
%%,0
%%,0
ann = annotations[0],0
%% Summary prep for LILA,0
"%% Generate preview, sanity-check labels",0
"viz_options.classes_to_include = ['jackalblackbacked','bustardkori']",0
%% Scrap,0
%% Find annotations for a particular image,0
%% Write a list of missing images,0
,0
save_the_elephants_survey_A.py,0
,0
Convert the .csv file provided for the Save the Elephants Survey A data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
Handle all unstructured fields in the source data as extra fields in the annotations,0
"photo_type really should be an image property, but there are a few conflicts",0
that forced me to handle it as an annotation proprerty,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
Maps relative paths to row indices in input_metadata,0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
Ignore directories,0
%% Make sure the multiple-annotation cases make sense,0
%%,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
Force the empty category to be ID 0,0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image],0
Example filename:,0
,0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG',0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG',0
i_row = row_indices[0],0
There are a small handful of datetime mismatches across annotations,0
for the same image,0
assert im['datetime'] == timestamp,0
Special cases based on the 'photo type' field,0
Various spellings of 'community',0
Have we seen this category before?,0
Create an annotation,0
fieldname = list(mapped_fields.keys())[0],0
...for each row,0
...for each image,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
%% Scrap,0
%% Find unique photo types,0
##,0
,0
timelapse_csv_set_to_json.py,0
,0
Given a directory full of reasonably-consistent Timelapse-exported,0
".csvs, assemble a CCT .json.",0
,0
"Assumes that you have a list of all files in the directory tree, including",0
image and .csv files.,0
,0
##,0
%% Constants and imports,0
Text file with relative paths to all files (images and .csv files),0
"%% Read file list, make a list of all image files and all .csv files",0
"%% Verify column consistency, create a giant array with all rows from all .csv files",0
i_csv = 0; csv_filename = csv_files[0],0
%% Prepare some data structures we'll need for mapping image rows in .csv files to actual image files,0
Enumerate all folders containing image files,0
"In this data set, a site folder looks like:",0
,0
Processed Images\\site_name,0
%% Map .csv files to candidate camera folders,0
fn = valid_csv_files[0],0
"Some site folders appear as ""XXNNNN"", some appear as ""XXNNNN_complete""",0
...for each .csv file,0
%% Map camera folders to candidate image folders,0
%% Main loop over labels (prep),0
Force the empty category to be ID 0,0
Images that are marked empty and also have a species label,0
%% Main loop over labels (loop),0
i_row = 0; row = input_metadata.iloc[i_row],0
"for i_row,row in input_metadata.iterrows():",0
"Usually this is just a single folder name, sometimes it's a full path,",0
which we don't want,0
Check whether this file exists on disk,0
Retrieve image width and height,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each row in the big table of concatenated .csv files,0
%% Fix cases where an image was annotated as 'unlabeled' and as something else,0
This annotation is 'unlabeled',0
Was there another category associated with this image?,0
%% Check for un-annnotated images,0
Enumerate all images,0
list(relative_path_to_image.keys())[0],0
%% Create info struct,0
%% Write output,0
%% Sanity-check the database's integrity,0
%% Render a bunch of images to make sure the labels got carried along correctly,0
,0
auckland_doc_to_json.py,0
,0
Convert Auckland DOC data set to COCO camera traps format.  This was,0
for a training data set where class names were encoded in path names.,0
,0
%% Constants and imports,0
Filenames will be stored in the output .json relative to this base dir,0
%% Enumerate files,0
%% Assemble dictionaries,0
Force the empty category to be ID 0,0
fn = image_files[0]; print(fn),0
Typically y:\Maukahuka_Auckland_Island\1_Training\Winter_Trial_2019\cat\cat\eat\20190903_IDdY_34_E3_tmp_201908240051.JPG,0
"This data set has two top-level folders, ""1_Training"" (which has class names encoded",0
"in paths) and ""2_Testing"" (which has no class information).",0
...for each image,0
%% Write output .json,0
%% Write train/test .jsons,0
%% Validate .json files,0
%% Preview labels,0
checkpoint,0
,0
awc_to_json.py,0
,0
Convert a particular .csv file to CCT format.,0
,0
%% Constants and environment,0
%% Read source data,0
%% Main loop over labels,0
Force the empty category to be ID 0,0
iRow = 0; row = input_metadata.iloc[iRow],0
"ImageID,FileName,FilePath,SpeciesID,CommonName",0
"This makes an assumption of one annotation per image, which happens to be",0
true in this data set.,0
Retrieve image width and height,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
list(relativePathToImage.keys())[0],0
%% Create info struct,0
%% Write output,0
%% Sanity-check the database's integrity,0
%% Render a bunch of images to make sure the labels got carried along correctly,0
,0
idaho-camera-traps.py,0
,0
Prepare the Idaho Camera Traps dataset for release on LILA.,0
,0
%% Imports and constants,0
Multi-threading for .csv file comparison and image existence validation,0
"We are going to map the original filenames/locations to obfuscated strings, but once",0
"we've done that, we will re-use the mappings every time we run this script.",0
This is the file to which mappings get saved,0
The maximum time (in seconds) between images within which two images are considered the,0
same sequence.,0
"This is a two-column file, where each line is [string in the original metadata],[category name we want to map it to]",0
"The output file, using the original strings",0
"The output file, using obfuscated strings for everything but filenamed",0
"The output file, using obfuscated strings and obfuscated filenames",0
"One time only, I ran MegaDetector on the whole dataset...",0
...then set aside any images that *may* have contained humans that had not already been,0
annotated as such.  Those went in this folder...,0
...and the ones that *actually* had humans (identified via manual review) got,0
copied to this folder...,0
"...which was enumerated to this text file, which is a manually-curated list of",0
images that were flagged as human.,0
Unopinionated .json conversion of the .csv metadata,0
%% List files (images + .csv),0
Ignore .csv files in folders with multiple .csv files,0
...which would require some extra work to decipher.,0
fn = csv_files[0],0
%% Parse each .csv file into sequences (function),0
csv_file = csv_files[-1],0
os.startfile(csv_file_absolute),0
survey = csv_file.split('\\')[0],0
Sample paths from which we need to derive locations:,0
,0
St.Joe_elk\AM99\Trip 1\100RECNX\TimelapseData.csv,0
Beaverhead_elk\AM34\Trip 1\100RECNX\TimelapseData.csv,0
,0
ClearCreek_mustelids\Winter2015-16\FS-001-P\FS-001-P.csv,0
ClearCreek_mustelids\Summer2015\FS-001\FS-001.csv,0
ClearCreek_mustelids\Summer2016\IDFG-016\IDFG-016.csv,0
,0
I:\idfg-images\ClearCreek_mustelids\Summer2016\IDFG-017b,0
I:\idfg-images\ClearCreek_mustelids\Summer2016\IDFG-017a,0
Load .csv file,0
Validate the opstate column,0
# Create datetimes,0
print('Creating datetimes'),0
i_row = 0; row = df.iloc[i_row],0
Make sure data are sorted chronologically,0
,0
"In odd circumstances, they are not... so sort them first, but warn",0
Debugging when I was trying to see what was up with the unsorted dates,0
# Parse into sequences,0
print('Creating sequences'),0
i_row = 0; row = df.iloc[i_row],0
Start a new sequence if:,0
,0
* This image has no timestamp,0
* This image has a frame number of zero,0
* We have no previous image timestamp,0
,0
Start a new sequence if necessary,0
...for each row,0
# Parse labels for each sequence,0
sequence_id = location_sequences[0],0
Row indices in a sequence should be adjacent,0
sequence_df = df[df['seq_id']==sequence_id],0
# Determine what's present,0
Be conservative; assume humans are present in all maintenance images,0
The presence columns are *almost* always identical for all images in a sequence,0
assert single_presence_value,0
print('Warning: presence value for {} is inconsistent for {}'.format(,0
"presence_column,sequence_id))",0
...for each presence column,0
Tally up the standard (survey) species,0
"If no presence columns are marked, all counts should be zero",0
count_column = count_columns[0],0
Occasionally a count gets entered (correctly) without the presence column being marked,0
"assert len(values) == 1 and values[0] == 0, 'Non-zero counts with no presence",0
columns marked for sequence {}'.format(sequence_id),0
"Handle this by virtually checking the ""right"" box",0
Make sure we found a match,0
Handle 'other' tags,0
column_name = otherpresent_columns[0],0
print('Found non-survey counted species column: {}'.format(column_name)),0
...for each non-empty presence column,0
"Very rarely, the ""otherpresent"" column is checked, but no more detail is available",0
...handling non-survey species,0
Build the sequence data,0
i_row = 0; row = sequence_df.iloc[i_row],0
Only one folder used a single .csv file for two subfolders,0
...for each sequence,0
...def csv_to_sequences(),0
%% Parse each .csv file into sequences (loop),0
%%,0
%%,0
i_file = -1; csv_file = csv_files[i_file],0
%% Save sequence data,0
%% Load sequence data,0
%%,0
%% Validate file mapping (based on the existing enumeration),0
sequences = sequences_by_file[0],0
sequence = sequences[0],0
"Actually, one folder has relative paths",0
assert '\\' not in image_file_relative and '/' not in image_file_relative,0
os.startfile(csv_folder),0
assert os.path.isfile(image_file_absolute),0
found_file = os.path.isfile(image_file_absolute),0
...for each image,0
...for each sequence,0
...for each .csv file,0
%% Load manual category mappings,0
The second column is blank when the first column already represents the category name,0
%% Convert to CCT .json (original strings),0
Force the empty category to be ID 0,0
For each .csv file...,0
,0
sequences = sequences_by_file[0],0
For each sequence...,0
,0
sequence = sequences[0],0
Find categories for this image,0
"When 'unknown' is used in combination with another label, use that",0
"label; the ""unknown"" here doesn't mean ""another unknown species"", it means",0
there is some other unknown property about the main species.,0
category_name_string = species_present[0],0
"This piece of text had a lot of complicated syntax in it, and it would have",0
been too complicated to handle in a general way,0
print('Ignoring category {}'.format(category_name_string)),0
Don't process redundant labels,0
category_name = category_names[0],0
If we've seen this category before...,0
If this is a new category...,0
print('Adding new category for {}'.format(category_name)),0
...for each category (inner),0
...for each category (outer),0
...if we do/don't have species in this sequence,0
"We should have at least one category assigned (which may be ""empty"" or ""unknown"")",0
assert len(sequence_category_ids) > 0,0
Was any image in this sequence manually flagged as human?,0
print('Flagging sequence {} as human based on manual review'.format(sequence_id)),0
For each image in this sequence...,0
,0
i_image = 0; im = images[i_image],0
Create annotations for this image,0
...for each image in this sequence,0
...for each sequence,0
...for each .csv file,0
Verify that all images have annotations,0
ann = ict_data['annotations'][0],0
For debugging only,0
%% Create output (original strings),0
%% Validate .json file,0
%% Preview labels,0
%% Look for humans that were found by MegaDetector that haven't already been identified as human,0
This whole step only needed to get run once,0
%%,0
Load MD results,0
Get a list of filenames that MD tagged as human,0
im = md_results['images'][0],0
...for each detection,0
...for each image,0
Map images to annotations in ICT,0
ann = ict_data['annotations'][0],0
For every image,0
im = ict_data['images'][0],0
Does this image already have a human annotation?,0
...for each annotation,0
...for each image,0
%% Copy images for review to a new folder,0
fn = missing_human_images[0],0
%% Manual step...,0
Copy any images from that list that have humans in them to...,0
%% Create a list of the images we just manually flagged,0
fn = human_tagged_filenames[0],0
E.g. '0000_Beaverhead_elk~AM174~Trip 1~100RECNX~IMG_1397.JPG',0
"%% Translate location, image, sequence IDs",0
Load mappings if available,0
Generate mappings,0
If we've seen this location before...,0
Otherwise assign a string-formatted int as the ID,0
If we've seen this sequence before...,0
Otherwise assign a string-formatted int as the ID,0
Assign an image ID,0
...for each image,0
Assign annotation mappings,0
Save mappings,0
"Back this file up, lest we should accidentally re-run this script",0
with force_generate_mappings = True and overwrite the mappings we used.,0
...if we are/aren't re-generating mappings,0
%% Apply mappings,0
"%% Write new dictionaries (modified strings, original files)",0
"%% Validate .json file (modified strings, original files)",0
%% Preview labels (original files),0
"viz_options.classes_to_exclude = ['empty','deer','elk']",0
viz_options.classes_to_include = ['bobcat'],0
%% Copy images to final output folder (prep),0
ann = d['annotations'][0],0
Is this a public or private image?,0
Generate absolute path,0
Copy to output,0
Update the filename reference,0
...def process_image(im),0
%% Copy images to final output folder (execution),0
For each image,0
im = images[0],0
Write output .json,0
%% Make sure the right number of images got there,0
%% Validate .json file (final filenames),0
%% Preview labels (final filenames),0
"viz_options.classes_to_exclude = ['empty','deer','elk']",0
viz_options.classes_to_include = ['horse'],0
viz_options.classes_to_include = [viz_options.multiple_categories_tag],0
"viz_options.classes_to_include = ['human','vehicle','domestic dog']",0
%% Create zipfiles,0
%% List public files,0
%% Find the size of each file,0
fn = all_public_output_files[0],0
%% Split into chunks of approximately-equal size,0
...for each file,0
%% Create a zipfile for each chunk,0
...for each filename,0
with ZipFile(),0
...def create_zipfile(),0
i_file_list = 0; file_list = file_lists[i_file_list],0
"....if __name__ == ""__main__""",0
,0
wellington_to_json.py,0
,0
Convert the .csv file provided for the Wellington data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
%% Read source data,0
"Filenames were provided as *.jpg, but images were *.JPG, converting here",0
"%% Map filenames to rows, verify image existence",0
"Takes ~30 seconds, since it's checking the existence of ~270k images",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
"I didn't expect this to be true a priori, but it appears to be true, and",0
it saves us the trouble of checking consistency across multiple occurrences,0
of an image.,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
imageName = imageFilenames[0],0
"As per above, this is convenient and appears to be true; asserting to be safe",0
"Filenames look like ""290716114012001a1116.jpg""",0
This gets imported as an int64,0
"These appear as ""image1"", ""image2"", etc.",0
"In the form ""001a""",0
Can be in the form '111' or 's46',0
"In the form ""7/29/2016 11:40""",0
Check image height and width,0
"Use 'empty', to be consistent with other data on lila",0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
,0
mcgill_to_json.py,0
,0
Convert the .csv file provided for the McGill test data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
Create an additional column for concatenated filenames,0
Maps relative filenames to rows,0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
row = input_metadata.iloc[0],0
"I didn't expect this to be true a priori, but it appears to be true, and",0
it saves us the trouble of checking consistency across multiple occurrences,0
of an image.,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
row = input_metadata.iloc[0],0
"Filenames look like ""290716114012001a1116.jpg""",0
"In the form ""001a""",0
Can be in the form '111' or 's46',0
"In the form ""7/29/2016 11:40""",0
Check image height and width,0
NaN is the only thing we should see that's not a string,0
NaN is the only thing we should see that's not a string,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
,0
bellevue_to_json.py,0
,0
"""Bellevue Camera Traps"" is the rather unremarkable camera trap data set",0
used by one of the repo's maintainers for testing.  It's organized as:,0
,0
approximate_date/[loose_camera_specifier/]/species,0
,0
E.g.:,0
,0
"""2018.03.30\coyote\DSCF0091.JPG""",0
"""2018.07.18\oldcam\empty\DSCF0001.JPG""",0
,0
%% Constants and imports,0
from the ai4eutils repo,0
Filenames will be stored in the output .json relative to this base dir,0
%% Exif functions,0
"%% Enumerate files, create image/annotation/category info",0
Force the empty category to be ID 0,0
Keep track of unique camera folders,0
Each element will be a dictionary with fields:,0
,0
"relative_path, width, height, datetime",0
fname = image_files[0],0
Corrupt or not an image,0
Store file info,0
E.g. 2018.03.30/coyote/DSCF0091.JPG,0
...for each image file,0
%% Synthesize sequence information,0
Sort images by time within each folder,0
camera_path = camera_folders[0],0
previous_datetime = sorted_images_this_camera[0]['datetime'],0
im = sorted_images_this_camera[1],0
Start a new sequence if necessary,0
...for each image in this camera,0
...for each camera,0
Fill in seq_num_frames,0
%% A little cleanup,0
%% Write output .json,0
%% Sanity-check data,0
%% Label previews,0
,0
snapshot_safar_importer_reprise.py,0
,0
This is a 2023 update to snapshot_safari_importer.py.  We do a bunch of things now that,0
we didn't do the last time we imported Snapshot data (like updating the big taxonomy),0
"file, and we skip a bunch of things now that we used to do (like generating massive",0
"zipfiles).  So, new year, new importer.",0
,0
%% Constants and imports,0
%% List files,0
"Do a one-time enumeration of the entire drive; this will take a long time,",0
but will save a lot of hassle later.,0
%% Create derived lists,0
Takes about 60 seconds,0
CSV files are one of:,0
,0
"_report_lila.csv (this is the one we want to use, with the species/count/etc. for each sequence)",0
_report_lila_image_inventory.csv (maps captures to images),0
_report_lila_overview.csv (distrubution of species),0
%% List project folders,0
Project folders look like one of these:,0
,0
APN,0
Snapshot Cameo/DEB,0
%% Map report and inventory files to codes,0
fn = csv_files[0],0
%% Make sure that every report has a corresponding inventory file,0
%% Count species based on overview and report files,0
%% Print counts,0
%% Make sure that capture IDs in the reports/inventory files match,0
...and that all the images in the inventory tables are actually present on disk.,0
assert image_path_relative in all_files_relative_set,0
Make sure this isn't just a case issue,0
...for each report on this project,0
...for each project,0
"%% For all the files we have on disk, see which are and aren't in the inventory files",0
"There aren't any capital-P .PNG files, but if I don't include that",0
"in this list, I'll look at this in a year and wonder whether I forgot",0
to include it.,0
fn = all_files_relative[0],0
print('Skipping project {}'.format(project_code)),0
,0
plot_wni_giraffes.py,0
,0
Plot keypoints on a random sample of images from the wni-giraffes data set.,0
,0
%% Constants and imports,0
%% Load and select data,0
%% Support functions,0
https://stackoverflow.com/questions/32504246/draw-ellipse-in-python-pil-with-line-thickness,0
Use a single channel image (mode='L') as mask.,0
The size of the mask can be increased relative to the imput image,0
to get smoother looking results.,0
draw outer shape in white (color) and inner shape in black (transparent),0
downsample the mask using PIL.Image.LANCZOS,0
(a high-quality downsampling filter).,0
paste outline color to input image through the mask,0
%% Plot some images,0
ann = annotations_to_plot[0],0
i_tool = 0; tool_name = short_tool_names[i_tool],0
Don't plot tools that don't have a consensus annotation,0
...for each tool,0
...for each annotation,0
,0
idfg_iwildcam_lila_prep.py,0
,0
Adding class labels (from the private test .csv) to the iWildCam 2019 IDFG,0
"test set, in preparation for release on LILA.",0
,0
This version works with the public iWildCam release images.,0
,0
"%% ############ Take one, from iWildCam .json files ############",0
%% Imports and constants,0
%% Read input files,0
Remove the header line,0
%% Parse annotations,0
Lines look like:,0
,0
"b005e5b2-2c0b-11e9-bcad-06f1011196c4,1,Private",0
%% Minor cleanup re: images,0
%% Create annotations,0
%% Prepare info,0
%% Minor adjustments to categories,0
Remove unused categories,0
Name adjustments,0
%% Create output,0
%% Write output,0
%% Validate .json file,0
%% Preview labels,0
viz_options.classes_to_exclude = ['test'],0
"%% ############ Take two, from pre-iWildCam .json files created from IDFG .csv files ############",0
%% Imports and constants,0
%% One-time line break addition,0
%% Read input files,0
%% Prepare info,0
%% Minor adjustments to categories,0
%% Minor adjustments to annotations,0
%% Create output,0
%% Write output,0
%% Validate .json file,0
%% Preview labels,0
viz_options.classes_to_exclude = ['test'],0
,0
missouri_to_json.py,0
,0
Create .json files from the original source files for the Missouri Camera Traps,0
data set.  Metadata was provided here in two formats:,0
,0
"1) In one subset of the data, folder names indicated species names.  In Set 1,",0
there are no empty sequences.  Set 1 has a metadata file to indicate image-level,0
bounding boxes.,0
,0
2) A subset of the data (overlapping with (1)) was annotated with bounding,0
"boxes, specified in a whitespace-delimited text file.  In set 2, there are",0
"some sequences omitted from the metadata file, which implied emptiness.",0
,0
"In the end, set 2 labels were not reliable enough to publish, so LILA includes only set 1.",0
,0
%% Constants and imports,0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Filenames will be stored in the output .json relative to this base dir,0
This will be a list of filenames that need re-annotation due to redundant boxes,0
"%% Enumerate files, read image sizes (both sets)",0
"Takes a few minutes, since we're reading image sizes.",0
Each element will be a list of relative path/full path/width/height,0
"Only process leaf nodes corresponding to sequences, which look like:",0
,0
E:\wildlife_data\missouri_camera_traps\Set1\1.02-Agouti\SEQ75583,0
E:\wildlife_data\missouri_camera_traps\Set2\p1d101,0
,0
assert len(files) <= 2,0
Read the image,0
Not an image...,0
Store file info,0
"...if we didn't hit the max file limit, keep going",0
...for each file,0
%% Add sequence lengths (both sets),0
%% Load the set 1 metadata file,0
"List of lists, length varies according to number of bounding boxes",0
,0
Preserves original ordering,0
iLine = 0; line = metadataSet1Lines[0],0
"Lines should be filename, number of bounding boxes, boxes (four values per box)",0
Make sure we have image info for this image,0
%% Print missing files from Set 1 metadata,0
Manual changes I made to the metadata file:,0
,0
'IMG' --> 'IMG_',0
Red_Brocket_Deer --> Red_Deer,0
European-Hare --> European_Hare,0
Wood-Mouse --> Wood_Mouse,0
Coiban-Agouti --> Coiban_Agouti,0
%% Load the set 2 metadata file,0
"This metadata file contains most (but not all) images, and a class label (person/animal/empty)",0
"for each, plus bounding boxes.",0
"List of lists, length varies according to number of bounding boxes",0
,0
Preserves original ordering,0
"Create class IDs for each *sequence*, which we'll use to attach classes to",0
images for which we don't have metadata,0
,0
This only contains mappings for sequences that appear in the metadata.,0
iLine = 0; line = metadataSet2Lines[0],0
"Lines should be filename, number of bounding boxes, labeled boxes (five values per box)",0
,0
Empty images look like filename\t0\t0,0
E.g. 'Set2\\p1d101\\p1d101s100i10.JPG',0
Make sure we don't have mixed classes within an image,0
"Figure out what class this *sequence* is, so we know how to handle unlabeled",0
images from this sequence,0
Can't un-do a mixed sequence,0
Previously-empty sequences get the image class label,0
"If the sequence has a non-empty class, possibly change it",0
Make sure we have image info for this image,0
...for each line in the set 2 metadata file,0
%% What Set 2 images do I not have metadata for?,0
These are *mostly* empty images,0
iImage = 0; imageID = set2ImageIDs[iImage],0
%% Create categories and annotations for set 1,0
"Though we have no empty sequences, we do have empty images in this set",0
For each image,0
,0
iImage = 0; imageID = set1ImageIDs[iImage],0
E.g. Set1\\1.80-Coiban_Agouti\\SEQ83155\\SEQ83155_IMG_0010.JPG,0
Find the species name,0
This image may still be empty...,0
category['count'] = category['count'] + 1,0
"If we have bounding boxes, create image-level annotations",0
"filename, number of bounding boxes, boxes (four values per box)",0
"Make sure the relative filename matches, allowing for the fact that",0
some of the filenames in the metadata aren't quite right,0
This image is non-empty,0
"Some redundant bounding boxes crept in, don't add them twice",0
Check this bbox against previous bboxes,0
,0
Inefficient?  Yes.  In an important way?  No.,0
"Bounding box values are in absolute coordinates, with the origin",0
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1].",0
,0
"Convert to floats and to x/y/w/h, as per CCT standard",0
...for each box,0
if we do/don't have boxes for this image,0
Else create a sequence-level annotation,0
...for each image,0
%% Write out the list of images with redundant boxes,0
%% Create categories and annotations for set 2,0
For each image,0
,0
iImage = 0; imageID = set2ImageIDs[iImage],0
E.g. 'Set2\\p1d100\\p1d100s10i1.JPG',0
"Find the sequence ID, sanity check filename against what we stored",0
"If we have bounding boxes or an explicit empty label, create image-level annotations",0
"filename, number of bounding boxes, labeled boxes (five values per box)",0
"Make sure the relative filename matches, allowing for the fact that",0
some of the filenames in the metadata aren't quite right,0
"Bounding box values are in absolute coordinates, with the origin",0
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1].",0
,0
Convert to floats and to x/y/w/h,0
...for each box,0
...if we do/don't have boxes for this image,0
Else create a sequence-level annotation,0
...for each image,0
"%% The 'count' field isn't really meaningful, delete it",0
"It's really the count of image-level annotations, not total images assigned to a class",0
%% Write output .json files,0
%% Sanity-check final set 1 .json file,0
"python sanity_check_json_db.py --bCheckImageSizes --baseDir ""E:\wildlife_data\missouri_camera_traps"" ""E:\wildlife_data\missouri_camera_traps\missouri_camera_traps_set1.json""",0
%% Generate previews,0
,0
filenames_to_json.py,0
,0
Take a directory of images in which species labels are encoded by folder,0
"names, and produces a COCO-style .json file",0
,0
%% Constants and imports,0
from the ai4eutils repo,0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Filenames will be stored in the output .json relative to this base dir,0
"rawClassListFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_classes.csv')",0
"classMappingsFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_class_mapping.csv')",0
"%% Enumerate files, read image sizes",0
Each element will be a list of relative path/full path/width/height,0
Read the image,0
Corrupt or not an image,0
Store file info,0
Write to output file,0
...for each image file,0
...csv file output,0
...if the file list is/isn't available,0
%% Enumerate classes,0
Maps classes to counts,0
We like 'empty' to be class 0,0
%% Assemble dictionaries,0
...for each category,0
Each element is a list of relative path/full path/width/height/className,0
...for each image,0
%% External class mapping,0
%% Write raw class table,0
cat = categories[0],0
%% Read the mapped class table,0
"id, source, count, target",0
"%% Make classMappings contain *all* classes, not just remapped classes",0
cat = categories[0],0
%% Create new class list,0
"Start at 1, explicitly assign 0 to ""empty""",0
One-off issue with character encoding,0
%% Re-map annotations,0
ann = annotations[0],0
%% Write output .json,0
%% Utilities,0
%%,0
Find images with a particular tag,0
%% Randomly sample annotations,0
,0
umn_to_json.py,0
,0
Prepare images and metadata for the Orinoqua Camera Traps dataset.,0
,0
%% Imports and constants,0
"For two deployments, we're only processing imagse in the ""detections"" subfolder",0
%% Enumerate deployment folders,0
%% Load ground truth,0
i_row = 0; row = ground_truth_df.iloc[i_row],0
%% Create relative paths for ground truth data,0
"Some deployment folders have no subfolders, e.g. this is a valid file name:",0
,0
M00/01010132.JPG,0
,0
"But some deployment folders have subfolders, e.g. this is also a valid file name:",0
,0
N17/100EK113/07160020.JPG,0
,0
"So we can't find files by just concatenating folder and file names, we have to enumerate and explicitly",0
"map what will appear in the ground truth as ""folder/filename"" to complete relative paths.",0
deployment_name = list(deployment_folders)[0],0
Enumerate all files in this folder,0
"print('Enumerated {} files for deployment {}'.format(len(files),deployment_name))",0
filename = files[100],0
...for each file in this deployment,0
...for each deployment,0
%% Add relative paths to our ground truth table,0
i_row = 0; row = ground_truth_df.iloc[i_row],0
"row['filename'] looks like, e.g. A01/01080001.JPG.  This is not actually a path, it's",0
"just the deployment ID and the image name, separated by a slash.",0
Find the relative path for this image,0
Make sure this image file exists,0
...for each row in the ground truth table,0
%% Take everything out of Pandas,0
%% Convert string timestamps to Python datetimes,0
im = ground_truth_dicts[0],0
"Filenames look like, e.g., N36/100EK113/06040726.JPG",0
%% Synthesize sequence information,0
Sort images by time within each location,0
i_location=0; location = locations[i_location],0
previous_datetime = sorted_images_this_location[0]['datetime'],0
im = sorted_images_this_camera[1],0
"Timestamp for this image, may be None",0
Start a new sequence if:,0
,0
* This image has no timestamp,0
* This iamge has a frame number of zero,0
* We have no previous image timestamp,0
,0
Start a new sequence if necessary,0
...for each image in this location,0
...for each location,0
%% Create category dict and category IDs,0
im = images[0],0
%% Imports and constants (.json generation),0
%% Count frames in each sequence,0
%% Double check images with multiple annotations,0
im = images[0],0
%% Assemble dictionaries,0
Force the empty category to be ID 0,0
input_im = images[0],0
...for each image,0
%% Write output .json,0
%% Validate .json file,0
%% Map relative paths to annotation categories,0
ann = data['annotations'][0],0
%% Copy images to output,0
EXCLUDE HUMAN AND MISSING,0
im = data['images'][0],0
im = images[0],0
%% Preview labels,0
viz_options.classes_to_exclude = ['test'],0
,0
nacti_fieldname_adjustments.py,0
,0
"NACTI metadata was posted with ""filename"" in images instead of ""file_name"", and",0
"used string (rather than int) category IDs (in categories, but not in annotations).",0
,0
This script fixes those issues and rev's the version number.,0
,0
%% Constants and environment,0
%% Read .json file,0
"%% Rev version number, update field names and types",0
%% Write json file,0
,0
cct_field_adjustments.py,0
,0
CCT metadata was posted with int locations instead of strings.,0
,0
This script fixes those issues and rev's the version number.,0
,0
%% Constants and environment,0
%% Read .json file,0
"%% Rev version number, update field names and types",0
%% Write json file,0
%% Check output data file,0
,0
carrizo_shrubfree_2018.py,0
,0
Convert the .csv file provided for the Carrizo Mojave data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
%% Read source data,0
Original .csv file had superfluous spaces in column names,0
"%% Map filenames to rows, verify image existence",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
58 missing files (of 17652),0
%% Check for images that aren't included in the metadata file,0
3012 of 20606 files are not in metadata,0
%% Create CCT dictionaries,0
Map categories to integer IDs,0
,0
The category '0' is reserved for 'empty',0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
Each filename should just match one row,0
Don't include images that don't exist on disk,0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
,0
snapshot_serengeti_lila.py,0
,0
Create zipfiles of Snapshot Serengeti S1-S11.,0
,0
"Create a metadata file for S1-S10, plus separate metadata files",0
"for S1-S11.  At the time this code was written, S11 was under embargo.",0
,0
Create zip archives of each season without humans.,0
,0
Create a human zip archive.,0
,0
%% Constants and imports,0
import sys; sys.path.append(r'c:\git\ai4eutils'),0
import sys; sys.path.append(r'c:\git\cameratraps'),0
assert(os.path.isdir(metadata_base)),0
"There are two redundant categories, and we re-map ""blank"" to ""empty"" as per CCT convention",0
"%% Load metadata files, concatenate into a single table",0
iSeason = 1,0
%% Convert to dictionaries (prep),0
%% Convert to dictionaries (loops),0
"TODO: iterrows() is a terrible way to do this, but this is one of those days",1
"where I want to get this done, not get better at Python.",0
irow = 0; row = image_table.iloc[0],0
"Loaded as an int64, converting to int here",0
...for each row in the image table,0
Make sure image IDs are what we think they are,0
...for each row in the annotation table,0
%% Load previously-saved dictionaries when re-starting mid-script,0
%%,0
%% Take a look at categories (just sanity-checking),0
%%,0
%% Fill in some image fields we didn't have when we created the image table,0
"width, height, corrupt, seq_num_frames, location, datetime",0
Every annotation in this list should have the same sequence ID,0
"Figure out ""seq_num_frames"", which really should be done in a separate lopp;",0
there's no reason to do this redundantly for every image,0
Every image in this sequence should point back to the same equence,0
Every annotation in this list should have the same location,0
Every annotation in this list should have the same datetime,0
Is this image on disk?,0
iImage = 0; im = images[0],0
...for each image,0
"images_processed = pool.map(process_image, images)",0
"images_processed = list(tqdm(pool.imap_unordered(process_image, images), total=len(images)))",0
%%,0
%% Count missing/corrupted images,0
%% Print distribution of sequence lengths (sanity-check),0
%% Replicate annotations across images,0
iAnn = 0; ann = annotations[iAnn],0
%% See what files are on disk but not annotated (~15 mins),0
247370 files not in the database (of 7425810),0
%% Load old image database,0
%% Look for old images not in the new DB and vice-versa,0
"At the time this was written, ""old"" was S1-S6",0
old_im = cct_old['images'][0],0
new_im = images[0],0
4 old images not in new db,0
12 new images not in old db,0
%% Save our work,0
%% Load our work,0
%%,0
%% Examine size mismatches,0
i_mismatch = -1; old_im = size_mismatches[i_mismatch],0
%% Sanity-check image and annotation uniqueness,0
"%% Split data by seasons, create master list for public seasons",0
ann = annotations[0],0
%% Minor updates to fields,0
"%% Write master .json out for S1-10, write individual season .jsons (including S11)",0
"Loop over all seasons, plus one iteration for the ""all public data"" iteration, and",0
"one for the ""all data"" iteration",0
%% Find categories that only exist in S11,0
List of categories in each season,0
Category 55 (fire) only in S11,0
Category 56 (hyenabrown) only in S11,0
Category 57 (wilddog) only in S11,0
Category 58 (kudu) only in S11,0
Category 59 (pangolin) only in S11,0
Category 60 (lioncub) only in S11,0
%% Prepare season-specific .csv files,0
iSeason = 1,0
%% Create a list of human files,0
ann = annotations[0],0
%% Save our work,0
%% Load our work,0
%%,0
"%% Create archives (human, per-season) (prep)",0
im = images[0],0
im = images[0],0
Don't include humans,0
Only include files from this season,0
Possibly start a new archive,0
...for each image,0
i_season = 0,0
"for i_season in range(0,nSeasons):",0
create_season_archive(i_season),0
%% Create archives (loop),0
pool = ThreadPool(nSeasons+1),0
"n_images = pool.map(create_archive, range(-1,nSeasons))",0
"seasons_to_zip = range(-1,nSeasons)",0
...for each season,0
%% Sanity-check .json files,0
%logstart -o r'E:\snapshot_temp\python.txt',0
%% Zip up .json and .csv files,0
pool = ThreadPool(len(files_to_zip)),0
"pool.map(zip_single_file, files_to_zip)",0
%% Super-sanity-check that S11 info isn't leaking,0
im = data_public['images'][0],0
ann = data_public['annotations'][0],0
iRow = 0; row = annotation_df.iloc[iRow],0
iRow = 0; row = image_df.iloc[iRow],0
%% Create bounding box archive,0
i_image = 0; im = data['images'][0],0
i_box = 0; boxann = bbox_data['annotations'][0],0
%% Sanity-check a few files to make sure bounding boxes are still sensible,0
import sys; sys.path.append(r'C:\git\CameraTraps'),0
%% Check categories,0
%% Summary prep for LILA,0
,0
wi_to_json,0
,0
Prepares CCT-formatted metadata based on a Wildlife Insights data export.,0
,0
"Mostly assumes you have the images also, for validation/QA.",0
,0
%% Imports and constants,0
"Because WI filenames are GUIDs, it's not practical to page through sequences in an",0
image viewer.  So we're going to (optionally) create a copy of the data set where,0
images are ordered.,0
%% Load ground truth,0
%% Take everything out of Pandas,0
%% Synthesize common names when they're not available,0
"Blank rows should always have ""Blank"" as the common name",0
"print('Warning: missing common name for row {} ({})'.format(i_row,row['filename']))",0
%% Convert string timestamps to Python datetimes,0
im = ground_truth_dicts[0],0
"The field called ""location"" in the WI .csv file is a URL, we want to reclaim",0
"the ""location"" keyword for CCT output",0
"Filenames look like, e.g., N36/100EK113/06040726.JPG",0
%% Synthesize sequence information,0
Sort images by time within each location,0
i_location=0; location = locations[i_location],0
previous_datetime = sorted_images_this_location[0]['datetime'],0
im = sorted_images_this_camera[1],0
"Timestamp for this image, may be None",0
Start a new sequence if:,0
,0
* This image has no timestamp,0
* This iamge has a frame number of zero,0
* We have no previous image timestamp,0
,0
Start a new sequence if necessary,0
...for each image in this location,0
...for each location,0
%% Create category dict and category IDs,0
im = images[0],0
%% Count frames in each sequence,0
%% Build relative paths,0
im = images[0],0
Sample URL:,0
,0
gs://project-asfasdfd/deployment/21444549/asdfasdfd-616a-4d10-a921-45ac456c568a.jpg',0
%% Double check images with multiple annotations,0
im = images[0],0
%% Assemble dictionaries,0
Force the empty category to be ID 0,0
input_im = images[0],0
"print('Warning: image ID {} ({}) has multiple annotations'.format(im['id'],im['id'].replace('_','/')))",0
...for each image,0
%% Write output .json,0
%% Validate .json file,0
%% Preview labels,0
"open_file(os.path.join(image_base,'2100703/1141a545-88d2-498b-a684-7431f7aeb324.jpg'))",0
%%,0
%% Create ordered dataset,0
"Because WI filenames are GUIDs, it's not practical to page through sequences in an",0
image viewer.  So we're going to create a copy of the data set where images are,0
ordered.,0
im = images_out[0]; im,0
%% Create ordered .json,0
%% Copy files to their new locations,0
im = ordered_images[0],0
im = data_ordered['images'][0],0
%% Preview labels in the ordered dataset,0
"open_file(os.path.join(image_base,'2100703/1141a545-88d2-498b-a684-7431f7aeb324.jpg'))",0
%% Open an ordered filename from the unordered filename,0
,0
rspb_to_json.py,0
,0
Convert the .csv file provided for the RSPB data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
[location] is an obfuscation,0
%% Create info struct,0
%% Read source data,0
metadataTable.columns.values,0
,0
"array(['Project', 'inDir', 'FileName', 'Station', 'Camera',",0
"'StationCameraFileName', 'DateTimeOriginal', 'DateReadable',",0
"'outDir', 'filename_new', 'fileExistsAlready', 'CopyStatus',",0
"'Species'], dtype=object)",0
We'll populate these later,0
keys should be lowercase,0
"%% Enumerate images, confirm filename uniqueness",0
"%% Update metadata filenames to include site and camera folders, check existence",0
,0
Takes ~1min,0
iRow = 0; row = metadataTable.iloc[iRow],0
There's a bug in the metadata; the 'camera' column isn't correct.,0
camera = row['Camera'],0
"These appear as, e.g., '3.22e12'",0
camera = str(int(float(camera))),0
Let's pull this out of the file name instead,0
,0
Filenames look like one of the following:,0
,0
A1__03224850850507__2015-11-28__10-45-04(1).JPG,0
Bayama2PH__C05__NA(NA).JPG,0
assert(os.path.isfile(fullPath)),0
metadataTable.iloc[iRow] = row,0
Re-assemble into an updated table,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
Write to a text file,0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
iRow = 0; row = metadataTable.iloc[iRow],0
A1__03224850850507__2015-11-28__10-45-04(1).JPG,0
'A1\\03224850850507\\A1__03224850850507__2015-11-28__10-45-04(1).JPG',0
Not currently populated,0
"Often -1, sometimes a semi-meaningful int",0
A1,0
03224850850507,0
"In variable form, but sometimes '28/11/2015 10:45'",0
Check image height and width,0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Write output,0
%% Check database integrity,0
%% Preview a few images to make sure labels were passed along sensibly,0
%% One-time processing step: copy images to a flat directory for annotation,0
%%,0
,0
pc_to_json.py,0
,0
Convert a particular collection of .csv files to CCT format.,0
,0
%% Constants and environment,0
%% Read and concatenate source data,0
List files,0
"List of dataframes, one per .csv file; we'll concatenate later",0
i_file = 87; fn = input_files[i_file],0
Concatenate into a giant data frame,0
%% List files,0
%% Main loop over labels (prep),0
Force the empty category to be ID 0,0
%% Main loop over labels (loop),0
iRow = 0; row = input_metadata.iloc[iRow],0
"ImageID,FileName,FilePath,SpeciesID,CommonName",0
assert os.path.isfile(full_path),0
Retrieve image width and height,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
%% See what's up with missing files,0
s = list(image_relative_paths)[0],0
s = missing_files[0],0
%% Check for images that aren't included in the metadata file,0
%% Sample the database,0
%%,0
Collect the images we want,0
%% Create info struct,0
%% Write output,0
%% Sanity-check the database's integrity,0
%% Render a bunch of images to make sure the labels got carried along correctly,0
options.classes_to_exclude = ['unlabeled'],0
%% Write out a list of files to annotate,0
,0
save_the_elephants_survey_B.py,0
,0
Convert the .csv file provided for the Save the Elephants Survey B data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
input_base = r'/mnt/blobfuse/wildlifeblobssc/ste_2019_08_drop',0
output_base = r'/home/gramener/survey_b',0
Handle all unstructured fields in the source data as extra fields in the annotations,0
"photo_type really should be an image property, but there are a few conflicts",0
that forced me to handle it as an annotation proprerty,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
"%% Map filenames to rows, verify image existence",0
Maps relative paths to row indices in input_metadata,0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
Ignore directories,0
%% Make sure the multiple-annotation cases make sense,0
%%,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
Force the empty category to be ID 0,0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image],0
Example filename:,0
,0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG',0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG',0
i_row = row_indices[0],0
timestamp = row['Date'],0
There are a small handful of datetime mismatches across annotations,0
for the same image,0
assert im['datetime'] == timestamp,0
Special cases based on the 'photo type' field,0
Various spellings of 'community',0
Have we seen this category before?,0
Create an annotation,0
fieldname = list(mapped_fields.keys())[0],0
...for each row,0
...for each image,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
%% Scrap,0
%% Find unique photo types,0
,0
ubc_to_json.py,0
,0
Convert the .csv file provided for the UBC data set to a,0
COCO-camera-traps .json file,0
,0
"Images were provided in eight folders, each of which contained a .csv",0
file with annotations.  Those annotations came in two slightly different,0
"formats, the two formats corresponding to folders starting with ""SC_"" and",0
otherwise.,0
,0
%% Constants and environment,0
Map Excel column names - which vary a little across spreadsheets - to a common set of names,0
%% Enumerate images,0
Load from file if we've already enumerated,0
%% Create CCT dictionaries,0
Force the empty category to be ID 0,0
To simplify debugging of the loop below,0
#%% Create CCT dictionaries (loop),0
#%%,0
Read source data for this folder,0
Rename columns,0
Folder name is the first two characters of the filename,0
,0
Create relative path names from the filename itself,0
Folder name is the camera name,0
,0
Create relative path names from camera name and filename,0
Which of our images are in the spreadsheet?,0
i_row = 0; fn = input_metadata['image_relative_path'][i_row],0
#%% Check for images that aren't included in the metadata file,0
Find all the images in this folder,0
Which of these aren't in the spreadsheet?,0
#%% Create entries in CCT dictionaries,0
Only process images we have on disk,0
"This is redundant, but doing this for clarity, at basically no performance",0
cost since we need to *read* the images below to check validity.,0
i_row = row_indices[0],0
"These generally represent zero-byte images in this data set, don't try",0
to find the very small handful that might be other kinds of failures we,0
might want to keep around.,0
print('Error opening image {}'.format(image_relative_path)),0
If we've seen this category before...,0
...make sure it used the same latin --> common mapping,0
,0
"If the previous instance had no mapping, use the new one.",0
assert common_name == category['common_name'],0
Create an annotation,0
...for each annotation we found for this image,0
...for each image,0
...for each dataset,0
Print all of our species mappings,0
"%% Copy images for which we actually have annotations to a new folder, lowercase everything",0
im = images[0],0
%% Create info struct,0
"%% Convert image IDs to lowercase in annotations, tag as sequence level",0
"While there isn't any sequence information, the nature of false positives",0
"here leads me to believe the images were labeled at the sequence level, so",0
we should trust labels more when positives are verified.  Overall false,0
positive rate looks to be between 1% and 5%.,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
,0
helena_to_cct.py,0
,0
Convert the Helena Detections data set to a COCO-camera-traps .json file,0
,0
%% Constants and environment,0
This is one time process,0
%% Create Filenames and timestamps mapping CSV,0
import pdb;pdb.set_trace(),0
%% To create CCT JSON for RSPB dataset,0
%% Read source data,0
Original Excel file had timestamp in different columns,0
"%% Map filenames to rows, verify image existence",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
%% Skipping this check because one image has multiple species,0
assert len(duplicate_rows) == 0,0
%% Check for images that aren't included in the metadata file,0
%% Create CCT dictionaries,0
Map categories to integer IDs,0
,0
The category '0' is reserved for 'empty',0
For each image,0
Don't include images that don't exist on disk,0
Some filenames will match to multiple rows,0
assert(len(rows) == 1),0
iRow = rows[0],0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
ann['datetime'] = row['datetime'],0
ann['site'] = row['site'],0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
%% Imports and constants,0
from github.com/microsoft/ai4eutils,0
from github.com/ecologize/CameraTraps,0
A list of files in the lilablobssc container for this data set,0
The raw detection files provided by NOAA,0
A version of the above with filename columns added,0
%% Read input .csv,0
%% Read list of files,0
%% Convert paths to full paths,0
i_row = 0; row = df.iloc[i_row],0
assert ir_image_path in all_files,0
...for each row,0
%% Write results,0
"%% Load output file, just to be sure",0
%% Render annotations on an image,0
i_image = 2004,0
%% Download the image,0
%% Find all the rows (detections) associated with this image,0
"as l,r,t,b",0
%% Render the detections on the image(s),0
In pixel coordinates,0
In pixel coordinates,0
%% Save images,0
%% Clean up,0
,0
carrizo_trail_cam_2017.py,0
,0
"Convert the .csv files provided for the ""Trail Cam Carrizo"" 2017 data set to",0
a COCO-camera-traps .json file.,0
,0
%% Constants and environment,0
%% Read source data,0
Removing the empty records,0
"%% Map filenames to rows, verify image existence",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
908 missing files (of 60562),0
%% Check for images that aren't included in the metadata file,0
105329 of 164983 files are not in metadata,0
%% Create CCT dictionaries,0
Map categories to integer IDs,0
,0
The category '0' is reserved for 'empty',0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
Each filename should just match one row,0
Don't include images that don't exist on disk,0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
,0
channel_islands_to_cct.py,0
,0
Convert the Channel Islands data set to a COCO-camera-traps .json file,0
,0
"Uses the command-line tool ExifTool (exiftool.org) to pull EXIF tags from images,",0
"because every Python package we tried failed to pull the ""Maker Notes"" field properly.",0
,0
"%% Imports, constants, paths",0
# Imports ##,0
# Constants ##,0
# Paths ##,0
Confirm that exiftool is available,0
"assert which(exiftool_command_name) is not None, 'Could not locate the ExifTool executable'",0
%% Load information from every .json file,0
"Ignore the sample file... actually, first make sure there is a sample file",0
...and now ignore that sample file.,0
json_file = json_files[0],0
ann = annotations[0],0
...for each annotation in this file,0
...for each .json file,0
"%% Confirm URL uniqueness, handle redundant tags",0
Have we already added this image?,0
"One .json file was basically duplicated, but as:",0
,0
Ellie_2016-2017 SC12.json,0
Ellie_2016-2017-SC12.json,0
"If the new image has no output, just leave the old one there",0
"If the old image has no output, and the new one has output, default to the one with output",0
Don't worry about the cases where someone tagged 'fox' and someone tagged 'fox_partial',0
...for each image we've already added,0
...if this URL is/isn't in the list of URLs we've already processed,0
...for each image,0
%% Save progress,0
%%,0
%%,0
%% Download files (functions),0
https://www.quickprogrammingtips.com/python/how-to-download-multiple-files-concurrently-in-python.html,0
"This is returned with a leading slash, remove it",0
%% Download files (execution),0
%% Read required fields from EXIF data (functions),0
"-G means ""Print group name for each tag"", e.g. print:",0
,0
[File]          Bits Per Sample                 : 8,0
,0
...instead of:,0
,0
Bits Per Sample                 : 8,0
"If we don't get any EXIF information, this probably isn't an image",0
line_raw = exif_lines[0],0
"Split on the first occurrence of "":""",0
Typically:,0
,0
"'[MakerNotes]    Sequence                        ', '1 of 3']",0
Not a typo; we are using serial number as a location,0
"If there are multiple timestamps, make sure they're *almost* the same",0
"If there are multiple timestamps, make sure they're *almost* the same",0
...for each line in the exiftool output,0
"This isn't directly related to the lack of maker notes, but it happens that files that are missing",0
maker notes also happen to be missing EXIF date information,0
...process_exif(),0
"This is returned with a leading slash, remove it",0
Ignore non-image files,0
%% Read EXIF data (execution),0
ann = images[0],0
%% Save progress,0
Use default=str to handle datetime objects,0
%%,0
%%,0
"Not deserializing datetimes yet, will do this if I actually need to run this",0
%% Check for EXIF read errors,0
%% Remove junk,0
Ignore non-image files,0
%% Fill in some None values,0
"...so we can sort by datetime later, and let None's be sorted arbitrarily",0
%% Find unique locations,0
%% Synthesize sequence information,0
Sort images by time within each location,0
i_location=0; location = locations[i_location],0
previous_datetime = sorted_images_this_location[0]['datetime'],0
im = sorted_images_this_camera[1],0
"Timestamp for this image, may be None",0
Start a new sequence if:,0
,0
* This image has no timestamp,0
* This image has a frame number of zero,0
* We have no previous image timestamp,0
,0
Start a new sequence if necessary,0
...for each image in this location,0
...for each location,0
%% Count frames in each sequence,0
images_this_sequence = [im for im in images if im['seq_id'] == seq_id],0
"%% Create output filenames for each image, store original filenames",0
i_location = 0; location = locations[i_location],0
i_image = 0; im = sorted_images_this_location[i_image],0
%% Save progress,0
Use default=str to handle datetime objects,0
%%,0
%%,0
%% Copy images to their output files (functions),0
%% Copy images to output files (execution),0
%% Rename the main image list for consistency with other scripts,0
%% Create CCT dictionaries,0
Make sure this is really a box,0
Transform to CCT format,0
Force the empty category to be ID 0,0
i_image = 0; input_im = all_image_info[0],0
"This issue only impacted one image that wasn't a real image, it was just a screenshot",0
"showing ""no images available for this camera""",0
Convert datetime if necessary,0
Process temperature if available,0
Read width and height if necessary,0
I don't know what this field is; confirming that it's always None,0
Process object and bbox,0
os.startfile(output_image_full_path),0
"Zero is hard-coded as the empty category, but check to be safe",0
"I can't figure out the 'index' field, but I'm not losing sleep about it",0
assert input_annotation['index'] == 1+i_ann,0
"Some annotators (but not all) included ""_partial"" when animals were partially obscured",0
"Annotators *mostly* used 'none', but sometimes 'empty'.  'empty' is CCT-correct.",0
If we've seen this category before...,0
If this is a new category...,0
...if this is an empty/non-empty annotation,0
Create an annotation,0
...for each annotation on this image,0
...for each image,0
%% Change *two* annotations on images that I discovered contains a human after running MDv4,0
%% Move human images,0
ann = annotations[0],0
%% Count images by location,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
viz_options.classes_to_exclude = [0],0
,0
ena24_to_json_2017.py,0
,0
Convert the ENA24 data set to a COCO-camera-traps .json file,0
,0
%% Constants and environment,0
Temporary folders for human and non-human images,0
Clean existing output folders/zipfiles,0
%% Support functions,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
Check image height and width,0
"Each row is category, [box coordinates]",0
"If there's just one row, loadtxt reads it as a 1d array; make it a 2d array",0
with one row,0
Each row is a bounding box,0
Have we seen this category before?,0
Create an annotation,0
...for each bounding box,0
"This was here for debugging; nearly every instance is Human+Horse, Human+Vehicle,",0
"or Human+Dog, but there is one Rabbit+Opossium, and a few Deer+Chicken!",0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
%% Create ZIP files for human and non human,0
%% Validate output,0
%% Preview labels,0
%% Imports and constants,0
configurations and paths,0
%% Helper functions,0
"dest_path = copy(source_path, dest_folder)",0
num_workers = multiprocessing.cpu_count(),0
pool = ThreadPool(num_workers),0
"results = pool.starmap(_copy_unzip, zip(sources, itertools.repeat(dest_folder)))",0
,0
print('Waiting for processes to finish...'),0
pool.close(),0
pool.join(),0
sequential,0
%% Command-line driver,0
if the blob container is already mounted on the VM,0
or you can download them using the storage Python SDK,0
store storage account key in environment variable AZ_STORAGE_KEY,0
,0
eMammal_helpers.py,0
,0
Support functions for processing eMammal metadata,0
,0
%% Constants and imports,0
%% Support functions,0
"pad to a total of 3 digits if < 1000, or 4 digits otherwise",0
img_frame is a string from the xml tree,0
"length 4 frame order is returned as is, others are left padded to be 3 digit long",0
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels,0
pad to a total of 4 digits,0
img_frame is a string from the xml tree,0
"length 4 frame order is returned as is, others are left padded to be 3 digit long",0
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels,0
others column,0
summer day hours: 6am - 7pm,0
others day hours: 7am - 6pm,0
,0
make_eMammal_json.py,0
,0
"Produces the COCO-formatted json database for an eMammal dataset, i.e. a",0
"collection of folders, each of which contains a deployment_manifest.xml file.",0
,0
"In this process, each image needs to be loaded to size it.",0
,0
"To add bounding box annotations to the resulting database, use",0
add_annotations_to_eMammal_json.py.,0
,0
%% Constants and imports,0
"Either add the eMammal directory to your path, or run from there",0
os.chdir(r'd:\git\CameraTraps\database_tools\eMammal'),0
import warnings,0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
warnings.filterwarnings('ignore'),0
Should we run the image size retrieval in parallel?,0
%% Support functions,0
%% Main loop (metadata processing; image sizes are retrieved later),0
deployment = folders[0],0
sequence = image_sequences[0],0
get species info for this sequence,0
add each image's info to database,0
img = images[0],0
"some manifests don't have the ImageOrder info, but the info is in the file name",0
full_img_id has no frame info,0
,0
frame number only used in requests to iMerit for ordering,0
...for each image,0
...for each sequence,0
...for each deployment,0
%% Get image sizes,0
"'tasks' is currently a list of 2-tuples, with each entry as [image dictionary,path].",0
,0
"Go through that and copy just the image dictionaries to 'db_images', adding size",0
information to each entry.  Takes a couple hours.,0
opening each image seems too fast for this multi-threaded version to be faster than sequential code.,0
%% Assemble top-level dictionaries,0
%% Write out .json,0
,0
make_full_SS_json.py,0
,0
Create a COCO-camera-traps .json file for Snapshot Serengeti data from,0
the original .csv files provided on Dryad.,0
,0
"This was used for ""version 1.0"" of the public Snapshot Serengeti archive; it's no",0
longer used as of v2.0 (early 2020).  See snapshot_serengeti_lila.py for the updated,0
Snapshot Safari preparation process.,0
,0
%% Imports and constants,0
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays",0
Count the number of images with multiple species,0
"%% Read image .csv file, format into a dictionary mapping images to capture events",0
%% Create CCT-style .json,0
still need image width and height,0
...for each image,0
%% Write output files,0
,0
make_per_season_SS_json.py,0
,0
Create a COCO-camera-traps .json file for each Snapshot Serengeti season from,0
the original .csv files provided on Dryad.,0
,0
"This was used for ""version 1.0"" of the public Snapshot Serengeti archive; it's no",0
longer used as of v2.0 (early 2020).  See snapshot_serengeti_lila.py for the updated,0
Snapshot Safari preparation process.,0
,0
%% Imports and constants,0
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays",0
"%% Read image .csv file, format into a dictionary mapping images to capture events",0
%% Create CCT-style .json,0
still need image width and height,0
...for each image ID,0
%% Write output files,0
...for each season,0
,0
generate_lila_per_image_labels.py,0
,0
"Generate a .csv file with one row per annotation, containing full URLs to every",0
"camera trap image on LILA, with taxonomically expanded labels.",0
,0
"Typically there will be one row per image, though images with multiple annotations",0
will have multiple rows.,0
,0
"Some images may not physically exist, particularly images that are labeled as ""human"".",0
This script does not validate image URLs.,0
,0
Does not include bounding box annotations.,0
,0
%% Constants and imports,0
"We'll write images, metadata downloads, and temporary files here",0
"Some datasets don't have ""sequence_level_annotation"" fields populated, but we know their",0
annotation level,0
%% Download and parse the metadata file,0
To select an individual data set for debugging,0
%% Download and extract metadata for the datasets we're interested in,0
%% Load taxonomy data,0
"%% Build a dictionary that maps each [dataset,query] pair to the full taxonomic label set",0
i_row = 0; row = taxonomy_df.iloc[i_row],0
%% Process annotations for each dataset,0
ds_name = list(metadata_table.keys())[0],0
"Go through annotations, marking each image with the categories that are present",0
,0
ann = annotations[0],0
im = images[10],0
This field name was only used for Caltech Camera Traps,0
raise ValueError('Suspicious date parsing result'),0
Special case we don't want to print a warning about,0
"Location, sequence, and image IDs are only guaranteed to be unique within",0
"a dataset, so for the output .csv file, include both",0
category_name = list(categories_this_image)[0],0
Only print a warning the first time we see an unmapped label,0
...for each category that was applied at least once to this image,0
...for each image in this dataset,0
print('Warning: no date information available for this dataset'),0
print('Warning: no location information available for this dataset'),0
...for each dataset,0
...with open(),0
%% Read the .csv back,0
%% Do some post-hoc integrity checking,0
"-1 isn't *really* valid, but we use it sometimes for sequences of unknown length",0
i_row = 0; row = df.iloc[i_row],0
%% Preview constants,0
%% Choose images to download,0
ds_name = list(metadata_table.keys())[2],0
Find all rows for this dataset,0
...for each dataset,0
%% Download images,0
i_image = 0; image = images_to_download[i_image],0
%% Write preview HTML,0
im = images_to_download[0],0
,0
Common constants and functions related to LILA data management/retrieval.,0
,0
%% Imports and constants,0
LILA camera trap master metadata file,0
"wildlife_insights_taxonomy_url = 'https://api.wildlifeinsights.org/api/v1/taxonomy?fields=class,order,family,genus,species,authority,taxonomyType,uniqueIdentifier,commonNameEnglish&page[size]={}'.format(wildlife_insights_page_size)",0
from ai4eutils,0
%% Common functions,0
"We haven't implemented paging, make sure that's not an issue",0
d['data'] is a list of items that look like:,0
Put the master metadata file in the same folder where we're putting images,0
Read lines from the master metadata file,0
Parse those lines into a table,0
Each line in this file is name/sas_url/json_url/[bbox_json_url],0
Create a separate entry for bounding boxes if they exist,0
Unzip if necessary,0
,0
get_lila_category_list.py,0
,0
Generates a .json-formatted dictionary mapping each LILA dataset to all categories,0
"that exist for that dataset, with counts for the number of occurrences of each category",0
"(the number of *annotations* for each category, not the number of *images*).",0
,0
"Also loads the taxonomy mapping file, to include scientific names for each category.",0
,0
get_lila_category_counts counts the number of *images* for each category in each dataset.,0
,0
%% Constants and imports,0
array to fill for output,0
"We'll write images, metadata downloads, and temporary files here",0
Created by get_lila_category_list.py... contains counts for each category,0
%% Load category and taxonomy files,0
%% Map dataset names and category names to scientific names,0
i_row = 1; row = taxonomy_df.iloc[i_row]; row,0
%% Download and parse the metadata file,0
%% Download and extract metadata for the datasets we're interested in,0
%% Get category names and counts for each dataset,0
ds_name = 'NACTI',0
Open the metadata file,0
Collect list of categories and mappings to category name,0
ann = annotations[0],0
c = categories[0],0
"Don't do taxonomy mapping for bbox data sets, which are sometimes just binary and are",0
always redundant with the class-level data sets.,0
"As of right now, this is the only quirky case",0
...for each dataset,0
%% Save dict,0
%% Print the results,0
ds_name = list(dataset_to_categories.keys())[0],0
...for each dataset,0
,0
download_lila_subset.py,0
,0
"Example of how to download a list of files from LILA, e.g. all the files",0
in a data set corresponding to a particular species.,0
,0
%% Constants and imports,0
LILA camera trap master metadata file,0
"In this example, we're using the Missouri Camera Traps data set and the Caltech Camera Traps dataset",0
All lower-case; we'll convert category names to lower-case when comparing,0
"We'll write images, metadata downloads, and temporary files here",0
"We will demonstrate two approaches to downloading, one that loops over files",0
"and downloads directly in Python, another that uses AzCopy.",0
,0
AzCopy will generally be more performant and supports resuming if the,0
transfers are interrupted.  This script assumes that azcopy is on the system path.,0
Number of concurrent download threads (when not using AzCopy) (AzCopy does its,0
own magical parallelism),0
%% Support functions,0
remove the leading '/',0
%% Download and parse the metadata file,0
Put the master metadata file in the same folder where we're putting images,0
Read lines from the master metadata file,0
Parse those lines into a table,0
Each line in this file is name/base_url/json_url/[box_url],0
%% Download and extract metadata for the datasets we're interested in,0
Unzip if necessary,0
...for each dataset of interest,0
%% List of files we're going to download (for all data sets),0
"Flat list or URLS, for use with direct Python downloads",0
For use with azcopy,0
This may or may not be a SAS URL,0
# Open the metadata file,0
# Build a list of image files (relative path names) that match the target species,0
Retrieve all the images that match that category,0
Retrieve image file names,0
Convert to URLs,0
...for each dataset,0
%% Download those image files,0
ds_name = 'Caltech Camera Traps',0
ds_name = 'SWG Camera Traps',0
We want to use the whole relative path for this script (relative to the base of the container),0
"to build the output filename, to make sure that different data sets end up in different folders.",0
This may or may not be a SAS URL,0
For example:,0
,0
caltech-unzipped/cct_images,0
swg-camera-traps,0
Check whether the URL includes a folder,0
E.g. caltech-unzipped,0
E.g. cct_images,0
E.g. swg-camera-traps,0
"Write out a list of files, and use the azcopy ""list-of-files"" option to download those files.",0
,0
This azcopy feature is unofficially documented at:,0
,0
https://github.com/Azure/azure-storage-azcopy/wiki/Listing-specific-files-to-transfer,0
,0
import clipboard; clipboard.copy(cmd),0
Loop over files,0
,0
get_lila_category_counts.py,0
,0
Count the number of images and bounding boxes with each label in one or more LILA datasets.,0
,0
"This script doesn't write these counts out anywhere other than the console, it's just intended",0
as a template for doing operations like this on LILA data.  get_lila_category_list.py writes,0
"information out to a .json file, but it counts *annotations*, not *images*, for each category.",0
,0
%% Constants and imports,0
"If None, will use all datasets",0
"We'll write images, metadata downloads, and temporary files here",0
%% Download and parse the metadata file,0
%% Download and extract metadata for the datasets we're interested in,0
%% Count categories,0
ds_name = datasets_of_interest[0],0
"Go through annotations, marking each image with the categories that are present",0
,0
ann = annotations[0],0
Now go through images and count categories,0
im = images[0],0
...for each dataset,0
%% Print the results,0
...for each dataset,0
,0
add_url_to_database.py,0
,0
"Adds a ""url"" field to a coco-camera-traps .json database, specifically to allow the db to",0
be reviewed in the Visipedia annotation tool.,0
,0
,0
remove_corrupted_images_from_database.py,0
,0
"Given a coco-camera-traps .json file, checks all images for TF-friendliness and generates",0
a new .json file that only contains the non-corrupted images.,0
,0
%% Imports and constants,0
import multiprocessing,0
"I leave this at an annoying low number, since by definition weird stuff will",0
"be happening in the TF kernel, and it's useful to keep having content in the console.",0
%% Function definitions,0
"I sometimes pass in a list of images, sometimes a dict with a single",0
element mapping a job ID to the list of images,0
"We're about to start a lot of TF sessions, and we don't want gobs",0
of debugging information printing out for every session.,0
At some point we were creating a single session and looping over images,0
"within that session, but the only way I found to reliably not run out",0
of GPU memory was to create a session per image and gc.collect() after,0
each session.,0
Map Image IDs to boolean (should I keep this image?),0
"Convert to lists, append job numbers to the image lists",0
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)",0
Merge results,0
%% Interactive driver,0
%%,0
base_dir = r'D:\temp\snapshot_serengeti_tfrecord_generation',0
Load annotations,0
Check for corruption,0
Write out only the uncorrupted data,0
%% Command-line driver,0
#######,0
,0
integrity_check_json_db.py,0
,0
"Does some integrity-checking and computes basic statistics on a db, specifically:",0
,0
* Verifies that required fields are present and have the right types,0
* Verifies that annotations refer to valid images,0
* Verifies that annotations refer to valid categories,0
"* Verifies that image, category, and annotation IDs are unique",0
,0
* Optionally checks file existence,0
,0
* Finds un-annotated images,0
* Finds unused categories,0
,0
* Prints a list of categories sorted by count,0
,0
#######,0
%% Constants and environment,0
%% Functions,0
"If baseDir is non-empty, checks image existence",0
This is used in a medium-hacky way to share modified options across threads,1
print('Image path {} does not exist'.format(filePath)),0
"#%% Read .json file if necessary, integrity-check fields",0
info = data['info'],0
"#%% Build dictionaries, checking ID uniqueness and internal validity as we go",0
Confirm that required fields are present,0
Confirm ID uniqueness,0
...for each category,0
image = images[0],0
Confirm that required fields are present,0
Confirm ID uniqueness,0
We previously supported ints here; this should be strings now,0
"assert isinstance(image['location'], str) or isinstance(image['location'], int), \",0
'Illegal image location type',0
Are we checking for unused images?,0
Recursively enumerate images,0
print('Image {} is unused'.format(p)),0
Are we checking file existence and/or image size?,0
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)",0
...for each image,0
Confirm that required fields are present,0
Confirm ID uniqueness,0
Confirm validity,0
...for each annotation,0
#%% Print statistics,0
Find un-annotated images and multi-annotation images,0
Find unused categories,0
Prints a list of categories sorted by count,0
,0
https://stackoverflow.com/questions/72899/how-do-i-sort-a-list-of-dictionaries-by-a-value-of-the-dictionary,0
...def integrity_check_json_db(),0
%% Command-line driver,0
%% Interactive driver(s),0
%%,0
Integrity-check .json files for LILA,0
options.iMaxNumImages = 10,0
,0
combine_two_json_files.py,0
,0
"Merges two coco-camera-traps .json files. In particular, categories are combined and re-numbered.",0
,0
Combined Info,0
Combined Images,0
Combined Categories,0
## categories to merge,0
Combined Annotations,0
,0
make_detection_db_for_viewing.py,0
,0
"Given a .json file with ground truth bounding boxes, and a .p file containing detections for the same images,",0
"creates a new .json file with separate classes for ground truth and detection, suitable for viewing in the Visipedia",0
annotation tool.,0
,0
%% Imports and constants,0
%% Main function,0
im_id_to_im = {im['id']:im for im in images},0
make new categories to distinguish between ground truth and detections,0
"update all gt annotations to be class ""gt""",0
collect all detections by image,0
keep any detection with score above det_thresh,0
"need to convert bbox from [x1,y1,x2,y2] to [x,y,w,h]",0
"add ""info"" and ""licenses"" for annotation tools to function",0
create new db,0
%% Command-line handling,0
,0
analyze_json_database.py,0
,0
Plots location/class/etc. distributions for classes in a coco-camera-traps .json file.,0
,0
Currently includes some one-off code for specific species.,0
,0
%% Constants and imports,0
%% Path configuration,0
%% Load source data,0
%% Build image/category dictionaries,0
%% Make plot of category distribution,0
%% make plots of location distribution,0
"plt.title('Number of images per location, by category')",0
"plt.tight_layout(rect=[0,0,1,0.9])",0
#make plot of images per season,0
%% Make plot of lions per location,0
%% Make plot of elephants per location,0
for loc in sorted_by_total[:25]:,0
"print('Location:' + loc[0] +', Lions: ' + str(len(loc_to_lion_ims[loc[0]])) + ', Elephants: ' + str(len(loc_to_elephant_ims[loc[0]])) + ', Total ims: ' + str(len(loc_to_ims[loc[0]])))",0
,0
subset_json_db.py,0
,0
Select a subset of images (and associated annotations) from a .json file,0
in COCO Camera Traps format.,0
,0
"To subset the .json files produced by our batch processing API, see",0
subset_json_detector_output.py,0
,0
Sample invocation:,0
,0
"python subset_json_db.py ""E:\Statewide_wolf_container\idfg_20190409.json"" ""E:\Statewide_wolf_container\idfg_20190409_clearcreek.json"" ""clearcreek"" --ignore_case",0
,0
%% Constants and imports,0
%% Functions,0
Load the input file if necessary,0
Find images matching the query,0
Find annotations referring to those images,0
Write the output file if requested,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
%% Constants and imports,0
%% Merge functions,0
i_input_dict = 0; input_dict = input_dicts[i_input_dict],0
We will prepend an index to every ID to guarantee uniqueness,0
Map detection categories from the original data set into the merged data set,0
...for each category,0
Merge original image list into the merged data set,0
Create a unique ID,0
...for each image,0
Same for annotations,0
...for each annotation,0
"Merge info dicts, don't check completion time fields",0
...for each dictionary,0
Convert merged image dictionaries to a sorted list,0
%% Driver,0
,0
add_width_and_height_to_database.py,0
,0
Grabs width and height from actual image files for a .json database that is missing w/h.,0
,0
Originally used when we created a .json file for snapshot serengeti from .csv.,0
,0
"This file converts the JSON output of the batch processing API to a pickle file, which can be used by the",0
script ./make_classification_dataset.py,0
"Convert boxes from JSON   [x_min, y_min, width_of_box, height_of_box]",0
"to PICKLE [ymin,  xmin,  ymax,         xmax]",0
Write detections to file with pickle,0
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the",0
script ./make_classification_dataset.py,0
Write detections to file with pickle,0
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the",0
script ./make_classification_dataset.py,0
Assumes that the root of the CameraTrap repo is in the PYTHONPATH,0
Minimum threshold to put a detection into the output JSON file,0
Parameter check,0
Load detections from input,0
Load COCO style annotations,0
Build output JSON in format version 1.0,0
Adding the only known metadata info,0
"The pickle file does not contain category information, so we assume the default",0
For each image with detections,0
for each detection,0
"Convert boxes from [ymin, xmin, ymax, xmax] format to",0
"[x_min, y_min, width_of_box, height_of_box]",0
Write output json,0
global flag for whether or not we encounter missing images,0
"- will only print ""missing image"" warning once",0
TFRecords variables,0
1.3 for the cropping during test time and 1.3 for the context that the,0
CNN requires in the left-over image,0
Create output directories,0
Load COCO style annotations from the input dataset,0
"Get all categories, their names, and create updated ID for the json file",0
Prepare the coco-style json files,0
Split the dataset by locations,0
Load detections,0
Write out COCO-style json files to the output directory,0
Write detections to file with pickle,0
## Preparations: get all the output tensors,0
For all images listed in the annotations file,0
Skip the image if it is annotated with more than one category,0
"Get ""old"" and ""new"" category ID and category name for this image.",0
Skip if in excluded categories.,0
get path to image,0
"If we already have detection results, we can use them",0
Otherwise run detector and add detections to the collection,0
Only select detections with confidence larger than DETECTION_THRESHOLD,0
Skip image if no detection selected,0
whether it belongs to a training or testing location,0
Skip images that we do not have available right now,0
- this is useful for processing parts of large datasets,0
Load image,0
Run inference,0
"remove batch dimension, and convert from float32 to appropriate type",0
convert normalized bbox coordinates to pixel coordinates,0
Pad the detected animal to a square box and additionally by,0
"PADDING_FACTOR. The result will be in crop_boxes. However, we need to make",0
sure that its box coordinates are still within the image.,0
"for each bounding box, crop the image to the padded box and save it",0
"Create the file path as it will appear in the annotation json,",0
adding the box number if there are multiple boxes,0
"if the cropped file already exists, verify its size",0
Add annotations to the appropriate json,0
js_keys = ['/'.join(im['file_name'].split('/')[1:])[:-4] for im in js['images']],0
for tk in js_keys:,0
"assert np.isclose(1, np.sum(detections[tk]['detection_scores'] > 0.5))",0
,0
separate_detections_by_size,0
,0
Not-super-well-maintained script to break a list of API output files up,0
based on bounding box size.,0
,0
%% Imports and constants,0
Folder with one or more .json files in it that we want to split up,0
Enumerate .json files,0
Define size thresholds and confidence thresholds,0
"Not used directly in this script, but useful if we want to generate previews",0
%% Split by size,0
For each size threshold...,0
For each file...,0
fn = input_files[0],0
Just double-checking; we already filtered this out above,0
Don't reprocess .json files we generated with this script,0
Load the input file,0
For each image...,0
1.1 is the same as infinity here; no box can be bigger than a whole image,0
What's the smallest detection above threshold?,0
"[x_min, y_min, width_of_box, height_of_box]",0
,0
size = w * h,0
...for each detection,0
Which list do we put this image on?,0
...for each image in this file,0
Make sure the number of images adds up,0
Write out all files,0
...for each size threshold,0
...for each file,0
,0
tile_images.py,0
,0
Split a folder of images into tiles.  Preserves relative folder structure in a,0
"new output folder, with a/b/c/d.jpg becoming, e.g.:",0
,0
a/b/c/d_row_0_col_0.jpg,0
a/b/c/d_row_0_col_1.jpg,0
,0
%% Imports and constants,0
from ai4eutils,0
%% Main function,0
TODO: parallelization,1
,0
i_fn = 2; relative_fn = image_relative_paths[i_fn],0
Can we skip this image because we've already generated all the tiles?,0
TODO: super-sloppy that I'm pasting this code from below,1
From:,0
,0
https://github.com/whiplashoo/split-image/blob/main/src/split_image/split.py,0
i_col = 0; i_row = 1,0
left/top/right/bottom,0
...for each row,0
...for each column,0
...for each image,0
%% Interactive driver,0
%%,0
%%,0
,0
rde_debug.py,0
,0
Some useful cells for comparing the outputs of the repeat detection,0
"elimination process, specifically to make sure that after optimizations,",0
results are the same up to ordering.,0
,0
%% Compare two RDE files,0
i_dir = 0,0
break,0
"Regardless of ordering within a directory, we should have the same",0
number of unique detections,0
Re-sort,0
Make sure that we have the same number of instances for each detection,0
Make sure the box values match,0
,0
aggregate_video.py,0
,0
Aggregate results and render output video for a video that's already been run through MD,0
,0
%% Constants,0
%% Processing,0
im = d['images'][0],0
...for each detection,0
This is no longer included in output files by default,0
# Split into frames,0
# Render output video,0
## Render detections to images,0
## Combine into a video,0
,0
ocr_sandbox.py,0
,0
sandbox for experimenting with using OCR to pull metadata from camera trap images,0
,0
The general approach is:,0
,0
"* Crop a fixed percentage from the top and bottom of an image, slightly larger",0
than the largest examples we've seen of how much space is used for metadata.,0
,0
"* Refine that crop by blurring a little, then looking for huge peaks in the",0
"color histogram suggesting a solid background, then finding rows that are",0
mostly that color.,0
,0
"* Crop to the refined crop, then run pytesseract to extract text",0
,0
"* Use regular expressions to find time and date, in the future can add, e.g.,",0
"temperature (which is often present *only* in the images, unlike time/date which",0
are also usually in EXIF but often wrong or lost in processing),0
,0
"The metadata extraction (EXIF, IPTC) here is just sample code that seemed to",0
belong in this file.,0
,0
Contact: Dan Morris (cameratraps@lila.science),0
,0
%% Constants and imports,0
pip install pytesseract,0
,0
"Also intall tesseract from: https://github.com/UB-Mannheim/tesseract/wiki, and add",0
"the installation dir to your path (on Windows, typically C:\Program Files (x86)\Tesseract-OCR)",0
pip install IPTCInfo3,0
from the ai4eutils repo: https://github.com/Microsoft/ai4eutils,0
,0
"Only used for writing out a summary, not important for the core metadata extraction",0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Using a semi-arbitrary metric of how much it feels like we found the,0
"text-containing region, discard regions that appear to be extraction failures",0
Pad each crop with a few pixels to make tesseract happy,0
Discard text from the top,0
"When we're looking for pixels that match the background color, allow some",0
tolerance around the dominant color,0
We need to see a consistent color in at least this fraction of pixels in our rough,0
crop to believe that we actually found a candidate metadata region.,0
"What fraction of the [top,bottom] of the image should we use for our rough crop?",0
A row is considered a probable metadata row if it contains at least this fraction,0
"of the background color.  This is used only to find the top and bottom of the crop area,",0
"so it's not that *every* row needs to hit this criteria, only the rows that are generally",0
above and below the text.,0
%% Support functions,0
"%% Load some images, pull EXIF and IPTC data for fun",0
%% Rough crop,0
"This will be an nImages x 1 list of 2 x 1 lists (image top, image bottom)",0
image = images[0],0
"l,t,r,b",0
,0
"0,0 is upper-left",0
"%% Close-crop around the text, return a revised image and success metric",0
Did we find a sensible mode that looks like a background value?,0
"This looks very scientific, right?  Definitely a probability?",0
"print('Failed min background fraction test: {} of {}'.format(pBackGroundValue,minBackgroundFraction))",0
"Notes to self, things I tried that didn't really go anywhere...",0
"analysisImage = cv2.blur(analysisImage, (3,3))",0
"analysisImage = cv2.medianBlur(analysisImage,5)",0
"analysisImage = cv2.Canny(analysisImage,100,100)",0
imagePil = Image.fromarray(analysisImage); imagePil,0
Use row heuristics to refine the crop,0
,0
This egregious block of code makes me miss my fluency in Matlab.,1
"print('Cropping to {},{},{},{}'.format(x,y,w,h))",0
Crop the image,0
"For some reason, tesseract doesn't like characters really close to the edge",0
imagePil = Image.fromarray(croppedImage); imagePil,0
%% Go to OCR-town,0
"An nImages x 2 list of strings, extracted from the top and bottom of each image",0
An nImages x 2 list of cropped images,0
iImage = 0; iRegion = 1; regionSet = imageRegions[iImage]; region = regionSet[iRegion],0
text = pytesseract.image_to_string(region),0
pil --> cv2,0
"image = cv2.medianBlur(image, 3)",0
"image = cv2.erode(image, None, iterations=2)",0
"image = cv2.dilate(image, None, iterations=4)",0
"image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]",0
"image = cv2.blur(image, (3,3))",0
"image = cv2.copyMakeBorder(image,10,10,10,10,cv2.BORDER_CONSTANT,value=[0,0,0])",0
"text = pytesseract.image_to_string(imagePil, lang='eng')",0
https://github.com/tesseract-ocr/tesseract/wiki/Command-Line-Usage,0
"psm 6: ""assume a single uniform block of text""",0
,0
...for each cropped region,0
...for each image,0
%% Extract dates and times,0
s = '1:22 pm',0
s = '1:23:44 pm',0
%% Write results to a handy html file,0
Add image name and resized image,0
Add results and individual region images,0
"textStyle = ""font-family:calibri,verdana,arial;font-weight:bold;font-size:150%;text-align:left;margin-left:50px;""",0
%% Scrap,0
Alternative approaches to finding the text/background  region,0
Using findCountours(),0
imagePil = Image.fromarray(analysisImage); imagePil,0
"analysisImage = cv2.erode(analysisImage, None, iterations=3)",0
"analysisImage = cv2.dilate(analysisImage, None, iterations=3)",0
"analysisImage = cv2.threshold(analysisImage, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]",0
Find object with the biggest bounding box,0
Using connectedComponents(),0
analysisImage = image,0
print('Found {} components'.format(nb_components)),0
We just want the *background* image,0
,0
"Takes a .json file with MD results for an individual video, and converts to a .csv that includes",0
frame times.  This is very bespoke to animal detection and does not include other classes.,0
,0
%% Imports and constants,0
Only necessary if you want to extract the sample rate from the video,0
%% Extract the sample rate if necessary,0
%% Load results,0
%% Convert to .csv,0
i_image = 0; im = results['images'][i_image],0
,0
umn-pr-analysis.py,0
,0
Precision/recall analysis for UMN data,0
,0
%% Imports and constants,0
results_file = results_file_filtered,0
"For two deployments, we're only processing imagse in the ""detections"" subfolder",0
String to remove from MegaDetector results,0
%% Enumerate deployment folders,0
%% Load MD results,0
im = md_results['images'][0],0
%% Load ground truth,0
i_row = 0; row = ground_truth_df.iloc[i_row],0
%% Create relative paths for ground truth data,0
"Some deployment folders have no subfolders, e.g. this is a valid file name:",0
,0
M00/01010132.JPG,0
,0
"But some deployment folders have subfolders, e.g. this is also a valid file name:",0
,0
N17/100EK113/07160020.JPG,0
,0
"So we can't find files by just concatenating folder and file names, we have to enumerate and explicitly",0
"map what will appear in the ground truth as ""folder/filename"" to complete relative paths.",0
deployment_name = list(deployment_folders)[0],0
Enumerate all files in this folder,0
"print('Enumerated {} files for deployment {}'.format(len(files),deployment_name))",0
filename = files[100],0
...for each file in this deployment,0
...for each deployment,0
%% Map relative paths to MD results,0
%% Add relative paths to our ground truth table,0
i_row = 0; row = ground_truth_df.iloc[i_row],0
"row['filename'] looks like, e.g. A01/01080001.JPG.  This is not actually a path, it's",0
"just the deployment ID and the image name, separated by a slash.",0
Find the relative path for this image,0
Make sure we have MegaDetector results for this file,0
Make sure this image file exists,0
...for each row in the ground truth table,0
%% Take everything out of Pandas,0
%% Some additional error-checking of the ground truth,0
An early version of the data required consistency between common_name and is_blank,0
%% Combine MD and ground truth results,0
d = ground_truth_dicts[0],0
Find the maximum confidence for each category,0
...for each detection,0
...for each image,0
%% Precision/recall analysis,0
...for each image,0
Thresholds go up throughout precisions/recalls/thresholds; find the last,0
value where recall is at or above target.  That's our precision @ target recall.,0
Flatten the confusion matrix,0
Write precision/recall plot to .png file in output directory,0
pr_figure_relative_filename = 'prec_recall.png',0
"pr_figure_filename = os.path.join(output_dir, pr_figure_relative_filename)",0
plt.show(block=False),0
"precision_recall_analysis(human_gt_labels,human_prediction_probs,'humans')",0
%% Find and manually review all images of humans,0
%%,0
"...if this image is annotated as ""human""",0
...for each image,0
%% Find and manually review all MegaDetector animal misses,0
%%,0
im = merged_images[0],0
GT says this is not an animal,0
GT says this is an animal,0
%% Convert .json to .csv,0
%%,0
,0
kga-pr-analysis.py,0
,0
Precision/recall analysis for KGA data,0
,0
%% Imports and constants,0
%% Load and filter MD results,0
%% Load and filter ground truth,0
%% Map images to image-level results,0
%% Map sequence IDs to images and annotations to images,0
Verify consistency of annotation within a sequence,0
TODO,1
%% Find max confidence values for each category for each sequence,0
seq_id = list(sequence_id_to_images.keys())[1000],0
im = images_this_sequence[0],0
det = md_result['detections'][],0
...for each detection,0
...for each image in this sequence,0
...for each sequence,0
%% Prepare for precision/recall analysis,0
seq_id = list(sequence_id_to_images.keys())[1000],0
cat_id = list(category_ids_this_sequence)[0],0
...for each category in this sequence,0
...for each sequence,0
%% Precision/recall analysis,0
"Confirm that thresholds are increasing, recall is decreasing",0
This is not necessarily true,0
assert np.all(precisions[:-1] <= precisions[1:]),0
Thresholds go up throughout precisions/recalls/thresholds; find the max,0
value where recall is at or above target.  That's our precision @ target recall.,0
"This is very slightly optimistic in its handling of non-monotonic recall curves,",0
but is an easy scheme to deal with.,0
Flatten the confusion matrix,0
Write precision/recall plot to .png file in output directory,0
pr_figure_relative_filename = 'prec_recall.png',0
"pr_figure_filename = os.path.join(output_dir, pr_figure_relative_filename)",0
plt.show(block=False),0
"precision_recall_analysis(human_gt_labels,human_prediction_probs,'humans')",0
%% Scrap,0
%% Find and manually review all sequence-level MegaDetector animal misses,0
input_base = '/media/user/lila-01/lila/snapshot-safari/KGA/KGA_public',0
i_sequence = 0; seq_id = pr_sequence_ids[i_sequence],0
i_seq = 0; seq_id = false_negative_sequences[i_seq],0
"sequence_folder = os.path.join(sequence_preview_dir,'seq_{}'.format(str(i_seq).zfill(3)))",0
fn = image_files[0],0
"print('Copying {} to {}'.format(input_path,output_path))",0
...for each file in this sequence.,0
...for each sequence,0
%% Image-level postprocessing,0
parse arguments,0
check if a GPU is available,0
load a pretrained embedding model,0
setup experiment,0
load the embedding model,0
setup the target dataset,0
setup finetuning criterion,0
setup an active learning environment,0
create a classifier,0
the main active learning loop,0
Active Learning,0
finetune the embedding model and load new embedding values,0
gather labeled pool and train the classifier,0
save a snapshot,0
Load a checkpoint if necessary,0
setup the training dataset and the validation dataset,0
setup data loaders,0
check if a GPU is available,0
create a model,0
setup loss criterion,0
define optimizer,0
load a checkpoint if provided,0
setup a deep learning engine and start running,0
train the model,0
train for one epoch,0
evaluate on validation set,0
save a checkpoint,0
open the file,0
read it,0
do the substitution,0
matplotlib.use('Agg'),0
from UIComponents.DBObjects import *,0
Initialize Database,0
# database connection credentials,0
# try to connect as USER to database DB_NAME through peewee,0
Load the saved embedding model,0
dataset_query = Detection.select().limit(5),0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})",0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getalllabels()) , dataset.getallpaths(), {})",0
Random examples to start,0
"random_ids = np.random.choice(dataset.current_set, 1000, replace=False).tolist()",0
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)",0
print(random_ids),0
Move Records,0
"moveRecords(dataset, DetectionKind.ModelDetection.value, DetectionKind.UserDetection.value, random_ids)",0
#print([len(x) for x in dataset.set_indices]),0
# Finetune the embedding model,0
#dataset.set_kind(DetectionKind.UserDetection.value),0
#dataset.train(),0
"#train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)",0
"#finetune_embedding(model, checkpoint['loss_type'], dataset, 32, 4, 100)",0
#save_checkpoint({,0
"#        'arch': model.arch,",0
"#        'state_dict': model.state_dict(),",0
"#        'optimizer' : optimizer.state_dict(),",0
"#        'loss_type' : loss_type,",0
"#        }, False, ""%s%s_%s_%04d.tar""%('finetuned', loss_type, model.arch, len(dataset.set_indices[DetectionKind.UserDetection.value])))",0
Get indices of samples to get user to label,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
"kwargs[""already_selected""].extend(indices)",0
Train on samples that have been labeled so far,0
Test on the samples that have not been labeled,0
"'optimizer' : optimizer.state_dict(),",0
num_classes= len(train_dataset.getClassesInfo()[0]),0
"print(""Num Classes= ""+str(num_classes))",0
define loss function (criterion) and optimizer,0
"optimizer = torch.optim.SGD(params, momentum = 0.9, lr = args.lr, weight_decay = args.weight_decay)",0
optimizer.load_state_dict(checkpoint['optimizer']),0
train for one epoch,0
"adjust_lr(optimizer,epoch)",0
if epoch % 1 == 0 and epoch > 0:,0
"a, b, c = e.predict(train_embd_loader, load_info = True, dim = args.feat_dim)",0
"plot_embedding(reduce_dimensionality(a), b, c, {})",0
evaluate on validation set,0
--------some stuff needed to get AJAX to work with bottle?--------#,0
-------------------------------------------------------------------------------- #,0
PREPARE TO QUEUE IMAGES FOR LABELING,0
-------------------------------------------------------------------------------- #,0
# Connect as USER to database DB_NAME through peewee and initialize database proxy,0
# Load embedding model,0
---------------------------------------------------------------------- #,0
CREATE QUEUE OF IMAGES TO LABEL,0
---------------------------------------------------------------------- #,0
Use classifier to generate predictions,0
# Update model predicted class in PostgreSQL database,0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?,0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
-------------------------------------------------------------------------------- #,0
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI,0
-------------------------------------------------------------------------------- #,0
"# static routes (to serve CSS, etc.)",0
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps",0
"return bottle.static_file(filename, root='../../../../../../../../../../../.')",0
# dynamic routes,0
if data['display_grayscale']:,0
indices_to_exclude.update(set(color_indices)),0
elif not data['display_grayscale']:,0
indices_to_exclude.update(set(grayscale_indices)),0
data['display_images'] = {},0
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices],0
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices],0
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices],0
data['display_images']['detection_categories'] = [],0
for i in indices:,0
if str(dataset.samples[i][1]) == 'None':,0
data['display_images']['detection_categories'].append('None'),0
else:,0
existing_category_entries = {cat.id: cat.name for cat in Category.select()},0
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()",0
data['display_images']['detection_categories'].append(cat_name),0
Use image ids in images_to_label to get the corresponding dataset indices,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
print(set(dataset.set_indices[4]).update(set(indices_to_label))),0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Use image id images_to_label to get the corresponding dataset index,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?,0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Train on samples that have been labeled so far,0
dataset.set_kind(DetectionKind.UserDetection.value),0
print(y_train),0
Predict on the samples that have not been labeled,0
print(y_pred),0
Update model predicted class in PostgreSQL database,0
timer = time.time(),0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
print('Updating the database took %0.2f seconds'%(time.time() - timer)),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids)),0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
"once the classifier has been trained the first time, switch to AL sampling",0
# sourceMappingURL=bootstrap.bundle.min.js.map,0
"print(ap_distances.size(),an_distances.size())",0
losses = -(((-ap_distances)/128)+1+1e-16).log() - (((-(128-an_distances))/128)+1+1e-16).log(),0
import pdb,0
pdb.set_trace(),0
losses = ap_distances - an_distances + self.margin,0
print(losses.size()),0
from UIComponents.DBObjects import *,0
TODO: should this also change self.kind?,1
get the embedding representations for all samples (i.e. set current_set to all indices),0
"return PILImage.open(os.path.join(self.img_base,path)).convert('RGB')",0
"print(self.labels_set, self.n_classes)",0
from sklearn.manifold import TSNE,0
embedding= TSNE(n_components=2).fit_transform(X),0
embedding= PCA(n_components=2).fit_transform(X),0
return X,0
"print(dir(event), type(sc))",0
"print(label,bgcolor)",0
"patches.append(mpatches.Patch(color=indexcolors[i], label=label))",0
"plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,",0
"ncol=12, mode=""expand"", borderaxespad=0., handles=patches)",0
plt.legend(handles=patches),0
"plt.xlabel('Dim 1', fontsize=12)",0
"plt.ylabel('Dim 2', fontsize=12)",0
plt.grid(True),0
"plt.xlabel('Dim 1', fontsize=12)",0
"plt.ylabel('Dim 2', fontsize=12)",0
plt.grid(True),0
print(thumb),0
"img.thumbnail((16, 12), PILImage.ANTIALIAS)",0
plt.show(),0
Add all negatives for all positive pairs,0
print(triplets.shape[0]),0
compute output,0
measure accuracy,0
compute loss on this batch,0
"train on a batch, record loss, and measure accuracy (if calc_accuracy)",0
compute output,0
measure accuracy and record loss,0
switch to evaluate mode,0
compute output,0
switch to evaluate mode,0
"self.fc13 = nn.Linear(128, 64)",0
self.bn2 = nn.BatchNorm1d(64),0
x = F.relu(self.fc12(x)),0
x = F.relu(self.bn1(self.fc13(x))),0
x = F.relu(self.fc13(x)),0
"x = F.dropout(x, training=self.training)",0
save features last FC layer,0
x = F.relu(x),0
save features last FC layer,0
import matplotlib.pyplot as plt,0
plt.switch_backend('agg'),0
class EmbeddingNet(nn.Module):,0
"def __init__(self, architecture, feat_dim, use_pretrained=False):",0
"super(EmbeddingNet, self).__init__()",0
self.feat_dim= feat_dim,0
self.inner_model = models.__dict__[architecture](pretrained=use_pretrained),0
if architecture.startswith('resnet'):,0
in_feats= self.inner_model.fc.in_features,0
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)",0
elif architecture.startswith('inception'):,0
in_feats= self.inner_model.fc.in_features,0
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)",0
if architecture.startswith('densenet'):,0
in_feats= self.inner_model.classifier.in_features,0
"self.inner_model.classifier = nn.Linear(in_feats, feat_dim)",0
if architecture.startswith('vgg'):,0
in_feats= self.inner_model.classifier._modules['6'].in_features,0
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)",0
if architecture.startswith('alexnet'):,0
in_feats= self.inner_model.classifier._modules['6'].in_features,0
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)",0
"def forward(self, x):",0
return self.inner_model.forward(x),0
class NormalizedEmbeddingNet(EmbeddingNet):,0
"def __init__(self, architecture, feat_dim, use_pretrained=False):",0
"EmbeddingNet.__init__(self, architecture, feat_dim, use_pretrained = use_pretrained)",0
"def forward(self, x):",0
embedding =  F.normalize(self.inner_model.forward(x))*10.0,0
"return embedding, embedding",0
"def get_random_images(num, image_dir, test_transforms):",0
"data = datasets.ImageFolder(image_dir, transform=test_transforms) # slight abuse; this expects subfolders corresponding to classes but we have no classes here",0
indices = list(range(len(data))),0
np.random.shuffle(indices),0
idx = indices[:num],0
from torch.utils.data.sampler import SubsetRandomSampler,0
sampler = SubsetRandomSampler(idx),0
"loader = torch.utils.data.DataLoader(data,",0
"sampler=sampler, batch_size=num)",0
dataiter = iter(loader),0
"images, labels = dataiter.next()",0
"return images, labels",0
"def predict_image(image, model, test_transforms):",0
"device = torch.device(""cuda"" if torch.cuda.is_available()",0
"else ""cpu"")",0
image_tensor = test_transforms(image).float(),0
image_tensor = image_tensor.unsqueeze_(0),0
input = Variable(image_tensor),0
input = input.to(device),0
output = model(input)[0],0
return output.data.cpu().numpy(),0
Connect to database and initialize db_proxy,0
# database connection credentials,0
# load the dataset,0
Load the saved embedding model from the checkpoint,0
# update the dataset embedding,0
# Create a folder for saving embedding visualizations with this model checkpoint,0
model_emb_dirname = os.path.basename(args.base_model).split('.')[0],0
"os.makedirs(model_emb_dirname, exist_ok=True)",0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')",0
dataset.embedding_mode(),0
"assert 2==3, 'break'",0
datasetindices = list(range(len(dataset))),0
np.random.shuffle(datasetindices),0
random_indices = datasetindices[:args.num],0
print(random_indices),0
"selected_sample_features = np.array([]).reshape(0, 256)",0
selected_sample_labels = [],0
for idx in random_indices:,0
"selected_sample_features = np.vstack([selected_sample_features, X_train[idx]])",0
selected_sample_labels.append(y_train[idx]),0
img_path = imagepaths[idx].split('.JPG')[0],0
image = dataset.loader(img_path),0
selected_sample_images.append(image),0
# TRY NEAREST NEIGHBORS WALK THROUGH EMBEDDING,0
nbrs = NearestNeighbors(n_neighbors=args.num).fit(selected_sample_features),0
"distances, indices = nbrs.kneighbors(selected_sample_features)",0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')",0
"idx_w_closest_nbr = np.where(distances[:,1] == min(distances[:,1]))[0][0]",0
order = [idx_w_closest_nbr],0
for ii in range(len(distances)):,0
"distances[ii, 0] = np.inf",0
while len(order)<args.num:,0
curr_idx = order[-1],0
curr_neighbors = indices[curr_idx],0
curr_dists = list(distances[curr_idx]),0
# print(min(curr_dists)),0
next_closest_pos = curr_dists.index(min(curr_dists)),0
next_closest = curr_neighbors[next_closest_pos],0
order.append(next_closest),0
# make sure you can't revisit past nodes,0
for vi in order:,0
vi_pos = list(indices[next_closest]).index(vi),0
"distances[next_closest, vi_pos] = np.inf",0
for ii in range(len(order)):,0
imgidx = order[ii],0
image = selected_sample_images[imgidx],0
"image.save(model_emb_dirname+""/img""+str(ii)+""_""+str(selected_sample_labels[imgidx])+"".png"")",0
# Specify the transformations on the input images before inference,0
"# test_transforms = transforms.Compose([transforms.Resize([224, 224]), transforms.ToTensor()])",0
"test_transforms = transforms.Compose([transforms.Resize([256, 256]), transforms.RandomCrop([224, 224]), transforms.RandomHorizontalFlip(), transforms.ColorJitter(), transforms.ToTensor(), transforms.Normalize([0.407328, 0.407328, 0.407328], [0.118641, 0.118641, 0.118641])])",0
"images, labels = get_random_images(num, image_dir, test_transforms)",0
"all_features = np.array([]).reshape(0, 256)",0
for ii in range(len(images)):,0
image = to_pil(images[ii]),0
"features = predict_image(image, model, test_transforms)",0
"all_features = np.vstack([all_features, features])",0
# for ii in range(len(images)):,0
#     image = to_pil(images[ii]),0
"#     image.save(""img""+str(ii)+"".png"")",0
# TRY CLUSTERING,0
kmeans1 = KMeans(n_clusters=5).fit(StandardScaler().fit_transform(all_features)),0
print(kmeans1.labels_),0
for ii in range(len(images)):,0
image = to_pil(images[ii]),0
"filename = str(kmeans1.labels_[ii])+""/img""+str(ii)+"".png""",0
if not os.path.exists(os.path.dirname(filename)):,0
os.makedirs(os.path.dirname(filename)),0
image.save(filename),0
class SaveFeatures():,0
"def __init__(self, module):",0
self.hook = module.register_forward_hook(self.hook_fn),0
"def hook_fn(self, module, input, output):",0
"self.features = torch.tensor(output, requires_grad=True).cuda()",0
def close(self):,0
self.hook.remove(),0
Load the saved embedding model from the checkpoint,0
"Get a sample from the database, with eval transforms applied, etc.",0
Connect to database and sample a dataset,0
output = model.forward(sample_image.unsqueeze(0)),0
print(output),0
with torch.no_grad():,0
sample_image_input = sample_image.cuda(non_blocking=True),0
"_, output = model(sample_image_input) # compute output",0
print(output),0
sample_image = PILImage.open(sample_image_path).convert('RGB'),0
"sample_image = transforms.Compose([Resize([256, 256]), CenterCrop(([[224,224]])), ToTensor(), Normalize([0.369875, 0.388726, 0.347536], [0.136821, 0.143952, 0.145229])])(sample_image)",0
print(list(model_inner_resnet.children())),0
print(model_inner_resnet.fc),0
print(model_inner_resnet.fc0),0
# print(model_inner_resnet.layer4[0].conv2),0
# print(type(model)),0
# print(len(list(model_inner_resnet.children()))),0
# print(list(model.children())),0
# print(list(list(model.children())[0].children())),0
"img = np.uint8(np.random.uniform(150, 180, (56, 56, 3)))/255",0
"img_tensor = torch.unsqueeze(torch.from_numpy(img), 0)",0
full_out = model_inner_resnet.forward(img_tensor),0
print(full_out),0
model(img_tensor),0
activations = SaveFeatures(model_inner_resnet.layer4[0].conv2),0
print(activations.features),0
print(type(activations.features)),0
activations.close(),0
Connect to database and sample a dataset,0
Load the saved embedding model from the checkpoint,0
Update the dataset embedding,0
Get a random query image,0
# # # IMAGES IN THE SAME SEQUENCE # # # #,0
"assert 2==3, 'break'",0
# # # CLOSEST IN (EMBEDDING) FEATURE SPACE # # # #,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Variables for the hierarchical cluster,0
Variables for the AL algorithm,0
Data variables,0
"connectivity = kneighbors_graph(self.transformed_X,max_features)",0
Fit cluster and update cluster variables,0
The sklearn hierarchical clustering algo numbers leaves which correspond,0
to actual datapoints 0 to n_points - 1 and all internal nodes have,0
ids greater than n_points - 1 with the root having the highest node id,0
"If no labels have been observed, simply return uniform distribution",0
"If no observations, return worst possible upper lower bounds",0
Loop through generations from bottom to top,0
Update admissible labels for node,0
Calculate score,0
Determine if node should be split,0
Make sure label set for node so that we can flow to children,0
if necessary,0
Only split if all ancestors are admissible nodes,0
This is part  of definition of admissible pruning,0
Check that pruning covers all leave nodes,0
Fill in labels,0
Observe labels for previously recommended batches,0
TODO(lishal): implement multiple selection methods,1
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
A list of initialized samplers is allowed as an input because,0
"for AL_methods that search over different mixtures, may want mixtures to",0
have shared AL_methods so that initialization is only performed once for,0
computation intensive methods like HierarchicalClusteringAL and,0
states are shared between mixtures.,0
"If initialized samplers are not provided, initialize them ourselves.",0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copy these objects to make sure they are not modified while simulating,0
trajectories as they are used later by the main run_experiment script.,0
Assumes that model has already by fit using all labeled data so,0
the probabilities can be used immediately to hallucinate labels,0
All models need to have predict_proba method,0
Hallucinate labels for selected datapoints to be label,0
using class probabilities from model,0
"Not saving already_selected here, if saving then should sort",0
only for the input to fit but preserve ordering of indices in,0
already_selected,0
Useful to know how accuracy compares for model trained on hallucinated,0
labels vs trained on true labels.  But can remove this train to speed,0
up simulations.  Won't speed up significantly since many more models,0
are being trained inside the loop above.,0
Save trajectory for reference,0
Delete created copies,0
THE INPUTS CANNOT BE MODIFIED SO WE MAKE COPIES FOR THE CHECK LATER,0
Should check model but kernel_svm does not have coef_ so need better,0
handling here,0
Make sure that model object fed in did not change during simulations,0
Return indices based on return type specified,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Set gamma for gaussian kernel to be equal to 1/n_features,0
kneighbors graph is constructed using k=10,0
"Make connectivity matrix symmetric, if a point is a k nearest neighbor of",0
"another point, make it vice versa",0
Graph edges are weighted by applying gaussian kernel to manhattan dist.,0
"By default, gamma for rbf kernel is equal to 1/n_features but may",0
get better results if gamma is tuned.,0
Define graph density for an observation to be sum of weights for all,0
edges to the node representing the datapoint.  Normalize sum weights,0
by total number of neighbors.,0
"If a neighbor has already been sampled, reduce the graph density",0
for its direct neighbors to promote diversity.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Update observed reward and arm probabilities,0
Sample an arm,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
y only used for determining how many clusters there should be,0
probably not practical to assume we know # of classes before hand,0
should also probably scale with dimensionality of data,0
Probably okay to always use MiniBatchKMeans,0
Should standardize data before clustering,0
Can cluster on standardized data but train on raw features if desired,0
TODO(lishal): have MarginSampler and this share margin function,1
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Probably okay to always use MiniBatchKMeans,0
Should standardize data before clustering,0
Can cluster on standardized data but train on raw features if desired,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Update min_distances for all examples given new cluster center.,0
Assumes that the transform function takes in original data and not,0
flattened data.,0
Initialize centers with a randomly selected datapoint,0
New examples should not be in already selected since those points,0
should have min_distance of zero to a cluster center.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
This is uniform given the remaining pool but biased wrt the entire pool.,0
sample = [i for i in range(self.X.shape[0]) if i not in already_selected],0
return sample[0:N],0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Fields for hierarchical clustering AL,0
Setting parent and storing nodes in dict for fast access,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Connect to database DB_NAME as USER and initialize tables,0
Populate Info table,0
Populate Category table,0
"# For now, we have a predefined list of species we expect to see in the camera trap database (e.g. maybe from a quick look through the images)",0
# TODO: allow user to update the class list through the labeling tool UI as they see different species,1
Populate Image and Detection tables,0
Image entry data,0
Detection entry data,0
Connect to database DB_NAME as USER and initialize tables,0
Populate Info table,0
Populate Category table,0
# Create category records from species present in the COCO camera trap classification dataset (COCO classes minus any excluded classes),0
# TODO: allow user to update the class list through the labeling tool UI as they see different species,1
Populate Image and Detection tables,0
"with open(os.path.join(args.crop_dir,'crops.json'), 'r') as infile:",0
crops_json = json.load(infile),0
counter = 0,0
timer = time.time(),0
num_detections = len(crops_json),0
for detectionid in crops_json:,0
counter += 1,0
detection_data = crops_json[detectionid],0
# Image entry data,0
existing_image_entries = Image.select().where((Image.file_name == detection_data['file_name'])),0
try:,0
existing_image_entry = existing_image_entries.get(),0
except:,0
"image_entry = Image.create(id=detectionid, file_name=detection_data['file_name'], width=detection_data['width'], height=detection_data['height'], grayscale=detection_data['grayscale'],",0
"source_file_name=detection_data['source_file_name'], relative_size=detection_data['relative_size'],",0
"seq_id=detection_data['seq_id'], seq_num_frames=detection_data['seq_num_frames'], frame_num=detection_data['frame_num'])",0
image_entry.save(),0
# Detection entry data,0
"detection_entry = Detection.create(id=detectionid, image=detectionid, bbox_confidence=detection_data['bbox_confidence'],",0
"bbox_X1=detection_data['bbox_X1'], bbox_Y1=detection_data['bbox_Y1'], bbox_X2=detection_data['bbox_X2'], bbox_Y2=detection_data['bbox_Y2'],",0
kind=DetectionKind.ModelDetection.value),0
detection_entry.save(),0
if counter%100 == 0:,0
"print('Updated database with Image and Detection table entries for %d out of %d crops in %0.2f seconds'%(counter, num_detections, time.time() - timer))",0
# data related to original image,0
Get class names from .txt list,0
Initialize Oracle table,0
Map filenames to classes (NOTE: we assume a single image does not contain more than one class),0
"For each detection, use source image path to get class",0
TODO update: Assumes that crops have already,1
been generated for the images using make_active_learning_classification_dataset.py. The created DB contains tables:,0
- info: information about the dataset,0
- image: images present in the dataset,0
- detections: crops of images with detections with confidence greater than a specified threshold,0
Initialize Database,0
# database connection credentials,0
HOST = 'localhost',0
PORT = 5432,0
"# first, make sure the (user, password) has been created",0
"# sudo -u postgres psql -c ""CREATE USER <db_user> WITH PASSWORD <db_password>;""",0
"# sudo -u postgres psql -c ""CREATE DATABASE <db_name> WITH OWNER <db_user> CONNECTION LIMIT -1;""",0
"# sudo -u postgres psql -c ""GRANT CONNECT ON DATABASE <db_name> TO <db_user>;""",0
"# sudo -u postgres psql -d <db_name> -c ""CREATE EXTENSION IF NOT EXISTS \""uuid-ossp\"";""",0
# Try to connect as USER to database DB_NAME through peewee,0
Populate Tables,0
# create Info table,0
# get class names for Category table,0
Faster anD available in Python 3.5 and above,0
# iterate through images in each class folder,0
killing this process after over 38 hours adding over 500k white-tailed deer crops from emammal,0
resuming for remaining classes,0
# get cropped image data for Image table,0
"if mean of each channel is about the same, image is likely grayscale",0
# still have no info on these:,0
seq_id = CharField(null= True)                # sequence identifier for the original image,0
seq_num_frames = IntegerField(null = True)    # number of frames in sequence,0
frame_num = IntegerField(null = True)         # which frame number in sequence,0
location = CharField(null = True)             # location of camera trap,0
datetime = DateTimeField(null = True),0
# store info about the detection corresponding to this image,0
# store info about the true labels for the detection,0
#  - for pretrain dataset this is the same as the detection_category if the detection categories,0
print(classes),0
Connect to database and sample a dataset,0
Load the saved embedding model from the checkpoint,0
Update the dataset embedding,0
save the images,0
save the features,0
"with open(os.path.join(args.output_dir, 'lastlayer_features.mat'), 'wb') as f:",0
"pickle.dump(sample_features, f)",0
"with open(os.path.join(args.output_dir, 'labels.mat'), 'wb') as f:",0
"pickle.dump(sample_labels, f)",0
"parser.add_argument('--db_name', default='missouricameratraps', type=str, help='Name of the training (target) data Postgres DB.')",0
"parser.add_argument('--db_user', default='user', type=str, help='Name of the user accessing the Postgres DB.')",0
"parser.add_argument('--db_password', default='password', type=str, help='Password of the user accessing the Postgres DB.')",0
"parser.add_argument('--base_model', type=str, help='Path to latest embedding model checkpoint.')",0
"parser.add_argument('--output_dir', type=str, help='Output directory for subset of crops')",0
Add json entry for this crop,0
Copy file for this crop to subset dataset crop dir,0
Copy file for its full-size source image to subset dataset image dir,0
Write crops.json to subset dataset crop dir,0
store info about the crops produced in a JSON file,0
------------------------------------------------------------------------------------------------------------#,0
COMMENT OUT IF NOT USING A SPECIFIC PROJECT WITHIN ROBERT LONG EMAMMAL DATASET,0
------------------------------------------------------------------------------------------------------------#,0
get some information about the source image,0
------------------------------------------------------------------------------------------------------------#,0
NOTE: EDIT THIS SECTION BASED ON DATASET SOURCE,0
get info about sequence the source image belongs to from path and directory,0
# missouricameratraps:,0
imgframenum = int(os.path.basename(imgfile).split('.JPG')[0].split('_')[-1]),0
imgseqid = int(os.path.split(os.path.dirname(imgfile))[-1]),0
"imgseqnumframes = len([name for name in os.listdir(os.path.dirname(imgfile)) if os.path.isfile(os.path.join(os.path.dirname(imgfile), name))])",0
# emammal:,0
------------------------------------------------------------------------------------------------------------#,0
utility function,0
compute output,0
measure accuracy and record loss,0
switch to train mode,0
measure accuracy and record loss,0
compute output,0
measure accuracy and record loss,0
switch to evaluate mode,0
compute output,0
switch to evaluate mode,0
compute output,0
switch to evaluate mode,0
"print(self.labels_set, self.n_classes)",0
Add all negatives for all positive pairs,0
print(triplets.shape[0]),0
constructor,0
update embedding values after a finetuning,0
select either the default or active pools,0
gather test set,0
gather train set,0
finetune the embedding model over the labeled pool,0
a utility function for saving the snapshot,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Variables for the hierarchical cluster,0
Variables for the AL algorithm,0
Data variables,0
"connectivity = kneighbors_graph(self.transformed_X,max_features)",0
Fit cluster and update cluster variables,0
The sklearn hierarchical clustering algo numbers leaves which correspond,0
to actual datapoints 0 to n_points - 1 and all internal nodes have,0
ids greater than n_points - 1 with the root having the highest node id,0
"If no labels have been observed, simply return uniform distribution",0
"If no observations, return worst possible upper lower bounds",0
Loop through generations from bottom to top,0
Update admissible labels for node,0
Calculate score,0
Determine if node should be split,0
Make sure label set for node so that we can flow to children,0
if necessary,0
Only split if all ancestors are admissible nodes,0
This is part  of definition of admissible pruning,0
Check that pruning covers all leave nodes,0
Fill in labels,0
Observe labels for previously recommended batches,0
TODO(lishal): implement multiple selection methods,1
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
A list of initialized samplers is allowed as an input because,0
"for AL_methods that search over different mixtures, may want mixtures to",0
have shared AL_methods so that initialization is only performed once for,0
computation intensive methods like HierarchicalClusteringAL and,0
states are shared between mixtures.,0
"If initialized samplers are not provided, initialize them ourselves.",0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copy these objects to make sure they are not modified while simulating,0
trajectories as they are used later by the main run_experiment script.,0
Assumes that model has already by fit using all labeled data so,0
the probabilities can be used immediately to hallucinate labels,0
All models need to have predict_proba method,0
Hallucinate labels for selected datapoints to be label,0
using class probabilities from model,0
"Not saving already_selected here, if saving then should sort",0
only for the input to fit but preserve ordering of indices in,0
already_selected,0
Useful to know how accuracy compares for model trained on hallucinated,0
labels vs trained on true labels.  But can remove this train to speed,0
up simulations.  Won't speed up significantly since many more models,0
are being trained inside the loop above.,0
Save trajectory for reference,0
Delete created copies,0
THE INPUTS CANNOT BE MODIFIED SO WE MAKE COPIES FOR THE CHECK LATER,0
Should check model but kernel_svm does not have coef_ so need better,0
handling here,0
Make sure that model object fed in did not change during simulations,0
Return indices based on return type specified,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Set gamma for gaussian kernel to be equal to 1/n_features,0
kneighbors graph is constructed using k=10,0
"Make connectivity matrix symmetric, if a point is a k nearest neighbor of",0
"another point, make it vice versa",0
Graph edges are weighted by applying gaussian kernel to manhattan dist.,0
"By default, gamma for rbf kernel is equal to 1/n_features but may",0
get better results if gamma is tuned.,0
Define graph density for an observation to be sum of weights for all,0
edges to the node representing the datapoint.  Normalize sum weights,0
by total number of neighbors.,0
"If a neighbor has already been sampled, reduce the graph density",0
for its direct neighbors to promote diversity.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Update observed reward and arm probabilities,0
Sample an arm,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
y only used for determining how many clusters there should be,0
probably not practical to assume we know # of classes before hand,0
should also probably scale with dimensionality of data,0
Probably okay to always use MiniBatchKMeans,0
Should standardize data before clustering,0
Can cluster on standardized data but train on raw features if desired,0
TODO(lishal): have MarginSampler and this share margin function,1
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Probably okay to always use MiniBatchKMeans,0
Should standardize data before clustering,0
Can cluster on standardized data but train on raw features if desired,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Update min_distances for all examples given new cluster center.,0
Assumes that the transform function takes in original data and not,0
flattened data.,0
Initialize centers with a randomly selected datapoint,0
New examples should not be in already selected since those points,0
should have min_distance of zero to a cluster center.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
This is uniform given the remaining pool but biased wrt the entire pool.,0
sample = [i for i in range(self.X.shape[0]) if i not in already_selected],0
return sample[0:N],0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Fields for hierarchical clustering AL,0
Setting parent and storing nodes in dict for fast access,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Faster and available in Python 3.5 and above,0
transform_list.append(CenterCrop((processed_size))),0
transform_list.append(Lambda(lambda X: normalize(X))),0
"print(self.labels_set, self.n_classes)",0
"from PyQt5 import QtCore, QtWidgets,QtGui",0
from collections import deque,0
from peewee import *,0
from UIComponents.Tag import Tag,0
policy.setHeightForWidth(True),0
"print(self.tab1.parentWidget(),self)",0
self.tab4.add.clicked.connect(self.addSpecies),0
self.tab4.update.clicked.connect(self.updateSpecies),0
checkpoint= load_checkpoint('../merge/triplet_model_0054.tar'),0
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))",0
"embedding_net = EmbeddingNet(checkpoint['arch'], checkpoint['feat_dim'])",0
if checkpoint['loss_type'].lower()=='center':,0
"model = torch.nn.DataParallel(ClassificationNet(embedding_net, n_classes=14)).cuda()",0
else:,0
model= torch.nn.DataParallel(embedding_net).cuda(),0
model.load_state_dict(checkpoint['state_dict']),0
self.parentWidget().progressBar.setMaximum(len(run_dataset)//2048),0
"e=Engine(model,None,None, verbose=True,progressBar= self.parentWidget().progressBar)",0
label = [ x[1] for x in run_dataset.samples],0
"print(indices,selected_set)",0
print(query.sql()),0
src.delete().where(src.image_id<<rList)),0
"det= UserDetection.create(category_id=0, id=str(index+label[1][2]),image_id=final[0], bbox_X=label[1][0], bbox_Y=label[1][1], bbox_W=label[1][2], bbox_H=label[1][3])",0
for x in self.tab1.grid.tags:,0
x.delete_instance(),0
db.create_tables([Detection]),0
This is simply to show the bar,0
"p = Process(target=ex.active, args=())",0
p.start(),0
p.join(),0
ex.active(),0
ex.centralWidget().setCurrentIndex(1),0
main(),0
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))",0
print(row),0
"unq_id= ""crops_""+str(uuid.uuid1())",0
"print(line,imageWidth,imageHeight)",0
"print(""%s,%s,%s,%0.6f,%0.6f,%0.6f,%0.6f,%0.6f""%(line[0], line[1],line[2],float(line[3]),topRel,leftRel,bottomRel,rightRel))",0
if not os.path.exists(dest):,0
os.mkdir(dest),0
raise,0
out.close(),0
"print length,(i-1)*length,i*length",0
matplotlib.use('Agg'),0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
"conf.reset()""""""",0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
conf.reset(),0
for clf in classifiers:,0
"clf.fit(X_train, y_train)",0
preds= clf.predict_proba(X),0
uncertainty+= preds.max(axis=1),0
print(uncertainty[ind]),0
print(uncertainty[ind]),0
true_labels = y[unlabeled_indices],0
select up to 5 digit examples that the classifier is most uncertain about,0
remember best acc@1 and save checkpoint,0
"completeClassificationLoop(run_dataset, model,num_classes)",0
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)",0
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd),0
labels = db.labels_,0
"mapp=(find_probablemap(label,labels, K=args.K))",0
"print(""Clusters"")",0
"for i,x in enumerate(labels):",0
labels[i]= mapp[x],0
print(np.sum(labels == label)/labels.size),0
"print(""Confidence Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning(embd, label, idx)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(""Entropy Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning_entropy(embd, label, idx)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(idx,idx.shape)",0
for i in idx:,0
print(paths[i]),0
"print(""Silohette active learning"")",0
"idx= active_learning2(embd, 1000, args.num_clusters)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
embd= reduce_dimensionality(embd)#[0:10000]),0
labels= labels[0:10000],0
label= label[0:10000],0
paths= paths[0:10000],0
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])",0
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])",0
plt.show(),0
"np.save(args.name_prefix+""_embeddings.npy"",embd)",0
"np.save(args.name_prefix+""_labels.npy"",label)",0
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")",0
del output,0
define loss function (criterion) and optimizer,0
"optimizer = torch.optim.Adam(params, lr=args.lr, weight_decay= args.weight_decay)",0
optimizer.load_state_dict(checkpoint['optimizer']),0
train for one epoch,0
evaluate on validation set,0
matplotlib.use('Agg'),0
selected_set.add(rand_ind[i]),0
"embedding_net = EmbeddingNet('resnet50', 256, True)",0
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)",0
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)",0
print('Embedding Done'),0
sys.stdout.flush(),0
Random examples to start,0
"random_ids = noveltySamples(unlabeled_dataset.em, unlabeled_dataset.getIDs(), 1000)",0
print(random_ids),0
Move Records,0
Finetune the embedding model,0
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)",0
unlabeled_dataset.updateEmbedding(model),0
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})",0
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()",0
matplotlib.use('Agg'),0
"uncertainty= np.apply_along_axis(stats.entropy,1,probs) * (1 - probs.max(axis=1))",0
"copy(paths[srt[i]], ""active"")",0
"plot_together( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()), preds, base_ind, dataset.getpaths(), {})",0
"return np.random.choice(range(0,prob_list[0].shape[0]), 100, replace=False).tolist()",0
selected_set.add(rand_ind[i]),0
"query= Detection.update(kind = destKind.value).where(Detection.id.in_(rList), Detection.kind == srcKind.value)",0
query.execute(),0
"embedding_net = EmbeddingNet('resnet50', 256, True)",0
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)",0
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)",0
print('Embedding Done'),0
sys.stdout.flush(),0
"plot_embedding(dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})",0
Random examples to start,0
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)",0
print(random_ids),0
Move Records,0
Finetune the embedding model,0
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)",0
unlabeled_dataset.updateEmbedding(model),0
"print(dataset.em[dataset.current_set].shape, np.asarray(dataset.getlabels()).shape, len(dataset.getpaths()))",0
"plot_embedding( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})",0
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})",0
train_eval_classifier(),0
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()",0
"names = [""Linear SVM"", ""RBF SVM"", ""Random Forest"", ""Neural Net"", ""Naive Bayes""]",0
"classifiers = [SVC(kernel=""linear"", C=0.025, probability= True, class_weight='balanced'),",0
"SVC(gamma=2, C=1, probability= True, class_weight='balanced'),",0
"RandomForestClassifier(max_depth=None, n_estimators=100, class_weight='balanced'),",0
"MLPClassifier(alpha=1),",0
GaussianNB()],0
estimators= [],0
"for name, clf in zip(names, classifiers):",0
"estimators.append((name, clf))",0
"eclf1 = VotingClassifier(estimators= estimators, voting='hard')",0
"eclf2 = VotingClassifier(estimators= estimators, voting='soft')",0
"names.append(""ensemble hard"")",0
classifiers.append(eclf1),0
"names.append(""ensemble soft"")",0
classifiers.append(eclf2),0
dataset.image_mode(),0
dataset.updateEmbedding(model),0
y_pred= clf.predict(X_test),0
"print(confusion_matrix(y_test, y_pred))",0
paths= dataset.getpaths(),0
"for i, (yp, yt) in enumerate(zip(y_pred, y_test)):",0
if yp != yt:,0
"copy(paths[i],""mistakes"")",0
"print(yt, yp, paths[i],i)",0
"clf_output= clf_e.embedding(eval_loader, dim=48)",0
"self.train_transform = transforms.Compose([Resize(raw_size), CenterCrop((processed_size)), ToTensor(), Normalize(mean, std)])",0
"print(self.labels_set, self.n_classes)",0
"print(line,imageWidth,imageHeight)",0
"print length,(i-1)*length,i*length",0
#########################################################,0
## Configuration,0
#########################################################,0
## The actual code,0
Check arguments,0
Create output directories,0
Padding around the detected objects when cropping,0
1.3 for the cropping during test time and 1.3 for,0
the context that the CNN requires in the left-over,0
image,0
Load a (frozen) Tensorflow model into memory.,0
## Preparations: get all the output tensors,0
The following processing is only for single image,0
Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.,0
Follow the convention by adding back the batch dimension,0
For all images in the image directoryig,0
Load image,0
Run inference,0
"all outputs are float32 numpy arrays, so convert types as appropriate",0
Add detections to the collection,0
Get info about the image,0
Select detections with a confidence larger than DETECTION_CONFIDENCE,0
Get these boxes and convert normalized coordinates to pixel coordinates,0
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes",0
"However, we need to make sure that it box coordinates are still within the image",0
"For each detected bounding box with high confidence, we will",0
crop the image to the padded box and save it,0
generate a unique identifier for the detection,0
"bbox is the detected box, crop_box the padded / enlarged box",0
Add numbering to the original file name if there are multiple boxes,0
The absolute file path where we will store the image,0
"if COCO_OUTPUT_DIR is set, then we will only use the shape",0
of cropped_img in the following code. So instead of reading,0
cropped_img = np.array(Image.open(out_file)),0
we can speed everything up by reading only the size of the image,0
matplotlib.use('Agg'),0
conf= ConfusionMatrix(24),0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
"conf.reset()""""""",0
define loss function (criterion) and optimizer,0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
conf.reset(),0
for clf in classifiers:,0
"clf.fit(X_train, y_train)",0
preds= clf.predict_proba(X),0
uncertainty+= preds.max(axis=1),0
print(uncertainty[ind]),0
print(uncertainty[ind]),0
true_labels = y[unlabeled_indices],0
select up to 5 digit examples that the classifier is most uncertain about,0
"print(indices,selected_set)",0
print(query.sql()),0
remember best acc@1 and save checkpoint,0
"completeClassificationLoop(run_dataset, model,num_classes)",0
"embd, label, paths = extract_embeddings(run_loader, model)",0
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)",0
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd),0
labels = db.labels_,0
"mapp=(find_probablemap(label,labels, K=args.K))",0
"print(""Clusters"")",0
"for i,x in enumerate(labels):",0
labels[i]= mapp[x],0
print(np.sum(labels == label)/labels.size),0
"print(""Confidence Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning(embd, label, idx)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(""Entropy Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning_entropy(embd, label, idx)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"new_selected= selectSamples(embd,paths,3000)",0
"print(idx,idx.shape)",0
for i in idx:,0
print(paths[i]),0
"print(""Silohette active learning"")",0
"idx= active_learning2(embd, 1000, args.num_clusters)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(""Random"")",0
"idx = np.random.choice(np.arange(len(paths)), 1000, replace=False)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
embd= reduce_dimensionality(embd)#[0:10000]),0
labels= labels[0:10000],0
label= label[0:10000],0
paths= paths[0:10000],0
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])",0
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])",0
plt.show(),0
"np.save(args.name_prefix+""_embeddings.npy"",embd)",0
"np.save(args.name_prefix+""_labels.npy"",label)",0
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")",0
combo.setEnabled(not finalized),0
print(self.bbox),0
self.addWidget(self.child),0
Moving container with arrows,0
Left - Bottom,0
Right-Bottom,0
Left-Top,0
Right-Top,0
Left - Bottom,0
Right - Bottom,0
Left - Top,0
Right - Top,0
check cursor horizontal position,0
check cursor vertical position,0
self.resizeEvent=self.onResize,0
"print(""Parent"", parent, parent.width(), parent.height())",0
"self.setGeometry(0,0,410,307)",0
"print(w,h,""w,h"")",0
"print(""final"",tag.getFinal())",0
pass,0
"self.tags.append(TContainer(self,Category.get(-1),[0,0,0.1,0.1],True, Qt.red))",0
label= CharField(),0
fullname=str(self.model),0
"self.name= (fullname[fullname.find("":"")+2:fullname.find("">"")].strip()+'_set').lower()",0
"print(""Parent"", self.parentWidget().width(), self.parentWidget().height())",0
"print(self.model,self.name,query.sql())",0
self.tab4.speciesList.setModel(species),0
"self.tab4.speciesList.setRowHidden(len(species.stringList())-1, True)",0
self.speciesList.itemChanged.connect(self.itemChanged),0
#####,0
,0
video_utils.py,0
,0
"Utilities for splitting, rendering, and assembling videos.",0
,0
#####,0
"%% Constants, imports, environment",0
from ai4eutils,0
%% Path utilities,0
%% Function for rendering frames to video and vice-versa,0
http://tsaith.github.io/combine-images-into-a-video-with-python-3-and-opencv-3.html,0
Determine the width and height from the first image,0
Define the codec and create VideoWriter object,0
"If we're not over-writing, check whether all frame images already exist",0
"OpenCV seems to over-report the number of frames by 1 in some cases, or fails",0
"to read the last frame; either way, I'm allowing one missing frame.",0
"print(""Rendering video {}, couldn't find frame {}"".format(",0
"input_video_file,missing_frame_number))",0
...if we need to check whether to skip this video entirely,0
"for frame_number in tqdm(range(0,n_frames)):",0
print('Skipping frame {}'.format(frame_filename)),0
Recursively enumerate video files,0
Create the target output folder,0
Render frames,0
input_video_file = input_fn_absolute; output_folder = output_folder_video,0
For each video,0
,0
input_fn_relative = input_files_relative_paths[0],0
"process_detection_with_options = partial(process_detection, options=options)",0
zero-indexed,0
Load results,0
# Break into videos,0
im = images[0],0
# For each video...,0
video_name = list(video_to_frames.keys())[0],0
frame = frames[0],0
At most one detection for each category for the whole video,0
category_id = list(detection_categories.keys())[0],0
Find the nth-highest-confidence video to choose a confidence value,0
Prepare the output representation for this video,0
'max_detection_conf' is no longer included in output files by default,0
...for each video,0
Write the output file,0
%% Test driver,0
%% Constants,0
%% Split videos into frames,0
"%% List image files, break into folders",0
Find unique folders,0
fn = frame_files[0],0
%% Load detector output,0
%% Render detector frames,0
folder = list(folders)[0],0
d = detection_results_this_folder[0],0
...for each file in this folder,0
...for each folder,0
%% Render output videos,0
folder = list(folders)[0],0
...for each video,0
,0
run_inference_with_yolov5_val.py,0
,0
Runs a folder of images through MegaDetector (or another YOLOv5 model) with YOLOv5's,0
"val.py, converting the output to the standard MD format.  The main goal is to leverage",0
YOLO's test-time augmentation tools.,0
,0
"YOLOv5's val.py uses each file's base name as a unique identifier, which doesn't work",0
when you have typical camera trap images like:,0
,0
a/b/c/RECONYX0001.JPG,0
d/e/f/RECONYX0001.JPG,0
,0
...so this script jumps through a bunch of hoops to put a symlinks in a flat,0
"folder, run YOLOv5 on that folder, and map the results back to the real files.",0
,0
"Currently requires the user to supply the path where a working YOLOv5 install lives,",0
and assumes that the current conda environment is all set up for YOLOv5.,0
,0
TODO:,1
,0
* Figure out what happens when images are corrupted... right now this is the #1,0
"reason not to use this script, it may be the case that corrupted images look the",0
same as empty images.,0
,0
* Multiple GPU support,0
,0
* Checkpointing,0
,0
* Windows support (I have no idea what all the symlink operations will do on Windows),0
,0
"* Support alternative class names at the command line (currently defaults to MD classes,",0
though other class names can be supplied programmatically),0
,0
%% Imports,0
%% Options class,0
# Required ##,0
# Optional ##,0
%% Main function,0
#%% Path handling,0
#%% Enumerate images,0
#%% Create symlinks to give a unique ID to each image,0
i_image = 0; image_fn = image_files_absolute[i_image],0
...for each image,0
#%% Create the dataset file,0
Category IDs need to be continuous integers starting at 0,0
#%% Prepare YOLOv5 command,0
#%% Run YOLOv5 command,0
#%% Convert results to MD format,0
"We'll use the absolute path as a relative path, and pass '/'",0
as the base path in this case.,0
#%% Clean up,0
...def run_inference_with_yolo_val(),0
%% Command-line driver,0
%% Scrap,0
%% Test driver (folder),0
%% Test driver (file),0
%% Preview results,0
options.sample_seed = 0,0
...for each prediction file,0
%% Compare results,0
Choose all pairwise combinations of the files in [filenames],0
"%% Constants, imports, environment",0
from multiprocessing.pool import ThreadPool as workerpool,0
Number of images to pre-fetch,0
Useful hack to force CPU inference.,1
,0
"Need to do this before any PT/TF imports, which happen when we import",0
from run_detector.,0
Numpy FutureWarnings from tensorflow import,0
%% Support functions for multiprocessing,0
TODO,1
,0
The queue system is a little more elegant if we start one thread for reading and one,1
"for processing, and this works fine on Windows, but because we import TF at module load,",0
"CUDA will only work in the main process, so currently the consumer function runs here.",0
,0
"To enable proper multi-GPU support, we may need to move the TF import to a separate module",0
that isn't loaded until very close to where inference actually happens.,0
%% Other support funtions,0
%% Image processing functions,0
%% Main function,0
Handle the case where image_file_names is not yet actually a list,0
"Find the images to score; images can be a directory, may need to recurse",0
A json list of image paths,0
A single image file,0
Load the detector,0
Does not count those already processed,0
Will not add additional entries not in the starter checkpoint,0
Write a checkpoint if necessary,0
"Back up any previous checkpoints, to protect against crashes while we're writing",0
the checkpoint file.,0
Write the new checkpoint,0
Remove the backup checkpoint if it exists,0
...if it's time to make a checkpoint,0
"When using multiprocessing, let the workers load the model",0
"Results may have been modified in place, but we also return it for",0
backwards-compatibility.,0
The typical case: we need to build the 'info' struct,0
"If the caller supplied the entire ""info"" struct",0
"The 'max_detection_conf' field used to be included by default, and it caused all kinds",0
"of headaches, so it's no longer included unless the user explicitly requests it.",0
%% Interactive driver,0
%%,0
%% Command-line driver,0
This is an experimental hack to allow the use of non-MD YOLOv5 models through,1
the same infrastructure; it disables the code that enforces MDv5-like class lists.,0
Load the checkpoint if available,0
,0
Relative file names are only output at the end; all file paths in the checkpoint are,0
still full paths.,0
"Find the images to score; images can be a directory, may need to recurse",0
A json list of image paths,0
A single image file,0
Test that we can write to the output_file's dir if checkpointing requested,0
"Don't overwrite existing checkpoint files, this is a sure-fire way to eventually",0
erase someone's checkpoint.,0
"Commenting this out for now... the scenario where we are resuming from a checkpoint,",0
then immediately overwrite that checkpoint with empty data is higher-risk than the,0
annoyance of crashing a few minutes after starting a job.,0
Confirm that we can write to the checkpoint path; this avoids issues where,0
we crash after several thousand images.,0
%% Imports,0
import pre- and post-processing functions from the YOLOv5 repo,0
scale_coords() became scale_boxes() in later YOLOv5 versions,0
%% Classes,0
padded resize,0
"Image size can be an int (which translates to a square target size) or (h,w)",0
...if the caller has specified an image size,0
NMS,0
"As of v1.13.0.dev20220824, nms is not implemented for MPS.",0
,0
Send predication back to the CPU to fix.,0
format detections/bounding boxes,0
"This is a loop over detection batches, which will always be length 1 in our case,",0
since we're not doing batch inference.,0
Rescale boxes from img_size to im0 size,0
"normalized center-x, center-y, width and height",0
"MegaDetector output format's categories start at 1, but the MD",0
model's categories start at 0.,0
...for each detection in this batch,0
...if this is a non-empty batch,0
...for each detection batch,0
...try,0
for testing,0
"MegaDetector was trained with batch size of 1, and the resizing function is a part",0
of the inference graph,0
"change from [y1, x1, y2, x2] to [x1, y1, width, height]",0
convert numpy floats to Python floats,0
need to change the above line to the following if supporting a batch size > 1 and resizing to the same size,0
"np_images = [np.asarray(image, np.uint8) for image in images]",0
"images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)",0
performs inference,0
our batch size is 1; need to loop the batch dim if supporting batch size > 1,0
"%% Constants, imports, environment",0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Numpy FutureWarnings from tensorflow import,0
Useful hack to force CPU inference,1
os.environ['CUDA_VISIBLE_DEVICES'] = '-1',0
An enumeration of failure reasons,0
Number of decimal places to round to for confidence and bbox coordinates,0
Label mapping for MegaDetector,0
Should we allow classes that don't look anything like the MegaDetector classes?,0
"Each version of the detector is associated with some ""typical"" values",0
"that are included in output files, so that downstream applications can",0
use them as defaults.,0
%% Classes,0
Stick this into filenames before the extension for the rendered result,0
%% Utility functions,0
mps backend only available in torch >= 1.12.0,0
%% Main function,0
Dictionary mapping output file names to a collision-avoidance count.,0
,0
"Since we'll be writing a bunch of files to the same folder, we rename",0
as necessary to avoid collisions.,0
...def input_file_to_detection_file(),0
Image is modified in place,0
...for each image,0
...def load_and_run_detector(),0
%% Command-line driver,0
Must specify either an image file or a directory,0
"but for a single image, args.image_dir is also None",0
%% Interactive driver,0
%%,0
#####,0
,0
process_video.py,0
,0
"Split a video (or folder of videos) into frames, run the frames through run_detector_batch.py,",0
and optionally stitch together results into a new video with detection boxes.,0
,0
TODO: allow video rendering when processing a whole folder,1
TODO: allow video rendering from existing results,1
,0
#####,0
"%% Constants, imports, environment",0
Only relevant if render_output_video is True,0
Folder to use for extracted frames,0
Folder to use for rendered frames (if rendering output video),0
Should we render a video with detection boxes?,0
,0
"Only supported when processing a single video, not a folder.",0
"If we are rendering boxes to a new video, should we keep the temporary",0
rendered frames?,0
Should we keep the extracted frames?,0
%% Main functions,0
"TODO: keep track of whether we created this folder, delete if we're deleting the extracted",0
"frames and we created the folder, and the output files aren't in the same folder.  For now,",0
we're just deleting the extracted frames and leaving the empty folder around in this case.,0
Render detections to images,0
"TODO: keep track of whether we created this folder, delete if we're deleting the rendered",0
"frames and we created the folder, and the output files aren't in the same folder.  For now,",0
we're just deleting the rendered frames and leaving the empty folder around in this case.,0
Combine into a video,0
Delete the temporary directory we used for detection images,0
shutil.rmtree(rendering_output_dir),0
(Optionally) delete the frames on which we ran MegaDetector,0
shutil.rmtree(frame_output_folder),0
...process_video(),0
# Validate options,0
# Split every video into frames,0
# Run MegaDetector on the extracted frames,0
# (Optionally) delete the frames on which we ran MegaDetector,0
shutil.rmtree(frame_output_folder),0
# Convert frame-level results to video-level results,0
...process_video_folder(),0
%% Interactive driver,0
%% Process a folder of videos,0
import clipboard; clipboard.copy(cmd),0
%% Process a single video,0
import clipboard; clipboard.copy(cmd),0
"%% Render a folder of videos, one file at a time",0
import clipboard; clipboard.copy(s),0
%% Command-line driver,0
Lint as: python3,0
Copyright 2020 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TPU is automatically inferred if tpu_name is None and,0
we are running under cloud ai-platform.,0
,0
copy_checkpoints.py,0
,0
Run this script with specified source_dir and target_dir while the model is training to make a copy,0
of every checkpoint (checkpoints are kept once an hour by default and is difficult to adjust),0
,0
do not copy event or evaluation results,0
input validation,0
plot the images,0
adjust the figure,0
"read in dataset CSV and create merged (dataset, location) col",0
map label to label_index,0
load the splits,0
only weight the training set by detection confidence,0
TODO: consider weighting val and test set as well,1
isotonic regression calibration of MegaDetector confidence,0
treat each split separately,0
new_weight[i] = confidence[i] * (n / c) / total_confidence(i's label),0
- n = # examples in split (weighted by confidence); c = # labels,0
- weight allocated to each label is n/c,0
"- within each label, weigh each example proportional to confidence",0
- new weights sum to n,0
error checking,0
"maps output label name to set of (dataset, dataset_label) tuples",0
find which other label (label_b) has intersection,0
input validation,0
create label index JSON,0
look into sklearn.preprocessing.MultiLabelBinarizer,0
Note: JSON always saves keys as strings!,0
"True for ground truth, False for MegaDetector",0
always save as .jpg for consistency,0
get bounding boxes,0
convert from category ID to category name,0
"check if crops are already downloaded, and ignore bboxes below the",0
confidence threshold,0
assign all images without location info to 'unknown_location',0
remove images from labels that have fewer than min_locs locations,0
merge dataset and location into a single string '<dataset>/<location>',0
"create DataFrame of counts. rows = locations, columns = labels",0
label_count: label => number of examples,0
loc_count: label => number of locs containing that label,0
generate a new split,0
score the split,0
SSE for # of images per label (with 2x weight),0
SSE for # of locs per label,0
label => list of datasets to prioritize for test and validation sets,0
"merge dataset and location into a tuple (dataset, location)",0
sorted smallest to largest,0
greedily add to test set until it has >= 15% of images,0
sort the resulting locs,0
"modify loc_to_size in place, so copy its keys before iterating",0
arguments relevant to both creating the dataset CSV and splits.json,0
arguments only relevant for creating the dataset CSV,0
arguments only relevant for creating the splits JSON,0
comment lines starting with '#' are allowed,0
,0
prepare_classification_script.py,0
,0
Notebook-y script used to prepare a series of shell commands to run a classifier,0
(other than MegaClassifier) on a MegaDetector result set.,0
,0
Differs from prepare_classification_script_mc.py only in the final class mapping step.,0
,0
%% Job options,0
%% Constants,0
"This is just passed along to the metadata in the output file, it has no impact",0
on how the classification scripts run.,0
%% Set up environment,0
commands.append('cd CameraTraps/classification\n'),0
commands.append('conda activate cameratraps-classifier\n'),0
%% Crop images,0
fn = input_files[0],0
%% Run classifier,0
fn = input_files[0],0
%% Merge classification and detection outputs,0
fn = input_files[0],0
%% Write everything out,0
input validation,0
"Evaluating with accimage is much faster than Pillow or Pillow-SIMD, but accimage",0
is Linux-only.,0
create output directory,0
override saved params with kwargs,0
"For now, we don't weight crops by detection confidence during",0
evaluation. But consider changing this.,0
"create model, compile with TorchScript if given checkpoint is not compiled",0
"verify that target names matches original ""label names"" from dataset",0
"if the dataset does not already have a 'other' category, then the",0
'other' category must come last in label_names to avoid conflicting,0
with an existing label_id,0
define loss function (criterion),0
"this file ends up being huge, so we GZIP compress it",0
double check that the accuracy metrics are computed properly,0
save the confusion matrices to .npz,0
save per-label statistics,0
set dropout and BN layers to eval mode,0
"even if batch contains sample weights, don't use them",0
Do target mapping on the outputs (unnormalized logits) instead of,0
"the normalized (softmax) probabilities, because the loss function",0
uses unnormalized logits. Summing probabilities is equivalent to,0
log-sum-exp of unnormalized logits.,0
"a confusion matrix C is such that C[i,j] is the # of observations known to",0
be in group i and predicted to be in group j.,0
match pytorch EfficientNet model names,0
images dataset,0
"for smaller disk / memory usage, we cache the raw JPEG bytes instead",0
of the decoded Tensor,0
convert JPEG bytes to a 3D uint8 Tensor,0
"keras EfficientNet already includes normalization from [0, 255] to [0, 1],",0
so we don't need to do that here,0
labels dataset,0
img_files dataset,0
weights dataset,0
define the transforms,0
efficientnet data preprocessing:,0
- train:,0
"1) random crop: aspect_ratio_range=(0.75, 1.33), area_range=(0.08, 1.0)",0
2) bicubic resize to img_size,0
3) random horizontal flip,0
- test:,0
1) center crop,0
2) bicubic resize to img_size,0
weights sums to:,0
- if weight_by_detection_conf: (# images in split - conf delta),0
- otherwise: (# images in split),0
"freeze the base model's weights, including BatchNorm statistics",0
https://www.tensorflow.org/guide/keras/transfer_learning#fine-tuning,0
rebuild output,0
input validation,0
set seed,0
create logdir and save params,0
create dataloaders and log the index_to_label mapping,0
define loss function and optimizer,0
using EfficientNet training defaults,0
- batch norm momentum: 0.99,0
"- optimizer: RMSProp, decay 0.9 and momentum 0.9",0
- epochs: 350,0
"- learning rate: 0.256, decays by 0.97 every 2.4 epochs",0
- weight decay: 1e-5,0
TODO: change weighted to False if oversampling minority classes,1
stop training after 8 epochs without improvement,0
log metrics,0
log confusion matrix,0
log tp/fp/fn images,0
"tf.summary.image requires input of shape [N, H, W, C]",0
false positive for top3_pred[0],0
false negative for label,0
"if evaluating or finetuning, set dropout & BN layers to eval mode",0
"for each label, track 5 most-confident and least-confident examples",0
"even if batch contains sample weights, don't use them",0
we do not track L2-regularization loss in the loss metric,0
This dictionary will get written out at the end of this process; store,0
diagnostic variables here,0
error checking,0
refresh detection cache,0
save log of bad images,0
cache of Detector outputs: dataset name => {img_path => detection_dict},0
img_path: <dataset-name>/<img-filename>,0
get SAS URL for images container,0
strip image paths of dataset name,0
save list of dataset names and task IDs for resuming,0
complete task name: 'detect_for_classifier_caltech_20200722_110816_task01',0
HACK! Sleep for 10s between task submissions in the hopes that it,1
"decreases the chance of backend JSON ""database"" corruption",0
task still running => continue,0
"task finished successfully, save response to disk",0
error checking before we download and crop any images,0
convert from category ID to category name,0
we need the datasets table for getting SAS keys,0
"we already did all error checking above, so we don't do any here",0
get ContainerClient,0
get bounding boxes,0
we must include the dataset <ds> in <crop_path_template> because,0
'{img_path}' actually gets populated with <img_file> in,0
load_and_crop(),0
"get the image, either from disk or from Blob Storage",0
inelegant way to close the container_clients,0
,0
prepare_classification_script_mc.py,0
,0
Notebook-y script used to prepare a series of shell commands to run MegaClassifier,0
on a MegaDetector result set.,0
,0
Differs from prepare_classification_script.py only in the final class mapping step.,0
,0
%% Job options,0
%% Constants,0
"This is just passed along to the metadata in the output file, it has no impact",0
on how the classification scripts run.,0
%% Set up environment,0
commands.append('cd CameraTraps/classification\n'),0
commands.append('conda activate cameratraps-classifier\n'),0
%% Crop images,0
fn = input_files[0],0
%% Run classifier,0
fn = input_files[0],0
%% Remap classifier outputs,0
fn = input_files[0],0
%% Merge classification and detection outputs,0
fn = input_files[0],0
%% Write everything out,0
mean/std values from https://pytorch.org/docs/stable/torchvision/models.html,0
define the transforms,0
resizes smaller edge to img_size,0
weights sums to:,0
- if weight_by_detection_conf: (# images in split - conf delta),0
- otherwise: # images in split,0
for normal (non-weighted) shuffling,0
set all parameters to not require gradients except final FC layer,0
replace final fully-connected layer (which has 1000 ImageNet classes),0
"detect GPU, use all if available",0
input validation,0
set seed,0
create logdir and save params,0
create dataloaders and log the index_to_label mapping,0
create model,0
define loss function and optimizer,0
using EfficientNet training defaults,0
- batch norm momentum: 0.99,0
"- optimizer: RMSProp, decay 0.9 and momentum 0.9",0
- epochs: 350,0
"- learning rate: 0.256, decays by 0.97 every 2.4 epochs",0
- weight decay: 1e-5,0
stop training after 8 epochs without improvement,0
do a complete evaluation run,0
log metrics,0
log confusion matrix,0
log tp/fp/fn images,0
"for every image: undo normalization, clamp to [0, 1], CHW -> HWC",0
"- cannot be in-place, because the HeapItem might be in multiple heaps",0
writer.add_figure() has issues => using add_image() instead,0
"writer.add_figure(f'{label_name}/{tag}', fig, global_step=epoch)",0
false positive for top3_pred[0],0
false negative for label,0
"preds and labels both have shape [N, k]",0
"if evaluating or finetuning, set dropout and BN layers to eval mode",0
"for each label, track k_extreme most-confident and least-confident images",0
"even if batch contains sample weights, don't use them",0
lock before changing ImageFile.LOAD_TRUNCATED_IMAGES,0
input validation,0
use MegaDB to generate list of images,0
only keep images that:,0
"1) end in a supported file extension, and",0
2) actually exist in Azure Blob Storage,0
3) belong to a label with at least min_locs locations,0
write out log of images / labels that were removed,0
"save label counts, pre-subsampling",0
"save label counts, post-subsampling",0
spec_dict['taxa']: list of dict,0
[,0
"{'level': 'family', 'name': 'cervidae', 'datasets': ['idfg']},",0
"{'level': 'genus',  'name': 'meleagris'}",0
],0
"spec_dict['dataset_labels']: dict, dataset => list of dataset_label",0
{,0
"""idfg"": [""deer"", ""elk"", ""prong""],",0
"""idfg_swwlf_2019"": [""elk"", ""muledeer"", ""whitetaileddeer""]",0
},0
"maps output label name to set of (dataset, dataset_label) tuples",0
"because MegaDB is organized by dataset, we do the same",0
ds_to_labels = {,0
'dataset_name': {,0
"'dataset_label': [output_label1, output_label2]",0
},0
},0
we need the datasets table for getting full image paths,0
The line,0
"[img.class[0], seq.class[0]][0] as class",0
selects the image-level class label if available. Otherwise it selects the,0
"sequence-level class label. This line assumes the following conditions,",0
expressed in the WHERE clause:,0
- at least one of the image or sequence class label is given,0
- the image and sequence class labels are arrays with length at most 1,0
- the image class label takes priority over the sequence class label,0
,0
"In Azure Cosmos DB, if a field is not defined, then it is simply excluded",0
"from the result. For example, on the following JSON object,",0
{,0
"""dataset"": ""camera_traps"",",0
"""seq_id"": ""1234"",",0
"""location"": ""A1"",",0
"""images"": [{""file"": ""abcd.jpeg""}],",0
"""class"": [""deer""],",0
},0
"the array [img.class[0], seq.class[0]] just gives ['deer'] because",0
img.class is undefined and therefore excluded.,0
"if no path prefix, set it to the empty string '', because",0
"os.path.join('', x, '') = '{x}/'",0
result keys,0
"- already has: ['dataset', 'location', 'file', 'class', 'bbox']",0
"- add ['label'], remove ['file']",0
"if img is mislabeled, but we don't know the correct class, skip it",0
"otherwise, update the img with the correct class, but skip the",0
img if the correct class is not one we queried for,0
sort keys for determinism,0
we need the datasets table for getting SAS keys,0
strip leading '?' from SAS token,0
only check Azure Blob Storage,0
check local directory first before checking Azure Blob Storage,0
1st pass: populate label_to_locs,0
"label (tuple of str) => set of (dataset, location)",0
2nd pass: eliminate bad images,0
prioritize is a list of prioritization levels,0
number of already matching images,0
main(,0
"label_spec_json_path='idfg_classes.json',",0
"taxonomy_csv_path='../../camera-traps-private/camera_trap_taxonomy_mapping.csv',",0
"output_dir='run_idfg',",0
json_indent=4),0
recursively find all files in cropped_images_dir,0
only find crops of images from detections JSON,0
resizes smaller edge to img_size,0
"Evaluating with accimage is much faster than Pillow or Pillow-SIMD, but accimage",0
is Linux-only.,0
create dataset,0
create model,0
set dropout and BN layers to eval mode,0
load files,0
dataset => set of img_file,0
crop_path: <dataset>/<img_file>___cropXX_mdvY.Y.jpg,0
[----<img_path>----],0
error checking,0
any row with 'correct_class' should be marked 'mislabeled',0
filter to only the mislabeled rows,0
convert '\' to '/',0
verify that overlapping indices are the same,0
"""add"" any new mislabelings",0
write out results,0
error checking,0
load detections JSON,0
get detector version,0
convert from category ID to category name,0
copy keys to modify dict in-place,0
This will be removed later when we filter for animals,0
save log of bad images,0
"True for ground truth, False for MegaDetector",0
always save as .jpg for consistency,0
"we already did all error checking above, so we don't do any here",0
"get the image, either from disk or from Blob Storage",0
inelegant way to close the container_client,0
"crop_path => normalized bbox coordinates [xmin, ymin, width, height]",0
"only ground-truth bboxes do not have a ""confidence"" value",0
try loading image from local directory,0
try to download image from Blob Storage,0
crop the image,0
"expand box width or height to be square, but limit to img size",0
"Image.crop() takes box=[left, upper, right, lower]",0
pad to square using 0s,0
Author: lukemelas (github username),0
Github repo: https://github.com/lukemelas/EfficientNet-PyTorch,0
With adjustments and added comments by workingcoder (github username).,0
Support the construction of 'efficientnet-l2' without pretrained weights,0
Expansion phase (Inverted Bottleneck),0
"image_size = calculate_output_image_size(image_size, 1) <-- this wouldn't modify image_size",0
Depthwise convolution phase,0
"Squeeze and Excitation layer, if desired",0
Pointwise convolution phase,0
Expansion and Depthwise Convolution,0
Squeeze and Excitation,0
Pointwise Convolution,0
Skip connection and drop connect,0
The combination of skip connection and drop connect brings about stochastic depth.,0
Batch norm parameters,0
Get stem static or dynamic convolution depending on image size,0
Stem,0
Build blocks,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
"image_size = calculate_output_image_size(image_size, block_args.stride)  # stride = 1",0
Head,0
Final linear layer,0
Stem,0
Blocks,0
Head,0
Stem,0
Blocks,0
Head,0
Convolution layers,0
Pooling and final linear layer,0
Author: lukemelas (github username),0
Github repo: https://github.com/lukemelas/EfficientNet-PyTorch,0
With adjustments and added comments by workingcoder (github username).,0
###############################################################################,0
## Help functions for model architecture,0
###############################################################################,0
GlobalParams and BlockArgs: Two namedtuples,0
Swish and MemoryEfficientSwish: Two implementations of the method,0
round_filters and round_repeats:,0
Functions to calculate params for scaling model width and depth ! ! !,0
get_width_and_height_from_size and calculate_output_image_size,0
drop_connect: A structural design,0
get_same_padding_conv2d:,0
Conv2dDynamicSamePadding,0
Conv2dStaticSamePadding,0
get_same_padding_maxPool2d:,0
MaxPool2dDynamicSamePadding,0
MaxPool2dStaticSamePadding,0
"It's an additional function, not used in EfficientNet,",0
but can be used in other model (such as EfficientDet).,0
"Parameters for the entire model (stem, all blocks, and head)",0
Parameters for an individual model block,0
Set GlobalParams and BlockArgs's defaults,0
An ordinary implementation of Swish function,0
A memory-efficient implementation of Swish function,0
TODO: modify the params names.,1
"maybe the names (width_divisor,min_width)",0
"are more suitable than (depth_divisor,min_depth).",0
follow the formula transferred from official TensorFlow implementation,0
follow the formula transferred from official TensorFlow implementation,0
"generate binary_tensor mask according to probability (p for 0, 1-p for 1)",0
Note:,0
The following 'SamePadding' functions make output size equal ceil(input size/stride).,0
"Only when stride equals 1, can the output size be the same as input size.",0
Don't be confused by their function names ! ! !,0
Tips for 'SAME' mode padding.,0
Given the following:,0
i: width or height,0
s: stride,0
k: kernel size,0
d: dilation,0
p: padding,0
Output after Conv2d:,0
o = floor((i+p-((k-1)*d+1))/s+1),0
"If o equals i, i = floor((i+p-((k-1)*d+1))/s+1),",0
=> p = (i-1)*s+((k-1)*d+1)-i,0
With the same calculation as Conv2dDynamicSamePadding,0
Calculate padding based on image size and save it,0
Calculate padding based on image size and save it,0
###############################################################################,0
## Helper functions for loading model params,0
###############################################################################,0
BlockDecoder: A Class for encoding and decoding BlockArgs,0
efficientnet_params: A function to query compound coefficient,0
get_model_params and efficientnet:,0
Functions to get BlockArgs and GlobalParams for efficientnet,0
url_map and url_map_advprop: Dicts of url_map for pretrained weights,0
load_pretrained_weights: A function to load pretrained weights,0
Check stride,0
"Coefficients:   width,depth,res,dropout",0
Blocks args for the whole model(efficientnet-b0 by default),0
It will be modified in the construction of EfficientNet Class according to model,0
note: all models have drop connect rate = 0.2,0
ValueError will be raised here if override_params has fields not included in global_params.,0
train with Standard methods,0
check more details in paper(EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks),0
train with Adversarial Examples(AdvProp),0
check more details in paper(Adversarial Examples Improve Image Recognition),0
TODO: add the petrained weights url map of 'efficientnet-l2',1
AutoAugment or Advprop (different preprocessing),0
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils),0
from data_management.megadb.schema import sequences_schema_check,0
resize is for displaying them more quickly,0
dataset and seq_id are required fields,0
sort the images in the sequence,0
"if no class label on the image, show class label on the sequence",0
pool = ThreadPool(),0
"print('len of rendering_info', len(rendering_info))",0
"tqdm(pool.imap_unordered(render_image_info_partial, rendering_info), total=len(rendering_info))",0
options = write_html_image_list(),0
options['headerHtml'],0
print('Checking that the MegaDB entries conform to the schema...'),0
sequences_schema_check.sequences_schema_check(sequences),0
#######,0
,0
visualize_db.py,0
,0
Outputs an HTML page visualizing annotations (class labels and/or bounding boxes),0
on a sample of images in a database in the COCO Camera Traps format,0
,0
#######,0
%% Imports,0
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils),0
Assumes the cameratraps repo root is on the path,0
%% Settings,0
Set to None to visualize all images,0
Target size for rendering; set either dimension to -1 to preserve aspect ratio,0
"These are mutually exclusive; both are category names, not IDs",0
"Special tag used to say ""show me all images with multiple categories""",0
We sometimes flatten image directories by replacing a path separator with,0
another character.  Leave blank for the typical case where this isn't necessary.,0
Control rendering parallelization,0
Process-based parallelization in this function is currently unsupported,0
"due to pickling issues I didn't care to look at, but I'm going to just",0
"flip this with a warning, since I intend to support it in the future.",0
%% Helper functions,0
"Translate the file name in an image entry in the json database to a path, possibly doing",0
some manipulation of path separators,0
%% Core functions,0
"Optionally remove all images without bounding boxes, *before* sampling",0
"Optionally include/remove images with specific labels, *before* sampling",0
Put the annotations in a dataframe so we can select all annotations for a given image,0
Construct label map,0
Take a sample of images,0
Set of dicts representing inputs to render_db_bounding_boxes:,0
,0
"bboxes, boxClasses, image_path",0
All the class labels we've seen for this image (with out without bboxes),0
Iterate over annotations for this image,0
iAnn = 0; anno = annos_i.iloc[iAnn],0
...for each of this image's annotations,0
"We're adding html for an image before we render it, so it's possible this image will",0
fail to render.  For applications where this script is being used to debua a database,0
"(the common case?), this is useful behavior, for other applications, this is annoying.",0
...for each image,0
...def render_image_info,0
def process_images(...),0
%% Command-line driver,0
"Copy all fields from a Namespace (i.e., the output from parse_args) to an object.",0
,0
Skips fields starting with _.  Does not check existence in the target object.,0
Convert to an options object,0
%% Interactive driver(s),0
%%,0
os.startfile(htmlOutputFile),0
%% Constants and imports,0
convert category ID from int to str,0
Retry on blob storage read failures,0
%% Functions,0
PIL.Image.convert() returns a converted copy of this image,0
Alter orientation as needed according to EXIF tag 0x112 (274) for Orientation,0
,0
https://gist.github.com/dangtrinhnt/a577ece4cbe5364aad28,0
https://www.media.mit.edu/pia/Research/deepview/exif.html,0
,0
Null operation,0
Aspect ratio as width over height,0
ar = w / h,0
h = w / ar,0
w = ar * h,0
The following three functions are modified versions of those at:,0
,0
https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py,0
Convert to pixels so we can use the PIL crop() function,0
PIL's crop() does surprising things if you provide values outside of,0
"the image, clip inputs",0
...if this detection is above threshold,0
...for each detection,0
"list of lists, one list of strings for each bounding box (to accommodate multiple labels)",0
for color selection,0
"Always render objects with a confidence of ""None"", this is typically used",0
for ground truth data.,0
"{} is the default, which means ""show labels with no mapping"", so don't use ""if label_map"" here",0
if label_map:,0
"To avoid duplicate colors with detection-only visualization, offset",0
the classification class index by the number of detection classes,0
...for each classification,0
...if we have classification results,0
...if the confidence of this detection is above threshold,0
...for each detection,0
"print('Input must be of size [N, 4], but is ' + str(boxes_shape))",0
Deliberately trimming to the width of the image only in the case where,0
"box expansion is turned on.  There's not an obvious correct behavior here,",0
but the thinking is that if the caller provided an out-of-range bounding,0
"box, they meant to do that, but at least in the eyes of the person writing",0
"this comment, if you expand a box for visualization reasons, you don't want",0
to end up with part of a box.,0
,0
A slightly more sophisticated might check whether it was in fact the expansion,0
"that made this box larger than the image, but this is the case 99.999% of the time",0
"here, so that doesn't seem necessary.",1
...if we need to expand boxes,0
If the total height of the display strings added to the top of the bounding,0
"box exceeds the top of the image, stack the strings below the bounding box",0
instead of above.,0
Each display_str has a top and bottom margin of 0.05x.,0
Reverse list and print from bottom to top.,0
Skip empty strings,0
"list of lists, one list of strings for each bounding box (to accommodate multiple labels)",0
need to be a string here because PIL needs to iterate through chars,0
,0
stacked bar charts are made with each segment starting from a y position,0
"To fit the legend in, shrink current axis by 20%",0
Put a legend to the right of the current axis,0
####,0
,0
visualize_incoming_annotations.py,0
,0
Spot-check the annotations received from iMerit by visualizing annotated bounding,0
boxes on a sample of images and display them in HTML.,0
,0
"Modified in 2021 March to use the new format (iMerit batch 12 onwards), which is a",0
COCO formatted JSON with relative coordinates for the bbox.,0
,0
from data_management.megadb.schema import sequences_schema_check,0
we used frame_num of 1 when sending out images to annotators when it is not explicitly stored (wcs esp.),0
The file_name field in the incoming json looks like,0
alka_squirrels.seq2020_05_07_25C.frame119221.jpg,0
"we need to use the dataset, sequence and frame info to find the actual path in blob storage",0
using the sequences,0
category_id 5 is No Object Visible,0
download the image,0
Write to HTML,0
allow forward references in typing annotations,0
class variables,0
instance variables,0
get path to root,0
taxonomy CSV rows without 'taxonomy_string' entries are excluded,0
"from the taxonomy graph, but can be included in a classification",0
label specification JSON via the 'dataset_labels' key,0
use the lower parent,0
special cases,0
%% Imports,0
%% Taxnomy checking,0
taxonomy CSV rows without 'taxonomy_string' entries are excluded,0
"from the taxonomy graph, but can be included in a classification",0
label specification JSON via the 'dataset_labels' key,0
...for each row in the taxnomy file,0
%% Command-line driver,0
%% Interactive driver,0
%%,0
which datasets are already processed?,0
"sequence-level query should be fairly fast, ~1 sec",0
cases when the class field is on the image level (images in a sequence,0
"that had different class labels, 'caltech' dataset is like this)",0
"this query may take a long time, >1hr",0
"this query should be fairly fast, ~1 sec",0
read species presence info from the JSON files for each dataset,0
has this class name appeared in a previous dataset?,0
columns to populate the spreadsheet,0
sort by descending species count,0
make the spreadsheet,0
hyperlink Bing search URLs,0
hyperlink example image SAS URLs,0
TODO hardcoded columns: change if # of examples or col_order changes,1
,0
map_lila_taxonomy_to_wi_taxonomy.py,0
,0
Loads the LILA category mapping (in which taxonomy information comes from an iNat taxonomy snapshot),0
and tries to map each class to the Wildlife Insights taxonomy.,0
,0
%% Constants and imports,0
Created by get_lila_category_list.py... contains counts for each category,0
This is a manually-curated file used to store mappings that had to be made manually,0
This is the main output file from this whole process,0
%% Load category and taxonomy files,0
%% Pull everything out of pandas,0
%% Cache WI taxonomy lookups,0
This is just a handy lookup table that we'll use to debug mismatches,0
taxon = wi_taxonomy[21653]; print(taxon),0
Look for keywords that don't refer to specific taxa: blank/animal/unknown,0
Do we have a species name?,0
"If 'species' is populated, 'genus' should always be populated; one item currently breaks",0
this rule.,0
...for each taxon,0
%% Find redundant taxa,0
%% Manual review of redundant taxa,0
%% Clean up redundant taxa,0
taxon_name = list(taxon_name_to_preferred_taxon_id.keys())[0],0
"If we've gotten this far, we should be choosing from multiple taxa.",0
,0
"This will become untrue if any of these are resolved later, at which point we shoudl",0
remove them from taxon_name_to_preferred_id,0
Choose the preferred taxa,0
%% Read supplementary mappings,0
"Each line is [lila query],[WI taxon name],[notes]",0
%% Map LILA categories to WI categories,0
Must be ordered from kingdom --> species,0
TODO:,1
"['subspecies','variety']",0
i_taxon = 0; taxon = lila_taxonomy[i_taxon]; print(taxon),0
"Go from kingdom --> species, choosing the lowest-level description as the query",0
"E.g., 'car'",0
"print('Made a supplementary mapping from {} to {}'.format(query,wi_taxon['taxon_name']))",0
print('No match for {}'.format(query)),0
...for each LILA taxon,0
%% Manual mapping,0
%% Build a dictionary from LILA dataset names and categories to LILA taxa,0
i_d = 0; d = lila_taxonomy[i_d],0
"%% Map LILA datasets to WI taxa, and count the number of each taxon available in each dataset",0
dataset_name = list(lila_dataset_to_categories.keys())[0],0
dataset_category = dataset_categories[0],0
"Write out the dataset name, category name, WI GUID, WI scientific name, WI common name,",0
and count,0
...for each category in this dataset,0
...for each dataset,0
...with open(),0
,0
retrieve_sample_image.py,0
,0
"Downloader that retrieves images from Google images, used for verifying taxonomy",0
"lookups and looking for egregious mismappings (e.g., ""snake"" being mapped to a fish called",0
"""snake"").",0
,0
"Simple wrapper around simple_image_download, but I've had to swap in and out the underlying",0
downloader a few times.,0
,0
%% Imports and environment,0
%%,0
%% Main entry point,0
%% Test driver,0
%%,0
,0
"Using the taxonomy .csv file, map all LILA datasets to the standard taxonomy",0
,0
Does not currently produce results; this is just used to confirm that all category names,0
have mappings in the taxonomy file.,0
,0
%% Constants and imports,0
Created by get_lila_category_list.py... contains counts for each category,0
%% Load category and taxonomy files,0
%% Map dataset names and category names to scientific names,0
i_row = 1; row = taxonomy_df.iloc[i_row]; row,0
"%% For each dataset, make sure we can map every category to the taxonomy",0
dataset_name = list(lila_dataset_to_categories.keys())[0],0
c = categories[0],0
,0
"Takes the megadb taxonomy mapping, extracts the rows that are relevant to",0
"LILA, and does some cleanup.",0
,0
%% Constants and imports,0
This is a partially-completed taxonomy file that was created from a different set of,0
"scripts, but covers *most* of LILA as of June 2022",0
Created by get_lila_category_list.py,0
%% Read the input files,0
Get everything out of pandas,0
"%% Find all unique dataset names in the input list, compare them with data names from LILA",0
d = input_taxonomy_rows[0],0
The script that generates this dictionary creates a separate entry for bounding box,0
"metadata files, but those don't represent new dataset names",0
%% Map input columns to output datasets,0
Make sure all of those datasets actually correspond to datasets on LILA,0
%% Re-write the input taxonomy file to refer to LILA datasets,0
Map the string datasetname:token to a taxonomic tree json,0
mapping = input_taxonomy_rows[0],0
Make sure that all occurrences of this mapping_string give us the same output,0
assert taxonomy_string == taxonomy_mappings[mapping_string],0
%% Re-write the input file in the target format,0
mapping_string = list(taxonomy_mappings.keys())[0],0
,0
prepare_lila_taxonomy_release.py,0
,0
"Given the private intermediate taxonomy mapping, prepare the public (release)",0
taxonomy mapping file.,0
,0
%% Imports and constants,0
Created by get_lila_category_list.py... contains counts for each category,0
%% Find out which categories are actually used,0
dataset_name = datasets_to_map[0],0
i_row = 0; row = df.iloc[i_row]; row,0
%% Generate the final output file,0
i_row = 0; row = df.iloc[i_row]; row,0
match_at_level = taxonomic_match[0],0
i_row = 0; row = df.iloc[i_row]; row,0
"E.g.: (43117, 'genus', 'lepus', ['hares and jackrabbits']",0
###############,0
---> CONSTANTS,0
###############,0
max_progressbar = count * (list(range(limit+1))[-1]+1),0
"bar = progressbar.ProgressBar(maxval=max_progressbar,",0
"widgets=[progressbar.Bar('=', '[', ']'), ' ', progressbar.Percentage()]).start()",0
bar.update(bar.currval + 1),0
bar.finish(),0
,0
"Given a subset of LILA datasets, find all the categories, and start the taxonomy",0
mapping process.,0
,0
%% Constants and imports,0
Created by get_lila_category_list.py,0
'NACTI',0
'Channel Islands Camera Traps',0
%% Read the list of datasets,0
The script that generates this dictionary creates a separate entry for bounding box,0
"metadata files, but those don't represent new dataset names",0
%% Find all categories,0
dataset_name = datasets_to_map[0],0
%% Initialize taxonomic lookup,0
from taxonomy_mapping.species_lookup import (,0
"get_taxonomic_info, print_taxonomy_matche)",0
%% Manual lookup,0
%%,0
q = 'white-throated monkey',0
raise ValueError(''),0
%% Match every query against our taxonomies,0
mapping_string = category_mappings[1]; print(mapping_string),0
...for each mapping,0
%% Write output rows,0
,0
"Does some consistency-checking on the LILA taxonomy file, and generates",0
an HTML preview page that we can use to determine whether the mappings,0
make sense.,0
,0
%% Imports and constants,0
"lila_taxonomy_file = r""G:\git\agentmorrisprivate\lila-taxonomy\lila-taxonomy-mapping.csv""",0
"lila_taxonomy_file = r""G:\temp\lila\lila_additions_2022.06.29.csv""",0
%% Support functions,0
%% Read the taxonomy mapping file,0
%% Prepare taxonomy lookup,0
from taxonomy_mapping.species_lookup import (,0
"get_taxonomic_info, print_taxonomy_matche)",0
%% Optionally remap all gbif-based mappings to inat (or vice-versa),0
%%,0
i_row = 1; row = df.iloc[i_row]; row,0
This should be zero for the release .csv,0
%%,0
%% Check for mappings that disagree with the taxonomy string,0
Look for internal inconsistency,0
Look for outdated mappings,0
i_row = 0; row = df.iloc[i_row],0
%% List null mappings,0
,0
"These should all be things like ""unidentified"" and ""fire""",0
,0
i_row = 0; row = df.iloc[i_row],0
%% List mappings with scientific names but no common names,0
%% List mappings that map to different things in different data sets,0
x = suppress_multiple_matches[-1],0
...for each row where we saw this query,0
...for each row,0
"%% Verify that nothing ""unidentified"" maps to a species or subspecies",0
"E.g., ""unidentified skunk"" should never map to a specific species of skunk",0
%% Make sure there are valid source and level values for everything with a mapping,0
%% Find WCS mappings that aren't species or aren't the same as the input,0
"WCS used scientific names, so these remappings are slightly more controversial",0
then the standard remappings.,0
row = df.iloc[-500],0
"Anything marked ""species"" or ""unknown"" by definition doesn't map to a species,",0
so ignore these.,0
print('WCS query {} ({}) remapped to {} ({})'.format(,0
"query,common_name,scientific_name,common_names_from_taxonomy))",0
%% Download sample images for all scientific names,0
i_row = 0; row = df.iloc[i_row],0
if s != 'mirafra':,0
continue,0
Check whether we already have enough images for this query,0
"print('Skipping query {}, already have {} images'.format(s,len(sizes_above_threshold)))",0
Check whether we've already run this query for a previous row,0
...for each row in the mapping table,0
%% Rename .jpeg to .jpg,0
"print('Renaming {} to {}'.format(fn,new_fn))",0
%% Choose representative images for each scientific name,0
s = list(scientific_name_to_paths.keys())[0],0
Be suspicious of duplicate sizes,0
...for each scientific name,0
%% Delete unused images,0
%% Produce HTML preview,0
i_row = 2; row = df.iloc[i_row],0
"image_paths = [os.path.relpath(p, output_base) for p in image_paths]",0
...for each row,0
%% Open HTML preview,0
######,0
,0
species_lookup.py,0
,0
Look up species names (common or scientific) in the GBIF and iNaturalist,0
taxonomies.,0
,0
Run initialize_taxonomy_lookup() before calling any other function.,0
,0
######,0
%% Constants and imports,0
As of 2020.05.12:,0
,0
"GBIF: ~777MB zipped, ~1.6GB taxonomy",0
"iNat: ~2.2GB zipped, ~51MB taxonomy",0
These are un-initialized globals that must be initialized by,0
the initialize_taxonomy_lookup() function below.,0
%% Functions,0
Initialization function,0
# Load serialized taxonomy info if we've already saved it,0
"# If we don't have serialized taxonomy info, create it from scratch.",0
Download and unzip taxonomy files,0
taxonomy_name = list(taxonomy_urls.items())[0][0]; zip_url = list(taxonomy_urls.items())[0][1],0
Don't download the zipfile if we've already unzipped what we need,0
Bypasses download if the file exists already,0
Unzip the files we need,0
...for each file that we need from this zipfile,0
Remove the zipfile,0
os.remove(zipfile_path),0
...for each taxonomy,0
"Create dataframes from each of the taxonomy files, and the GBIF common",0
name file,0
Load iNat taxonomy,0
Load GBIF taxonomy,0
Remove questionable rows from the GBIF taxonomy,0
Load GBIF vernacular name mapping,0
Only keep English mappings,0
Convert everything to lowercase,0
"For each taxonomy table, create a mapping from taxon IDs to rows",0
Create name mapping dictionaries,0
Build iNat dictionaries,0
row = inat_taxonomy.iloc[0],0
Build GBIF dictionaries,0
"The canonical name is the Latin name; the ""scientific name""",0
include the taxonomy name.,0
,0
http://globalnames.org/docs/glossary/,0
This only seems to happen for really esoteric species that aren't,0
"likely to apply to our problems, but doing this for completeness.",0
Don't include taxon IDs that were removed from the master table,0
Save everything to file,0
...def initialize_taxonomy_lookup(),0
"list of dicts: {'source': source_name, 'taxonomy': match_details}",0
i_match = 0,0
"list of (taxon_id, taxonRank, scientific name, [vernacular names])",0
corresponding to an exact match and its parents,0
Walk taxonomy hierarchy,0
This can happen because we remove questionable rows from the,0
GBIF taxonomy,0
"print(f'Warning: no row exists for parent_taxon_id {parent_taxon_id},' + \",0
"f'child taxon_id: {taxon_id}, query: {query}')",0
The GBIF taxonomy contains unranked entries,0
...while there is taxonomy left to walk,0
...for each match,0
Remove redundant matches,0
i_tree_a = 0; tree_a = matching_trees[i_tree_a],0
i_tree_b = 1; tree_b = matching_trees[i_tree_b],0
"If tree a's primary taxon ID is inside tree b, discard tree a",0
,0
taxonomy_level_b = tree_b['taxonomy'][0],0
...for each level in taxonomy B,0
...for each tree (inner),0
...for each tree (outer),0
...def traverse_taxonomy(),0
"print(""Finding taxonomy information for: {0}"".format(query))",0
"In GBIF, some queries hit for both common and scientific, make sure we end",0
up with unique inputs,0
"If the species is not found in either taxonomy, return None",0
Both GBIF and iNat have a 1-to-1 mapping between taxon_id and row number,0
Walk both taxonomies,0
...def get_taxonomic_info(),0
m = matches[0],0
"For example: [(9761484, 'species', 'anas platyrhynchos')]",0
...for each taxonomy level,0
...for each match,0
...def print_taxonomy_matches(),0
%% Taxonomy functions that make subjective judgements,0
query = 'person',0
Do we have an iNat match?,0
"print_taxonomy_matches(inat_matches, verbose=True)",0
"print_taxonomy_matches(gbif_matches, verbose=True)",0
print('Warning: multiple iNat matches for {}'.format(query)),0
Prefer chordates... most of the names that aren't what we want,0
"are esoteric insects, like a moth called ""cheetah""",0
,0
"If we can't find a chordate, just take the first match.",0
,0
i_test_match = 0,0
"This is (taxonID, taxonLevel, scientific, [list of common])",0
print(f'Warning: multiple iNat common names for {query}'),0
Default to returning the query,0
"print(f'Matched iNat {query} to {scientific_name},{common_name}')",0
...if we had iNat matches,0
If we either prefer GBIF or didn't have iNat matches,0
,0
Code is deliberately redundant here; I'm expecting some subtleties in how,0
handle GBIF and iNat.,0
print('Warning: multiple GBIF matches for {}'.format(query)),0
Prefer chordates... most of the names that aren't what we want,0
"are esoteric insects, like a moth called ""cheetah""",0
,0
"If we can't find a chordate, just take the first match.",0
,0
i_test_match = 0,0
"This is (taxonID, taxonLevel, scientific, [list of common])",0
print(f'Warning: multiple GBIF common names for {query}'),0
Default to returning the query,0
...if we needed to look in the GBIF taxonomy,0
...def _get_preferred_taxonomic_match(),0
%% Interactive drivers and debug,0
%% Initialization,0
%% Taxonomic lookup,0
query = 'lion',0
print(matches),0
Print the taxonomy in the taxonomy spreadsheet format,0
%% Directly access the taxonomy tables,0
%% Command-line driver,0
Read command line inputs (absolute path),0
Read the tokens from the input text file,0
Loop through each token and get scientific name,0
,0
process_species_by_dataset,0
,0
We generated a list of all the annotations in our universe; this script is,0
used to (interactively) map them onto the GBIF and iNat taxonomies.  Don't,0
"try to run this script from top to bottom; it's used like a notebook, not like",0
"a script, since manual review steps are required.",0
,0
%% Imports,0
%autoreload 0,0
%autoreload -species_lookup,0
%% Constants,0
Input file,0
Output file after automatic remapping,0
File to which we manually copy that file and do all the manual review; this,0
should never be programmatically written to,0
The final output spreadsheet,0
HTML file generated to facilitate the identificaiton of egregious mismappings,0
%% Functions,0
Prefer iNat matches over GBIF matches,0
query = 'person',0
Do we have an iNat match?,0
"print_taxonomy_matches(inat_matches, verbose=True)",0
"print_taxonomy_matches(gbif_matches, verbose=True)",0
print('Warning: multiple iNat matches for {}'.format(query)),0
Prefer chordates... most of the names that aren't what we want,0
"are esoteric insects, like a moth called ""cheetah""",0
,0
"If we can't find a chordate, just take the first match.",0
,0
i_test_match = 0,0
"This is (taxonID, taxonLevel, scientific, [list of common])",0
print(f'Warning: multiple iNat common names for {query}'),0
Default to returning the query,0
"print(f'Matched iNat {query} to {scientific_name},{common_name}')",0
...if we had iNat matches,0
If we either prefer GBIF or didn't have iNat matches,0
,0
Code is deliberately redundant here; I'm expecting some subtleties in how,0
handle GBIF and iNat.,0
print('Warning: multiple GBIF matches for {}'.format(query)),0
Prefer chordates... most of the names that aren't what we want,0
"are esoteric insects, like a moth called ""cheetah""",0
,0
"If we can't find a chordate, just take the first match.",0
,0
i_test_match = 0,0
"This is (taxonID, taxonLevel, scientific, [list of common])",0
print(f'Warning: multiple GBIF common names for {query}'),0
Default to returning the query,0
...if we needed to look in the GBIF taxonomy,0
...def get_preferred_taxonomic_match(),0
%% Initialization,0
%% Test single-query lookup,0
%%,0
%%,0
"q = ""grevy's zebra""",0
%% Read the input data,0
%% Run all our taxonomic lookups,0
i_row = 0; row = df.iloc[i_row],0
query = 'lion',0
...for each query,0
Write to the excel file that we'll use for manual review,0
%% Download preview images for everything we successfully mapped,0
uncomment this to load saved output_file,0
"output_df = pd.read_excel(output_file, keep_default_na=False)",0
i_row = 0; row = output_df.iloc[i_row],0
...for each query,0
%% Write HTML file with representative images to scan for obvious mis-mappings,0
i_row = 0; row = output_df.iloc[i_row],0
...for each row,0
%% Look for redundancy with the master table,0
Note: `master_table_file` is a CSV file that is the concatenation of the,0
"manually-remapped files (""manual_remapped.xlsx""), which are the output of",0
this script run across from different groups of datasets. The concatenation,0
"should be done manually. If `master_table_file` doesn't exist yet, skip this",0
"code cell. Then, after going through the manual steps below, set the final",0
manually-remapped version to be the `master_table_file`.,0
%% Manual review,0
Copy the spreadsheet to another file; you're about to do a ton of manual,0
review work and you don't want that programmatically overwrriten.,0
,0
See manual_review_xlsx above,0
%% Read back the results of the manual review process,0
%% Look for manual mapping errors,0
Manually inspect df for typos in 'dataset_names' and 'taxonomy_level' columns,0
Identify rows where:,0
,0
- 'taxonomy_level' does not match level of 1st element in 'taxonomy_string',0
- 'scientific_name' does not match name of 1st element in 'taxonomy_string',0
,0
...both of which typically represent manual mapping errors.,0
i_row = 0; row = df.iloc[i_row],0
"I'm not sure why both of these checks are necessary, best guess is that",1
the Excel parser was reading blanks as na on one OS/Excel version and as '',0
on another.,0
The taxonomy_string column is a .json-formatted string; expand it into,0
an object via eval(),0
"%% Find scientific names that were added manually, and match them to taxonomies",0
i_row = 0; row = df.iloc[i_row],0
...for each query,0
%% Write out final version,0
,0
"Generate read-only SAS URLs for all LILA containers, to facilitate partial downloads.",0
,0
The results of this script end up here:,0
,0
http://lila.science/wp-content/uploads/2020/03/lila_sas_urls.txt,0
,0
"Update: that file is manually maintained now, it can't be programmatically generated",0
,0
%% Imports,0
Read-only,0
%% Enumerate containers,0
%% Generate SAS tokens,0
%% Generate SAS URLs,0
%% Write to output file,0
--------some stuff needed to get AJAX to work with bottle?--------#,0
-------------------------------------------------------------------------------- #,0
PREPARE TO QUEUE IMAGES FOR LABELING,0
-------------------------------------------------------------------------------- #,0
# Connect as USER to database DB_NAME through peewee and initialize database proxy,0
# Load embedding model,0
---------------------------------------------------------------------- #,0
CREATE QUEUE OF IMAGES TO LABEL,0
---------------------------------------------------------------------- #,0
Use classifier to generate predictions,0
# Update model predicted class in PostgreSQL database,0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?,0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
-------------------------------------------------------------------------------- #,0
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI,0
-------------------------------------------------------------------------------- #,0
"# static routes (to serve CSS, etc.)",0
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps",0
"return bottle.static_file(filename, root='../../../../../../../../../../../.')",0
# dynamic routes,0
if data['display_grayscale']:,0
indices_to_exclude.update(set(color_indices)),0
elif not data['display_grayscale']:,0
indices_to_exclude.update(set(grayscale_indices)),0
data['display_images'] = {},0
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices],0
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices],0
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices],0
data['display_images']['detection_categories'] = [],0
for i in indices:,0
if str(dataset.samples[i][1]) == 'None':,0
data['display_images']['detection_categories'].append('None'),0
else:,0
existing_category_entries = {cat.id: cat.name for cat in Category.select()},0
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()",0
data['display_images']['detection_categories'].append(cat_name),0
Use image ids in images_to_label to get the corresponding dataset indices,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
print(set(dataset.set_indices[4]).update(set(indices_to_label))),0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Use image id images_to_label to get the corresponding dataset index,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?,0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Train on samples that have been labeled so far,0
dataset.set_kind(DetectionKind.UserDetection.value),0
print(y_train),0
Predict on the samples that have not been labeled,0
print(y_pred),0
Update model predicted class in PostgreSQL database,0
timer = time.time(),0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
print('Updating the database took %0.2f seconds'%(time.time() - timer)),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids)),0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
"once the classifier has been trained the first time, switch to AL sampling",0
,0
make_oneclass_json.py,0
,0
"Takes a coco-camera-traps .json database and collapses species classes to binary,",0
optionally removing labels from empty images (to be detector-friendly) (depending on,0
"""experiment_type"").",0
,0
"Assumes that empty images are labeled as ""empty"".",0
,0
%% Imports and environment,0
%% Core conversion function,0
"We're removing empty images from the annotation list, but not from",0
"the ""images"" list; they'll still get used in detector training.",0
print('Ignoring empty annotation'),0
%% Interactive driver,0
%%,0
Load annotations,0
Convert from multi-class to one-class,0
Write out the one-class data,0
%% Command-line driver,0
,0
plot_bounding_boxes.py,0
,0
Takes a .json database containing bounding boxes and renders those boxes on the,0
source images.,0
,0
"This assumes annotations in coco-camera-traps format, with absolute bbox",0
coordinates.,0
,0
%% Imports and environment,0
How many images should we process?  Set to -1 to process all images.,0
Should we randomize the image order?,0
%%  Read database and build up convenience mappings,0
Category ID to category name,0
Image ID to image info,0
Image ID to image path,0
Image ID to all annotations referring to this image,0
"%% Iterate over images, draw bounding boxes, write to file",0
For each image,0
image = images[0],0
Build up a list of bounding boxes to draw on this image,0
Load the image,0
Create figure and axes,0
Display the image,0
ann = imageAnnotations[0],0
"For each annotation associated with this image, render bounding box and label",0
"In the Rectangle() function, the first argument (""location"") is the bottom-left",0
of the rectangle.,0
,0
Origin is the upper-left of the image.,0
Add the patch to the Axes,0
Add a class label,0
This is magic goop that removes whitespace around image plots (sort of),0
Write the output image,0
...for each image,0
,0
plot_imerit_annotations.py,0
,0
Takes a .json file full of bounding box annotations and renders those boxes on the,0
source images.,0
,0
"This assumes annotations in the format we receive them, specifically:",0
,0
1) Relative bbox coordinates,0
"2) A list of .json objects, not a well-formatted .json file",0
,0
"I.e., don't use this on a COCO-style .json file.  See plot_bounding_boxes.py",0
for the same operation performed on a proper COCO-camera-traps database.,0
,0
%% Imports and environment,0
"os.makedirs(outputBase, exist_ok=True)",0
%%  Read all source images and build up a hash table from image name to full path,0
"This spans training and validation directories, so it's not the same as",0
just joining the image name to a base path,0
"%% Iterate over annotations, draw bounding boxes, write to file",0
annData has keys:,0
,0
"annotations, categories, images",0
,0
Each of these are lists of dictionaries,0
%% Render all annotations on each image in the sequence,0
%% Pull out image metadata,0
Build up a list of bounding boxes to draw on this image,0
Pull out just the image name from the filename,0
,0
File names look like:,0
,0
seq6efffac2-5567-11e8-b3fe-dca9047ef277.frame1.img59a94e52-23d2-11e8-a6a3-ec086b02610b.jpg,0
"m = re.findall(r'img(.*\.jpg)$', imgFileName, re.M|re.I)",0
print(m),0
assert(len(m) == 1),0
queryFileName = m[0],0
Map this image back to the original directory,0
"%% Loop over annotations, find annotations that match this image",0
%%,0
"x,y,w,h",0
,0
"x,y is the bottom-left of the rectangle",0
,0
"x,y origin is the upper-left",0
...for each annotation,0
%% Render with PIL (scrap),0
%% Render with Matplotlib,0
Create figure and axes,0
Display the image,0
Location is the bottom-left of the rect,0
,0
Origin is the upper-left,0
Add the patch to the Axes,0
This is magic goop that removes whitespace around image plots (sort of),0
os.startfile(outputFileName),0
%% Showing figures on-screen during debugging,0
plt.show(),0
Various (mostly unsuccessful) approaches to getting the plot window to show up,0
"in the foreground, which is a backend-specific operation...",0
,0
fig.canvas.manager.window.activateWindow(),0
fig.canvas.manager.window.raise_(),0
fm = plt.get_current_fig_manager(),0
"fm.window.attributes('-topmost', 1)",0
"fm.window.attributes('-topmost', 0)",0
,0
# This is the one that I found to be most robust... at like 80% robust.,0
plt.get_current_fig_manager().window.raise_(),0
%%,0
...for each image,0
...for each file,0
,0
convert_imerit_json_to_coco_json.py,0
,0
"Takes a .json file with bounding boxes but no class labels, and a .json file containing the",0
"class labels for those images, and creates a new json file with class labels and bounding",0
boxes.,0
,0
"The bounding box .json file is in the format returned by our annotators, which is not",0
actually a fully-formed .json file; rather it's a series of .json objects,0
,0
"Leaves filenames intact.  Stores imerit ""category IDs"" (empty/human/group/animal) in a new",0
"field called ""annotation_type"".",0
,0
%% Imports and constants,0
%% Configure files and paths,0
"For Snapshot Serengeti, we stored image IDs in our annotation files as:",0
,0
S1_B06_R1_PICT0008,0
,0
...but the corresponding ID in the master database is actually:,0
,0
S1\B06\R1\S1_B06_R1_PICT0008,0
,0
"If this is ""True"", we'll expand the former to the latter",0
Handling a one-off issue in which .'s were mysteriously replaced with -'s,0
"in our annotations.  This will be set dynamically, but I keep it here as",0
a constant to remind me to remove this code when we clean this issue up.,1
Used in the (rare) case where a bounding box was added to an image that was originally,0
annotated as empty,0
Used in the (rare) case where we added bounding boxes to an image with multiple species,0
"Should we include ambiguous bounding boxes (with the ""ambiguous"" category label)",0
in the output file?  Ambiguous boxes are boxes drawn on images with multiple species.,0
"%%  Read metadata from the master database, bounding boxes from the annotations file",0
"The bounding box .json file is in the format returned by our annotators, which is not",0
actually a fully-formed .json file; rather it's a series of .json objects,0
"Each element of annData is a dictionary corresponding to a single sequence, with keys:",0
,0
"annotations, categories, images",0
sequence = annData[0],0
%% Build convenience mappings,0
Image ID to images,0
Category ID to categories (referring to the database categories),0
"Image ID to categories (i.e., species labels)",0
Utility function we'll use to create annotations for images in empty,0
sequences (empty images in non-empty sequences already have annotations),0
"%% Reformat annotations, grabbing category IDs from the master database (prep)",0
iSequence = 0; sequence = annData[0],0
"%% Reformat annotations, grabbing category IDs from the master database (loop)",0
Make a copy here; we're going to manipulate the sequence annotations,0
when we need to add synthetic annotations for empty images,0
im = sequenceImages[0],0
Are there any annotations in this sequence?,0
Which images in this sequence have annotations?,0
For each image in this sequence...,0
imeritImageID = im['id'],0
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG,0
Confirm that the file exists,0
Hande a one-off issue with our annotations,0
datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG,0
,0
...had become:,0
,0
datasetsnapshotserengeti.seqASG000001a-frame0.imgS1_B06_R1_PICT0008.JPG,0
Does it look like we encountered this issue?,0
Convert:,0
,0
S1_B06_R1_PICT0008,0
,0
...to:,0
,0
S1/B06/B06_R1/S1_B06_R1_PICT0008,0
Make sure we only see each image once,0
Create empty annotations for empty images,0
,0
Here we use the *unmodified* file name,0
Create an empty annotation for this image,0
Annotations still use the annotation filename (not database ID) at this point;,0
these will get converted to database IDs below when we process the,0
whole sequence.,0
Sanity-check image size,0
"print('Warning: img {} was listed in DB as {}x{}, annotated as {}x{}, actual size{}x{}'.format(",0
"old_id,new_im['width'],new_im['height'],im['width'],im['height'],imgObj.width,imgObj.height))",0
...for each image in this sequence,0
For each annotation in this sequence...,0
ann = sequenceAnnotations[0],0
Prepare an annotation using the category ID from the database and,0
the bounding box from the annotations file,0
Maintain iMerit's annotation category,0
Generate an (arbitrary) ID for this annotation; the COCO format has a concept,0
"of annotation ID, but our annotation files don't",0
This was a one-off quirk with our file naming,1
We'll do special handling of images with multiple categories later,0
Store the annotation type (group/human/animal/empty),0
This annotation has no bounding box but the image wasn't originally,0
annotated as empty,0
This annotation has a bounding box but the image was originally,0
annotated as empty,0
unnormalize the bbox,0
... for each annotation in this sequence,0
... for each sequence,0
%% Post-processing,0
Count empty images,0
...for each file,0
%% Sanity-check empty images,0
,0
make_ss_annotation_image_folder.py,0
,0
Take a directory full of images with the very long filenames we give annotators:,0
,0
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension,0
,0
"...along with a COCO-camera-traps database referring to those files, and:",0
,0
1) Creates a new COCO-camera-traps database with the original filenames in them,0
(copying the annotations),0
,0
2) Optionally creates a new directory with those images named according to the,0
"Snapshot Serengeti naming convention, including complete relative paths.",0
,0
See convert_imerit_json_to_coco_json to see how we get from the original annotation,0
.json to a COCO-camera-traps database.,0
,0
%% Constants and imports,0
%% Configure files/paths,0
%% Read the annotations (referring to the old filenames),0
"%% Update filenames, optionally copying files",0
im = data['images'][0],0
For each image...,0
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG,0
"Find the image name, e.g. S1_B06_R1_PICT0008",0
Convert:,0
,0
S1_B06_R1_PICT0008.JPG,0
,0
...to:,0
,0
S1/B06/B06_R1/S1_B06_R1_PICT0008.JPG,0
...for each image,0
%% Write the revised database,0
,0
get_annotation_tool_link.py,0
,0
"Takes a COCO-camera-traps-style .json file with URLs already embedded, and prepares",0
a link to the visipedia annotation tool that reviews a subset of those images.,0
,0
,0
create_new_annotation_json.py,0
,0
"Creates a subset of a larger .json database, in this case specifically to pick some images",0
from Snapshot Serengeti.,0
,0
from utils import get_db_dicts,0
for seq in already_annotated:,0
seq_to_ims.pop(seq),0
remove already annotated images,0
add lion images,0
lion_seqs = cat_to_seqs[cat_to_id['lionMale']] + cat_to_seqs[cat_to_id['lionFemale']],0
#print(len(lion_seqs)),0
lion_seqs = [seq for seq in lion_seqs if seq not in already_annotated],0
#print(len(lion_seqs)),0
"lion_seqs_to_annotate.extend(random.sample(lion_seqs, min(len(lion_seqs),num_lions)))",0
print(len(lion_seqs_to_annotate)),0
num_elephants = 1000,0
elephant_seqs = cat_to_seqs[cat_to_id['elephant']],0
#print(len(lion_seqs)),0
elephant_seqs = [seq for seq in elephant_seqs if seq not in already_annotated],0
#print(len(lion_seqs)),0
"elephant_seqs_to_annotate = random.sample(elephant_seqs, num_elephants)",0
num_empty = 10,0
empty_seqs_to_annotate = [],0
for loc in loc_to_seqs:,0
empty_seqs = cats_per_location[loc][cat_to_id['empty']],0
empty_seqs = [seq for seq in empty_seqs if seq not in already_annotated],0
empty_seqs = [seq for seq in empty_seqs if seq_to_season[seq] in seasons_to_keep],0
"empty_seqs_to_annotate.extend(random.sample(empty_seqs, min(len(empty_seqs),num_empty)))",0
ims_to_annotate.extend(empty_ims_to_annotate),0
,0
filter_database.py,0
,0
"Look through a COCO-ct database and find images matching some crtieria, writing",0
a subset of images and annotations to a new file.,0
,0
%% Constants and imports,0
%% Configuration,0
%%  Read database and build up convenience mappings,0
Category ID to category name,0
Image ID to image info,0
Image ID to image path,0
%% Filter,0
ann = annotations[0],0
Is this a tiny box or a group annotation?,0
"x,y,w,h",0
All empty annotations should be classified as either empty or ambiguous,0
,0
"The ambiguous cases are basically minor misses on the annotators' part,",0
where two different small animals were present somewhere.,0
%% Write output file,0
,0
find_images_for_classes.py,0
,0
"Given a .json database, find images that are associated with one or more",0
classes.,0
,0
%% Constants and imports,0
%% Configuration,0
%%  Read database and build up convenience mappings,0
Category ID to category name,0
Image ID to image info,0
Image ID to image path,0
%% Look for target-class annotations,0
ann = annotations[0],0
,0
create_tfrecords_format.py,0
,0
This script creates a tfrecords file from a dataset in VOTT format.,0
%% Imports and environment,0
set up the filenames and annotations,0
This loop reads the bboxes and corresponding labels and assigns them,0
the correct image. Kind of slow at the moment...,0
If needed: merging all classes,0
bbox_labels = ['Animal' for _ in bbox_labels],0
BBox coords are stored in the format,0
"x_min (of width axis) y_min (of height axis), x_max, y_max",0
Coordinate system starts in top left corner,0
"In this framework, we need ('ymin', 'xmin', 'ymax', 'xmax') format",0
print out some stats,0
To make sure we loaded the bboxes correctly:,0
self.validate_bboxes(),0
For each image in the data set...,0
Make sure all are greater equal 0,0
%% Main tfrecord generation function,0
Propagate optional metadata to tfrecords,0
endfor each annotation for the current image,0
endfor each image,0
"Calculate number of shards to get the desired number of images per record,",0
ensure it is evenly divisible by the number of threads,0
%% Command-line driver,0
%% Driver,0
,0
make_tfrecords_cis_trans.py,0
,0
Given a .json file that contains a three-element list (train/val/test) of image IDs and a .json database that contains,0
"those image IDs, generates tfrecords whose filenames include ""train""/""val""/""test""",0
,0
"dataset = json.load(open('/ai4efs/databases/snapshotserengeti/oneclass/SnapshotSerengeti_Seasons_1_to_4_tfrecord_format_valid_ims.json','r'))",0
print('Creating trans_val tfrecords'),0
dataset = [im_id_to_im[idx] for idx in trans_val],0
,0
create_classification_tfrecords_from_json.py,0
,0
Called from make_tfrecords_cis_trans.py if you're running a classification experiment.,0
,0
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/',0
datafile = 'eccv_18_annotation_files_oneclass/CaltechCameraTrapsECCV18',0
image_file_root = datafolder+'eccv_18_all_images/',0
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/',0
datafolder = '/data/iwildcam/',0
datafile = 'combined_iwildcam_annotations_oneclass/eccv_train_and_imerit_2',0
need consecutive category ids,0
old_cat_id_to_new_cat_id = {categories[idx]['id']:idx+1 for idx in range(len(categories))},0
print(old_cat_id_to_new_cat_id),0
remove multiclass images,0
print(images[0]),0
print(vis_data[0]),0
,0
make_tfrecords_from_json.py,0
,0
"Given a coco-camera-traps .json file, creates tfrecords",0
,0
Thin wrapper for create_tfrecords_from_json.,0
,0
%% Constants and imports,0
%% Constants and imports (interactive),0
%%,0
%% Main tfrecord generation function,0
"check whether the input file has already been converted to the tfrecords format,",0
"if not, convert",0
"Calculate number of shards to get the desired number of images per record,",0
ensure it is evenly divisible by the number of threads,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
eMammal_make_splits.py,0
,0
"Based on a tfrecords_format json file of the database, creates 3 splits according to",0
the specified fractions based on location (images from the same location should be in,0
one split) or based on images.,0
,0
"If a previous split is provided (append_to_previous_split is True), the entries in",0
"each split will be preserved, and new entries will be appended, so that new models",0
can warm start with a model trained on the original splits.,0
configurations and paths,0
approximate fraction for the new entries,0
read in the previous splits of image ID or location ID if available,0
"find new locations and assign them to a split, without reassigning any previous locations",0
"find out which images are new, shuffle and split them",0
do NOT sort the IDs to keep the shuffled order,0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
add empty category,0
"add all images that don't have annotations, with cat empty",0
print(detection_results.keys()),0
group the detections and gts by image id:,0
print(image_id),0
print(len(scores)),0
print(len(labels)),0
recall_thresh = 0.9,0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall]),0
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])",0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall]),0
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])",0
exp_name = 'eccv_train',0
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json',0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(len(detection_results['images'])),0
print(len(seqs)),0
"print(len(seqs[0]),len(seqs[",0
print(detection_results.keys()),0
group the detections by image id:,0
group the ground truth annotations by image id:,0
print(seq),0
"for image_id, dets in per_image_detections.iteritems():",0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
print(num_gts),0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
if sum(detected_class_labels)>0:,0
print('detected animal box'),0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])",0
"print(scores, tp_fp_labels)",0
num_total_gts += num_gts,0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
print('valid box'),0
"print(best_im, best_score)",0
exp_name = 'eccv_train',0
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json',0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
need to loop over confidence values,0
"for each value, check if any detections on the image are > conf",0
"If so, that image gets class ""animal""",0
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0",0
"for image_id, dets in per_image_detections.iteritems():",0
"calc prec, rec for this confidence thresh",0
group the ground truth annotations by image id:,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(detection_results.keys()),0
group the detections and gts by image id:,0
print(image_id),0
print(len(scores)),0
print(len(labels)),0
recall_thresh = 0.9,0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall]),0
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])",0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall]),0
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])",0
print(sorted_ap),0
plt.bar(sorted_ap),0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(detection_results.keys()),0
group the detections by image id:,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
print(gts),0
print(num_gts),0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[:2])",0
"print(scores, tp_fp_labels)",0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
print('detected animal box'),0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])",0
"print(scores, tp_fp_labels)",0
num_total_gts += num_gts,0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
print('valid box'),0
best_im = np.argmax(max_im_scores),0
"print(best_im, best_score)",0
"temp_labels = np.zeros(len(im_detection_labels),  dtype=np.int32)",0
"temp_scores = np.zeros(len(im_detection_scores), dtype=np.float32)",0
"for j in range(min(im_num_gts, len(im_detection_labels))):",0
temp_labels[j] = True #TODO: this currently only works for oneclass?,1
temp_scores[j] = best_score,0
im_detection_labels = temp_labels,0
im_detection_scores = temp_scores,0
num_total_gts+=im_num_gts,0
"print(len(detection_scores), len(detection_scores[0]), len(detection_scores[1]))",0
exp_name = 'small_balanced_cct',0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(seq),0
"for image_id, dets in per_image_detections.iteritems():",0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
seq_num_gts.append(num_gts),0
print(num_gts),0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
num_total_gts += 1,0
print('valid box'),0
"print(best_im, best_score)",0
print('no valid box'),0
if sum(seq_num_gts)>0:,0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
need to loop over confidence values,0
"for each value, check if any detections on the image are > conf",0
"If so, that image gets class ""animal""",0
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0",0
"calc prec, rec for this confidence thresh",0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
group the detections by image id:,0
group the ground truth annotations by image id:,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
print('detected animal box'),0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])",0
"print(scores, tp_fp_labels)",0
num_total_gts += num_gts,0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
print('valid box'),0
"print(best_im, best_score)",0
,0
evaluate_detections.py,0
,0
Adapted from analyze_detection.py which is now archived.,0
,0
%% Imports and constants,0
%% Functions,0
"labels input to compute_object_detection_metrics() needs to start at 0, not 1",0
num_detections = len(dets['boxes']),0
to prevent 'Invalid dimensions for box data.' error,0
this box will not match any detections,0
compute one-class precision/recall/average precision (if every box is just of an object class),0
%% Command-line driver,0
,0
detection_eval_utils.py,0
,0
Utility functions used in evaluate_detections.py,0
,0
group the ground truth annotations by image id,0
#####,0
,0
detect_and_predict_image.py,0
,0
"Functions to load a TensorFlow detection and a classification model, run inference,",0
"render bounding boxes on images, and write out the resulting",0
images (with bounding boxes and classes).,0
,0
"See the ""test driver"" cell for example invocation.",0
,0
,0
#####,0
"%% Constants, imports, environment",0
Minimum detection confidence for showing a bounding box on the output image,0
Stick this into filenames before the extension for the rendered result,0
Number of top-scoring classes to show at each bounding box,0
%% Core detection functions,0
Load images if they're not already numpy arrays,0
iImage = 0; image = images[iImage],0
"Load the image as an nparray of size h,w,nChannels",0
"There was a time when I was loading with PIL and switched to mpimg,",0
"but I can't remember why, and converting to RGB is a very good reason",0
"to load with PIL, since mpimg doesn't give any indication of color",0
"order, which basically breaks all .png files.",0
,0
"So if you find a bug related to using PIL, update this comment",0
"to indicate what it was, but also disable .png support.",0
image = mpimg.imread(image),0
This shouldn't be necessarily when loading with PIL and converting to RGB,0
Actual detection,0
...for each image,0
"Currently ""boxes"" is a list of length nImages, where each element is shaped as",0
,0
"1,nDetections,4",0
,0
"This implicitly banks on TF giving us back a fixed number of boxes, let's assert on this",0
to make sure this doesn't silently break in the future.,0
iBox = 0; box = boxes[iBox],0
"""scores"" is a length-nImages list of elements with size 1,nDetections",0
"""classes"" is a length-nImages list of elements with size 1,nDetections",0
,0
"Still as floats, but really representing ints",0
Squeeze out the empty axis,0
boxes is nImages x nDetections x 4,0
scores and classes are both nImages x nDetections,0
Get input and output tensors of classification model,0
"imsize = cur_image['width'], cur_image['height']",0
Select detections with a confidence larger 0.5,0
Get these boxes and convert normalized coordinates to pixel coordinates,0
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes",0
"However, we need to make sure that it box coordinates are still within the image",0
For convenience:,0
Create an array with contains the index of the corresponding crop_box for each selected box,0
i.e. [False False 0 False 1 2 3 False False],0
For each box,0
If this box should be classified,0
Run inference,0
if box should not be classified,0
...for each box,0
species_scores should have shape len(images) x len(boxes) x num_species,0
...for each image,0
...with tf.Session,0
with classification_graph,0
species_scores should have shape len(images) x len(boxes) x num_species,0
%% Rendering functions,0
Display the image,0
plt.show(),0
"top, left, bottom, right",0
,0
"x,y origin is the upper-left",0
Generate bounding box text,0
Choose color based on class,0
Location is the bottom-left of the rect,0
,0
Origin is the upper-left,0
iRight = x + w,0
iTop = y + h,0
Add the patch to the Axes,0
Add class description,0
First determine best location by finding the corner that is closest to the image center,0
relative corner coordinates,0
relative coordinates of image center,0
Compute pair-wise squared distance and get the index of the one with minimal distance,0
Get the corresponding coordinates ...,0
... and alignment for the text box,0
Plot the text box with background,0
...for each box,0
This is magic goop that removes whitespace around image plots (sort of),0
"plt.savefig(outputFileName, bbox_inches='tight', pad_inches=0.0, dpi=dpi, transparent=True)",0
os.startfile(outputFileName),0
...for each image,0
...def render_bounding_boxes,0
Load and run detector on target images,0
Read the name of all classes,0
remove empty lines,0
%% Interactive driver,0
%%,0
%%,0
%% File helper functions,0
%% Command-line driver,0
Hack to avoid running on already-detected images,1
,0
"Script for evaluating a frozen graph given a COCO-style dataset, which was generated by",0
the ../data_management/databases/classification/make_classification_dataset.py script.,0
This is useful for debugging as the accuracy reported by this script should match the,0
accuracy reported by the Tensorflow training.,0
,0
Check that all files exists for easier debugging,0
Load frozen graph,0
Collect tensors for input and output,0
Read image,0
Run inference,0
Check if correct,0
,0
Script for evaluating precision/recall on a two-class problem given,0
"a frozen graph and a COCO-style dataset, which was generated by",0
the ../data_management/databases/classification/make_classification_dataset.py script.,0
We assume the positive class is at index 0 (with a zero-based indexing),0
,0
Check that all files exists for easier debugging,0
Load frozen graph,0
Collect tensors for input and output,0
Read image,0
"with open(image_path, 'rb') as fi:",0
"image =  sess.run(tf.image.decode_jpeg(fi.read(), channels=3))",0
image = image / 255.,0
Run inference,0
predicted_class = np.argmax(predictions),0
Check if correct,0
if coco.imgToAnns[image_id][0]['category_id'] == predicted_class:,0
correct = correct + 1,0
"In matplotlib < 1.5, plt.fill_between does not have a 'step' argument",0
,0
Script for selecting testing images of a COCO-style dataset generated by the script,0
../data_management/databases/classification/make_classification_dataset.py in a consistent,0
manner and predicting the class for it.,0
,0
Mandatory parameters,0
Optional parameters,0
Validate parameters,0
Derived parameters,0
"We assume that the dataset was generated with the make_classification_dataset.py script,",0
hence the images should be located in the same folder as the json,0
Make seletion deterministic,0
Load frozen graph,0
Get dataset information,0
Get classes,0
...and the class list corresponding to the model outputs by assuming,0
that they are in order of their ids,0
Get images of each class,0
Shuffle the image list,0
Start the image sampling,0
"Set of avaiable class IDs, will be filled below",0
If there are still images left for that class,0
"Get image for the sampled class, we already shuffled the class images before so",0
we can simply pop(),0
Start prediction,0
Collect tensors for input and output,0
Read image,0
Run inference,0
Print output to log file,0
#####,0
,0
api_apply_classifier_single_node.py,0
,0
Takes the JSON file produced by the detection API and,0
classifies all boxes above a confidence threshold.,0
,0
#####,0
"%% Constants, imports, environment",0
Assumes that the root of the CameraTraps repo is on the PYTHONPATH,0
Minimum detection confidence for classifying an object,0
Number of top-scoring classes to show at each bounding box,0
Enlargment factor applied to boxes before passing them to the classifier,0
,0
Provides more context and can lead to better results,0
List of detection categories for which we will run the classification,0
,0
"Currently there are {""1"": ""animal"", ""2"": ""person"", ""4"": ""vehicle""}",0
,0
Should be a list of string-formatted ints.,0
Number of significant float digits in JSON output,0
%% Core detection functions,0
Read the name of all classes,0
remove empty lines,0
Create field with name *classification_categories*,0
Add classes using 0-based indexing,0
def add_classification_categories,0
Make sure we have the right json object,0
Get input and output tensors of classification model,0
For each image,0
Read image,0
"Scale pixel values to [0,1]",0
For each box,0
Skip detections with low confidence,0
Skip if detection category is not in whitelist,0
Skip if already classified,0
"Get current box in relative coordinates and format [x_min, y_min, width_of_box, height_of_box]",0
"Convert to [ymin, xmin, ymax, xmax] and store it as 1x4 numpy array so we can",0
re-use the generic multi-box padding code,0
Convert normalized coordinates to pixel coordinates,0
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes.",0
,0
"However, we need to make sure that it box coordinates are still within the image.",0
Get the first (and only) row as our bbox to classify,0
Get the image data for that box,0
Run inference,0
Add an empty list to the json for our predictions,0
Add the *num_annotated_classes* top scoring classes,0
...for each box,0
...for each image,0
...with tf.Session,0
with classification_graph,0
def classify_boxes,0
Load classification model,0
Load detector json,0
Add classes to detector_json,0
"Run classifier on all images, changes will be writting directly to the json",0
Write output json,0
def load_and_run_classifier,0
%% Command-line driver,0
,0
Script for selecting testing images of a COCO-style dataset generated by the script,0
../data_management/databases/classification/make_classification_dataset.py in a consistent,0
manner and predicting the class for it.,0
,0
Assumes the cameratraps repo root is on the path,0
Make seletion deterministic,0
Mandatory parameters,0
Optional parameters,0
Validate parameters,0
Tranfer parameters to post-processing format,0
Load frozen graph,0
Reading image list,0
Reading class list,0
Image sampling,0
Start prediction,0
Collect tensors for input and output,0
Array for collecting infos for rendering the html,0
Read image,0
"Scale pixel values to [0,1]",0
Run inference,0
Add links to all available classes,0
,0
predict_image.py,0
,0
"Given a pointer to a frozen detection graph, runs inference on a single image,",0
printing the top classes to the console,0
,0
%% Imports,0
%% Command-line processing,0
Check that all files exist for easier debugging,0
%% Inference,0
Load frozen graph,0
Load class list,0
Remove empty lines,0
Collect tensors for input and output,0
Read image,0
Run inference,0
Print output,0
,0
Mostly unmodified script for freezing a model,0
Added for convenience and for possible future optimizations,0
,0
Copyright 2015 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
'input_checkpoint' may be a prefix if we're using Saver V2 format,0
Remove all the explicit device specifications for this node. This helps to,0
make the graph more portable.,0
List of all partition variables. Because the condition is heuristic,0
"based, the list could include false positives.",0
This tensor doesn't exist in the graph (for example it's,0
'global_step' or a similar housekeeping element) so skip it.,0
`var_list` is required to be a map of variable names to Variable,0
tensors. Partition variables are Identity tensors that cannot be,0
handled by Saver.,0
Models that have been frozen previously do not contain Variables.,0
optimized_output_graph_def = optimize_for_inference_lib.optimize_for_inference(,0
"output_graph_def,",0
"input_node_names.replace("" "", """").split("",""),",0
"output_node_names.replace("" "", """").split("",""),",0
tf.float32.as_datatype_enum),0
Write GraphDef to file if output path has been given.,0
,0
"Creates a graph description, which is required to create a frozen graph.",0
Adapted from from ./tf-slim/export_inference_graph.py,0
Added preprocessing to the definition for easier handling,0
,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
tf.app.flags.DEFINE_integer(,0
"'batch_size', None,",0
"'Batch size for the exported model. Defaulted to ""None"" so batch size can '",0
'be specified at model runtime.'),0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
#####################,0
Optimization Flags #,0
#####################,0
######################,0
Learning Rate Flags #,0
######################,0
######################,0
Dataset Flags #,0
######################,0
####################,0
Fine-Tuning Flags #,0
####################,0
"Note: when num_clones is > 1, this will actually have each clone to go",0
over each epoch FLAGS.num_epochs_per_decay times. This is different,0
behavior from sync replicas and is expected to produce different results.,0
Warn the user if a checkpoint exists in the train_dir. Then we'll be,0
ignoring the checkpoint anyway.,0
TODO(sguada) variables.filter_variables(),1
######################,0
Config model_deploy #,0
######################,0
Create global_step,0
#####################,0
Select the dataset #,0
#####################,0
#####################,0
Select the network #,0
#####################,0
####################################,0
Select the preprocessing function #,0
####################################,0
#############################################################,0
Create a dataset provider that loads data from the dataset #,0
#############################################################,0
###################,0
Define the model #,0
###################,0
############################,0
Specify the loss function #,0
############################,0
Gather initial summaries.,0
"Gather update_ops from the first clone. These contain, for example,",0
the updates for the batch_norm variables created by network_fn.,0
Add summaries for end_points.,0
Add summaries for losses.,0
Add summaries for variables.,0
################################,0
Configure the moving averages #,0
################################,0
########################################,0
Configure the optimization procedure. #,0
########################################,0
"If sync_replicas is enabled, the averaging will be done in the chief",0
queue runner.,0
Update ops executed locally by trainer.,0
Variables to train.,0
and returns a train_tensor and summary_op,0
Add total_loss to summary.,0
Create gradient updates.,0
Add the summaries from the first clone. These contain the summaries,0
created by model_fn and either optimize_clones() or _gather_clone_loss().,0
Merge all summaries together.,0
##########################,0
Kicks off the training. #,0
##########################,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
#####################,0
Select the dataset #,0
#####################,0
###################,0
Select the model #,0
###################,0
#############################################################,0
Create a dataset provider that loads data from the dataset #,0
#############################################################,0
####################################,0
Select the preprocessing function #,0
####################################,0
###################,0
Define the model #,0
###################,0
Define the metrics:,0
Print the summaries to screen.,0
TODO(sguada) use num_epochs=1,1
This ensures that we make a single pass over all of the data.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Transform the image to floats.,0
"Randomly crop a [height, width] section of the image.",0
Randomly flip the image horizontally.,0
"Because these operations are not commutative, consider randomizing",0
the order their operation.,0
Subtract off the mean and divide by the variance of the pixels.,0
Transform the image to floats.,0
Resize and crop if needed.,0
Subtract off the mean and divide by the variance of the pixels.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Use tf.slice instead of crop_to_bounding box as it accepts tensors to,0
define the crop size.,0
Compute the rank assertions.,0
Create a random bounding box.,0
,0
Use tf.random_uniform and not numpy.random.rand as doing the former would,0
"generate random numbers at graph eval time, unlike the latter which",0
generates random numbers at graph definition time.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Pass the real x only to one of the func calls.,0
The random_* ops do not necessarily clamp.,0
"Each bounding box has shape [1, num_boxes, box coords] and",0
"the coordinates are ordered [ymin, xmin, ymax, xmax].",0
A large fraction of image datasets contain a human-annotated bounding,0
box delineating the region of the image containing the object of interest.,0
We choose to create a new bounding box for the object which is a randomly,0
distorted version of the human-annotated bounding box that obeys an,0
"allowed range of aspect ratios, sizes and overlap with the human-annotated",0
"bounding box. If no box is supplied, then we assume the bounding box is",0
the entire image.,0
Crop the image to the specified bounding box.,0
"Each bounding box has shape [1, num_boxes, box coords] and",0
"the coordinates are ordered [ymin, xmin, ymax, xmax].",0
Restore the shape since the dynamic slice based upon the bbox_size loses,0
the third dimension.,0
This resizing operation may distort the images because the aspect,0
ratio is not respected. We select a resize method in a round robin,0
fashion based on the thread number.,0
Note that ResizeMethod contains 4 enumerated resizing methods.,0
We select only 1 case for fast_mode bilinear.,0
Randomly flip the image horizontally.,0
Randomly distort the colors. There are 1 or 4 ways to do it.,0
Crop the central region of the image with an area containing 87.5% of,0
the original image.,0
Resize the image to the specified height and width.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TODO(sguada) docstring paragraph by (a) motivating the need for the file and,1
(b) defining clones.,0
TODO(sguada) describe the high-level components of model deployment.,1
"E.g. ""each model deployment is composed of several parts: a DeploymentConfig,",0
"which captures A, B and C, an input_fn which loads data.. etc",0
Set up DeploymentConfig,0
Create the global step on the device storing the variables.,0
Define the inputs,0
Define the optimizer.,0
Define the model including the loss.,0
Run training.,0
Namedtuple used to represent a clone during deployment.,0
"Namedtuple used to represent a DeployedModel, returned by deploy().",0
Default parameters for DeploymentConfig,0
Create clones.,0
The return value.,0
Individual components of the loss that will need summaries.,0
Compute and aggregate losses on the clone device.,0
Add the summaries out of the clone device block.,0
Only use regularization_losses for the first clone,0
Compute the total_loss summing all the clones_losses.,0
Sum the gradients across clones.,0
Gather initial summaries.,0
Create Clones.,0
"Gather update_ops from the first clone. These contain, for example,",0
the updates for the batch_norm variables created by model_fn.,0
Place the global step on the device storing the variables.,0
Compute the gradients for the clones.,0
Add summaries to the gradients.,0
Create gradient updates.,0
Only use regularization_losses for the first clone,0
Add the summaries from the first clone. These contain the summaries,0
created by model_fn and either optimize_clones() or _gather_clone_loss().,0
Add total_loss to summary.,0
Merge all summaries together.,0
Note that each grad_and_vars looks like the following:,0
"((grad_var0_clone0, var0), ... (grad_varN_cloneN, varN))",0
!/usr/bin/python,0
Copyright 2016 Google Inc. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Failed to find ""index"" occurrence of item.",0
pylint: disable=broad-except,0
pylint: enable=broad-except,0
Grab the 'index' annotation.,0
Some images contain bounding box annotations that,0
"extend outside of the supplied image. See, e.g.",0
n03127925/n03127925_147.xml,0
"Additionally, for some bounding boxes, the min > max",0
or the box is entirely outside of the image.,0
Example: <...>/n06470073/n00141669_6790.xml,0
Determine if the annotation is from an ImageNet Challenge label.,0
Note: There is a slight bug in the bounding box annotation data.,0
Many of the dog labels have the human label 'Scottish_deerhound',0
instead of the synset ID 'n02092002' in the bbox.label field. As a,0
"simple hack to overcome this issue, we only exclude bbox labels",1
*which are synset ID's* that do not match original synset label for,0
the XML file.,0
Guard against improperly specified boxes.,0
Note bbox.filename occasionally contains '%s' in the name. This is,0
data set noise that is fixed by just using the basename of the XML file.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The URLs where the MNIST data can be downloaded.,0
The names of the classes.,0
"First, process the training data:",0
"Next, process the testing data:",0
"Finally, write the labels file:",0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The URL where the Flowers data can be downloaded.,0
The number of images in the validation set.,0
Seed for repeatability.,0
The number of shards per dataset split.,0
Initializes function that decodes RGB JPEG data.,0
Read the filename:,0
Divide into train and test:,0
"First, convert the training and validation sets.",0
"Finally, write the labels file:",0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TODO(nsilberman): Add tfrecord file type once the script is updated.,1
"If set to false, will not try to set label_to_names in dataset",0
by reading them from labels.txt or github.,0
n01440764,0
n01443537,0
n02119247    black fox,0
n02119359    silver fox,0
pylint: disable=g-line-too-long,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The URL where the CIFAR data can be downloaded.,0
The number of training files.,0
The height and width of each image.,0
The names of the classes.,0
"First, process the training data:",0
"Next, process the testing data:",0
"Finally, write the labels file:",0
Copyright 2016 Google Inc. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The labels file contains a list of valid labels are held in this file.,0
Assumes that the file contains entries as such:,0
n01440764,0
n01443537,0
n01484850,0
where each line corresponds to a label expressed as a synset. We map,0
each synset contained in the file to an integer (based on the alphabetical,0
ordering). See below for details.,0
This file containing mapping from synset to human-readable label.,0
Assumes each line of the file looks like:,0
,0
n02119247    black fox,0
n02119359    silver fox,0
"n02119477    red fox, Vulpes fulva",0
,0
where each line corresponds to a unique mapping. Note that each line is,0
formatted as <synset>\t<human readable label>.,0
This file is the output of process_bounding_box.py,0
Assumes each line of the file looks like:,0
,0
"n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940",0
,0
where each line corresponds to one bounding box annotation associated,0
with an image. Each line can be parsed as:,0
,0
"<JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>",0
,0
Note that there might exist mulitple bounding box annotations associated,0
with an image file.,0
pylint: disable=expression-not-assigned,0
pylint: enable=expression-not-assigned,0
Create a single Session to run all image coding calls.,0
Initializes function that converts PNG to JPEG data.,0
Initializes function that converts CMYK JPEG data to RGB JPEG data.,0
Initializes function that decodes RGB JPEG data.,0
File list from:,0
https://groups.google.com/forum/embed/?place=forum/torch7#!topic/torch7/fOSTXHIESSU,0
File list from:,0
https://github.com/cytsai/ilsvrc-cmyk-image-list,0
Read the image file.,0
Clean the dirty data.,0
1 image is a PNG.,0
22 JPEG images are in CMYK colorspace.,0
Decode the RGB JPEG.,0
Check that image converted to RGB,0
Each thread produces N shards where N = int(num_shards / num_threads).,0
"For instance, if num_shards = 128, and the num_threads = 2, then the first",0
"thread would produce shards [0, 64).",0
"Generate a sharded version of the file name, e.g. 'train-00002-of-00010'",0
"Break all images into batches with a [ranges[i][0], ranges[i][1]].",0
Launch a thread for each batch.,0
Create a mechanism for monitoring when all threads are finished.,0
Create a generic TensorFlow-based utility for converting all image codings.,0
Wait for all the threads to terminate.,0
Leave label index 0 empty as a background class.,0
Construct the list of JPEG files and labels.,0
Shuffle the ordering of all image files in order to guarantee,0
random ordering of the images with respect to label in the,0
saved TFRecord files. Make the randomization repeatable.,0
Build a map from synset to human-readable label.,0
Run it!,0
Allowing None in the signature so that dataset_factory can use the default.,0
!/usr/bin/python,0
Copyright 2016 Google Inc. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Read in the 50000 synsets associated with the validation data set.,0
Make all sub-directories in the validation data dir.,0
Move all of the image to the appropriate sub-directory.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
"The dataset has classes with no images (empty and human), which have ID 0 and 1,",0
so we need to specify 49 here despite having only 47 classes with images,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Final pooling and prediction,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The current_stride variable keeps track of the effective stride of the,0
activations. This allows us to invoke atrous convolution whenever applying,0
the next residual unit would result in the activations having stride larger,0
than the target output_stride.,0
The atrous convolution rate parameter.,0
Move stride from the block's last unit to the end of the block.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Collect activations at the block's end before performing subsampling.,0
Subsampling of the block's output activations.,0
"The following implies padding='SAME' for pool1, which makes feature",0
alignment easier for dense prediction tasks. This is also used in,0
https://github.com/facebook/fb.resnet.torch. However the accompanying,0
code of 'Deep Residual Learning for Image Recognition' uses,0
padding='VALID' for pool1. You can switch to that choice by setting,0
"slim.arg_scope([slim.max_pool2d], padding='VALID').",0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Collect outputs for conv2d, fully_connected and max_pool2d",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"inputs has shape [batch, 224, 224, 3]",0
"inputs has shape [batch, 513, 513, 3]",0
We do not include batch normalization or activation functions in,0
conv1 because the first ResNet unit will perform these. Cf.,0
Appendix of [2].,0
This is needed because the pre-activation variant does not have batch,1
normalization or activation functions in the residual unit output. See,0
Appendix of [2].,0
Convert end_points_collection into a dictionary of end_points.,0
Global average pooling.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
=============================================================================,0
"These parameters come from the online port, which don't necessarily match",0
those in the paper.,0
TODO(nsilberman): confirm these values with Philip.,1
##########,0
Encoder #,0
##########,0
"No normalizer for the first encoder layers as per 'Image-to-Image',",0
Section 5.1.1,0
First layer doesn't use normalizer_fn,0
Last layer doesn't use activation_fn nor normalizer_fn,0
##########,0
Decoder #,0
##########,0
"Dropout is used at both train and test time as per 'Image-to-Image',",0
Section 2.1 (last paragraph).,0
The Relu comes BEFORE the upsample op:,0
Explicitly set the normalizer_fn to None to override any default value,0
"that may come from an arg_scope, such as pix2pix_arg_scope.",0
No normalization on the input layer.,0
Stride 1 on the last layer.,0
"1-dim logits, stride 1, no activation, no normalization.",0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
Alias,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pylint: disable=unused-import,0
pylint: enable=unused-import,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
If we are fine tuning a checkpoint we need to start at a lower learning,0
rate since we are farther along on training.,0
We can start quantizing immediately if we are finetuning.,0
We need to wait for the model to train a bit before we quantize if we are,0
training from scratch.,0
Call rewriter to produce graph with fake quant ops and folded batch norms,0
"quant_delay delays start of quantization till quant_delay steps, allowing",0
for better model accuracy.,0
Configure the learning rate using an exponential decay.,0
"When restoring from a floating point model, the min/max values for",0
quantized weights and activations are not present.,0
We instruct slim to ignore variables that are missing during restoration,0
by setting ignore_missing_vars=True,0
"If we are restoring from a floating point model, we need to initialize",0
the global step to zero for the exponential decay to result in,0
reasonable learning rates.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Orignaly, arg_scope = slim.arg_scope and layers = slim, now switch to more",0
update-to-date tf.contrib.* API.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Turns off fused batch norm.,0
collection containing the moving mean and moving variance.,0
"For inverted pyramid models, we start with gating switched off.",0
batch_size x 32 x 112 x 112 x 64,0
Separable conv is slow when used at first conv layer.,0
batch_size x 32 x 56 x 56 x 64,0
batch_size x 32 x 56 x 56 x 64,0
batch_size x 32 x 56 x 56 x 192,0
batch_size x 32 x 28 x 28 x 192,0
batch_size x 32 x 28 x 28 x 256,0
batch_size x 16 x 14 x 14 x 512,0
batch_size x 16 x 14 x 14 x 512,0
batch_size x 16 x 14 x 14 x 512,0
batch_size x 16 x 14 x 14 x 528,0
batch_size x 16 x 14 x 14 x 832,0
batch_size x 8 x 7 x 7 x 832,0
batch_size x 8 x 7 x 7 x 1024,0
Final pooling and prediction,0
Temporal average pooling.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
299 x 299 x 3,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 160,0
71 x 71 x 192,0
35 x 35 x 384,0
4 x Inception-A blocks,0
35 x 35 x 384,0
Reduction-A block,0
17 x 17 x 1024,0
7 x Inception-B blocks,0
17 x 17 x 1024,0
Reduction-B block,0
8 x 8 x 1536,0
3 x Inception-C blocks,0
Auxiliary Head logits,0
17 x 17 x 1024,0
Final pooling and prediction,0
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which",1
can be set to False to disable pooling here (as in resnet_*()).,0
8 x 8 x 1536,0
1 x 1 x 1536,0
1536,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN,1
setups need the gradient of gradient FusedBatchNormGrad.,0
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN,1
setups need the gradient of gradient FusedBatchNormGrad.,0
First upscaling is different because it takes the input vector.,0
Last layer has different normalizer and activation.,0
Convert to proper channels.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80,0
71 x 71 x 192,0
35 x 35 x 192,0
35 x 35 x 320,0
TODO(alemi): Register intermediate endpoints,1
"17 x 17 x 1088 if output_stride == 8,",0
33 x 33 x 1088 if output_stride == 16,0
TODO(alemi): register intermediate endpoints,1
TODO(gpapan): Properly support output_stride for the rest of the net.,1
8 x 8 x 2080,0
TODO(alemi): register intermediate endpoints,1
8 x 8 x 1536,0
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which",1
can be set to False to disable pooling here (as in resnet_*()).,0
Set weight_decay for weights in conv2d and fully_connected layers.,0
Set activation_fn and parameters for batch_norm.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
collection containing update_ops.,0
use fused batch norm if possible.,0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"inputs has shape [batch, 224, 224, 3]",0
"inputs has shape [batch, 513, 513, 3]",0
Use clip_by_value to simulate bandpass activation.,0
Convert end_points_collection into a dictionary of end_points.,0
Global average pooling.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"end_points will collect relevant activations for external use, for example",0
summaries or losses.,0
299 x 299 x 3,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80.,0
71 x 71 x 192.,0
35 x 35 x 192.,0
Inception blocks,0
mixed: 35 x 35 x 256.,0
mixed_1: 35 x 35 x 288.,0
mixed_2: 35 x 35 x 288.,0
mixed_3: 17 x 17 x 768.,0
mixed4: 17 x 17 x 768.,0
mixed_5: 17 x 17 x 768.,0
mixed_6: 17 x 17 x 768.,0
mixed_7: 17 x 17 x 768.,0
mixed_8: 8 x 8 x 1280.,0
mixed_9: 8 x 8 x 2048.,0
mixed_10: 8 x 8 x 2048.,0
Auxiliary Head logits,0
Shape of feature map before the final layer.,0
Final pooling and prediction,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 2048,0
2048,0
1000,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Reflection pad by 1 in spatial dimensions (axes 1, 2 = h, w) to make a 3x3",0
'valid' convolution produce an output with the same dimension as the,0
input.,0
This corrects 1 pixel offset for images with even width and height.,0
conv2d is left aligned and conv2d_transpose is right aligned for even,0
sized images (while doing 'SAME' padding).,0
Note: This doesn't reflect actual model in paper.,0
Neither dropout nor batch norm -> dont need is_training,0
##########,0
Encoder #,0
##########,0
7x7 input stage,0
##################,0
Residual Blocks #,0
##################,0
##########,0
Decoder #,0
##########,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"end_points will collect relevant activations for external use, for example",0
summaries or losses.,0
Used to find thinned depths for each layer.,0
Note that sizes in the comments below assume an input spatial size of,0
"224x224, however, the inputs can be of any size greater 32x32.",0
224 x 224 x 3,0
depthwise_multiplier here is different from depth_multiplier.,0
depthwise_multiplier determines the output channels of the initial,0
"depthwise conv (see docs for tf.nn.separable_conv2d), while",0
depth_multiplier controls the # channels of the subsequent 1x1,0
convolution. Must have,0
in_channels * depthwise_multipler <= out_channels,0
so that the separable convolution is not overparameterized.,0
Use a normal convolution instead of a separable convolution.,0
112 x 112 x 64,0
56 x 56 x 64,0
56 x 56 x 64,0
56 x 56 x 192,0
28 x 28 x 192,0
Inception module.,0
28 x 28 x 256,0
28 x 28 x 320,0
14 x 14 x 576,0
14 x 14 x 576,0
14 x 14 x 576,0
14 x 14 x 576,0
14 x 14 x 576,0
7 x 7 x 1024,0
7 x 7 x 1024,0
Final pooling and prediction,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 1024,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
=============================================================================,0
Tensorflow mandates these.,0
Conv and DepthSepConv namedtuple define layers of the MobileNet architecture,0
Conv defines 3x3 convolution layers,0
DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.,0
stride is the stride of the convolution,0
depth is the number of channels or filters in a layer,0
MOBILENETV1_CONV_DEFS specifies the MobileNet body,0
Used to find thinned depths for each layer.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
By passing filters=None,0
separable_conv2d produces only a depthwise convolution layer,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 1024,0
Set weight_decay for weights in Conv and DepthSepConv layers.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Turns off fused batch norm.,0
collection containing the moving mean and moving variance.,0
Final pooling and prediction,0
Temporal average pooling.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Orignaly, add_arg_scope = slim.add_arg_scope and layers = slim, now switch to",0
more update-to-date tf.contrib.* API.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
Only insert rate to params if rate > 1.,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Notes for training NASNet Cifar Model,0
-------------------------------------,0
batch_size: 32,0
learning rate: 0.025,0
cosine (single period) learning rate decay,0
auxiliary head loss weighting: 0.4,0
clip global norm of all gradients by 5,0
600 epochs with a batch size of 32,0
This is used for the drop path probabilities since it needs to increase,0
the drop out probability over the course of training.,0
Notes for training large NASNet model on ImageNet,0
-------------------------------------,0
batch size (per replica): 16,0
learning rate: 0.015 * 100,0
learning rate decay factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 100 replicas,0
auxiliary head loss weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Notes for training the mobile NASNet ImageNet model,0
-------------------------------------,0
batch size (per replica): 32,0
learning rate: 0.04 * 50,0
learning rate scaling factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 50 replicas,0
auxiliary head weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Shape of feature map before the final layer.,0
149 x 149 x 32,0
Run the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Find where to place the reduction cells or stride normal cells,0
Setup for building in the auxiliary head.,0
Run the cells,0
true_cell_num accounts for the stem cells,0
Final softmax layer,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Find where to place the reduction cells or stride normal cells,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Setup for building in the auxiliary head.,0
Run the cells,0
true_cell_num accounts for the stem cells,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Final softmax layer,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Calculate the total number of cells in the network.,0
There is no distinction between reduction and normal cells in PNAS so the,0
total number of cells is equal to the number normal cells plus the number,0
of stem cells (two by default).,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Calculate the total number of cells in the network.,0
There is no distinction between reduction and normal cells in PNAS so the,0
total number of cells is equal to the number normal cells plus the number,0
of stem cells (two by default).,0
Configuration for the PNASNet-5 model.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"The cap for tf.clip_by_value, it's hinted from the activation distribution",0
"that the majority of activation values are in the range [-6, 6].",0
Skip path 1,0
Skip path 2,0
"First pad with 0's on the right and bottom, then shift the filter to",0
include those 0's that were added.,0
"If odd number of filters, add an additional one to the second path.",0
Concat and apply BN,0
Set the prev layer to the current layer if it is none,0
Check to be sure prev layer stuff is setup correctly,0
num_or_size_splits=1,0
Apply conv operations,0
Combine hidden states using 'add'.,0
Add hiddenstate to the list of hiddenstates we can choose from,0
Dont stride if this is not one of the original hiddenstates,0
"Check if a stride is needed, then use a strided 1x1 here",0
Determine if a reduction should be applied to make the number of,0
filters match.,0
Return the concat of all the states,0
Scale keep prob by layer number,0
The added 2 is for the reduction cells,0
Decrease the keep probability over time,0
,0
Script for generating a two-class dataset in COCO format for training an obscured image classifier,0
,0
Requires Python >= 3.6 because of the glob ** expression,0
,0
Collect images and labels,0
"Labels: clean = 0, obscured = 1",0
,0
top_folders_to_bottom.py,0
,0
Given a base folder with files like:,0
,0
A/1/2/a.jpg,0
B/3/4/b.jpg,0
,0
"...moves the top-level folders to the bottom in a new output folder, i.e., creates:",0
,0
1/2/A/a.jpg,0
3/4/B/b.jpg,0
,0
"In practice, this is used to make this:",0
,0
animal/camera01/image01.jpg,0
,0
...look like:,0
,0
camera01/animal/image01.jpg,0
,0
%% Constants and imports,0
%% Support functions,0
%% Main functions,0
Find top-level folder,0
Find file/folder names,0
Move or copy,0
...def process_file(),0
Enumerate input folder,0
Convert absolute paths to relative paths,0
Standardize delimiters,0
Make sure each input file maps to a unique output file,0
relative_filename = relative_files[0],0
Loop,0
...def top_folders_to_bottom(),0
%% Interactive driver,0
%%,0
%%,0
%% Command-line driver,0
"python top_folders_to_bottom.py ""g:\temp\separated_images"" ""g:\temp\separated_images_inverted"" --n_threads 100",0
Convert to an options object,0
%% Constants and imports,0
Assumes ai4eutils is on the python path (https://github.com/Microsoft/ai4eutils),0
Assumes the cameratraps repo root is on the path,0
%% Options,0
"Make sure there is no overlap between the two sets, because this will cause",0
issues in the code,0
## Required inputs,0
## Options,0
Can be a folder or a SAS URL,0
# These apply only when we're doing ground-truth comparisons,0
Classes we'll treat as negative,0
,0
"Include the token ""#NO_LABELS#"" to indicate that an image with no annotations",0
should be considered empty.,0
Classes we'll treat as neither positive nor negative,0
"A list of output sets that we should count, but not render images for.",0
,0
"Typically used to preview sets with lots of empties, where you don't want to",0
"subset but also don't want to render 100,000 empty images.",0
,0
"detections, non_detections",0
"detections_animal, detections_person, detections_vehicle",0
"By default, choose a confidence threshold based on the detector version",0
Used for summary statistics only,0
"Number of images to sample, -1 for ""all images""",0
"Random seed for sampling, or None",0
Optionally separate detections into categories (animal/vehicle/human),0
,0
Currently only supported when ground truth is unavailable,0
Optionally replace one or more strings in filenames with other strings;,0
useful for taking a set of results generated for one folder structure,0
and applying them to a slightly different folder structure.,0
Allow bypassing API output loading when operating on previously-loaded,0
results,0
Should we also split out a separate report about the detections that were,0
just below our main confidence threshold?,0
,0
Currently only supported when ground truth is unavailable,0
Control rendering parallelization,0
...PostProcessingOptions,0
#%% Helper classes and functions,0
Anything greater than this isn't clearly positive or negative,0
image has annotations suggesting both negative and positive,0
"image is not annotated or is annotated with 'unknown', 'unlabeled', ETC.",0
image has not yet been assigned a state,0
"In some analyses, we add an additional class that lets us look at",0
detections just below our main confidence threshold,0
count the # of images with each type of DetectionStatus,0
Check whether this image has:,0
- unknown / unassigned-type labels,0
- negative-type labels,0
"- positive labels (i.e., labels that are neither unknown nor negative)",0
"assert has_unknown_labels is False, '{} has unknown labels'.format(annotations)",0
If there are no image annotations...,0
n_negative += 1,0
im['_detection_status'] = DetectionStatus.DS_NEGATIVE,0
"If the image has more than one type of labels, it's ambiguous",0
"note: bools are automatically converted to 0/1, so we can sum",0
"After the check above, we can be sure it's only one of positive,",0
"negative, or unknown.",0
,0
Important: do not merge the following 'unknown' branch with the first,0
"'unknown' branch above, where we tested 'if len(categories) == 0'",0
,0
If the image has only unknown labels,0
If the image has only negative labels,0
If the images has only positive labels,0
"Annotate the category, if it is unambiguous",0
...for each image,0
...mark_detection_status(),0
"Leaving code in place for reading from blob storage, may support this",0
in the future.,0
resize is to display them in this notebook or in the HTML more quickly,0
os.path.isfile() is slow when mounting remote directories; much faster,0
to just try/except on the image open.,0
return '',0
Render images to a flat folder... we can use os.sep here because we've,0
already normalized paths,0
"Create class labels like ""gt_1"" or ""gt_27""",0
"for i_box,box in enumerate(ground_truth_boxes):",0
gt_classes.append('_' + str(box[-1])),0
"errno.ENAMETOOLONG doesn't get thrown properly on Windows, so",0
we awkwardly check against a hard-coded limit,0
Use slashes regardless of os,0
Optionally add links back to the original images,0
...render_bounding_boxes,0
Count items in each category,0
Optionally sort by filename before writing to html,0
Write the individual HTML files,0
...prepare_html_subpages(),0
Get unique categories above the threshold for this image,0
Render an image (with no ground truth information),0
"This is a list of [class,confidence] pairs, sorted by confidence",0
"If we either don't have a confidence threshold, or we've met our",0
confidence threshold,0
...if this detection has classification info,0
...for each detection,0
...def render_image_no_gt(),0
This should already have been normalized to either '/' or '\',0
...def render_image_with_gt(),0
%% Main function,0
#%% Expand some options for convenience,0
#%% Prepare output dir,0
#%% Load ground truth if available,0
Mark images in the ground truth as positive or negative,0
#%% Load detection (and possibly classification) results,0
"If the caller hasn't supplied results, load them",0
Determine confidence thresholds if necessary,0
Remove failed rows,0
Convert keys and values to lowercase,0
"Add column 'pred_detection_label' to indicate predicted detection status,",0
not separating out the classes,0
#%% Pull out descriptive metadata,0
This is rare; it only happens during debugging when the caller,0
is supplying already-loaded API results.,0
"#%% If we have ground truth, remove images we can't match to ground truth",0
#%% Sample images for visualization,0
#%% Fork here depending on whether or not ground truth is available,0
"If we have ground truth, we'll compute precision/recall and sample tp/fp/tn/fn.",0
,0
Otherwise we'll just visualize detections/non-detections.,0
#%% Detection evaluation: compute precision/recall,0
numpy array of detection probabilities,0
"numpy array of bools (0.0/1.0), and -1 as null value",0
Don't include ambiguous/unknown ground truth in precision/recall analysis,0
"For completeness, include the result at a confidence threshold of 1.0",0
Compute and print summary statistics,0
Thresholds go up throughout precisions/recalls/thresholds; find the last,0
value where recall is at or above target.  That's our precision @ target recall.,0
"np.where returns a tuple of arrays, but in this syntax where we're",0
"comparing an array with a scalar, there will only be one element.",0
Convert back to a list,0
Flatten the confusion matrix,0
"#%% Collect classification results, if they exist",0
Mapping of classnames to idx for the confusion matrix.,0
,0
"The lambda is actually kind of a hack, because we use assume that",1
the following code does not reassign classname_to_idx,0
Confusion matrix as defaultdict of defaultdict,0
,0
"Rows / first index is ground truth, columns / second index is predicted category",0
iDetection = 0; fn = detector_files[iDetection]; print(fn),0
"If this image has classification predictions, and an unambiguous class",0
"annotated, and is a positive image...",0
"The unambiguous category, we make this a set for easier handling afterward",0
"Compute the accuracy as intersection of union,",0
i.e. (# of categories in both prediciton and GT),0
divided by (# of categories in either prediction or GT,0
,0
"In case of only one GT category, the result will be 1.0, if",0
prediction is one category and this category matches GT,0
,0
"It is 1.0/(# of predicted top-1 categories), if the GT is",0
one of the predicted top-1 categories.,0
,0
"It is 0.0, if none of the predicted categories is correct",0
Distribute this accuracy across all predicted categories in the,0
confusion matrix,0
...for each file in the detection results,0
If we have classification results,0
Build confusion matrix as array from classifier_cm,0
Print some statistics,0
Prepare confusion matrix output,0
Get confusion matrix as string,0
Get fixed-size classname for each idx,0
Prepend class name on each line and add to the top,0
Print formatted confusion matrix,0
"Actually don't, this gets really messy in all but the widest consoles",0
Plot confusion matrix,0
To manually add more space at bottom: plt.rcParams['figure.subplot.bottom'] = 0.1,0
,0
"Add 0.5 to figsize for every class. For two classes, this will result in",0
"fig = plt.figure(figsize=[4,4])",0
...if we have classification results,0
#%% Render output,0
Write p/r table to .csv file in output directory,0
Write precision/recall plot to .png file in output directory,0
#%% Sampling,0
Sample true/false positives/negatives with correct/incorrect top-1,0
classification and render to html,0
Accumulate html image structs (in the format expected by write_html_image_lists),0
"for each category, e.g. 'tp', 'fp', ..., 'class_bird', ...",0
Add default entries by accessing them for the first time,0
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]",0
"Each element will be a three-tuple with elements file,max_conf,detections",0
"Assemble the information we need for rendering, so we can parallelize without",0
dealing with Pandas,0
i_row = 0; row = images_to_visualize.iloc[0],0
Filenames should already have been normalized to either '/' or '\',0
Map all the rendering results in the list rendering_results into the,0
dictionary images_html,0
Prepare the individual html image files,0
Write index.html,0
Show links to each GT class,0
,0
We could do this without classification results; currently we don't.,0
Add links to all available classes,0
Close body and html tags,0
...for each image,0
"#%% Otherwise, if we don't have ground truth...",0
#%% Sample detections/non-detections,0
Accumulate html image structs (in the format expected by write_html_image_list),0
for each category,0
Add default entries by accessing them for the first time,0
"Maps detection categories - e.g. ""human"" - to result set names, e.g.",0
"""detections_human""",0
Add a set of results for each category and combination of categories,0
Create output directories,0
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]",0
"list of 3-tuples with elements (file, max_conf, detections)",0
"Assemble the information we need for rendering, so we can parallelize without",0
dealing with Pandas,0
i_row = 0; row = images_to_visualize.iloc[0],0
Filenames should already have been normalized to either '/' or '\',0
"render_image_no_gt(file_info,detection_categories_to_results_name,",0
"detection_categories,classification_categories)",0
Map all the rendering results in the list rendering_results into the,0
dictionary images_html,0
Prepare the individual html image files,0
Write index.html,0
"We can't just sum these, because image_counts includes images in both their",0
detection and classification classes,0
total_images = sum(image_counts.values()),0
Don't print classification classes here; we'll do that later with a slightly,0
different structure,0
Add links to all available classes,0
os.startfile(output_html_file),0
...if we do/don't have ground truth,0
...process_batch_results,0
%% Interactive driver(s),0
%%,0
options.unlabeled_classes = ['human'],0
os.start(ppresults.output_html_file),0
%% Command-line driver,0
,0
load_api_results.py,0
,0
Loads the output of the batch processing API (json) into a pandas dataframe.,0
,0
Also functions to group entries by seq_id.,0
,0
Includes the deprecated functions that worked with the old CSV API output format.,0
,0
%% Constants and imports,0
%% Functions for grouping by sequence_id,0
example,0
input 'file': 'SER/S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG',0
output 'id': 'S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG',0
%% Functions for loading the result as a Pandas DataFrame,0
Validate that this is really a detector output file,0
Fields in the API output json other than 'images',0
Normalize paths to simplify comparisons later,0
"image['file'] = image['file'].replace('\\','/')",0
Replace some path tokens to match local paths to original blob structure,0
"If this is a newer file that doesn't include maximum detection confidence values,",0
"add them, because our unofficial internal dataframe format includes this.",0
Pack the json output into a Pandas DataFrame,0
Sanity-check that this is really a detector output file,0
Normalize paths to simplify comparisons later,0
De-serialize detections,0
Optionally replace some path tokens to match local paths to the original blob structure,0
string_to_replace = list(options.detector_output_filename_replacements.keys())[0],0
"TODO: hit some silly issues with vectorized str() and escaped characters, vectorize",1
this later.,0
,0
"detection_results['image_path'].str.replace(string_to_replace,replacement_string)",0
iRow = 0,0
%% Imports,0
Assumes ai4eutils is on the python path (https://github.com/Microsoft/ai4eutils),0
%% Constants and support classes,0
We will confirm that this matches what we load from each file,0
Process-based parallelization isn't supported yet,0
%% Main function,0
"Warn the user if some ""detections"" might not get rendered",0
#%% Validate inputs,0
#%% Load both result sets,0
assert results_a['detection_categories'] == default_detection_categories,0
assert results_b['detection_categories'] == default_detection_categories,0
#%% Make sure they represent the same set of images,0
#%% Find differences,0
"Each of these maps a filename to a two-element list (the image in set A, the image in set B)",0
,0
"Right now, we only handle a very simple notion of class transition, where the detection",0
of maximum confidence changes class *and* both images have an above-threshold detection.,0
fn = filenames_a[0],0
We shouldn't have gotten this far if error_on_non_matching_lists is set,0
det = im_a['detections'][0],0
...for each filename,0
#%% Sample and plot differences,0
"Render two sets of results (i.e., a comparison) for a single",0
image.,0
...def render_image_pair(),0
fn = image_filenames[0],0
...def render_detection_comparisons(),0
"For each category, generate comparison images and the",0
comparison HTML page.,0
,0
category = 'common_detections',0
Choose detection pairs we're going to render for this category,0
...for each category,0
#%% Write the top-level HTML file content,0
...def compare_batch_results(),0
%% Interactive driver,0
%% KRU,0
%% Command-line driver,0
# TODO,1
,0
"Merge high-confidence detections from one results file into another file,",0
when the target file does not detect anything on an image.,0
,0
Does not currently attempt to merge every detection based on whether individual,0
detections are missing; only merges detections into images that would otherwise,0
be considered blank.,0
,0
"If you want to literally merge two .json files, see combine_api_outputs.py.",0
,0
%% Constants and imports,0
%% Structs,0
Don't bother merging into target images where the max detection is already,0
higher than this threshold,0
"If you want to merge only certain categories, specify one",0
(but not both) of these.,0
%% Main function,0
im = output_data['images'][0],0
"Determine whether we should be processing all categories, or just a subset",0
of categories.,0
i_source_file = 0; source_file = source_files[i_source_file],0
source_im = source_data['images'][0],0
detection_category = list(detection_categories)[0],0
"This is already a detection, no need to proceed looking for detections to",0
transfer,0
Boxes are x/y/w/h,0
source_sizes = [det['bbox'][2]*det['bbox'][3] for det in source_detections_this_category_raw],0
Only look at boxes below the size threshold,0
...for each detection category,0
"print('Adding {} detections to image {}'.format(len(detections_to_transfer),image_filename))",0
Update the max_detection_conf field (if present),0
...for each image,0
...for each source file,0
%% Test driver,0
%%,0
%% Command-line driver (TODO),0
,0
separate_detections_into_folders.py,0
,0
## Overview,0
,0
"Given a .json file with batch processing results, separate the files in that",0
"set of results into folders that contain animals/people/vehicles/nothing,",0
according to per-class thresholds.,0
,0
"Image files are copied, not moved.",0
,0
,0
## Output structure,0
,0
Preserves relative paths within each of those folders; cannot be used with .json,0
files that have absolute paths in them.,0
,0
"For example, if your .json file has these images:",0
,0
a/b/c/1.jpg,0
a/b/d/2.jpg,0
a/b/e/3.jpg,0
a/b/f/4.jpg,0
a/x/y/5.jpg,0
,0
And let's say:,0
,0
"* The results say that the first three images are empty/person/vehicle, respectively",0
"* The fourth image is above threshold for ""animal"" and ""person""",0
* The fifth image contains an animal,0
,0
* You specify an output base folder of c:\out,0
,0
You will get the following files:,0
,0
c:\out\empty\a\b\c\1.jpg,0
c:\out\people\a\b\d\2.jpg,0
c:\out\vehicles\a\b\e\3.jpg,0
c:\out\animal_person\a\b\f\4.jpg,0
c:\out\animals\a\x\y\5.jpg,0
,0
## Rendering bounding boxes,0
,0
"By default, images are just copied to the target output folder.  If you specify --render_boxes,",0
bounding boxes will be rendered on the output images.  Because this is no longer strictly,0
"a copy operation, this may result in the loss of metadata.  More accurately, this *may*",0
result in the loss of some EXIF metadata; this *will* result in the loss of IPTC/XMP metadata.,0
,0
Rendering boxes also makes this script a lot slower.,0
,0
## Classification-based separation,0
,0
"If you have a results file with classification data, you can also specify classes to put",0
"in their own folders, within the ""animals"" folder, like this:",0
,0
"--classification_thresholds ""deer=0.75,cow=0.75""",0
,0
"So, e.g., you might get:",0
,0
c:\out\animals\deer\a\x\y\5.jpg,0
,0
"In this scenario, the folders within ""animals"" will be:",0
,0
"deer, cow, multiple, unclassified",0
,0
"""multiple"" in this case only means ""deer and cow""; if an image is classified as containing a",0
"bird and a bear, that would end up in ""unclassified"", since the folder separation is based only",0
on the categories you provide at the command line.,0
,0
"No classification-based separation is done within the animal_person, animal_vehicle, or",0
animal_person_vehicle folders.,0
,0
%% Constants and imports,0
Occasionally we have near-zero confidence detections associated with COCO classes that,0
didn't quite get squeezed out of the model in training.  As long as they're near zero,0
"confidence, we just ignore them.",0
%% Options class,0
"Dictionary mapping categories (plus combinations of categories, and 'empty') to output folders",0
Populated only when using classification results,0
"Originally specified as a string, converted to a dict mapping name:threshold",0
...__init__(),0
...class SeparateDetectionsIntoFoldersOptions,0
%% Support functions,0
Find the maximum confidence for each category,0
,0
det = detections[0],0
"For zero-confidence detections, we occasionally have leftover goop",0
from COCO classes,0
assert det['conf'] < invalid_category_epsilon,0
...for each detection on this image,0
Count the number of thresholds exceeded,0
...for each category,0
If this is above multiple thresholds,0
"TODO: handle species-based separation in, e.g., the animal_person case",1
"Are we making species classification folders, and is this an animal?",0
Do we need to put this into a specific species folder?,0
Find the animal-class detections that are above threshold,0
Count the number of classification categories that are above threshold for at,0
least one detection,0
d = valid_animal_detections[0],0
classification = d['classifications'][0],0
"Do we have a threshold for this category, and if so, is",0
this classification above threshold?,0
...for each classification,0
...for each detection,0
...if we have to deal with classification subfolders,0
...if we have 0/1/more categories above threshold,0
...if this is/isn't a failure case,0
Skip this image if it's empty and we're not processing empty images,0
"At this point, this image is getting copied; we may or may not also need to",0
draw bounding boxes.,0
Do a simple copy operation if we don't need to render any boxes,0
Open the source image,0
"Render bounding boxes for each category separately, beacuse",0
we allow different thresholds for each category.,0
"When we're not using classification folders, remove classification",0
information to maintain standard detection colors.,0
...for each category,0
Read EXIF metadata,0
"Write output with EXIF metadata if available, and quality='keep' if this is a JPEG",0
"image.  Unfortunately, neither parameter likes ""None"", so we get a slightly",0
icky cascade of if's here.,0
Also see:,0
,0
https://discuss.dizzycoding.com/determining-jpg-quality-in-python-pil/,0
,0
...for more ways to preserve jpeg quality if quality='keep' doesn't do the trick.,0
...if we don't/do need to render boxes,0
...def process_detections(),0
%% Main function,0
Create output folder if necessary,0
Load detection results,0
Map class names to output folders,0
Create all combinations of categories,0
category_name = category_names[0],0
Do we have a custom threshold for this category?,0
Create folder mappings for each category,0
Create the actual folders,0
"Handle species classification thresholds, if specified",0
"E.g. deer=0.75,cow=0.75",0
token = tokens[0],0
...for each token,0
...if classification thresholds are still in string format,0
Validate the classes in the threshold list,0
...if we need to deal with classification categories,0
i_image = 14; im = images[i_image]; im,0
...for each image,0
...def separate_detections_into_folders,0
%% Interactive driver,0
%%,0
%%,0
%%,0
%% Testing various command-line invocations,0
"With boxes, no classification",0
"No boxes, no classification (default)",0
"With boxes, with classification",0
"No boxes, with classification",0
%% Command-line driver,0
"python api\batch_processing\postprocessing\separate_detections_into_folders.py ""d:\temp\rspb_mini.json"" ""d:\temp\demo_images\rspb_2018_2019_mini"" ""d:\temp\separation_test"" --nthreads 2",0
Convert to an options object,0
"print('{} {}'.format(v,name))",0
List of category numbers to use in separation; uses all categories if None,0
"Can be ""size"", ""width"", or ""height""",0
For each image...,0
,0
im = images[0],0
d = im['detections'][0],0
Are there really any detections here?,0
Is this a category we're supposed to process?,0
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing#detector-outputs,0
...for each detection,0
...for each image,0
...def categorize_detections_by_size(),0
,0
add_max_conf.py,0
,0
"The MD output format included a ""max_detection_conf"" field with each image",0
up to and including version 1.2; it was removed as of version 1.3 (it's,0
redundant with the individual detection confidence values).,1
,0
"Just in case someone took a dependency on that field, this script allows you",0
to add it back to an existing .json file.,0
,0
%% Imports and constants,0
%% Main function,0
%% Driver,0
#######,0
,0
remove_repeat_detections.py,0
,0
"Used after running find_repeat_detections, then manually filtering the results,",0
to create a final filtered output file.,0
,0
"If you want to use this script, we recommend that you read the user's guide:",0
,0
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms,0
,0
#######,0
%% Constants and imports,0
%% Main function,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
#######,0
,0
repeat_detections_core.py,0
,0
Core utilities shared by find_repeat_detections and remove_repeat_detections.,0
,0
#######,0
%% Imports and environment,0
"from ai4eutils; this is assumed to be on the path, as per repo convention",0
"""PIL cannot read EXIF metainfo for the images""",0
"""Metadata Warning, tag 256 had too many entries: 42, expected 1""",0
%% Constants,0
%% Classes,0
Relevant for rendering the folder of images for filtering,0
,0
"imageBase can also be a SAS URL, in which case some error-checking is",0
disabled.,0
Don't consider detections with confidence lower than this as suspicious,0
Don't consider detections with confidence higher than this as suspicious,0
What's the IOU threshold for considering two boxes the same?,0
How many occurrences of a single location (as defined by the IOU threshold),0
are required before we declare it suspicious?,0
"Ignore ""suspicious"" detections larger than some size; these are often animals",0
taking up the whole image.  This is expressed as a fraction of the image size.,0
"Ignore ""suspicious"" detections smaller than some size",0
Ignore folders with more than this many images in them,0
A list of classes we don't want to treat as suspicious. Each element is an int.,0
Load detections from a filter file rather than finding them from the detector output,0
".json file containing detections, should be called detectionIndex.json in the filtering_* folder",0
produced in the first pass,0
(optional) List of filenames remaining after deletion of identified,0
repeated detections that are actually animals.  This should be a flat,0
"text file, one relative filename per line.  See enumerate_images().",0
Turn on/off optional outputs,0
Determines whether bounding-box rendering errors (typically network errors) should,0
be treated as failures,0
Box rendering options,0
State variables,0
"Replace filename tokens after reading, useful when the directory structure",0
has changed relative to the structure the detector saw,0
How many folders up from the leaf nodes should we be going to aggregate images?,0
An optional function that takes a string (an image file name) and returns,0
"a string (the corresponding  folder ID), typically used when multiple folders",0
actually correspond to the same camera in a manufacturer-specific way (e.g.,0
a/b/c/RECONYX100 and a/b/c/RECONYX101 may really be the same camera).,0
Include/exclude specific folders... only one of these may be,0
"specified; ""including"" folders includes *only* those folders.",0
"Optionally show *other* detections (i.e., detections other than the",0
one the user is evaluating) in a light gray,0
"If bRenderOtherDetections is True, what color should we use to render the",0
(hopefully pretty subtle) non-target detections?,0
,0
"In theory I'd like these ""other detection"" rectangles to be partially",0
"transparent, but this is not straightforward, and the alpha is ignored",0
"here.  But maybe if I leave it here and wish hard enough, someday it",0
will work.,0
,0
otherDetectionsColors = ['dimgray'],0
Sort detections within a directory so nearby detections are adjacent,0
"in the list, for faster review.",0
,0
"Can be None, 'xsort', or 'clustersort'",0
,0
* None sorts detections chronologically by first occurrence,0
* 'xsort' sorts detections from left to right,0
* 'clustersort' clusters detections and sorts by cluster,0
Only relevant if smartSort == 'clustersort',0
"The data table (Pandas DataFrame), as loaded from the input json file via",0
load_api_results(),0
"The other fields in the input json file, loaded via load_api_results()",0
The data table after modification,0
dict mapping folder names to whole rows from the data table,0
dict mapping filenames to rows in the master table,0
"An array of length nDirs, where each element is a list of DetectionLocation",0
objects for that directory that have been flagged as suspicious,0
"This is a bit of a hack right now, but for future-proofing, I don't want to call this",1
"to retrieve anything other than the highest-confidence detection, and I'm assuming this",0
"is already sorted, so assert() that.",0
It's not clear whether it's better to use instances[0].bbox or self.bbox,1
"here... they should be very similar, unless iouThreshold is very low.",0
self.bbox is a better representation of the overal DetectionLocation.,0
%% Helper functions,0
"We store detetions as x/y/w/h, rtree and pyqtree use l/b/r/t",0
"We store detetions as x/y/w/h, rtree and pyqtree use l/b/r/t",0
%% Sort a list of candidate detections to make them visually easier to review,0
Just sort by the X location of each box,0
"Prepare a list of points to represent each box,",0
that's what we'll use for clustering,0
Upper-left,0
"points.append([det.bbox[0],det.bbox[1]])",0
Center,0
"Labels *could* be any unique labels according to the docs, but in practice",0
they are unique integers from 0:nClusters,0
Make sure the labels are unique incrementing integers,0
Store the label assigned to each cluster,0
"Now sort the clusters by their x coordinate, and re-assign labels",0
so the labels are sortable,0
"Compute the centroid for debugging, but we're only going to use the x",0
"coordinate.  This is the centroid of points used to represent detections,",0
which may be box centers or box corners.,0
old_cluster_label_to_new_cluster_label[old_cluster_label] =\,0
new_cluster_labels[old_cluster_label],0
%% Look for matches (one directory),0
List of DetectionLocations,0
candidateDetections = [],0
Create a tree to store candidate detections,0
For each image in this directory,0
,0
iDirectoryRow = 0; row = rows.iloc[iDirectoryRow],0
,0
"iDirectoryRow is a pandas index, so it may not start from zero;",0
"for debugging, we maintain i_iteration as a loop index.",0
print('Searching row {} of {} (index {}) in dir {}'.\,0
"format(i_iteration,len(rows),iDirectoryRow,dirName))",0
Don't bother checking images with no detections above threshold,0
"Array of dicts, where each element is",0
{,0
"'category': '1',  # str value, category ID",0
"'conf': 0.926,  # confidence of this detections",0
,0
"# (x_min, y_min) is upper-left, all in relative coordinates",0
"'bbox': [x_min, y_min, width_of_box, height_of_box]",0
,0
},0
For each detection in this image,0
"This is no longer strictly true; I sometimes run RDE in stages, so",0
some probabilities have already been made negative,0
,0
assert confidence >= 0.0 and confidence <= 1.0,0
Optionally exclude some classes from consideration as suspicious,0
Is this detection too big to be suspicious?,0
print('Illegal zero-size bounding box on image {}'.format(filename)),0
These are relative coordinates,0
print('Ignoring very small detection with area {}'.format(area)),0
print('Ignoring very large detection with area {}'.format(area)),0
This will return candidates of all classes,0
For each detection in our candidate list,0
Don't match across categories,0
Is this a match?,0
"If so, add this example to the list for this detection",0
We *don't* break here; we allow this instance to possibly,0
match multiple candidates.  There isn't an obvious right or,0
wrong here.,0
...for each detection on our candidate list,0
"If we found no matches, add this to the candidate list",0
candidateDetections.append(candidate),0
pyqtree,0
...for each detection,0
...for each row,0
Get all candidate detections,0
print('Found {} candidate detections for folder {}'.format(,0
"len(candidateDetections),dirName))",0
"For debugging only, it's convenient to have these sorted",0
as if they had never gone into a tree structure.  Typically,0
this is in practce a sort by filename.,0
...def find_matches_in_directory(dirName),0
"%% Update the detection table based on suspicious results, write .csv output",0
"An array of length nDirs, where each element is a list of DetectionLocation",0
objects for that directory that have been flagged as suspicious,0
For each directory,0
For each suspicious detection group in this directory,0
For each instance of this suspicious detection,0
This should match the bbox for the detection event,0
The bbox for this instance should be almost the same as the bbox,0
"for this detection group, where ""almost"" is defined by the IOU",0
threshold.,0
if iou < options.iouThreshold:,0
"print('IOU warning: {},{}'.format(iou,options.iouThreshold))",0
Make sure the bounding box matches,0
"Make the probability negative, if it hasn't been switched by",0
another bounding box,0
...for each instance,0
...for each detection,0
...for each directory,0
Update maximum probabilities,0
For each row...,0
No longer strictly true; sometimes I run RDE on RDE output,0
assert maxPOriginal >= 0,0
We should only be making detections *less* likely in this process,0
"If there was a meaningful change, count it",0
"Negative probabilities should be the only reason maxP changed, so",0
we should have found at least one negative value if we reached,0
this point.,0
...if there was a meaningful change to the max probability for this row,0
...for each row,0
If we're also writing output...,0
"...def update_detection_table(RepeatDetectionResults,options)",0
%% Main function,0
#%% Input handling,0
Validate some options,0
Load the filtering file,0
Load the same options we used when finding repeat detections,0
...except for things that explicitly tell this function not to,0
find repeat detections.,0
...if we're loading from an existing filtering file,0
Check early to avoid problems with the output folder,0
"Load file to a pandas dataframe.  Also populates 'max_detection_conf', even if it's",0
not present in the .json file.,0
detectionResults[detectionResults['failure'].notna()],0
"Before doing any real work, make sure we can *probably* access images",0
"This is just a cursory check on the first image, but it heads off most",0
"problems related to incorrect mount points, etc.  Better to do this before",0
spending 20 minutes finding repeat detections.,0
#%% Separate files into directories,0
This will be a map from a directory name to smaller data frames,0
This is a mapping back into the rows of the original table,0
iRow = 0; row = detectionResults.iloc[0],0
Create a new DataFrame with just this row,0
rowsByDirectory[dirName] = pd.DataFrame(row),0
...for each unique detection,0
Convert lists of rows to proper DataFrames,0
#% Look for matches (or load them from file),0
"Are we actually looking for matches, or just loading from a file?",0
length-nDirs list of lists of DetectionLocation objects,0
We're actually looking for matches...,0
"We get slightly nicer progress bar behavior using threads, by passing a pbar",0
object and letting it get updated.  We can't serialize this object across,0
processes.,0
#%% Find suspicious locations based on match results,0
For each directory,0
,0
iDir = 51,0
A list of DetectionLocation objects,0
A list of DetectionLocation objects,0
occurrenceList is a list of file/detection pairs,0
Sort the above-threshold detections for easier review,0
...for each directory,0
If we're just loading detections from a file...,0
"We're skipping detection-finding, but to see which images are actually legit false",0
"positives, we may be looking for physical files or loading from a text file.",0
For each directory,0
iDir = 0; detections = suspiciousDetections[0],0
,0
"suspiciousDetections is an array of DetectionLocation objects,",0
one per directory.,0
For each detection that was present before filtering,0
iDetection = 0; detection = detections[iDetection],0
Are we checking the directory to see whether detections were actually false,0
"positives, or reading from a list?",0
Is the image still there?,0
"If not, remove this from the list of suspicious detections",0
...for each detection,0
...for each directory,0
...if we are/aren't finding detections (vs. loading from file),0
Create filtering directory,0
iDir = 0; suspiciousDetectionsThisDir = suspiciousDetections[iDir],0
suspiciousDetectionsThisDir is a list of DetectionLocation objects,0
iDetection = 0; detection = suspiciousDetectionsThisDir[0],0
Sort instances in descending order by confidence,0
Choose the highest-confidence index,0
Should we render (typically in a very light color) detections,0
*other* than the one we're highlighting here?,0
Render other detections first (typically in a thin+light box),0
Now render the example detection (on top of at least one,0
of the other detections),0
This converts the *first* instance to an API standard detection;,0
"because we just sorted this list in descending order by confidence,",0
this is the highest-confidence detection.,0
...if we are/aren't rendering other bounding boxes,0
...for each detection in this folder,0
...for each folder,0
Write out the detection index,0
...if we're writing filtering info,0
...find_repeat_detections(),0
#######,0
,0
find_repeat_detections.py,0
,0
"If you want to use this script, we recommend that you read the user's guide:",0
,0
https://github.com/ecologize/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms,0
,0
"Really, don't try to run this script without reading the user's guide, you'll think",0
it's more magical than it is.,0
,0
"This script looks through a sequence of detections in the API output json file, and finds",0
"candidates that might be ""repeated false positives"", i.e. that random branch that the",0
detector thinks is an animal/person/vehicle.,0
,0
"Typically after running this script, you would do a manual step to remove",0
"true positives, then run remove_repeat_detections to produce a final output file.",0
,0
There's no way that statement was self-explanatory; see the user's guide.,0
,0
#######,0
%% Constants and imports,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
Convert to an options object,0
,0
If a request has been sent to AML for batch scoring but the monitoring thread of the API was,0
"interrupted (uncaught exception or having to re-start the API container), we could manually",0
"aggregate results from each shard using this script, assuming all jobs submitted to AML have finished.",0
,0
Need to have set environment variables STORAGE_ACCOUNT_NAME and STORAGE_ACCOUNT_KEY to those of the,0
"storage account backing the API. Also need to adjust the INTERNAL_CONTAINER, AML_CONTAINER and",0
AML_CONFIG fields in api_core/orchestrator_api/api_config.py to match the instance of the API that this,0
request was submitted to.,0
,0
May need to change the import statement in api_core/orchestrator_api/orchestrator.py,0
"""from sas_blob_utils import SasBlob"" to",0
"""from .sas_blob_utils import SasBlob"" to not confuse with the module in AI4Eutils;",0
"and change ""import api_config"" to",0
"""from api.batch_processing.api_core.orchestrator_api import api_config""",0
Execute this script from the root of the repository. You may need to add the repository to PYTHONPATH.,0
"list_jobs_submitted cannot be serialized (""can't pickle _thread.RLock objects ""), but",0
do not need it for aggregating results,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%% helper classes and functions,0
TODO log exception when we have more telemetry,1
TODO check that the expiry date of input_container_sas is at least a month,0
into the future,0
"if no permission specified explicitly but has an access policy, assumes okay",0
TODO - check based on access policy as well,1
return current UTC time as a string in the ISO 8601 format (so we can query by,0
timestamp in the Cosmos DB job status table.,0
example: '2021-02-08T20:02:05.699689Z',0
"image_paths will have length at least 1, otherwise would have ended before this step",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
a job moves from created to running/problem after the Batch Job has been submitted,0
"job_id should be unique across all instances, and is also the partition key",0
TODO do not read the entry first to get the call_params when the Cosmos SDK add a,1
patching functionality:,0
https://feedback.azure.com/forums/263030-azure-cosmos-db/suggestions/6693091-be-able-to-do-partial-updates-on-document,0
need to retain other fields in 'status' to be able to restart monitoring thread,0
retain existing fields; update as needed,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
sentinel should change if new configurations are available,0
configs have not changed,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Gunicorn logger handler will get attached if needed in server.py,0
set for all tasks in the job,0
cannot execute the scoring script that is in the mounted directory; has to be copied to cwd,0
not luck giving the commandline arguments via formatted string - set as env vars instead,0
form shards of images and assign each shard to a Task,0
for persisting stdout and stderr,0
persist stdout and stderr (will be removed when node removed),0
paths are relative to the Task working directory,0
can also just upload on failure,0
first try submitting Tasks,0
retry submitting Tasks,0
Change the Job's on_all_tasks_complete option to 'terminateJob' so the Job's status changes automatically,0
after all submitted tasks are done,0
This is so that we do not take up the quota for active Jobs in the Batch account.,0
return type: TaskAddCollectionResult,0
actually we should probably only re-submit if it's a server_error,0
docs: # https://docs.microsoft.com/en-us/rest/api/batchservice/odata-filters-in-batch#list-tasks,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%% Flask app,0
reference: https://trstringer.com/logging-flask-gunicorn-the-manageable-way/,0
%% Helper classes,0
%% Flask endpoints,0
required params,0
can be an URL to a file not hosted in an Azure blob storage container,0
"if use_url, then images_requested_json_sas is required",0
optional params,0
check model_version is among the available model versions,0
check request_name has only allowed characters,0
optional params for telemetry collection - logged to status table for now as part of call_params,0
All API instances / node pools share a quota on total number of active Jobs;,0
we cannot accept new Job submissions if we are at the quota,0
required fields,0
request_status is either completed or failed,0
the create_batch_job thread will stop when it wakes up the next time,0
"Fix for Zooniverse - deleting any ""-"" characters in the job_id",0
"If the status is running, it could be a Job submitted before the last restart of this",0
"API instance. If that is the case, we should start to monitor its progress again.",0
WARNING model_version could be wrong (a newer version number gets written to the output file) around,0
"the time that  the model is updated, if this request was submitted before the model update",0
and the API restart; this should be quite rare,0
conform to previous schemes,0
%% undocumented endpoints,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Gunicorn logger handler will get attached if needed in server.py,0
request_name and request_submission_timestamp are for appending to,0
output file names,0
image_paths can be a list of strings (Azure blob names or public URLs),0
"or a list of length-2 lists where each is a [image_id, metadata] pair",0
Case 1: listing all images in the container,0
- not possible to have attached metadata if listing images in a blob,0
list all images to process,0
+ 1 so if the number of images listed > MAX_NUMBER_IMAGES_ACCEPTED_PER_JOB,0
we will know and not proceed,0
Case 2: user supplied a list of images to process; can include metadata,0
filter down to those conforming to the provided prefix and accepted suffixes (image file types),0
prefix is case-sensitive; suffix is not,0
"Although urlparse(p).path preserves the extension on local paths, it will not work for",0
"blob file names that contains ""#"", which will be treated as indication of a query.",0
"If the URL is generated via Azure Blob Storage, the ""#"" char will be properly encoded",0
apply the first_n and sample_n filters,0
OK if first_n > total number of images,0
sample by shuffling image paths and take the first sample_n images,0
"upload the image list to the container, which is also mounted on all nodes",0
all sharding and scoring use the uploaded list,0
now request_status moves from created to running,0
an extra field to allow the monitoring thread to restart after an API restart: total number of tasks,0
also record the number of images to process for reporting,0
start the monitor thread with the same name,0
"both succeeded and failed tasks are marked ""completed"" on Batch",0
"preserving format from before, but SAS URL to 'failed_images' and 'images' are no longer provided",0
"failures should be contained in the output entries, indicated by an 'error' field",0
"when people download this, the timestamp will have : replaced by _",0
check if the result blob has already been written (could be another instance of the API / worker thread),0
"and if so, skip aggregating and uploading the results, and just generate the SAS URL, which",0
could be needed still if the previous request_status was `problem`.,0
upload the output JSON to the Job folder,0
%% Helper functions *copied* from ct_utils.py and visualization/visualization_utils.py,0
"Determine the factor, which shifts the decimal point of x",0
just behind the last significant digit,0
"Shift decimal point by multiplicatipon with factor, flooring, and",0
division by factor,0
PIL.Image.convert() returns a converted copy of this image,0
alter orientation as needed according to EXIF tag 0x112 (274) for Orientation,0
https://gist.github.com/dangtrinhnt/a577ece4cbe5364aad28,0
https://www.media.mit.edu/pia/Research/deepview/exif.html,0
"%% TFDetector class, an unmodified *copy* of the class in detection/tf_detector.py,",0
so we do not have to import the packages required by run_detector.py,0
Number of decimal places to round to for confidence and bbox coordinates,0
"MegaDetector was trained with batch size of 1, and the resizing function is a part",0
of the inference graph,0
An enumeration of failure reasons,0
"change from [y1, x1, y2, x2] to [x1, y1, width, height]",0
convert numpy floats to Python floats,0
need to change the above line to the following if supporting a batch size > 1 and resizing to the same size,0
"np_images = [np.asarray(image, np.uint8) for image in images]",0
"images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)",0
performs inference,0
our batch size is 1; need to loop the batch dim if supporting batch size > 1,0
%% Scoring script,0
determine if there is metadata attached to each image_id,0
information to determine input and output locations,0
other parameters for the task,0
test that we can write to output path; also in case there is no image to process,0
list images to process,0
"items in this list can be strings or [image_id, metadata]",0
model path,0
"Path to .pb TensorFlow detector model file, relative to the",0
models/megadetector_copies folder in mounted container,0
score the images,0
,0
manage_video_batch.py,0
,0
Notebook-esque script to manage the process of running a local batch of videos,0
through MD.  Defers most of the heavy lifting to manage_local_batch.py .,0
,0
%% Imports and constants,0
%% Split videos into frames,0
"%% List frame files, break into folders",0
Find unique (relative) folders,0
fn = frame_files[0],0
%% List videos,0
%% Check for videos that are missing entirely,0
list(folder_to_frame_files.keys())[0],0
video_filenames[0],0
fn = video_filenames[0],0
%% Check for videos with very few frames,0
%% Print the list of videos that are problematic,0
%% Process images like we would for any other camera trap job,0
"...typically using manage_local_batch.py, but do this however you like, as long",0
as you get a results file at the end.,0
,0
"If you do RDE, remember to use the second folder from the bottom, rather than the",0
bottom-most folder.,0
%% Convert frame results to video results,0
%% Confirm that the videos in the .json file are what we expect them to be,0
%% Scrap,0
%% Test a possibly-broken video,0
%% List videos in a folder,0
%% Imports,0
%% Constants,0
%% Classes,0
class variables,0
"instance variables, in order of when they are typically set",0
Leaving this commented out to remind us that we don't want this check here; let,0
the API fail on these images.  It's a huge hassle to remove non-image,0
files.,0
,0
for path_or_url in images_list:,0
if not is_image_file_or_url(path_or_url):,0
raise ValueError('{} is not an image'.format(path_or_url)),0
Commented out as a reminder: don't check task status (which is a rest API call),0
in __repr__; require the caller to explicitly request status,0
"status=getattr(self, 'status', None))",0
estimate # of failed images from failed shards,0
Download all three JSON urls to memory,0
Remove files that were submitted but don't appear to be images,0
assert all(is_image_file_or_url(s) for s in submitted_images),0
Diff submitted and processed images,0
Confirm that the procesed images are a subset of the submitted images,0
https://www.geeksforgeeks.org/break-list-chunks-size-n-python/,0
%% Interactive driver,0
%%,0
%%,0
%%,0
%% Imports and constants,0
%% Constants I set per script,0
## Required,0
base_output_folder_name = os.path.expanduser('~/postprocessing/' + organization_name_short),0
Shared Access Signature (SAS) tokens for the Azure Blob Storage container.,0
Leading question mark is optional.,0
,0
The read-only token is used for accessing images; the write-enabled token is,0
used for writing file lists.,0
## Typically left as default,0
"Pre-pended to all folder names/prefixes, if they're defined below",0
"This is how we break the container up into multiple taskgroups, e.g., for",0
separate surveys. The typical case is to do the whole container as a single,0
taskgroup.,0
"If your ""folders"" are really logical folders corresponding to multiple folders,",0
map them here,0
"A list of .json files to load images from, instead of enumerating.  Formatted as a",0
"dictionary, like folder_prefixes.",0
This is only necessary if you will be performing postprocessing steps that,0
"don't yet support SAS URLs, specifically the ""subsetting"" step, or in some",0
cases the splitting of files into multiple output directories for,0
empty/animal/vehicle/people.,0
,0
"For those applications, you will need to mount the container to a local drive.",0
For this case I recommend using rclone whether you are on Windows or Linux;,0
rclone is much easier than blobfuse for transient mounting.,0
,0
"But most of the time, you can ignore this.",0
"Supported model_versions: '4', '3', '4_prelim'",0
,0
Also available at the /supported_model_versions and /default_model_version,0
endpoints,0
,0
"Unless you have any specific reason to set this to a non-default value, leave",0
"it at the default, which as of 2020.04.28 is MegaDetector 4.1",0
,0
"additional_task_args = {""model_version"":""4_prelim""}",0
,0
"file_lists_by_folder will contain a list of local JSON file names,",0
each JSON file contains a list of blob names corresponding to an API taskgroup,0
"%% Derived variables, path setup",0
local folders,0
Turn warnings into errors if more than this many images are missing,0
%% Support functions,0
"scheme, netloc, path, query, fragment",0
%% Read images from lists or enumerate blobs to files,0
folder_name = folder_names[0],0
"Load file lists for this ""folder""",0
,0
file_list = input_file_lists[folder][0],0
Write to file,0
A flat list of blob paths for each folder,0
folder_name = folder_names[0],0
"If we don't/do have multiple prefixes to enumerate for this ""folder""",0
"If this is intended to be a folder, it needs to end in '/', otherwise",0
files that start with the same string will match too,0
...for each prefix,0
Write to file,0
...for each folder,0
%% Some just-to-be-safe double-checking around enumeration,0
Make sure each folder has at least one image matched; the opposite is usually a sign of a copy/paste issue,0
...for each image,0
...for each prefix,0
...for each folder,0
Make sure each image comes from one of our folders; the opposite is usually a sign of a bug up above,0
...for each prefix,0
...for each folder,0
...for each image,0
%% Divide images into chunks for each folder,0
The JSON file at folder_chunks[i][j] corresponds to task j of taskgroup i,0
list_file = file_lists_by_folder[0],0
"%% Create taskgroups and tasks, and upload image lists to blob storage",0
periods not allowed in task names,0
%% Generate API calls for each task,0
clipboard.copy(request_strings[0]),0
clipboard.copy('\n\n'.join(request_strings)),0
%% Run the tasks (don't run this cell unless you are absolutely sure!),0
I really want to make sure I'm sure...,0
%% Estimate total time,0
Around 0.8s/image on 16 GPUs,0
%% Manually create task groups if we ran the tasks manually,0
%%,0
"%% Write task information out to disk, in case we need to resume",0
%% Status check,0
print(task.id),0
%% Resume jobs if this notebook closes,0
%% For multiple tasks (use this only when we're merging with another job),0
%% For just the one task,0
%% Load into separate taskgroups,0
p = task_cache_paths[0],0
%% Typically merge everything into one taskgroup,0
"%% Look for failed shards or missing images, start new tasks if necessary",0
List of lists of paths,0
i_taskgroup = 0; taskgroup = taskgroups[i_taskgroup];,0
"Make a copy, because we append to taskgroup",0
i_task = 0; task = tasks[i_task],0
Each taskgroup corresponds to one of our folders,0
Check that we have (almost) all the images,0
Now look for failed images,0
Write it out as a flat list as well (without explanation of failures),0
...for each task,0
...for each task group,0
%% Pull results,0
i_taskgroup = 0; taskgroup = taskgroups[i_taskgroup]; task = taskgroup[0],0
Each taskgroup corresponds to one of our folders,0
...for each task,0
...for each task group,0
%% Combine results from task groups into final output files,0
Check that we have (almost) all the images,0
The only reason we should ever have a repeated request is the case where an,0
"image was missing and we reprocessed it, or where it failed and later succeeded",0
"There may be non-image files in the request list, ignore those",0
Something has gone bonkers if there are images in the results that,0
aren't in the request,0
...for each folder,0
%% Post-processing (no ground truth),0
i_folder = 0; folder_name_raw = folder_names[i_folder],0
"Omit some pages from the output, useful when animals are rare",0
%% Manual processing follows,0
,0
"Everything after this should be considered mostly manual, and no longer includes",0
looping over folders.,0
,0
%% RDE (sample directory collapsing),0
"In this example, the camera created folders called ""100EK113"", ""101EK113"", etc., for every N images",0
"%% Repeat detection elimination, phase 1",0
"Deliberately leaving these imports here, rather than at the top, because this cell is not",0
typically executed,0
To invoke custom collapsing of folders for a particular manufacturer's naming scheme,0
options.customDirNameFunction = remove_overflow_folders,0
Exclude people and vehicles from RDE,0
"options.excludeClasses = [2,3]",0
%% Manual RDE step,0
# DELETE THE VALID DETECTIONS ##,0
%% Re-filtering,0
%% Post-processing (post-RDE),0
i_folder = 0; folder_name_raw = folder_names[i_folder],0
"Omit some pages from the output, useful when animals are rare",0
"print('No RDE file available for {}, skipping'.format(folder_name))",0
continue,0
%% Create a new category for large boxes,0
%% Subsetting,0
i_folder = 0; folder_name = folders[i_folder],0
%% String replacement,0
%% Folder splitting,0
i_folder = 0; folder_name_raw = folder_names[i_folder],0
api_output_file = folder_name_to_combined_output_file[folder_name],0
%% Imports and constants,0
from ai4eutils,0
To specify a non-default confidence threshold for including detections in the .json file,0
Turn warnings into errors if more than this many images are missing,0
Only relevant when we're using a single GPU,0
"Specify a target image size when running MD... strongly recommended to leave this at ""None""",0
Only relevant when running on CPU,0
OS-specific script line continuation character,0
OS-specific script comment character,0
"Prefer threads on Windows, processes on Linux",0
"This is for things like image rendering, not for MegaDetector",0
Should we use YOLOv5's val.py instead of run_detector_batch.py?,1
Directory in which to run val.py.  Only relevant if use_yolo_inference_scripts is True.,0
Should we remove intermediate files used for running YOLOv5's val.py?,0
,0
Only relevant if use_yolo_inference_scripts is True.,0
Should we apply YOLOv5's augmentation?  Only allowed when use_yolo_inference_scripts,0
is True.,0
%% Constants I set per script,0
Optional descriptor,0
model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v5.0.0/md_v5b.0.0.pt'),0
model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v4.1.0/md_v4.1.0.pb'),0
"Number of jobs to split data into, typically equal to the number of available GPUs",0
Only used to print out a time estimate,0
"%% Derived variables, constant validation, path setup",0
%% Enumerate files,0
%% Load files from prior enumeration,0
%% Divide images into chunks,0
%% Estimate total time,0
%% Write file lists,0
%% Generate commands,0
"A list of the scripts tied to each GPU, as absolute paths.  We'll write this out at",0
the end so each GPU's list of commands can be run at once.  Generally only used when,0
"running lots of small batches via YOLOv5's val.py, which doesn't support checkpointing.",0
i_task = 0; task = task_info[i_task],0
Generate the script to run MD,0
Check whether this output file exists,0
Generate the script to resume from the checkpoint (only supported with MD inference code),0
...for each task,0
Write out a script for each GPU that runs all of the commands associated with,0
that GPU.  Typically only used when running lots of little scripts in lieu,0
of checkpointing.,0
...for each GPU,0
%% Run the tasks,0
%%% Run the tasks (commented out),0
i_task = 0; task = task_info[i_task],0
"This will write absolute paths to the file, we'll fix this later",1
...for each chunk,0
...if False,0
"%% Load results, look for failed or missing images in each task",0
i_task = 0; task = task_info[i_task],0
im = task_results['images'][0],0
...for each task,0
%% Merge results files and make images relative,0
im = combined_results['images'][0],0
%% Post-processing (pre-RDE),0
options.sample_seed = 0,0
"Omit some pages from the output, useful when animals are rare",0
%% RDE (sample directory collapsing),0
"In this example, the camera created folders called ""100EK113"", ""101EK113"", etc., for every N images",0
%%,0
%%,0
%%,0
relativePath = image_filenames[0],0
"%% Repeat detection elimination, phase 1",0
"Deliberately leaving these imports here, rather than at the top, because this",0
cell is not typically executed,0
options.minSuspiciousDetectionSize = 0.05,0
This will cause a very light gray box to get drawn around all the detections,0
we're *not* considering as suspicious.,0
options.lineThickness = 5,0
options.boxExpansion = 8,0
To invoke custom collapsing of folders for a particular manufacturer's naming scheme,0
options.customDirNameFunction = remove_overflow_folders,0
Exclude people and vehicles from RDE,0
"options.excludeClasses = [2,3]",0
options.maxImagesPerFolder = 50000,0
options.includeFolders = ['a/b/c'],0
options.excludeFolder = ['a/b/c'],0
"Can be None, 'xsort', or 'clustersort'",0
import clipboard; clipboard.copy(os.path.dirname(suspiciousDetectionResults.filterFile)),0
path_utils.open_file(os.path.dirname(suspiciousDetectionResults.filterFile)),0
%% Manual RDE step,0
# DELETE THE VALID DETECTIONS ##,0
%% Re-filtering,0
%% Post-processing (post-RDE),0
options.sample_seed = 0,0
"Omit some pages from the output, useful when animals are rare",0
"%% Run MegaClassifier (actually, write out a script that runs MegaClassifier)",0
"This is just passed along to the metadata in the output file, it has no impact",0
on how the classification scripts run.,0
#%% Set up environment,0
commands.append('cd CameraTraps/classification\n'),0
commands.append('conda activate cameratraps-classifier\n'),0
#%% Crop images,0
fn = input_files[0],0
#%% Run classifier,0
fn = input_files[0],0
#%% Remap classifier outputs,0
fn = input_files[0],0
#%% Merge classification and detection outputs,0
fn = input_files[0],0
#%% Write  out classification script,0
"%% Run a non-MegaClassifier classifier (i.e., a classifier with no output mapping)",0
"This is just passed along to the metadata in the output file, it has no impact",0
on how the classification scripts run.,0
#%% Set up environment,0
#%% Crop images,0
fn = input_files[0],0
#%% Run classifier,0
fn = input_files[0],0
#%% Merge classification and detection outputs,0
fn = input_files[0],0
#%% Write everything out,0
%% Run the classifier(s) via the .sh script(s) or batch file(s) we just wrote,0
...,0
%% Within-image classification smoothing,0
,0
Only count detections with a classification confidence threshold above,0
"*classification_confidence_threshold*, which in practice means we're only",0
looking at one category per detection.,0
,0
If an image has at least *min_detections_above_threshold* such detections,0
"in the most common category, and no more than *max_detections_secondary_class*",0
"in the second-most-common category, flip all detections to the most common",0
category.,0
,0
"Optionally treat some classes as particularly unreliable, typically used to overwrite an",0
"""other"" class.",0
,0
This cell also removes everything but the non-dominant classification for each detection.,0
,0
How many detections do we need above the classification threshold to determine a dominant category,0
for an image?,0
"Even if we have a dominant class, if a non-dominant class has at least this many classifications",0
"in an image, leave them alone.",0
"If the dominant class has at least this many classifications, overwrite ""other"" classifications",0
What confidence threshold should we use for assessing the dominant category in an image?,0
Which classifications should we even bother over-writing?,0
Detection confidence threshold for things we count when determining a dominant class,0
Which detections should we even bother over-writing?,0
"Before we do anything else, get rid of everything but the top classification",0
for each detection.,0
...for each detection in this image,0
...for each image,0
im = d['images'][0],0
...for each classification,0
...if there are classifications for this detection,0
...for each detection,0
"Handle a quirky special case: if the most common category is ""other"" and",0
"it's ""tied"" with the second-most-common category, swap them",0
secondary_count = category_to_count[keys[1]],0
The 'secondary count' is the most common non-other class,0
If we have at least *min_detections_to_overwrite_other* in a category that isn't,0
"""other"", change all ""other"" classifications to that category",0
...for each classification,0
...if there are classifications for this detection,0
...for each detection,0
"...if we should overwrite all ""other"" classifications",0
"At this point, we know we have a dominant category; change all other above-threshold",0
"classifications to that category.  That category may have been ""other"", in which",0
case we may have already made the relevant changes.,0
det = detections[0],0
...for each classification,0
...if there are classifications for this detection,0
...for each detection,0
...for each image,0
...for each file we want to smooth,0
"%% Post-processing (post-classification, post-within-image-smoothing)",0
classification_detection_file = classification_detection_files[1],0
%% Read EXIF data from all images,0
%% Prepare COCO-camera-traps-compatible image objects for EXIF results,0
import dateutil,0
"This is a standard format for EXIF datetime, and dateutil.parser",0
doesn't handle it correctly.,0
return dateutil.parser.parse(s),0
exif_result = exif_results[0],0
Currently we assume that each leaf-node folder is a location,0
"We collected this image this century, but not today, make sure the parsed datetime",0
jives with that.,0
,0
The latter check is to make sure we don't repeat a particular pathological approach,0
"to datetime parsing, where dateutil parses time correctly, but swaps in the current",0
date when it's not sure where the date is.,0
...for each exif image result,0
%% Assemble into sequences,0
Make a list of images appearing at each location,0
im = image_info[0],0
%% Load classification results,0
Map each filename to classification results for that file,0
%% Smooth classification results over sequences (prep),0
These are the only classes to which we're going to switch other classifications,0
Only switch classifications to the dominant class if we see the dominant class at least,0
this many times,0
"If we see more than this many of a class that are above threshold, don't switch those",0
classifications to the dominant class.,0
"If the ratio between a dominant class and a secondary class count is greater than this,",0
"regardless of the secondary class count, switch those classificaitons (i.e., ignore",0
max_secondary_class_classifications_above_threshold_for_class_smoothing).,0
,0
"This may be different for different dominant classes, e.g. if we see lots of cows, they really",0
"tend to be cows.  Less so for canids, so we set a higher ""override ratio"" for canids.",0
"If there are at least this many classifications for the dominant class in a sequence,",0
"regardless of what that class is, convert all 'other' classifications (regardless of",0
confidence) to that class.,0
"If there are at least this many classifications for the dominant class in a sequence,",0
"regardless of what that class is, classify all previously-unclassified detections",0
as that class.,0
Only count classifications above this confidence level when determining the dominant,0
"class, and when deciding whether to switch other classifications.",0
Confidence values to use when we change a detection's classification (the,0
original confidence value is irrelevant at that point),0
%% Smooth classification results over sequences (supporting functions),0
im = images_this_sequence[0],0
det = results_this_image['detections'][0],0
Only process animal detections,0
Only process detections with classification information,0
"We only care about top-1 classifications, remove everything else",0
Make sure the list of classifications is already sorted by confidence,0
...and just keep the first one,0
"Confidence values should be sorted within a detection; verify this, and ignore",0
...for each detection in this image,0
...for each image in this sequence,0
...top_classifications_for_sequence(),0
Count above-threshold classifications in this sequence,0
Sort the dictionary in descending order by count,0
"Handle a quirky special case: if the most common category is ""other"" and",0
"it's ""tied"" with the second-most-common category, swap them.",0
...def count_above_threshold_classifications(),0
%% Smooth classifications at the sequence level (main loop),0
Break if this token is contained in a filename (set to None for normal operation),0
i_sequence = 0; seq_id = all_sequences[i_sequence],0
Count top-1 classifications in this sequence (regardless of confidence),0
Handy debugging code for looking at the numbers for a particular sequence,0
Count above-threshold classifications for each category,0
"If our dominant category ID isn't something we want to smooth to, don't mess around with this sequence",0
"# Smooth ""other"" classifications ##",0
"By not re-computing ""max_count"" here, we are making a decision that the count used",0
"to decide whether a class should overwrite another class does not include any ""other""",0
classifications we changed to be the dominant class.  If we wanted to include those...,0
,0
sorted_category_to_count = count_above_threshold_classifications(classifications_this_sequence),0
max_count = get_first_value_from_sorted_dictionary(sorted_category_to_count),0
assert dominant_category_id == get_first_key_from_sorted_dictionary(sorted_category_to_count),0
# Smooth non-dominant classes ##,0
Don't flip classes to the dominant class if they have a large number of classifications,0
"Don't smooth over this class if there are a bunch of them, and the ratio",0
if primary to secondary class count isn't too large,0
Default ratio,0
Does this dominant class have a custom ratio?,0
# Smooth unclassified detections ##,0
...for each sequence,0
%% Write smoothed classification results,0
"%% Post-processing (post-classification, post-within-image-and-within-sequence-smoothing)",0
%% Zip .json files,0
%% 99.9% of jobs end here,0
Everything after this is run ad hoc and/or requires some manual editing.,0
%% Compare results files for different model versions (or before/after RDE),0
Choose all pairwise combinations of the files in [filenames],0
%% Merge in high-confidence detections from another results file,0
%% Create a new category for large boxes,0
"This is a size threshold, not a confidence threshold",0
size_options.categories_to_separate = [3],0
%% Preview large boxes,0
%% .json splitting,0
options.query = None,0
options.replacement = None,0
Reminder: 'n_from_bottom' with a parameter of zero is the same as 'bottom',0
%% Custom splitting/subsetting,0
i_folder = 0; folder_name = folders[i_folder],0
"This doesn't do anything in this case, since we're not splitting folders",0
options.make_folder_relative = True,0
%% String replacement,0
%% Splitting images into folders,0
%% Generate commands for a subset of tasks,0
i_task = 8,0
...for each task,0
%% End notebook: turn this script into a notebook (how meta!),0
Exclude everything before the first cell,0
Remove the first [first_non_empty_lines] from the list,0
Add the last cell,0
,0
xmp_integration.py,0
,0
"Tools for loading MegaDetector batch API results into XMP metadata, specifically",0
for consumption in digiKam:,0
,0
https://cran.r-project.org/web/packages/camtrapR/vignettes/camtrapr2.html,0
,0
%% Imports and constants,0
%% Class definitions,0
Folder where images are stored,0
.json file containing MegaDetector output,0
"String to remove from all path names, typically representing a",0
prefix that was added during MegaDetector processing,0
Optionally *rename* (not copy) all images that have no detections,0
above [rename_conf] for the categories in rename_cats from x.jpg to,0
x.check.jpg,0
"Comma-deleted list of category names (or ""all"") to apply the rename_conf",0
behavior to.,0
"Minimum detection threshold (applies to all classes, defaults to None,",0
i.e. 0.0,0
%% Functions,0
Relative image path,0
Absolute image path,0
List of categories to write to XMP metadata,0
Categories with above-threshold detections present for,0
this image,0
Maximum confidence for each category,0
Have we already added this to the list of categories to,0
write out to this image?,0
If we're supposed to compare to a threshold...,0
Else we treat *any* detection as valid...,0
Keep track of the highest-confidence detection for this class,0
If we're doing the rename/.check behavior...,0
Legacy code to rename files where XMP writing failed,0
%% Interactive/test driver,0
%%,0
%% Command-line driver,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Cosmos DB `batch-api-jobs` table for job status,0
"aggregate the number of images, country and organization names info from each job",0
submitted during yesterday (UTC time),0
create the card,0
,0
api_frontend.py,0
,0
"Defines the Flask app, which takes requests (one or more images) from",0
"remote callers and pushes the images onto the shared Redis queue, to be processed",0
by the main service in api_backend.py .,0
,0
%% Imports,0
%% Initialization,0
%% Support functions,0
Make a dict that the request_processing_function can return to the endpoint,0
function to notify it of an error,0
Verify that the content uploaded is not too big,0
,0
request.content_length is the length of the total payload,0
Verify that the number of images is acceptable,0
...def check_posted_data(request),0
%% Main loop,0
Check whether the request_processing_function had an error,0
Write images to temporary files,0
,0
TODO: read from memory rather than using intermediate files,1
"Submit the image(s) for processing by api_backend.py, who is waiting on this queue",0
TODO: convert to a blocking read and eliminate the sleep() statement in this loop,1
"image = Image.open(os.path.join(temp_direc, image_name))",0
...if we do/don't have a request available on the queue,0
...while(True),0
...def detect_sync(),0
%% Command-line driver,0
use --non-docker if you are testing without Docker,0
,0
python api_frontend.py --non-docker,0
# Camera trap real-time API configuration,0
"Full path to the temporary folder for image storage, only meaningful",0
within the Docker container,0
Upper limit on total content length (all images and parameters),0
Minimum confidence threshold for detections,0
Minimum confidence threshold for showing a bounding box on the output image,0
Use this when testing without Docker,0
,0
api_backend.py,0
,0
"Defines the model execution service, which pulls requests (one or more images)",0
"from the shared Redis queue, and runs them through the TF model.",0
,0
%% Imports,0
%% Initialization,0
%% Main loop,0
TODO: convert to a blocking read and eliminate the sleep() statement in this loop,1
Filter the detections by the confidence threshold,0
,0
"Each result is [ymin, xmin, ymax, xmax, confidence, category]",0
,0
"Coordinates are relative, with the origin in the upper-left",0
...if serialized_entry,0
...while(True),0
...def detect_process(),0
%% Command-line driver,0
use --non-docker if you are testing without Docker,0
,0
python api_frontend.py --non-docker,0
run detections on a test image to load the model,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%%,0
Importing necessary basic libraries and modules,0
%%,0
PyTorch imports for tensor operations,0
Setting the active CUDA device. This ensures that the computations take place on the specified GPU.,0
%%,0
"Importing the models, transformations, and utility functions from PytorchWildlife",0
%%,0
Setting constants for device and video paths TODO: add argparse,1
%%,0
Initializing the model for image detection,0
%%,0
Initializing the model for image classification,0
%%,0
Defining transformations for detection and classification,0
%%,0
Initializing a box annotator for visualizing detections,0
Processing the video and saving the result with annotated detections and classifications,0
%%,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%%,0
Importing basic libraries,0
%%,0
"Importing the models, dataset, transformations, and utility functions from PytorchWildlife",0
%%,0
Setting the device to use for computations ('cuda' indicates GPU),0
Initializing a supervision box annotator for visualizing detections,0
Initializing the detection and classification models,0
Defining transformations for detection and classification,0
%% Defining functions for different detection scenarios,0
Only run classifier when detection class is animal,0
%% Building Gradio UI,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%%,0
Importing necessary basic libraries and modules,0
%%,0
PyTorch imports,0
Setting the active CUDA device. Change the number according to your GPU setup,0
%%,0
"Importing the model, dataset, transformations and utility functions from PytorchWildlife",0
%%,0
"Setting the device to use for computations. Change to ""cpu"" if no GPU is available",0
%%,0
Initializing the MegaDetectorV5 model for image detection,0
%% Single image detection,0
Specifying the path to the target image TODO: Allow argparsing,0
Opening and converting the image to RGB format,0
Initializing the Yolo-specific transform for the image,0
Performing the detection on the single image,0
Saving the detection results,0
%% Batch detection,0
Specifying the folder path containing multiple images for batch detection,0
Creating a dataset of images with the specified transform,0
Creating a DataLoader for batching and parallel processing of the images,0
Performing batch detection on the images,0
%% Output to annotated images,0
Saving the batch detection results as annotated images,0
%% Output to cropped images,0
Saving the detected objects as cropped images,0
%% Output to JSON results,0
Saving the detection results in JSON format,0
"Determine the factor, which shifts the decimal point of x",0
just behind the last significant digit,0
"Shift decimal point by multiplicatipon with factor, flooring, and",0
division by factor,0
_ = pretty_print_object(obj),0
Sloppy that I'm making a module-wide change here...,1
Determine the coordinates of the intersection rectangle,0
The intersection of two axis-aligned bounding boxes is always an,0
axis-aligned bounding box,0
Compute the area of both AABBs,0
Compute the intersection over union by taking the intersection,0
area and dividing it by the sum of prediction + ground-truth,0
areas - the intersection area.,0
%% Constants and imports,0
%% Classes,0
Collect all names,0
Make names unique and sort,0
cast location to string as the entries in locations are strings,0
Convert classnames to lowercase to simplify comparisons later,0
Normalize paths to simplify comparisons later,0
"Make custom replacements in filenames, typically used to",0
accommodate changes in root paths after DB construction,0
## Build useful mappings to facilitate working with the DB,0
Category ID <--> name,0
Image filename --> ID,0
Image ID --> image object,0
Image ID --> annotations,0
"Each image can potentially multiple annotations, hence using lists",0
...__init__,0
...class IndexedJsonDb,0
,0
cct_json_to_filename_json.py,0
,0
"Given a .json file in COCO Camera Traps format, outputs a .json-formatted list of",0
relative file names present in the CCT file.,0
,0
%% Constants and environment,0
%% Main function,0
"json.dump(s,open(outputFilename,'w'))",0
%% Command-line driver,0
%% Interactive driver,0
%%,0
,0
cct_to_csv.py,0
,0
"""Converts"" a COCO Camera Traps .json file to .csv, in quotes because",0
"all kinds of assumptions are made here, and if you have a particular .csv",0
"format in mind, YMMV.  Most notably, does not include any bounding box information",0
or any non-standard fields that may be present in the .json file.  Does not,0
propagate information about sequence-level vs. image-level annotations.,0
,0
"Does not assume access to the images, therefore does not open .jpg files to find",0
"datetime information if it's not in the metadata, just writes datetime as 'unknown'.",0
,0
%% Imports,0
%% Main function,0
#%% Read input,0
#%% Build internal mappings,0
annotation = annotations[0],0
#%% Write output file,0
im = images[0],0
Write out one line per class:,0
...for each class name,0
...for each image,0
...with open(output_file),0
...def cct_to_csv,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
,0
remove_exif.py,0
,0
"Removes all EXIF/IPTC/XMP metadata from a folder of images, without making backup copies, using pyexiv2.",0
,0
%% Imports and constants,0
%% List files,0
%% Remove EXIF data (support),0
PYEXIV2 IS NOT THREAD SAFE; DO NOT CALL THIS IN PARALLEL FROM A SINGLE PROCESS,1
data = img.read_exif(); print(data),0
%% Debug,0
%%,0
%%,0
%% Remove EXIF data (execution),0
fn = image_files[0],0
"joblib.Parallel defaults to a process-based backend, but let's be sure",0
"results = Parallel(n_jobs=n_exif_threads,verbose=2,prefer='processes')(delayed(remove_exif)(fn) for fn in image_files[0:10])",0
,0
read_exif.py,0
,0
"Given a folder of images, read relevant EXIF fields from all images, and write them to",0
"a .json file.  Depends on having exiftool available, since every pure-Python approach",0
we've tried fails on at least some fields.,0
,0
Does not currently support command-line operation.,0
,0
%% Imports and constants,0
from functools import partial,0
From ai4eutils,0
exiftool_command_name = r'c:\exiftool-12.13\exiftool(-k).exe',0
%% Multiprocessing init,0
'i' means integer,0
%% Functions,0
image_files will contain the *relative* paths to all image files in the input folder,0
"file_path = os.path.join(input_base,image_files[0])",0
"-G means ""Print group name for each tag"", e.g. print:",0
,0
[File]          Bits Per Sample                 : 8,0
,0
...instead of:,0
,0
Bits Per Sample                 : 8,0
A list of three-element lists (type/tag/value),0
line_raw = exif_lines[0],0
A typical line:,0
,0
[ExifTool]      ExifTool Version Number         : 12.13,0
"Split on the first occurrence of "":""",0
...for each output line,0
...process_exif(),0
results = Parallel(n_jobs=n_threads)(delayed(add_exif_data)(im) for im in images[0:10]),0
%% Interactive driver,0
%%,0
%% Command-line driver,0
%% Create image objects,0
%% Write results,0
%% Scrap,0
%%,0
%%,0
,0
"Given a json-formatted list of image filenames, retrieve the width and height of every image.",0
,0
%% Constants and imports,0
%% Processing functions,0
Is this image on disk?,0
"all_results = list(tqdm(pool.imap(process_image, filenames), total=len(filenames)))",0
%% Interactive driver,0
%%,0
List images in a test folder,0
%%,0
"process_list_file(image_list_file,image_size_file,image_prefix=base_dir)",0
%% Command-line driver,0
"python sanity_check_json_db.py ""e:\wildlife_data\wellington_data\wellington_camera_traps.json"" --baseDir ""e:\wildlife_data\wellington_data\images"" --bFindUnusedImages --bCheckImageSizes",0
"python sanity_check_json_db.py ""D:/wildlife_data/mcgill_test/mcgill_test.json"" --baseDir ""D:/wildlife_data/mcgill_test"" --bFindUnusedImages --bCheckImageSizes",0
"Here the '-u' prevents buffering, which makes tee happier",0
,0
python -u sanity_check_json_db.py '/datadrive1/nacti_metadata.json' --baseDir '/datadrive1/nactiUnzip/' --bFindUnusedImages --bCheckImageSizes | tee ~/nactiTest.out,0
,0
Partially-formed stub to get from MegaDetector output files to COCO Camera Traps data.,0
,0
"Was actually written to convert *many* MD .json files to a single CCT file, hence",0
the loop over .json file.,0
,0
"THIS CODE HAS NEVER BEEN RUN, it was added as a demonstration of how to do this.  YMMV.",0
,0
%% Constants and imports,0
"Images sizes are required to convert between absolute and relative coordinates,",0
so we need to read the images.,0
Only required if you want to write a database preview,0
%% Create CCT dictionaries,0
image_ids_to_images = {},0
Force the empty category to be ID 0,0
Load .json annotations for this data set,0
i_entry = 0; entry = data['images'][i_entry],0
,0
"PERF: Not exactly trivially parallelizable, but about 100% of the",0
time here is spent reading image sizes (which we need to do to get from,0
"absolute to relative coordinates), so worth parallelizing.",0
Generate a unique ID from the path,0
detection = detections[0],0
Have we seen this category before?,0
Create an annotation,0
"MegaDetector: [x,y,width,eight] (normalized, origin upper-left)",0
"CCT: [x,y,width,height] (absolute, origin upper-left)",0
...for each detection,0
...for each image,0
Remove non-reviewed images and associated annotations,0
%% Create info struct,0
%% Write .json output,0
%% Clean start,0
## Everything after this should work from a clean start ###,0
%% Validate output,0
%% Preview animal labels,0
%% Preview empty labels,0
"viz_options.classes_to_exclude = ['empty','human']",0
,0
cct_to_wi.py,0
,0
Converts COCO Camera Traps .json files to the Wildlife Insights,0
batch upload format,0
,0
Also see:,0
,0
https://github.com/ConservationInternational/Wildlife-Insights----Data-Migration,0
,0
https://data.naturalsciences.org/wildlife-insights/taxonomy/search,0
,0
%% Imports,0
%% Paths,0
A COCO camera traps file with information about this dataset,0
A .json dictionary mapping common names in this dataset to dictionaries with the,0
"WI taxonomy fields: common_name, wi_taxon_id, class, orer, family, genus, species",0
%% Constants,0
%% Project information,0
%% Read templates,0
%% Compare dictionary to template lists,0
Write the header,0
Write values,0
%% Project file,0
%% Camera file,0
%% Deployment file,0
%% Images file,0
Read .json file with image information,0
Read taxonomy dictionary,0
Populate output information,0
df = pd.DataFrame(columns = images_fields),0
annotation = annotations[0],0
im = input_data['images'][0],0
"We don't have counts, but we can differentiate between zero and 1",0
This is the label mapping used for our incoming iMerit annotations,0
"Only used to parse the incoming annotations. In our database, the string name is used to avoid confusion",0
MegaDetector outputs,0
,0
add_bounding_boxes_to_megadb.py,0
,0
"Given COCO-formatted JSONs containing manually labeled bounding box annotations, add them to",0
"MegaDB sequence entries, which can then be ingested into MegaDB.",0
the category map that comes in the COCO JSONs for iMerit batch 12 - to check that each,0
JSON,0
"dataset name : (seq_id, frame_num) : [bbox, bbox]",0
where bbox is a dict with str 'category' and list 'bbox',0
iterate over image_id_to_image rather than image_id_to_annotations so we include,0
the confirmed empty images,0
The file_name field in the incoming json looks like,0
alka_squirrels.seq2020_05_07_25C.frame119221.jpg,0
there seems to be a bug in the annotations where sometimes there's a,0
non-empty label along with a label of category_id 5,0
ignore the empty label (they seem to be actually non-empty),0
,0
add_bounding_boxes_to_json.py,0
,0
This script takes a image database in the COCO Camera Traps format and merges in a set of bounding,0
box annotations in the format that iMerit uses (a .json where actually only each row is a valid json).,0
,0
"If you need to update an existing bbox database, please get all the original annotation files and",0
re-generate from scratch,0
,0
%% Imports,0
%% Configurations and paths,0
images database,0
output bboxes database,0
annotation files (pseudo json) obtained from our annotation vendor that contain annotations for this dataset,0
None or a string or tuple of strings that is the prefix to all file_name of interest / in this dataset in the annotation files,0
functions for mapping the image_id in the annotation files (pseudo jsons) to the image_id used in the image DB,0
our img_id doesn't contain frame info,0
"batch3 - ""file_name"":""ASG0000019_0_S1_B06_R1_PICT0007.JPG""",0
"batch5 and 7 - ""file_name"":""datasetsnapshotserengeti.seqASG000002m-frame0.imgS1_B06_R1_PICT0056.JPG""",0
sometimes - 'datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG',0
"id in DB (old_token): 'S6/J01/J01_R1/S6_J01_R1_IMAG0001', 'S1/B05/B05_R1/S1_B05_R1_PICT0036'",0
specify which one to use for your dataset here,0
%% Load the image database and fill in DB info for the output bbox database,0
load the images database,0
%% Find the height and width of images from the annotation files,0
,0
...if they are not available in the images DB,0
each row in this pseudo-json is a COCO formatted entry for an image sequence,0
%% Other functions required by specific datasets,0
the IDFG image database does not include images from unlabeled folders that were annotated with bounding boxes,0
%% Create the bbox database from all annotation files pertaining to this dataset,0
"for the incoming annotations, look up by category name (common) and convert to the numerical id used in our databases",0
"for each annotation pseudo-json, check that the image it refers to exists in the original database",0
each row in this pseudo-json is a COCO formatted entry for an image sequence,0
check that entry is for this dataset,0
category map for this entry in the annotation file - usually the same across all entries but just in case,0
rspb_add_image_entry(img_id),0
use the image length and width in the image DB,0
"[top left x, top left y, width, height] in relative coordinates",0
"add all images that have been sent to annotation, some of which may be empty of bounding boxes",0
rspb_add_image_entry(db_image_id),0
%% Common queries,0
This query is used when preparing tfrecords for object detector training.,0
We do not want to get the whole seq obj where at least one image has bbox because,0
some images in that sequence will not be bbox labeled so will be confusing.,0
Include images with bbox length 0 - these are confirmed empty by bbox annotators.,0
"If frame_num is not available, it will not be a field in the result iterable.",0
"Note that the seq_id is the Cosmos DB assigned ID for that sequence, not the",0
"seq_id field, which may contain ""/"" characters.",0
"For public datasets to be converted to the CCT format, we get the whole seq object because",0
sequence level attributes need to be included too. megadb/converters/megadb_to_cct.py handles,0
the case of bbox-only JSONs with the flag exclude_species_class,0
Getting all sequences in a dataset - for updating or deleting entries which need the id field,0
%% Parameters,0
Use None if querying across all partitions,0
"The `sequences` table has the `dataset` as the partition key, so if only querying",0
"entries from one dataset, set the dataset name here.",0
"e.g. {'name': '@top_n', 'value': 100} - see query_and_upsert_examples/query_for_data.ipynb",0
Use False if do not want all results stored in a single JSON.,0
%% Script,0
execute the query,0
loop through and save the results,0
MODIFY HERE depending on the query,0
wiitigers Unicode issue - no good mapping from DB file names to file names in blob URL,0
build filename,0
if need to re-download a dataset's images in case of corruption,0
entries_to_download = {,0
"filename: entry for filename, entry in entries_to_download.items()",0
if entry['dataset'] == DATASET,0
},0
input validation,0
"existing files, with paths relative to <store_dir>",0
parse JSON or TXT file,0
"create a new storage container client for this dataset,",0
and cache it,0
schema already checks that the min possible value of frame_num is 0,0
"if there are more than one image item, each needs a frame_num",0
checks across all sequence items,0
per sequence item checks,0
load the schema,0
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory,0
load the schema,0
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory,0
checks across all datasets items,0
check for expiry date of the SAS keys,0
update the sequences,0
"`id` is from the database, as well as all attributes starting with _",0
"if valuable sequence information is available, add them to the image",0
required fields for an image object,0
add seq-level class labels for this image,0
add other sequence-level properties to each image too,0
add other image-level properties,0
... for im in seq['images'],0
... for seq in mega_db,0
consolidate categories,0
some property names have changed in the new schema,0
a dummy sequence ID will be generated if the image entry does not have a seq_id field,0
seq_id only needs to be unique within this dataset; MegaDB does not rely on it as the _id field,0
"""annotations"" fields are opened and have its sub-field surfaced one level up",0
set the `dataset` property on each sequence to the provided dataset_name,0
check that the location field is the same for all images in a sequence,0
check which fields in a CCT image entry are sequence-level,0
image-level properties that really should be sequence-level,0
need to add (misidentified) seq properties not present for each image in a sequence to img_level_properties,0
"(some properties act like flags - all have the same value, but not present on each img)",0
add the sequence-level properties to the sequence objects,0
not every sequence have to have all the seq_level_properties,0
get the value of this sequence-level property from the first image entry,0
check which fields are really dataset-level and should be included in the dataset table instead.,0
delete sequence-level properties that should be dataset-level,0
make all `class` fields lower-case; cast `seq_id` to type string in case they're integers,0
turn all float NaN values into None so it gets converted to null when serialized,0
this was an issue in the Snapshot Safari datasets,0
%% validation,0
"at first a dict of image_id: image_obj with annotations embedded, then it becomes",0
an array of image objects,0
%% integrate the image DB,0
takes in image entries and species and other annotations in the image DB,0
convert the species category to explicit string name,0
there may be other fields in the annotation object,0
these fields should already be gotten from the image object,0
%% integrate the bbox DB,0
add any images that are not in the image DB,0
also add any fields in the image object that are not present already,0
add bbox to the annotations field,0
for any newly added images,0
"'bbox_abs': bbox_anno['bbox'],",0
not keeping height and width,0
,0
jb_csv_to_json.py,0
,0
Convert a particular .csv file to CCT format.  Images were not available at,0
"the time I wrote this script, so this is much shorter than other scripts",0
in this folder.,0
,0
%% Constants and environment,0
%% Read source data,0
%% Confirm filename uniqueness (this data set has one label per image),0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
%% Create CCT dictionaries,0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
imageName = imageFilenames[0],0
"As per above, this is convenient and appears to be true; asserting to be safe",0
"Filenames look like ""290716114012001a1116.jpg""",0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
%% Create info struct,0
%% Write output,0
%% Sanity-check,0
%% Imports and constants,0
%% Load data,0
"with open(image_json,'r') as f:",0
data = json.load(f),0
%% Sanity-check data,0
%% Label previews,0
%% Collect images to annotate,0
%% Sort by sequence and frame,0
%% Copy to a folder by GUID,0
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension,0
im = images_to_annotate[0],0
...for each image,0
%% Write out the annotation list,0
,0
"Import a Snapshot Safari project (one project, one season)",0
,0
Before running this script:,0
,0
"* Mount the blob container where the images live, or copy the",0
images to local storage,0
,0
What this script does:,0
,0
* Creates a .json file,0
* Creates zip archives of the season without humans.,0
* Copies animals and humans to separate folders,0
,0
After running this script:,0
,0
* Create or update LILA page,0
* Push zipfile and unzipped images to LILA,0
* Push unzipped humans to wildlifeblobssc,0
* Delete images from UMN uplaod storage,0
,0
"Snapshot Serengeti is handled specially, because we're dealing with bounding",0
boxes too.  See snapshot_serengeti_lila.py.,0
,0
%% Imports,0
From ai4eutils,0
From CameraTraps,0
%% Constants,0
project_name = 'XXX'; season_name = 'S1'; project_friendly_name = 'Snapshot Unknown',0
project_name = 'KRU'; season_name = 'S1'; project_friendly_name = 'Snapshot Kruger',0
project_name = 'CDB'; season_name = 'S1'; project_friendly_name = 'Snapshot Camdeboo',0
project_name = 'MTZ'; season_name = 'S1'; project_friendly_name = 'Snapshot Mountain Zebra',0
project_name = 'ENO'; season_name = 'S1'; project_friendly_name = 'Snapshot Enonkishu',0
project_name = 'KAR'; season_name = 'S1'; project_friendly_name = 'Snapshot Karoo',0
project_name = 'KGA'; season_name = 'S1'; project_friendly_name = 'Snapshot Kgalagadi',0
%% Folder/file creation,0
E.g. KRU_S1,0
E.g. Z:\KRU,0
E.g. Z:\KRU\KRU_S1,0
Contains annotations for each capture event (sequence),0
Maps image IDs to filenames; each line looks like:,0
,0
"KRU_S1#1#1#2,3,KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0004.JPG",0
"Total number of each answer to each question, e.g. total number of times each species was identified",0
,0
Not used here,1
Create output folders,0
Images will be placed in a season-specific folder inside this (the source data includes,0
this in path names),0
%% Load metadata files,0
%% Convert to dictionaries (prep),0
%% Convert to dictionaries (loops),0
"TODO: iterrows() is a terrible way to do this, but this is one of those days",1
"where I want to get this done, not get better at Python.",0
irow = 0; row = image_table.iloc[0],0
"Loaded as an int64, converting to int here",0
...for each row in the image table,0
Make sure image IDs are what we think they are,0
...for each row in the annotation table,0
%% Take a look at categories (just sanity-checking),0
print('\nCategories by species:'),0
pp.pprint(categories_by_species),0
%% Fill in some image fields we didn't have when we created the image table,0
"width, height, corrupt, seq_num_frames, location, datetime",0
Every annotation in this list should have the same sequence ID,0
"Figure out ""seq_num_frames"", which really should be done in a separate lopp;",0
there's no reason to do this redundantly for every image,0
Every image in this sequence should point back to the same equence,0
Every annotation in this list should have the same location,0
Every annotation in this list should have the same datetime,0
Is this image on disk?,0
iImage = 0; im = images[0],0
...for each image,0
"images_processed = pool.map(process_image, images)",0
"images_processed = list(tqdm(pool.imap_unordered(process_image, images), total=len(images)))",0
%% Count missing/corrupted images,0
%% Print distribution of sequence lengths (sanity-check),0
%% Replicate annotations across images,0
iAnn = 0; ann = annotations[iAnn],0
%% See what files are on disk but not annotated,0
%% Sanity-check image and annotation uniqueness,0
%% Minor updates to fields,0
%% Write .json file,0
%% Create a list of human files,0
ann = annotations[0],0
%% Create public archive and public/private folders,0
im = images[0],0
E.g. KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0001.JPG,0
Copy to private output folder,0
Add to zipfile,0
Possibly start a new archive,0
Copy to public output folder,0
...for each image,0
%% Sanity-check .json file,0
"This will produce some validation errors, because this zipfile doesn't include humans",0
%% Zip up .json and .csv files,0
%% When I skip to this part (using a pre-rendered .json file),0
%%,0
%%,0
ann = annotations[0],0
%% Summary prep for LILA,0
"%% Generate preview, sanity-check labels",0
"viz_options.classes_to_include = ['jackalblackbacked','bustardkori']",0
%% Scrap,0
%% Find annotations for a particular image,0
%% Write a list of missing images,0
,0
save_the_elephants_survey_A.py,0
,0
Convert the .csv file provided for the Save the Elephants Survey A data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
Handle all unstructured fields in the source data as extra fields in the annotations,0
"photo_type really should be an image property, but there are a few conflicts",0
that forced me to handle it as an annotation proprerty,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
Maps relative paths to row indices in input_metadata,0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
Ignore directories,0
%% Make sure the multiple-annotation cases make sense,0
%%,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
Force the empty category to be ID 0,0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image],0
Example filename:,0
,0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG',0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG',0
i_row = row_indices[0],0
There are a small handful of datetime mismatches across annotations,0
for the same image,0
assert im['datetime'] == timestamp,0
Special cases based on the 'photo type' field,0
Various spellings of 'community',0
Have we seen this category before?,0
Create an annotation,0
fieldname = list(mapped_fields.keys())[0],0
...for each row,0
...for each image,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
%% Scrap,0
%% Find unique photo types,0
##,0
,0
timelapse_csv_set_to_json.py,0
,0
Given a directory full of reasonably-consistent Timelapse-exported,0
".csvs, assemble a CCT .json.",0
,0
"Assumes that you have a list of all files in the directory tree, including",0
image and .csv files.,0
,0
##,0
%% Constants and imports,0
Text file with relative paths to all files (images and .csv files),0
"%% Read file list, make a list of all image files and all .csv files",0
"%% Verify column consistency, create a giant array with all rows from all .csv files",0
i_csv = 0; csv_filename = csv_files[0],0
%% Prepare some data structures we'll need for mapping image rows in .csv files to actual image files,0
Enumerate all folders containing image files,0
"In this data set, a site folder looks like:",0
,0
Processed Images\\site_name,0
%% Map .csv files to candidate camera folders,0
fn = valid_csv_files[0],0
"Some site folders appear as ""XXNNNN"", some appear as ""XXNNNN_complete""",0
...for each .csv file,0
%% Map camera folders to candidate image folders,0
%% Main loop over labels (prep),0
Force the empty category to be ID 0,0
Images that are marked empty and also have a species label,0
%% Main loop over labels (loop),0
i_row = 0; row = input_metadata.iloc[i_row],0
"for i_row,row in input_metadata.iterrows():",0
"Usually this is just a single folder name, sometimes it's a full path,",0
which we don't want,0
Check whether this file exists on disk,0
Retrieve image width and height,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each row in the big table of concatenated .csv files,0
%% Fix cases where an image was annotated as 'unlabeled' and as something else,0
This annotation is 'unlabeled',0
Was there another category associated with this image?,0
%% Check for un-annnotated images,0
Enumerate all images,0
list(relative_path_to_image.keys())[0],0
%% Create info struct,0
%% Write output,0
%% Sanity-check the database's integrity,0
%% Render a bunch of images to make sure the labels got carried along correctly,0
,0
auckland_doc_to_json.py,0
,0
Convert Auckland DOC data set to COCO camera traps format.  This was,0
for a training data set where class names were encoded in path names.,0
,0
%% Constants and imports,0
Filenames will be stored in the output .json relative to this base dir,0
%% Enumerate files,0
%% Assemble dictionaries,0
Force the empty category to be ID 0,0
fn = image_files[0]; print(fn),0
Typically y:\Maukahuka_Auckland_Island\1_Training\Winter_Trial_2019\cat\cat\eat\20190903_IDdY_34_E3_tmp_201908240051.JPG,0
"This data set has two top-level folders, ""1_Training"" (which has class names encoded",0
"in paths) and ""2_Testing"" (which has no class information).",0
...for each image,0
%% Write output .json,0
%% Write train/test .jsons,0
%% Validate .json files,0
%% Preview labels,0
checkpoint,0
,0
awc_to_json.py,0
,0
Convert a particular .csv file to CCT format.,0
,0
%% Constants and environment,0
%% Read source data,0
%% Main loop over labels,0
Force the empty category to be ID 0,0
iRow = 0; row = input_metadata.iloc[iRow],0
"ImageID,FileName,FilePath,SpeciesID,CommonName",0
"This makes an assumption of one annotation per image, which happens to be",0
true in this data set.,0
Retrieve image width and height,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
list(relativePathToImage.keys())[0],0
%% Create info struct,0
%% Write output,0
%% Sanity-check the database's integrity,0
%% Render a bunch of images to make sure the labels got carried along correctly,0
,0
idaho-camera-traps.py,0
,0
Prepare the Idaho Camera Traps dataset for release on LILA.,0
,0
%% Imports and constants,0
Multi-threading for .csv file comparison and image existence validation,0
"We are going to map the original filenames/locations to obfuscated strings, but once",0
"we've done that, we will re-use the mappings every time we run this script.",0
This is the file to which mappings get saved,0
The maximum time (in seconds) between images within which two images are considered the,0
same sequence.,0
"This is a two-column file, where each line is [string in the original metadata],[category name we want to map it to]",0
"The output file, using the original strings",0
"The output file, using obfuscated strings for everything but filenamed",0
"The output file, using obfuscated strings and obfuscated filenames",0
"One time only, I ran MegaDetector on the whole dataset...",0
...then set aside any images that *may* have contained humans that had not already been,0
annotated as such.  Those went in this folder...,0
...and the ones that *actually* had humans (identified via manual review) got,0
copied to this folder...,0
"...which was enumerated to this text file, which is a manually-curated list of",0
images that were flagged as human.,0
Unopinionated .json conversion of the .csv metadata,0
%% List files (images + .csv),0
Ignore .csv files in folders with multiple .csv files,0
...which would require some extra work to decipher.,0
fn = csv_files[0],0
%% Parse each .csv file into sequences (function),0
csv_file = csv_files[-1],0
os.startfile(csv_file_absolute),0
survey = csv_file.split('\\')[0],0
Sample paths from which we need to derive locations:,0
,0
St.Joe_elk\AM99\Trip 1\100RECNX\TimelapseData.csv,0
Beaverhead_elk\AM34\Trip 1\100RECNX\TimelapseData.csv,0
,0
ClearCreek_mustelids\Winter2015-16\FS-001-P\FS-001-P.csv,0
ClearCreek_mustelids\Summer2015\FS-001\FS-001.csv,0
ClearCreek_mustelids\Summer2016\IDFG-016\IDFG-016.csv,0
,0
I:\idfg-images\ClearCreek_mustelids\Summer2016\IDFG-017b,0
I:\idfg-images\ClearCreek_mustelids\Summer2016\IDFG-017a,0
Load .csv file,0
Validate the opstate column,0
# Create datetimes,0
print('Creating datetimes'),0
i_row = 0; row = df.iloc[i_row],0
Make sure data are sorted chronologically,0
,0
"In odd circumstances, they are not... so sort them first, but warn",0
Debugging when I was trying to see what was up with the unsorted dates,0
# Parse into sequences,0
print('Creating sequences'),0
i_row = 0; row = df.iloc[i_row],0
Start a new sequence if:,0
,0
* This image has no timestamp,0
* This image has a frame number of zero,0
* We have no previous image timestamp,0
,0
Start a new sequence if necessary,0
...for each row,0
# Parse labels for each sequence,0
sequence_id = location_sequences[0],0
Row indices in a sequence should be adjacent,0
sequence_df = df[df['seq_id']==sequence_id],0
# Determine what's present,0
Be conservative; assume humans are present in all maintenance images,0
The presence columns are *almost* always identical for all images in a sequence,0
assert single_presence_value,0
"print('Warning: presence value for {} is inconsistent for {}'.format(presence_column,sequence_id))",0
...for each presence column,0
Tally up the standard (survey) species,0
"If no presence columns are marked, all counts should be zero",0
count_column = count_columns[0],0
Occasionally a count gets entered (correctly) without the presence column being marked,0
"assert len(values) == 1 and values[0] == 0, 'Non-zero counts with no presence columns marked for sequence {}'.format(sequence_id)",0
"Handle this by virtually checking the ""right"" box",0
Make sure we found a match,0
Handle 'other' tags,0
column_name = otherpresent_columns[0],0
print('Found non-survey counted species column: {}'.format(column_name)),0
...for each non-empty presence column,0
"Very rarely, the ""otherpresent"" column is checked, but no more detail is available",0
...handling non-survey species,0
Build the sequence data,0
i_row = 0; row = sequence_df.iloc[i_row],0
Only one folder used a single .csv file for two subfolders,0
...for each sequence,0
...def csv_to_sequences(),0
%% Parse each .csv file into sequences (loop),0
%%,0
%%,0
i_file = -1; csv_file = csv_files[i_file],0
%% Save sequence data,0
%% Load sequence data,0
%%,0
%% Validate file mapping (based on the existing enumeration),0
sequences = sequences_by_file[0],0
sequence = sequences[0],0
"Actually, one folder has relative paths",0
assert '\\' not in image_file_relative and '/' not in image_file_relative,0
os.startfile(csv_folder),0
assert os.path.isfile(image_file_absolute),0
found_file = os.path.isfile(image_file_absolute),0
...for each image,0
...for each sequence,0
...for each .csv file,0
%% Load manual category mappings,0
The second column is blank when the first column already represents the category name,0
%% Convert to CCT .json (original strings),0
Force the empty category to be ID 0,0
For each .csv file...,0
,0
sequences = sequences_by_file[0],0
For each sequence...,0
,0
sequence = sequences[0],0
Find categories for this image,0
"When 'unknown' is used in combination with another label, use that",0
"label; the ""unknown"" here doesn't mean ""another unknown species"", it means",0
there is some other unknown property about the main species.,0
category_name_string = species_present[0],0
"This piece of text had a lot of complicated syntax in it, and it would have",0
been too complicated to handle in a general way,0
print('Ignoring category {}'.format(category_name_string)),0
Don't process redundant labels,0
category_name = category_names[0],0
If we've seen this category before...,0
If this is a new category...,0
print('Adding new category for {}'.format(category_name)),0
...for each category (inner),0
...for each category (outer),0
...if we do/don't have species in this sequence,0
"We should have at least one category assigned (which may be ""empty"" or ""unknown"")",0
assert len(sequence_category_ids) > 0,0
Was any image in this sequence manually flagged as human?,0
print('Flagging sequence {} as human based on manual review'.format(sequence_id)),0
For each image in this sequence...,0
,0
i_image = 0; im = images[i_image],0
Create annotations for this image,0
...for each image in this sequence,0
...for each sequence,0
...for each .csv file,0
Verify that all images have annotations,0
ann = ict_data['annotations'][0],0
For debugging only,0
%% Create output (original strings),0
%% Validate .json file,0
%% Preview labels,0
%% Look for humans that were found by MegaDetector that haven't already been identified as human,0
This whole step only needed to get run once,0
%%,0
Load MD results,0
Get a list of filenames that MD tagged as human,0
im = md_results['images'][0],0
...for each detection,0
...for each image,0
Map images to annotations in ICT,0
ann = ict_data['annotations'][0],0
For every image,0
im = ict_data['images'][0],0
Does this image already have a human annotation?,0
...for each annotation,0
...for each image,0
%% Copy images for review to a new folder,0
fn = missing_human_images[0],0
%% Manual step...,0
Copy any images from that list that have humans in them to...,0
%% Create a list of the images we just manually flagged,0
fn = human_tagged_filenames[0],0
E.g. '0000_Beaverhead_elk~AM174~Trip 1~100RECNX~IMG_1397.JPG',0
"%% Translate location, image, sequence IDs",0
Load mappings if available,0
Generate mappings,0
If we've seen this location before...,0
Otherwise assign a string-formatted int as the ID,0
If we've seen this sequence before...,0
Otherwise assign a string-formatted int as the ID,0
Assign an image ID,0
...for each image,0
Assign annotation mappings,0
Save mappings,0
"Back this file up, lest we should accidentally re-run this script with force_generate_mappings = True",0
and overwrite the mappings we used.,0
...if we are/aren't re-generating mappings,0
%% Apply mappings,0
"%% Write new dictionaries (modified strings, original files)",0
"%% Validate .json file (modified strings, original files)",0
%% Preview labels (original files),0
"viz_options.classes_to_exclude = ['empty','deer','elk']",0
viz_options.classes_to_include = ['bobcat'],0
%% Copy images to final output folder (prep),0
ann = d['annotations'][0],0
Is this a public or private image?,0
Generate absolute path,0
Copy to output,0
Update the filename reference,0
...def process_image(im),0
%% Copy images to final output folder (execution),0
For each image,0
im = images[0],0
Write output .json,0
%% Make sure the right number of images got there,0
%% Validate .json file (final filenames),0
%% Preview labels (final filenames),0
"viz_options.classes_to_exclude = ['empty','deer','elk']",0
viz_options.classes_to_include = ['horse'],0
viz_options.classes_to_include = [viz_options.multiple_categories_tag],0
"viz_options.classes_to_include = ['human','vehicle','domestic dog']",0
%% Create zipfiles,0
%% List public files,0
%% Find the size of each file,0
fn = all_public_output_files[0],0
%% Split into chunks of approximately-equal size,0
...for each file,0
%% Create a zipfile for each chunk,0
...for each filename,0
with ZipFile(),0
...def create_zipfile(),0
i_file_list = 0; file_list = file_lists[i_file_list],0
,0
wellington_to_json.py,0
,0
Convert the .csv file provided for the Wellington data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
%% Read source data,0
"Filenames were provided as *.jpg, but images were *.JPG, converting here",0
"%% Map filenames to rows, verify image existence",0
"Takes ~30 seconds, since it's checking the existence of ~270k images",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
"I didn't expect this to be true a priori, but it appears to be true, and",0
it saves us the trouble of checking consistency across multiple occurrences,0
of an image.,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
imageName = imageFilenames[0],0
"As per above, this is convenient and appears to be true; asserting to be safe",0
"Filenames look like ""290716114012001a1116.jpg""",0
This gets imported as an int64,0
"These appear as ""image1"", ""image2"", etc.",0
"In the form ""001a""",0
Can be in the form '111' or 's46',0
"In the form ""7/29/2016 11:40""",0
Check image height and width,0
"Use 'empty', to be consistent with other data on lila",0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
,0
mcgill_to_json.py,0
,0
Convert the .csv file provided for the McGill test data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
Create an additional column for concatenated filenames,0
Maps relative filenames to rows,0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
row = input_metadata.iloc[0],0
"I didn't expect this to be true a priori, but it appears to be true, and",0
it saves us the trouble of checking consistency across multiple occurrences,0
of an image.,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
row = input_metadata.iloc[0],0
"Filenames look like ""290716114012001a1116.jpg""",0
"In the form ""001a""",0
Can be in the form '111' or 's46',0
"In the form ""7/29/2016 11:40""",0
Check image height and width,0
NaN is the only thing we should see that's not a string,0
NaN is the only thing we should see that's not a string,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
,0
bellevue_to_json.py,0
,0
"""Bellevue Camera Traps"" is the rather unremarkable camera trap data set",0
used by one of the repo's maintainers for testing.  It's organized as:,0
,0
approximate_date/[loose_camera_specifier/]/species,0
,0
E.g.:,0
,0
"""2018.03.30\coyote\DSCF0091.JPG""",0
"""2018.07.18\oldcam\empty\DSCF0001.JPG""",0
,0
%% Constants and imports,0
from the ai4eutils repo,0
Filenames will be stored in the output .json relative to this base dir,0
%% Exif functions,0
"%% Enumerate files, create image/annotation/category info",0
Force the empty category to be ID 0,0
Keep track of unique camera folders,0
Each element will be a dictionary with fields:,0
,0
"relative_path, width, height, datetime",0
fname = image_files[0],0
Corrupt or not an image,0
Store file info,0
E.g. 2018.03.30/coyote/DSCF0091.JPG,0
...for each image file,0
%% Synthesize sequence information,0
Sort images by time within each folder,0
camera_path = camera_folders[0],0
previous_datetime = sorted_images_this_camera[0]['datetime'],0
im = sorted_images_this_camera[1],0
Start a new sequence if necessary,0
...for each image in this camera,0
...for each camera,0
Fill in seq_num_frames,0
%% A little cleanup,0
%% Write output .json,0
%% Sanity-check data,0
%% Label previews,0
,0
auckland_doc_test_to_json.py,0
,0
Convert Auckland DOC data set to COCO camera traps format.  This was,0
for a testing data set where a .csv file was provided with class,0
information.,0
,0
%% Constants and imports,0
Filenames will be stored in the output .json relative to this base dir,0
%% Enumerate files,0
%% Create unique identifier for each image,0
The ground truth doesn't have full paths in it; create unique identifiers for each image,0
based on the camera name and filename.,0
,0
We store file identifiers as cameraname_filename.,0
relative_path = relative_image_paths[0],0
Example relative paths,0
,0
Summer_Trial_2019/A1_1_42_SD114_20190210/AucklandIsland_A1_1_42_SD114_20190210_01300001.jpg,0
Winter_Trial_2019/Installation/10_F4/10_F4_tmp_201908210001.JPG,0
Find the camera name,0
"E..g. ""A1_1_42_SD114_20190210"" in the above example",0
For camera tokens like C1_5_D_190207,0
%% Load input data,0
"The spreadsheet has a space after ""Camera""",0
%% Assemble dictionaries,0
Force the empty category to be ID 0,0
"array([nan, 'Blackbird', 'Bellbird', 'Tomtit', 'Song thrush', 'Pippit',",0
"'Pippet', '?', 'Dunnock', 'Song thursh', 'Kakariki', 'Tui', ' ',",0
"'Silvereye', 'NZ Pipit', 'Blackbird and Dunnock', 'Unknown',",0
"'Pipit', 'Songthrush'], dtype=object)",0
i_row = 0; row = input_metadata.iloc[i_row],0
E.g.: AucklandIsland_A1_1_42_SD114_20190210_01300009.jpg,0
"We have multiple files matching this identifier, can we uniquely resolve this",0
to one of those files based on the camera ID?,0
"create_annotation(image_id,category_name,count)",0
"'SD_Change', 'Cat', 'Mouse', 'Bird', 'Bird_ID', 'False_trig', 'Unknown',",0
"'Human', 'Collared_cat', 'Cat_ID', 'Pig', 'Sea_lion', 'Open_adjusted',",0
"'Penguin', 'Dog', 'Comments', 'Unnamed: 22']",0
Each of these categories is handled a little differently...,0
These are straightforward,0
...for each image,0
%% Summarize errors,0
%% Write output .json,0
%% Validate .json file,0
%% Preview labels,0
%% Precision-recall analysis,0
,0
plot_wni_giraffes.py,0
,0
Plot keypoints on a random sample of images from the wni-giraffes data set.,0
,0
%% Constants and imports,0
%% Load and select data,0
%% Support functions,0
https://stackoverflow.com/questions/32504246/draw-ellipse-in-python-pil-with-line-thickness,0
Use a single channel image (mode='L') as mask.,0
The size of the mask can be increased relative to the imput image,0
to get smoother looking results.,0
draw outer shape in white (color) and inner shape in black (transparent),0
downsample the mask using PIL.Image.LANCZOS,0
(a high-quality downsampling filter).,0
paste outline color to input image through the mask,0
%% Plot some images,0
ann = annotations_to_plot[0],0
i_tool = 0; tool_name = short_tool_names[i_tool],0
Don't plot tools that don't have a consensus annotation,0
...for each tool,0
...for each annotation,0
,0
idfg_iwildcam_lila_prep.py,0
,0
Adding class labels (from the private test .csv) to the iWildCam 2019 IDFG,0
"test set, in preparation for release on LILA.",0
,0
This version works with the public iWildCam release images.,0
,0
"%% ############ Take one, from iWildCam .json files ############",0
%% Imports and constants,0
%% Read input files,0
Remove the header line,0
%% Parse annotations,0
Lines look like:,0
,0
"b005e5b2-2c0b-11e9-bcad-06f1011196c4,1,Private",0
%% Minor cleanup re: images,0
%% Create annotations,0
%% Prepare info,0
%% Minor adjustments to categories,0
Remove unused categories,0
Name adjustments,0
%% Create output,0
%% Write output,0
%% Validate .json file,0
%% Preview labels,0
viz_options.classes_to_exclude = ['test'],0
"%% ############ Take two, from pre-iWildCam .json files created from IDFG .csv files ############",0
%% Imports and constants,0
%% One-time line break addition,0
%% Read input files,0
%% Prepare info,0
%% Minor adjustments to categories,0
%% Minor adjustments to annotations,0
%% Create output,0
%% Write output,0
%% Validate .json file,0
%% Preview labels,0
viz_options.classes_to_exclude = ['test'],0
,0
missouri_to_json.py,0
,0
Create .json files from the original source files for the Missouri Camera Traps,0
data set.  Metadata was provided here in two formats:,0
,0
"1) In one subset of the data, folder names indicated species names.  In Set 1,",0
there are no empty sequences.  Set 1 has a metadata file to indicate image-level,0
bounding boxes.,0
,0
2) A subset of the data (overlapping with (1)) was annotated with bounding,0
"boxes, specified in a whitespace-delimited text file.  In set 2, there are",0
"some sequences omitted from the metadata file, which implied emptiness.",0
,0
"In the end, set 2 labels were not reliable enough to publish, so LILA includes only set 1.",0
,0
%% Constants and imports,0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Filenames will be stored in the output .json relative to this base dir,0
This will be a list of filenames that need re-annotation due to redundant boxes,0
"%% Enumerate files, read image sizes (both sets)",0
"Takes a few minutes, since we're reading image sizes.",0
Each element will be a list of relative path/full path/width/height,0
"Only process leaf nodes corresponding to sequences, which look like:",0
,0
E:\wildlife_data\missouri_camera_traps\Set1\1.02-Agouti\SEQ75583,0
E:\wildlife_data\missouri_camera_traps\Set2\p1d101,0
,0
assert len(files) <= 2,0
Read the image,0
Not an image...,0
Store file info,0
"...if we didn't hit the max file limit, keep going",0
...for each file,0
%% Add sequence lengths (both sets),0
%% Load the set 1 metadata file,0
"List of lists, length varies according to number of bounding boxes",0
,0
Preserves original ordering,0
iLine = 0; line = metadataSet1Lines[0],0
"Lines should be filename, number of bounding boxes, boxes (four values per box)",0
Make sure we have image info for this image,0
%% Print missing files from Set 1 metadata,0
Manual changes I made to the metadata file:,0
,0
'IMG' --> 'IMG_',0
Red_Brocket_Deer --> Red_Deer,0
European-Hare --> European_Hare,0
Wood-Mouse --> Wood_Mouse,0
Coiban-Agouti --> Coiban_Agouti,0
%% Load the set 2 metadata file,0
"This metadata file contains most (but not all) images, and a class label (person/animal/empty)",0
"for each, plus bounding boxes.",0
"List of lists, length varies according to number of bounding boxes",0
,0
Preserves original ordering,0
"Create class IDs for each *sequence*, which we'll use to attach classes to",0
images for which we don't have metadata,0
,0
This only contains mappings for sequences that appear in the metadata.,0
iLine = 0; line = metadataSet2Lines[0],0
"Lines should be filename, number of bounding boxes, labeled boxes (five values per box)",0
,0
Empty images look like filename\t0\t0,0
E.g. 'Set2\\p1d101\\p1d101s100i10.JPG',0
Make sure we don't have mixed classes within an image,0
"Figure out what class this *sequence* is, so we know how to handle unlabeled",0
images from this sequence,0
Can't un-do a mixed sequence,0
Previously-empty sequences get the image class label,0
"If the sequence has a non-empty class, possibly change it",0
Make sure we have image info for this image,0
...for each line in the set 2 metadata file,0
%% What Set 2 images do I not have metadata for?,0
These are *mostly* empty images,0
iImage = 0; imageID = set2ImageIDs[iImage],0
%% Create categories and annotations for set 1,0
"Though we have no empty sequences, we do have empty images in this set",0
For each image,0
,0
iImage = 0; imageID = set1ImageIDs[iImage],0
E.g. Set1\\1.80-Coiban_Agouti\\SEQ83155\\SEQ83155_IMG_0010.JPG,0
Find the species name,0
This image may still be empty...,0
category['count'] = category['count'] + 1,0
"If we have bounding boxes, create image-level annotations",0
"filename, number of bounding boxes, boxes (four values per box)",0
"Make sure the relative filename matches, allowing for the fact that",0
some of the filenames in the metadata aren't quite right,0
This image is non-empty,0
"Some redundant bounding boxes crept in, don't add them twice",0
Check this bbox against previous bboxes,0
,0
Inefficient?  Yes.  In an important way?  No.,0
"Bounding box values are in absolute coordinates, with the origin",0
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1].",0
,0
"Convert to floats and to x/y/w/h, as per CCT standard",0
...for each box,0
if we do/don't have boxes for this image,0
Else create a sequence-level annotation,0
...for each image,0
%% Write out the list of images with redundant boxes,0
%% Create categories and annotations for set 2,0
For each image,0
,0
iImage = 0; imageID = set2ImageIDs[iImage],0
E.g. 'Set2\\p1d100\\p1d100s10i1.JPG',0
"Find the sequence ID, sanity check filename against what we stored",0
"If we have bounding boxes or an explicit empty label, create image-level annotations",0
"filename, number of bounding boxes, labeled boxes (five values per box)",0
"Make sure the relative filename matches, allowing for the fact that",0
some of the filenames in the metadata aren't quite right,0
"Bounding box values are in absolute coordinates, with the origin",0
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1].",0
,0
Convert to floats and to x/y/w/h,0
...for each box,0
...if we do/don't have boxes for this image,0
Else create a sequence-level annotation,0
...for each image,0
"%% The 'count' field isn't really meaningful, delete it",0
"It's really the count of image-level annotations, not total images assigned to a class",0
%% Write output .json files,0
%% Sanity-check final set 1 .json file,0
"python sanity_check_json_db.py --bCheckImageSizes --baseDir ""E:\wildlife_data\missouri_camera_traps"" ""E:\wildlife_data\missouri_camera_traps\missouri_camera_traps_set1.json""",0
%% Generate previews,0
,0
filenames_to_json.py,0
,0
Take a directory of images in which species labels are encoded by folder,0
"names, and produces a COCO-style .json file",0
,0
%% Constants and imports,0
from the ai4eutils repo,0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Filenames will be stored in the output .json relative to this base dir,0
"rawClassListFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_classes.csv')",0
"classMappingsFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_class_mapping.csv')",0
"%% Enumerate files, read image sizes",0
Each element will be a list of relative path/full path/width/height,0
Read the image,0
Corrupt or not an image,0
Store file info,0
Write to output file,0
...for each image file,0
...csv file output,0
...if the file list is/isn't available,0
%% Enumerate classes,0
Maps classes to counts,0
We like 'empty' to be class 0,0
%% Assemble dictionaries,0
...for each category,0
Each element is a list of relative path/full path/width/height/className,0
...for each image,0
%% External class mapping,0
%% Write raw class table,0
cat = categories[0],0
%% Read the mapped class table,0
"id, source, count, target",0
"%% Make classMappings contain *all* classes, not just remapped classes",0
cat = categories[0],0
%% Create new class list,0
"Start at 1, explicitly assign 0 to ""empty""",0
One-off issue with character encoding,0
%% Re-map annotations,0
ann = annotations[0],0
%% Write output .json,0
%% Utilities,0
%%,0
Find images with a particular tag,0
%% Randomly sample annotations,0
,0
umn_to_json.py,0
,0
Prepare images and metadata for the Orinoqua Camera Traps dataset.,0
,0
%% Imports and constants,0
"For two deployments, we're only processing imagse in the ""detections"" subfolder",0
%% Enumerate deployment folders,0
%% Load ground truth,0
i_row = 0; row = ground_truth_df.iloc[i_row],0
%% Create relative paths for ground truth data,0
"Some deployment folders have no subfolders, e.g. this is a valid file name:",0
,0
M00/01010132.JPG,0
,0
"But some deployment folders have subfolders, e.g. this is also a valid file name:",0
,0
N17/100EK113/07160020.JPG,0
,0
"So we can't find files by just concatenating folder and file names, we have to enumerate and explicitly",0
"map what will appear in the ground truth as ""folder/filename"" to complete relative paths.",0
deployment_name = list(deployment_folders)[0],0
Enumerate all files in this folder,0
"print('Enumerated {} files for deployment {}'.format(len(files),deployment_name))",0
filename = files[100],0
...for each file in this deployment,0
...for each deployment,0
%% Add relative paths to our ground truth table,0
i_row = 0; row = ground_truth_df.iloc[i_row],0
"row['filename'] looks like, e.g. A01/01080001.JPG.  This is not actually a path, it's",0
"just the deployment ID and the image name, separated by a slash.",0
Find the relative path for this image,0
Make sure this image file exists,0
...for each row in the ground truth table,0
%% Take everything out of Pandas,0
%% Convert string timestamps to Python datetimes,0
im = ground_truth_dicts[0],0
"Filenames look like, e.g., N36/100EK113/06040726.JPG",0
%% Synthesize sequence information,0
Sort images by time within each location,0
i_location=0; location = locations[i_location],0
previous_datetime = sorted_images_this_location[0]['datetime'],0
im = sorted_images_this_camera[1],0
"Timestamp for this image, may be None",0
Start a new sequence if:,0
,0
* This image has no timestamp,0
* This iamge has a frame number of zero,0
* We have no previous image timestamp,0
,0
Start a new sequence if necessary,0
...for each image in this location,0
...for each location,0
%% Create category dict and category IDs,0
im = images[0],0
%% Imports and constants (.json generation),0
%% Count frames in each sequence,0
%% Double check images with multiple annotations,0
im = images[0],0
%% Assemble dictionaries,0
Force the empty category to be ID 0,0
input_im = images[0],0
...for each image,0
%% Write output .json,0
%% Validate .json file,0
%% Map relative paths to annotation categories,0
ann = data['annotations'][0],0
%% Copy images to output,0
EXCLUDE HUMAN AND MISSING,0
im = data['images'][0],0
im = images[0],0
%% Preview labels,0
viz_options.classes_to_exclude = ['test'],0
,0
nacti_fieldname_adjustments.py,0
,0
"NACTI metadata was posted with ""filename"" in images instead of ""file_name"", and",0
"used string (rather than int) category IDs (in categories, but not in annotations).",0
,0
This script fixes those issues and rev's the version number.,0
,0
%% Constants and environment,0
%% Read .json file,0
"%% Rev version number, update field names and types",0
%% Write json file,0
,0
cct_field_adjustments.py,0
,0
CCT metadata was posted with int locations instead of strings.,0
,0
This script fixes those issues and rev's the version number.,0
,0
%% Constants and environment,0
%% Read .json file,0
"%% Rev version number, update field names and types",0
%% Write json file,0
%% Check output data file,0
,0
carrizo_shrubfree_2018.py,0
,0
Convert the .csv file provided for the Carrizo Mojave data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
%% Read source data,0
Original .csv file had superfluous spaces in column names,0
"%% Map filenames to rows, verify image existence",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
58 missing files (of 17652),0
%% Check for images that aren't included in the metadata file,0
3012 of 20606 files are not in metadata,0
%% Create CCT dictionaries,0
Map categories to integer IDs,0
,0
The category '0' is reserved for 'empty',0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
Each filename should just match one row,0
Don't include images that don't exist on disk,0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
,0
snapshot_serengeti_lila.py,0
,0
Create zipfiles of Snapshot Serengeti S1-S11.,0
,0
"Create a metadata file for S1-S10, plus separate metadata files",0
"for S1-S11.  At the time this code was written, S11 was under embargo.",0
,0
Create zip archives of each season without humans.,0
,0
Create a human zip archive.,0
,0
%% Constants and imports,0
import sys; sys.path.append(r'c:\git\ai4eutils'),0
import sys; sys.path.append(r'c:\git\cameratraps'),0
assert(os.path.isdir(metadata_base)),0
"There are two redundant categories, and we re-map ""blank"" to ""empty"" as per CCT convention",0
"%% Load metadata files, concatenate into a single table",0
iSeason = 1,0
%% Convert to dictionaries (prep),0
%% Convert to dictionaries (loops),0
"TODO: iterrows() is a terrible way to do this, but this is one of those days",1
"where I want to get this done, not get better at Python.",0
irow = 0; row = image_table.iloc[0],0
"Loaded as an int64, converting to int here",0
...for each row in the image table,0
Make sure image IDs are what we think they are,0
...for each row in the annotation table,0
%% Load previously-saved dictionaries when re-starting mid-script,0
%%,0
%% Take a look at categories (just sanity-checking),0
%%,0
%% Fill in some image fields we didn't have when we created the image table,0
"width, height, corrupt, seq_num_frames, location, datetime",0
Every annotation in this list should have the same sequence ID,0
"Figure out ""seq_num_frames"", which really should be done in a separate lopp;",0
there's no reason to do this redundantly for every image,0
Every image in this sequence should point back to the same equence,0
Every annotation in this list should have the same location,0
Every annotation in this list should have the same datetime,0
Is this image on disk?,0
iImage = 0; im = images[0],0
...for each image,0
"images_processed = pool.map(process_image, images)",0
"images_processed = list(tqdm(pool.imap_unordered(process_image, images), total=len(images)))",0
%%,0
%% Count missing/corrupted images,0
%% Print distribution of sequence lengths (sanity-check),0
%% Replicate annotations across images,0
iAnn = 0; ann = annotations[iAnn],0
%% See what files are on disk but not annotated (~15 mins),0
247370 files not in the database (of 7425810),0
%% Load old image database,0
%% Look for old images not in the new DB and vice-versa,0
"At the time this was written, ""old"" was S1-S6",0
old_im = cct_old['images'][0],0
new_im = images[0],0
4 old images not in new db,0
12 new images not in old db,0
%% Save our work,0
%% Load our work,0
%%,0
%% Examine size mismatches,0
i_mismatch = -1; old_im = size_mismatches[i_mismatch],0
%% Sanity-check image and annotation uniqueness,0
"%% Split data by seasons, create master list for public seasons",0
ann = annotations[0],0
%% Minor updates to fields,0
"%% Write master .json out for S1-10, write individual season .jsons (including S11)",0
"Loop over all seasons, plus one iteration for the ""all public data"" iteration, and",0
"one for the ""all data"" iteration",0
%% Find categories that only exist in S11,0
List of categories in each season,0
Category 55 (fire) only in S11,0
Category 56 (hyenabrown) only in S11,0
Category 57 (wilddog) only in S11,0
Category 58 (kudu) only in S11,0
Category 59 (pangolin) only in S11,0
Category 60 (lioncub) only in S11,0
%% Prepare season-specific .csv files,0
iSeason = 1,0
%% Create a list of human files,0
ann = annotations[0],0
%% Save our work,0
%% Load our work,0
%%,0
"%% Create archives (human, per-season) (prep)",0
im = images[0],0
im = images[0],0
Don't include humans,0
Only include files from this season,0
Possibly start a new archive,0
...for each image,0
i_season = 0,0
"for i_season in range(0,nSeasons):",0
create_season_archive(i_season),0
%% Create archives (loop),0
pool = ThreadPool(nSeasons+1),0
"n_images = pool.map(create_archive, range(-1,nSeasons))",0
"seasons_to_zip = range(-1,nSeasons)",0
...for each season,0
%% Sanity-check .json files,0
%logstart -o r'E:\snapshot_temp\python.txt',0
%% Zip up .json and .csv files,0
pool = ThreadPool(len(files_to_zip)),0
"pool.map(zip_single_file, files_to_zip)",0
%% Super-sanity-check that S11 info isn't leaking,0
im = data_public['images'][0],0
ann = data_public['annotations'][0],0
iRow = 0; row = annotation_df.iloc[iRow],0
iRow = 0; row = image_df.iloc[iRow],0
%% Create bounding box archive,0
i_image = 0; im = data['images'][0],0
i_box = 0; boxann = bbox_data['annotations'][0],0
%% Sanity-check a few files to make sure bounding boxes are still sensible,0
import sys; sys.path.append(r'C:\git\CameraTraps'),0
%% Check categories,0
%% Summary prep for LILA,0
,0
wi_to_json,0
,0
Prepares CCT-formatted metadata based on a Wildlife Insights data export.,0
,0
"Mostly assumes you have the images also, for validation/QA.",0
,0
%% Imports and constants,0
"Because WI filenames are GUIDs, it's not practical to page through sequences in an",0
image viewer.  So we're going to (optionally) create a copy of the data set where,0
images are ordered.,0
%% Load ground truth,0
%% Take everything out of Pandas,0
%% Synthesize common names when they're not available,0
"Blank rows should always have ""Blank"" as the common name",0
"print('Warning: missing common name for row {} ({})'.format(i_row,row['filename']))",0
%% Convert string timestamps to Python datetimes,0
im = ground_truth_dicts[0],0
"The field called ""location"" in the WI .csv file is a URL, we want to reclaim",0
"the ""location"" keyword for CCT output",0
"Filenames look like, e.g., N36/100EK113/06040726.JPG",0
%% Synthesize sequence information,0
Sort images by time within each location,0
i_location=0; location = locations[i_location],0
previous_datetime = sorted_images_this_location[0]['datetime'],0
im = sorted_images_this_camera[1],0
"Timestamp for this image, may be None",0
Start a new sequence if:,0
,0
* This image has no timestamp,0
* This iamge has a frame number of zero,0
* We have no previous image timestamp,0
,0
Start a new sequence if necessary,0
...for each image in this location,0
...for each location,0
%% Create category dict and category IDs,0
im = images[0],0
%% Count frames in each sequence,0
%% Build relative paths,0
im = images[0],0
Sample URL:,0
,0
gs://project-asfasdfd/deployment/21444549/asdfasdfd-616a-4d10-a921-45ac456c568a.jpg',0
%% Double check images with multiple annotations,0
im = images[0],0
%% Assemble dictionaries,0
Force the empty category to be ID 0,0
input_im = images[0],0
"print('Warning: image ID {} ({}) has multiple annotations'.format(im['id'],im['id'].replace('_','/')))",0
...for each image,0
%% Write output .json,0
%% Validate .json file,0
%% Preview labels,0
"open_file(os.path.join(image_base,'2100703/1141a545-88d2-498b-a684-7431f7aeb324.jpg'))",0
%%,0
%% Create ordered dataset,0
"Because WI filenames are GUIDs, it's not practical to page through sequences in an",0
image viewer.  So we're going to create a copy of the data set where images are,0
ordered.,0
im = images_out[0]; im,0
%% Create ordered .json,0
%% Copy files to their new locations,0
im = ordered_images[0],0
im = data_ordered['images'][0],0
%% Preview labels in the ordered dataset,0
"open_file(os.path.join(image_base,'2100703/1141a545-88d2-498b-a684-7431f7aeb324.jpg'))",0
%% Open an ordered filename from the unordered filename,0
,0
rspb_to_json.py,0
,0
Convert the .csv file provided for the RSPB data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
[location] is an obfuscation,0
%% Create info struct,0
%% Read source data,0
metadataTable.columns.values,0
,0
"array(['Project', 'inDir', 'FileName', 'Station', 'Camera',",0
"'StationCameraFileName', 'DateTimeOriginal', 'DateReadable',",0
"'outDir', 'filename_new', 'fileExistsAlready', 'CopyStatus',",0
"'Species'], dtype=object)",0
We'll populate these later,0
keys should be lowercase,0
"%% Enumerate images, confirm filename uniqueness",0
"%% Update metadata filenames to include site and camera folders, check existence",0
,0
Takes ~1min,0
iRow = 0; row = metadataTable.iloc[iRow],0
There's a bug in the metadata; the 'camera' column isn't correct.,0
camera = row['Camera'],0
"These appear as, e.g., '3.22e12'",0
camera = str(int(float(camera))),0
Let's pull this out of the file name instead,0
,0
Filenames look like one of the following:,0
,0
A1__03224850850507__2015-11-28__10-45-04(1).JPG,0
Bayama2PH__C05__NA(NA).JPG,0
assert(os.path.isfile(fullPath)),0
metadataTable.iloc[iRow] = row,0
Re-assemble into an updated table,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
Write to a text file,0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
iRow = 0; row = metadataTable.iloc[iRow],0
A1__03224850850507__2015-11-28__10-45-04(1).JPG,0
'A1\\03224850850507\\A1__03224850850507__2015-11-28__10-45-04(1).JPG',0
Not currently populated,0
"Often -1, sometimes a semi-meaningful int",0
A1,0
03224850850507,0
"In variable form, but sometimes '28/11/2015 10:45'",0
Check image height and width,0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Write output,0
%% Check database integrity,0
%% Preview a few images to make sure labels were passed along sensibly,0
%% One-time processing step: copy images to a flat directory for annotation,0
%%,0
,0
pc_to_json.py,0
,0
Convert a particular collection of .csv files to CCT format.,0
,0
%% Constants and environment,0
%% Read and concatenate source data,0
List files,0
"List of dataframes, one per .csv file; we'll concatenate later",0
i_file = 87; fn = input_files[i_file],0
Concatenate into a giant data frame,0
%% List files,0
%% Main loop over labels (prep),0
Force the empty category to be ID 0,0
%% Main loop over labels (loop),0
iRow = 0; row = input_metadata.iloc[iRow],0
"ImageID,FileName,FilePath,SpeciesID,CommonName",0
assert os.path.isfile(full_path),0
Retrieve image width and height,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
%% See what's up with missing files,0
s = list(image_relative_paths)[0],0
s = missing_files[0],0
%% Check for images that aren't included in the metadata file,0
%% Sample the database,0
%%,0
Collect the images we want,0
%% Create info struct,0
%% Write output,0
%% Sanity-check the database's integrity,0
%% Render a bunch of images to make sure the labels got carried along correctly,0
options.classes_to_exclude = ['unlabeled'],0
%% Write out a list of files to annotate,0
,0
save_the_elephants_survey_B.py,0
,0
Convert the .csv file provided for the Save the Elephants Survey B data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
input_base = r'/mnt/blobfuse/wildlifeblobssc/ste_2019_08_drop',0
output_base = r'/home/gramener/survey_b',0
Handle all unstructured fields in the source data as extra fields in the annotations,0
"photo_type really should be an image property, but there are a few conflicts",0
that forced me to handle it as an annotation proprerty,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
"%% Map filenames to rows, verify image existence",0
Maps relative paths to row indices in input_metadata,0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
Ignore directories,0
%% Make sure the multiple-annotation cases make sense,0
%%,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
Force the empty category to be ID 0,0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image],0
Example filename:,0
,0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG',0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG',0
i_row = row_indices[0],0
timestamp = row['Date'],0
There are a small handful of datetime mismatches across annotations,0
for the same image,0
assert im['datetime'] == timestamp,0
Special cases based on the 'photo type' field,0
Various spellings of 'community',0
Have we seen this category before?,0
Create an annotation,0
fieldname = list(mapped_fields.keys())[0],0
...for each row,0
...for each image,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
%% Scrap,0
%% Find unique photo types,0
,0
ubc_to_json.py,0
,0
Convert the .csv file provided for the UBC data set to a,0
COCO-camera-traps .json file,0
,0
"Images were provided in eight folders, each of which contained a .csv",0
file with annotations.  Those annotations came in two slightly different,0
"formats, the two formats corresponding to folders starting with ""SC_"" and",0
otherwise.,0
,0
%% Constants and environment,0
Map Excel column names - which vary a little across spreadsheets - to a common set of names,0
%% Enumerate images,0
Load from file if we've already enumerated,0
%% Create CCT dictionaries,0
Force the empty category to be ID 0,0
To simplify debugging of the loop below,0
#%% Create CCT dictionaries (loop),0
#%%,0
Read source data for this folder,0
Rename columns,0
Folder name is the first two characters of the filename,0
,0
Create relative path names from the filename itself,0
Folder name is the camera name,0
,0
Create relative path names from camera name and filename,0
Which of our images are in the spreadsheet?,0
i_row = 0; fn = input_metadata['image_relative_path'][i_row],0
#%% Check for images that aren't included in the metadata file,0
Find all the images in this folder,0
Which of these aren't in the spreadsheet?,0
#%% Create entries in CCT dictionaries,0
Only process images we have on disk,0
"This is redundant, but doing this for clarity, at basically no performance",0
cost since we need to *read* the images below to check validity.,0
i_row = row_indices[0],0
"These generally represent zero-byte images in this data set, don't try",0
to find the very small handful that might be other kinds of failures we,0
might want to keep around.,0
print('Error opening image {}'.format(image_relative_path)),0
If we've seen this category before...,0
...make sure it used the same latin --> common mapping,0
,0
"If the previous instance had no mapping, use the new one.",0
assert common_name == category['common_name'],0
Create an annotation,0
...for each annotation we found for this image,0
...for each image,0
...for each dataset,0
Print all of our species mappings,0
"%% Copy images for which we actually have annotations to a new folder, lowercase everything",0
im = images[0],0
%% Create info struct,0
"%% Convert image IDs to lowercase in annotations, tag as sequence level",0
"While there isn't any sequence information, the nature of false positives",0
"here leads me to believe the images were labeled at the sequence level, so",0
we should trust labels more when positives are verified.  Overall false,0
positive rate looks to be between 1% and 5%.,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
,0
helena_to_cct.py,0
,0
Convert the Helena Detections data set to a COCO-camera-traps .json file,0
,0
%% Constants and environment,0
This is one time process,0
%% Create Filenames and timestamps mapping CSV,0
import pdb;pdb.set_trace(),0
%% To create CCT JSON for RSPB dataset,0
%% Read source data,0
Original Excel file had timestamp in different columns,0
"%% Map filenames to rows, verify image existence",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
%% Skipping this check because one image has multiple species,0
assert len(duplicate_rows) == 0,0
%% Check for images that aren't included in the metadata file,0
%% Create CCT dictionaries,0
Map categories to integer IDs,0
,0
The category '0' is reserved for 'empty',0
For each image,0
Don't include images that don't exist on disk,0
Some filenames will match to multiple rows,0
assert(len(rows) == 1),0
iRow = rows[0],0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
ann['datetime'] = row['datetime'],0
ann['site'] = row['site'],0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
%% Imports and constants,0
from github.com/microsoft/ai4eutils,0
from github.com/microsoft/cameratraps,0
A list of files in the lilablobssc container for this data set,0
The raw detection files provided by NOAA,0
A version of the above with filename columns added,0
%% Read input .csv,0
%% Read list of files,0
%% Convert paths to full paths,0
i_row = 0; row = df.iloc[i_row],0
assert ir_image_path in all_files,0
...for each row,0
%% Write results,0
"%% Load output file, just to be sure",0
%% Render annotations on an image,0
i_image = 2004,0
%% Download the image,0
%% Find all the rows (detections) associated with this image,0
"as l,r,t,b",0
%% Render the detections on the image(s),0
In pixel coordinates,0
In pixel coordinates,0
%% Save images,0
%% Clean up,0
,0
carrizo_trail_cam_2017.py,0
,0
"Convert the .csv files provided for the ""Trail Cam Carrizo"" 2017 data set to",0
a COCO-camera-traps .json file.,0
,0
%% Constants and environment,0
%% Read source data,0
Removing the empty records,0
"%% Map filenames to rows, verify image existence",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
908 missing files (of 60562),0
%% Check for images that aren't included in the metadata file,0
105329 of 164983 files are not in metadata,0
%% Create CCT dictionaries,0
Map categories to integer IDs,0
,0
The category '0' is reserved for 'empty',0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
Each filename should just match one row,0
Don't include images that don't exist on disk,0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
,0
channel_islands_to_cct.py,0
,0
Convert the Channel Islands data set to a COCO-camera-traps .json file,0
,0
"Uses the command-line tool ExifTool (exiftool.org) to pull EXIF tags from images,",0
"because every Python package we tried failed to pull the ""Maker Notes"" field properly.",0
,0
"%% Imports, constants, paths",0
# Imports ##,0
# Constants ##,0
# Paths ##,0
Confirm that exiftool is available,0
"assert which(exiftool_command_name) is not None, 'Could not locate the ExifTool executable'",0
%% Load information from every .json file,0
"Ignore the sample file... actually, first make sure there is a sample file",0
...and now ignore that sample file.,0
json_file = json_files[0],0
ann = annotations[0],0
...for each annotation in this file,0
...for each .json file,0
"%% Confirm URL uniqueness, handle redundant tags",0
Have we already added this image?,0
"One .json file was basically duplicated, but as:",0
,0
Ellie_2016-2017 SC12.json,0
Ellie_2016-2017-SC12.json,0
"If the new image has no output, just leave the old one there",0
"If the old image has no output, and the new one has output, default to the one with output",0
Don't worry about the cases where someone tagged 'fox' and someone tagged 'fox_partial',0
...for each image we've already added,0
...if this URL is/isn't in the list of URLs we've already processed,0
...for each image,0
%% Save progress,0
%%,0
%%,0
%% Download files (functions),0
https://www.quickprogrammingtips.com/python/how-to-download-multiple-files-concurrently-in-python.html,0
"This is returned with a leading slash, remove it",0
%% Download files (execution),0
%% Read required fields from EXIF data (functions),0
"-G means ""Print group name for each tag"", e.g. print:",0
,0
[File]          Bits Per Sample                 : 8,0
,0
...instead of:,0
,0
Bits Per Sample                 : 8,0
"If we don't get any EXIF information, this probably isn't an image",0
line_raw = exif_lines[0],0
"Split on the first occurrence of "":""",0
Typically:,0
,0
"'[MakerNotes]    Sequence                        ', '1 of 3']",0
Not a typo; we are using serial number as a location,0
"If there are multiple timestamps, make sure they're *almost* the same",0
"If there are multiple timestamps, make sure they're *almost* the same",0
...for each line in the exiftool output,0
"This isn't directly related to the lack of maker notes, but it happens that files that are missing",0
maker notes also happen to be missing EXIF date information,0
...process_exif(),0
"This is returned with a leading slash, remove it",0
Ignore non-image files,0
%% Read EXIF data (execution),0
ann = images[0],0
%% Save progress,0
Use default=str to handle datetime objects,0
%%,0
%%,0
"Not deserializing datetimes yet, will do this if I actually need to run this",0
%% Check for EXIF read errors,0
%% Remove junk,0
Ignore non-image files,0
%% Fill in some None values,0
"...so we can sort by datetime later, and let None's be sorted arbitrarily",0
%% Find unique locations,0
%% Synthesize sequence information,0
Sort images by time within each location,0
i_location=0; location = locations[i_location],0
previous_datetime = sorted_images_this_location[0]['datetime'],0
im = sorted_images_this_camera[1],0
"Timestamp for this image, may be None",0
Start a new sequence if:,0
,0
* This image has no timestamp,0
* This image has a frame number of zero,0
* We have no previous image timestamp,0
,0
Start a new sequence if necessary,0
...for each image in this location,0
...for each location,0
%% Count frames in each sequence,0
images_this_sequence = [im for im in images if im['seq_id'] == seq_id],0
"%% Create output filenames for each image, store original filenames",0
i_location = 0; location = locations[i_location],0
i_image = 0; im = sorted_images_this_location[i_image],0
%% Save progress,0
Use default=str to handle datetime objects,0
%%,0
%%,0
%% Copy images to their output files (functions),0
%% Copy images to output files (execution),0
%% Rename the main image list for consistency with other scripts,0
%% Create CCT dictionaries,0
Make sure this is really a box,0
Transform to CCT format,0
Force the empty category to be ID 0,0
i_image = 0; input_im = all_image_info[0],0
"This issue only impacted one image that wasn't a real image, it was just a screenshot",0
"showing ""no images available for this camera""",0
Convert datetime if necessary,0
Process temperature if available,0
Read width and height if necessary,0
I don't know what this field is; confirming that it's always None,0
Process object and bbox,0
os.startfile(output_image_full_path),0
"Zero is hard-coded as the empty category, but check to be safe",0
"I can't figure out the 'index' field, but I'm not losing sleep about it",0
assert input_annotation['index'] == 1+i_ann,0
"Some annotators (but not all) included ""_partial"" when animals were partially obscured",0
"Annotators *mostly* used 'none', but sometimes 'empty'.  'empty' is CCT-correct.",0
If we've seen this category before...,0
If this is a new category...,0
...if this is an empty/non-empty annotation,0
Create an annotation,0
...for each annotation on this image,0
...for each image,0
%% Change *two* annotations on images that I discovered contains a human after running MDv4,0
%% Move human images,0
ann = annotations[0],0
%% Count images by location,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
viz_options.classes_to_exclude = [0],0
,0
ena24_to_json_2017.py,0
,0
Convert the ENA24 data set to a COCO-camera-traps .json file,0
,0
%% Constants and environment,0
Temporary folders for human and non-human images,0
Clean existing output folders/zipfiles,0
%% Support functions,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
Check image height and width,0
"Each row is category, [box coordinates]",0
"If there's just one row, loadtxt reads it as a 1d array; make it a 2d array",0
with one row,0
Each row is a bounding box,0
Have we seen this category before?,0
Create an annotation,0
...for each bounding box,0
"This was here for debugging; nearly every instance is Human+Horse, Human+Vehicle,",0
"or Human+Dog, but there is one Rabbit+Opossium, and a few Deer+Chicken!",0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
%% Create ZIP files for human and non human,0
%% Validate output,0
%% Preview labels,0
%% Imports and constants,0
configurations and paths,0
%% Helper functions,0
"dest_path = copy(source_path, dest_folder)",0
num_workers = multiprocessing.cpu_count(),0
pool = ThreadPool(num_workers),0
"results = pool.starmap(_copy_unzip, zip(sources, itertools.repeat(dest_folder)))",0
,0
print('Waiting for processes to finish...'),0
pool.close(),0
pool.join(),0
sequential,0
%% Command-line driver,0
if the blob container is already mounted on the VM,0
or you can download them using the storage Python SDK,0
store storage account key in environment variable AZ_STORAGE_KEY,0
,0
eMammal_helpers.py,0
,0
Support functions for processing eMammal metadata,0
,0
%% Constants and imports,0
%% Support functions,0
"pad to a total of 3 digits if < 1000, or 4 digits otherwise",0
img_frame is a string from the xml tree,0
"length 4 frame order is returned as is, others are left padded to be 3 digit long",0
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels,0
pad to a total of 4 digits,0
img_frame is a string from the xml tree,0
"length 4 frame order is returned as is, others are left padded to be 3 digit long",0
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels,0
others column,0
summer day hours: 6am - 7pm,0
others day hours: 7am - 6pm,0
,0
make_eMammal_json.py,0
,0
"Produces the COCO-formatted json database for an eMammal dataset, i.e. a",0
"collection of folders, each of which contains a deployment_manifest.xml file.",0
,0
"In this process, each image needs to be loaded to size it.",0
,0
"To add bounding box annotations to the resulting database, use",0
add_annotations_to_eMammal_json.py.,0
,0
%% Constants and imports,0
"Either add the eMammal directory to your path, or run from there",0
os.chdir(r'd:\git\CameraTraps\database_tools\eMammal'),0
import warnings,0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
warnings.filterwarnings('ignore'),0
Should we run the image size retrieval in parallel?,0
%% Support functions,0
%% Main loop (metadata processing; image sizes are retrieved later),0
deployment = folders[0],0
sequence = image_sequences[0],0
get species info for this sequence,0
add each image's info to database,0
img = images[0],0
"some manifests don't have the ImageOrder info, but the info is in the file name",0
full_img_id has no frame info,0
,0
frame number only used in requests to iMerit for ordering,0
...for each image,0
...for each sequence,0
...for each deployment,0
%% Get image sizes,0
"'tasks' is currently a list of 2-tuples, with each entry as [image dictionary,path].",0
,0
"Go through that and copy just the image dictionaries to 'db_images', adding size",0
information to each entry.  Takes a couple hours.,0
opening each image seems too fast for this multi-threaded version to be faster than sequential code.,0
%% Assemble top-level dictionaries,0
%% Write out .json,0
,0
make_full_SS_json.py,0
,0
Create a COCO-camera-traps .json file for Snapshot Serengeti data from,0
the original .csv files provided on Dryad.,0
,0
"This was used for ""version 1.0"" of the public Snapshot Serengeti archive; it's no",0
longer used as of v2.0 (early 2020).  See snapshot_serengeti_lila.py for the updated,0
Snapshot Safari preparation process.,0
,0
%% Imports and constants,0
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays",0
Count the number of images with multiple species,0
"%% Read image .csv file, format into a dictionary mapping images to capture events",0
%% Create CCT-style .json,0
still need image width and height,0
...for each image,0
%% Write output files,0
,0
make_per_season_SS_json.py,0
,0
Create a COCO-camera-traps .json file for each Snapshot Serengeti season from,0
the original .csv files provided on Dryad.,0
,0
"This was used for ""version 1.0"" of the public Snapshot Serengeti archive; it's no",0
longer used as of v2.0 (early 2020).  See snapshot_serengeti_lila.py for the updated,0
Snapshot Safari preparation process.,0
,0
%% Imports and constants,0
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays",0
"%% Read image .csv file, format into a dictionary mapping images to capture events",0
%% Create CCT-style .json,0
still need image width and height,0
...for each image ID,0
%% Write output files,0
...for each season,0
%% Configurations,0
"if the following two files are provided, only the tfrecord generation section will be executed",0
proceed to generate tfrecords if True,0
approximate fraction to split the new entries by,0
categories in the database to include,0
"in addition, any images with a 'group' label will not be included to avoid confusion",0
see 'image_contains_group' in create_tfrecords_format.py,0
%% Input validation,0
%% Convert the COCO Camera Trap format data to another json,0
that aligns with the fields in the resulting tfrecords,0
%% Make train/val/test splits,0
%% Write the tfrecords,0
want the file names of all tfrecords to be of the same format and length in each part,0
%% Parameters,0
%% Load the annotations queried from megadb,0
"%% Make the ""dataset"" required by create_tfrecords.py",0
%% Create tfrecords,0
Construct a Reader to read examples from the .tfrecords file,0
Basic info,0
Print out the per class image counts,0
Can we detect if there any missing classes?,0
"We expect class id for each value in the range [0, max_class_id]",0
So lets see if we are missing any of these values,0
Construct a Reader to read examples from the .tfrecords file,0
Reversed coordinates?,0
Too small of an area?,0
Basic info,0
"print(""Images with areas < 10:"")",0
for img_id in images_with_small_bboxes:,0
print(img_id),0
"print(""Images with reversed coordinates:"")",0
for img_id in images_with_reversed_coords:,0
print(img_id),0
for img_id in images_with_bbox_count_mismatch:,0
print(img_id),0
,0
read_from_tf_records.py,0
,0
"Reads detection results from a tfrecords file of the style generated by the TFODAPI inference script,",0
"and converts it to a .p file that's friendly to other tools in this repo, e.g. detection/detector_eval.",0
,0
"Detection and ground truth bounding box coordinates are in the format of [ymin, xmin, ymax, xmax].",0
,0
coding: utf-8,0
In[1]:,0
In[2]:,0
In[ ]:,0
Defaults are not specified since both keys are required.,0
"image, label, height, width",0
"print(sess.run([output['image/filename'], output['image/class/text'], output['image/class/label'], output['image/height'], output['image/width']]))",0
tasks = list(glob.glob('/data/lila/nacti/cropped_tfrecords/t*')),0
"for _ in tqdm.tqdm(p.imap(analyze_record, tasks, chunksize=10), total=len(tasks)):",0
pass,0
Parallel(n_jobs=8)(delayed(analyze_record)(path) for path in list(glob.glob('/data/lila/nacti/cropped_tfrecords/t*'))),0
,0
iterate_tf_records.py,0
,0
Inherited from Visipedia tfrecords repo.,0
,0
print(feature_key),0
return a dictionary of the features,0
Construct a Reader to read examples from the .tfrecords file,0
Copyright 2016 Google Inc. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Required,0
Class label for the whole image,0
Objects,0
Bounding Boxes,0
Parts,0
Areas,0
Ids,0
Any extra data (e.g. stringified json),0
Additional fields for the format needed by the Object Detection repository,0
"For explanation of the fields, see https://github.com/visipedia/tfrecords",0
Additional fields for format needed by Object Detection repository,0
Create a single Session to run all image coding calls.,0
Initializes function that converts PNG to JPEG data.,0
Initializes function that decodes RGB JPEG data.,0
Convert the image data from png to jpg,0
Decode the image data as a jpeg image,0
Read the image file.,0
Clean the dirty data.,0
Decode the RGB JPEG.,0
Check that image converted to RGB,0
Each thread produces N shards where N = int(num_shards / num_threads).,0
"For instance, if num_shards = 128, and the num_threads = 2, then the first",0
"thread would produce shards [0, 64).",0
"Generate a sharded version of the file name, e.g. 'train-00002-of-00010'",0
raise,0
Images in the tfrecords set must be shuffled properly,0
"Break all images into batches with a [ranges[i][0], ranges[i][1]].",0
Launch a thread for each batch.,0
Create a mechanism for monitoring when all threads are finished.,0
Create a generic TensorFlow-based utility for converting all image codings.,0
A Queue to hold the image examples that fail to process.,0
Wait for all the threads to terminate.,0
Collect the errors,0
,0
create_splits.py,0
,0
"Based on a tfrecords_format json file of the database, creates 3 splits according to",0
the specified fractions based on location (images from the same location should be in,0
one split) or based on images.,0
,0
"If a previous split is provided (append_to_previous_split is True), the entries in",0
"each split will be preserved, and new entries will be appended, so that new models",0
can warm start with a model trained on the original splits.,0
,0
%% Imports,0
%% Main function,0
"code below references 'locatioin' as the attribute to split on, but it works for any split_by attribute",0
present in the image entryes,0
"find new locations and assign them to a split, without reassigning any previous locations",0
do NOT sort the IDs to keep the shuffled order,0
,0
create_tfrecords_format.py,0
,0
This script converts a COCO formatted json database to another json file that would be the,0
input to create_tfrecords.py or similar scripts to create tf_records,0
,0
%% Imports and environment,0
%% Main tfrecord generation function,0
Remap category IDs; TF needs consecutive category ids,0
Sanity-check number of empty annotations and annotation-less images,0
Images without annotations don't have bounding boxes,0
Images with annotations *may* have bounding boxes,0
"prepend the dataset name to image_id because after inference on val set, records from",0
different datasets are stored in one tfrecord,0
Propagate optional metadata to tfrecords,0
checking to ignore any images that contain 'group' needs to happen before ignoring non-valid categories!,0
Only include valid categories,0
...for each annotation for the current image,0
...for each image,0
,0
create_tfrecords_from_coco.py,0
,0
This script creates a tfrecords file from a classification dataset in COCO format.,0
%% Imports and environment,0
%% Main tfrecord generation function,0
We remap all category IDs such that they are consecutive starting from zero,0
"If this is already the case for the input dataset, then the remapping will not",0
"have any effect, i.e. the order of the classes will remain unchanged",0
Propagate optional metadata to tfrecords,0
endfor each annotation for the current image,0
endfor each image,0
"Calculate number of shards to get the desired number of images per record,",0
ensure it is evenly divisible by the number of threads,0
%% Command-line driver,0
%% Driver,0
,0
create_lila_test_set.py,0
,0
"Create a test set of camera trap images, containing N empty and N non-empty",0
images from each LILA data set.,0
,0
%% Constants and imports,0
LILA camera trap master metadata file,0
"We'll write images, metadata downloads, and temporary files here",0
%% Support functions,0
remove the leading '/',0
%% Download and parse the metadata file,0
Put the master metadata file in the same folder where we're putting images,0
Read lines from the master metadata file,0
Parse those lines into a table,0
Each line in this file is name/base_url/json_url/[box_url],0
%% Download and extract metadata for every dataset,0
ds_name = (list(metadata_table.keys()))[0],0
Unzip if necessary,0
...for each dataset of interest,0
%% Choose images from each dataset,0
ds_name = (list(metadata_table.keys()))[0],0
...for each dataset,0
%% Convert to URLs,0
ds_name = (list(metadata_table.keys()))[0],0
Retrieve image file names,0
Convert to URLs,0
...for each dataset,0
%% Download those image files,0
ds_name = (list(metadata_table.keys()))[0],0
"This URL may not be a SAS URL, we will remove a SAS token if it's present",0
url = urls_to_download[0],0
...for each url,0
...for each dataset,0
,0
"Generate read-only SAS URLs for all LILA containers, to facilitate partial downloads.",0
,0
The results of this script end up here:,0
,0
http://lila.science/wp-content/uploads/2020/03/lila_sas_urls.txt,0
,0
"Update: that file is manually maintained now, it can't be programmatically generated",0
,0
%% Imports,0
Read-only,0
%% Enumerate containers,0
%% Generate SAS tokens,0
%% Generate SAS URLs,0
%% Write to output file,0
,0
get_lila_category_list.py,0
,0
Example of making a text file listing all category names in specific LILA datasets,0
,0
%% Constants and imports,0
LILA camera trap master metadata file,0
array to fill for output,0
# datasets and categories to look at,0
"if False, will only collect data for categories in datasets_of_interest",0
only need if restrict_category is false,0
"We'll write images, metadata downloads, and temporary files here",0
%% Support functions,0
remove the leading '/',0
%% Download and parse the metadata file,0
Put the master metadata file in the same folder where we're putting images,0
Read lines from the master metadata file,0
Parse those lines into a table,0
Each line in this file is name/sas_url/json_url/[bbox_json_url],0
Create a separate entry for bounding boxes if they exist,0
%% Download and extract metadata for the datasets we're interested in,0
Unzip if necessary,0
...for each dataset of interest,0
%% Get category names,0
Open the metadata file,0
Collect list of categories and mappings to category name,0
Append category to categories_list,0
%% Save category names to file,0
,0
download_lila_subset.py,0
,0
"Example of how to download a list of files from LILA, e.g. all the files",0
in a data set corresponding to a particular species.,0
,0
%% Constants and imports,0
LILA camera trap master metadata file,0
"In this example, we're using the Missouri Camera Traps data set and the Caltech Camera Traps dataset",0
All lower-case; we'll convert category names to lower-case when comparing,0
"We'll write images, metadata downloads, and temporary files here",0
"We will demonstrate two approaches to downloading, one that loops over files",0
"and downloads directly in Python, another that uses AzCopy.",0
,0
AzCopy will generally be more performant and supports resuming if the,0
transfers are interrupted.  This script assumes that azcopy is on the system path.,0
Number of concurrent download threads (when not using AzCopy) (AzCopy does its,0
own magical parallelism),0
%% Support functions,0
remove the leading '/',0
%% Download and parse the metadata file,0
Put the master metadata file in the same folder where we're putting images,0
Read lines from the master metadata file,0
Parse those lines into a table,0
Each line in this file is name/base_url/json_url/[box_url],0
%% Download and extract metadata for the datasets we're interested in,0
Unzip if necessary,0
...for each dataset of interest,0
%% List of files we're going to download (for all data sets),0
"Flat list or URLS, for use with direct Python downloads",0
For use with azcopy,0
This may or may not be a SAS URL,0
# Open the metadata file,0
# Build a list of image files (relative path names) that match the target species,0
Retrieve all the images that match that category,0
Retrieve image file names,0
Convert to URLs,0
...for each dataset,0
%% Download those image files,0
ds_name = 'Caltech Camera Traps',0
ds_name = 'SWG Camera Traps',0
We want to use the whole relative path for this script (relative to the base of the container),0
"to build the output filename, to make sure that different data sets end up in different folders.",0
This may or may not be a SAS URL,0
For example:,0
,0
caltech-unzipped/cct_images,0
swg-camera-traps,0
Check whether the URL includes a folder,0
E.g. caltech-unzipped,0
E.g. cct_images,0
E.g. swg-camera-traps,0
"Write out a list of files, and use the azcopy ""list-of-files"" option to download those files.",0
,0
This azcopy feature is unofficially documented at:,0
,0
https://github.com/Azure/azure-storage-azcopy/wiki/Listing-specific-files-to-transfer,0
,0
import clipboard; clipboard.copy(cmd),0
Loop over files,0
,0
get_lila_category_counts.py,0
,0
Example of how to create a csv of category counts per dataset (at '<output_dir> +,0
category_counts.xlsx') and a coco_file (at'<output_dir> + compiled_coco.json') for,0
all identified images of the given category over the given datasets.,0
,0
You can constrain the category and datasets looked at in code below.,0
,0
Columns in the csv are:,0
number of images labeled with given category(im_cnt),0
number of images with a bbox labeled with given category (bbox_im_cnt),0
number of bbox's labeled with given category (bbox_cnt),0
,0
This script also outputs two pickle files which contain dictionaries.,0
,0
The 'total_category_counts_by_set.pkl' file contains a dictionary of dictionaries of the form:,0
,0
{<dataset-with-image-level-annotations>:,0
"{<category> : { 'image_urls': [<image-urls-of-images-with-category>],",0
"'im_cnt :<number>},",0
"<category2>: ...},",0
<dataset-with-bbox-level-annotations>:,0
"{<category> : { 'image_urls': [<image-urls-of-images-with-category>],",0
"'bbox_im_cnt :<number>},",0
"'bbox_cnt':<number>},",0
"<category2>: ...},",0
<dataset3>: ...},0
,0
The 'total_category_counts.pkl' file contains a dictionary of dictionaries of the form:,0
,0
"{<category> : { 'image_urls': [<image-urls-of-images-with-category>],",0
"'im_cnt :<number,",0
"'bbox_im_cnt :<number>},",0
"'bbox_cnt':<number>},",0
<category2>: ...},0
,0
%% Constants and imports,0
LILA camera trap master metadata file,0
# dictionaries to fill for output,0
category count over all datasets,0
category count seperated by datasets,0
# datasets and categories to look at,0
"if False, will only collect data from datasets_of_interest",0
only meaningful if use_all_datasets is false,0
"if False, will only collect data for categories in categories_of_interest",0
only meaningly if use_all_categories is False,0
"How the categories should be labeled in the csv. key is label in lila dataset,",0
value is label to use in csv,0
"We'll write images, metadata downloads, and temporary files here",0
%% Support functions,0
%% Download and parse the metadata file,0
Put the master metadata file in the same folder where we're putting images,0
Read lines from the master metadata file,0
Parse those lines into a table,0
Each line in this file is name/sas_url/json_url/[bbox_json_url],0
Create a separate entry for bounding boxes if they exist,0
%% Download and extract metadata for the datasets we're interested in,0
Unzip if necessary,0
...for each dataset of interest,0
%% Count categories,0
# Open the metadata file,0
"Double check the annotations url provided corresponds to that implied by ds_name, or else fix",0
Only need to look at first entry b/c json files with image-level annotations,0
are seperated from those with box-level annotations.,0
Build a list of image files (relative path names) that match the target categories,0
Add categories entry to total_category_counts if not already present,0
Retrieve all the images that match that category,0
Convert to URLs and add to category_counts dicts,0
Record total category count in dataset,0
Add relevant annotations to custom coco file,0
%% Save output COCO files,0
%% Save category counts to csv,0
%% Save dictionary of category counts and image urls to file,0
,0
add_url_to_database.py,0
,0
"Adds a ""url"" field to a coco-camera-traps .json database, specifically to allow the db to",0
be reviewed in the Visipedia annotation tool.,0
,0
,0
remove_corrupted_images_from_database.py,0
,0
"Given a coco-camera-traps .json file, checks all images for TF-friendliness and generates",0
a new .json file that only contains the non-corrupted images.,0
,0
%% Imports and constants,0
import multiprocessing,0
"I leave this at an annoying low number, since by definition weird stuff will",0
"be happening in the TF kernel, and it's useful to keep having content in the console.",0
%% Function definitions,0
"I sometimes pass in a list of images, sometimes a dict with a single",0
element mapping a job ID to the list of images,0
"We're about to start a lot of TF sessions, and we don't want gobs",0
of debugging information printing out for every session.,0
At some point we were creating a single session and looping over images,0
"within that session, but the only way I found to reliably not run out",0
of GPU memory was to create a session per image and gc.collect() after,0
each session.,0
Map Image IDs to boolean (should I keep this image?),0
"Convert to lists, append job numbers to the image lists",0
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)",0
Merge results,0
%% Interactive driver,0
%%,0
base_dir = r'D:\temp\snapshot_serengeti_tfrecord_generation',0
Load annotations,0
Check for corruption,0
Write out only the uncorrupted data,0
%% Command-line driver,0
#######,0
,0
integrity_check_json_db.py,0
,0
"Does some integrity-checking and computes basic statistics on a db, specifically:",0
,0
* Verifies that required fields are present and have the right types,0
* Verifies that annotations refer to valid images,0
* Verifies that annotations refer to valid categories,0
"* Verifies that image, category, and annotation IDs are unique",0
,0
* Optionally checks file existence,0
,0
* Finds un-annotated images,0
* Finds unused categories,0
,0
* Prints a list of categories sorted by count,0
,0
#######,0
%% Constants and environment,0
%% Functions,0
"If baseDir is non-empty, checks image existence",0
This is used in a medium-hacky way to share modified options across threads,1
print('Image path {} does not exist'.format(filePath)),0
"#%% Read .json file if necessary, integrity-check fields",0
info = data['info'],0
"#%% Build dictionaries, checking ID uniqueness and internal validity as we go",0
Confirm that required fields are present,0
Confirm ID uniqueness,0
...for each category,0
image = images[0],0
Confirm that required fields are present,0
Confirm ID uniqueness,0
We previously supported ints here; this should be strings now,0
"assert isinstance(image['location'], str) or isinstance(image['location'], int), 'Illegal image location type'",0
Are we checking for unused images?,0
Recursively enumerate images,0
print('Image {} is unused'.format(p)),0
Are we checking file existence and/or image size?,0
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)",0
"print('Image validation error for image {} ({})'.format(iImage,images[iImage]['file_name']))",0
...for each image,0
Confirm that required fields are present,0
Confirm ID uniqueness,0
Confirm validity,0
...for each annotation,0
#%% Print statistics,0
Find un-annotated images and multi-annotation images,0
Find unused categories,0
Prints a list of categories sorted by count,0
https://stackoverflow.com/questions/72899/how-do-i-sort-a-list-of-dictionaries-by-a-value-of-the-dictionary,0
...def integrity_check_json_db(),0
%% Command-line driver,0
"python integrity_check_json_db.py ""e:\wildlife_data\wellington_data\wellington_camera_traps.json"" --baseDir ""e:\wildlife_data\wellington_data\images"" --bFindUnusedImages --bCheckImageSizes",0
"python integrity_check_json_db.py ""D:/wildlife_data/mcgill_test/mcgill_test.json"" --baseDir ""D:/wildlife_data/mcgill_test"" --bFindUnusedImages --bCheckImageSizes",0
"Here the '-u' prevents buffering, which makes tee happier",0
,0
python -u integrity_check_json_db.py '/datadrive1/nacti_metadata.json' --baseDir '/datadrive1/nactiUnzip/' --bFindUnusedImages --bCheckImageSizes | tee ~/nactiTest.out,0
%% Interactive driver(s),0
%%,0
Integrity-check .json files for LILA,0
options.iMaxNumImages = 10,0
,0
combine_two_json_files.py,0
,0
"Merges two coco-camera-traps .json files. In particular, categories are combined and re-numbered.",0
,0
Combined Info,0
Combined Images,0
Combined Categories,0
## categories to merge,0
Combined Annotations,0
,0
make_detection_db_for_viewing.py,0
,0
"Given a .json file with ground truth bounding boxes, and a .p file containing detections for the same images,",0
"creates a new .json file with separate classes for ground truth and detection, suitable for viewing in the Visipedia",0
annotation tool.,0
,0
%% Imports and constants,0
%% Main function,0
im_id_to_im = {im['id']:im for im in images},0
make new categories to distinguish between ground truth and detections,0
"update all gt annotations to be class ""gt""",0
collect all detections by image,0
keep any detection with score above det_thresh,0
"need to convert bbox from [x1,y1,x2,y2] to [x,y,w,h]",0
"add ""info"" and ""licenses"" for annotation tools to function",0
create new db,0
%% Command-line handling,0
,0
analyze_json_database.py,0
,0
Plots location/class/etc. distributions for classes in a coco-camera-traps .json file.,0
,0
Currently includes some one-off code for specific species.,0
,0
%% Constants and imports,0
%% Path configuration,0
%% Load source data,0
%% Build image/category dictionaries,0
%% Make plot of category distribution,0
%% make plots of location distribution,0
"plt.title('Number of images per location, by category')",0
"plt.tight_layout(rect=[0,0,1,0.9])",0
#make plot of images per season,0
%% Make plot of lions per location,0
%% Make plot of elephants per location,0
for loc in sorted_by_total[:25]:,0
"print('Location:' + loc[0] +', Lions: ' + str(len(loc_to_lion_ims[loc[0]])) + ', Elephants: ' + str(len(loc_to_elephant_ims[loc[0]])) + ', Total ims: ' + str(len(loc_to_ims[loc[0]])))",0
,0
subset_json_db.py,0
,0
Select a subset of images (and associated annotations) from a .json file,0
in COCO Camera Traps format.,0
,0
"To subset the .json files produced by our batch processing API, see",0
subset_json_detector_output.py,0
,0
Sample invocation:,0
,0
"python subset_json_db.py ""E:\Statewide_wolf_container\idfg_20190409.json"" ""E:\Statewide_wolf_container\idfg_20190409_clearcreek.json"" ""clearcreek"" --ignore_case",0
,0
%% Constants and imports,0
%% Functions,0
Load the input file if necessary,0
Find images matching the query,0
Find annotations referring to those images,0
Write the output file if requested,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
%% Constants and imports,0
%% Merge functions,0
i_input_dict = 0; input_dict = input_dicts[i_input_dict],0
We will prepend an index to every ID to guarantee uniqueness,0
Map detection categories from the original data set into the merged data set,0
...for each category,0
Merge original image list into the merged data set,0
Create a unique ID,0
...for each image,0
Same for annotations,0
...for each annotation,0
"Merge info dicts, don't check completion time fields",0
...for each dictionary,0
Convert merged image dictionaries to a sorted list,0
%% Driver,0
,0
add_width_and_height_to_database.py,0
,0
Grabs width and height from actual image files for a .json database that is missing w/h.,0
,0
Originally used when we created a .json file for snapshot serengeti from .csv.,0
,0
"This file converts the JSON output of the batch processing API to a pickle file, which can be used by the",0
script ./make_classification_dataset.py,0
"Convert boxes from JSON   [x_min, y_min, width_of_box, height_of_box]",0
"to PICKLE [ymin,  xmin,  ymax,         xmax]",0
Write detections to file with pickle,0
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the",0
script ./make_classification_dataset.py,0
Write detections to file with pickle,0
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the",0
script ./make_classification_dataset.py,0
Assumes that the root of the CameraTrap repo is in the PYTHONPATH,0
Minimum threshold to put a detection into the output JSON file,0
Parameter check,0
Load detections from input,0
Load COCO style annotations,0
Build output JSON in format version 1.0,0
Adding the only known metadata info,0
"The pickle file does not contain category information, so we assume the default",0
For each image with detections,0
for each detection,0
"Convert boxes from [ymin, xmin, ymax, xmax] format to",0
"[x_min, y_min, width_of_box, height_of_box]",0
Write output json,0
global flag for whether or not we encounter missing images,0
"- will only print ""missing image"" warning once",0
TFRecords variables,0
1.3 for the cropping during test time and 1.3 for the context that the,0
CNN requires in the left-over image,0
Create output directories,0
Load COCO style annotations from the input dataset,0
"Get all categories, their names, and create updated ID for the json file",0
Prepare the coco-style json files,0
Split the dataset by locations,0
Load detections,0
Write out COCO-style json files to the output directory,0
Write detections to file with pickle,0
## Preparations: get all the output tensors,0
For all images listed in the annotations file,0
Skip the image if it is annotated with more than one category,0
"Get ""old"" and ""new"" category ID and category name for this image.",0
Skip if in excluded categories.,0
get path to image,0
"If we already have detection results, we can use them",0
Otherwise run detector and add detections to the collection,0
Only select detections with confidence larger than DETECTION_THRESHOLD,0
Skip image if no detection selected,0
whether it belongs to a training or testing location,0
Skip images that we do not have available right now,0
- this is useful for processing parts of large datasets,0
Load image,0
Run inference,0
"remove batch dimension, and convert from float32 to appropriate type",0
convert normalized bbox coordinates to pixel coordinates,0
Pad the detected animal to a square box and additionally by,0
"PADDING_FACTOR. The result will be in crop_boxes. However, we need to make",0
sure that its box coordinates are still within the image.,0
"for each bounding box, crop the image to the padded box and save it",0
"Create the file path as it will appear in the annotation json,",0
adding the box number if there are multiple boxes,0
"if the cropped file already exists, verify its size",0
Add annotations to the appropriate json,0
js_keys = ['/'.join(im['file_name'].split('/')[1:])[:-4] for im in js['images']],0
for tk in js_keys:,0
"assert np.isclose(1, np.sum(detections[tk]['detection_scores'] > 0.5))",0
,0
separate_detections_by_size,0
,0
Not-super-well-maintained script to break a list of API output files up,0
based on bounding box size.,0
,0
%% Imports and constants,0
Folder with one or more .json files in it that we want to split up,0
Enumerate .json files,0
Define size thresholds and confidence thresholds,0
"Not used directly in this script, but useful if we want to generate previews",0
%% Split by size,0
For each size threshold...,0
For each file...,0
fn = input_files[0],0
Just double-checking; we already filtered this out above,0
Don't reprocess .json files we generated with this script,0
Load the input file,0
For each image...,0
1.1 is the same as infinity here; no box can be bigger than a whole image,0
What's the smallest detection above threshold?,0
"[x_min, y_min, width_of_box, height_of_box]",0
,0
size = w * h,0
...for each detection,0
Which list do we put this image on?,0
...for each image in this file,0
Make sure the number of images adds up,0
Write out all files,0
...for each size threshold,0
...for each file,0
,0
rde_debug.py,0
,0
Some useful cells for comparing the outputs of the repeat detection,0
"elimination process, specifically to make sure that after optimizations,",0
results are the same up to ordering.,0
,0
%% Compare two RDE files,0
i_dir = 0,0
break,0
"Regardless of ordering within a directory, we should have the same",0
number of unique detections,0
Re-sort,0
Make sure that we have the same number of instances for each detection,0
Make sure the box values match,0
,0
ocr_sandbox.py,0
,0
sandbox for experimenting with using OCR to pull metadata from camera trap images,0
,0
The general approach is:,0
,0
"* Crop a fixed percentage from the top and bottom of an image, slightly larger",0
than the largest examples we've seen of how much space is used for metadata.,0
,0
"* Refine that crop by blurring a little, then looking for huge peaks in the",0
"color histogram suggesting a solid background, then finding rows that are",0
mostly that color.,0
,0
"* Crop to the refined crop, then run pytesseract to extract text",0
,0
"* Use regular expressions to find time and date, in the future can add, e.g.,",0
"temperature (which is often present *only* in the images, unlike time/date which",0
are also usually in EXIF but often wrong or lost in processing),0
,0
"The metadata extraction (EXIF, IPTC) here is just sample code that seemed to",0
belong in this file.,0
,0
Contact: Dan Morris (dan@microsoft.com),0
,0
%% Constants and imports,0
pip install pytesseract,0
,0
"Also intall tesseract from: https://github.com/UB-Mannheim/tesseract/wiki, and add",0
"the installation dir to your path (on Windows, typically C:\Program Files (x86)\Tesseract-OCR)",0
pip install IPTCInfo3,0
from the ai4eutils repo: https://github.com/Microsoft/ai4eutils,0
,0
"Only used for writing out a summary, not important for the core metadata extraction",0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Using a semi-arbitrary metric of how much it feels like we found the,0
"text-containing region, discard regions that appear to be extraction failures",0
Pad each crop with a few pixels to make tesseract happy,0
Discard text from the top,0
"When we're looking for pixels that match the background color, allow some",0
tolerance around the dominant color,0
We need to see a consistent color in at least this fraction of pixels in our rough,0
crop to believe that we actually found a candidate metadata region.,0
"What fraction of the [top,bottom] of the image should we use for our rough crop?",0
A row is considered a probable metadata row if it contains at least this fraction,0
"of the background color.  This is used only to find the top and bottom of the crop area,",0
"so it's not that *every* row needs to hit this criteria, only the rows that are generally",0
above and below the text.,0
%% Support functions,0
"%% Load some images, pull EXIF and IPTC data for fun",0
%% Rough crop,0
"This will be an nImages x 1 list of 2 x 1 lists (image top, image bottom)",0
image = images[0],0
"l,t,r,b",0
,0
"0,0 is upper-left",0
"%% Close-crop around the text, return a revised image and success metric",0
Did we find a sensible mode that looks like a background value?,0
"This looks very scientific, right?  Definitely a probability?",0
"print('Failed min background fraction test: {} of {}'.format(pBackGroundValue,minBackgroundFraction))",0
"Notes to self, things I tried that didn't really go anywhere...",0
"analysisImage = cv2.blur(analysisImage, (3,3))",0
"analysisImage = cv2.medianBlur(analysisImage,5)",0
"analysisImage = cv2.Canny(analysisImage,100,100)",0
imagePil = Image.fromarray(analysisImage); imagePil,0
Use row heuristics to refine the crop,0
,0
This egregious block of code makes me miss my fluency in Matlab.,1
"print('Cropping to {},{},{},{}'.format(x,y,w,h))",0
Crop the image,0
"For some reason, tesseract doesn't like characters really close to the edge",0
imagePil = Image.fromarray(croppedImage); imagePil,0
%% Go to OCR-town,0
"An nImages x 2 list of strings, extracted from the top and bottom of each image",0
An nImages x 2 list of cropped images,0
iImage = 0; iRegion = 1; regionSet = imageRegions[iImage]; region = regionSet[iRegion],0
text = pytesseract.image_to_string(region),0
pil --> cv2,0
"image = cv2.medianBlur(image, 3)",0
"image = cv2.erode(image, None, iterations=2)",0
"image = cv2.dilate(image, None, iterations=4)",0
"image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]",0
"image = cv2.blur(image, (3,3))",0
"image = cv2.copyMakeBorder(image,10,10,10,10,cv2.BORDER_CONSTANT,value=[0,0,0])",0
"text = pytesseract.image_to_string(imagePil, lang='eng')",0
https://github.com/tesseract-ocr/tesseract/wiki/Command-Line-Usage,0
"psm 6: ""assume a single uniform block of text""",0
,0
...for each cropped region,0
...for each image,0
%% Extract dates and times,0
s = '1:22 pm',0
s = '1:23:44 pm',0
%% Write results to a handy html file,0
Add image name and resized image,0
Add results and individual region images,0
"textStyle = ""font-family:calibri,verdana,arial;font-weight:bold;font-size:150%;text-align:left;margin-left:50px;""",0
%% Scrap,0
Alternative approaches to finding the text/background  region,0
Using findCountours(),0
imagePil = Image.fromarray(analysisImage); imagePil,0
"analysisImage = cv2.erode(analysisImage, None, iterations=3)",0
"analysisImage = cv2.dilate(analysisImage, None, iterations=3)",0
"analysisImage = cv2.threshold(analysisImage, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]",0
Find object with the biggest bounding box,0
Using connectedComponents(),0
analysisImage = image,0
print('Found {} components'.format(nb_components)),0
We just want the *background* image,0
,0
"Takes a .json file with MD results for an individual video, and converts to a .csv that includes",0
frame times.  This is very bespoke to animal detection and does not include other classes.,0
,0
%% Imports and constants,0
Only necessary if you want to extract the sample rate from the video,0
%% Extract the sample rate if necessary,0
%% Load results,0
%% Convert to .csv,0
i_image = 0; im = results['images'][i_image],0
,0
umn-pr-analysis.py,0
,0
Precision/recall analysis for UMN data,0
,0
%% Imports and constants,0
results_file = results_file_filtered,0
"For two deployments, we're only processing imagse in the ""detections"" subfolder",0
String to remove from MegaDetector results,0
%% Enumerate deployment folders,0
%% Load MD results,0
im = md_results['images'][0],0
%% Load ground truth,0
i_row = 0; row = ground_truth_df.iloc[i_row],0
%% Create relative paths for ground truth data,0
"Some deployment folders have no subfolders, e.g. this is a valid file name:",0
,0
M00/01010132.JPG,0
,0
"But some deployment folders have subfolders, e.g. this is also a valid file name:",0
,0
N17/100EK113/07160020.JPG,0
,0
"So we can't find files by just concatenating folder and file names, we have to enumerate and explicitly",0
"map what will appear in the ground truth as ""folder/filename"" to complete relative paths.",0
deployment_name = list(deployment_folders)[0],0
Enumerate all files in this folder,0
"print('Enumerated {} files for deployment {}'.format(len(files),deployment_name))",0
filename = files[100],0
...for each file in this deployment,0
...for each deployment,0
%% Map relative paths to MD results,0
%% Add relative paths to our ground truth table,0
i_row = 0; row = ground_truth_df.iloc[i_row],0
"row['filename'] looks like, e.g. A01/01080001.JPG.  This is not actually a path, it's",0
"just the deployment ID and the image name, separated by a slash.",0
Find the relative path for this image,0
Make sure we have MegaDetector results for this file,0
Make sure this image file exists,0
...for each row in the ground truth table,0
%% Take everything out of Pandas,0
%% Some additional error-checking of the ground truth,0
An early version of the data required consistency between common_name and is_blank,0
%% Combine MD and ground truth results,0
d = ground_truth_dicts[0],0
Find the maximum confidence for each category,0
...for each detection,0
...for each image,0
%% Precision/recall analysis,0
...for each image,0
Thresholds go up throughout precisions/recalls/thresholds; find the last,0
value where recall is at or above target.  That's our precision @ target recall.,0
Flatten the confusion matrix,0
Write precision/recall plot to .png file in output directory,0
pr_figure_relative_filename = 'prec_recall.png',0
"pr_figure_filename = os.path.join(output_dir, pr_figure_relative_filename)",0
plt.show(block=False),0
"precision_recall_analysis(human_gt_labels,human_prediction_probs,'humans')",0
%% Find and manually review all images of humans,0
%%,0
"...if this image is annotated as ""human""",0
...for each image,0
%% Find and manually review all MegaDetector animal misses,0
%%,0
im = merged_images[0],0
GT says this is not an animal,0
GT says this is an animal,0
%% Convert .json to .csv,0
%%,0
,0
kga-pr-analysis.py,0
,0
Precision/recall analysis for KGA data,0
,0
%% Imports and constants,0
%% Load and filter MD results,0
%% Load and filter ground truth,0
%% Map images to image-level results,0
%% Map sequence IDs to images and annotations to images,0
Verify consistency of annotation within a sequence,0
TODO,1
%% Find max confidence values for each category for each sequence,0
seq_id = list(sequence_id_to_images.keys())[1000],0
im = images_this_sequence[0],0
det = md_result['detections'][],0
...for each detection,0
...for each image in this sequence,0
...for each sequence,0
%% Prepare for precision/recall analysis,0
seq_id = list(sequence_id_to_images.keys())[1000],0
cat_id = list(category_ids_this_sequence)[0],0
...for each category in this sequence,0
...for each sequence,0
%% Precision/recall analysis,0
"Confirm that thresholds are increasing, recall is decreasing",0
This is not necessarily true,0
assert np.all(precisions[:-1] <= precisions[1:]),0
Thresholds go up throughout precisions/recalls/thresholds; find the max,0
value where recall is at or above target.  That's our precision @ target recall.,0
"This is very slightly optimistic in its handling of non-monotonic recall curves,",0
but is an easy scheme to deal with.,0
Flatten the confusion matrix,0
Write precision/recall plot to .png file in output directory,0
pr_figure_relative_filename = 'prec_recall.png',0
"pr_figure_filename = os.path.join(output_dir, pr_figure_relative_filename)",0
plt.show(block=False),0
"precision_recall_analysis(human_gt_labels,human_prediction_probs,'humans')",0
%% Scrap,0
%% Find and manually review all sequence-level MegaDetector animal misses,0
input_base = '/media/user/lila-01/lila/snapshot-safari/KGA/KGA_public',0
i_sequence = 0; seq_id = pr_sequence_ids[i_sequence],0
i_seq = 0; seq_id = false_negative_sequences[i_seq],0
"sequence_folder = os.path.join(sequence_preview_dir,'seq_{}'.format(str(i_seq).zfill(3)))",0
fn = image_files[0],0
"print('Copying {} to {}'.format(input_path,output_path))",0
...for each file in this sequence.,0
...for each sequence,0
%% Image-level postprocessing,0
parse arguments,0
check if a GPU is available,0
load a pretrained embedding model,0
setup experiment,0
load the embedding model,0
setup the target dataset,0
setup finetuning criterion,0
setup an active learning environment,0
create a classifier,0
the main active learning loop,0
Active Learning,0
finetune the embedding model and load new embedding values,0
gather labeled pool and train the classifier,0
save a snapshot,0
Load a checkpoint if necessary,0
setup the training dataset and the validation dataset,0
setup data loaders,0
check if a GPU is available,0
create a model,0
setup loss criterion,0
define optimizer,0
load a checkpoint if provided,0
setup a deep learning engine and start running,0
train the model,0
train for one epoch,0
evaluate on validation set,0
save a checkpoint,0
open the file,0
read it,0
do the substitution,0
matplotlib.use('Agg'),0
from UIComponents.DBObjects import *,0
Initialize Database,0
# database connection credentials,0
# try to connect as USER to database DB_NAME through peewee,0
Load the saved embedding model,0
dataset_query = Detection.select().limit(5),0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})",0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getalllabels()) , dataset.getallpaths(), {})",0
Random examples to start,0
"random_ids = np.random.choice(dataset.current_set, 1000, replace=False).tolist()",0
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)",0
print(random_ids),0
Move Records,0
"moveRecords(dataset, DetectionKind.ModelDetection.value, DetectionKind.UserDetection.value, random_ids)",0
#print([len(x) for x in dataset.set_indices]),0
# Finetune the embedding model,0
#dataset.set_kind(DetectionKind.UserDetection.value),0
#dataset.train(),0
"#train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)",0
"#finetune_embedding(model, checkpoint['loss_type'], dataset, 32, 4, 100)",0
#save_checkpoint({,0
"#        'arch': model.arch,",0
"#        'state_dict': model.state_dict(),",0
"#        'optimizer' : optimizer.state_dict(),",0
"#        'loss_type' : loss_type,",0
"#        }, False, ""%s%s_%s_%04d.tar""%('finetuned', loss_type, model.arch, len(dataset.set_indices[DetectionKind.UserDetection.value])))",0
Get indices of samples to get user to label,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
"kwargs[""already_selected""].extend(indices)",0
Train on samples that have been labeled so far,0
Test on the samples that have not been labeled,0
"'optimizer' : optimizer.state_dict(),",0
num_classes= len(train_dataset.getClassesInfo()[0]),0
"print(""Num Classes= ""+str(num_classes))",0
define loss function (criterion) and optimizer,0
"optimizer = torch.optim.SGD(params, momentum = 0.9, lr = args.lr, weight_decay = args.weight_decay)",0
optimizer.load_state_dict(checkpoint['optimizer']),0
train for one epoch,0
"adjust_lr(optimizer,epoch)",0
if epoch % 1 == 0 and epoch > 0:,0
"a, b, c = e.predict(train_embd_loader, load_info = True, dim = args.feat_dim)",0
"plot_embedding(reduce_dimensionality(a), b, c, {})",0
evaluate on validation set,0
"embedding_net = EmbeddingNet('resnet50', 256, True)",0
compute output,0
val_loader = train_dataset.getSingleLoader(batch_size = 8),0
"for a, b , c in val_loader:",0
print(b[0]),0
"plt.imshow(np.rollaxis(np.rollaxis(a[0].numpy(), 1, 0), 2, 1))",0
plt.show(),0
"print(np.rollaxis(a[0].numpy() , 1, 0).shape)",0
--------some stuff needed to get AJAX to work with bottle?--------#,0
-------------------------------------------------------------------------------- #,0
PREPARE TO QUEUE IMAGES FOR LABELING,0
-------------------------------------------------------------------------------- #,0
# Connect as USER to database DB_NAME through peewee and initialize database proxy,0
# Load embedding model,0
---------------------------------------------------------------------- #,0
CREATE QUEUE OF IMAGES TO LABEL,0
---------------------------------------------------------------------- #,0
Use classifier to generate predictions,0
# Update model predicted class in PostgreSQL database,0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?,0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
-------------------------------------------------------------------------------- #,0
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI,0
-------------------------------------------------------------------------------- #,0
"# static routes (to serve CSS, etc.)",0
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps",0
"return bottle.static_file(filename, root='../../../../../../../../../../../.')",0
# dynamic routes,0
if data['display_grayscale']:,0
indices_to_exclude.update(set(color_indices)),0
elif not data['display_grayscale']:,0
indices_to_exclude.update(set(grayscale_indices)),0
data['display_images'] = {},0
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices],0
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices],0
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices],0
data['display_images']['detection_categories'] = [],0
for i in indices:,0
if str(dataset.samples[i][1]) == 'None':,0
data['display_images']['detection_categories'].append('None'),0
else:,0
existing_category_entries = {cat.id: cat.name for cat in Category.select()},0
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()",0
data['display_images']['detection_categories'].append(cat_name),0
Use image ids in images_to_label to get the corresponding dataset indices,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
print(set(dataset.set_indices[4]).update(set(indices_to_label))),0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Use image id images_to_label to get the corresponding dataset index,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?,0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Train on samples that have been labeled so far,0
dataset.set_kind(DetectionKind.UserDetection.value),0
print(y_train),0
Predict on the samples that have not been labeled,0
print(y_pred),0
Update model predicted class in PostgreSQL database,0
timer = time.time(),0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
print('Updating the database took %0.2f seconds'%(time.time() - timer)),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids)),0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
"once the classifier has been trained the first time, switch to AL sampling",0
# sourceMappingURL=bootstrap.bundle.min.js.map,0
"print(ap_distances.size(),an_distances.size())",0
losses = -(((-ap_distances)/128)+1+1e-16).log() - (((-(128-an_distances))/128)+1+1e-16).log(),0
import pdb,0
pdb.set_trace(),0
losses = ap_distances - an_distances + self.margin,0
print(losses.size()),0
from UIComponents.DBObjects import *,0
TODO: should this also change self.kind?,1
get the embedding representations for all samples (i.e. set current_set to all indices),0
"return PILImage.open(os.path.join(self.img_base,path)).convert('RGB')",0
"print(self.labels_set, self.n_classes)",0
from sklearn.manifold import TSNE,0
embedding= TSNE(n_components=2).fit_transform(X),0
embedding= PCA(n_components=2).fit_transform(X),0
return X,0
"print(dir(event), type(sc))",0
"print(label,bgcolor)",0
"patches.append(mpatches.Patch(color=indexcolors[i], label=label))",0
"plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,",0
"ncol=12, mode=""expand"", borderaxespad=0., handles=patches)",0
plt.legend(handles=patches),0
"plt.xlabel('Dim 1', fontsize=12)",0
"plt.ylabel('Dim 2', fontsize=12)",0
plt.grid(True),0
"plt.xlabel('Dim 1', fontsize=12)",0
"plt.ylabel('Dim 2', fontsize=12)",0
plt.grid(True),0
print(thumb),0
"img.thumbnail((16, 12), PILImage.ANTIALIAS)",0
plt.show(),0
Add all negatives for all positive pairs,0
print(triplets.shape[0]),0
compute output,0
measure accuracy,0
compute loss on this batch,0
"train on a batch, record loss, and measure accuracy (if calc_accuracy)",0
compute output,0
measure accuracy and record loss,0
switch to evaluate mode,0
compute output,0
switch to evaluate mode,0
"self.fc13 = nn.Linear(128, 64)",0
self.bn2 = nn.BatchNorm1d(64),0
x = F.relu(self.fc12(x)),0
x = F.relu(self.bn1(self.fc13(x))),0
x = F.relu(self.fc13(x)),0
"x = F.dropout(x, training=self.training)",0
save features last FC layer,0
x = F.relu(x),0
save features last FC layer,0
import matplotlib.pyplot as plt,0
plt.switch_backend('agg'),0
class EmbeddingNet(nn.Module):,0
"def __init__(self, architecture, feat_dim, use_pretrained=False):",0
"super(EmbeddingNet, self).__init__()",0
self.feat_dim= feat_dim,0
self.inner_model = models.__dict__[architecture](pretrained=use_pretrained),0
if architecture.startswith('resnet'):,0
in_feats= self.inner_model.fc.in_features,0
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)",0
elif architecture.startswith('inception'):,0
in_feats= self.inner_model.fc.in_features,0
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)",0
if architecture.startswith('densenet'):,0
in_feats= self.inner_model.classifier.in_features,0
"self.inner_model.classifier = nn.Linear(in_feats, feat_dim)",0
if architecture.startswith('vgg'):,0
in_feats= self.inner_model.classifier._modules['6'].in_features,0
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)",0
if architecture.startswith('alexnet'):,0
in_feats= self.inner_model.classifier._modules['6'].in_features,0
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)",0
"def forward(self, x):",0
return self.inner_model.forward(x),0
class NormalizedEmbeddingNet(EmbeddingNet):,0
"def __init__(self, architecture, feat_dim, use_pretrained=False):",0
"EmbeddingNet.__init__(self, architecture, feat_dim, use_pretrained = use_pretrained)",0
"def forward(self, x):",0
embedding =  F.normalize(self.inner_model.forward(x))*10.0,0
"return embedding, embedding",0
"def get_random_images(num, image_dir, test_transforms):",0
"data = datasets.ImageFolder(image_dir, transform=test_transforms) # slight abuse; this expects subfolders corresponding to classes but we have no classes here",0
indices = list(range(len(data))),0
np.random.shuffle(indices),0
idx = indices[:num],0
from torch.utils.data.sampler import SubsetRandomSampler,0
sampler = SubsetRandomSampler(idx),0
"loader = torch.utils.data.DataLoader(data,",0
"sampler=sampler, batch_size=num)",0
dataiter = iter(loader),0
"images, labels = dataiter.next()",0
"return images, labels",0
"def predict_image(image, model, test_transforms):",0
"device = torch.device(""cuda"" if torch.cuda.is_available()",0
"else ""cpu"")",0
image_tensor = test_transforms(image).float(),0
image_tensor = image_tensor.unsqueeze_(0),0
input = Variable(image_tensor),0
input = input.to(device),0
output = model(input)[0],0
return output.data.cpu().numpy(),0
Connect to database and initialize db_proxy,0
# database connection credentials,0
# load the dataset,0
Load the saved embedding model from the checkpoint,0
# update the dataset embedding,0
# Create a folder for saving embedding visualizations with this model checkpoint,0
model_emb_dirname = os.path.basename(args.base_model).split('.')[0],0
"os.makedirs(model_emb_dirname, exist_ok=True)",0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')",0
dataset.embedding_mode(),0
"assert 2==3, 'break'",0
datasetindices = list(range(len(dataset))),0
np.random.shuffle(datasetindices),0
random_indices = datasetindices[:args.num],0
print(random_indices),0
"selected_sample_features = np.array([]).reshape(0, 256)",0
selected_sample_labels = [],0
for idx in random_indices:,0
"selected_sample_features = np.vstack([selected_sample_features, X_train[idx]])",0
selected_sample_labels.append(y_train[idx]),0
img_path = imagepaths[idx].split('.JPG')[0],0
image = dataset.loader(img_path),0
selected_sample_images.append(image),0
# TRY NEAREST NEIGHBORS WALK THROUGH EMBEDDING,0
nbrs = NearestNeighbors(n_neighbors=args.num).fit(selected_sample_features),0
"distances, indices = nbrs.kneighbors(selected_sample_features)",0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')",0
"idx_w_closest_nbr = np.where(distances[:,1] == min(distances[:,1]))[0][0]",0
order = [idx_w_closest_nbr],0
for ii in range(len(distances)):,0
"distances[ii, 0] = np.inf",0
while len(order)<args.num:,0
curr_idx = order[-1],0
curr_neighbors = indices[curr_idx],0
curr_dists = list(distances[curr_idx]),0
# print(min(curr_dists)),0
next_closest_pos = curr_dists.index(min(curr_dists)),0
next_closest = curr_neighbors[next_closest_pos],0
order.append(next_closest),0
# make sure you can't revisit past nodes,0
for vi in order:,0
vi_pos = list(indices[next_closest]).index(vi),0
"distances[next_closest, vi_pos] = np.inf",0
for ii in range(len(order)):,0
imgidx = order[ii],0
image = selected_sample_images[imgidx],0
"image.save(model_emb_dirname+""/img""+str(ii)+""_""+str(selected_sample_labels[imgidx])+"".png"")",0
# Specify the transformations on the input images before inference,0
"# test_transforms = transforms.Compose([transforms.Resize([224, 224]), transforms.ToTensor()])",0
"test_transforms = transforms.Compose([transforms.Resize([256, 256]), transforms.RandomCrop([224, 224]), transforms.RandomHorizontalFlip(), transforms.ColorJitter(), transforms.ToTensor(), transforms.Normalize([0.407328, 0.407328, 0.407328], [0.118641, 0.118641, 0.118641])])",0
"images, labels = get_random_images(num, image_dir, test_transforms)",0
"all_features = np.array([]).reshape(0, 256)",0
for ii in range(len(images)):,0
image = to_pil(images[ii]),0
"features = predict_image(image, model, test_transforms)",0
"all_features = np.vstack([all_features, features])",0
# for ii in range(len(images)):,0
#     image = to_pil(images[ii]),0
"#     image.save(""img""+str(ii)+"".png"")",0
# TRY CLUSTERING,0
kmeans1 = KMeans(n_clusters=5).fit(StandardScaler().fit_transform(all_features)),0
print(kmeans1.labels_),0
for ii in range(len(images)):,0
image = to_pil(images[ii]),0
"filename = str(kmeans1.labels_[ii])+""/img""+str(ii)+"".png""",0
if not os.path.exists(os.path.dirname(filename)):,0
os.makedirs(os.path.dirname(filename)),0
image.save(filename),0
class SaveFeatures():,0
"def __init__(self, module):",0
self.hook = module.register_forward_hook(self.hook_fn),0
"def hook_fn(self, module, input, output):",0
"self.features = torch.tensor(output, requires_grad=True).cuda()",0
def close(self):,0
self.hook.remove(),0
Load the saved embedding model from the checkpoint,0
"Get a sample from the database, with eval transforms applied, etc.",0
Connect to database and sample a dataset,0
output = model.forward(sample_image.unsqueeze(0)),0
print(output),0
with torch.no_grad():,0
sample_image_input = sample_image.cuda(non_blocking=True),0
"_, output = model(sample_image_input) # compute output",0
print(output),0
sample_image = PILImage.open(sample_image_path).convert('RGB'),0
"sample_image = transforms.Compose([Resize([256, 256]), CenterCrop(([[224,224]])), ToTensor(), Normalize([0.369875, 0.388726, 0.347536], [0.136821, 0.143952, 0.145229])])(sample_image)",0
print(list(model_inner_resnet.children())),0
print(model_inner_resnet.fc),0
print(model_inner_resnet.fc0),0
# print(model_inner_resnet.layer4[0].conv2),0
# print(type(model)),0
# print(len(list(model_inner_resnet.children()))),0
# print(list(model.children())),0
# print(list(list(model.children())[0].children())),0
"img = np.uint8(np.random.uniform(150, 180, (56, 56, 3)))/255",0
"img_tensor = torch.unsqueeze(torch.from_numpy(img), 0)",0
full_out = model_inner_resnet.forward(img_tensor),0
print(full_out),0
model(img_tensor),0
activations = SaveFeatures(model_inner_resnet.layer4[0].conv2),0
print(activations.features),0
print(type(activations.features)),0
activations.close(),0
Connect to database and sample a dataset,0
Load the saved embedding model from the checkpoint,0
Update the dataset embedding,0
Get a random query image,0
# # # IMAGES IN THE SAME SEQUENCE # # # #,0
"assert 2==3, 'break'",0
# # # CLOSEST IN (EMBEDDING) FEATURE SPACE # # # #,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Variables for the hierarchical cluster,0
Variables for the AL algorithm,0
Data variables,0
"connectivity = kneighbors_graph(self.transformed_X,max_features)",0
Fit cluster and update cluster variables,0
The sklearn hierarchical clustering algo numbers leaves which correspond,0
to actual datapoints 0 to n_points - 1 and all internal nodes have,0
ids greater than n_points - 1 with the root having the highest node id,0
"If no labels have been observed, simply return uniform distribution",0
"If no observations, return worst possible upper lower bounds",0
Loop through generations from bottom to top,0
Update admissible labels for node,0
Calculate score,0
Determine if node should be split,0
Make sure label set for node so that we can flow to children,0
if necessary,0
Only split if all ancestors are admissible nodes,0
This is part  of definition of admissible pruning,0
Check that pruning covers all leave nodes,0
Fill in labels,0
Observe labels for previously recommended batches,0
TODO(lishal): implement multiple selection methods,1
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
A list of initialized samplers is allowed as an input because,0
"for AL_methods that search over different mixtures, may want mixtures to",0
have shared AL_methods so that initialization is only performed once for,0
computation intensive methods like HierarchicalClusteringAL and,0
states are shared between mixtures.,0
"If initialized samplers are not provided, initialize them ourselves.",0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copy these objects to make sure they are not modified while simulating,0
trajectories as they are used later by the main run_experiment script.,0
Assumes that model has already by fit using all labeled data so,0
the probabilities can be used immediately to hallucinate labels,0
All models need to have predict_proba method,0
Hallucinate labels for selected datapoints to be label,0
using class probabilities from model,0
"Not saving already_selected here, if saving then should sort",0
only for the input to fit but preserve ordering of indices in,0
already_selected,0
Useful to know how accuracy compares for model trained on hallucinated,0
labels vs trained on true labels.  But can remove this train to speed,0
up simulations.  Won't speed up significantly since many more models,0
are being trained inside the loop above.,0
Save trajectory for reference,0
Delete created copies,0
THE INPUTS CANNOT BE MODIFIED SO WE MAKE COPIES FOR THE CHECK LATER,0
Should check model but kernel_svm does not have coef_ so need better,0
handling here,0
Make sure that model object fed in did not change during simulations,0
Return indices based on return type specified,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Set gamma for gaussian kernel to be equal to 1/n_features,0
kneighbors graph is constructed using k=10,0
"Make connectivity matrix symmetric, if a point is a k nearest neighbor of",0
"another point, make it vice versa",0
Graph edges are weighted by applying gaussian kernel to manhattan dist.,0
"By default, gamma for rbf kernel is equal to 1/n_features but may",0
get better results if gamma is tuned.,0
Define graph density for an observation to be sum of weights for all,0
edges to the node representing the datapoint.  Normalize sum weights,0
by total number of neighbors.,0
"If a neighbor has already been sampled, reduce the graph density",0
for its direct neighbors to promote diversity.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Update observed reward and arm probabilities,0
Sample an arm,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
y only used for determining how many clusters there should be,0
probably not practical to assume we know # of classes before hand,0
should also probably scale with dimensionality of data,0
Probably okay to always use MiniBatchKMeans,0
Should standardize data before clustering,0
Can cluster on standardized data but train on raw features if desired,0
TODO(lishal): have MarginSampler and this share margin function,1
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Probably okay to always use MiniBatchKMeans,0
Should standardize data before clustering,0
Can cluster on standardized data but train on raw features if desired,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Update min_distances for all examples given new cluster center.,0
Assumes that the transform function takes in original data and not,0
flattened data.,0
Initialize centers with a randomly selected datapoint,0
New examples should not be in already selected since those points,0
should have min_distance of zero to a cluster center.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
This is uniform given the remaining pool but biased wrt the entire pool.,0
sample = [i for i in range(self.X.shape[0]) if i not in already_selected],0
return sample[0:N],0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Fields for hierarchical clustering AL,0
Setting parent and storing nodes in dict for fast access,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Connect to database DB_NAME as USER and initialize tables,0
Populate Info table,0
Populate Category table,0
"# For now, we have a predefined list of species we expect to see in the camera trap database (e.g. maybe from a quick look through the images)",0
# TODO: allow user to update the class list through the labeling tool UI as they see different species,1
Populate Image and Detection tables,0
Image entry data,0
Detection entry data,0
Connect to database DB_NAME as USER and initialize tables,0
Populate Info table,0
Populate Category table,0
# Create category records from species present in the COCO camera trap classification dataset (COCO classes minus any excluded classes),0
# TODO: allow user to update the class list through the labeling tool UI as they see different species,1
Populate Image and Detection tables,0
"with open(os.path.join(args.crop_dir,'crops.json'), 'r') as infile:",0
crops_json = json.load(infile),0
counter = 0,0
timer = time.time(),0
num_detections = len(crops_json),0
for detectionid in crops_json:,0
counter += 1,0
detection_data = crops_json[detectionid],0
# Image entry data,0
existing_image_entries = Image.select().where((Image.file_name == detection_data['file_name'])),0
try:,0
existing_image_entry = existing_image_entries.get(),0
except:,0
"image_entry = Image.create(id=detectionid, file_name=detection_data['file_name'], width=detection_data['width'], height=detection_data['height'], grayscale=detection_data['grayscale'],",0
"source_file_name=detection_data['source_file_name'], relative_size=detection_data['relative_size'],",0
"seq_id=detection_data['seq_id'], seq_num_frames=detection_data['seq_num_frames'], frame_num=detection_data['frame_num'])",0
image_entry.save(),0
# Detection entry data,0
"detection_entry = Detection.create(id=detectionid, image=detectionid, bbox_confidence=detection_data['bbox_confidence'],",0
"bbox_X1=detection_data['bbox_X1'], bbox_Y1=detection_data['bbox_Y1'], bbox_X2=detection_data['bbox_X2'], bbox_Y2=detection_data['bbox_Y2'],",0
kind=DetectionKind.ModelDetection.value),0
detection_entry.save(),0
if counter%100 == 0:,0
"print('Updated database with Image and Detection table entries for %d out of %d crops in %0.2f seconds'%(counter, num_detections, time.time() - timer))",0
# data related to original image,0
Get class names from .txt list,0
Initialize Oracle table,0
Map filenames to classes (NOTE: we assume a single image does not contain more than one class),0
"For each detection, use source image path to get class",0
TODO update: Assumes that crops have already,1
been generated for the images using make_active_learning_classification_dataset.py. The created DB contains tables:,0
- info: information about the dataset,0
- image: images present in the dataset,0
- detections: crops of images with detections with confidence greater than a specified threshold,0
Initialize Database,0
# database connection credentials,0
HOST = 'localhost',0
PORT = 5432,0
"# first, make sure the (user, password) has been created",0
"# sudo -u postgres psql -c ""CREATE USER <db_user> WITH PASSWORD <db_password>;""",0
"# sudo -u postgres psql -c ""CREATE DATABASE <db_name> WITH OWNER <db_user> CONNECTION LIMIT -1;""",0
"# sudo -u postgres psql -c ""GRANT CONNECT ON DATABASE <db_name> TO <db_user>;""",0
"# sudo -u postgres psql -d <db_name> -c ""CREATE EXTENSION IF NOT EXISTS \""uuid-ossp\"";""",0
# Try to connect as USER to database DB_NAME through peewee,0
Populate Tables,0
# create Info table,0
# get class names for Category table,0
Faster anD available in Python 3.5 and above,0
# iterate through images in each class folder,0
killing this process after over 38 hours adding over 500k white-tailed deer crops from emammal,0
resuming for remaining classes,0
# get cropped image data for Image table,0
"if mean of each channel is about the same, image is likely grayscale",0
# still have no info on these:,0
seq_id = CharField(null= True)                # sequence identifier for the original image,0
seq_num_frames = IntegerField(null = True)    # number of frames in sequence,0
frame_num = IntegerField(null = True)         # which frame number in sequence,0
location = CharField(null = True)             # location of camera trap,0
datetime = DateTimeField(null = True),0
# store info about the detection corresponding to this image,0
# store info about the true labels for the detection,0
#  - for pretrain dataset this is the same as the detection_category if the detection categories,0
print(classes),0
Connect to database and sample a dataset,0
Load the saved embedding model from the checkpoint,0
Update the dataset embedding,0
save the images,0
save the features,0
"with open(os.path.join(args.output_dir, 'lastlayer_features.mat'), 'wb') as f:",0
"pickle.dump(sample_features, f)",0
"with open(os.path.join(args.output_dir, 'labels.mat'), 'wb') as f:",0
"pickle.dump(sample_labels, f)",0
"parser.add_argument('--db_name', default='missouricameratraps', type=str, help='Name of the training (target) data Postgres DB.')",0
"parser.add_argument('--db_user', default='user', type=str, help='Name of the user accessing the Postgres DB.')",0
"parser.add_argument('--db_password', default='password', type=str, help='Password of the user accessing the Postgres DB.')",0
"parser.add_argument('--base_model', type=str, help='Path to latest embedding model checkpoint.')",0
"parser.add_argument('--output_dir', type=str, help='Output directory for subset of crops')",0
Add json entry for this crop,0
Copy file for this crop to subset dataset crop dir,0
Copy file for its full-size source image to subset dataset image dir,0
Write crops.json to subset dataset crop dir,0
store info about the crops produced in a JSON file,0
------------------------------------------------------------------------------------------------------------#,0
COMMENT OUT IF NOT USING A SPECIFIC PROJECT WITHIN ROBERT LONG EMAMMAL DATASET,0
------------------------------------------------------------------------------------------------------------#,0
get some information about the source image,0
------------------------------------------------------------------------------------------------------------#,0
NOTE: EDIT THIS SECTION BASED ON DATASET SOURCE,0
get info about sequence the source image belongs to from path and directory,0
# missouricameratraps:,0
imgframenum = int(os.path.basename(imgfile).split('.JPG')[0].split('_')[-1]),0
imgseqid = int(os.path.split(os.path.dirname(imgfile))[-1]),0
"imgseqnumframes = len([name for name in os.listdir(os.path.dirname(imgfile)) if os.path.isfile(os.path.join(os.path.dirname(imgfile), name))])",0
# emammal:,0
------------------------------------------------------------------------------------------------------------#,0
utility function,0
compute output,0
measure accuracy and record loss,0
switch to train mode,0
measure accuracy and record loss,0
compute output,0
measure accuracy and record loss,0
switch to evaluate mode,0
compute output,0
switch to evaluate mode,0
compute output,0
switch to evaluate mode,0
"print(self.labels_set, self.n_classes)",0
Add all negatives for all positive pairs,0
print(triplets.shape[0]),0
constructor,0
update embedding values after a finetuning,0
select either the default or active pools,0
gather test set,0
gather train set,0
finetune the embedding model over the labeled pool,0
a utility function for saving the snapshot,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Variables for the hierarchical cluster,0
Variables for the AL algorithm,0
Data variables,0
"connectivity = kneighbors_graph(self.transformed_X,max_features)",0
Fit cluster and update cluster variables,0
The sklearn hierarchical clustering algo numbers leaves which correspond,0
to actual datapoints 0 to n_points - 1 and all internal nodes have,0
ids greater than n_points - 1 with the root having the highest node id,0
"If no labels have been observed, simply return uniform distribution",0
"If no observations, return worst possible upper lower bounds",0
Loop through generations from bottom to top,0
Update admissible labels for node,0
Calculate score,0
Determine if node should be split,0
Make sure label set for node so that we can flow to children,0
if necessary,0
Only split if all ancestors are admissible nodes,0
This is part  of definition of admissible pruning,0
Check that pruning covers all leave nodes,0
Fill in labels,0
Observe labels for previously recommended batches,0
TODO(lishal): implement multiple selection methods,1
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
A list of initialized samplers is allowed as an input because,0
"for AL_methods that search over different mixtures, may want mixtures to",0
have shared AL_methods so that initialization is only performed once for,0
computation intensive methods like HierarchicalClusteringAL and,0
states are shared between mixtures.,0
"If initialized samplers are not provided, initialize them ourselves.",0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copy these objects to make sure they are not modified while simulating,0
trajectories as they are used later by the main run_experiment script.,0
Assumes that model has already by fit using all labeled data so,0
the probabilities can be used immediately to hallucinate labels,0
All models need to have predict_proba method,0
Hallucinate labels for selected datapoints to be label,0
using class probabilities from model,0
"Not saving already_selected here, if saving then should sort",0
only for the input to fit but preserve ordering of indices in,0
already_selected,0
Useful to know how accuracy compares for model trained on hallucinated,0
labels vs trained on true labels.  But can remove this train to speed,0
up simulations.  Won't speed up significantly since many more models,0
are being trained inside the loop above.,0
Save trajectory for reference,0
Delete created copies,0
THE INPUTS CANNOT BE MODIFIED SO WE MAKE COPIES FOR THE CHECK LATER,0
Should check model but kernel_svm does not have coef_ so need better,0
handling here,0
Make sure that model object fed in did not change during simulations,0
Return indices based on return type specified,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Set gamma for gaussian kernel to be equal to 1/n_features,0
kneighbors graph is constructed using k=10,0
"Make connectivity matrix symmetric, if a point is a k nearest neighbor of",0
"another point, make it vice versa",0
Graph edges are weighted by applying gaussian kernel to manhattan dist.,0
"By default, gamma for rbf kernel is equal to 1/n_features but may",0
get better results if gamma is tuned.,0
Define graph density for an observation to be sum of weights for all,0
edges to the node representing the datapoint.  Normalize sum weights,0
by total number of neighbors.,0
"If a neighbor has already been sampled, reduce the graph density",0
for its direct neighbors to promote diversity.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Update observed reward and arm probabilities,0
Sample an arm,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
y only used for determining how many clusters there should be,0
probably not practical to assume we know # of classes before hand,0
should also probably scale with dimensionality of data,0
Probably okay to always use MiniBatchKMeans,0
Should standardize data before clustering,0
Can cluster on standardized data but train on raw features if desired,0
TODO(lishal): have MarginSampler and this share margin function,1
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Probably okay to always use MiniBatchKMeans,0
Should standardize data before clustering,0
Can cluster on standardized data but train on raw features if desired,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Update min_distances for all examples given new cluster center.,0
Assumes that the transform function takes in original data and not,0
flattened data.,0
Initialize centers with a randomly selected datapoint,0
New examples should not be in already selected since those points,0
should have min_distance of zero to a cluster center.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
This is uniform given the remaining pool but biased wrt the entire pool.,0
sample = [i for i in range(self.X.shape[0]) if i not in already_selected],0
return sample[0:N],0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Fields for hierarchical clustering AL,0
Setting parent and storing nodes in dict for fast access,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Faster and available in Python 3.5 and above,0
transform_list.append(CenterCrop((processed_size))),0
transform_list.append(Lambda(lambda X: normalize(X))),0
"print(self.labels_set, self.n_classes)",0
"from PyQt5 import QtCore, QtWidgets,QtGui",0
from collections import deque,0
from peewee import *,0
from UIComponents.Tag import Tag,0
policy.setHeightForWidth(True),0
"print(self.tab1.parentWidget(),self)",0
self.tab4.add.clicked.connect(self.addSpecies),0
self.tab4.update.clicked.connect(self.updateSpecies),0
checkpoint= load_checkpoint('../merge/triplet_model_0054.tar'),0
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))",0
"embedding_net = EmbeddingNet(checkpoint['arch'], checkpoint['feat_dim'])",0
if checkpoint['loss_type'].lower()=='center':,0
"model = torch.nn.DataParallel(ClassificationNet(embedding_net, n_classes=14)).cuda()",0
else:,0
model= torch.nn.DataParallel(embedding_net).cuda(),0
model.load_state_dict(checkpoint['state_dict']),0
self.parentWidget().progressBar.setMaximum(len(run_dataset)//2048),0
"e=Engine(model,None,None, verbose=True,progressBar= self.parentWidget().progressBar)",0
label = [ x[1] for x in run_dataset.samples],0
"print(indices,selected_set)",0
print(query.sql()),0
src.delete().where(src.image_id<<rList)),0
"det= UserDetection.create(category_id=0, id=str(index+label[1][2]),image_id=final[0], bbox_X=label[1][0], bbox_Y=label[1][1], bbox_W=label[1][2], bbox_H=label[1][3])",0
for x in self.tab1.grid.tags:,0
x.delete_instance(),0
db.create_tables([Detection]),0
This is simply to show the bar,0
"p = Process(target=ex.active, args=())",0
p.start(),0
p.join(),0
ex.active(),0
ex.centralWidget().setCurrentIndex(1),0
main(),0
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))",0
print(row),0
"unq_id= ""crops_""+str(uuid.uuid1())",0
"print(line,imageWidth,imageHeight)",0
"print(""%s,%s,%s,%0.6f,%0.6f,%0.6f,%0.6f,%0.6f""%(line[0], line[1],line[2],float(line[3]),topRel,leftRel,bottomRel,rightRel))",0
if not os.path.exists(dest):,0
os.mkdir(dest),0
raise,0
out.close(),0
"print length,(i-1)*length,i*length",0
matplotlib.use('Agg'),0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
"conf.reset()""""""",0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
conf.reset(),0
for clf in classifiers:,0
"clf.fit(X_train, y_train)",0
preds= clf.predict_proba(X),0
uncertainty+= preds.max(axis=1),0
print(uncertainty[ind]),0
print(uncertainty[ind]),0
true_labels = y[unlabeled_indices],0
select up to 5 digit examples that the classifier is most uncertain about,0
remember best acc@1 and save checkpoint,0
"completeClassificationLoop(run_dataset, model,num_classes)",0
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)",0
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd),0
labels = db.labels_,0
"mapp=(find_probablemap(label,labels, K=args.K))",0
"print(""Clusters"")",0
"for i,x in enumerate(labels):",0
labels[i]= mapp[x],0
print(np.sum(labels == label)/labels.size),0
"print(""Confidence Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning(embd, label, idx)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(""Entropy Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning_entropy(embd, label, idx)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(idx,idx.shape)",0
for i in idx:,0
print(paths[i]),0
"print(""Silohette active learning"")",0
"idx= active_learning2(embd, 1000, args.num_clusters)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
embd= reduce_dimensionality(embd)#[0:10000]),0
labels= labels[0:10000],0
label= label[0:10000],0
paths= paths[0:10000],0
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])",0
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])",0
plt.show(),0
"np.save(args.name_prefix+""_embeddings.npy"",embd)",0
"np.save(args.name_prefix+""_labels.npy"",label)",0
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")",0
del output,0
define loss function (criterion) and optimizer,0
"optimizer = torch.optim.Adam(params, lr=args.lr, weight_decay= args.weight_decay)",0
optimizer.load_state_dict(checkpoint['optimizer']),0
train for one epoch,0
evaluate on validation set,0
matplotlib.use('Agg'),0
selected_set.add(rand_ind[i]),0
"embedding_net = EmbeddingNet('resnet50', 256, True)",0
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)",0
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)",0
print('Embedding Done'),0
sys.stdout.flush(),0
Random examples to start,0
"random_ids = noveltySamples(unlabeled_dataset.em, unlabeled_dataset.getIDs(), 1000)",0
print(random_ids),0
Move Records,0
Finetune the embedding model,0
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)",0
unlabeled_dataset.updateEmbedding(model),0
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})",0
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()",0
matplotlib.use('Agg'),0
"uncertainty= np.apply_along_axis(stats.entropy,1,probs) * (1 - probs.max(axis=1))",0
"copy(paths[srt[i]], ""active"")",0
"plot_together( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()), preds, base_ind, dataset.getpaths(), {})",0
"return np.random.choice(range(0,prob_list[0].shape[0]), 100, replace=False).tolist()",0
selected_set.add(rand_ind[i]),0
"query= Detection.update(kind = destKind.value).where(Detection.id.in_(rList), Detection.kind == srcKind.value)",0
query.execute(),0
"embedding_net = EmbeddingNet('resnet50', 256, True)",0
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)",0
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)",0
print('Embedding Done'),0
sys.stdout.flush(),0
"plot_embedding(dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})",0
Random examples to start,0
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)",0
print(random_ids),0
Move Records,0
Finetune the embedding model,0
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)",0
unlabeled_dataset.updateEmbedding(model),0
"print(dataset.em[dataset.current_set].shape, np.asarray(dataset.getlabels()).shape, len(dataset.getpaths()))",0
"plot_embedding( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})",0
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})",0
train_eval_classifier(),0
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()",0
"names = [""Linear SVM"", ""RBF SVM"", ""Random Forest"", ""Neural Net"", ""Naive Bayes""]",0
"classifiers = [SVC(kernel=""linear"", C=0.025, probability= True, class_weight='balanced'),",0
"SVC(gamma=2, C=1, probability= True, class_weight='balanced'),",0
"RandomForestClassifier(max_depth=None, n_estimators=100, class_weight='balanced'),",0
"MLPClassifier(alpha=1),",0
GaussianNB()],0
estimators= [],0
"for name, clf in zip(names, classifiers):",0
"estimators.append((name, clf))",0
"eclf1 = VotingClassifier(estimators= estimators, voting='hard')",0
"eclf2 = VotingClassifier(estimators= estimators, voting='soft')",0
"names.append(""ensemble hard"")",0
classifiers.append(eclf1),0
"names.append(""ensemble soft"")",0
classifiers.append(eclf2),0
dataset.image_mode(),0
dataset.updateEmbedding(model),0
y_pred= clf.predict(X_test),0
"print(confusion_matrix(y_test, y_pred))",0
paths= dataset.getpaths(),0
"for i, (yp, yt) in enumerate(zip(y_pred, y_test)):",0
if yp != yt:,0
"copy(paths[i],""mistakes"")",0
"print(yt, yp, paths[i],i)",0
"clf_output= clf_e.embedding(eval_loader, dim=48)",0
"self.train_transform = transforms.Compose([Resize(raw_size), CenterCrop((processed_size)), ToTensor(), Normalize(mean, std)])",0
"print(self.labels_set, self.n_classes)",0
"print(line,imageWidth,imageHeight)",0
"print length,(i-1)*length,i*length",0
#########################################################,0
## Configuration,0
#########################################################,0
## The actual code,0
Check arguments,0
Create output directories,0
Padding around the detected objects when cropping,0
1.3 for the cropping during test time and 1.3 for,0
the context that the CNN requires in the left-over,0
image,0
Load a (frozen) Tensorflow model into memory.,0
## Preparations: get all the output tensors,0
The following processing is only for single image,0
Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.,0
Follow the convention by adding back the batch dimension,0
For all images in the image directoryig,0
Load image,0
Run inference,0
"all outputs are float32 numpy arrays, so convert types as appropriate",0
Add detections to the collection,0
Get info about the image,0
Select detections with a confidence larger than DETECTION_CONFIDENCE,0
Get these boxes and convert normalized coordinates to pixel coordinates,0
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes",0
"However, we need to make sure that it box coordinates are still within the image",0
"For each detected bounding box with high confidence, we will",0
crop the image to the padded box and save it,0
generate a unique identifier for the detection,0
"bbox is the detected box, crop_box the padded / enlarged box",0
Add numbering to the original file name if there are multiple boxes,0
The absolute file path where we will store the image,0
"if COCO_OUTPUT_DIR is set, then we will only use the shape",0
of cropped_img in the following code. So instead of reading,0
cropped_img = np.array(Image.open(out_file)),0
we can speed everything up by reading only the size of the image,0
matplotlib.use('Agg'),0
conf= ConfusionMatrix(24),0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
"conf.reset()""""""",0
define loss function (criterion) and optimizer,0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
conf.reset(),0
for clf in classifiers:,0
"clf.fit(X_train, y_train)",0
preds= clf.predict_proba(X),0
uncertainty+= preds.max(axis=1),0
print(uncertainty[ind]),0
print(uncertainty[ind]),0
true_labels = y[unlabeled_indices],0
select up to 5 digit examples that the classifier is most uncertain about,0
"print(indices,selected_set)",0
print(query.sql()),0
remember best acc@1 and save checkpoint,0
"completeClassificationLoop(run_dataset, model,num_classes)",0
"embd, label, paths = extract_embeddings(run_loader, model)",0
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)",0
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd),0
labels = db.labels_,0
"mapp=(find_probablemap(label,labels, K=args.K))",0
"print(""Clusters"")",0
"for i,x in enumerate(labels):",0
labels[i]= mapp[x],0
print(np.sum(labels == label)/labels.size),0
"print(""Confidence Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning(embd, label, idx)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(""Entropy Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning_entropy(embd, label, idx)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"new_selected= selectSamples(embd,paths,3000)",0
"print(idx,idx.shape)",0
for i in idx:,0
print(paths[i]),0
"print(""Silohette active learning"")",0
"idx= active_learning2(embd, 1000, args.num_clusters)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(""Random"")",0
"idx = np.random.choice(np.arange(len(paths)), 1000, replace=False)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
embd= reduce_dimensionality(embd)#[0:10000]),0
labels= labels[0:10000],0
label= label[0:10000],0
paths= paths[0:10000],0
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])",0
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])",0
plt.show(),0
"np.save(args.name_prefix+""_embeddings.npy"",embd)",0
"np.save(args.name_prefix+""_labels.npy"",label)",0
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")",0
combo.setEnabled(not finalized),0
print(self.bbox),0
self.addWidget(self.child),0
Moving container with arrows,0
Left - Bottom,0
Right-Bottom,0
Left-Top,0
Right-Top,0
Left - Bottom,0
Right - Bottom,0
Left - Top,0
Right - Top,0
check cursor horizontal position,0
check cursor vertical position,0
self.resizeEvent=self.onResize,0
"print(""Parent"", parent, parent.width(), parent.height())",0
"self.setGeometry(0,0,410,307)",0
"print(w,h,""w,h"")",0
"print(""final"",tag.getFinal())",0
pass,0
"self.tags.append(TContainer(self,Category.get(-1),[0,0,0.1,0.1],True, Qt.red))",0
label= CharField(),0
fullname=str(self.model),0
"self.name= (fullname[fullname.find("":"")+2:fullname.find("">"")].strip()+'_set').lower()",0
"print(""Parent"", self.parentWidget().width(), self.parentWidget().height())",0
"print(self.model,self.name,query.sql())",0
self.tab4.speciesList.setModel(species),0
"self.tab4.speciesList.setRowHidden(len(species.stringList())-1, True)",0
self.speciesList.itemChanged.connect(self.itemChanged),0
###############,0
,0
iwildcam_dataset.py,0
,0
Loader for the iWildCam detection data set.,0
,0
###############,0
loads the taxonomy data and converts to ints,0
set up dummy data,0
create a dictionary of lists containing taxonomic labels,0
set up the filenames and annotations,0
This loop reads the bboxes and corresponding labels and assigns them,0
the correct image. Kind of slow at the moment...,0
"To speed up the loop, creating mapping for image_id to list index",0
"check that the image contains an animal, if not, don't append a box or label to the",0
image list,0
"Bboxes should have ('ymin', 'xmin', 'ymax', 'xmax') format",0
"Currently we take the label from the annotation file, non-consecutive-",0
label-support would be great,0
"self.bboxes[idx].append([-1.,-1.,0.,0.])",0
self.labels[idx].append(30),0
load classes,0
print out some stats,0
To make sure we loaded the bboxes correctly:,0
self.validate_bboxes(),0
Make sure all are greater equal 0,0
We have to add 1 as the framework assumes that labels start from 0,0
print(bboxes),0
###############,0
,0
util.py,0
,0
Image utilities used in the FasterRCNN framework.,0
,0
###############,0
"reshape (H, W) -> (1, H, W)",0
"transpose (H, W, C) -> (C, H, W)",0
pass in list of files,0
print(im_id),0
print(images[im_id]['seq_id']),0
get unique colors in segmentation,0
get box for each color,0
this is the background class,0
get a box around this color,0
"x1,y1 is top left corner, x2,y2 is bottom right corner",0
create coco-style json,0
In settings.json first activate computer vision mode:,0
https://github.com/Microsoft/AirSim/blob/master/docs/image_apis.md#computer-vision-mode,0
import setup_path,0
load animal class name lookup,0
set segmentation values for everything to 0,0
set segmentation for each animal to a different value,0
"client.simSetCameraOrientation(""0"", airsim.to_quaternion(-0.161799, 0, 0)); #radians",0
pose = client.simGetVehiclePose(),0
pp.pprint(pose),0
print('Pose ' + str(cam_num)),0
print(pose),0
"print(""Type %d, size %d, pos %s"" % (response.image_type, len(response.image_data_float), pprint.pformat(response.camera_position)))",0
"print(""Type %d, size %d, pos %s"" % (response.image_type, len(response.image_data_uint8), pprint.pformat(response.camera_position)))",0
pose = client.simGetVehiclePose(),0
pp.pprint(pose),0
currently reset() doesn't work in CV mode. Below is the workaround,1
"client.simSetPose(airsim.Pose(airsim.Vector3r(0, 0, 0), airsim.to_quaternion(0, 0, 0)), True)",0
environment_lookup = {},0
print(len(list(env_list))),0
save environment dict every time so you don't lose the info if airsim crashes,0
#####,0
,0
video_utils.py,0
,0
"Utilities for splitting, rendering, and assembling videos.",0
,0
#####,0
"%% Constants, imports, environment",0
from ai4eutils,0
%% Path utilities,0
%% Function for rendering frames to video and vice-versa,0
http://tsaith.github.io/combine-images-into-a-video-with-python-3-and-opencv-3.html,0
Determine the width and height from the first image,0
Define the codec and create VideoWriter object,0
"If we're not over-writing, check whether all frame images already exist",0
"OpenCV seems to over-report the number of frames by 1 in some cases, or fails",0
"to read the last frame; either way, I'm allowing one missing frame.",0
"print(""Rendering video {}, couldn't find frame {}"".format(",0
"input_video_file,missing_frame_number))",0
...if we need to check whether to skip this video entirely,0
"for frame_number in tqdm(range(0,n_frames)):",0
print('Skipping frame {}'.format(frame_filename)),0
Recursively enumerate video files,0
Create the target output folder,0
Render frames,0
input_video_file = input_fn_absolute; output_folder = output_folder_video,0
For each video,0
,0
input_fn_relative = input_files_relative_paths[0],0
"process_detection_with_options = partial(process_detection, options=options)",0
zero-indexed,0
Load results,0
# Break into videos,0
im = images[0],0
# For each video...,0
video_name = list(video_to_frames.keys())[0],0
frame = frames[0],0
At most one detection for each category for the whole video,0
category_id = list(detection_categories.keys())[0],0
Find the nth-highest-confidence video to choose a confidence value,0
Prepare the output representation for this video,0
...for each video,0
Write the output file,0
%%,0
%% Test driver,0
%% Constants,0
%% Split videos into frames,0
"%% List image files, break into folders",0
Find unique folders,0
fn = frame_files[0],0
%% Load detector output,0
%% Render detector frames,0
folder = list(folders)[0],0
d = detection_results_this_folder[0],0
...for each file in this folder,0
...for each folder,0
%% Render output videos,0
folder = list(folders)[0],0
...for each video,0
All on the 1212-image test subset,0
"%% Constants, imports, environment",0
from multiprocessing.pool import ThreadPool as workerpool,0
Number of images to pre-fetch,0
Useful hack to force CPU inference.,1
,0
"Need to do this before any PT/TF imports, which happen when we import",0
from run_detector.,0
Numpy FutureWarnings from tensorflow import,0
%% Support functions for multiprocessing,0
TODO,1
,0
The queue system is a little more elegant if we start one thread for reading and one,1
"for processing, and this works fine on Windows, but because we import TF at module load,",0
"CUDA will only work in the main process, so currently the consumer function runs here.",0
,0
"To enable proper multi-GPU support, we may need to move the TF import to a separate module",0
that isn't loaded until very close to where inference actually happens.,0
%% Other support funtions,0
%% Image processing functions,0
%% Main function,0
Load the detector,0
Does not count those already processed,0
Will not add additional entries not in the starter checkpoint,0
Write a checkpoint if necessary,0
Back up any previous checkpoints,0
Write the new checkpoint,0
Remove the backup checkpoint if it exists,0
...if it's time to make a checkpoint,0
"When using multiprocessing, let the workers load the model",0
"Results may have been modified in place, but we also return it for",0
backwards-compatibility.,0
%% Interactive driver,0
%%,0
image_file_names = image_file_names[0:2],0
"python run_detector_batch.py ""g:\temp\models\md_v4.1.0.pb"" ""g:\temp\demo_images\ssmini"" ""g:\temp\ssmini.json"" --recursive --output_relative_filenames --use_image_queue",0
%% Command-line driver,0
Load the checkpoint if available,0
,0
Relative file names are only output at the end; all file paths in the checkpoint are,0
still full paths.,0
"Find the images to score; images can be a directory, may need to recurse",0
A json list of image paths,0
A single image file,0
Test that we can write to the output_file's dir if checkpointing requested,0
"Confirm that we can write to the checkpoint path, rather than failing after 10000 images",0
%% Imports,0
import pre- and post-processing functions from the YOLOv5 repo https://github.com/ultralytics/yolov5,0
%% Classes,0
padded resize,0
NMS,0
format detections/bounding boxes,0
Rescale boxes from img_size to im0 size,0
"normalized center-x, center-y, width and height",0
"MegaDetector output format's categories start at 1, but this model's start at 0",0
for testing,0
"MegaDetector was trained with batch size of 1, and the resizing function is a part",0
of the inference graph,0
"change from [y1, x1, y2, x2] to [x1, y1, width, height]",0
convert numpy floats to Python floats,0
need to change the above line to the following if supporting a batch size > 1 and resizing to the same size,0
"np_images = [np.asarray(image, np.uint8) for image in images]",0
"images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)",0
performs inference,0
our batch size is 1; need to loop the batch dim if supporting batch size > 1,0
"%% Constants, imports, environment",0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Numpy FutureWarnings from tensorflow import,0
Useful hack to force CPU inference,1
os.environ['CUDA_VISIBLE_DEVICES'] = '-1',0
An enumeration of failure reasons,0
Number of decimal places to round to for confidence and bbox coordinates,0
Label mapping for MegaDetector,0
"Each version of the detector is associated with some ""typical"" values",0
"that are included in output files, so that downstream applications can",0
use them as defaults.,0
%% Classes,0
Stick this into filenames before the extension for the rendered result,0
%% Utility functions,0
%% Main function,0
Dictionary mapping output file names to a collision-avoidance count.,0
,0
"Since we'll be writing a bunch of files to the same folder, we rename",0
as necessary to avoid collisions.,0
image is modified in place,0
...for each image,0
%% Command-line driver,0
"but for a single image, args.image_dir is also None",0
%% Interactive driver,0
%%,0
#####,0
,0
process_video.py,0
,0
"Split a video into frames, run the frames through run_tf_detector_batch.py, and",0
optionally stitch together results into a new video with detection boxes.,0
,0
#####,0
"%% Constants, imports, environment",0
%% Main function,0
Render detections to images,0
Combine into a video,0
Delete the temporary directory we used for detection images,0
(Optionally) delete the frames on which we ran MegaDetector,0
# Validate options,0
# Split every video into frames,0
# Run MegaDetector,0
# Convert frame-level results to video-level results,0
%% Interactive driver,0
%% Process a folder of videos,0
process_video_folder(options),0
import clipboard; clipboard.copy(cmd),0
%% Process a single video,0
"python process_video.py ""c:\temp\models\md_v4.0.0.pb"" ""c:\temp\LIFT0003.MP4"" --debug_max_frames=10 --render_output_video=True",0
%% For a video that's already been run through MD,0
im = d['images'][0],0
...for each detection,0
Split into frames,0
Render output video,0
# Render detections to images,0
# Combine into a video,0
%% Command-line driver,0
Lint as: python3,0
Copyright 2020 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TPU is automatically inferred if tpu_name is None and,0
we are running under cloud ai-platform.,0
,0
copy_checkpoints.py,0
,0
Run this script with specified source_dir and target_dir while the model is training to make a copy,0
of every checkpoint (checkpoints are kept once an hour by default and is difficult to adjust),0
,0
do not copy event or evaluation results,0
keys are categories (int),0
"we force *_boxes to have shape [N, 4], even in case that N = 0",0
"labels input to compute_object_detection_metrics() needs to start at 0, not 1",0
num_detections = len(dets['boxes']),0
place holders - we don't have these,0
true positives < gt of that category,0
compute one-class precision/recall/average precision (if every box is just,0
of an object class),0
"iterate through each image in the gt file, not the detection file",0
ground truth,0
convert gt box coordinates to TFODAPI format,0
detections,0
only include a detection entry if that image had detections,0
minus the 'one_class' set of metrics,0
,0
api_frontend.py,0
,0
"Defines the Flask app, which takes requests (one or more images) from",0
"remote callers and pushes the images onto the shared Redis queue, to be processed",0
by the main service in api_backend.py .,0
,0
%% Imports,0
%% Initialization,0
%% Support functions,0
Make a dict that the request_processing_function can return to the endpoint,0
function to notify it of an error,0
Verify that the content uploaded is not too big,0
,0
request.content_length is the length of the total payload,0
Verify that the number of images is acceptable,0
...def check_posted_data(request),0
%% Main loop,0
Check whether the request_processing_function had an error,0
Write images to temporary files,0
,0
TODO: read from memory rather than using intermediate files,1
"Submit the image(s) for processing by api_backend.py, who is waiting on this queue",0
TODO: convert to a blocking read and eliminate the sleep() statement in this loop,1
"image = Image.open(os.path.join(temp_direc, image_name))",0
...if we do/don't have a request available on the queue,0
...while(True),0
...def detect_sync(),0
%% Command-line driver,0
use --non-docker if you are testing without Docker,0
,0
python api_frontend.py --non-docker,0
# Camera trap real-time API configuration,0
"Full path to the temporary folder for image storage, only meaningful",0
within the Docker container,0
Upper limit on total content length (all images and parameters),0
Minimum confidence threshold for detections,0
Minimum confidence threshold for showing a bounding box on the output image,0
Use this when testing without Docker,0
,0
api_backend.py,0
,0
"Defines the model execution service, which pulls requests (one or more images)",0
"from the shared Redis queue, and runs them through the TF model.",0
,0
%% Imports,0
%% Initialization,0
%% Main loop,0
TODO: convert to a blocking read and eliminate the sleep() statement in this loop,1
Filter the detections by the confidence threshold,0
,0
"Each result is [ymin, xmin, ymax, xmax, confidence, category]",0
,0
"Coordinates are relative, with the origin in the upper-left",0
...if serialized_entry,0
...while(True),0
...def detect_process(),0
%% Command-line driver,0
use --non-docker if you are testing without Docker,0
,0
python api_frontend.py --non-docker,0
run detections on a test image to load the model,0
"part is a BodyPart object with b'Content-Type', and b'Content-Disposition', the later includes 'name' and 'filename' info",0
read the images anew for each request,0
images are read and in each request by the time we call the API in map(),0
input validation,0
plot the images,0
adjust the figure,0
"read in dataset CSV and create merged (dataset, location) col",0
map label to label_index,0
load the splits,0
only weight the training set by detection confidence,0
TODO: consider weighting val and test set as well,1
isotonic regression calibration of MegaDetector confidence,0
treat each split separately,0
new_weight[i] = confidence[i] * (n / c) / total_confidence(i's label),0
- n = # examples in split (weighted by confidence); c = # labels,0
- weight allocated to each label is n/c,0
"- within each label, weigh each example proportional to confidence",0
- new weights sum to n,0
error checking,0
"maps output label name to set of (dataset, dataset_label) tuples",0
find which other label (label_b) has intersection,0
input validation,0
create label index JSON,0
look into sklearn.preprocessing.MultiLabelBinarizer,0
Note: JSON always saves keys as strings!,0
"True for ground truth, False for MegaDetector",0
always save as .jpg for consistency,0
get bounding boxes,0
convert from category ID to category name,0
"check if crops are already downloaded, and ignore bboxes below the",0
confidence threshold,0
assign all images without location info to 'unknown_location',0
remove images from labels that have fewer than min_locs locations,0
merge dataset and location into a single string '<dataset>/<location>',0
"create DataFrame of counts. rows = locations, columns = labels",0
label_count: label => number of examples,0
loc_count: label => number of locs containing that label,0
generate a new split,0
score the split,0
SSE for # of images per label (with 2x weight),0
SSE for # of locs per label,0
label => list of datasets to prioritize for test and validation sets,0
"merge dataset and location into a tuple (dataset, location)",0
sorted smallest to largest,0
greedily add to test set until it has >= 15% of images,0
sort the resulting locs,0
"modify loc_to_size in place, so copy its keys before iterating",0
arguments relevant to both creating the dataset CSV and splits.json,0
arguments only relevant for creating the dataset CSV,0
arguments only relevant for creating the splits JSON,0
comment lines starting with '#' are allowed,0
,0
prepare_classification_script.py,0
,0
Notebook-y script used to prepare a series of shell commands to run a classifier,0
(other than MegaClassifier) on a MegaDetector result set.,0
,0
Differs from prepare_classification_script_mc.py only in the final class mapping step.,0
,0
%% Job options,0
%% Constants,0
"This is just passed along to the metadata in the output file, it has no impact",0
on how the classification scripts run.,0
%% Set up environment,0
commands.append('cd CameraTraps/classification\n'),0
commands.append('conda activate cameratraps-classifier\n'),0
%% Crop images,0
fn = input_files[0],0
%% Run classifier,0
fn = input_files[0],0
%% Merge classification and detection outputs,0
fn = input_files[0],0
%% Write everything out,0
input validation,0
evaluating with accimage is much faster than Pillow or Pillow-SIMD,0
create output directory,0
override saved params with kwargs,0
"For now, we don't weight crops by detection confidence during",0
evaluation. But consider changing this.,0
"create model, compile with TorchScript if given checkpoint is not compiled",0
"verify that target names matches original ""label names"" from dataset",0
"if the dataset does not already have a 'other' category, then the",0
'other' category must come last in label_names to avoid conflicting,0
with an existing label_id,0
define loss function (criterion),0
"this file ends up being huge, so we GZIP compress it",0
double check that the accuracy metrics are computed properly,0
save the confusion matrices to .npz,0
save per-label statistics,0
set dropout and BN layers to eval mode,0
"even if batch contains sample weights, don't use them",0
Do target mapping on the outputs (unnormalized logits) instead of,0
"the normalized (softmax) probabilities, because the loss function",0
uses unnormalized logits. Summing probabilities is equivalent to,0
log-sum-exp of unnormalized logits.,0
"a confusion matrix C is such that C[i,j] is the # of observations known to",0
be in group i and predicted to be in group j.,0
match pytorch EfficientNet model names,0
images dataset,0
"for smaller disk / memory usage, we cache the raw JPEG bytes instead",0
of the decoded Tensor,0
convert JPEG bytes to a 3D uint8 Tensor,0
"keras EfficientNet already includes normalization from [0, 255] to [0, 1],",0
so we don't need to do that here,0
labels dataset,0
img_files dataset,0
weights dataset,0
define the transforms,0
efficientnet data preprocessing:,0
- train:,0
"1) random crop: aspect_ratio_range=(0.75, 1.33), area_range=(0.08, 1.0)",0
2) bicubic resize to img_size,0
3) random horizontal flip,0
- test:,0
1) center crop,0
2) bicubic resize to img_size,0
weights sums to:,0
- if weight_by_detection_conf: (# images in split - conf delta),0
- otherwise: (# images in split),0
"freeze the base model's weights, including BatchNorm statistics",0
https://www.tensorflow.org/guide/keras/transfer_learning#fine-tuning,0
rebuild output,0
input validation,0
set seed,0
create logdir and save params,0
create dataloaders and log the index_to_label mapping,0
define loss function and optimizer,0
using EfficientNet training defaults,0
- batch norm momentum: 0.99,0
"- optimizer: RMSProp, decay 0.9 and momentum 0.9",0
- epochs: 350,0
"- learning rate: 0.256, decays by 0.97 every 2.4 epochs",0
- weight decay: 1e-5,0
TODO: change weighted to False if oversampling minority classes,1
stop training after 8 epochs without improvement,0
log metrics,0
log confusion matrix,0
log tp/fp/fn images,0
"tf.summary.image requires input of shape [N, H, W, C]",0
false positive for top3_pred[0],0
false negative for label,0
"if evaluating or finetuning, set dropout & BN layers to eval mode",0
"for each label, track 5 most-confident and least-confident examples",0
"even if batch contains sample weights, don't use them",0
we do not track L2-regularization loss in the loss metric,0
This dictionary will get written out at the end of this process; store,0
diagnostic variables here,0
error checking,0
refresh detection cache,0
save log of bad images,0
cache of Detector outputs: dataset name => {img_path => detection_dict},0
img_path: <dataset-name>/<img-filename>,0
get SAS URL for images container,0
strip image paths of dataset name,0
save list of dataset names and task IDs for resuming,0
complete task name: 'detect_for_classifier_caltech_20200722_110816_task01',0
HACK! Sleep for 10s between task submissions in the hopes that it,1
"decreases the chance of backend JSON ""database"" corruption",0
task still running => continue,0
"task finished successfully, save response to disk",0
error checking before we download and crop any images,0
convert from category ID to category name,0
we need the datasets table for getting SAS keys,0
"we already did all error checking above, so we don't do any here",0
get ContainerClient,0
get bounding boxes,0
we must include the dataset <ds> in <crop_path_template> because,0
'{img_path}' actually gets populated with <img_file> in,0
load_and_crop(),0
"get the image, either from disk or from Blob Storage",0
inelegant way to close the container_clients,0
,0
prepare_classification_script_mc.py,0
,0
Notebook-y script used to prepare a series of shell commands to run MegaClassifier,0
on a MegaDetector result set.,0
,0
Differs from prepare_classification_script.py only in the final class mapping step.,0
,0
%% Job options,0
%% Constants,0
"This is just passed along to the metadata in the output file, it has no impact",0
on how the classification scripts run.,0
%% Set up environment,0
commands.append('cd CameraTraps/classification\n'),0
commands.append('conda activate cameratraps-classifier\n'),0
%% Crop images,0
fn = input_files[0],0
%% Run classifier,0
fn = input_files[0],0
%% Remap classifier outputs,0
fn = input_files[0],0
%% Merge classification and detection outputs,0
fn = input_files[0],0
%% Write everything out,0
"accimage backend is faster than Pillow/Pillow-SIMD, but occasionally crashes",0
tv.set_image_backend('accimage'),0
mean/std values from https://pytorch.org/docs/stable/torchvision/models.html,0
define the transforms,0
resizes smaller edge to img_size,0
weights sums to:,0
- if weight_by_detection_conf: (# images in split - conf delta),0
- otherwise: # images in split,0
for normal (non-weighted) shuffling,0
set all parameters to not require gradients except final FC layer,0
replace final fully-connected layer (which has 1000 ImageNet classes),0
"detect GPU, use all if available",0
input validation,0
set seed,0
create logdir and save params,0
create dataloaders and log the index_to_label mapping,0
create model,0
define loss function and optimizer,0
using EfficientNet training defaults,0
- batch norm momentum: 0.99,0
"- optimizer: RMSProp, decay 0.9 and momentum 0.9",0
- epochs: 350,0
"- learning rate: 0.256, decays by 0.97 every 2.4 epochs",0
- weight decay: 1e-5,0
stop training after 8 epochs without improvement,0
do a complete evaluation run,0
log metrics,0
log confusion matrix,0
log tp/fp/fn images,0
"for every image: undo normalization, clamp to [0, 1], CHW -> HWC",0
"- cannot be in-place, because the HeapItem might be in multiple heaps",0
writer.add_figure() has issues => using add_image() instead,0
"writer.add_figure(f'{label_name}/{tag}', fig, global_step=epoch)",0
false positive for top3_pred[0],0
false negative for label,0
"preds and labels both have shape [N, k]",0
"if evaluating or finetuning, set dropout and BN layers to eval mode",0
"for each label, track k_extreme most-confident and least-confident images",0
"even if batch contains sample weights, don't use them",0
lock before changing ImageFile.LOAD_TRUNCATED_IMAGES,0
"filter out confidences below the threshold, and set precision to 4",0
sort from highest to lowest probability,0
input validation,0
load classification CSV,0
load label names,0
input validation,0
extract dataset name from crop path so we can process 1 dataset at a time,0
randomly sample images for each class,0
"load queried images JSON, needed for ground-truth bbox info",0
compare info dicts,0
compare detection categories,0
crop_path: <dataset>/<img_file>___cropXX_mdvY.Y.jpg,0
[----<img_path>----]       [-<suffix>--],0
file has detection entry,0
bounding box is from ground truth,0
crop_path: <dataset>/<img_file>___cropXX_mdvY.Y.jpg,0
[----<img_path>----]       [-<suffix>--],0
input validation,0
use MegaDB to generate list of images,0
only keep images that:,0
"1) end in a supported file extension, and",0
2) actually exist in Azure Blob Storage,0
3) belong to a label with at least min_locs locations,0
write out log of images / labels that were removed,0
"save label counts, pre-subsampling",0
"save label counts, post-subsampling",0
spec_dict['taxa']: list of dict,0
[,0
"{'level': 'family', 'name': 'cervidae', 'datasets': ['idfg']},",0
"{'level': 'genus',  'name': 'meleagris'}",0
],0
"spec_dict['dataset_labels']: dict, dataset => list of dataset_label",0
{,0
"""idfg"": [""deer"", ""elk"", ""prong""],",0
"""idfg_swwlf_2019"": [""elk"", ""muledeer"", ""whitetaileddeer""]",0
},0
"maps output label name to set of (dataset, dataset_label) tuples",0
"because MegaDB is organized by dataset, we do the same",0
ds_to_labels = {,0
'dataset_name': {,0
"'dataset_label': [output_label1, output_label2]",0
},0
},0
we need the datasets table for getting full image paths,0
The line,0
"[img.class[0], seq.class[0]][0] as class",0
selects the image-level class label if available. Otherwise it selects the,0
"sequence-level class label. This line assumes the following conditions,",0
expressed in the WHERE clause:,0
- at least one of the image or sequence class label is given,0
- the image and sequence class labels are arrays with length at most 1,0
- the image class label takes priority over the sequence class label,0
,0
"In Azure Cosmos DB, if a field is not defined, then it is simply excluded",0
"from the result. For example, on the following JSON object,",0
{,0
"""dataset"": ""camera_traps"",",0
"""seq_id"": ""1234"",",0
"""location"": ""A1"",",0
"""images"": [{""file"": ""abcd.jpeg""}],",0
"""class"": [""deer""],",0
},0
"the array [img.class[0], seq.class[0]] just gives ['deer'] because",0
img.class is undefined and therefore excluded.,0
"if no path prefix, set it to the empty string '', because",0
"os.path.join('', x, '') = '{x}/'",0
result keys,0
"- already has: ['dataset', 'location', 'file', 'class', 'bbox']",0
"- add ['label'], remove ['file']",0
"if img is mislabeled, but we don't know the correct class, skip it",0
"otherwise, update the img with the correct class, but skip the",0
img if the correct class is not one we queried for,0
sort keys for determinism,0
we need the datasets table for getting SAS keys,0
strip leading '?' from SAS token,0
only check Azure Blob Storage,0
check local directory first before checking Azure Blob Storage,0
1st pass: populate label_to_locs,0
"label (tuple of str) => set of (dataset, location)",0
2nd pass: eliminate bad images,0
prioritize is a list of prioritization levels,0
number of already matching images,0
main(,0
"label_spec_json_path='idfg_classes.json',",0
"taxonomy_csv_path='../../camera-traps-private/camera_trap_taxonomy_mapping.csv',",0
"output_dir='run_idfg',",0
json_indent=4),0
recursively find all files in cropped_images_dir,0
only find crops of images from detections JSON,0
resizes smaller edge to img_size,0
evaluating with accimage is much faster than Pillow or Pillow-SIMD,0
create dataset,0
create model,0
set dropout and BN layers to eval mode,0
load files,0
dataset => set of img_file,0
crop_path: <dataset>/<img_file>___cropXX_mdvY.Y.jpg,0
[----<img_path>----],0
task finished successfully,0
parse the task ID,0
print info about missing and failed images,0
get the detections,0
add detections to the detections cache,0
combine detections with cache,0
write combined detections back out to cache,0
error checking,0
any row with 'correct_class' should be marked 'mislabeled',0
filter to only the mislabeled rows,0
convert '\' to '/',0
verify that overlapping indices are the same,0
"""add"" any new mislabelings",0
write out results,0
error checking,0
load detections JSON,0
get detector version,0
convert from category ID to category name,0
copy keys to modify dict in-place,0
This will be removed later when we filter for animals,0
save log of bad images,0
"True for ground truth, False for MegaDetector",0
always save as .jpg for consistency,0
"we already did all error checking above, so we don't do any here",0
"get the image, either from disk or from Blob Storage",0
inelegant way to close the container_client,0
"crop_path => normalized bbox coordinates [xmin, ymin, width, height]",0
"only ground-truth bboxes do not have a ""confidence"" value",0
try loading image from local directory,0
try to download image from Blob Storage,0
crop the image,0
"expand box width or height to be square, but limit to img size",0
"Image.crop() takes box=[left, upper, right, lower]",0
pad to square using 0s,0
Author: lukemelas (github username),0
Github repo: https://github.com/lukemelas/EfficientNet-PyTorch,0
With adjustments and added comments by workingcoder (github username).,0
Support the construction of 'efficientnet-l2' without pretrained weights,0
Expansion phase (Inverted Bottleneck),0
"image_size = calculate_output_image_size(image_size, 1) <-- this wouldn't modify image_size",0
Depthwise convolution phase,0
"Squeeze and Excitation layer, if desired",0
Pointwise convolution phase,0
Expansion and Depthwise Convolution,0
Squeeze and Excitation,0
Pointwise Convolution,0
Skip connection and drop connect,0
The combination of skip connection and drop connect brings about stochastic depth.,0
Batch norm parameters,0
Get stem static or dynamic convolution depending on image size,0
Stem,0
Build blocks,0
Update block input and output filters based on depth multiplier.,0
The first block needs to take care of stride and filter size increase.,0
"image_size = calculate_output_image_size(image_size, block_args.stride)  # stride = 1",0
Head,0
Final linear layer,0
Stem,0
Blocks,0
Head,0
Stem,0
Blocks,0
Head,0
Convolution layers,0
Pooling and final linear layer,0
Author: lukemelas (github username),0
Github repo: https://github.com/lukemelas/EfficientNet-PyTorch,0
With adjustments and added comments by workingcoder (github username).,0
###############################################################################,0
## Help functions for model architecture,0
###############################################################################,0
GlobalParams and BlockArgs: Two namedtuples,0
Swish and MemoryEfficientSwish: Two implementations of the method,0
round_filters and round_repeats:,0
Functions to calculate params for scaling model width and depth ! ! !,0
get_width_and_height_from_size and calculate_output_image_size,0
drop_connect: A structural design,0
get_same_padding_conv2d:,0
Conv2dDynamicSamePadding,0
Conv2dStaticSamePadding,0
get_same_padding_maxPool2d:,0
MaxPool2dDynamicSamePadding,0
MaxPool2dStaticSamePadding,0
"It's an additional function, not used in EfficientNet,",0
but can be used in other model (such as EfficientDet).,0
"Parameters for the entire model (stem, all blocks, and head)",0
Parameters for an individual model block,0
Set GlobalParams and BlockArgs's defaults,0
An ordinary implementation of Swish function,0
A memory-efficient implementation of Swish function,0
TODO: modify the params names.,1
"maybe the names (width_divisor,min_width)",0
"are more suitable than (depth_divisor,min_depth).",0
follow the formula transferred from official TensorFlow implementation,0
follow the formula transferred from official TensorFlow implementation,0
"generate binary_tensor mask according to probability (p for 0, 1-p for 1)",0
Note:,0
The following 'SamePadding' functions make output size equal ceil(input size/stride).,0
"Only when stride equals 1, can the output size be the same as input size.",0
Don't be confused by their function names ! ! !,0
Tips for 'SAME' mode padding.,0
Given the following:,0
i: width or height,0
s: stride,0
k: kernel size,0
d: dilation,0
p: padding,0
Output after Conv2d:,0
o = floor((i+p-((k-1)*d+1))/s+1),0
"If o equals i, i = floor((i+p-((k-1)*d+1))/s+1),",0
=> p = (i-1)*s+((k-1)*d+1)-i,0
With the same calculation as Conv2dDynamicSamePadding,0
Calculate padding based on image size and save it,0
Calculate padding based on image size and save it,0
###############################################################################,0
## Helper functions for loading model params,0
###############################################################################,0
BlockDecoder: A Class for encoding and decoding BlockArgs,0
efficientnet_params: A function to query compound coefficient,0
get_model_params and efficientnet:,0
Functions to get BlockArgs and GlobalParams for efficientnet,0
url_map and url_map_advprop: Dicts of url_map for pretrained weights,0
load_pretrained_weights: A function to load pretrained weights,0
Check stride,0
"Coefficients:   width,depth,res,dropout",0
Blocks args for the whole model(efficientnet-b0 by default),0
It will be modified in the construction of EfficientNet Class according to model,0
note: all models have drop connect rate = 0.2,0
ValueError will be raised here if override_params has fields not included in global_params.,0
train with Standard methods,0
check more details in paper(EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks),0
train with Adversarial Examples(AdvProp),0
check more details in paper(Adversarial Examples Improve Image Recognition),0
TODO: add the petrained weights url map of 'efficientnet-l2',1
AutoAugment or Advprop (different preprocessing),0
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils),0
from data_management.megadb.schema import sequences_schema_check,0
resize is for displaying them more quickly,0
dataset and seq_id are required fields,0
sort the images in the sequence,0
"if no class label on the image, show class label on the sequence",0
pool = ThreadPool(),0
"print('len of rendering_info', len(rendering_info))",0
"tqdm(pool.imap_unordered(render_image_info_partial, rendering_info), total=len(rendering_info))",0
options = write_html_image_list(),0
options['headerHtml'],0
print('Checking that the MegaDB entries conform to the schema...'),0
sequences_schema_check.sequences_schema_check(sequences),0
#######,0
,0
visualize_db.py,0
,0
Outputs an HTML page visualizing annotations (class labels and/or bounding boxes),0
on a sample of images in a database in the COCO Camera Traps format,0
,0
#######,0
%% Imports,0
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils),0
Assumes the cameratraps repo root is on the path,0
%% Settings,0
Set to None to visualize all images,0
Target size for rendering; set either dimension to -1 to preserve aspect ratio,0
"These are mutually exclusive; both are category names, not IDs",0
"Special tag used to say ""show me all images with multiple categories""",0
We sometimes flatten image directories by replacing a path separator with,0
another character.  Leave blank for the typical case where this isn't necessary.,0
Control rendering parallelization,0
%% Helper functions,0
"Translate the file name in an image entry in the json database to a path, possibly doing",0
some manipulation of path separators,0
%% Core functions,0
"Optionally remove all images without bounding boxes, *before* sampling",0
"Optionally include/remove images with specific labels, *before* sampling",0
Put the annotations in a dataframe so we can select all annotations for a given image,0
Construct label map,0
Take a sample of images,0
Set of dicts representing inputs to render_db_bounding_boxes:,0
,0
"bboxes, boxClasses, image_path",0
iImage = 0,0
All the class labels we've seen for this image (with out without bboxes),0
Iterate over annotations for this image,0
iAnn = 0; anno = annos_i.iloc[iAnn],0
"We're adding html for an image before we render it, so it's possible this image will",0
fail to render.  For applications where this script is being used to debua a database,0
"(the common case?), this is useful behavior, for other applications, this is annoying.",0
,0
TODO: optionally write html only for images where rendering succeeded,1
...for each image,0
...def render_image_info,0
def process_images(...),0
%% Command-line driver,0
"Copy all fields from a Namespace (i.e., the output from parse_args) to an object.",0
,0
Skips fields starting with _.  Does not check existence in the target object.,0
Convert to an options object,0
%% Interactive driver(s),0
%%,0
os.startfile(htmlOutputFile),0
%% Constants and imports,0
convert category ID from int to str,0
Retry on blob storage read failures,0
%% Functions,0
PIL.Image.convert() returns a converted copy of this image,0
Alter orientation as needed according to EXIF tag 0x112 (274) for Orientation,0
,0
https://gist.github.com/dangtrinhnt/a577ece4cbe5364aad28,0
https://www.media.mit.edu/pia/Research/deepview/exif.html,0
,0
Null operation,0
Aspect ratio as width over height,0
ar = w / h,0
h = w / ar,0
w = ar * h,0
The following three functions are modified versions of those at:,0
https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py,0
Convert to pixels so we can use the PIL crop() function,0
PIL's crop() does surprising things if you provide values outside of,0
"the image, clip inputs",0
...if this detection is above threshold,0
...for each detection,0
"To avoid duplicate colors with detection-only visualization, offset",0
the classification class index by the number of detection classes,0
...if we have detection results,0
...if the confidence of this detection is above threshold,0
...for each detection,0
"print('Input must be of size [N, 4], but is ' + str(boxes_shape))",0
Deliberately trimming to the width of the image only in the case where,0
"box expansion is turned on.  There's not an obvious correct behavior here,",0
but the thinking is that if the caller provided an out-of-range bounding,0
"box, they meant to do that, but at least in the eyes of the person writing",0
"this comment, if you expand a box for visualization reasons, you don't want",0
to end up with part of a box.,0
,0
A slightly more sophisticated might check whether it was in fact the expansion,0
"that made this box larger than the image, but this is the case 99.999% of the time",0
"here, so that doesn't seem necessary.",1
If the total height of the display strings added to the top of the bounding,0
"box exceeds the top of the image, stack the strings below the bounding box",0
instead of above.,0
Each display_str has a top and bottom margin of 0.05x.,0
Reverse list and print from bottom to top.,0
%% Imports,0
%% Constants,0
convert category ID from int to str,0
%% Main function,0
arguments error checking,0
we don't import sas_blob_utils at the top of this file in order to,0
accommodate the MegaDetector Colab notebook which does not have,0
the azure-storage-blob package installed,0
%% Load detector output,0
"%% Load images, annotate them and save",0
max_conf = entry['max_detection_conf'],0
resize is for displaying them more quickly,0
%% Command-line driver,0
,0
stacked bar charts are made with each segment starting from a y position,0
"To fit the legend in, shrink current axis by 20%",0
Put a legend to the right of the current axis,0
####,0
,0
visualize_incoming_annotations.py,0
,0
Spot-check the annotations received from iMerit by visualizing annotated bounding,0
boxes on a sample of images and display them in HTML.,0
,0
"Modified in 2021 March to use the new format (iMerit batch 12 onwards), which is a",0
COCO formatted JSON with relative coordinates for the bbox.,0
,0
from data_management.megadb.schema import sequences_schema_check,0
we used frame_num of 1 when sending out images to annotators when it is not explicitly stored (wcs esp.),0
The file_name field in the incoming json looks like,0
alka_squirrels.seq2020_05_07_25C.frame119221.jpg,0
"we need to use the dataset, sequence and frame info to find the actual path in blob storage",0
using the sequences,0
category_id 5 is No Object Visible,0
download the image,0
Write to HTML,0
allow forward references in typing annotations,0
class variables,0
instance variables,0
get path to root,0
taxonomy CSV rows without 'taxonomy_string' entries are excluded,0
"from the taxonomy graph, but can be included in a classification",0
label specification JSON via the 'dataset_labels' key,0
use the lower parent,0
special cases,0
%% Imports,0
%% Taxnomy checking,0
taxonomy CSV rows without 'taxonomy_string' entries are excluded,0
"from the taxonomy graph, but can be included in a classification",0
label specification JSON via the 'dataset_labels' key,0
...for each row in the taxnomy file,0
%% Command-line driver,0
%% Interactive driver,0
%%,0
which datasets are already processed?,0
"sequence-level query should be fairly fast, ~1 sec",0
cases when the class field is on the image level (images in a sequence,0
"that had different class labels, 'caltech' dataset is like this)",0
"this query may take a long time, >1hr",0
"this query should be fairly fast, ~1 sec",0
read species presence info from the JSON files for each dataset,0
has this class name appeared in a previous dataset?,0
columns to populate the spreadsheet,0
sort by descending species count,0
make the spreadsheet,0
hyperlink Bing search URLs,0
hyperlink example image SAS URLs,0
TODO hardcoded columns: change if # of examples or col_order changes,1
,0
retrieve_sample_image.py,0
,0
"Downloader that retrieves images from Google images, used for verifying taxonomy",0
"lookups and looking for egregious mismappings (e.g., ""snake"" being mapped to a fish called",0
"""snake"").",0
,0
"Simple wrapper around simple_image_download, but I've had to swap in and out the underlying",0
downloader a few times.,0
,0
%% Imports and environment,0
%% Test driver,0
%% Main entry point,0
,0
simple_image_download.py,0
,0
Cloned from:,0
,0
https://github.com/RiddlerQ/simple_image_download,0
,0
Slighty modified to take an output directory.,0
,0
or '.ico' in object_raw or '.gif' in object_raw:,0
Todo: we have no evidence these are jpegs,1
######,0
,0
species_lookup.py,0
,0
Look up species names (common or scientific) in the GBIF and iNaturalist,0
taxonomies.,0
,0
Run initialize_taxonomy_lookup() before calling any other function.,0
,0
######,0
%% Constants and imports,0
As of 2020.05.12:,0
,0
"GBIF: ~777MB zipped, ~1.6GB taxonomy",0
"iNat: ~2.2GB zipped, ~51MB taxonomy",0
These are un-initialized globals that must be initialized by,0
the initialize_taxonomy_lookup() function below.,0
%% Functions,0
Initialization function,0
# Load serialized taxonomy info if we've already saved it,0
"# If we don't have serialized taxonomy info, create it from scratch.",0
Download and unzip taxonomy files,0
Don't download the zipfile if we've already unzipped what we need,0
Bypasses download if the file exists already,0
Unzip the files we need,0
...for each file that we need from this zipfile,0
Remove the zipfile,0
os.remove(zipfile_path),0
...for each taxonomy,0
"Create dataframes from each of the taxonomy files, and the GBIF common",0
name file,0
Load iNat taxonomy,0
Load GBIF taxonomy,0
Remove questionable rows from the GBIF taxonomy,0
Load GBIF vernacular name mapping,0
Only keep English mappings,0
Convert everything to lowercase,0
"For each taxonomy table, create a mapping from taxon IDs to rows",0
Create name mapping dictionaries,0
Build iNat dictionaries,0
row = inat_taxonomy.iloc[0],0
Build GBIF dictionaries,0
"The canonical name is the Latin name; the ""scientific name""",0
include the taxonomy name.,0
,0
http://globalnames.org/docs/glossary/,0
This only seems to happen for really esoteric species that aren't,0
"likely to apply to our problems, but doing this for completeness.",0
Don't include taxon IDs that were removed from the master table,0
Save everything to file,0
...def initialize_taxonomy_lookup(),0
"list of dicts: {'source': source_name, 'taxonomy': match_details}",0
i_match = 0,0
"list of (taxon_id, taxonRank, scientific name, [vernacular names])",0
corresponding to an exact match and its parents,0
Walk taxonomy hierarchy,0
This can happen because we remove questionable rows from the,0
GBIF taxonomy,0
The GBIF taxonomy contains unranked entries,0
...while there is taxonomy left to walk,0
...for each match,0
Remove redundant matches,0
i_tree_a = 0; tree_a = matching_trees[i_tree_a],0
i_tree_b = 1; tree_b = matching_trees[i_tree_b],0
"If tree a's primary taxon ID is inside tree b, discard tree a",0
,0
taxonomy_level_b = tree_b['taxonomy'][0],0
...for each level in taxonomy B,0
...for each tree (inner),0
...for each tree (outer),0
...def traverse_taxonomy(),0
"print(""Finding taxonomy information for: {0}"".format(query))",0
"In GBIF, some queries hit for both common and scientific, make sure we end",0
up with unique inputs,0
"If the species is not found in either taxonomy, return None",0
Both GBIF and iNat have a 1-to-1 mapping between taxon_id and row number,0
Walk both taxonomies,0
...def get_taxonomic_info(),0
m = matches[0],0
"For example: [(9761484, 'species', 'anas platyrhynchos')]",0
...for each taxonomy level,0
...for each match,0
...def print_taxonomy_matches(),0
%% Interactive drivers and debug,0
%% Initialization,0
%% Taxonomic lookup,0
query = 'lion',0
print(matches),0
Print the taxonomy in the taxonomy spreadsheet format,0
%% Directly access the taxonomy tables,0
%% Command-line driver,0
Read command line inputs (absolute path),0
Read the tokens from the input text file,0
Loop through each token and get scientific name,0
,0
process_species_by_dataset,0
,0
We generated a list of all the annotations in our universe; this script is,0
used to (interactively) map them onto the GBIF and iNat taxonomies.  Don't,0
"try to run this script from top to bottom; it's used like a notebook, not like",0
"a script, since manual review steps are required.",0
,0
%% Imports,0
%autoreload 0,0
%autoreload -species_lookup,0
%% Constants,0
Input file,0
Output file after automatic remapping,0
File to which we manually copy that file and do all the manual review; this,0
should never be programmatically written to,0
The final output spreadsheet,0
HTML file generated to facilitate the identificaiton of egregious mismappings,0
%% Functions,0
Prefer iNat matches over GBIF matches,0
query = 'person',0
Do we have an iNat match?,0
"print_taxonomy_matches(inat_matches, verbose=True)",0
"print_taxonomy_matches(gbif_matches, verbose=True)",0
print('Warning: multiple iNat matches for {}'.format(query)),0
Prefer chordates... most of the names that aren't what we want,0
"are esoteric insects, like a moth called ""cheetah""",0
,0
"If we can't find a chordate, just take the first match.",0
,0
i_test_match = 0,0
"This is (taxonID, taxonLevel, scientific, [list of common])",0
print(f'Warning: multiple iNat common names for {query}'),0
Default to returning the query,0
"print(f'Matched iNat {query} to {scientific_name},{common_name}')",0
...if we had iNat matches,0
If we either prefer GBIF or didn't have iNat matches,0
,0
Code is deliberately redundant here; I'm expecting some subtleties in how,0
handle GBIF and iNat.,0
print('Warning: multiple GBIF matches for {}'.format(query)),0
Prefer chordates... most of the names that aren't what we want,0
"are esoteric insects, like a moth called ""cheetah""",0
,0
"If we can't find a chordate, just take the first match.",0
,0
i_test_match = 0,0
"This is (taxonID, taxonLevel, scientific, [list of common])",0
print(f'Warning: multiple GBIF common names for {query}'),0
Default to returning the query,0
...if we needed to look in the GBIF taxonomy,0
...def get_preferred_taxonomic_match(),0
%% Initialization,0
%% Test single-query lookup,0
%%,0
%%,0
"q = ""grevy's zebra""",0
%% Read the input data,0
%% Run all our taxonomic lookups,0
i_row = 0; row = df.iloc[i_row],0
query = 'lion',0
...for each query,0
Write to the excel file that we'll use for manual review,0
%% Download preview images for everything we successfully mapped,0
uncomment this to load saved output_file,0
"output_df = pd.read_excel(output_file, keep_default_na=False)",0
i_row = 0; row = output_df.iloc[i_row],0
...for each query,0
%% Write HTML file with representative images to scan for obvious mis-mappings,0
i_row = 0; row = output_df.iloc[i_row],0
...for each row,0
%% Look for redundancy with the master table,0
Note: `master_table_file` is a CSV file that is the concatenation of the,0
"manually-remapped files (""manual_remapped.xlsx""), which are the output of",0
this script run across from different groups of datasets. The concatenation,0
"should be done manually. If `master_table_file` doesn't exist yet, skip this",0
"code cell. Then, after going through the manual steps below, set the final",0
manually-remapped version to be the `master_table_file`.,0
%% Manual review,0
Copy the spreadsheet to another file; you're about to do a ton of manual,0
review work and you don't want that programmatically overwrriten.,0
,0
See manual_review_xlsx above,0
%% Read back the results of the manual review process,0
%% Look for manual mapping errors,0
Manually inspect df for typos in 'dataset_names' and 'taxonomy_level' columns,0
Identify rows where:,0
,0
- 'taxonomy_level' does not match level of 1st element in 'taxonomy_string',0
- 'scientific_name' does not match name of 1st element in 'taxonomy_string',0
,0
...both of which typically represent manual mapping errors.,0
i_row = 0; row = df.iloc[i_row],0
"I'm not sure why both of these checks are necessary, best guess is that",1
the Excel parser was reading blanks as na on one OS/Excel version and as '',0
on another.,0
The taxonomy_string column is a .json-formatted string; expand it into,0
an object via eval(),0
"%% Find scientific names that were added manually, and match them to taxonomies",0
i_row = 0; row = df.iloc[i_row],0
...for each query,0
%% Write out final version,0
,0
prepare_api_output_for_timelapse.py,0
,0
Takes output from the batch API and does some conversions to prepare,0
it for use in Timelapse.,0
,0
Specifically:,0
,0
* Removes the class field from each bounding box,0
* Optionally does query-based subsetting of rows,0
* Optionally does a search and replace on filenames,0
* Replaces backslashes with forward slashes,0
"* Renames ""detections"" to ""predicted_boxes""",0
,0
"Note that ""relative"" paths as interpreted by Timelapse aren't strictly relative as",0
of 6/5/2019.  If your project is in:,0
,0
c:\myproject,0
,0
...and your .tdb file is:,0
,0
c:\myproject\blah.tdb,0
,0
...and you have an image at:,0
,0
c:\myproject\imagefolder1\img.jpg,0
,0
The .csv that Timelapse sees should refer to this as:,0
,0
myproject/imagefolder1/img.jpg,0
,0
...*not* as:,0
,0
imagefolder1/img.jpg,0
,0
Hence all the search/replace functionality in this script.  It's very straightforward,0
"once you get this and doesn't take time, but it's easy to forget to do this.  This will",0
be fixed in an upcoming release.,0
,0
%% Constants and imports,0
Python standard,0
pip-installable,0
"AI4E repos, expected to be available on the path",0
%% Helper classes,0
Only process rows matching this query (if not None); this is processed,0
after applying os.normpath to filenames.,0
"If not none, replace the query token with this",0
"If not none, prepend matching filenames with this",0
%% Helper functions,0
"If there's no query, we're just pre-pending",0
%% Main function,0
Create a temporary column we'll use to mark the rows we want to keep,0
This is the main loop over rows,0
Trim to matching rows,0
Timelapse legacy issue; we used to call this column 'predicted_boxes',0
Write output,0
"write_api_results(detectionResults,outputFilename)",0
%% Interactive driver,0
%%,0
%% Command-line driver (** outdated **),0
"Copy all fields from a Namespace (i.e., the output from parse_args) to an object.",0
,0
Skips fields starting with _.  Does not check existence in the target object.,0
"print('Setting {} to {}'.format(n,v))",0
Convert to an options object,0
--------some stuff needed to get AJAX to work with bottle?--------#,0
-------------------------------------------------------------------------------- #,0
PREPARE TO QUEUE IMAGES FOR LABELING,0
-------------------------------------------------------------------------------- #,0
# Connect as USER to database DB_NAME through peewee and initialize database proxy,0
# Load embedding model,0
---------------------------------------------------------------------- #,0
CREATE QUEUE OF IMAGES TO LABEL,0
---------------------------------------------------------------------- #,0
Use classifier to generate predictions,0
# Update model predicted class in PostgreSQL database,0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?,0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
-------------------------------------------------------------------------------- #,0
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI,0
-------------------------------------------------------------------------------- #,0
"# static routes (to serve CSS, etc.)",0
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps",0
"return bottle.static_file(filename, root='../../../../../../../../../../../.')",0
# dynamic routes,0
if data['display_grayscale']:,0
indices_to_exclude.update(set(color_indices)),0
elif not data['display_grayscale']:,0
indices_to_exclude.update(set(grayscale_indices)),0
data['display_images'] = {},0
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices],0
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices],0
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices],0
data['display_images']['detection_categories'] = [],0
for i in indices:,0
if str(dataset.samples[i][1]) == 'None':,0
data['display_images']['detection_categories'].append('None'),0
else:,0
existing_category_entries = {cat.id: cat.name for cat in Category.select()},0
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()",0
data['display_images']['detection_categories'].append(cat_name),0
Use image ids in images_to_label to get the corresponding dataset indices,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
print(set(dataset.set_indices[4]).update(set(indices_to_label))),0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Use image id images_to_label to get the corresponding dataset index,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?,0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Train on samples that have been labeled so far,0
dataset.set_kind(DetectionKind.UserDetection.value),0
print(y_train),0
Predict on the samples that have not been labeled,0
print(y_pred),0
Update model predicted class in PostgreSQL database,0
timer = time.time(),0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
print('Updating the database took %0.2f seconds'%(time.time() - timer)),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids)),0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
"once the classifier has been trained the first time, switch to AL sampling",0
,0
make_oneclass_json.py,0
,0
"Takes a coco-camera-traps .json database and collapses species classes to binary,",0
optionally removing labels from empty images (to be detector-friendly) (depending on,0
"""experiment_type"").",0
,0
"Assumes that empty images are labeled as ""empty"".",0
,0
%% Imports and environment,0
%% Core conversion function,0
"We're removing empty images from the annotation list, but not from",0
"the ""images"" list; they'll still get used in detector training.",0
print('Ignoring empty annotation'),0
%% Interactive driver,0
%%,0
Load annotations,0
Convert from multi-class to one-class,0
Write out the one-class data,0
%% Command-line driver,0
,0
plot_bounding_boxes.py,0
,0
Takes a .json database containing bounding boxes and renders those boxes on the,0
source images.,0
,0
"This assumes annotations in coco-camera-traps format, with absolute bbox",0
coordinates.,0
,0
%% Imports and environment,0
How many images should we process?  Set to -1 to process all images.,0
Should we randomize the image order?,0
%%  Read database and build up convenience mappings,0
Category ID to category name,0
Image ID to image info,0
Image ID to image path,0
Image ID to all annotations referring to this image,0
"%% Iterate over images, draw bounding boxes, write to file",0
For each image,0
image = images[0],0
Build up a list of bounding boxes to draw on this image,0
Load the image,0
Create figure and axes,0
Display the image,0
ann = imageAnnotations[0],0
"For each annotation associated with this image, render bounding box and label",0
"In the Rectangle() function, the first argument (""location"") is the bottom-left",0
of the rectangle.,0
,0
Origin is the upper-left of the image.,0
Add the patch to the Axes,0
Add a class label,0
This is magic goop that removes whitespace around image plots (sort of),0
Write the output image,0
...for each image,0
,0
plot_imerit_annotations.py,0
,0
Takes a .json file full of bounding box annotations and renders those boxes on the,0
source images.,0
,0
"This assumes annotations in the format we receive them, specifically:",0
,0
1) Relative bbox coordinates,0
"2) A list of .json objects, not a well-formatted .json file",0
,0
"I.e., don't use this on a COCO-style .json file.  See plot_bounding_boxes.py",0
for the same operation performed on a proper COCO-camera-traps database.,0
,0
%% Imports and environment,0
"os.makedirs(outputBase, exist_ok=True)",0
%%  Read all source images and build up a hash table from image name to full path,0
"This spans training and validation directories, so it's not the same as",0
just joining the image name to a base path,0
"%% Iterate over annotations, draw bounding boxes, write to file",0
annData has keys:,0
,0
"annotations, categories, images",0
,0
Each of these are lists of dictionaries,0
%% Render all annotations on each image in the sequence,0
%% Pull out image metadata,0
Build up a list of bounding boxes to draw on this image,0
Pull out just the image name from the filename,0
,0
File names look like:,0
,0
seq6efffac2-5567-11e8-b3fe-dca9047ef277.frame1.img59a94e52-23d2-11e8-a6a3-ec086b02610b.jpg,0
"m = re.findall(r'img(.*\.jpg)$', imgFileName, re.M|re.I)",0
print(m),0
assert(len(m) == 1),0
queryFileName = m[0],0
Map this image back to the original directory,0
"%% Loop over annotations, find annotations that match this image",0
%%,0
"x,y,w,h",0
,0
"x,y is the bottom-left of the rectangle",0
,0
"x,y origin is the upper-left",0
...for each annotation,0
%% Render with PIL (scrap),0
%% Render with Matplotlib,0
Create figure and axes,0
Display the image,0
Location is the bottom-left of the rect,0
,0
Origin is the upper-left,0
Add the patch to the Axes,0
This is magic goop that removes whitespace around image plots (sort of),0
os.startfile(outputFileName),0
%% Showing figures on-screen during debugging,0
plt.show(),0
Various (mostly unsuccessful) approaches to getting the plot window to show up,0
"in the foreground, which is a backend-specific operation...",0
,0
fig.canvas.manager.window.activateWindow(),0
fig.canvas.manager.window.raise_(),0
fm = plt.get_current_fig_manager(),0
"fm.window.attributes('-topmost', 1)",0
"fm.window.attributes('-topmost', 0)",0
,0
# This is the one that I found to be most robust... at like 80% robust.,0
plt.get_current_fig_manager().window.raise_(),0
%%,0
...for each image,0
...for each file,0
,0
convert_imerit_json_to_coco_json.py,0
,0
"Takes a .json file with bounding boxes but no class labels, and a .json file containing the",0
"class labels for those images, and creates a new json file with class labels and bounding",0
boxes.,0
,0
"The bounding box .json file is in the format returned by our annotators, which is not",0
actually a fully-formed .json file; rather it's a series of .json objects,0
,0
"Leaves filenames intact.  Stores imerit ""category IDs"" (empty/human/group/animal) in a new",0
"field called ""annotation_type"".",0
,0
%% Imports and constants,0
%% Configure files and paths,0
"For Snapshot Serengeti, we stored image IDs in our annotation files as:",0
,0
S1_B06_R1_PICT0008,0
,0
...but the corresponding ID in the master database is actually:,0
,0
S1\B06\R1\S1_B06_R1_PICT0008,0
,0
"If this is ""True"", we'll expand the former to the latter",0
Handling a one-off issue in which .'s were mysteriously replaced with -'s,0
"in our annotations.  This will be set dynamically, but I keep it here as",0
a constant to remind me to remove this code when we clean this issue up.,1
Used in the (rare) case where a bounding box was added to an image that was originally,0
annotated as empty,0
Used in the (rare) case where we added bounding boxes to an image with multiple species,0
"Should we include ambiguous bounding boxes (with the ""ambiguous"" category label)",0
in the output file?  Ambiguous boxes are boxes drawn on images with multiple species.,0
"%%  Read metadata from the master database, bounding boxes from the annotations file",0
"The bounding box .json file is in the format returned by our annotators, which is not",0
actually a fully-formed .json file; rather it's a series of .json objects,0
"Each element of annData is a dictionary corresponding to a single sequence, with keys:",0
,0
"annotations, categories, images",0
sequence = annData[0],0
%% Build convenience mappings,0
Image ID to images,0
Category ID to categories (referring to the database categories),0
"Image ID to categories (i.e., species labels)",0
Utility function we'll use to create annotations for images in empty,0
sequences (empty images in non-empty sequences already have annotations),0
"%% Reformat annotations, grabbing category IDs from the master database (prep)",0
iSequence = 0; sequence = annData[0],0
"%% Reformat annotations, grabbing category IDs from the master database (loop)",0
Make a copy here; we're going to manipulate the sequence annotations,0
when we need to add synthetic annotations for empty images,0
im = sequenceImages[0],0
Are there any annotations in this sequence?,0
Which images in this sequence have annotations?,0
For each image in this sequence...,0
imeritImageID = im['id'],0
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG,0
Confirm that the file exists,0
Hande a one-off issue with our annotations,0
datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG,0
,0
...had become:,0
,0
datasetsnapshotserengeti.seqASG000001a-frame0.imgS1_B06_R1_PICT0008.JPG,0
Does it look like we encountered this issue?,0
Convert:,0
,0
S1_B06_R1_PICT0008,0
,0
...to:,0
,0
S1/B06/B06_R1/S1_B06_R1_PICT0008,0
Make sure we only see each image once,0
Create empty annotations for empty images,0
,0
Here we use the *unmodified* file name,0
Create an empty annotation for this image,0
Annotations still use the annotation filename (not database ID) at this point;,0
these will get converted to database IDs below when we process the,0
whole sequence.,0
Sanity-check image size,0
"print('Warning: img {} was listed in DB as {}x{}, annotated as {}x{}, actual size{}x{}'.format(",0
"old_id,new_im['width'],new_im['height'],im['width'],im['height'],imgObj.width,imgObj.height))",0
...for each image in this sequence,0
For each annotation in this sequence...,0
ann = sequenceAnnotations[0],0
Prepare an annotation using the category ID from the database and,0
the bounding box from the annotations file,0
Maintain iMerit's annotation category,0
Generate an (arbitrary) ID for this annotation; the COCO format has a concept,0
"of annotation ID, but our annotation files don't",0
This was a one-off quirk with our file naming,1
We'll do special handling of images with multiple categories later,0
Store the annotation type (group/human/animal/empty),0
This annotation has no bounding box but the image wasn't originally,0
annotated as empty,0
This annotation has a bounding box but the image was originally,0
annotated as empty,0
unnormalize the bbox,0
... for each annotation in this sequence,0
... for each sequence,0
%% Post-processing,0
Count empty images,0
...for each file,0
%% Sanity-check empty images,0
,0
make_ss_annotation_image_folder.py,0
,0
Take a directory full of images with the very long filenames we give annotators:,0
,0
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension,0
,0
"...along with a COCO-camera-traps database referring to those files, and:",0
,0
1) Creates a new COCO-camera-traps database with the original filenames in them,0
(copying the annotations),0
,0
2) Optionally creates a new directory with those images named according to the,0
"Snapshot Serengeti naming convention, including complete relative paths.",0
,0
See convert_imerit_json_to_coco_json to see how we get from the original annotation,0
.json to a COCO-camera-traps database.,0
,0
%% Constants and imports,0
%% Configure files/paths,0
%% Read the annotations (referring to the old filenames),0
"%% Update filenames, optionally copying files",0
im = data['images'][0],0
For each image...,0
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG,0
"Find the image name, e.g. S1_B06_R1_PICT0008",0
Convert:,0
,0
S1_B06_R1_PICT0008.JPG,0
,0
...to:,0
,0
S1/B06/B06_R1/S1_B06_R1_PICT0008.JPG,0
...for each image,0
%% Write the revised database,0
,0
get_annotation_tool_link.py,0
,0
"Takes a COCO-camera-traps-style .json file with URLs already embedded, and prepares",0
a link to the visipedia annotation tool that reviews a subset of those images.,0
,0
,0
create_new_annotation_json.py,0
,0
"Creates a subset of a larger .json database, in this case specifically to pick some images",0
from Snapshot Serengeti.,0
,0
from utils import get_db_dicts,0
for seq in already_annotated:,0
seq_to_ims.pop(seq),0
remove already annotated images,0
add lion images,0
lion_seqs = cat_to_seqs[cat_to_id['lionMale']] + cat_to_seqs[cat_to_id['lionFemale']],0
#print(len(lion_seqs)),0
lion_seqs = [seq for seq in lion_seqs if seq not in already_annotated],0
#print(len(lion_seqs)),0
"lion_seqs_to_annotate.extend(random.sample(lion_seqs, min(len(lion_seqs),num_lions)))",0
print(len(lion_seqs_to_annotate)),0
num_elephants = 1000,0
elephant_seqs = cat_to_seqs[cat_to_id['elephant']],0
#print(len(lion_seqs)),0
elephant_seqs = [seq for seq in elephant_seqs if seq not in already_annotated],0
#print(len(lion_seqs)),0
"elephant_seqs_to_annotate = random.sample(elephant_seqs, num_elephants)",0
num_empty = 10,0
empty_seqs_to_annotate = [],0
for loc in loc_to_seqs:,0
empty_seqs = cats_per_location[loc][cat_to_id['empty']],0
empty_seqs = [seq for seq in empty_seqs if seq not in already_annotated],0
empty_seqs = [seq for seq in empty_seqs if seq_to_season[seq] in seasons_to_keep],0
"empty_seqs_to_annotate.extend(random.sample(empty_seqs, min(len(empty_seqs),num_empty)))",0
ims_to_annotate.extend(empty_ims_to_annotate),0
,0
filter_database.py,0
,0
"Look through a COCO-ct database and find images matching some crtieria, writing",0
a subset of images and annotations to a new file.,0
,0
%% Constants and imports,0
%% Configuration,0
%%  Read database and build up convenience mappings,0
Category ID to category name,0
Image ID to image info,0
Image ID to image path,0
%% Filter,0
ann = annotations[0],0
Is this a tiny box or a group annotation?,0
"x,y,w,h",0
All empty annotations should be classified as either empty or ambiguous,0
,0
"The ambiguous cases are basically minor misses on the annotators' part,",0
where two different small animals were present somewhere.,0
%% Write output file,0
,0
find_images_for_classes.py,0
,0
"Given a .json database, find images that are associated with one or more",0
classes.,0
,0
%% Constants and imports,0
%% Configuration,0
%%  Read database and build up convenience mappings,0
Category ID to category name,0
Image ID to image info,0
Image ID to image path,0
%% Look for target-class annotations,0
ann = annotations[0],0
,0
create_tfrecords_format.py,0
,0
This script creates a tfrecords file from a dataset in VOTT format.,0
%% Imports and environment,0
set up the filenames and annotations,0
This loop reads the bboxes and corresponding labels and assigns them,0
the correct image. Kind of slow at the moment...,0
If needed: merging all classes,0
bbox_labels = ['Animal' for _ in bbox_labels],0
BBox coords are stored in the format,0
"x_min (of width axis) y_min (of height axis), x_max, y_max",0
Coordinate system starts in top left corner,0
"In this framework, we need ('ymin', 'xmin', 'ymax', 'xmax') format",0
print out some stats,0
To make sure we loaded the bboxes correctly:,0
self.validate_bboxes(),0
For each image in the data set...,0
Make sure all are greater equal 0,0
%% Main tfrecord generation function,0
Propagate optional metadata to tfrecords,0
endfor each annotation for the current image,0
endfor each image,0
"Calculate number of shards to get the desired number of images per record,",0
ensure it is evenly divisible by the number of threads,0
%% Command-line driver,0
%% Driver,0
,0
make_tfrecords_cis_trans.py,0
,0
Given a .json file that contains a three-element list (train/val/test) of image IDs and a .json database that contains,0
"those image IDs, generates tfrecords whose filenames include ""train""/""val""/""test""",0
,0
"dataset = json.load(open('/ai4efs/databases/snapshotserengeti/oneclass/SnapshotSerengeti_Seasons_1_to_4_tfrecord_format_valid_ims.json','r'))",0
print('Creating trans_val tfrecords'),0
dataset = [im_id_to_im[idx] for idx in trans_val],0
,0
create_classification_tfrecords_from_json.py,0
,0
Called from make_tfrecords_cis_trans.py if you're running a classification experiment.,0
,0
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/',0
datafile = 'eccv_18_annotation_files_oneclass/CaltechCameraTrapsECCV18',0
image_file_root = datafolder+'eccv_18_all_images/',0
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/',0
datafolder = '/data/iwildcam/',0
datafile = 'combined_iwildcam_annotations_oneclass/eccv_train_and_imerit_2',0
need consecutive category ids,0
old_cat_id_to_new_cat_id = {categories[idx]['id']:idx+1 for idx in range(len(categories))},0
print(old_cat_id_to_new_cat_id),0
remove multiclass images,0
print(images[0]),0
print(vis_data[0]),0
,0
make_tfrecords_from_json.py,0
,0
"Given a coco-camera-traps .json file, creates tfrecords",0
,0
Thin wrapper for create_tfrecords_from_json.,0
,0
%% Constants and imports,0
%% Constants and imports (interactive),0
%%,0
%% Main tfrecord generation function,0
"check whether the input file has already been converted to the tfrecords format,",0
"if not, convert",0
"Calculate number of shards to get the desired number of images per record,",0
ensure it is evenly divisible by the number of threads,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
eMammal_make_tfrecords_train_val_test.py,0
,0
"From the tfrecords_format json version of the database, creates three splits",0
of tf_records according to a previously decided split of full image IDs.,0
configurations and paths,0
a tfrecord_format json,0
these are number of images,0
do not include empty images in the train set; note that some images from non-empty sequences,0
"end up being empty (no bbox can be labeled), so these will be included in train set anyways",0
eMammal_make_splits.py,0
,0
"Based on a tfrecords_format json file of the database, creates 3 splits according to",0
the specified fractions based on location (images from the same location should be in,0
one split) or based on images.,0
,0
"If a previous split is provided (append_to_previous_split is True), the entries in",0
"each split will be preserved, and new entries will be appended, so that new models",0
can warm start with a model trained on the original splits.,0
configurations and paths,0
approximate fraction for the new entries,0
read in the previous splits of image ID or location ID if available,0
"find new locations and assign them to a split, without reassigning any previous locations",0
"find out which images are new, shuffle and split them",0
do NOT sort the IDs to keep the shuffled order,0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
add empty category,0
"add all images that don't have annotations, with cat empty",0
print(detection_results.keys()),0
group the detections and gts by image id:,0
print(image_id),0
print(len(scores)),0
print(len(labels)),0
recall_thresh = 0.9,0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall]),0
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])",0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall]),0
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])",0
exp_name = 'eccv_train',0
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json',0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(len(detection_results['images'])),0
print(len(seqs)),0
"print(len(seqs[0]),len(seqs[",0
print(detection_results.keys()),0
group the detections by image id:,0
group the ground truth annotations by image id:,0
print(seq),0
"for image_id, dets in per_image_detections.iteritems():",0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
print(num_gts),0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
if sum(detected_class_labels)>0:,0
print('detected animal box'),0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])",0
"print(scores, tp_fp_labels)",0
num_total_gts += num_gts,0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
print('valid box'),0
"print(best_im, best_score)",0
exp_name = 'eccv_train',0
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json',0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
need to loop over confidence values,0
"for each value, check if any detections on the image are > conf",0
"If so, that image gets class ""animal""",0
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0",0
"for image_id, dets in per_image_detections.iteritems():",0
"calc prec, rec for this confidence thresh",0
group the ground truth annotations by image id:,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(detection_results.keys()),0
group the detections and gts by image id:,0
print(image_id),0
print(len(scores)),0
print(len(labels)),0
recall_thresh = 0.9,0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall]),0
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])",0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall]),0
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])",0
print(sorted_ap),0
plt.bar(sorted_ap),0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(detection_results.keys()),0
group the detections by image id:,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
print(gts),0
print(num_gts),0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[:2])",0
"print(scores, tp_fp_labels)",0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
print('detected animal box'),0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])",0
"print(scores, tp_fp_labels)",0
num_total_gts += num_gts,0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
print('valid box'),0
best_im = np.argmax(max_im_scores),0
"print(best_im, best_score)",0
"temp_labels = np.zeros(len(im_detection_labels),  dtype=np.int32)",0
"temp_scores = np.zeros(len(im_detection_scores), dtype=np.float32)",0
"for j in range(min(im_num_gts, len(im_detection_labels))):",0
temp_labels[j] = True #TODO: this currently only works for oneclass?,1
temp_scores[j] = best_score,0
im_detection_labels = temp_labels,0
im_detection_scores = temp_scores,0
num_total_gts+=im_num_gts,0
"print(len(detection_scores), len(detection_scores[0]), len(detection_scores[1]))",0
exp_name = 'small_balanced_cct',0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(seq),0
"for image_id, dets in per_image_detections.iteritems():",0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
seq_num_gts.append(num_gts),0
print(num_gts),0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
num_total_gts += 1,0
print('valid box'),0
"print(best_im, best_score)",0
print('no valid box'),0
if sum(seq_num_gts)>0:,0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
need to loop over confidence values,0
"for each value, check if any detections on the image are > conf",0
"If so, that image gets class ""animal""",0
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0",0
"calc prec, rec for this confidence thresh",0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
group the detections by image id:,0
group the ground truth annotations by image id:,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
print('detected animal box'),0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])",0
"print(scores, tp_fp_labels)",0
num_total_gts += num_gts,0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
print('valid box'),0
"print(best_im, best_score)",0
,0
evaluate_detections.py,0
,0
Adapted from analyze_detection.py which is now archived.,0
,0
%% Imports and constants,0
%% Functions,0
"labels input to compute_object_detection_metrics() needs to start at 0, not 1",0
num_detections = len(dets['boxes']),0
to prevent 'Invalid dimensions for box data.' error,0
this box will not match any detections,0
compute one-class precision/recall/average precision (if every box is just of an object class),0
%% Command-line driver,0
,0
detection_eval_utils.py,0
,0
Utility functions used in evaluate_detections.py,0
,0
group the ground truth annotations by image id,0
#####,0
,0
detect_and_predict_image.py,0
,0
"Functions to load a TensorFlow detection and a classification model, run inference,",0
"render bounding boxes on images, and write out the resulting",0
images (with bounding boxes and classes).,0
,0
"See the ""test driver"" cell for example invocation.",0
,0
,0
#####,0
"%% Constants, imports, environment",0
Minimum detection confidence for showing a bounding box on the output image,0
Stick this into filenames before the extension for the rendered result,0
Number of top-scoring classes to show at each bounding box,0
%% Core detection functions,0
Load images if they're not already numpy arrays,0
iImage = 0; image = images[iImage],0
"Load the image as an nparray of size h,w,nChannels",0
"There was a time when I was loading with PIL and switched to mpimg,",0
"but I can't remember why, and converting to RGB is a very good reason",0
"to load with PIL, since mpimg doesn't give any indication of color",0
"order, which basically breaks all .png files.",0
,0
"So if you find a bug related to using PIL, update this comment",0
"to indicate what it was, but also disable .png support.",0
image = mpimg.imread(image),0
This shouldn't be necessarily when loading with PIL and converting to RGB,0
Actual detection,0
...for each image,0
"Currently ""boxes"" is a list of length nImages, where each element is shaped as",0
,0
"1,nDetections,4",0
,0
"This implicitly banks on TF giving us back a fixed number of boxes, let's assert on this",0
to make sure this doesn't silently break in the future.,0
iBox = 0; box = boxes[iBox],0
"""scores"" is a length-nImages list of elements with size 1,nDetections",0
"""classes"" is a length-nImages list of elements with size 1,nDetections",0
,0
"Still as floats, but really representing ints",0
Squeeze out the empty axis,0
boxes is nImages x nDetections x 4,0
scores and classes are both nImages x nDetections,0
Get input and output tensors of classification model,0
"imsize = cur_image['width'], cur_image['height']",0
Select detections with a confidence larger 0.5,0
Get these boxes and convert normalized coordinates to pixel coordinates,0
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes",0
"However, we need to make sure that it box coordinates are still within the image",0
For convenience:,0
Create an array with contains the index of the corresponding crop_box for each selected box,0
i.e. [False False 0 False 1 2 3 False False],0
For each box,0
If this box should be classified,0
Run inference,0
if box should not be classified,0
...for each box,0
species_scores should have shape len(images) x len(boxes) x num_species,0
...for each image,0
...with tf.Session,0
with classification_graph,0
species_scores should have shape len(images) x len(boxes) x num_species,0
%% Rendering functions,0
Display the image,0
plt.show(),0
"top, left, bottom, right",0
,0
"x,y origin is the upper-left",0
Generate bounding box text,0
Choose color based on class,0
Location is the bottom-left of the rect,0
,0
Origin is the upper-left,0
iRight = x + w,0
iTop = y + h,0
Add the patch to the Axes,0
Add class description,0
First determine best location by finding the corner that is closest to the image center,0
relative corner coordinates,0
relative coordinates of image center,0
Compute pair-wise squared distance and get the index of the one with minimal distance,0
Get the corresponding coordinates ...,0
... and alignment for the text box,0
Plot the text box with background,0
...for each box,0
This is magic goop that removes whitespace around image plots (sort of),0
"plt.savefig(outputFileName, bbox_inches='tight', pad_inches=0.0, dpi=dpi, transparent=True)",0
os.startfile(outputFileName),0
...for each image,0
...def render_bounding_boxes,0
Load and run detector on target images,0
Read the name of all classes,0
remove empty lines,0
%% Interactive driver,0
%%,0
%%,0
%% File helper functions,0
%% Command-line driver,0
Hack to avoid running on already-detected images,1
,0
"Script for evaluating a frozen graph given a COCO-style dataset, which was generated by",0
the ../data_management/databases/classification/make_classification_dataset.py script.,0
This is useful for debugging as the accuracy reported by this script should match the,0
accuracy reported by the Tensorflow training.,0
,0
Check that all files exists for easier debugging,0
Load frozen graph,0
Collect tensors for input and output,0
Read image,0
Run inference,0
Check if correct,0
,0
Script for evaluating precision/recall on a two-class problem given,0
"a frozen graph and a COCO-style dataset, which was generated by",0
the ../data_management/databases/classification/make_classification_dataset.py script.,0
We assume the positive class is at index 0 (with a zero-based indexing),0
,0
Check that all files exists for easier debugging,0
Load frozen graph,0
Collect tensors for input and output,0
Read image,0
"with open(image_path, 'rb') as fi:",0
"image =  sess.run(tf.image.decode_jpeg(fi.read(), channels=3))",0
image = image / 255.,0
Run inference,0
predicted_class = np.argmax(predictions),0
Check if correct,0
if coco.imgToAnns[image_id][0]['category_id'] == predicted_class:,0
correct = correct + 1,0
"In matplotlib < 1.5, plt.fill_between does not have a 'step' argument",0
,0
Script for selecting testing images of a COCO-style dataset generated by the script,0
../data_management/databases/classification/make_classification_dataset.py in a consistent,0
manner and predicting the class for it.,0
,0
Mandatory parameters,0
Optional parameters,0
Validate parameters,0
Derived parameters,0
"We assume that the dataset was generated with the make_classification_dataset.py script,",0
hence the images should be located in the same folder as the json,0
Make seletion deterministic,0
Load frozen graph,0
Get dataset information,0
Get classes,0
...and the class list corresponding to the model outputs by assuming,0
that they are in order of their ids,0
Get images of each class,0
Shuffle the image list,0
Start the image sampling,0
"Set of avaiable class IDs, will be filled below",0
If there are still images left for that class,0
"Get image for the sampled class, we already shuffled the class images before so",0
we can simply pop(),0
Start prediction,0
Collect tensors for input and output,0
Read image,0
Run inference,0
Print output to log file,0
#####,0
,0
api_apply_classifier_single_node.py,0
,0
Takes the JSON file produced by the detection API and,0
classifies all boxes above a confidence threshold.,0
,0
#####,0
"%% Constants, imports, environment",0
Assumes that the root of the CameraTraps repo is on the PYTHONPATH,0
Minimum detection confidence for classifying an object,0
Number of top-scoring classes to show at each bounding box,0
Enlargment factor applied to boxes before passing them to the classifier,0
,0
Provides more context and can lead to better results,0
List of detection categories for which we will run the classification,0
,0
"Currently there are {""1"": ""animal"", ""2"": ""person"", ""4"": ""vehicle""}",0
,0
Should be a list of string-formatted ints.,0
Number of significant float digits in JSON output,0
%% Core detection functions,0
Read the name of all classes,0
remove empty lines,0
Create field with name *classification_categories*,0
Add classes using 0-based indexing,0
def add_classification_categories,0
Make sure we have the right json object,0
Get input and output tensors of classification model,0
For each image,0
Read image,0
"Scale pixel values to [0,1]",0
For each box,0
Skip detections with low confidence,0
Skip if detection category is not in whitelist,0
Skip if already classified,0
"Get current box in relative coordinates and format [x_min, y_min, width_of_box, height_of_box]",0
"Convert to [ymin, xmin, ymax, xmax] and store it as 1x4 numpy array so we can",0
re-use the generic multi-box padding code,0
Convert normalized coordinates to pixel coordinates,0
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes.",0
,0
"However, we need to make sure that it box coordinates are still within the image.",0
Get the first (and only) row as our bbox to classify,0
Get the image data for that box,0
Run inference,0
Add an empty list to the json for our predictions,0
Add the *num_annotated_classes* top scoring classes,0
...for each box,0
...for each image,0
...with tf.Session,0
with classification_graph,0
def classify_boxes,0
Load classification model,0
Load detector json,0
Add classes to detector_json,0
"Run classifier on all images, changes will be writting directly to the json",0
Write output json,0
def load_and_run_classifier,0
%% Command-line driver,0
,0
Script for selecting testing images of a COCO-style dataset generated by the script,0
../data_management/databases/classification/make_classification_dataset.py in a consistent,0
manner and predicting the class for it.,0
,0
Assumes the cameratraps repo root is on the path,0
Make seletion deterministic,0
Mandatory parameters,0
Optional parameters,0
Validate parameters,0
Tranfer parameters to post-processing format,0
Load frozen graph,0
Reading image list,0
Reading class list,0
Image sampling,0
Start prediction,0
Collect tensors for input and output,0
Array for collecting infos for rendering the html,0
Read image,0
"Scale pixel values to [0,1]",0
Run inference,0
Add links to all available classes,0
,0
predict_image.py,0
,0
"Given a pointer to a frozen detection graph, runs inference on a single image,",0
printing the top classes to the console,0
,0
%% Imports,0
%% Command-line processing,0
Check that all files exist for easier debugging,0
%% Inference,0
Load frozen graph,0
Load class list,0
Remove empty lines,0
Collect tensors for input and output,0
Read image,0
Run inference,0
Print output,0
,0
Mostly unmodified script for freezing a model,0
Added for convenience and for possible future optimizations,0
,0
Copyright 2015 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
'input_checkpoint' may be a prefix if we're using Saver V2 format,0
Remove all the explicit device specifications for this node. This helps to,0
make the graph more portable.,0
List of all partition variables. Because the condition is heuristic,0
"based, the list could include false positives.",0
This tensor doesn't exist in the graph (for example it's,0
'global_step' or a similar housekeeping element) so skip it.,0
`var_list` is required to be a map of variable names to Variable,0
tensors. Partition variables are Identity tensors that cannot be,0
handled by Saver.,0
Models that have been frozen previously do not contain Variables.,0
optimized_output_graph_def = optimize_for_inference_lib.optimize_for_inference(,0
"output_graph_def,",0
"input_node_names.replace("" "", """").split("",""),",0
"output_node_names.replace("" "", """").split("",""),",0
tf.float32.as_datatype_enum),0
Write GraphDef to file if output path has been given.,0
,0
"Creates a graph description, which is required to create a frozen graph.",0
Adapted from from ./tf-slim/export_inference_graph.py,0
Added preprocessing to the definition for easier handling,0
,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
tf.app.flags.DEFINE_integer(,0
"'batch_size', None,",0
"'Batch size for the exported model. Defaulted to ""None"" so batch size can '",0
'be specified at model runtime.'),0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
#####################,0
Optimization Flags #,0
#####################,0
######################,0
Learning Rate Flags #,0
######################,0
######################,0
Dataset Flags #,0
######################,0
####################,0
Fine-Tuning Flags #,0
####################,0
"Note: when num_clones is > 1, this will actually have each clone to go",0
over each epoch FLAGS.num_epochs_per_decay times. This is different,0
behavior from sync replicas and is expected to produce different results.,0
Warn the user if a checkpoint exists in the train_dir. Then we'll be,0
ignoring the checkpoint anyway.,0
TODO(sguada) variables.filter_variables(),1
######################,0
Config model_deploy #,0
######################,0
Create global_step,0
#####################,0
Select the dataset #,0
#####################,0
#####################,0
Select the network #,0
#####################,0
####################################,0
Select the preprocessing function #,0
####################################,0
#############################################################,0
Create a dataset provider that loads data from the dataset #,0
#############################################################,0
###################,0
Define the model #,0
###################,0
############################,0
Specify the loss function #,0
############################,0
Gather initial summaries.,0
"Gather update_ops from the first clone. These contain, for example,",0
the updates for the batch_norm variables created by network_fn.,0
Add summaries for end_points.,0
Add summaries for losses.,0
Add summaries for variables.,0
################################,0
Configure the moving averages #,0
################################,0
########################################,0
Configure the optimization procedure. #,0
########################################,0
"If sync_replicas is enabled, the averaging will be done in the chief",0
queue runner.,0
Update ops executed locally by trainer.,0
Variables to train.,0
and returns a train_tensor and summary_op,0
Add total_loss to summary.,0
Create gradient updates.,0
Add the summaries from the first clone. These contain the summaries,0
created by model_fn and either optimize_clones() or _gather_clone_loss().,0
Merge all summaries together.,0
##########################,0
Kicks off the training. #,0
##########################,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
#####################,0
Select the dataset #,0
#####################,0
###################,0
Select the model #,0
###################,0
#############################################################,0
Create a dataset provider that loads data from the dataset #,0
#############################################################,0
####################################,0
Select the preprocessing function #,0
####################################,0
###################,0
Define the model #,0
###################,0
Define the metrics:,0
Print the summaries to screen.,0
TODO(sguada) use num_epochs=1,1
This ensures that we make a single pass over all of the data.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Transform the image to floats.,0
"Randomly crop a [height, width] section of the image.",0
Randomly flip the image horizontally.,0
"Because these operations are not commutative, consider randomizing",0
the order their operation.,0
Subtract off the mean and divide by the variance of the pixels.,0
Transform the image to floats.,0
Resize and crop if needed.,0
Subtract off the mean and divide by the variance of the pixels.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Use tf.slice instead of crop_to_bounding box as it accepts tensors to,0
define the crop size.,0
Compute the rank assertions.,0
Create a random bounding box.,0
,0
Use tf.random_uniform and not numpy.random.rand as doing the former would,0
"generate random numbers at graph eval time, unlike the latter which",0
generates random numbers at graph definition time.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Pass the real x only to one of the func calls.,0
The random_* ops do not necessarily clamp.,0
"Each bounding box has shape [1, num_boxes, box coords] and",0
"the coordinates are ordered [ymin, xmin, ymax, xmax].",0
A large fraction of image datasets contain a human-annotated bounding,0
box delineating the region of the image containing the object of interest.,0
We choose to create a new bounding box for the object which is a randomly,0
distorted version of the human-annotated bounding box that obeys an,0
"allowed range of aspect ratios, sizes and overlap with the human-annotated",0
"bounding box. If no box is supplied, then we assume the bounding box is",0
the entire image.,0
Crop the image to the specified bounding box.,0
"Each bounding box has shape [1, num_boxes, box coords] and",0
"the coordinates are ordered [ymin, xmin, ymax, xmax].",0
Restore the shape since the dynamic slice based upon the bbox_size loses,0
the third dimension.,0
This resizing operation may distort the images because the aspect,0
ratio is not respected. We select a resize method in a round robin,0
fashion based on the thread number.,0
Note that ResizeMethod contains 4 enumerated resizing methods.,0
We select only 1 case for fast_mode bilinear.,0
Randomly flip the image horizontally.,0
Randomly distort the colors. There are 1 or 4 ways to do it.,0
Crop the central region of the image with an area containing 87.5% of,0
the original image.,0
Resize the image to the specified height and width.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TODO(sguada) docstring paragraph by (a) motivating the need for the file and,1
(b) defining clones.,0
TODO(sguada) describe the high-level components of model deployment.,1
"E.g. ""each model deployment is composed of several parts: a DeploymentConfig,",0
"which captures A, B and C, an input_fn which loads data.. etc",0
Set up DeploymentConfig,0
Create the global step on the device storing the variables.,0
Define the inputs,0
Define the optimizer.,0
Define the model including the loss.,0
Run training.,0
Namedtuple used to represent a clone during deployment.,0
"Namedtuple used to represent a DeployedModel, returned by deploy().",0
Default parameters for DeploymentConfig,0
Create clones.,0
The return value.,0
Individual components of the loss that will need summaries.,0
Compute and aggregate losses on the clone device.,0
Add the summaries out of the clone device block.,0
Only use regularization_losses for the first clone,0
Compute the total_loss summing all the clones_losses.,0
Sum the gradients across clones.,0
Gather initial summaries.,0
Create Clones.,0
"Gather update_ops from the first clone. These contain, for example,",0
the updates for the batch_norm variables created by model_fn.,0
Place the global step on the device storing the variables.,0
Compute the gradients for the clones.,0
Add summaries to the gradients.,0
Create gradient updates.,0
Only use regularization_losses for the first clone,0
Add the summaries from the first clone. These contain the summaries,0
created by model_fn and either optimize_clones() or _gather_clone_loss().,0
Add total_loss to summary.,0
Merge all summaries together.,0
Note that each grad_and_vars looks like the following:,0
"((grad_var0_clone0, var0), ... (grad_varN_cloneN, varN))",0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Create an easy training set:,0
Create an easy training set:,0
Create an easy training set:,0
clone function creates a fully_connected layer with a regularizer loss.,0
The model summary op should have a few summary inputs and all of them,0
should be on the CPU.,0
clone function creates a fully_connected layer with a regularizer loss.,0
"No optimizer here, it's an eval.",0
The model summary op should have a few summary inputs and all of them,0
should be on the CPU.,0
!/usr/bin/python,0
Copyright 2016 Google Inc. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Failed to find ""index"" occurrence of item.",0
pylint: disable=broad-except,0
pylint: enable=broad-except,0
Grab the 'index' annotation.,0
Some images contain bounding box annotations that,0
"extend outside of the supplied image. See, e.g.",0
n03127925/n03127925_147.xml,0
"Additionally, for some bounding boxes, the min > max",0
or the box is entirely outside of the image.,0
Example: <...>/n06470073/n00141669_6790.xml,0
Determine if the annotation is from an ImageNet Challenge label.,0
Note: There is a slight bug in the bounding box annotation data.,0
Many of the dog labels have the human label 'Scottish_deerhound',0
instead of the synset ID 'n02092002' in the bbox.label field. As a,0
"simple hack to overcome this issue, we only exclude bbox labels",1
*which are synset ID's* that do not match original synset label for,0
the XML file.,0
Guard against improperly specified boxes.,0
Note bbox.filename occasionally contains '%s' in the name. This is,0
data set noise that is fixed by just using the basename of the XML file.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The URLs where the MNIST data can be downloaded.,0
The names of the classes.,0
"First, process the training data:",0
"Next, process the testing data:",0
"Finally, write the labels file:",0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The URL where the Flowers data can be downloaded.,0
The number of images in the validation set.,0
Seed for repeatability.,0
The number of shards per dataset split.,0
Initializes function that decodes RGB JPEG data.,0
Read the filename:,0
Divide into train and test:,0
"First, convert the training and validation sets.",0
"Finally, write the labels file:",0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TODO(nsilberman): Add tfrecord file type once the script is updated.,1
"If set to false, will not try to set label_to_names in dataset",0
by reading them from labels.txt or github.,0
n01440764,0
n01443537,0
n02119247    black fox,0
n02119359    silver fox,0
pylint: disable=g-line-too-long,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The URL where the CIFAR data can be downloaded.,0
The number of training files.,0
The height and width of each image.,0
The names of the classes.,0
"First, process the training data:",0
"Next, process the testing data:",0
"Finally, write the labels file:",0
Copyright 2016 Google Inc. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The labels file contains a list of valid labels are held in this file.,0
Assumes that the file contains entries as such:,0
n01440764,0
n01443537,0
n01484850,0
where each line corresponds to a label expressed as a synset. We map,0
each synset contained in the file to an integer (based on the alphabetical,0
ordering). See below for details.,0
This file containing mapping from synset to human-readable label.,0
Assumes each line of the file looks like:,0
,0
n02119247    black fox,0
n02119359    silver fox,0
"n02119477    red fox, Vulpes fulva",0
,0
where each line corresponds to a unique mapping. Note that each line is,0
formatted as <synset>\t<human readable label>.,0
This file is the output of process_bounding_box.py,0
Assumes each line of the file looks like:,0
,0
"n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940",0
,0
where each line corresponds to one bounding box annotation associated,0
with an image. Each line can be parsed as:,0
,0
"<JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>",0
,0
Note that there might exist mulitple bounding box annotations associated,0
with an image file.,0
pylint: disable=expression-not-assigned,0
pylint: enable=expression-not-assigned,0
Create a single Session to run all image coding calls.,0
Initializes function that converts PNG to JPEG data.,0
Initializes function that converts CMYK JPEG data to RGB JPEG data.,0
Initializes function that decodes RGB JPEG data.,0
File list from:,0
https://groups.google.com/forum/embed/?place=forum/torch7#!topic/torch7/fOSTXHIESSU,0
File list from:,0
https://github.com/cytsai/ilsvrc-cmyk-image-list,0
Read the image file.,0
Clean the dirty data.,0
1 image is a PNG.,0
22 JPEG images are in CMYK colorspace.,0
Decode the RGB JPEG.,0
Check that image converted to RGB,0
Each thread produces N shards where N = int(num_shards / num_threads).,0
"For instance, if num_shards = 128, and the num_threads = 2, then the first",0
"thread would produce shards [0, 64).",0
"Generate a sharded version of the file name, e.g. 'train-00002-of-00010'",0
"Break all images into batches with a [ranges[i][0], ranges[i][1]].",0
Launch a thread for each batch.,0
Create a mechanism for monitoring when all threads are finished.,0
Create a generic TensorFlow-based utility for converting all image codings.,0
Wait for all the threads to terminate.,0
Leave label index 0 empty as a background class.,0
Construct the list of JPEG files and labels.,0
Shuffle the ordering of all image files in order to guarantee,0
random ordering of the images with respect to label in the,0
saved TFRecord files. Make the randomization repeatable.,0
Build a map from synset to human-readable label.,0
Run it!,0
Allowing None in the signature so that dataset_factory can use the default.,0
!/usr/bin/python,0
Copyright 2016 Google Inc. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Read in the 50000 synsets associated with the validation data set.,0
Make all sub-directories in the validation data dir.,0
Move all of the image to the appropriate sub-directory.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
"The dataset has classes with no images (empty and human), which have ID 0 and 1,",0
so we need to specify 49 here despite having only 47 classes with images,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
4 x Inception-A blocks,0
Reduction-A block,0
7 x Inception-B blocks,0
Reduction-A block,0
3 x Inception-C blocks,0
Logits and predictions,0
Force all Variables to reside on the device.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The endpoint shapes must be equal to the original shape even when the,0
separable convolution is replaced with a normal convolution.,0
"With the 'NCHW' data format, all endpoint activations have a transposed",0
shape from the original shape with the 'NHWC' layout.,0
'NCWH' data format is not supported.,0
'NCHW' data format is not supported for separable convolution.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Final pooling and prediction,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Check graph construction for a number of image size/depths and batch,0
sizes.,0
Check layer depths.,0
Check graph construction for a number of image size/depths and batch,0
sizes.,0
Check layer depths.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The current_stride variable keeps track of the effective stride of the,0
activations. This allows us to invoke atrous convolution whenever applying,0
the next residual unit would result in the activations having stride larger,0
than the target output_stride.,0
The atrous convolution rate parameter.,0
Move stride from the block's last unit to the end of the block.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Collect activations at the block's end before performing subsampling.,0
Subsampling of the block's output activations.,0
"The following implies padding='SAME' for pool1, which makes feature",0
alignment easier for dense prediction tasks. This is also used in,0
https://github.com/facebook/fb.resnet.torch. However the accompanying,0
code of 'Deep Residual Learning for Image Recognition' uses,0
padding='VALID' for pool1. You can switch to that choice by setting,0
"slim.arg_scope([slim.max_pool2d], padding='VALID').",0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Collect outputs for conv2d, fully_connected and max_pool2d",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Input image.,0
Convolution kernel.,0
Input image.,0
Convolution kernel.,0
Test both odd and even input dimensions.,0
Dense feature extraction followed by subsampling.,0
Make the two networks use the same weights.,0
Feature extraction at the nominal network rate.,0
Test both odd and even input dimensions.,0
Subsampling at the last unit of the block.,0
Make the two networks use the same weights.,0
Subsample activations at the end of the blocks.,0
Make sure that the final output is the same.,0
Make sure that intermediate block activations in,0
output_end_points are subsampled versions of the corresponding,0
ones in expected_end_points.,0
"Like ResnetUtilsTest.testEndPointsV1(), but for the public API.",0
Dense feature extraction followed by subsampling.,0
Make the two networks use the same weights.,0
Feature extraction at the nominal network rate.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"inputs has shape [batch, 224, 224, 3]",0
"inputs has shape [batch, 513, 513, 3]",0
We do not include batch normalization or activation functions in,0
conv1 because the first ResNet unit will perform these. Cf.,0
Appendix of [2].,0
This is needed because the pre-activation variant does not have batch,1
normalization or activation functions in the residual unit output. See,0
Appendix of [2].,0
Convert end_points_collection into a dictionary of end_points.,0
Global average pooling.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
=============================================================================,0
"These parameters come from the online port, which don't necessarily match",0
those in the paper.,0
TODO(nsilberman): confirm these values with Philip.,1
##########,0
Encoder #,0
##########,0
"No normalizer for the first encoder layers as per 'Image-to-Image',",0
Section 5.1.1,0
First layer doesn't use normalizer_fn,0
Last layer doesn't use activation_fn nor normalizer_fn,0
##########,0
Decoder #,0
##########,0
"Dropout is used at both train and test time as per 'Image-to-Image',",0
Section 2.1 (last paragraph).,0
The Relu comes BEFORE the upsample op:,0
Explicitly set the normalizer_fn to None to override any default value,0
"that may come from an arg_scope, such as pix2pix_arg_scope.",0
No normalization on the input layer.,0
Stride 1 on the last layer.,0
"1-dim logits, stride 1, no activation, no normalization.",0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 Google Inc. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Most networks use 224 as their default_image_size,0
Most networks use 224 as their default_image_size,0
Most networks use 224 as their default_image_size,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TODO(joelshor): Add a test to check generator endpoints.,1
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Force all Variables to reside on the device.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
Alias,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pylint: disable=unused-import,0
pylint: enable=unused-import,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Input image.,0
Convolution kernel.,0
Input image.,0
Convolution kernel.,0
Test both odd and even input dimensions.,0
Dense feature extraction followed by subsampling.,0
Make the two networks use the same weights.,0
Feature extraction at the nominal network rate.,0
"Like ResnetUtilsTest.testEndPointsV2(), but for the public API.",0
Dense feature extraction followed by subsampling.,0
Make the two networks use the same weights.,0
Feature extraction at the nominal network rate.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
If we are fine tuning a checkpoint we need to start at a lower learning,0
rate since we are farther along on training.,0
We can start quantizing immediately if we are finetuning.,0
We need to wait for the model to train a bit before we quantize if we are,0
training from scratch.,0
Call rewriter to produce graph with fake quant ops and folded batch norms,0
"quant_delay delays start of quantization till quant_delay steps, allowing",0
for better model accuracy.,0
Configure the learning rate using an exponential decay.,0
"When restoring from a floating point model, the min/max values for",0
quantized weights and activations are not present.,0
We instruct slim to ignore variables that are missing during restoration,0
by setting ignore_missing_vars=True,0
"If we are restoring from a floating point model, we need to initialize",0
the global step to zero for the exponential decay to result in,0
reasonable learning rates.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Orignaly, arg_scope = slim.arg_scope and layers = slim, now switch to more",0
update-to-date tf.contrib.* API.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Turns off fused batch norm.,0
collection containing the moving mean and moving variance.,0
"For inverted pyramid models, we start with gating switched off.",0
batch_size x 32 x 112 x 112 x 64,0
Separable conv is slow when used at first conv layer.,0
batch_size x 32 x 56 x 56 x 64,0
batch_size x 32 x 56 x 56 x 64,0
batch_size x 32 x 56 x 56 x 192,0
batch_size x 32 x 28 x 28 x 192,0
batch_size x 32 x 28 x 28 x 256,0
batch_size x 16 x 14 x 14 x 512,0
batch_size x 16 x 14 x 14 x 512,0
batch_size x 16 x 14 x 14 x 512,0
batch_size x 16 x 14 x 14 x 528,0
batch_size x 16 x 14 x 14 x 832,0
batch_size x 8 x 7 x 7 x 832,0
batch_size x 8 x 7 x 7 x 1024,0
Final pooling and prediction,0
Temporal average pooling.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
299 x 299 x 3,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 160,0
71 x 71 x 192,0
35 x 35 x 384,0
4 x Inception-A blocks,0
35 x 35 x 384,0
Reduction-A block,0
17 x 17 x 1024,0
7 x Inception-B blocks,0
17 x 17 x 1024,0
Reduction-B block,0
8 x 8 x 1536,0
3 x Inception-C blocks,0
Auxiliary Head logits,0
17 x 17 x 1024,0
Final pooling and prediction,0
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which",1
can be set to False to disable pooling here (as in resnet_*()).,0
8 x 8 x 1536,0
1 x 1 x 1536,0
1536,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN,1
setups need the gradient of gradient FusedBatchNormGrad.,0
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN,1
setups need the gradient of gradient FusedBatchNormGrad.,0
First upscaling is different because it takes the input vector.,0
Last layer has different normalizer and activation.,0
Convert to proper channels.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80,0
71 x 71 x 192,0
35 x 35 x 192,0
35 x 35 x 320,0
TODO(alemi): Register intermediate endpoints,1
"17 x 17 x 1088 if output_stride == 8,",0
33 x 33 x 1088 if output_stride == 16,0
TODO(alemi): register intermediate endpoints,1
TODO(gpapan): Properly support output_stride for the rest of the net.,1
8 x 8 x 2080,0
TODO(alemi): register intermediate endpoints,1
8 x 8 x 1536,0
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which",1
can be set to False to disable pooling here (as in resnet_*()).,0
Set weight_decay for weights in conv2d and fully_connected layers.,0
Set activation_fn and parameters for batch_norm.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
collection containing update_ops.,0
use fused batch norm if possible.,0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"inputs has shape [batch, 224, 224, 3]",0
"inputs has shape [batch, 513, 513, 3]",0
Use clip_by_value to simulate bandpass activation.,0
Convert end_points_collection into a dictionary of end_points.,0
Global average pooling.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"end_points will collect relevant activations for external use, for example",0
summaries or losses.,0
299 x 299 x 3,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80.,0
71 x 71 x 192.,0
35 x 35 x 192.,0
Inception blocks,0
mixed: 35 x 35 x 256.,0
mixed_1: 35 x 35 x 288.,0
mixed_2: 35 x 35 x 288.,0
mixed_3: 17 x 17 x 768.,0
mixed4: 17 x 17 x 768.,0
mixed_5: 17 x 17 x 768.,0
mixed_6: 17 x 17 x 768.,0
mixed_7: 17 x 17 x 768.,0
mixed_8: 8 x 8 x 1280.,0
mixed_9: 8 x 8 x 2048.,0
mixed_10: 8 x 8 x 2048.,0
Auxiliary Head logits,0
Shape of feature map before the final layer.,0
Final pooling and prediction,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 2048,0
2048,0
1000,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Reflection pad by 1 in spatial dimensions (axes 1, 2 = h, w) to make a 3x3",0
'valid' convolution produce an output with the same dimension as the,0
input.,0
This corrects 1 pixel offset for images with even width and height.,0
conv2d is left aligned and conv2d_transpose is right aligned for even,0
sized images (while doing 'SAME' padding).,0
Note: This doesn't reflect actual model in paper.,0
Neither dropout nor batch norm -> dont need is_training,0
##########,0
Encoder #,0
##########,0
7x7 input stage,0
##################,0
Residual Blocks #,0
##################,0
##########,0
Decoder #,0
##########,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"end_points will collect relevant activations for external use, for example",0
summaries or losses.,0
Used to find thinned depths for each layer.,0
Note that sizes in the comments below assume an input spatial size of,0
"224x224, however, the inputs can be of any size greater 32x32.",0
224 x 224 x 3,0
depthwise_multiplier here is different from depth_multiplier.,0
depthwise_multiplier determines the output channels of the initial,0
"depthwise conv (see docs for tf.nn.separable_conv2d), while",0
depth_multiplier controls the # channels of the subsequent 1x1,0
convolution. Must have,0
in_channels * depthwise_multipler <= out_channels,0
so that the separable convolution is not overparameterized.,0
Use a normal convolution instead of a separable convolution.,0
112 x 112 x 64,0
56 x 56 x 64,0
56 x 56 x 64,0
56 x 56 x 192,0
28 x 28 x 192,0
Inception module.,0
28 x 28 x 256,0
28 x 28 x 320,0
14 x 14 x 576,0
14 x 14 x 576,0
14 x 14 x 576,0
14 x 14 x 576,0
14 x 14 x 576,0
7 x 7 x 1024,0
7 x 7 x 1024,0
Final pooling and prediction,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 1024,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
=============================================================================,0
For the Conv2d_0 layer FaceNet has depth=16,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
=============================================================================,0
Tensorflow mandates these.,0
Conv and DepthSepConv namedtuple define layers of the MobileNet architecture,0
Conv defines 3x3 convolution layers,0
DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.,0
stride is the stride of the convolution,0
depth is the number of channels or filters in a layer,0
MOBILENETV1_CONV_DEFS specifies the MobileNet body,0
Used to find thinned depths for each layer.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
By passing filters=None,0
separable_conv2d produces only a depthwise convolution layer,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 1024,0
Set weight_decay for weights in Conv and DepthSepConv layers.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Turns off fused batch norm.,0
collection containing the moving mean and moving variance.,0
Final pooling and prediction,0
Temporal average pooling.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
=============================================================================,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Orignaly, add_arg_scope = slim.add_arg_scope and layers = slim, now switch to",0
more update-to-date tf.contrib.* API.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
Only insert rate to params if rate > 1.,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
This is mostly a sanity test. No deep reason for these particular,0
constants.,0
,0
"All but first 2 and last one have  two convolutions, and there is one",0
extra conv that is not in the spec. (logits),0
Check that depthwise are exposed.,0
"All but 3 op has 3 conv operatore, the remainign 3 have one",0
and there is one unaccounted.,0
Verifies that depth_multiplier arg scope actually works,0
if no default min_depth is provided.,0
Verifies that depth_multiplier arg scope actually works,0
if no default min_depth is provided.,0
"All convolutions will be 8->48, except for the last one.",0
Verifies that mobilenet_base returns pre-pooling layer.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Notes for training NASNet Cifar Model,0
-------------------------------------,0
batch_size: 32,0
learning rate: 0.025,0
cosine (single period) learning rate decay,0
auxiliary head loss weighting: 0.4,0
clip global norm of all gradients by 5,0
600 epochs with a batch size of 32,0
This is used for the drop path probabilities since it needs to increase,0
the drop out probability over the course of training.,0
Notes for training large NASNet model on ImageNet,0
-------------------------------------,0
batch size (per replica): 16,0
learning rate: 0.015 * 100,0
learning rate decay factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 100 replicas,0
auxiliary head loss weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Notes for training the mobile NASNet ImageNet model,0
-------------------------------------,0
batch size (per replica): 32,0
learning rate: 0.04 * 50,0
learning rate scaling factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 50 replicas,0
auxiliary head weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Shape of feature map before the final layer.,0
149 x 149 x 32,0
Run the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Find where to place the reduction cells or stride normal cells,0
Setup for building in the auxiliary head.,0
Run the cells,0
true_cell_num accounts for the stem cells,0
Final softmax layer,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Find where to place the reduction cells or stride normal cells,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Setup for building in the auxiliary head.,0
Run the cells,0
true_cell_num accounts for the stem cells,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Final softmax layer,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Calculate the total number of cells in the network.,0
There is no distinction between reduction and normal cells in PNAS so the,0
total number of cells is equal to the number normal cells plus the number,0
of stem cells (two by default).,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Calculate the total number of cells in the network.,0
There is no distinction between reduction and normal cells in PNAS so the,0
total number of cells is equal to the number normal cells plus the number,0
of stem cells (two by default).,0
Configuration for the PNASNet-5 model.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Logits and predictions,0
Logits and predictions,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Logits and predictions,0
Logits and predictions,0
Logits and predictions,0
Force all Variables to reside on the device.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"The cap for tf.clip_by_value, it's hinted from the activation distribution",0
"that the majority of activation values are in the range [-6, 6].",0
Skip path 1,0
Skip path 2,0
"First pad with 0's on the right and bottom, then shift the filter to",0
include those 0's that were added.,0
"If odd number of filters, add an additional one to the second path.",0
Concat and apply BN,0
Set the prev layer to the current layer if it is none,0
Check to be sure prev layer stuff is setup correctly,0
num_or_size_splits=1,0
Apply conv operations,0
Combine hidden states using 'add'.,0
Add hiddenstate to the list of hiddenstates we can choose from,0
Dont stride if this is not one of the original hiddenstates,0
"Check if a stride is needed, then use a strided 1x1 here",0
Determine if a reduction should be applied to make the number of,0
filters match.,0
Return the concat of all the states,0
Scale keep prob by layer number,0
The added 2 is for the reduction cells,0
Decrease the keep probability over time,0
,0
Script for generating a two-class dataset in COCO format for training an obscured image classifier,0
,0
Requires Python >= 3.6 because of the glob ** expression,0
,0
Collect images and labels,0
"Labels: clean = 0, obscured = 1",0
,0
top_folders_to_bottom.py,0
,0
Given a base folder with files like:,0
,0
A/1/2/a.jpg,0
B/3/4/b.jpg,0
,0
"...moves the top-level folders to the bottom in a new output folder, i.e., creates:",0
,0
1/2/A/a.jpg,0
3/4/B/b.jpg,0
,0
"In practice, this is used to make this:",0
,0
animal/camera01/image01.jpg,0
,0
...look like:,0
,0
camera01/animal/image01.jpg,0
,0
%% Constants and imports,0
%% Support functions,0
%% Main functions,0
Find top-level folder,0
Find file/folder names,0
Move or copy,0
...def process_file(),0
Enumerate input folder,0
Convert absolute paths to relative paths,0
Standardize delimiters,0
Make sure each input file maps to a unique output file,0
relative_filename = relative_files[0],0
Loop,0
...def top_folders_to_bottom(),0
%% Interactive driver,0
%%,0
%%,0
%% Command-line driver,0
"python top_folders_to_bottom.py ""g:\temp\separated_images"" ""g:\temp\separated_images_inverted"" --n_threads 100",0
Convert to an options object,0
%% Constants and imports,0
This line was added circa 2018 and it made sense at the time; removing it in 2022,0
"because matplotlib *mostly* does the right thing now, and overwriting the current",0
matplotlib environment is questionable behavior.  Possible breaking change for,0
some users.,0
,0
import matplotlib; matplotlib.use('agg'),0
Assumes ai4eutils is on the python path (https://github.com/Microsoft/ai4eutils),0
Assumes the cameratraps repo root is on the path,0
%% Options,0
"Make sure there is no overlap between the two sets, because this will cause",0
issues in the code,0
## Required inputs,0
## Options,0
Can be a folder or a SAS URL,0
These apply only when we're doing ground-truth comparisons,0
"A list of output sets that we should count, but not render images for.",0
,0
"Typically used to preview sets with lots of empties, where you don't want to",0
"subset but also don't want to render 100,000 empty images.",0
,0
"detections, non_detections",0
"detections_animal, detections_person, detections_vehicle",0
Used for summary statistics only,0
"Number of images to sample, -1 for ""all images""",0
"Random seed for sampling, or None",0
Optionally separate detections into categories (animal/vehicle/human),0
Optionally replace one or more strings in filenames with other strings;,0
useful for taking a set of results generated for one folder structure,0
and applying them to a slightly different folder structure.,0
Allow bypassing API output loading when operating on previously-loaded,0
results,0
Should we also split out a separate report about the detections that were,0
just below our main confidence threshold?,0
,0
Currently only supported when ground truth is unavailable,0
Control rendering parallelization,0
...PostProcessingOptions,0
#%% Helper classes and functions,0
Anything greater than this isn't clearly positive or negative,0
image has annotations suggesting both negative and positive,0
"image is not annotated or is annotated with 'unknown', 'unlabeled', ETC.",0
image has not yet been assigned a state,0
"In some analyses, we add an additional class that lets us look at",0
detections just below our main confidence threshold,0
count the # of images with each type of DetectionStatus,0
Check whether this image has:,0
- unknown / unassigned-type labels,0
- negative-type labels,0
"- positive labels (i.e., labels that are neither unknown nor negative)",0
"assert has_unknown_labels is False, '{} has unknown labels'.format(annotations)",0
"If there are no image annotations, treat this as unknown",0
n_negative += 1,0
im['_detection_status'] = DetectionStatus.DS_NEGATIVE,0
"If the image has more than one type of labels, it's ambiguous",0
"note: bools are automatically converted to 0/1, so we can sum",0
"After the check above, we can be sure it's only one of positive,",0
"negative, or unknown.",0
,0
Important: do not merge the following 'unknown' branch with the first,0
"'unknown' branch above, where we tested 'if len(categories) == 0'",0
,0
If the image has only unknown labels,0
If the image has only negative labels,0
If the images has only positive labels,0
"Annotate the category, if it is unambiguous",0
...for each image,0
...mark_detection_status(),0
"Leaving code in place for reading from blob storage, may support this",0
in the future.,0
resize is to display them in this notebook or in the HTML more quickly,0
os.path.isfile() is slow when mounting remote directories; much faster,0
to just try/except on the image open.,0
return '',0
Render images to a flat folder... we can use os.sep here because we've,0
already normalized paths,0
"errno.ENAMETOOLONG doesn't get thrown properly on Windows, so",0
we awkwardly check against a hard-coded limit,0
Use slashes regardless of os,0
...render_bounding_boxes,0
Count items in each category,0
Optionally sort by filename before writing to html,0
Write the individual HTML files,0
...prepare_html_subpages(),0
%% Main function,0
#%% Expand some options for convenience,0
#%% Prepare output dir,0
#%% Load ground truth if available,0
Mark images in the ground truth as positive or negative,0
#%% Load detection (and possibly classification) results,0
Remove failed rows,0
Convert keys and values to lowercase,0
"Add column 'pred_detection_label' to indicate predicted detection status,",0
not separating out the classes,0
#%% Pull out descriptive metadata,0
This is rare; it only happens during debugging when the caller,0
is supplying already-loaded API results.,0
"#%% If we have ground truth, remove images we can't match to ground truth",0
#%% Sample images for visualization,0
#%% Fork here depending on whether or not ground truth is available,0
"If we have ground truth, we'll compute precision/recall and sample tp/fp/tn/fn.",0
,0
Otherwise we'll just visualize detections/non-detections.,0
#%% Detection evaluation: compute precision/recall,0
numpy array of detection probabilities,0
"numpy array of bools (0.0/1.0), and -1 as null value",0
Don't include ambiguous/unknown ground truth in precision/recall analysis,0
"For completeness, include the result at a confidence threshold of 1.0",0
Compute and print summary statistics,0
Thresholds go up throughout precisions/recalls/thresholds; find the last,0
value where recall is at or above target.  That's our precision @ target recall.,0
"np.where returns a tuple of arrays, but in this syntax where we're",0
"comparing an array with a scalar, there will only be one element.",0
Convert back to a list,0
Flatten the confusion matrix,0
"#%% Collect classification results, if they exist",0
Mapping of classnames to idx for the confusion matrix.,0
,0
"The lambda is actually kind of a hack, because we use assume that",1
the following code does not reassign classname_to_idx,0
Confusion matrix as defaultdict of defaultdict,0
,0
"Rows / first index is ground truth, columns / second index is predicted category",0
iDetection = 0; fn = detector_files[iDetection]; print(fn),0
"If this image has classification predictions, and an unambiguous class",0
"annotated, and is a positive image...",0
"The unambiguous category, we make this a set for easier handling afterward",0
"Compute the accuracy as intersection of union,",0
i.e. (# of categories in both prediciton and GT),0
divided by (# of categories in either prediction or GT,0
,0
"In case of only one GT category, the result will be 1.0, if",0
prediction is one category and this category matches GT,0
,0
"It is 1.0/(# of predicted top-1 categories), if the GT is",0
one of the predicted top-1 categories.,0
,0
"It is 0.0, if none of the predicted categories is correct",0
Distribute this accuracy across all predicted categories in the,0
confusion matrix,0
...for each file in the detection results,0
If we have classification results,0
Build confusion matrix as array from classifier_cm,0
Print some statistics,0
Prepare confusion matrix output,0
Get confusion matrix as string,0
Get fixed-size classname for each idx,0
Prepend class name on each line and add to the top,0
Print formatted confusion matrix,0
Plot confusion matrix,0
To manually add more space at bottom: plt.rcParams['figure.subplot.bottom'] = 0.1,0
,0
"Add 0.5 to figsize for every class. For two classes, this will result in",0
"fig = plt.figure(figsize=[4,4])",0
...if we have classification results,0
#%% Render output,0
Write p/r table to .csv file in output directory,0
Write precision/recall plot to .png file in output directory,0
#%% Sampling,0
Sample true/false positives/negatives with correct/incorrect top-1,0
classification and render to html,0
Accumulate html image structs (in the format expected by write_html_image_lists),0
"for each category, e.g. 'tp', 'fp', ..., 'class_bird', ...",0
Add default entries by accessing them for the first time,0
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]",0
"Each element will be a three-tuple with elements file,max_conf,detections",0
"Assemble the information we need for rendering, so we can parallelize without",0
dealing with Pandas,0
i_row = 0; row = images_to_visualize.iloc[0],0
Filenames should already have been normalized to either '/' or '\',0
This should already have been normalized to either '/' or '\',0
...def render_image_with_gt(file_info),0
file_info = files_to_render[0],0
Map all the rendering results in the list rendering_results into the,0
dictionary images_html,0
Prepare the individual html image files,0
Write index.html,0
Show links to each GT class,0
,0
We could do this without classification results; currently we don't.,0
Add links to all available classes,0
Close body and html tags,0
...for each image,0
"#%% Otherwise, if we don't have ground truth...",0
#%% Sample detections/non-detections,0
Accumulate html image structs (in the format expected by write_html_image_list),0
for each category,0
Add default entries by accessing them for the first time,0
"Maps detection categories - e.g. ""human"" - to result set names, e.g.",0
"""detections_human""",0
Add a set of results for each category and combination of categories,0
Create output directories,0
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]",0
"list of 3-tuples with elements (file, max_conf, detections)",0
"Assemble the information we need for rendering, so we can parallelize without",0
dealing with Pandas,0
i_row = 0; row = images_to_visualize.iloc[0],0
Filenames should already have been normalized to either '/' or '\',0
Get unique categories above the threshold for this image,0
Local function for parallelization,0
"This is a list of [class,confidence] pairs, sorted by confidence",0
"If we either don't have a confidence threshold, or we've met our",0
confidence threshold,0
...if this detection has classification info,0
...for each detection,0
...def render_image_no_gt(file_info):,0
Map all the rendering results in the list rendering_results into the,0
dictionary images_html,0
Prepare the individual html image files,0
Write index.html,0
"We can't just sum these, because image_counts includes images in both their",0
detection and classification classes,0
total_images = sum(image_counts.values()),0
Don't print classification classes here; we'll do that later with a slightly,0
different structure,0
Add links to all available classes,0
os.startfile(output_html_file),0
...if we do/don't have ground truth,0
...process_batch_results,0
%% Interactive driver(s),0
%%,0
options.unlabeled_classes = ['human'],0
os.start(ppresults.output_html_file),0
%% Command-line driver,0
,0
subset_json_detector_output.py,0
,0
"Creates one or more subsets of a detector API output file (.json), doing either",0
"or both of the following (if both are requested, they happen in this order):",0
,0
"1) Retrieve all elements where filenames contain a specified query string,",0
"optionally replacing that query with a replacement token. If the query is blank,",0
can also be used to prepend content to all filenames.,0
,0
"2) Create separate .jsons for each unique path, optionally making the filenames",0
"in those .json's relative paths.  In this case, you specify an output directory,",0
rather than an output path.  All images in the folder blah\foo\bar will end up,0
in a .json file called blah_foo_bar.json.,0
,0
##,0
,0
Sample invocations (splitting into multiple json's):,0
,0
"Read from ""1800_idfg_statewide_wolf_detections_w_classifications.json"", split up into",0
"individual .jsons in 'd:\temp\idfg\output', making filenames relative to their individual",0
folders:,0
,0
"python subset_json_detector_output.py ""d:\temp\idfg\1800_idfg_statewide_wolf_detections_w_classifications.json"" ""d:\temp\idfg\output"" --split_folders --make_folder_relative",0
,0
"Now do the same thing, but instead of writing .json's to d:\temp\idfg\output, write them to *subfolders*",0
corresponding to the subfolders for each .json file.,0
,0
"python subset_json_detector_output.py ""d:\temp\idfg\1800_detections_S2.json"" ""d:\temp\idfg\output_to_folders"" --split_folders --make_folder_relative --copy_jsons_to_folders",0
,0
##,0
,0
Sample invocations (creating a single subset matching a query):,0
,0
"Read from ""1800_detections.json"", write to ""1800_detections_2017.json""",0
,0
"Include only images matching ""2017"", and change ""2017"" to ""blah""",0
,0
"python subset_json_detector_output.py ""d:\temp\1800_detections.json"" ""d:\temp\1800_detections_2017_blah.json"" --query 2017 --replacement blah",0
,0
"Include all images, prepend with ""prefix/""",0
,0
"python subset_json_detector_output.py ""d:\temp\1800_detections.json"" ""d:\temp\1800_detections_prefix.json"" --replacement ""prefix/""",0
,0
##,0
,0
"To subset a COCO Camera Traps .json database, see subset_json_db.py",0
,0
%% Constants and imports,0
%% Helper classes,0
Only process files containing the token 'query',0
"Replace 'query' with 'replacement' if 'replacement' is not None.  If 'query' is None,",0
prepend 'replacement',0
Should we split output into individual .json files for each folder?,1
"Folder level to use for splitting ['bottom','top','n_from_bottom','n_from_top','dict']",0
,0
'dict' requires 'split_folder_param' to be a dictionary mapping each filename,0
to a token.,0
"When using the 'n_from_bottom' parameter to define folder splitting, this",0
defines the number of directories from the bottom.  'n_from_bottom' with,0
a parameter of zero is the same as 'bottom'.,0
,0
Same story with 'n_from_top'.,0
,0
"When 'split_folder_mode' is 'dict', this should be a dictionary mapping each filename",0
to a token.,0
Only meaningful if split_folders is True: should we convert pathnames to be relative,0
the folder for each .json file?,0
"Only meaningful if split_folders and make_folder_relative are True: if not None,",0
"will copy .json files to their corresponding output directories, relative to",0
output_filename,0
Should we over-write .json files?,0
"If copy_jsons_to_folders is true, do we require that directories already exist?",0
Threshold on confidence,0
Should we remove failed images?,0
%% Main function,0
Format spec:,0
,0
https://github.com/microsoft/CameraTraps/tree/master/api/batch_processing,0
iImage = 0; im = images_in[0],0
Find all detections above threshold for this image,0
"If there are no detections above threshold, set the max probability",0
"to -1, unless it already had a negative probability.",0
Otherwise find the max confidence,0
Did this thresholding result in a max-confidence change?,0
"We should only be *lowering* max confidence values (i.e., making them negative)",0
...for each image,0
i_image = 0; im = images_in[0],0
...for each image,0
i_image = 0; im = images_in[0],0
Only take images that match the query,0
...for each image,0
"Path('/blah').parts is ('/','blah')",0
Handle paths like:,0
,0
"/, \, /stuff, c:, c:\stuff",0
Input validation,0
data = add_missing_detection_results_fields(data),0
Map images to unique folders,0
im = data['images'][0],0
"Split string into folders, keeping delimiters",0
"Don't use this, it removes delimiters",0
tokens = split_path(fn),0
Optionally make paths relative,0
dirname = list(folders_to_images.keys())[0],0
im = folders_to_images[dirname][0],0
dirname = list(folders_to_images.keys())[0],0
"Recycle the 'data' struct, replacing 'images' every time... medium-hacky, but",1
forward-compatible in that I don't take dependencies on the other fields,0
...for each directory,0
...if we're splitting folders,0
%% Interactive driver,0
%%,0
%% Subset a file without splitting,0
"%% Subset and split, but don't copy to individual folders",0
"%% Subset and split, copying to individual folders",0
%% Just do a filename replacement,0
"python subset_json_detector_output.py ""D:\temp\idfg\detections_idfg_20190625_refiltered.json"" ""D:\temp\idfg\detections_idfg_20190625_refiltered_renamed.json"" --query ""20190625-hddrop/"" --replacement """"",0
"python subset_json_detector_output.py ""D:\temp\idfg\detections_idfg_20190625_refiltered_renamed.json"" ""D:\temp\idfg\output"" --split_folders --make_folder_relative --copy_jsons_to_folders",0
%% Command-line driver,0
Convert to an options object,0
%% Constants and imports,0
%% Merge functions,0
"Map image filenames to detections, we'll convert to a list later",0
Check compatibility of detection categories,0
Check compatibility of classification categories,0
"Merge image lists, checking uniqueness",0
Replace a previous failure with a success,0
"Merge info dicts, don't check completion time fields",0
...for each dictionary,0
Convert merged image dictionaries to a sorted list,0
detection_list = input_lists[0],0
d = detection_list[0],0
%% Driver,0
,0
load_api_results.py,0
,0
Loads the output of the batch processing API (json) into a pandas dataframe.,0
,0
Also functions to group entries by seq_id.,0
,0
Includes the deprecated functions that worked with the old CSV API output format.,0
,0
%% Constants and imports,0
%% Functions for grouping by sequence_id,0
example,0
input 'file': 'SER/S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG',0
output 'id': 'S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG',0
%% Functions for loading the result as a Pandas DataFrame,0
Validate that this is really a detector output file,0
Fields in the API output json other than 'images',0
Normalize paths to simplify comparisons later,0
"image['file'] = image['file'].replace('\\','/')",0
Pack the json output into a Pandas DataFrame,0
Replace some path tokens to match local paths to original blob structure,0
string_to_replace = list(filename_replacements.keys())[0],0
Sanity-check that this is really a detector output file,0
Normalize paths to simplify comparisons later,0
De-serialize detections,0
Optionally replace some path tokens to match local paths to the original blob structure,0
string_to_replace = list(options.detector_output_filename_replacements.keys())[0],0
"TODO: hit some silly issues with vectorized str() and escaped characters, vectorize",1
this later.,0
,0
"detection_results['image_path'].str.replace(string_to_replace,replacement_string)",0
iRow = 0,0
%% Imports,0
Assumes ai4eutils is on the python path (https://github.com/Microsoft/ai4eutils),0
%% Constants and support classes,0
We will confirm that this matches what we load from each file,0
%% Main function,0
#%% Validate inputs,0
#%% Load both result sets,0
#%% Make sure they represent the same set of images,0
#%% Find differences,0
"Each of these maps a filename to a two-element list (the image in set A, the image in set B)",0
,0
"Right now, we only handle a very simple notion of class transition, where the detection",0
of maximum confidence changes class *and* both images have an above-threshold detection.,0
fn = filenames_a[0],0
det = im_a['detections'][0],0
...for each filename,0
#%% Sample and plot differences,0
"Render two sets of results (i.e., a comparison) for a single",0
image.,0
...def render_image_pair(),0
fn = image_filenames[0],0
...def render_detection_comparisons(),0
"For each category, generate comparison images and the",0
comparison HTML page.,0
,0
category = 'common_detections',0
Choose detection pairs we're going to render for this category,0
...for each category,0
#%% Write the top-level HTML file content,0
...def compare_batch_results(),0
%% Interactive driver,0
%% KRU,0
%% Command-line driver,0
# TODO,1
,0
"Merge high-confidence detections from one results file into another file,",0
when the target file does not detect anything on an image.,0
,0
Does not currently attempt to merge every detection based on whether individual,0
detections are missing; only merges detections into images that would otherwise,0
be considered blank.,0
,0
"If you want to literally merge two .json files, see combine_api_outputs.py.",0
,0
%% Constants and imports,0
%% Structs,0
"If you want to merge only certain categories, specify one",0
(but not both) of these.,0
%% Main function,0
im = output_data['images'][0],0
"Determine whether we should be processing all categories, or just a subset",0
of categories.,0
i_source_file = 0; source_file = source_files[i_source_file],0
source_im = source_data['images'][0],0
detection_category = list(detection_categories)[0],0
"This is already a detection, no need to proceed looking for detections to",0
transfer,0
Boxes are x/y/w/h,0
source_sizes = [det['bbox'][2]*det['bbox'][3] for det in source_detections_this_category_raw],0
Only look at boxes below the size threshold,0
...for each detection category,0
"print('Adding {} detections to image {}'.format(len(detections_to_transfer),image_filename))",0
Update the max_detection_conf field,0
...for each image,0
...for each source file,0
%% Test driver,0
%%,0
%% Command-line driver (TODO),0
#######,0
,0
convert_output_format.py,0
,0
Converts between file formats output by our batch processing API.  Currently,0
"supports json <--> csv conversion, but this should be the landing place for any",0
conversion - including between future .json versions - that we support in the,0
future.,0
,0
#######,0
%% Imports,0
%% Conversion functions,0
"We add an output column for each class other than 'empty',",0
containing the maximum probability of  that class for each image,0
Skip sub-threshold detections,0
Our .json format is xmin/ymin/w/h,0
,0
Our .csv format was ymin/xmin/ymax/xmax,0
"Category 0 is empty, for which we don't have a column, so the max",0
confidence for category N goes in column N-1,0
...for each detection,0
...for each image,0
Format spec:,0
,0
https://github.com/microsoft/CameraTraps/tree/master/api/batch_processing,0
iFile = 0; row = df.iloc[iFile],0
Our .csv format was ymin/xmin/ymax/xmax,0
,0
Our .json format is xmin/ymin/w/h,0
...for each detection,0
...for each image,0
%% Interactive driver,0
%%,0
%%,0
%% Concatenate .csv files from a folder,0
...for each .csv file,0
with open(master_csv),0
%% Command-line driver,0
,0
separate_detections_into_folders.py,0
,0
"Given a .json file with batch processing results, separate the files in that",0
"set of results into folders that contain animals/people/vehicles/nothing,",0
according to per-class thresholds.,0
,0
"Image files are copied, not moved.",0
,0
Preserves relative paths within each of those folders; cannot be used with .json,0
files that have absolute paths in them.,0
,0
"For example, if your .json file has these images:",0
,0
a/b/c/1.jpg,0
a/b/d/2.jpg,0
a/b/e/3.jpg,0
a/b/f/4.jpg,0
,0
And let's say:,0
,0
"* The results say that the first three images are empty/person/vehicle, respectively",0
"* The fourth image is above threshold for ""animal"" and ""person""",0
* You specify an output base folder of c:\out,0
,0
You will get the following files:,0
,0
c:\out\empty\a\b\c\1.jpg,0
c:\out\people\a\b\d\2.jpg,0
c:\out\vehicles\a\b\e\3.jpg,0
c:\out\animal_person\a\b\f\4.jpg,0
,0
Hard-coded to work with MDv3 and MDv4 output files.  Not currently future-proofed,0
"past the classes in MegaDetector v4, not currently ready for species-level classification.",0
,0
%% Constants and imports,0
Occasionally we have near-zero confidence detections associated with COCO classes that,0
didn't quite get squeezed out of the model in training.  As long as they're near zero,0
"confidence, we just ignore them.",0
%% Options class,0
"Dictionary mapping categories (plus combinations of categories, and 'empty') to output folders",0
%% Support functions,0
Find the maximum confidence for each category,0
,0
det = detections[0],0
"For zero-confidence detections, we occasionally have leftover goop",0
from COCO classes,0
assert det['conf'] < invalid_category_epsilon,0
Count the number of thresholds exceeded,0
Do we have a custom threshold for this category?,0
If this is above multiple thresholds,0
if this is/isn't a failure case,0
...def process_detection(),0
%% Main function,0
Create output folder if necessary,0
Load detection results,0
Map class names to output folders,0
Create all combinations of categories,0
Do we have a custom threshold for this category?,0
Create folder mappings for each category,0
Create the actual folders,0
%% Interactive driver,0
%%,0
%%,0
%% Find a particular file,0
%% Command-line driver,0
"python api\batch_processing\postprocessing\separate_detections_into_folders.py ""d:\temp\rspb_mini.json"" ""d:\temp\demo_images\rspb_2018_2019_mini"" ""d:\temp\separation_test"" --nthreads 2",0
Convert to an options object,0
For each image...,0
,0
im = images[0],0
d = im['detections'][0],0
https://github.com/microsoft/CameraTraps/tree/master/api/batch_processing#detector-outputs,0
...for each detection,0
...for each image,0
...def categorize_detections_by_size(),0
#######,0
,0
remove_repeat_detections.py,0
,0
"Used after running find_repeat_detections, then manually filtering the results,",0
to create a final filtered output file.,0
,0
"If you want to use this script, we recommend that you read the user's guide:",0
,0
https://github.com/microsoft/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms,0
,0
#######,0
%% Constants and imports,0
%% Main function,0
%% Interactive driver,0
%%,0
"python remove_repeat_detections.py ""F:\wpz\6714_detections_wpz_all_20191015233705.SUCP_subset.json"" ""F:\wpz\6714_detections_wpz_all_20191015233705.SUCP_subset_filtered.json"" ""F:\wpz\rde\filtering_2019.10.24.16.52.54""",0
%% Command-line driver,0
#######,0
,0
repeat_detections_core.py,0
,0
Core utilities shared by find_repeat_detections and remove_repeat_detections.,0
,0
#######,0
%% Imports and environment,0
Note to self: other indexing options:,0
,0
https://rtree.readthedocs.io (not thread- or process-safe),1
https://github.com/sergkr/rtreelib,0
https://github.com/Rhoana/pyrtree,0
"from ai4eutils; this is assumed to be on the path, as per repo convention",0
"""PIL cannot read EXIF metainfo for the images""",0
"""Metadata Warning, tag 256 had too many entries: 42, expected 1""",0
%% Constants,0
%% Classes,0
Relevant for rendering HTML or filtering folder of images,0
,0
"imageBase can also be a SAS URL, in which case some error-checking is",0
disabled.,0
Don't consider detections with confidence lower than this as suspicious,0
Don't consider detections with confidence higher than this as suspicious,0
What's the IOU threshold for considering two boxes the same?,0
How many occurrences of a single location (as defined by the IOU threshold),0
are required before we declare it suspicious?,0
"Ignore ""suspicious"" detections larger than some size; these are often animals",0
taking up the whole image.  This is expressed as a fraction of the image size.,0
"Ignore folders with more than this many images in them, which can stall the process",0
A list of classes we don't want to treat as suspicious. Each element is an int.,0
Load detections from a filter file rather than finding them from the detector output,0
".json file containing detections, should be called detectionIndex.json in the filtering_* folder",0
produced in the first pass,0
(optional) List of filenames remaining after deletion of identified,0
repeated detections that are actually animals.  This should be a flat,0
"text file, one relative filename per line.  See enumerate_images().",0
Turn on/off optional outputs,0
Determines whether bounding-box rendering errors (typically network errors) should,0
be treated as failures,0
Box rendering options,0
State variables,0
"Replace filename tokens after reading, useful when the directory structure",0
has changed relative to the structure the detector saw,0
How many folders up from the leaf nodes should we be going to aggregate images?,0
An optional function that takes a string (an image file name) and returns a string (the corresponding,0
"folder ID), typically used when multiple folders actually correspond to the same camera in a",0
manufacturer-specific way (e.g. a/b/c/RECONYX100 and a/b/c/RECONYX101 may really be the same camera).,0
Include/exclude specific folders... only one of these may be,0
"specified; ""including"" folders includes *only* those folders.",0
Sort detections within a directory so nearby detections are adjacent,0
"in the list, for faster review.",0
,0
"Can be None, 'xsort', or 'clustersort'",0
"The data table (Pandas DataFrame), as loaded from the input json file via",0
load_api_results(),0
"The other fields in the input json file, loaded via load_api_results()",0
The data table after modification,0
dict mapping folder names to whole rows from the data table,0
dict mapping filenames to rows in the master table,0
"An array of length nDirs, where each element is a list of DetectionLocation",0
objects for that directory that have been flagged as suspicious,0
%% Helper functions,0
"We store detetions as x/y/w/h, rtree and pyqtree use l/b/r/t",0
"We store detetions as x/y/w/h, rtree and pyqtree use l/b/r/t",0
%% Sort a list of candidate detections to make them visually easier to review,0
Just sort by the X location of each box,0
"Prepare a list of points to represent each box,",0
that's what we'll use for clustering,0
Upper-left,0
"points.append([det.bbox[0],det.bbox[1]])",0
Center,0
"Labels *could* be any unique labels according to the docs, but in practice",0
they are unique integers from 0:nClusters,0
Make sure the labels are unique incrementing integers,0
Store the label assigned to each cluster,0
"Now sort the clusters by their x coordinate, and re-assign labels",0
so the labels are sortable,0
"Compute the centroid for debugging, but we're only going to use the x",0
"coordinate.  This is the centroid of points used to represent detections,",0
which may be box centers or box corners.,0
old_cluster_label_to_new_cluster_label[old_cluster_label] =\,0
new_cluster_labels[old_cluster_label],0
%% Look for matches (one directory),0
List of DetectionLocations,0
candidateDetections = [],0
Create a tree to store candidate detections,0
"Each image in this folder is a row in ""rows""",0
For each image in this directory,0
,0
iDirectoryRow = 0; row = rows.iloc[iDirectoryRow],0
,0
"iDirectoryRow is a pandas index, so it may not start from zero;",0
"for debugging, we maintain i_iteration as a loop index.",0
"print('Searching row {} of {} (index {}) in dir {}'.format(i_iteration,len(rows),iDirectoryRow,dirName))",0
Don't bother checking images with no detections above threshold,0
"Array of dicts, where each element is",0
{,0
"'category': '1',  # str value, category ID",0
"'conf': 0.926,  # confidence of this detections",0
,0
"# (x_min, y_min) is upper-left, all in relative coordinates",0
"'bbox': [x_min, y_min, width_of_box, height_of_box]",0
,0
},0
For each detection in this image,0
"This is no longer strictly true; I sometimes run RDE in stages, so",0
some probabilities have already been made negative,0
,0
assert confidence >= 0.0 and confidence <= 1.0,0
Optionally exclude some classes from consideration as suspicious,0
Is this detection too big to be suspicious?,0
print('Illegal zero-size bounding box on image {}'.format(filename)),0
These are relative coordinates,0
print('Ignoring very large detection with area {}'.format(area)),0
This will return candidates of all classes,0
For each detection in our candidate list,0
Don't match across categories,0
Is this a match?,0
"If so, add this example to the list for this detection",0
We *don't* break here; we allow this instance to possibly,0
match multiple candidates.  There isn't an obvious right or,0
wrong here.,0
...for each detection on our candidate list,0
"If we found no matches, add this to the candidate list",0
candidateDetections.append(candidate),0
pyqtree,0
...for each detection,0
...for each row,0
Get all candidate detections,0
print('Found {} candidate detections for folder {}'.format(,0
"len(candidateDetections),dirName))",0
"For debugging only, it's convenient to have these sorted",0
as if they had never gone into a tree structure.  Typically,0
this is in practce a sort by filename.,0
...def find_matches_in_directory(dirName),0
%% Render candidate repeat detections to html,0
suspiciousDetectionsThisDir is a list of DetectionLocation objects,0
For each problematic detection in this directory,0
,0
iDetection = 0; detection = suspiciousDetectionsThisDir[iDetection];,0
iDetection = 0; detection = suspiciousDetectionsThisDir[0],0
_ = pretty_print_object(detection),0
Render images,0
iInstance = 0; instance = detection.instances[iInstance],0
...for each instance,0
Write html for this detection,0
Use the first image from this detection (arbitrary) as the canonical example,0
that we'll render for the directory-level page.,0
...for each detection,0
Write the html file for this directory,0
...def render_images_for_directory(iDir),0
"%% Update the detection table based on suspicious results, write .csv output",0
"An array of length nDirs, where each element is a list of DetectionLocation",0
objects for that directory that have been flagged as suspicious,0
For each directory,0
For each suspicious detection group in this directory,0
For each instance of this suspicious detection,0
This should match the bbox for the detection event,0
The bbox for this instance should be almost the same as the bbox,0
"for this detection group, where ""almost"" is defined by the IOU",0
threshold.,0
if iou < options.iouThreshold:,0
"print('IOU warning: {},{}'.format(iou,options.iouThreshold))",0
Make sure the bounding box matches,0
"Make the probability negative, if it hasn't been switched by",0
another bounding box,0
...for each instance,0
...for each detection,0
...for each directory,0
Update maximum probabilities,0
For each row...,0
No longer strictly true; sometimes I run RDE on RDE output,0
assert maxPOriginal >= 0,0
We should only be making detections *less* likely,0
row['max_confidence'] = str(maxP),0
"Negative probabilities should be the only reason maxP changed, so",0
we should have found at least one negative value,0
...if there was a meaningful change to the max probability for this row,0
...for each row,0
If we're also writing output...,0
"...def update_detection_table(RepeatDetectionResults,options)",0
%% Main function,0
#%% Input handling,0
Validate some options,0
Load the filtering file,0
Load the same options we used when finding repeat detections,0
...except for things that explicitly tell this function not to,0
find repeat detections.,0
...if we're loading from an existing filtering file,0
Check early to avoid problems with the output folder,0
Load file,0
detectionResults[detectionResults['failure'].notna()],0
"Before doing any real work, make sure we can *probably* access images",0
"This is just a cursory check on the first image, but it heads off most",0
"problems related to incorrect mount points, etc.  Better to do this before",0
spending 20 minutes finding repeat detections.,0
#%% Separate files into directories,0
This will be a map from a directory name to smaller data frames,0
This is a mapping back into the rows of the original table,0
TODO: in the case where we're loading an existing set of FPs after,1
"manual filtering, we should load these data frames too, rather than",0
re-building them from the input.,0
iRow = 0; row = detectionResults.iloc[0],0
Create a new DataFrame with just this row,0
rowsByDirectory[dirName] = pd.DataFrame(row),0
...for each unique detection,0
Convert lists of rows to proper DataFrames,0
#% Look for matches (or load them from file),0
"Are we actually looking for matches, or just loading from a file?",0
length-nDirs list of lists of DetectionLocation objects,0
We're actually looking for matches...,0
iDir = 4; dirName = dirsToSearch[iDir],0
"for iDir, dirName in enumerate(tqdm(dirsToSearch)):",0
#%% Find suspicious locations based on match results,0
For each directory,0
,0
iDir = 51,0
A list of DetectionLocation objects,0
A list of DetectionLocation objects,0
occurrenceList is a list of file/detection pairs,0
Sort the above-threshold detections for easier review,0
...for each directory,0
If we're just loading detections from a file...,0
"We're skipping detection-finding, but to see which images are actually legit false",0
"positives, we may be looking for physical files or loading from a text file.",0
For each directory,0
iDir = 0; detections = suspiciousDetections[0],0
,0
"suspiciousDetections is an array of DetectionLocation objects,",0
one per directory.,0
For each detection that was present before filtering,0
iDetection = 0; detection = detections[iDetection],0
Are we checking the directory to see whether detections were actually false,0
"positives, or reading from a list?",0
Is the image still there?,0
"If not, remove this from the list of suspicious detections",0
...for each detection,0
...for each directory,0
...if we are/aren't finding detections (vs. loading from file),0
Render problematic locations with html (loop),0
options.pbar = tqdm(total=nDirs),0
For each directory,0
iDir = 51,0
Add this directory to the master list of html files,0
...for each directory,0
Write master html file,0
Remove unicode characters before formatting,0
...if we're rendering html,0
Create filtering directory,0
iDir = 0; suspiciousDetectionsThisDir = suspiciousDetections[iDir],0
suspiciousDetectionsThisDir is a list of DetectionLocation objects,0
iDetection = 0; detection = suspiciousDetectionsThisDir[0],0
Write out the detection index,0
...if we're writing filtering info,0
...find_repeat_detections(),0
#######,0
,0
find_repeat_detections.py,0
,0
"If you want to use this script, we recommend that you read the user's guide:",0
,0
https://github.com/microsoft/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms,0
,0
"Really, don't try to run this script without reading the user's guide, you'll think",0
it's more magical than it is.,0
,0
"This script looks through a sequence of detections in the API output json file, and finds",0
"candidates that might be ""repeated false positives"", i.e. that random branch that the",0
detector thinks is an animal/person/vehicle.,0
,0
"Typically after running this script, you would do a manual step to remove",0
"true positives, then run remove_repeat_detections to produce a final output file.",0
,0
There's no way that statement was self-explanatory; see the user's guide.,0
,0
#######,0
%% Constants and imports,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
Convert to an options object,0
,0
If a request has been sent to AML for batch scoring but the monitoring thread of the API was,0
"interrupted (uncaught exception or having to re-start the API container), we could manually",0
"aggregate results from each shard using this script, assuming all jobs submitted to AML have finished.",0
,0
Need to have set environment variables STORAGE_ACCOUNT_NAME and STORAGE_ACCOUNT_KEY to those of the,0
"storage account backing the API. Also need to adjust the INTERNAL_CONTAINER, AML_CONTAINER and",0
AML_CONFIG fields in api_core/orchestrator_api/api_config.py to match the instance of the API that this,0
request was submitted to.,0
,0
May need to change the import statement in api_core/orchestrator_api/orchestrator.py,0
"""from sas_blob_utils import SasBlob"" to",0
"""from .sas_blob_utils import SasBlob"" to not confuse with the module in AI4Eutils;",0
"and change ""import api_config"" to",0
"""from api.batch_processing.api_core.orchestrator_api import api_config""",0
Execute this script from the root of the repository. You may need to add the repository to PYTHONPATH.,0
"list_jobs_submitted cannot be serialized (""can't pickle _thread.RLock objects ""), but",0
do not need it for aggregating results,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%% helper classes and functions,0
TODO log exception when we have more telemetry,1
TODO check that the expiry date of input_container_sas is at least a month,0
into the future,0
"if no permission specified explicitly but has an access policy, assumes okay",0
TODO - check based on access policy as well,1
return current UTC time as a string in the ISO 8601 format (so we can query by,0
timestamp in the Cosmos DB job status table.,0
example: '2021-02-08T20:02:05.699689Z',0
"image_paths will have length at least 1, otherwise would have ended before this step",0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%% instance-specific API settings,0
you likely need to modify these when deploying a new instance of the API,0
Azure Batch for batch processing,0
%% general API settings,0
"if this number of times the thread wakes up to check is exceeded, stop the monitoring thread",0
"quota of active Jobs in our Batch account, which all node pools i.e. API instances share;",0
cannot accept job submissions if there are this many active Jobs already,0
%% MegaDetector info,0
relative to the `megadetector_copies` folder in the container `models`,0
TODO add MD versions info to AppConfig,1
copied from TFDetector class in detection/run_detector.py,0
%% Azure Batch settings,0
"%% env variables for service credentials, and info related to these services",0
Cosmos DB `batch-api-jobs` table for job status,0
"Service principal of this ""application"", authorized to use Azure Batch",0
Blob storage account for storing Batch tasks' outputs and scoring script,0
STORAGE_CONTAINER_MODELS = 'models'  # names of the two containers supporting Batch,0
Azure Container Registry for Docker image used by our Batch node pools,0
Azure App Configuration instance to get configurations specific to,0
this instance of the API,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
a job moves from created to running/problem after the Batch Job has been submitted,0
"job_id should be unique across all instances, and is also the partition key",0
TODO do not read the entry first to get the call_params when the Cosmos SDK add a,1
patching functionality:,0
https://feedback.azure.com/forums/263030-azure-cosmos-db/suggestions/6693091-be-able-to-do-partial-updates-on-document,0
need to retain other fields in 'status' to be able to restart monitoring thread,0
retain existing fields; update as needed,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
sentinel should change if new configurations are available,0
configs have not changed,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Gunicorn logger handler will get attached if needed in server.py,0
set for all tasks in the job,0
cannot execute the scoring script that is in the mounted directory; has to be copied to cwd,0
not luck giving the commandline arguments via formatted string - set as env vars instead,0
form shards of images and assign each shard to a Task,0
for persisting stdout and stderr,0
persist stdout and stderr (will be removed when node removed),0
paths are relative to the Task working directory,0
can also just upload on failure,0
first try submitting Tasks,0
retry submitting Tasks,0
Change the Job's on_all_tasks_complete option to 'terminateJob' so the Job's status changes automatically,0
after all submitted tasks are done,0
This is so that we do not take up the quota for active Jobs in the Batch account.,0
return type: TaskAddCollectionResult,0
actually we should probably only re-submit if it's a server_error,0
docs: # https://docs.microsoft.com/en-us/rest/api/batchservice/odata-filters-in-batch#list-tasks,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
%% Flask app,0
reference: https://trstringer.com/logging-flask-gunicorn-the-manageable-way/,0
%% Helper classes,0
%% Flask endpoints,0
required params,0
can be an URL to a file not hosted in an Azure blob storage container,0
"if use_url, then images_requested_json_sas is required",0
optional params,0
check model_version is among the available model versions,0
check request_name has only allowed characters,0
optional params for telemetry collection - logged to status table for now as part of call_params,0
All API instances / node pools share a quota on total number of active Jobs;,0
we cannot accept new Job submissions if we are at the quota,0
required fields,0
request_status is either completed or failed,0
the create_batch_job thread will stop when it wakes up the next time,0
"Fix for Zooniverse - deleting any ""-"" characters in the job_id",0
"If the status is running, it could be a Job submitted before the last restart of this",0
"API instance. If that is the case, we should start to monitor its progress again.",0
WARNING model_version could be wrong (a newer version number gets written to the output file) around,0
"the time that  the model is updated, if this request was submitted before the model update",0
and the API restart; this should be quite rare,0
conform to previous schemes,0
%% undocumented endpoints,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Gunicorn logger handler will get attached if needed in server.py,0
request_name and request_submission_timestamp are for appending to,0
output file names,0
image_paths can be a list of strings (Azure blob names or public URLs),0
"or a list of length-2 lists where each is a [image_id, metadata] pair",0
Case 1: listing all images in the container,0
- not possible to have attached metadata if listing images in a blob,0
list all images to process,0
+ 1 so if the number of images listed > MAX_NUMBER_IMAGES_ACCEPTED_PER_JOB,0
we will know and not proceed,0
Case 2: user supplied a list of images to process; can include metadata,0
filter down to those conforming to the provided prefix and accepted suffixes (image file types),0
prefix is case-sensitive; suffix is not,0
"Although urlparse(p).path preserves the extension on local paths, it will not work for",0
"blob file names that contains ""#"", which will be treated as indication of a query.",0
"If the URL is generated via Azure Blob Storage, the ""#"" char will be properly encoded",0
apply the first_n and sample_n filters,0
OK if first_n > total number of images,0
sample by shuffling image paths and take the first sample_n images,0
"upload the image list to the container, which is also mounted on all nodes",0
all sharding and scoring use the uploaded list,0
now request_status moves from created to running,0
an extra field to allow the monitoring thread to restart after an API restart: total number of tasks,0
also record the number of images to process for reporting,0
start the monitor thread with the same name,0
"both succeeded and failed tasks are marked ""completed"" on Batch",0
"preserving format from before, but SAS URL to 'failed_images' and 'images' are no longer provided",0
"failures should be contained in the output entries, indicated by an 'error' field",0
"when people download this, the timestamp will have : replaced by _",0
check if the result blob has already been written (could be another instance of the API / worker thread),0
"and if so, skip aggregating and uploading the results, and just generate the SAS URL, which",0
could be needed still if the previous request_status was `problem`.,0
upload the output JSON to the Job folder,0
%% Helper functions *copied* from ct_utils.py and visualization/visualization_utils.py,0
"Determine the factor, which shifts the decimal point of x",0
just behind the last significant digit,0
"Shift decimal point by multiplicatipon with factor, flooring, and",0
division by factor,0
PIL.Image.convert() returns a converted copy of this image,0
alter orientation as needed according to EXIF tag 0x112 (274) for Orientation,0
https://gist.github.com/dangtrinhnt/a577ece4cbe5364aad28,0
https://www.media.mit.edu/pia/Research/deepview/exif.html,0
"%% TFDetector class, an unmodified *copy* of the class in detection/tf_detector.py,",0
so we do not have to import the packages required by run_detector.py,0
Number of decimal places to round to for confidence and bbox coordinates,0
"MegaDetector was trained with batch size of 1, and the resizing function is a part",0
of the inference graph,0
An enumeration of failure reasons,0
"change from [y1, x1, y2, x2] to [x1, y1, width, height]",0
convert numpy floats to Python floats,0
need to change the above line to the following if supporting a batch size > 1 and resizing to the same size,0
"np_images = [np.asarray(image, np.uint8) for image in images]",0
"images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)",0
performs inference,0
our batch size is 1; need to loop the batch dim if supporting batch size > 1,0
%% Scoring script,0
determine if there is metadata attached to each image_id,0
information to determine input and output locations,0
other parameters for the task,0
test that we can write to output path; also in case there is no image to process,0
list images to process,0
"items in this list can be strings or [image_id, metadata]",0
model path,0
"Path to .pb TensorFlow detector model file, relative to the",0
models/megadetector_copies folder in mounted container,0
score the images,0
%% Imports,0
%% Constants,0
%% Classes,0
class variables,0
"instance variables, in order of when they are typically set",0
Leaving this commented out to remind us that we don't want this check here; let,0
the API fail on these images.  It's a huge hassle to remove non-image,0
files.,0
,0
for path_or_url in images_list:,0
if not is_image_file_or_url(path_or_url):,0
raise ValueError('{} is not an image'.format(path_or_url)),0
Commented out as a reminder: don't check task status (which is a rest API call),0
in __repr__; require the caller to explicitly request status,0
"status=getattr(self, 'status', None))",0
estimate # of failed images from failed shards,0
Download all three JSON urls to memory,0
Remove files that were submitted but don't appear to be images,0
assert all(is_image_file_or_url(s) for s in submitted_images),0
Diff submitted and processed images,0
Confirm that the procesed images are a subset of the submitted images,0
https://www.geeksforgeeks.org/break-list-chunks-size-n-python/,0
%% Interactive driver,0
%%,0
%%,0
%%,0
%% Imports and constants,0
%% Constants I set per script,0
## Required,0
base_output_folder_name = os.path.expanduser('~/postprocessing/' + organization_name_short),0
Shared Access Signature (SAS) tokens for the Azure Blob Storage container.,0
Leading question mark is optional.,0
,0
The read-only token is used for accessing images; the write-enabled token is,0
used for writing file lists.,0
## Typically left as default,0
"Pre-pended to all folder names/prefixes, if they're defined below",0
"This is how we break the container up into multiple taskgroups, e.g., for",0
separate surveys. The typical case is to do the whole container as a single,0
taskgroup.,0
"If your ""folders"" are really logical folders corresponding to multiple folders,",0
map them here,0
"A list of .json files to load images from, instead of enumerating.  Formatted as a",0
"dictionary, like folder_prefixes.",0
This is only necessary if you will be performing postprocessing steps that,0
"don't yet support SAS URLs, specifically the ""subsetting"" step, or in some",0
cases the splitting of files into multiple output directories for,0
empty/animal/vehicle/people.,0
,0
"For those applications, you will need to mount the container to a local drive.",0
For this case I recommend using rclone whether you are on Windows or Linux;,0
rclone is much easier than blobfuse for transient mounting.,0
,0
"But most of the time, you can ignore this.",0
"Supported model_versions: '4', '3', '4_prelim'",0
,0
Also available at the /supported_model_versions and /default_model_version,0
endpoints,0
,0
"Unless you have any specific reason to set this to a non-default value, leave",0
"it at the default, which as of 2020.04.28 is MegaDetector 4.1",0
,0
"additional_task_args = {""model_version"":""4_prelim""}",0
,0
"file_lists_by_folder will contain a list of local JSON file names,",0
each JSON file contains a list of blob names corresponding to an API taskgroup,0
"%% Derived variables, path setup",0
local folders,0
Turn warnings into errors if more than this many images are missing,0
%% Support functions,0
"scheme, netloc, path, query, fragment",0
%% Read images from lists or enumerate blobs to files,0
folder_name = folder_names[0],0
"Load file lists for this ""folder""",0
,0
file_list = input_file_lists[folder][0],0
Write to file,0
A flat list of blob paths for each folder,0
folder_name = folder_names[0],0
"If we don't/do have multiple prefixes to enumerate for this ""folder""",0
"If this is intended to be a folder, it needs to end in '/', otherwise",0
files that start with the same string will match too,0
...for each prefix,0
Write to file,0
...for each folder,0
%% Some just-to-be-safe double-checking around enumeration,0
Make sure each folder has at least one image matched; the opposite is usually a sign of a copy/paste issue,0
...for each image,0
...for each prefix,0
...for each folder,0
Make sure each image comes from one of our folders; the opposite is usually a sign of a bug up above,0
...for each prefix,0
...for each folder,0
...for each image,0
%% Divide images into chunks for each folder,0
The JSON file at folder_chunks[i][j] corresponds to task j of taskgroup i,0
list_file = file_lists_by_folder[0],0
"%% Create taskgroups and tasks, and upload image lists to blob storage",0
periods not allowed in task names,0
%% Generate API calls for each task,0
clipboard.copy(request_strings[0]),0
clipboard.copy('\n\n'.join(request_strings)),0
%% Run the tasks (don't run this cell unless you are absolutely sure!),0
I really want to make sure I'm sure...,0
%% Estimate total time,0
Around 0.8s/image on 16 GPUs,0
%% Manually create task groups if we ran the tasks manually,0
%%,0
"%% Write task information out to disk, in case we need to resume",0
%% Status check,0
print(task.id),0
%% Resume jobs if this notebook closes,0
%% For multiple tasks (use this only when we're merging with another job),0
%% For just the one task,0
%% Load into separate taskgroups,0
p = task_cache_paths[0],0
%% Typically merge everything into one taskgroup,0
"%% Look for failed shards or missing images, start new tasks if necessary",0
List of lists of paths,0
i_taskgroup = 0; taskgroup = taskgroups[i_taskgroup];,0
"Make a copy, because we append to taskgroup",0
i_task = 0; task = tasks[i_task],0
Each taskgroup corresponds to one of our folders,0
Check that we have (almost) all the images,0
Now look for failed images,0
Write it out as a flat list as well (without explanation of failures),0
...for each task,0
...for each task group,0
%% Pull results,0
i_taskgroup = 0; taskgroup = taskgroups[i_taskgroup]; task = taskgroup[0],0
Each taskgroup corresponds to one of our folders,0
...for each task,0
...for each task group,0
%% Combine results from task groups into final output files,0
Check that we have (almost) all the images,0
The only reason we should ever have a repeated request is the case where an,0
"image was missing and we reprocessed it, or where it failed and later succeeded",0
"There may be non-image files in the request list, ignore those",0
Something has gone bonkers if there are images in the results that,0
aren't in the request,0
...for each folder,0
%% Post-processing (no ground truth),0
i_folder = 0; folder_name_raw = folder_names[i_folder],0
"Omit some pages from the output, useful when animals are rare",0
%% Manual processing follows,0
,0
"Everything after this should be considered mostly manual, and no longer includes",0
looping over folders.,0
,0
%% RDE (sample directory collapsing),0
"In this example, the camera created folders called ""100EK113"", ""101EK113"", etc., for every N images",0
"%% Repeat detection elimination, phase 1",0
"Deliberately leaving these imports here, rather than at the top, because this cell is not",0
typically executed,0
To invoke custom collapsing of folders for a particular manufacturer's naming scheme,0
options.customDirNameFunction = remove_overflow_folders,0
Exclude people and vehicles from RDE,0
"options.excludeClasses = [2,3]",0
%% Manual RDE step,0
# DELETE THE VALID DETECTIONS ##,0
%% Re-filtering,0
%% Post-processing (post-RDE),0
i_folder = 0; folder_name_raw = folder_names[i_folder],0
"Omit some pages from the output, useful when animals are rare",0
"print('No RDE file available for {}, skipping'.format(folder_name))",0
continue,0
%% Create a new category for large boxes,0
%% Subsetting,0
i_folder = 0; folder_name = folders[i_folder],0
%% String replacement,0
%% Folder splitting,0
i_folder = 0; folder_name_raw = folder_names[i_folder],0
api_output_file = folder_name_to_combined_output_file[folder_name],0
%% Imports and constants,0
from ai4eutils,0
Turn warnings into errors if more than this many images are missing,0
Only relevant when we're using a single GPU,0
Only relevant when running on CPU,0
%% Constants I set per script,0
job_date = '2022-01-01',0
model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v5.0.0/md_v5b.0.0.pt'),0
model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v4.1.0/md_v4.1.0.pb'),0
"Number of jobs to split data into, typically equal to the number of available GPUs",0
Only used to print out a time estimate,0
"%% Derived variables, path setup",0
local folders,0
%% Enumerate files,0
%% Divide images into chunks,0
%% Estimate total time,0
%% Write file lists,0
%% Generate commands,0
i_task = 0; task = task_info[i_task],0
%% Generate combined commands for a handful of tasks,0
%%,0
i_task = 8,0
...for each task,0
%% Run the tasks,0
Prefer to run manually,0
"%% Load results, look for failed or missing images in each task",0
i_task = 0; task = task_info[i_task],0
im = task_results['images'][0],0
...for each task,0
%% Merge results files and make images relative,0
im = combined_results['images'][0],0
%% Compare results files for different model versions (or before/after RDE),0
%% Post-processing (no ground truth),0
options.sample_seed = 0,0
"Omit some pages from the output, useful when animals are rare",0
%% Merge in high-confidence detections from another results file,0
%% RDE (sample directory collapsing),0
"In this example, the camera created folders called ""100EK113"", ""101EK113"", etc., for every N images",0
%%,0
%%,0
"%% Repeat detection elimination, phase 1",0
"Deliberately leaving these imports here, rather than at the top, because this cell is not",0
typically executed,0
options.lineThickness = 5,0
options.boxExpansion = 8,0
To invoke custom collapsing of folders for a particular manufacturer's naming scheme,0
options.customDirNameFunction = remove_overflow_folders,0
Exclude people and vehicles from RDE,0
"options.excludeClasses = [2,3]",0
options.maxImagesPerFolder = 50000,0
options.includeFolders = ['a/b/c'],0
options.excludeFolder = ['a/b/c'],0
"Can be None, 'xsort', or 'clustersort'",0
import clipboard; clipboard.copy(os.path.dirname(suspiciousDetectionResults.filterFile)),0
path_utils.open_file(os.path.dirname(suspiciousDetectionResults.filterFile)),0
%% Manual RDE step,0
# DELETE THE VALID DETECTIONS ##,0
%% Re-filtering,0
%% Post-processing (post-RDE),0
options.sample_seed = 0,0
"Omit some pages from the output, useful when animals are rare",0
"%% Run MegaClassifier (actually, write out a script that runs MegaClassifier)",0
"This is just passed along to the metadata in the output file, it has no impact",0
on how the classification scripts run.,0
#%% Set up environment,0
commands.append('cd CameraTraps/classification\n'),0
commands.append('conda activate cameratraps-classifier\n'),0
#%% Crop images,0
fn = input_files[0],0
#%% Run classifier,0
fn = input_files[0],0
#%% Remap classifier outputs,0
fn = input_files[0],0
#%% Merge classification and detection outputs,0
fn = input_files[0],0
#%% Write  out classification script,0
"%% Run a non-MegaClassifier classifier (i.e., a classifier with no output mapping)",0
"This is just passed along to the metadata in the output file, it has no impact",0
on how the classification scripts run.,0
#%% Set up environment,0
#%% Crop images,0
fn = input_files[0],0
#%% Run classifier,0
fn = input_files[0],0
#%% Merge classification and detection outputs,0
fn = input_files[0],0
#%% Write everything out,0
%% Create a new category for large boxes,0
%% Subsetting,0
i_folder = 0; folder_name = folders[i_folder],0
%% String replacement,0
%% Folder splitting,0
%% Post-processing (post-classification),0
classification_detection_file = classification_detection_files[1],0
%% Within-image classification smoothing,0
Only count detections with a classification confidence threshold above,0
"*classification_confidence_threshold*, which in practice means we're only",0
looking at one category per detection.,0
,0
If an image has at least *min_detections_above_threshold* such detections,0
"in the most common category, and no more than *max_detections_secondary_class*",0
"in the second-most-common category, flip all detections to the most common",0
category.,0
,0
"Optionally treat some classes as particularly unreliable, typically used to overwrite an",0
"""other"" class.",0
d['classification_categories'],0
im['detections'],0
"path_utils.open_file(os.path.join(input_path,im['file']))",0
im = d['images'][0],0
...for each classification,0
...if there are classifications for this detection,0
...for each detection,0
If we have at least *min_detections_to_overwrite_other* in a category that isn't,0
"""other"", change all ""other"" classifications to that category",0
...for each classification,0
...if there are classifications for this detection,0
...for each detection,0
"...if we should overwrite all ""other"" classifications",0
"At this point, we know we have a dominant category; change all other above-threshold",0
"classifications to that category.  That category may have been ""other"", in which case we may have",0
already made the relevant changes.,0
det = detections[0],0
...for each image,0
,0
xmp_integration.py,0
,0
"Tools for loading MegaDetector batch API results into XMP metadata, specifically",0
for consumption in digiKam:,0
,0
https://cran.r-project.org/web/packages/camtrapR/vignettes/camtrapr2.html,0
,0
%% Imports and constants,0
%% Class definitions,0
Folder where images are stored,0
.json file containing MegaDetector output,0
"String to remove from all path names, typically representing a",0
prefix that was added during MegaDetector processing,0
Optionally *rename* (not copy) all images that have no detections,0
above [rename_conf] for the categories in rename_cats from x.jpg to,0
x.check.jpg,0
"Comma-deleted list of category names (or ""all"") to apply the rename_conf",0
behavior to.,0
"Minimum detection threshold (applies to all classes, defaults to None,",0
i.e. 0.0,0
%% Functions,0
Relative image path,0
Absolute image path,0
List of categories to write to XMP metadata,0
Categories with above-threshold detections present for,0
this image,0
Maximum confidence for each category,0
Have we already added this to the list of categories to,0
write out to this image?,0
If we're supposed to compare to a threshold...,0
Else we treat *any* detection as valid...,0
Keep track of the highest-confidence detection for this class,0
If we're doing the rename/.check behavior...,0
Legacy code to rename files where XMP writing failed,0
%% Interactive/test driver,0
%%,0
%% Command-line driver,0
,0
Test script for pushing annotations to the eMammal db,0
,0
%% Imports,0
%% Database functions,0
%% Command-line driver,0
TODO: check project ID ?,1
No-animal category,0
%% Imports,0
%% Main function,0
if not os.path.exists(directory):,0
os.makedirs(directory),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Cosmos DB `batch-api-jobs` table for job status,0
"aggregate the number of images, country and organization names info from each job",0
submitted during yesterday (UTC time),0
create the card,0
get rid of formatting differences,0
compare result,0
"part is a BodyPart object with b'Content-Type', and b'Content-Disposition', the later",0
includes 'name' and 'filename' info,0
the filename and name info is all in one string with no obvious format,0
compare result,0
"part is a BodyPart object with b'Content-Type', and b'Content-Disposition', the later",0
includes 'name' and 'filename' info,0
images[part.headers['filename']] = part.content,0
the filename and name info is all in one string with no obvious format,0
"part is a BodyPart object with b'Content-Type', and b'Content-Disposition', the later includes 'name' and 'filename' info",0
https://stackoverflow.com/questions/11380413/python-unittest-passing-arguments,0
"part is a BodyPart object with b'Content-Type', and b'Content-Disposition', the later includes 'name' and 'filename' info",0
images[part.headers['filename']] = part.content,0
For now only requesting on one image at a time,0
"Attaching more images to the request is possible,",0
but you'd have to use callbacks rather than async/await:,0
https://stackoverflow.com/questions/34403670/superagent-multiple-files-attachment,0
Make sure there's an API key in the .env file,0
Minimum detection confidence for showing a bounding box on the output image,0
Number of top-scoring classes to show at each bounding box,0
Number of significant float digits in JSON output,0
remove empty lines,0
"Determine the factor, which shifts the decimal point of x",0
just behind the last significant digit,0
"Shift decimal point by multiplicatipon with factor, flooring, and",0
division by factor,0
"json_with_classes = self.add_classification_categories(detection_json, class_names)",0
Get input and output tensors of classification model,0
For each image,0
"Scale pixel values to [0,1]",0
For each box,0
Skip detections with low confidence,0
Skip if detection category is not in whitelist,0
"box ymin, xim, ymax, xmax",0
"Get current box in relative coordinates and format [x_min, y_min, width_of_box, height_of_box]",0
"Convert to [ymin, xmin, ymax, xmax] and",0
store it as 1x4 numpy array so we can re-use the generic multi-box padding code,0
Convert normalized coordinates to pixel coordinates,0
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes",0
"However, we need to make sure that it box coordinates are still within the image",0
Get the first (and only) row as our bbox to classify,0
Get the image data for that box,0
Run inference,0
Add the *num_annotated_classes* top scoring classes,0
sync API configurations,0
upper limit on total content length (all images and parameters),0
classification configurations,0
TODO,1
padding factor used for padding the detected animal,0
Minimum detection confidence for showing a bounding box on the output image,0
Number of top-scoring classes to show at each bounding box,0
Number of significant float digits in JSON output,0
detection configurations,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"# /ai4e_api_tools has been added to the PYTHONPATH, so we can reference those libraries directly.",0
from tf_classifer import TFClassifier,0
Use the AI4EAppInsights library to send log messages.,0
"Use the APIService to executes your functions within a logging trace, supports long-running/async functions,",0
"handles SIGTERM signals from AKS, etc., and handles concurrent requests.",0
Load the models when the API starts,0
"TODO classifier = TFClassifier(api_config.CLASSIFICATION_MODEL_PATHS, api_config.CLASSIFICATION_CLASS_NAMES)",0
here we make a dict that the request_processing_function can return to the endpoint function,0
to notify it of an error,0
check that the content uploaded is not too big,0
request.content_length is the length of the total payload,0
"also will not proceed if cannot find content_length, hence in the else we exceed the max limit",0
validate detection confidence value,0
check that the number of images is acceptable,0
"check if classification is requested and if so, which classifier to use",0
read input images and parameters,0
file of type SpooledTemporaryFile has attributes content_type and a read() method,0
"if the number of requests exceed this limit, a 503 is returned to the caller.",0
check if the request_processing_function had an error while parsing user specified parameters,0
filter the detections by the confidence threshold,0
"each result is [ymin, xmin, ymax, xmax, confidence, category]",0
classification,0
TODO,1
try:,0
if classification:,0
"print('runserver, classification...')",0
tic = datetime.now(),0
"classification_result = classifier.classify_boxes(images, image_names, result, classification)",0
toc = datetime.now(),0
classification_inference_duration = toc - tic,0
"print('runserver, classification, classifcation inference duraction: {}' \",0
.format({classification_inference_duration})),0
,0
else:,0
classification_result = {},0
,0
except Exception as e:,0
print('Error performing classification on the images: ' + str(e)),0
log.log_exception('Error performing classification on the images: ' + str(e)),0
"abort(500, 'Error performing classification on the images: ' + str(e))",0
return results; optionally render the detections on the images and send the annotated images back,0
"TODO 'classification mean inference time': str(''),",1
TODO,1
"@ai4e_service.api_sync_func(api_path='/supported_classifiers',",0
"methods=['GET'],",0
"maximum_concurrent_requests=1000,",0
trace_name='get:get_supported_classifiers'),0
"def get_supported_classifiers(*args, **kwargs):",0
try:,0
return list(api_config.CLASSIFICATION_CLASS_NAMES.keys()),0
except Exception as e:,0
return 'Supported classifiers unknown. Error: {}'.format(str(e)),0
img_file = BytesIO(urlopen.urlopen(url).read()),0
image = Image.open(img_file).convert('RGB'),0
image = mpimg.imread(url),0
Actual detection,0
calculate the size,0
img_file = BytesIO(urlopen.urlopen(inputFileName).read()),0
image = Image.open(img_file).convert('RGB'),0
Add the patch to the Axes,0
"plt.savefig(outputFileName, bbox_inches='tight', pad_inches=0.0, dpi=dpi, transparent=True)",0
model configuration,0
"if(path == ""/""):",0
"path = ""index""",0
#####,0
,0
run_detector.py,0
,0
"Functions to load a TensorFlow detection model, run inference,",0
and render bounding boxes on images.,0
,0
"See the ""test driver"" cell for example invocation.",0
,0
#####,0
"%% Constants, imports, environment",0
%% Core detection functions,0
image = mpimg.imread(url),0
Actual detection,0
Read the image file,0
image = mpimg.imread(inputFileName),0
Display the image,0
"top, left, bottom, right",0
,0
"x,y origin is the upper-left",0
Location is the bottom-left of the rect,0
,0
Origin is the upper-left,0
Add the patch to the Axes,0
This is magic goop that removes whitespace around image plots (sort of),0
"plt.savefig(outputFileName, bbox_inches='tight', pad_inches=0.0, dpi=dpi, transparent=True)",0
%% Test driver,0
import os,0
MODEL_FILE = r'/Users/ranjanbalappa/backup/camera-trap/checkpoint/frozen_inference_graph.pb',0
TARGET_IMAGES = os.listdir('static/gallery'),0
TARGET_IMAGES = ['static/gallery/' + f for f in TARGET_IMAGES],0
# # Load and run detector on target images,0
detection_graph = load_model(MODEL_FILE),0
startTime = time.time(),0
"boxes,scores,classes,images = generate_detections(detection_graph,TARGET_IMAGES)",0
elapsed = time.time() - startTime,0
"print(""Done running detector on {} files in {}"".format(len(images),humanfriendly.format_timespan(elapsed)))",0
assert len(boxes) == len(TARGET_IMAGES),0
inputFileNames = TARGET_IMAGES,0
outputFileNames=[],0
confidenceThreshold=0.9,0
plt.ioff(),0
"render_bounding_boxes(boxes, scores, classes, TARGET_IMAGES)",0
print(TARGET_IMAGES[0]),0
output_img = {},0
for img_file in TARGET_IMAGES:,0
"box, score, clss = generate_image_detections(detection_graph, img_file)",0
"name, ext = os.path.splitext(img_file.split('/')[-1])",0
"num_objects, bboxes = draw_image_detections(box, score, clss, img_file, 'static/results/' + name )",0
output_img[img_file.split('/')[-1]] = {,0
"'num_objects': num_objects,",0
"'image_name': img_file.split('/')[-1],",0
"'result': 'Animal Detected' if num_objects > 0 else 'No Animal Detected',",0
'bboxes': bboxes,0
},0
import json,0
"with open('static/gallery_results/results.json', 'w') as res:",0
"json.dump(output_img, res)",0
from . import model,0
from . import aadConfig as aad,0
api_url = apiconfig.api['base_url'] + '/camera-trap/detect?confidence={1}&render={1}',0
routes for cameratrapassets as these are being loaded,0
from the cameratrapassets directory instead of the static directory,0
"def track_images(file, name):",0
print(str(e)),0
resize_images(images),0
"bbox points, confidence",0
print(img_result),0
redirect to home if no images to display,0
"gallery_images = random.sample(gallery_images, 12)",0
from . import aadConfig as aad,0
api url,0
Dropzone settings,0
app.config['AUTHORITY_URL'] =  aad.AUTHORITY_HOST_URL + '/' + aad.TENANT,0
app.config['DROPZONE_IN_FORM'] = True,0
app.config['DROPZONE_UPLOAD_ON_CLICK'] = True,0
app.config['DROPZONE_UPLOAD_BTN_ID'] =  'submit',0
app.config[' DROPZONE_UPLOAD_ACTION'] = 'processimages',0
Uploads settings,0
model configuration,0
# sourceMappingURL=popper.min.js.map,0
Noty.overrideDefaults({,0
"layout   : 'topRight',",0
"theme    : 'mint',",0
"closeWith: ['click', 'button'],",0
"timeout: 1500,",0
animation: {,0
"open : 'animated fadeInRight',",0
close: 'animated fadeOutRight',0
},0
});,0
Initialize,0
var bLazy = new Blazy({,0
container: '.scroll-class',0
});,0
timeout: 2500,0
Defining this global in .eslintrc.json would create a danger of using the global,0
"unguarded in another place, it seems safer to define global only for this module",0
Define a local copy of jQuery,0
The jQuery object is actually just the init constructor 'enhanced',0
Need init if jQuery is called (just allow error to be thrown if not included),0
Support: Android <=4.0 only,0
Make sure we trim BOM and NBSP,0
The current version of jQuery being used,0
The default length of a jQuery object is 0,0
Get the Nth element in the matched element set OR,0
Get the whole matched element set as a clean array,0
Return all the elements in a clean array,0
Return just the one element from the set,0
Take an array of elements and push it onto the stack,0
(returning the new matched element set),0
Build a new jQuery matched element set,0
Add the old object onto the stack (as a reference),0
Return the newly-formed element set,0
Execute a callback for every element in the matched set.,0
For internal use only.,0
"Behaves like an Array's method, not like a jQuery method.",0
Handle a deep copy situation,0
Skip the boolean and the target,0
Handle case when target is a string or something (possible in deep copy),0
Extend jQuery itself if only one argument is passed,0
Only deal with non-null/undefined values,0
Extend the base object,0
Prevent Object.prototype pollution,0
Prevent never-ending loop,0
Recurse if we're merging plain objects or arrays,0
Ensure proper type for the source value,0
"Never move original objects, clone them",0
Don't bring in undefined values,0
Return the modified object,0
Unique for each copy of jQuery on the page,0
Assume jQuery is ready without the ready module,0
Detect obvious negatives,0
Use toString instead of jQuery.type to catch host objects,0
"Objects with no prototype (e.g., `Object.create( null )`) are plain",0
Objects with prototype are plain iff they were constructed by a global Object function,0
Evaluates a script in a global context,0
Support: Android <=4.0 only,0
results is for internal usage only,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
"push.apply(_, arraylike) throws on ancient WebKit",0
"Go through the array, only saving the items",0
that pass the validator function,0
arg is for internal usage only,0
"Go through the array, translating each of the items to their new values",0
"Go through every key on the object,",0
Flatten any nested arrays,0
A global GUID counter for objects,0
jQuery.support is not used in Core but other projects attach their,0
properties to it so it needs to exist.,0
Populate the class2type map,0
Support: real iOS 8.2 only (not reproducible in simulator),0
`in` check used to prevent JIT error (gh-2145),0
hasOwn isn't used here due to false negatives,0
regarding Nodelist length in IE,0
For CommonJS and CommonJS-like environments where a proper `window`,0
"is present, execute the factory and get jQuery.",0
For environments that do not have a `window` with a `document`,0
"(such as Node.js), expose a factory as module.exports.",0
This accentuates the need for the creation of a real `window`.,0
"e.g. var jQuery = require(""jquery"")(window);",0
See ticket #14549 for more info.,0
Pass this if window is not defined yet,0
"Edge <= 12 - 13+, Firefox <=18 - 45+, IE 10 - 11, Safari 5.1 - 9+, iOS 6 - 9.1",0
"throw exceptions when non-strict code (e.g., ASP.NET 4.5) accesses strict mode",0
"arguments.callee.caller (trac-13335). But as of jQuery 3.0 (2016), strict mode should be common",0
enough that all such attempts are guarded in a try block.,0
"Support: Chrome <=57, Firefox <=52",0
"In some browsers, typeof returns ""function"" for HTML <object> elements",0
"(i.e., `typeof document.createElement( ""object"" ) === ""function""`).",0
We don't want to classify *any* DOM node as a function.,0
"Support: Firefox 64+, Edge 18+",0
"Some browsers don't support the ""nonce"" property on scripts.",0
"On the other hand, just using `getAttribute` is not enough as",0
the `nonce` attribute is reset to an empty string whenever it,0
becomes browsing-context connected.,0
See https://github.com/whatwg/html/issues/2369,0
See https://html.spec.whatwg.org/#nonce-attributes,0
The `node.getAttribute` check was added for the sake of,0
`jQuery.globalEval` so that it can fake a nonce-containing node,0
via an object.,0
Support: Android <=2.3 only (functionish RegExp),0
Defining this global in .eslintrc.json would create a danger of using the global,0
"unguarded in another place, it seems safer to define global only for this module",0
Define a local copy of jQuery,0
The jQuery object is actually just the init constructor 'enhanced',0
Need init if jQuery is called (just allow error to be thrown if not included),0
Support: Android <=4.0 only,0
Make sure we trim BOM and NBSP,0
The current version of jQuery being used,0
The default length of a jQuery object is 0,0
Get the Nth element in the matched element set OR,0
Get the whole matched element set as a clean array,0
Return all the elements in a clean array,0
Return just the one element from the set,0
Take an array of elements and push it onto the stack,0
(returning the new matched element set),0
Build a new jQuery matched element set,0
Add the old object onto the stack (as a reference),0
Return the newly-formed element set,0
Execute a callback for every element in the matched set.,0
For internal use only.,0
"Behaves like an Array's method, not like a jQuery method.",0
Handle a deep copy situation,0
Skip the boolean and the target,0
Handle case when target is a string or something (possible in deep copy),0
Extend jQuery itself if only one argument is passed,0
Only deal with non-null/undefined values,0
Extend the base object,0
Prevent Object.prototype pollution,0
Prevent never-ending loop,0
Recurse if we're merging plain objects or arrays,0
Ensure proper type for the source value,0
"Never move original objects, clone them",0
Don't bring in undefined values,0
Return the modified object,0
Unique for each copy of jQuery on the page,0
Assume jQuery is ready without the ready module,0
Detect obvious negatives,0
Use toString instead of jQuery.type to catch host objects,0
"Objects with no prototype (e.g., `Object.create( null )`) are plain",0
Objects with prototype are plain iff they were constructed by a global Object function,0
Evaluates a script in a global context,0
Support: Android <=4.0 only,0
results is for internal usage only,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
"push.apply(_, arraylike) throws on ancient WebKit",0
"Go through the array, only saving the items",0
that pass the validator function,0
arg is for internal usage only,0
"Go through the array, translating each of the items to their new values",0
"Go through every key on the object,",0
Flatten any nested arrays,0
A global GUID counter for objects,0
jQuery.support is not used in Core but other projects attach their,0
properties to it so it needs to exist.,0
Populate the class2type map,0
Support: real iOS 8.2 only (not reproducible in simulator),0
`in` check used to prevent JIT error (gh-2145),0
hasOwn isn't used here due to false negatives,0
regarding Nodelist length in IE,0
Local document vars,0
Instance-specific data,0
Instance methods,0
Use a stripped-down indexOf as it's faster than native,0
https://jsperf.com/thor-indexof-vs-for/5,0
Regular expressions,0
http://www.w3.org/TR/css3-selectors/#whitespace,0
http://www.w3.org/TR/CSS21/syndata.html#value-def-identifier,0
Attribute selectors: http://www.w3.org/TR/selectors/#attribute-selectors,0
Operator (capture 2),0
"""Attribute values must be CSS identifiers [capture 5] or strings [capture 3 or capture 4]""",0
"To reduce the number of selectors needing tokenize in the preFilter, prefer arguments:",0
1. quoted (capture 3; capture 4 or capture 5),0
2. simple (capture 6),0
3. anything else (capture 2),0
"Leading and non-escaped trailing whitespace, capturing some non-whitespace characters preceding the latter",0
For use in libraries implementing .is(),0
We use this for POS matching in `select`,0
Easily-parseable/retrievable ID or TAG or CLASS selectors,0
CSS escapes,0
http://www.w3.org/TR/CSS21/syndata.html#escaped-characters,0
NaN means non-codepoint,0
Support: Firefox<24,0
"Workaround erroneous numeric interpretation of +""0x""",1
BMP codepoint,0
Supplemental Plane codepoint (surrogate pair),0
CSS string/identifier serialization,0
https://drafts.csswg.org/cssom/#common-serializing-idioms,0
U+0000 NULL becomes U+FFFD REPLACEMENT CHARACTER,0
Control characters and (dependent upon position) numbers get escaped as code points,0
Other potentially-special ASCII characters get backslash-escaped,0
Used for iframes,0
See setDocument(),0
"Removing the function wrapper causes a ""Permission Denied""",0
error in IE,0
"Optimize for push.apply( _, NodeList )",0
Support: Android<4.0,0
Detect silently failing push.apply,0
Leverage slice if possible,0
Support: IE<9,0
Otherwise append directly,0
Can't trust NodeList.length,0
"nodeType defaults to 9, since context defaults to document",0
Return early from calls with invalid selector or context,0
Try to shortcut find operations (as opposed to filters) in HTML documents,0
"If the selector is sufficiently simple, try using a ""get*By*"" DOM method",0
"(excepting DocumentFragment context, where the methods don't exist)",0
ID selector,0
Document context,0
"Support: IE, Opera, Webkit",0
TODO: identify versions,1
getElementById can match elements by name instead of ID,0
Element context,0
"Support: IE, Opera, Webkit",0
TODO: identify versions,1
getElementById can match elements by name instead of ID,0
Type selector,0
Class selector,0
Take advantage of querySelectorAll,0
Support: IE 8 only,0
Exclude object elements,0
qSA considers elements outside a scoping root when evaluating child or,0
"descendant combinators, which is not what we want.",0
"In such cases, we work around the behavior by prefixing every selector in the",0
list with an ID selector referencing the scope context.,0
Thanks to Andrew Dupont for this technique.,0
"Capture the context ID, setting it first if necessary",0
Prefix every selector in the list,0
Expand context for sibling selectors,0
All others,0
"Use (key + "" "") to avoid collision with native prototype properties (see Issue #157)",0
Only keep the most recent entries,0
Remove from its parent by default,0
release memory in IE,0
Use IE sourceIndex if available on both nodes,0
Check if b follows a,0
Known :disabled false positives: fieldset[disabled] > legend:nth-of-type(n+2) :can-disable,0
Only certain elements can match :enabled or :disabled,0
https://html.spec.whatwg.org/multipage/scripting.html#selector-enabled,0
https://html.spec.whatwg.org/multipage/scripting.html#selector-disabled,0
Check for inherited disabledness on relevant non-disabled elements:,0
* listed form-associated elements in a disabled fieldset,0
https://html.spec.whatwg.org/multipage/forms.html#category-listed,0
https://html.spec.whatwg.org/multipage/forms.html#concept-fe-disabled,0
* option elements in a disabled optgroup,0
https://html.spec.whatwg.org/multipage/forms.html#concept-option-disabled,0
"All such elements have a ""form"" property.",0
Option elements defer to a parent optgroup if present,0
Support: IE 6 - 11,0
Use the isDisabled shortcut property to check for disabled fieldset ancestors,0
"Where there is no isDisabled, check manually",0
Try to winnow out elements that can't be disabled before trusting the disabled property.,0
"Some victims get caught in our net (label, legend, menu, track), but it shouldn't",0
"even exist on them, let alone have a boolean value.",0
Remaining elements are neither :enabled nor :disabled,0
Match elements found at the specified indexes,0
Expose support vars for convenience,0
Support: IE <=8,0
"Assume HTML when documentElement doesn't yet exist, such as inside loading iframes",0
https://bugs.jquery.com/ticket/4833,0
Return early if doc is invalid or already selected,0
Update global variables,0
"Support: IE 9-11, Edge",0
"Accessing iframe documents after unload throws ""permission denied"" errors (jQuery #13936)",0
"Support: IE 11, Edge",0
Support: IE 9 - 10 only,0
Support: IE<8,0
Verify that getAttribute really returns attributes and not properties,0
(excepting IE8 booleans),0
"Check if getElementsByTagName(""*"") returns only elements",0
Support: IE<9,0
Support: IE<10,0
Check if getElementById returns elements by name,0
"The broken getElementById methods don't pick up programmatically-set names,",1
so use a roundabout getElementsByName test,0
ID filter and find,0
Support: IE 6 - 7 only,0
getElementById is not reliable as a find shortcut,0
Verify the id attribute,0
Fall back on getElementsByName,0
Tag,0
DocumentFragment nodes don't have gEBTN,0
"By happy coincidence, a (broken) gEBTN appears on DocumentFragment nodes too",0
Filter out possible comments,0
Class,0
QSA and matchesSelector support,0
matchesSelector(:active) reports false when true (IE9/Opera 11.5),0
qSa(:focus) reports false when true (Chrome 21),0
We allow this because of a bug in IE8/9 that throws an error,0
whenever `document.activeElement` is accessed on an iframe,0
"So, we allow :focus to pass through QSA all the time to avoid the IE error",0
See https://bugs.jquery.com/ticket/13378,0
Build QSA regex,0
Regex strategy adopted from Diego Perini,0
Select is set to empty string on purpose,0
This is to test IE's treatment of not explicitly,0
"setting a boolean content attribute,",0
since its presence should be enough,0
https://bugs.jquery.com/ticket/12359,0
"Support: IE8, Opera 11-12.16",0
Nothing should be selected when empty strings follow ^= or $= or *=,0
"The test attribute must be unknown in Opera but ""safe"" for WinRT",0
https://msdn.microsoft.com/en-us/library/ie/hh465388.aspx#attribute_section,0
Support: IE8,0
"Boolean attributes and ""value"" are not treated correctly",0
"Support: Chrome<29, Android<4.4, Safari<7.0+, iOS<7.0+, PhantomJS<1.9.8+",0
Webkit/Opera - :checked should return selected option elements,0
http://www.w3.org/TR/2011/REC-css3-selectors-20110929/#checked,0
IE8 throws error here and will not see later tests,0
"Support: Safari 8+, iOS 8+",0
https://bugs.webkit.org/show_bug.cgi?id=136851,0
In-page `selector#id sibling-combinator selector` fails,0
Support: Windows 8 Native Apps,0
The type and name attributes are restricted during .innerHTML assignment,0
Support: IE8,0
Enforce case-sensitivity of name attribute,0
FF 3.5 - :enabled/:disabled and hidden elements (hidden elements are still enabled),0
IE8 throws error here and will not see later tests,0
Support: IE9-11+,0
IE's :disabled selector does not pick up the children of disabled fieldsets,0
Opera 10-11 does not throw on post-comma invalid pseudos,0
Check to see if it's possible to do matchesSelector,0
on a disconnected node (IE 9),0
This should fail with an exception,0
"Gecko does not error, returns false instead",0
Element contains another,0
Purposefully self-exclusive,0
"As in, an element does not contain itself",0
Document order sorting,0
Flag for duplicate removal,0
Sort on method existence if only one input has compareDocumentPosition,0
Calculate position if both inputs belong to the same document,0
Otherwise we know they are disconnected,0
Disconnected nodes,0
Choose the first element that is related to our preferred document,0
Maintain original order,0
Exit early if the nodes are identical,0
Parentless nodes are either documents or disconnected,0
"If the nodes are siblings, we can do a quick check",0
Otherwise we need full lists of their ancestors for comparison,0
Walk down the tree looking for a discrepancy,0
Do a sibling check if the nodes have a common ancestor,0
Otherwise nodes in our document sort first,0
Set document vars if needed,0
IE 9's matchesSelector returns false on disconnected nodes,0
"As well, disconnected nodes are said to be in a document",0
fragment in IE 9,0
Set document vars if needed,0
Set document vars if needed,0
Don't get fooled by Object.prototype properties (jQuery #13807),0
"Unless we *know* we can detect duplicates, assume their presence",0
Clear input after sorting to release objects,0
See https://github.com/jquery/sizzle/pull/225,0
"If no nodeType, this is expected to be an array",0
Do not traverse comment nodes,0
Use textContent for elements,0
innerText usage removed for consistency of new lines (jQuery #11153),0
Traverse its children,0
Do not include comment or processing instruction nodes,0
Can be adjusted by the user,0
Move the given value to match[3] whether quoted or unquoted,0
nth-* requires argument,0
numeric x and y parameters for Expr.filter.CHILD,0
remember that false/true cast respectively to 0/1,0
other types prohibit arguments,0
Accept quoted arguments as-is,0
Strip excess characters from unquoted arguments,0
Get excess from tokenize (recursively),0
advance to the next closing parenthesis,0
excess is a negative index,0
Return only captures needed by the pseudo filter method (type and argument),0
Shortcut for :nth-*(n),0
:(first|last|only)-(child|of-type),0
Reverse direction for :only-* (if we haven't yet done so),0
non-xml :nth-child(...) stores cache data on `parent`,0
Seek `elem` from a previously-cached index,0
...in a gzip-friendly way,0
Support: IE <9 only,0
Defend against cloned attroperties (jQuery gh-1709),0
Fallback to seeking `elem` from the start,0
"When found, cache indexes on `parent` and break",0
Use previously-cached element index if available,0
...in a gzip-friendly way,0
Support: IE <9 only,0
Defend against cloned attroperties (jQuery gh-1709),0
xml :nth-child(...),0
or :nth-last-child(...) or :nth(-last)?-of-type(...),0
Use the same loop as above to seek `elem` from the start,0
Cache the index of each encountered element,0
Support: IE <9 only,0
Defend against cloned attroperties (jQuery gh-1709),0
"Incorporate the offset, then check against cycle size",0
pseudo-class names are case-insensitive,0
http://www.w3.org/TR/selectors/#pseudo-classes,0
Prioritize by case sensitivity in case custom pseudos are added with uppercase letters,0
Remember that setFilters inherits from pseudos,0
The user may use createPseudo to indicate that,0
arguments are needed to create the filter function,0
just as Sizzle does,0
But maintain support for old signatures,0
Potentially complex pseudos,0
Trim the selector passed to compile,0
to avoid treating leading and trailing,0
spaces as combinators,0
Match elements unmatched by `matcher`,0
Don't keep the element (issue #299),0
"""Whether an element is represented by a :lang() selector",0
is based solely on the element's language value,0
"being equal to the identifier C,",0
"or beginning with the identifier C immediately followed by ""-"".",0
The matching of C against the element's language value is performed case-insensitively.,0
"The identifier C does not have to be a valid language name.""",0
http://www.w3.org/TR/selectors/#lang-pseudo,0
lang value must be a valid identifier,0
Miscellaneous,0
Boolean properties,0
"In CSS3, :checked should return both checked and selected elements",0
http://www.w3.org/TR/2011/REC-css3-selectors-20110929/#checked,0
Accessing this property makes selected-by-default,0
options in Safari work properly,0
Contents,0
http://www.w3.org/TR/selectors/#empty-pseudo,0
":empty is negated by element (1) or content nodes (text: 3; cdata: 4; entity ref: 5),",0
but not by others (comment: 8; processing instruction: 7; etc.),0
nodeType < 6 works because attributes (2) do not appear as children,0
Element/input types,0
Support: IE<8,0
"New HTML5 attribute values (e.g., ""search"") appear with elem.type === ""text""",0
Position-in-collection,0
Add button/input type pseudos,0
Easy API for creating new setFilters,0
Comma and first run,0
Don't consume trailing commas as valid,0
Combinators,0
Cast descendant combinators to space,0
Filters,0
Return the length of the invalid excess,0
if we're just parsing,0
"Otherwise, throw an error or return tokens",0
Cache the tokens,0
Check against closest ancestor/preceding element,0
Check against all ancestor/preceding elements,0
"We can't set arbitrary data on XML nodes, so they don't benefit from combinator caching",0
Support: IE <9 only,0
Defend against cloned attroperties (jQuery gh-1709),0
Assign to newCache so results back-propagate to previous elements,0
Reuse newcache so results back-propagate to previous elements,0
A match means we're done; a fail means we have to keep checking,0
Get initial elements from seed or context,0
"Prefilter to get matcher input, preserving a map for seed-results synchronization",0
"If we have a postFinder, or filtered seed, or non-seed postFilter or preexisting results,",0
...intermediate processing is necessary,0
...otherwise use results directly,0
Find primary matches,0
Apply postFilter,0
Un-match failing elements by moving them back to matcherIn,0
Get the final matcherOut by condensing this intermediate into postFinder contexts,0
Restore matcherIn since elem is not yet a final match,0
Move matched elements from seed to results to keep them synchronized,0
"Add elements to results, through postFinder if defined",0
The foundational matcher ensures that elements are reachable from top-level context(s),0
Avoid hanging onto element (issue #299),0
Return special upon seeing a positional matcher,0
Find the next relative operator (if any) for proper handling,0
"If the preceding token was a descendant combinator, insert an implicit any-element `*`",0
We must always have either seed elements or outermost context,0
Use integer dirruns iff this is the outermost matcher,0
Add elements passing elementMatchers directly to results,0
"Support: IE<9, Safari",0
"Tolerate NodeList properties (IE: ""length""; Safari: <number>) matching elements by id",0
Track unmatched elements for set filters,0
They will have gone through all possible matchers,0
"Lengthen the array for every element, matched or not",0
"`i` is now the count of elements visited above, and adding it to `matchedCount`",0
makes the latter nonnegative.,0
Apply set filters to unmatched elements,0
"NOTE: This can be skipped if there are no unmatched elements (i.e., `matchedCount`",0
"equals `i`), unless we didn't visit _any_ elements in the above loop because we have",0
no element matchers and no seed.,0
"Incrementing an initially-string ""0"" `i` allows `i` to remain a string only in that",0
"case, which will result in a ""00"" `matchedCount` that differs from `i` but is also",0
numerically zero.,0
Reintegrate element matches to eliminate the need for sorting,0
Discard index placeholder values to get only actual matches,0
Add matches to results,0
Seedless set matches succeeding multiple successful matchers stipulate sorting,0
Override manipulation of globals by nested matchers,0
Generate a function of recursive functions that can be used to check each element,0
Cache the compiled function,0
Save selector and tokenization,0
Try to minimize operations if there is only one selector in the list and no seed,0
(the latter of which guarantees us context),0
Reduce context if the leading compound selector is an ID,0
"Precompiled matchers will still verify ancestry, so step up a level",0
Fetch a seed set for right-to-left matching,0
Abort if we hit a combinator,0
"Search, expanding context for leading sibling combinators",0
"If seed is empty or no tokens remain, we can return early",0
Compile and execute a filtering function if one is not provided,0
Provide `match` to avoid retokenization if we modified the selector above,0
One-time assignments,0
Sort stability,0
Support: Chrome 14-35+,0
Always assume duplicates if they aren't passed to the comparison function,0
Initialize against the default document,0
Support: Webkit<537.32 - Safari 6.0.3/Chrome 25 (fixed in Chrome 27),0
Detached nodes confoundingly follow *each other*,0
"Should return 1, but returns 4 (following)",0
Support: IE<8,0
"Prevent attribute/property ""interpolation""",0
https://msdn.microsoft.com/en-us/library/ms536429%28VS.85%29.aspx,0
Support: IE<9,0
"Use defaultValue in place of getAttribute(""value"")",0
Support: IE<9,0
Use getAttributeNode to fetch booleans when getAttribute lies,0
Deprecated,0
Implement the identical functionality for filter and not,0
Single element,0
"Arraylike of elements (jQuery, arguments, Array)",0
Filtered directly for both simple and complex selectors,0
"If this is a positional/relative selector, check membership in the returned set",0
"so $(""p:first"").is(""p:last"") won't return true for a doc with two ""p"".",0
Initialize a jQuery object,0
A central reference to the root jQuery(document),0
A simple way to check for HTML strings,0
Prioritize #id over <tag> to avoid XSS via location.hash (#9521),0
Strict HTML recognition (#11290: must start with <),0
Shortcut simple #id case for speed,0
"HANDLE: $(""""), $(null), $(undefined), $(false)",0
Method init() accepts an alternate rootjQuery,0
so migrate can support jQuery.sub (gh-2101),0
Handle HTML strings,0
Assume that strings that start and end with <> are HTML and skip the regex check,0
Match html or make sure no context is specified for #id,0
HANDLE: $(html) -> $(array),0
Option to run scripts is true for back-compat,0
Intentionally let the error be thrown if parseHTML is not present,0
"HANDLE: $(html, props)",0
Properties of context are called as methods if possible,0
...and otherwise set as attributes,0
HANDLE: $(#id),0
Inject the element directly into the jQuery object,0
"HANDLE: $(expr, $(...))",0
"HANDLE: $(expr, context)",0
(which is just equivalent to: $(context).find(expr),0
HANDLE: $(DOMElement),0
HANDLE: $(function),0
Shortcut for document ready,0
Execute immediately if ready is not present,0
Give the init function the jQuery prototype for later instantiation,0
Initialize central reference,0
Methods guaranteed to produce a unique set when starting from a unique set,0
"Positional selectors never match, since there's no _selection_ context",0
Always skip document fragments,0
Don't pass non-elements to Sizzle,0
Determine the position of an element within the set,0
"No argument, return index in parent",0
Index in selector,0
Locate the position of the desired element,0
"If it receives a jQuery object, the first element is used",0
"Support: IE 9 - 11 only, iOS 7 only, Android Browser <=4.3 only",0
Treat the template element as a regular one in browsers that,0
don't support it.,0
Remove duplicates,0
Reverse order for parents* and prev-derivatives,0
Convert String-formatted options into Object-formatted ones,0
Convert options from String-formatted to Object-formatted if needed,0
(we check in cache first),0
Last fire value for non-forgettable lists,0
Flag to know if list was already fired,0
Flag to prevent firing,0
Actual callback list,0
Queue of execution data for repeatable lists,0
Index of currently firing callback (modified by add/remove as needed),0
Fire callbacks,0
Enforce single-firing,0
"Execute callbacks for all pending executions,",0
respecting firingIndex overrides and runtime changes,0
Run callback and check for early termination,0
Jump to end and forget the data so .add doesn't re-fire,0
Forget the data if we're done with it,0
Clean up if we're done firing for good,0
Keep an empty list if we have data for future add calls,0
"Otherwise, this object is spent",0
Actual Callbacks object,0
Add a callback or a collection of callbacks to the list,0
"If we have memory from a past run, we should fire after adding",0
Inspect recursively,0
Remove a callback from the list,0
Handle firing indexes,0
Check if a given callback is in the list.,0
"If no argument is given, return whether or not list has callbacks attached.",0
Remove all callbacks from the list,0
Disable .fire and .add,0
Abort any current/pending executions,0
Clear all callbacks and values,0
Disable .fire,0
Also disable .add unless we have memory (since it would have no effect),0
Abort any pending executions,0
Call all callbacks with the given context and arguments,0
Call all the callbacks with the given arguments,0
To know if the callbacks have already been called at least once,0
Check for promise aspect first to privilege synchronous behavior,0
Other thenables,0
Other non-thenables,0
Control `resolve` arguments by letting Array#slice cast boolean `noValue` to integer:,0
* false: [ value ].slice( 0 ) => resolve( value ),0
* true: [ value ].slice( 1 ) => resolve(),0
"For Promises/A+, convert exceptions into rejections",0
"Since jQuery.when doesn't unwrap thenables, we can skip the extra checks appearing in",0
Deferred#then to conditionally suppress rejection.,0
Support: Android 4.0 only,0
Strict mode functions invoked without .call/.apply get global-object context,0
"action, add listener, callbacks,",0
"... .then handlers, argument index, [final state]",0
Keep pipe for back-compat,0
"Map tuples (progress, done, fail) to arguments (done, fail, progress)",0
deferred.progress(function() { bind to newDefer or newDefer.notify }),0
deferred.done(function() { bind to newDefer or newDefer.resolve }),0
deferred.fail(function() { bind to newDefer or newDefer.reject }),0
Support: Promises/A+ section 2.3.3.3.3,0
https://promisesaplus.com/#point-59,0
Ignore double-resolution attempts,0
Support: Promises/A+ section 2.3.1,0
https://promisesaplus.com/#point-48,0
"Support: Promises/A+ sections 2.3.3.1, 3.5",0
https://promisesaplus.com/#point-54,0
https://promisesaplus.com/#point-75,0
Retrieve `then` only once,0
Support: Promises/A+ section 2.3.4,0
https://promisesaplus.com/#point-64,0
Only check objects and functions for thenability,0
Handle a returned thenable,0
Special processors (notify) just wait for resolution,0
Normal processors (resolve) also hook into progress,0
...and disregard older resolution values,0
Handle all other returned values,0
Only substitute handlers pass on context,0
and multiple values (non-spec behavior),0
Process the value(s),0
Default process is resolve,0
Only normal processors (resolve) catch and reject exceptions,0
Support: Promises/A+ section 2.3.3.3.4.1,0
https://promisesaplus.com/#point-61,0
Ignore post-resolution exceptions,0
Only substitute handlers pass on context,0
and multiple values (non-spec behavior),0
Support: Promises/A+ section 2.3.3.3.1,0
https://promisesaplus.com/#point-57,0
Re-resolve promises immediately to dodge false rejection from,0
subsequent errors,0
"Call an optional hook to record the stack, in case of exception",0
since it's otherwise lost when execution goes async,0
progress_handlers.add( ... ),0
fulfilled_handlers.add( ... ),0
rejected_handlers.add( ... ),0
Get a promise for this deferred,0
"If obj is provided, the promise aspect is added to the object",0
Add list-specific methods,0
promise.progress = list.add,0
promise.done = list.add,0
promise.fail = list.add,0
Handle state,0
"state = ""resolved"" (i.e., fulfilled)",0
"state = ""rejected""",0
rejected_callbacks.disable,0
fulfilled_callbacks.disable,0
rejected_handlers.disable,0
fulfilled_handlers.disable,0
progress_callbacks.lock,0
progress_handlers.lock,0
progress_handlers.fire,0
fulfilled_handlers.fire,0
rejected_handlers.fire,0
deferred.notify = function() { deferred.notifyWith(...) },0
deferred.resolve = function() { deferred.resolveWith(...) },0
deferred.reject = function() { deferred.rejectWith(...) },0
deferred.notifyWith = list.fireWith,0
deferred.resolveWith = list.fireWith,0
deferred.rejectWith = list.fireWith,0
Make the deferred a promise,0
Call given func if any,0
All done!,0
Deferred helper,0
count of uncompleted subordinates,0
count of unprocessed arguments,0
subordinate fulfillment data,0
the master Deferred,0
subordinate callback factory,0
Single- and empty arguments are adopted like Promise.resolve,0
Use .then() to unwrap secondary thenables (cf. gh-3000),0
Multiple arguments are aggregated like Promise.all array elements,0
"These usually indicate a programmer mistake during development,",0
warn about them ASAP rather than swallowing them by default.,0
Support: IE 8 - 9 only,0
"Console exists when dev tools are open, which can happen at any time",0
The deferred used on DOM ready,0
Wrap jQuery.readyException in a function so that the lookup,0
happens at the time of error handling instead of callback,0
registration.,0
Is the DOM ready to be used? Set to true once it occurs.,0
A counter to track how many items to wait for before,0
the ready event fires. See #6781,0
Handle when the DOM is ready,0
Abort if there are pending holds or we're already ready,0
Remember that the DOM is ready,0
"If a normal DOM Ready event fired, decrement, and wait if need be",0
"If there are functions bound, to execute",0
The ready event handler and self cleanup method,0
Catch cases where $(document).ready() is called,0
after the browser event has already occurred.,0
Support: IE <=9 - 10 only,0
"Older IE sometimes signals ""interactive"" too soon",0
Handle it asynchronously to allow scripts the opportunity to delay ready,0
Use the handy event callback,0
"A fallback to window.onload, that will always work",0
Multifunctional method to get and set values of a collection,0
The value/s can optionally be executed if it's a function,0
Sets many values,0
Sets one value,0
Bulk operations run against the entire set,0
...except when executing function values,0
Gets,0
Matches dashed string for camelizing,0
Used by camelCase as callback to replace(),0
Convert dashed to camelCase; used by the css and data modules,0
"Support: IE <=9 - 11, Edge 12 - 15",0
Microsoft forgot to hump their vendor prefix (#9572),0
Accepts only:,0
- Node,0
- Node.ELEMENT_NODE,0
- Node.DOCUMENT_NODE,0
- Object,0
- Any,0
Check if the owner object already has a cache,0
"If not, create one",0
"We can accept data for non-element nodes in modern browsers,",0
"but we should not, see #8335.",0
Always return an empty object.,0
If it is a node unlikely to be stringify-ed or looped over,0
use plain assignment,0
Otherwise secure it in a non-enumerable property,0
configurable must be true to allow the property to be,0
deleted when data is removed,0
"Handle: [ owner, key, value ] args",0
Always use camelCase key (gh-2257),0
"Handle: [ owner, { properties } ] args",0
Copy the properties one-by-one to the cache object,0
Always use camelCase key (gh-2257),0
In cases where either:,0
,0
1. No key was specified,0
"2. A string key was specified, but no value provided",0
,0
"Take the ""read"" path and allow the get method to determine",0
"which value to return, respectively either:",0
,0
1. The entire cache object,0
2. The data stored at the key,0
,0
"When the key is not a string, or both a key and value",0
"are specified, set or extend (existing objects) with either:",0
,0
1. An object of properties,0
2. A key and value,0
,0
"Since the ""set"" path can have two possible entry points",0
return the expected data based on which path was taken[*],0
Support array or space separated string of keys,0
If key is an array of keys...,0
"We always set camelCase keys, so remove that.",0
"If a key with the spaces exists, use it.",0
"Otherwise, create an array by matching non-whitespace",0
Remove the expando if there's no more data,0
Support: Chrome <=35 - 45,0
Webkit & Blink performance suffers when deleting properties,0
"from DOM nodes, so set to undefined instead",0
https://bugs.chromium.org/p/chromium/issues/detail?id=378607 (bug restricted),0
Implementation Summary,0
,0
1. Enforce API surface and semantic compatibility with 1.9.x branch,0
2. Improve the module's maintainability by reducing the storage,0
paths to a single mechanism.,0
"3. Use the same single mechanism to support ""private"" and ""user"" data.",0
"4. _Never_ expose ""private"" data to user code (TODO: Drop _data, _removeData)",1
5. Avoid exposing implementation details on user objects (eg. expando properties),0
6. Provide a clear path for implementation upgrade to WeakMap in 2014,0
Only convert to a number if it doesn't change the string,0
"If nothing was found internally, try to fetch any",0
data from the HTML5 data-* attribute,0
Make sure we set the data so it isn't changed later,0
TODO: Now that all calls to _data and _removeData have been replaced,1
"with direct calls to dataPriv methods, these can be deprecated.",0
Gets all values,0
Support: IE 11 only,0
The attrs elements can be null (#14894),0
Sets multiple values,0
The calling jQuery object (element matches) is not empty,0
(and therefore has an element appears at this[ 0 ]) and the,0
`value` parameter was not undefined. An empty jQuery object,0
will result in `undefined` for elem = this[ 0 ] which will,0
throw an exception if an attempt to read a data cache is made.,0
Attempt to get data from the cache,0
The key will always be camelCased in Data,0
"Attempt to ""discover"" the data in",0
HTML5 custom data-* attrs,0
"We tried really hard, but the data doesn't exist.",0
Set the data...,0
We always store the camelCased key,0
Speed up dequeue by getting out quickly if this is just a lookup,0
"If the fx queue is dequeued, always remove the progress sentinel",0
Add a progress sentinel to prevent the fx queue from being,0
automatically dequeued,0
Clear up the last queue stop function,0
"Not public - generate a queueHooks object, or return the current one",0
Ensure a hooks for this queue,0
Get a promise resolved when queues of a certain type,0
are emptied (fx is the type by default),0
Check attachment across shadow DOM boundaries when possible (gh-3504),0
isHiddenWithinTree might be called from jQuery#filter function;,0
"in that case, element will be second argument",0
Inline style trumps all,0
"Otherwise, check computed style",0
Support: Firefox <=43 - 45,0
"Disconnected elements can have computed display: none, so first confirm that elem is",0
in the document.,0
"Remember the old values, and insert the new ones",0
Revert the old values,0
Starting value computation is required for potential unit mismatches,0
Support: Firefox <=54,0
Halve the iteration target value to prevent interference from CSS upper bounds (gh-2144),0
Trust units reported by jQuery.css,0
Iteratively approximate from a nonzero starting point,0
Evaluate and update our best guess (doubling guesses that zero out).,0
Finish if the scale equals or crosses 1 (making the old*new product non-positive).,0
Make sure we update the tween properties later on,0
Apply relative offset (+=/-=) if specified,0
Determine new display value for elements that need to change,0
"Since we force visibility upon cascade-hidden elements, an immediate (and slow)",0
check is required in this first loop unless we have a nonempty display value (either,0
inline or about-to-be-restored),0
Remember what we're overwriting,0
Set the display of the elements in a second loop to avoid constant reflow,0
We have to close these tags to support XHTML (#13200),0
Support: IE <=9 only,0
XHTML parsers do not magically insert elements in the,0
same way that tag soup parsers do. So we cannot shorten,0
this by omitting <tbody> or other required elements.,0
Support: IE <=9 only,0
Support: IE <=9 - 11 only,0
Use typeof to avoid zero-argument method invocation on host objects (#15151),0
Mark scripts as having already been evaluated,0
Add nodes directly,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
"push.apply(_, arraylike) throws on ancient WebKit",0
Convert non-html into a text node,0
Convert html into DOM nodes,0
Deserialize a standard representation,0
Descend through wrappers to the right content,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
"push.apply(_, arraylike) throws on ancient WebKit",0
Remember the top-level container,0
Ensure the created nodes are orphaned (#12392),0
Remove wrapper from fragment,0
Skip elements already in the context collection (trac-4087),0
Append to fragment,0
Preserve script evaluation history,0
Capture executables,0
Support: Android 4.0 - 4.3 only,0
Check state lost if the name is set (#11217),0
Support: Windows Web Apps (WWA),0
`name` and `type` must use .setAttribute for WWA (#14901),0
Support: Android <=4.1 only,0
Older WebKit doesn't clone checked state correctly in fragments,0
Support: IE <=11 only,0
Make sure textarea (and checkbox) defaultValue is properly cloned,0
Support: IE <=9 - 11+,0
"focus() and blur() are asynchronous, except when they are no-op.",0
"So expect focus to be synchronous when the element is already active,",0
and blur to be synchronous when the element is not already active.,0
"(focus and blur are always synchronous in other supported browsers,",0
this just defines when we can count on it).,0
Support: IE <=9 only,0
Accessing document.activeElement can throw unexpectedly,0
https://bugs.jquery.com/ticket/13393,0
Types can be a map of types/handlers,0
"( types-Object, selector, data )",0
"( types-Object, data )",0
"( types, fn )",0
"( types, selector, fn )",0
"( types, data, fn )",0
"Can use an empty set, since event contains the info",0
Use same guid so caller can remove using origFn,0
Don't attach events to noData or text/comment nodes (but allow plain objects),0
Caller can pass in an object of custom data in lieu of the handler,0
Ensure that invalid selectors throw exceptions at attach time,0
"Evaluate against documentElement in case elem is a non-element node (e.g., document)",0
"Make sure that the handler has a unique ID, used to find/remove it later",0
"Init the element's event structure and main handler, if this is the first",0
Discard the second event of a jQuery.event.trigger() and,0
when an event is called after a page has unloaded,0
Handle multiple events separated by a space,0
"There *must* be a type, no attaching namespace-only handlers",0
"If event changes its type, use the special event handlers for the changed type",0
"If selector defined, determine special event api type, otherwise given type",0
Update special based on newly reset type,0
handleObj is passed to all event handlers,0
Init the event handler queue if we're the first,0
Only use addEventListener if the special events handler returns false,0
"Add to the element's handler list, delegates in front",0
"Keep track of which events have ever been used, for event optimization",0
Detach an event or set of events from an element,0
Once for each type.namespace in types; type may be omitted,0
"Unbind all events (on this namespace, if provided) for the element",0
Remove matching events,0
Remove generic event handler if we removed something and no more handlers exist,0
(avoids potential for endless recursion during removal of special event handlers),0
Remove data and the expando if it's no longer used,0
Make a writable jQuery.Event from the native event object,0
Use the fix-ed jQuery.Event rather than the (read-only) native event,0
"Call the preDispatch hook for the mapped type, and let it bail if desired",0
Determine handlers,0
Run delegates first; they may want to stop propagation beneath us,0
"If the event is namespaced, then each handler is only invoked if it is",0
specially universal or its namespaces are a superset of the event's.,0
Call the postDispatch hook for the mapped type,0
Find delegate handlers,0
Support: IE <=9,0
Black-hole SVG <use> instance trees (trac-13180),0
Support: Firefox <=42,0
Suppress spec-violating clicks indicating a non-primary pointer button (trac-3861),0
https://www.w3.org/TR/DOM-Level-3-Events/#event-type-click,0
Support: IE 11 only,0
"...but not arrow key ""clicks"" of radio inputs, which can have `button` -1 (gh-2343)",0
Don't check non-elements (#13208),0
"Don't process clicks on disabled elements (#6911, #8165, #11382, #11764)",0
Don't conflict with Object.prototype properties (#13203),0
Add the remaining (directly-bound) handlers,0
Prevent triggered image.load events from bubbling to window.load,0
Utilize native event to ensure correct state for checkable inputs,0
"For mutual compressibility with _default, replace `this` access with a local var.",0
`|| data` is dead code meant only to preserve the variable through minification.,0
Claim the first handler,0
"dataPriv.set( el, ""click"", ... )",0
Return false to allow normal processing in the caller,0
"For mutual compressibility with _default, replace `this` access with a local var.",0
`|| data` is dead code meant only to preserve the variable through minification.,0
Force setup before triggering a click,0
Return non-false to allow normal event-path propagation,0
"For cross-browser consistency, suppress native .click() on links",0
Also prevent it if we're currently inside a leveraged native-event stack,0
Support: Firefox 20+,0
Firefox doesn't alert if the returnValue field is not set.,0
Ensure the presence of an event listener that handles manually-triggered,0
synthetic events by interrupting progress until reinvoked in response to,0
"*native* events that it fires directly, ensuring that state changes have",0
already occurred before other listeners are invoked.,0
"Missing expectSync indicates a trigger call, which must force setup through jQuery.event.add",0
Register the controller as a special universal handler for all event namespaces,0
Interrupt processing of the outer synthetic .trigger()ed event,0
Store arguments for use when handling the inner native event,0
Trigger the native event and capture its result,0
Support: IE <=9 - 11+,0
focus() and blur() are asynchronous,0
Cancel the outer synthetic event,0
If this is an inner synthetic event for an event with a bubbling surrogate,0
"(focus or blur), assume that the surrogate already propagated from triggering the",0
native event and prevent that from happening again here.,0
This technically gets the ordering wrong w.r.t. to `.trigger()` (in which the,0
"bubbling surrogate propagates *after* the non-bubbling base), but that seems",0
less bad than duplication.,0
"If this is a native event triggered above, everything is now in order",0
Fire an inner synthetic event with the original arguments,0
...and capture the result,0
Support: IE <=9 - 11+,0
Extend with the prototype to reset the above stopImmediatePropagation(),0
Abort handling of the native event,0
"This ""if"" is needed for plain objects",0
Allow instantiation without the 'new' keyword,0
Event object,0
Events bubbling up the document may have been marked as prevented,0
by a handler lower down the tree; reflect the correct value.,0
Support: Android <=2.3 only,0
Create target properties,0
Support: Safari <=6 - 7 only,0
"Target should not be a text node (#504, #13143)",0
Event type,0
Put explicitly provided properties onto the event object,0
Create a timestamp if incoming event doesn't have one,0
Mark it as fixed,0
jQuery.Event is based on DOM3 Events as specified by the ECMAScript Language Binding,0
https://www.w3.org/TR/2003/WD-DOM-Level-3-Events-20030331/ecma-script-binding.html,0
Includes all common event props including KeyEvent and MouseEvent specific props,0
Add which for key events,0
Add which for click: 1 === left; 2 === middle; 3 === right,0
Utilize native event if possible so blur/focus sequence is correct,0
Claim the first handler,0
"dataPriv.set( this, ""focus"", ... )",0
"dataPriv.set( this, ""blur"", ... )",0
Return false to allow normal processing in the caller,0
Force setup before trigger,0
Return non-false to allow normal event-path propagation,0
Create mouseenter/leave events using mouseover/out and event-time checks,0
so that event delegation works in jQuery.,0
Do the same for pointerenter/pointerleave and pointerover/pointerout,0
,0
Support: Safari 7 only,0
Safari sends mouseenter too often; see:,0
https://bugs.chromium.org/p/chromium/issues/detail?id=470258,0
for the description of the bug (it existed in older Chrome versions as well).,0
For mouseenter/leave call the handler if related is outside the target.,0
NB: No relatedTarget if the mouse left/entered the browser window,0
( event )  dispatched jQuery.Event,0
"( types-object [, selector] )",0
"( types [, fn] )",0
See https://github.com/eslint/eslint/issues/3229,0
"Support: IE <=10 - 11, Edge 12 - 13 only",0
In IE/Edge using regex groups here causes severe slowdowns.,0
See https://connect.microsoft.com/IE/feedback/details/1736512/,0
"checked=""checked"" or checked",0
Prefer a tbody over its parent table for containing new rows,0
Replace/restore the type attribute of script elements for safe DOM manipulation,0
"1. Copy private data: events, handlers, etc.",0
2. Copy user data,0
"Fix IE bugs, see support tests",0
Fails to persist the checked state of a cloned checkbox or radio button.,0
Fails to return the selected option to the default selected state when cloning options,0
Flatten any nested arrays,0
"We can't cloneNode fragments that contain checked, in WebKit",0
Require either new content or an interest in ignored elements to invoke the callback,0
Use the original fragment for the last item,0
instead of the first because it can end up,0
being emptied incorrectly in certain situations (#8070).,0
Keep references to cloned scripts for later restoration,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
"push.apply(_, arraylike) throws on ancient WebKit",0
Reenable scripts,0
Evaluate executable scripts on first document insertion,0
"Optional AJAX dependency, but won't run scripts if not present",0
Fix IE cloning issues,0
We eschew Sizzle here for performance reasons: https://jsperf.com/getall-vs-sizzle/2,0
Copy the events from the original to the clone,0
Preserve script evaluation history,0
Return the cloned set,0
This is a shortcut to avoid jQuery.event.remove's overhead,0
Support: Chrome <=35 - 45+,0
"Assign undefined instead of using delete, see Data#remove",0
Support: Chrome <=35 - 45+,0
"Assign undefined instead of using delete, see Data#remove",0
Prevent memory leaks,0
Remove any remaining nodes,0
See if we can take a shortcut and just use innerHTML,0
Remove element nodes and prevent memory leaks,0
"If using innerHTML throws an exception, use the fallback method",0
"Make the changes, replacing each non-ignored context element with the new content",0
Force callback invocation,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
".get() because push.apply(_, arraylike) throws on ancient WebKit",0
"Support: IE <=11 only, Firefox <=30 (#15098, #14150)",0
IE throws on elements created in popups,0
"FF meanwhile throws on frame elements through ""defaultView.getComputedStyle""",0
Executing both pixelPosition & boxSizingReliable tests require only one layout,0
so they're executed at the same time to save the second computation.,0
"This is a singleton, we need to execute it only once",0
"Support: Android 4.0 - 4.3 only, Firefox <=3 - 44",0
"Support: Android 4.0 - 4.3 only, Safari <=9.1 - 10.1, iOS <=7.0 - 9.3",0
"Some styles come back with percentage values, even though they shouldn't",0
Support: IE 9 - 11 only,0
Detect misreporting of content dimensions for box-sizing:border-box elements,0
Support: IE 9 only,0
Detect overflow:scroll screwiness (gh-3699),0
Support: Chrome <=64,0
Don't get tricked when zoom affects offsetWidth (gh-4029),0
Nullify the div so it wouldn't be stored in the memory and,0
it will also be a sign that checks already performed,0
Finish early in limited (non-browser) environments,0
Support: IE <=9 - 11 only,0
Style of cloned element affects source element cloned (#8908),0
Support: Firefox 51+,0
Retrieving style before computed somehow,0
fixes an issue with getting wrong values,0
on detached elements,0
getPropertyValue is needed for:,0
".css('filter') (IE 9 only, #12537)",0
.css('--customProperty) (#3144),0
"A tribute to the ""awesome hack by Dean Edwards""",1
"Android Browser returns percentage for some values,",0
but width seems to be reliably pixels.,0
This is against the CSSOM draft spec:,0
https://drafts.csswg.org/cssom/#resolved-values,0
Remember the original values,0
Put in the new values to get a computed value out,0
Revert the changed values,0
Support: IE <=9 - 11 only,0
IE returns zIndex value as an integer.,0
"Define the hook, we'll check on the first run if it's really needed.",0
Hook not needed (or it's not possible to use it due,0
"to missing dependency), remove it.",0
Hook needed; redefine it so that the support test is not executed again.,0
Return a vendor-prefixed property or undefined,0
Check for vendor prefixed names,0
Return a potentially-mapped jQuery.cssProps or vendor prefixed property,0
Swappable if display is none or starts with table,0
"except ""table"", ""table-cell"", or ""table-caption""",0
See here for display values: https://developer.mozilla.org/en-US/docs/CSS/display,0
Any relative (+/-) values have already been,0
normalized at this point,0
"Guard against undefined ""subtract"", e.g., when used as in cssHooks",0
Adjustment may not be necessary,0
Both box models exclude margin,0
"If we get here with a content-box, we're seeking ""padding"" or ""border"" or ""margin""",0
Add padding,0
"For ""border"" or ""margin"", add border",0
But still keep track of it otherwise,0
"If we get here with a border-box (content + padding + border), we're seeking ""content"" or",0
"""padding"" or ""margin""",0
"For ""content"", subtract padding",0
"For ""content"" or ""padding"", subtract border",0
Account for positive content-box scroll gutter when requested by providing computedVal,0
"offsetWidth/offsetHeight is a rounded sum of content, padding, scroll gutter, and border",0
"Assuming integer scroll gutter, subtract the rest and round down",0
"If offsetWidth/offsetHeight is unknown, then we can't determine content-box scroll gutter",0
Use an explicit zero to avoid NaN (gh-3964),0
Start with computed style,0
"To avoid forcing a reflow, only fetch boxSizing if we need it (gh-4322).",0
Fake content-box until we know it's needed to know the true value.,0
Support: Firefox <=54,0
"Return a confounding non-pixel value or feign ignorance, as appropriate.",0
"Fall back to offsetWidth/offsetHeight when value is ""auto""",0
This happens for inline elements with no explicit setting (gh-3571),0
Support: Android <=4.1 - 4.3 only,0
Also use offsetWidth/offsetHeight for misreported inline dimensions (gh-3602),0
Support: IE 9-11 only,0
Also use offsetWidth/offsetHeight for when box sizing is unreliable,0
We use getClientRects() to check for hidden/disconnected.,0
"In those cases, the computed value can be trusted to be border-box",0
"Where available, offsetWidth/offsetHeight approximate border box dimensions.",0
"Where not available (e.g., SVG), assume unreliable box-sizing and interpret the",0
retrieved value as a content box dimension.,0
"Normalize """" and auto",0
Adjust for the element's box model,0
Provide the current computed size to request scroll gutter calculation (gh-3589),0
Add in style property hooks for overriding the default,0
behavior of getting and setting a style property,0
We should always get a number back from opacity,0
"Don't automatically add ""px"" to these possibly-unitless properties",0
Add in properties whose names you wish to fix before,0
setting or getting the value,0
Get and set the style property on a DOM Node,0
Don't set styles on text and comment nodes,0
Make sure that we're working with the right name,0
Make sure that we're working with the right name. We don't,0
want to query the value if it is a CSS custom property,0
since they are user-defined.,0
"Gets hook for the prefixed version, then unprefixed version",0
Check if we're setting a value,0
"Convert ""+="" or ""-="" to relative numbers (#7345)",0
Fixes bug #9237,0
Make sure that null and NaN values aren't set (#7116),0
"If a number was passed in, add the unit (except for certain CSS properties)",0
The isCustomProp check can be removed in jQuery 4.0 when we only auto-append,0
"""px"" to a few hardcoded values.",0
background-* props affect original clone's values,0
"If a hook was provided, use that value, otherwise just set the specified value",0
If a hook was provided get the non-computed value from there,0
Otherwise just get the value from the style object,0
Make sure that we're working with the right name. We don't,0
want to modify the value if it is a CSS custom property,0
since they are user-defined.,0
Try prefixed name followed by the unprefixed name,0
If a hook was provided get the computed value from there,0
"Otherwise, if a way to get the computed value exists, use that",0
"Convert ""normal"" to computed value",0
Make numeric if forced or a qualifier was provided and val looks numeric,0
Certain elements can have dimension info if we invisibly show them,0
but it must have a current display style that would benefit,0
Support: Safari 8+,0
Table columns in Safari have non-zero offsetWidth & zero,0
getBoundingClientRect().width unless display is changed.,0
Support: IE <=11 only,0
Running getBoundingClientRect on a disconnected node,0
in IE throws an error.,0
Only read styles.position if the test has a chance to fail,0
to avoid forcing a reflow.,0
"To avoid forcing a reflow, only fetch boxSizing if we need it (gh-3991)",0
Account for unreliable border-box dimensions by comparing offset* to computed and,0
faking a content-box to get border and padding (gh-3699),0
Convert to pixels if value adjustment is needed,0
These hooks are used by animate to expand properties,0
Assumes a single number if not a string,0
"Use a property on the element directly when it is not a DOM element,",0
or when there is no matching style property that exists.,0
Passing an empty string as a 3rd parameter to .css will automatically,0
attempt a parseFloat and fallback to a string if the parse fails.,0
"Simple values such as ""10px"" are parsed to Float;",0
"complex values such as ""rotate(1rad)"" are returned as-is.",0
"Empty strings, null, undefined and ""auto"" are converted to 0.",0
Use step hook for back compat.,0
Use cssHook if its there.,0
Use .style if available and use plain properties where available.,0
Support: IE <=9 only,0
Panic based approach to setting things on disconnected nodes,0
Back compat <1.8 extension point,0
Animations created synchronously will run synchronously,0
Generate parameters to create a standard animation,0
"If we include width, step value is 1 to do all cssExpand values,",0
otherwise step value is 2 to skip over Left and Right,0
We're done with this property,0
Queue-skipping animations hijack the fx hooks,0
Ensure the complete handler is called before this completes,0
Detect show/hide animations,0
"Pretend to be hidden if this is a ""show"" and",0
there is still data from a stopped show/hide,0
Ignore all other no-op show/hide data,0
Bail out if this is a no-op like .hide().hide(),0
"Restrict ""overflow"" and ""display"" styles during box animations",0
"Support: IE <=9 - 11, Edge 12 - 15",0
Record all 3 overflow attributes because IE does not infer the shorthand,0
from identically-valued overflowX and overflowY and Edge just mirrors,0
the overflowX value there.,0
"Identify a display type, preferring old show/hide data over the CSS cascade",0
Get nonempty value(s) by temporarily forcing visibility,0
Animate inline elements as inline-block,0
Restore the original display value at the end of pure show/hide animations,0
Implement show/hide animations,0
General show/hide setup for this element animation,0
"Store hidden/visible for toggle so `.stop().toggle()` ""reverses""",0
Show elements before animating them,0
"The final step of a ""hide"" animation is actually hiding the element",0
Per-property setup,0
"camelCase, specialEasing and expand cssHook pass",0
"Not quite $.extend, this won't overwrite existing keys.",0
"Reusing 'index' because we have the correct ""name""",0
Don't match elem in the :animated selector,0
Support: Android 2.3 only,0
Archaic crash bug won't allow us to use `1 - ( 0.5 || 0 )` (#12497),0
"If there's more to do, yield",0
"If this was an empty animation, synthesize a final progress notification",0
Resolve the animation and report its conclusion,0
"If we are going to the end, we want to run all the tweens",0
otherwise we skip this part,0
"Resolve when we played the last frame; otherwise, reject",0
Attach callbacks from options,0
Go to the end state if fx are off,0
"Normalize opt.queue - true/undefined/null -> ""fx""",0
Queueing,0
Show any hidden elements after setting opacity to 0,0
Animate to the value specified,0
Operate on a copy of prop so per-property easing won't be lost,0
"Empty animations, or finishing resolves immediately",0
Start the next in the queue if the last step wasn't forced.,0
"Timers currently will call their complete callbacks, which",0
will dequeue but only if they were gotoEnd.,0
Enable finishing flag on private data,0
Empty the queue first,0
"Look for any active animations, and finish them",0
Look for any animations in the old queue and finish them,0
Turn off finishing flag,0
Generate shortcuts for custom animations,0
Run the timer and safely remove it when done (allowing for external removal),0
Default speed,0
"Based off of the plugin by Clint Helfers, with permission.",0
https://web.archive.org/web/20100324014747/http://blindsignals.com/index.php/2009/07/jquery-delay/,0
Support: Android <=4.3 only,0
"Default value for a checkbox should be ""on""",0
Support: IE <=11 only,0
Must access selectedIndex to make default options select,0
Support: IE <=11 only,0
An input loses its value after becoming a radio,0
"Don't get/set attributes on text, comment and attribute nodes",0
Fallback to prop when attributes are not supported,0
Attribute hooks are determined by the lowercase version,0
Grab necessary hook if one is defined,0
"Non-existent attributes return null, we normalize to undefined",0
Attribute names can contain non-HTML whitespace characters,0
https://html.spec.whatwg.org/multipage/syntax.html#attributes-2,0
Hooks for boolean attributes,0
Remove boolean attributes when set to false,0
Avoid an infinite loop by temporarily removing this function from the getter,0
"Don't get/set properties on text, comment and attribute nodes",0
Fix name and attach hooks,0
Support: IE <=9 - 11 only,0
elem.tabIndex doesn't always return the,0
correct value when it hasn't been explicitly set,0
https://web.archive.org/web/20141116233347/http://fluidproject.org/blog/2008/01/09/getting-setting-and-removing-tabindex-values-with-javascript/,0
Use proper attribute retrieval(#12072),0
Support: IE <=11 only,0
Accessing the selectedIndex property,0
forces the browser to respect setting selected,0
on the option,0
The getter ensures a default option is selected,0
when in an optgroup,0
"eslint rule ""no-unused-expressions"" is disabled for this code",0
since it considers such accessions noop,0
Strip and collapse whitespace according to HTML spec,0
https://infra.spec.whatwg.org/#strip-and-collapse-ascii-whitespace,0
Only assign if different to avoid unneeded rendering.,0
This expression is here for better compressibility (see addClass),0
Remove *all* instances,0
Only assign if different to avoid unneeded rendering.,0
Toggle individual class names,0
"Check each className given, space separated list",0
Toggle whole class name,0
Store className if set,0
"If the element has a class name or if we're passed `false`,",0
"then remove the whole classname (if there was one, the above saved it).",0
"Otherwise bring back whatever was previously saved (if anything),",0
falling back to the empty string if nothing was stored.,0
Handle most common string cases,0
Handle cases where value is null/undef or number,0
"Treat null/undefined as """"; convert numbers to string",0
"If set returns undefined, fall back to normal setting",0
Support: IE <=10 - 11 only,0
"option.text throws exceptions (#14686, #14858)",0
Strip and collapse whitespace,0
https://html.spec.whatwg.org/#strip-and-collapse-whitespace,0
Loop through all the selected options,0
Support: IE <=9 only,0
IE8-9 doesn't update selected after form reset (#2551),0
Don't return options that are disabled or in a disabled optgroup,0
Get the specific value for the option,0
We don't need an array for one selects,0
Multi-Selects return an array,0
Force browsers to behave consistently when non-matching value is set,0
Radios and checkboxes getter/setter,0
Return jQuery for attributes-only inclusion,0
Don't do events on text and comment nodes,0
focus/blur morphs to focusin/out; ensure we're not firing them right now,0
Namespaced trigger; create a regexp to match event type in handle(),0
"Caller can pass in a jQuery.Event object, Object, or just an event type string",0
Trigger bitmask: & 1 for native handlers; & 2 for jQuery (always true),0
Clean up the event in case it is being reused,0
"Clone any incoming data and prepend the event, creating the handler arg list",0
Allow special events to draw outside the lines,0
"Determine event propagation path in advance, per W3C events spec (#9951)",0
"Bubble up to document, then to window; watch for a global ownerDocument var (#9724)",0
"Only add window if we got to document (e.g., not plain obj or detached DOM)",0
Fire handlers on the event path,0
jQuery handler,0
Native handler,0
"If nobody prevented the default action, do it now",0
Call a native DOM method on the target with the same name as the event.,0
"Don't do default actions on window, that's where global variables be (#6170)",0
Don't re-trigger an onFOO event when we call its FOO() method,0
"Prevent re-triggering of the same event, since we already bubbled it above",0
Piggyback on a donor event to simulate a different one,0
Used only for `focus(in | out)` events,0
Support: Firefox <=44,0
Firefox doesn't have focus(in | out) events,0
Related ticket - https://bugzilla.mozilla.org/show_bug.cgi?id=687787,0
,0
"Support: Chrome <=48 - 49, Safari <=9.0 - 9.1",0
"focus(in | out) events fire after focus & blur events,",0
which is spec violation - http://www.w3.org/TR/DOM-Level-3-Events/#events-focusevent-event-order,0
Related ticket - https://bugs.chromium.org/p/chromium/issues/detail?id=449857,0
Attach a single capturing handler on the document while someone wants focusin/focusout,0
Cross-browser xml parsing,0
Support: IE 9 - 11 only,0
IE throws on parseFromString with invalid input.,0
Serialize array item.,0
Treat each array item as a scalar.,0
"Item is non-scalar (array or object), encode its numeric index.",0
Serialize object item.,0
Serialize scalar item.,0
Serialize an array of form elements or a set of,0
key/values into a query string,0
"If value is a function, invoke it and use its return value",0
"If an array was passed in, assume that it is an array of form elements.",0
Serialize the form elements,0
"If traditional, encode the ""old"" way (the way 1.3.2 or older",0
"did it), otherwise encode params recursively.",0
Return the resulting serialization,0
"Can add propHook for ""elements"" to filter or add form elements",0
"Use .is( "":disabled"" ) so that fieldset[disabled] works",0
"#7653, #8125, #8152: local protocol detection",0
Avoid comment-prolog char sequence (#10098); must appease lint and evade compression,0
Anchor tag for parsing the document origin,0
"Base ""constructor"" for jQuery.ajaxPrefilter and jQuery.ajaxTransport",0
"dataTypeExpression is optional and defaults to ""*""",0
For each dataType in the dataTypeExpression,0
Prepend if requested,0
Otherwise append,0
Base inspection function for prefilters and transports,0
A special extend for ajax options,0
"that takes ""flat"" options (not to be deep extended)",0
Fixes #9887,0
Remove auto dataType and get content-type in the process,0
Check if we're dealing with a known content-type,0
Check to see if we have a response for the expected dataType,0
Try convertible dataTypes,0
Or just use first one,0
If we found a dataType,0
We add the dataType to the list if needed,0
and return the corresponding response,0
Work with a copy of dataTypes in case we need to modify it for conversion,0
Create converters map with lowercased keys,0
Convert to each sequential dataType,0
Apply the dataFilter if provided,0
There's only work to do if current dataType is non-auto,0
Convert response if prev dataType is non-auto and differs from current,0
Seek a direct converter,0
"If none found, seek a pair",0
If conv2 outputs current,0
If prev can be converted to accepted input,0
Condense equivalence converters,0
"Otherwise, insert the intermediate dataType",0
Apply converter (if not an equivalence),0
"Unless errors are allowed to bubble, catch and return them",0
Counter for holding the number of active queries,0
Last-Modified header cache for next request,0
Data converters,0
"Keys separate source (or catchall ""*"") and destination types with a single space",0
Convert anything to text,0
Text to html (true = no transformation),0
Evaluate text as a json expression,0
Parse text as xml,0
For options that shouldn't be deep extended:,0
you can add your own custom options here if,0
and when you create one that shouldn't be,0
deep extended (see ajaxExtend),0
Creates a full fledged settings object into target,0
with both ajaxSettings and settings fields.,0
"If target is omitted, writes into ajaxSettings.",0
Building a settings object,0
Extending ajaxSettings,0
Main method,0
"If url is an object, simulate pre-1.5 signature",0
Force options to be an object,0
URL without anti-cache param,0
Response headers,0
timeout handle,0
Url cleanup var,0
Request state (becomes false upon send and true upon completion),0
To know if global events are to be dispatched,0
Loop variable,0
uncached part of the url,0
Create the final options object,0
Callbacks context,0
Context for global events is callbackContext if it is a DOM node or jQuery collection,0
Deferreds,0
Status-dependent callbacks,0
Headers (they are sent all at once),0
Default abort message,0
Fake xhr,0
Builds headers hashtable if needed,0
Raw string,0
Caches the header,0
Overrides response content-type header,0
Status-dependent callbacks,0
Execute the appropriate callbacks,0
Lazy-add the new callbacks in a way that preserves old ones,0
Cancel the request,0
Attach deferreds,0
Add protocol if not provided (prefilters might expect it),0
Handle falsy url in the settings object (#10093: consistency with old signature),0
We also use the url parameter if available,0
Alias method option to type as per ticket #12004,0
Extract dataTypes list,0
A cross-domain request is in order when the origin doesn't match the current origin.,0
"Support: IE <=8 - 11, Edge 12 - 15",0
"IE throws exception on accessing the href property if url is malformed,",0
e.g. http://example.com:80x/,0
Support: IE <=8 - 11 only,0
Anchor's host property isn't correctly set when s.url is relative,0
"If there is an error parsing the URL, assume it is crossDomain,",0
it can be rejected by the transport if it is invalid,0
Convert data if not already a string,0
Apply prefilters,0
"If request was aborted inside a prefilter, stop there",0
We can fire global events as of now if asked to,0
Don't fire events if jQuery.event is undefined in an AMD-usage scenario (#15118),0
Watch for a new set of requests,0
Uppercase the type,0
Determine if request has content,0
Save the URL in case we're toying with the If-Modified-Since,0
and/or If-None-Match header later on,0
Remove hash to simplify url manipulation,0
More options handling for requests with no content,0
Remember the hash so we can put it back,0
"If data is available and should be processed, append data to url",0
#9682: remove data so that it's not used in an eventual retry,0
Add or update anti-cache param if needed,0
Put hash and anti-cache on the URL that will be requested (gh-1732),0
Change '%20' to '+' if this is encoded form body content (gh-2658),0
"Set the If-Modified-Since and/or If-None-Match header, if in ifModified mode.",0
"Set the correct header, if data is being sent",0
"Set the Accepts header for the server, depending on the dataType",0
Check for headers option,0
Allow custom headers/mimetypes and early abort,0
Abort if not done already and return,0
Aborting is no longer a cancellation,0
Install callbacks on deferreds,0
Get transport,0
"If no transport, we auto-abort",0
Send global event,0
"If request was aborted inside ajaxSend, stop there",0
Timeout,0
Rethrow post-completion exceptions,0
Propagate others as results,0
Callback for when everything is done,0
Ignore repeat invocations,0
Clear timeout if it exists,0
Dereference transport for early garbage collection,0
(no matter how long the jqXHR object will be used),0
Cache response headers,0
Set readyState,0
Determine if successful,0
Get response data,0
Convert no matter what (that way responseXXX fields are always set),0
"If successful, handle type chaining",0
"Set the If-Modified-Since and/or If-None-Match header, if in ifModified mode.",0
if no content,0
if not modified,0
"If we have data, let's convert it",0
Extract error from statusText and normalize for non-aborts,0
Set data for the fake xhr object,0
Success/Error,0
Status-dependent callbacks,0
Complete,0
Handle the global AJAX counter,0
Shift arguments if data argument was omitted,0
The url can be an options object (which then must have .url),0
"Make this explicit, since user can override this through ajaxSetup (#11264)",0
Only evaluate the response if it is successful (gh-4126),0
"dataFilter is not invoked for failure responses, so using it instead",0
of the default converter is kludgy but it works.,0
The elements to wrap the target around,0
"File protocol always yields status code 0, assume 200",0
Support: IE <=9 only,0
#1450: sometimes IE returns 1223 when it should be 204,0
Cross domain only allowed if supported through XMLHttpRequest,0
Apply custom fields if provided,0
Override mime type if needed,0
X-Requested-With header,0
"For cross-domain requests, seeing as conditions for a preflight are",0
"akin to a jigsaw puzzle, we simply never set it to be sure.",0
(it can always be set on a per-request basis or even using ajaxSetup),0
"For same-domain requests, won't change header if already provided.",0
Set headers,0
Callback,0
Support: IE <=9 only,0
"On a manual native abort, IE9 throws",0
errors on any property access that is not readyState,0
"File: protocol always yields status 0; see #8605, #14207",0
Support: IE <=9 only,0
IE9 has no XHR2 but throws on binary (trac-11426),0
"For XHR2 non-text, let the caller handle it (gh-2498)",0
Listen to events,0
Support: IE 9 only,0
Use onreadystatechange to replace onabort,0
to handle uncaught aborts,0
Check readyState before timeout as it changes,0
"Allow onerror to be called first,",0
but that will not handle a native abort,0
"Also, save errorCallback to a variable",0
as xhr.onerror cannot be accessed,0
Create the abort callback,0
Do send the request (this may raise an exception),0
#14683: Only rethrow if this hasn't been notified as an error yet,0
Prevent auto-execution of scripts when no explicit dataType was provided (See gh-2432),0
Install script dataType,0
Handle cache's special case and crossDomain,0
Bind script tag hack transport,0
This transport only deals with cross domain or forced-by-attrs requests,0
Use native DOM manipulation to avoid our domManip AJAX trickery,0
Default jsonp settings,0
"Detect, normalize options and install callbacks for jsonp requests",0
"Handle iff the expected data type is ""jsonp"" or we have a parameter to set",0
"Get callback name, remembering preexisting value associated with it",0
Insert callback into url or form data,0
Use data converter to retrieve json after script execution,0
Force json dataType,0
Install callback,0
Clean-up function (fires after converters),0
If previous value didn't exist - remove it,0
Otherwise restore preexisting value,0
Save back as free,0
Make sure that re-using the options doesn't screw things around,0
Save the callback name for future use,0
Call if it was a function and we have a response,0
Delegate to script,0
Support: Safari 8 only,0
In Safari 8 documents created via document.implementation.createHTMLDocument,0
collapse sibling forms: the second one becomes a child of the first one.,0
"Because of that, this security measure has to be disabled in Safari 8.",0
https://bugs.webkit.org/show_bug.cgi?id=137337,0
"Argument ""data"" should be string of html",0
"context (optional): If specified, the fragment will be created in this context,",0
defaults to document,0
"keepScripts (optional): If true, will include scripts passed in the html string",0
Stop scripts or inline event handlers from being executed immediately,0
by using document.implementation,0
Set the base href for the created document,0
so any parsed elements with URLs,0
are based on the document's URL (gh-2965),0
Single tag,0
If it's a function,0
We assume that it's the callback,0
"Otherwise, build a param string",0
"If we have elements to modify, make the request",0
"If ""type"" variable is undefined, then ""GET"" method will be used.",0
Make value of this field explicit since,0
user can override it through ajaxSetup method,0
Save response for use in complete callback,0
"If a selector was specified, locate the right elements in a dummy div",0
Exclude scripts to avoid IE 'Permission Denied' errors,0
Otherwise use the full result,0
"If the request succeeds, this function gets ""data"", ""status"", ""jqXHR""",0
but they are ignored because response was set above.,0
"If it fails, this function gets ""jqXHR"", ""status"", ""error""",0
Attach a bunch of functions for handling common AJAX events,0
"Set position first, in-case top/left are set even on static elem",0
Need to be able to calculate position if either,0
top or left is auto and position is either absolute or fixed,0
Use jQuery.extend here to allow modification of coordinates argument (gh-1848),0
offset() relates an element's border box to the document origin,0
Preserve chaining for setter,0
Return zeros for disconnected and hidden (display: none) elements (gh-2310),0
Support: IE <=11 only,0
Running getBoundingClientRect on a,0
disconnected node in IE throws an error,0
Get document-relative position by adding viewport scroll to viewport-relative gBCR,0
position() relates an element's margin box to its offset parent's padding box,0
This corresponds to the behavior of CSS absolute positioning,0
"position:fixed elements are offset from the viewport, which itself always has zero offset",0
Assume position:fixed implies availability of getBoundingClientRect,0
"Account for the *real* offset parent, which can be the document or its root element",0
when a statically positioned element is identified,0
"Incorporate borders into its offset, since they are outside its content origin",0
Subtract parent offsets and element margins,0
This method will return documentElement in the following cases:,0
"1) For the element inside the iframe without offsetParent, this method will return",0
documentElement of the parent window,0
2) For the hidden or detached element,0
"3) For body or html element, i.e. in case of the html node - it will return itself",0
,0
but those exceptions were never presented as a real life use-cases,0
and might be considered as more preferable results.,0
,0
"This logic, however, is not guaranteed and can change at any point in the future",0
Create scrollLeft and scrollTop methods,0
Coalesce documents and windows,0
"Support: Safari <=7 - 9.1, Chrome <=37 - 49",0
Add the top/left cssHooks using jQuery.fn.position,0
Webkit bug: https://bugs.webkit.org/show_bug.cgi?id=29084,0
Blink bug: https://bugs.chromium.org/p/chromium/issues/detail?id=589347,0
getComputedStyle returns percent when specified for top/left/bottom/right;,0
"rather than make the css module depend on the offset module, just check for it here",0
"If curCSS returns percentage, fallback to offset",0
"Create innerHeight, innerWidth, height, width, outerHeight and outerWidth methods",0
"Margin is only for outerHeight, outerWidth",0
$( window ).outerWidth/Height return w/h including scrollbars (gh-1729),0
Get document width or height,0
"Either scroll[Width/Height] or offset[Width/Height] or client[Width/Height],",0
whichever is greatest,0
"Get width or height on the element, requesting but not forcing parseFloat",0
Set width or height on the element,0
Handle event binding,0
"( namespace ) or ( selector, types [, fn] )",0
"Bind a function to a context, optionally partially applying any",0
arguments.,0
jQuery.proxy is deprecated to promote standards (specifically Function#bind),0
"However, it is not slated for removal any time soon",0
"Quick check to determine if target is callable, in the spec",0
"this throws a TypeError, but we will just return undefined.",0
Simulated bind,0
"Set the guid of unique handler to the same of original handler, so it can be removed",0
"As of jQuery 3.0, isNumeric is limited to",0
strings and numbers (primitives or objects),0
that can be coerced to finite numbers (gh-2662),0
"parseFloat NaNs numeric-cast false positives ("""")",0
"...but misinterprets leading-number strings, particularly hex literals (""0x..."")",0
subtraction forces infinities to NaN,0
"Register as a named AMD module, since jQuery can be concatenated with other",0
"files that may use define, but not via a proper concatenation script that",0
understands anonymous AMD modules. A named AMD is safest and most robust,0
way to register. Lowercase jquery is used because AMD module names are,0
"derived from file names, and jQuery is normally delivered in a lowercase",0
file name. Do this after creating the global so that if an AMD module wants,0
"to call noConflict to hide this version of jQuery, it will work.",0
"Note that for maximum portability, libraries that are not jQuery should",0
"declare themselves as anonymous modules, and avoid setting a global if an",0
"AMD loader is present. jQuery is a special case. For more information, see",0
https://github.com/jrburke/requirejs/wiki/Updating-existing-libraries#wiki-anon,0
Map over jQuery in case of overwrite,0
Map over the $ in case of overwrite,0
"Expose jQuery and $ identifiers, even in AMD",0
"(#7102#comment:10, https://github.com/jquery/jquery/pull/557)",0
and CommonJS for browser emulators (#13566),0
For CommonJS and CommonJS-like environments where a proper `window`,0
"is present, execute the factory and get jQuery.",0
For environments that do not have a `window` with a `document`,0
"(such as Node.js), expose a factory as module.exports.",0
This accentuates the need for the creation of a real `window`.,0
"e.g. var jQuery = require(""jquery"")(window);",0
See ticket #14549 for more info.,0
Pass this if window is not defined yet,0
"Edge <= 12 - 13+, Firefox <=18 - 45+, IE 10 - 11, Safari 5.1 - 9+, iOS 6 - 9.1",0
"throw exceptions when non-strict code (e.g., ASP.NET 4.5) accesses strict mode",0
"arguments.callee.caller (trac-13335). But as of jQuery 3.0 (2016), strict mode should be common",0
enough that all such attempts are guarded in a try block.,0
"Support: Chrome <=57, Firefox <=52",0
"In some browsers, typeof returns ""function"" for HTML <object> elements",0
"(i.e., `typeof document.createElement( ""object"" ) === ""function""`).",0
We don't want to classify *any* DOM node as a function.,0
"Support: Firefox 64+, Edge 18+",0
"Some browsers don't support the ""nonce"" property on scripts.",0
"On the other hand, just using `getAttribute` is not enough as",0
the `nonce` attribute is reset to an empty string whenever it,0
becomes browsing-context connected.,0
See https://github.com/whatwg/html/issues/2369,0
See https://html.spec.whatwg.org/#nonce-attributes,0
The `node.getAttribute` check was added for the sake of,0
`jQuery.globalEval` so that it can fake a nonce-containing node,0
via an object.,0
Support: Android <=2.3 only (functionish RegExp),0
Defining this global in .eslintrc.json would create a danger of using the global,0
"unguarded in another place, it seems safer to define global only for this module",0
Define a local copy of jQuery,0
The jQuery object is actually just the init constructor 'enhanced',0
Need init if jQuery is called (just allow error to be thrown if not included),0
Support: Android <=4.0 only,0
Make sure we trim BOM and NBSP,0
The current version of jQuery being used,0
The default length of a jQuery object is 0,0
Get the Nth element in the matched element set OR,0
Get the whole matched element set as a clean array,0
Return all the elements in a clean array,0
Return just the one element from the set,0
Take an array of elements and push it onto the stack,0
(returning the new matched element set),0
Build a new jQuery matched element set,0
Add the old object onto the stack (as a reference),0
Return the newly-formed element set,0
Execute a callback for every element in the matched set.,0
For internal use only.,0
"Behaves like an Array's method, not like a jQuery method.",0
Handle a deep copy situation,0
Skip the boolean and the target,0
Handle case when target is a string or something (possible in deep copy),0
Extend jQuery itself if only one argument is passed,0
Only deal with non-null/undefined values,0
Extend the base object,0
Prevent Object.prototype pollution,0
Prevent never-ending loop,0
Recurse if we're merging plain objects or arrays,0
Ensure proper type for the source value,0
"Never move original objects, clone them",0
Don't bring in undefined values,0
Return the modified object,0
Unique for each copy of jQuery on the page,0
Assume jQuery is ready without the ready module,0
Detect obvious negatives,0
Use toString instead of jQuery.type to catch host objects,0
"Objects with no prototype (e.g., `Object.create( null )`) are plain",0
Objects with prototype are plain iff they were constructed by a global Object function,0
Evaluates a script in a global context,0
Support: Android <=4.0 only,0
results is for internal usage only,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
"push.apply(_, arraylike) throws on ancient WebKit",0
"Go through the array, only saving the items",0
that pass the validator function,0
arg is for internal usage only,0
"Go through the array, translating each of the items to their new values",0
"Go through every key on the object,",0
Flatten any nested arrays,0
A global GUID counter for objects,0
jQuery.support is not used in Core but other projects attach their,0
properties to it so it needs to exist.,0
Populate the class2type map,0
Support: real iOS 8.2 only (not reproducible in simulator),0
`in` check used to prevent JIT error (gh-2145),0
hasOwn isn't used here due to false negatives,0
regarding Nodelist length in IE,0
Local document vars,0
Instance-specific data,0
Instance methods,0
Use a stripped-down indexOf as it's faster than native,0
https://jsperf.com/thor-indexof-vs-for/5,0
Regular expressions,0
http://www.w3.org/TR/css3-selectors/#whitespace,0
http://www.w3.org/TR/CSS21/syndata.html#value-def-identifier,0
Attribute selectors: http://www.w3.org/TR/selectors/#attribute-selectors,0
Operator (capture 2),0
"""Attribute values must be CSS identifiers [capture 5] or strings [capture 3 or capture 4]""",0
"To reduce the number of selectors needing tokenize in the preFilter, prefer arguments:",0
1. quoted (capture 3; capture 4 or capture 5),0
2. simple (capture 6),0
3. anything else (capture 2),0
"Leading and non-escaped trailing whitespace, capturing some non-whitespace characters preceding the latter",0
For use in libraries implementing .is(),0
We use this for POS matching in `select`,0
Easily-parseable/retrievable ID or TAG or CLASS selectors,0
CSS escapes,0
http://www.w3.org/TR/CSS21/syndata.html#escaped-characters,0
NaN means non-codepoint,0
Support: Firefox<24,0
"Workaround erroneous numeric interpretation of +""0x""",1
BMP codepoint,0
Supplemental Plane codepoint (surrogate pair),0
CSS string/identifier serialization,0
https://drafts.csswg.org/cssom/#common-serializing-idioms,0
U+0000 NULL becomes U+FFFD REPLACEMENT CHARACTER,0
Control characters and (dependent upon position) numbers get escaped as code points,0
Other potentially-special ASCII characters get backslash-escaped,0
Used for iframes,0
See setDocument(),0
"Removing the function wrapper causes a ""Permission Denied""",0
error in IE,0
"Optimize for push.apply( _, NodeList )",0
Support: Android<4.0,0
Detect silently failing push.apply,0
Leverage slice if possible,0
Support: IE<9,0
Otherwise append directly,0
Can't trust NodeList.length,0
"nodeType defaults to 9, since context defaults to document",0
Return early from calls with invalid selector or context,0
Try to shortcut find operations (as opposed to filters) in HTML documents,0
"If the selector is sufficiently simple, try using a ""get*By*"" DOM method",0
"(excepting DocumentFragment context, where the methods don't exist)",0
ID selector,0
Document context,0
"Support: IE, Opera, Webkit",0
TODO: identify versions,1
getElementById can match elements by name instead of ID,0
Element context,0
"Support: IE, Opera, Webkit",0
TODO: identify versions,1
getElementById can match elements by name instead of ID,0
Type selector,0
Class selector,0
Take advantage of querySelectorAll,0
Support: IE 8 only,0
Exclude object elements,0
qSA considers elements outside a scoping root when evaluating child or,0
"descendant combinators, which is not what we want.",0
"In such cases, we work around the behavior by prefixing every selector in the",0
list with an ID selector referencing the scope context.,0
Thanks to Andrew Dupont for this technique.,0
"Capture the context ID, setting it first if necessary",0
Prefix every selector in the list,0
Expand context for sibling selectors,0
All others,0
"Use (key + "" "") to avoid collision with native prototype properties (see Issue #157)",0
Only keep the most recent entries,0
Remove from its parent by default,0
release memory in IE,0
Use IE sourceIndex if available on both nodes,0
Check if b follows a,0
Known :disabled false positives: fieldset[disabled] > legend:nth-of-type(n+2) :can-disable,0
Only certain elements can match :enabled or :disabled,0
https://html.spec.whatwg.org/multipage/scripting.html#selector-enabled,0
https://html.spec.whatwg.org/multipage/scripting.html#selector-disabled,0
Check for inherited disabledness on relevant non-disabled elements:,0
* listed form-associated elements in a disabled fieldset,0
https://html.spec.whatwg.org/multipage/forms.html#category-listed,0
https://html.spec.whatwg.org/multipage/forms.html#concept-fe-disabled,0
* option elements in a disabled optgroup,0
https://html.spec.whatwg.org/multipage/forms.html#concept-option-disabled,0
"All such elements have a ""form"" property.",0
Option elements defer to a parent optgroup if present,0
Support: IE 6 - 11,0
Use the isDisabled shortcut property to check for disabled fieldset ancestors,0
"Where there is no isDisabled, check manually",0
Try to winnow out elements that can't be disabled before trusting the disabled property.,0
"Some victims get caught in our net (label, legend, menu, track), but it shouldn't",0
"even exist on them, let alone have a boolean value.",0
Remaining elements are neither :enabled nor :disabled,0
Match elements found at the specified indexes,0
Expose support vars for convenience,0
Support: IE <=8,0
"Assume HTML when documentElement doesn't yet exist, such as inside loading iframes",0
https://bugs.jquery.com/ticket/4833,0
Return early if doc is invalid or already selected,0
Update global variables,0
"Support: IE 9-11, Edge",0
"Accessing iframe documents after unload throws ""permission denied"" errors (jQuery #13936)",0
"Support: IE 11, Edge",0
Support: IE 9 - 10 only,0
Support: IE<8,0
Verify that getAttribute really returns attributes and not properties,0
(excepting IE8 booleans),0
"Check if getElementsByTagName(""*"") returns only elements",0
Support: IE<9,0
Support: IE<10,0
Check if getElementById returns elements by name,0
"The broken getElementById methods don't pick up programmatically-set names,",1
so use a roundabout getElementsByName test,0
ID filter and find,0
Support: IE 6 - 7 only,0
getElementById is not reliable as a find shortcut,0
Verify the id attribute,0
Fall back on getElementsByName,0
Tag,0
DocumentFragment nodes don't have gEBTN,0
"By happy coincidence, a (broken) gEBTN appears on DocumentFragment nodes too",0
Filter out possible comments,0
Class,0
QSA and matchesSelector support,0
matchesSelector(:active) reports false when true (IE9/Opera 11.5),0
qSa(:focus) reports false when true (Chrome 21),0
We allow this because of a bug in IE8/9 that throws an error,0
whenever `document.activeElement` is accessed on an iframe,0
"So, we allow :focus to pass through QSA all the time to avoid the IE error",0
See https://bugs.jquery.com/ticket/13378,0
Build QSA regex,0
Regex strategy adopted from Diego Perini,0
Select is set to empty string on purpose,0
This is to test IE's treatment of not explicitly,0
"setting a boolean content attribute,",0
since its presence should be enough,0
https://bugs.jquery.com/ticket/12359,0
"Support: IE8, Opera 11-12.16",0
Nothing should be selected when empty strings follow ^= or $= or *=,0
"The test attribute must be unknown in Opera but ""safe"" for WinRT",0
https://msdn.microsoft.com/en-us/library/ie/hh465388.aspx#attribute_section,0
Support: IE8,0
"Boolean attributes and ""value"" are not treated correctly",0
"Support: Chrome<29, Android<4.4, Safari<7.0+, iOS<7.0+, PhantomJS<1.9.8+",0
Webkit/Opera - :checked should return selected option elements,0
http://www.w3.org/TR/2011/REC-css3-selectors-20110929/#checked,0
IE8 throws error here and will not see later tests,0
"Support: Safari 8+, iOS 8+",0
https://bugs.webkit.org/show_bug.cgi?id=136851,0
In-page `selector#id sibling-combinator selector` fails,0
Support: Windows 8 Native Apps,0
The type and name attributes are restricted during .innerHTML assignment,0
Support: IE8,0
Enforce case-sensitivity of name attribute,0
FF 3.5 - :enabled/:disabled and hidden elements (hidden elements are still enabled),0
IE8 throws error here and will not see later tests,0
Support: IE9-11+,0
IE's :disabled selector does not pick up the children of disabled fieldsets,0
Opera 10-11 does not throw on post-comma invalid pseudos,0
Check to see if it's possible to do matchesSelector,0
on a disconnected node (IE 9),0
This should fail with an exception,0
"Gecko does not error, returns false instead",0
Element contains another,0
Purposefully self-exclusive,0
"As in, an element does not contain itself",0
Document order sorting,0
Flag for duplicate removal,0
Sort on method existence if only one input has compareDocumentPosition,0
Calculate position if both inputs belong to the same document,0
Otherwise we know they are disconnected,0
Disconnected nodes,0
Choose the first element that is related to our preferred document,0
Maintain original order,0
Exit early if the nodes are identical,0
Parentless nodes are either documents or disconnected,0
"If the nodes are siblings, we can do a quick check",0
Otherwise we need full lists of their ancestors for comparison,0
Walk down the tree looking for a discrepancy,0
Do a sibling check if the nodes have a common ancestor,0
Otherwise nodes in our document sort first,0
Set document vars if needed,0
IE 9's matchesSelector returns false on disconnected nodes,0
"As well, disconnected nodes are said to be in a document",0
fragment in IE 9,0
Set document vars if needed,0
Set document vars if needed,0
Don't get fooled by Object.prototype properties (jQuery #13807),0
"Unless we *know* we can detect duplicates, assume their presence",0
Clear input after sorting to release objects,0
See https://github.com/jquery/sizzle/pull/225,0
"If no nodeType, this is expected to be an array",0
Do not traverse comment nodes,0
Use textContent for elements,0
innerText usage removed for consistency of new lines (jQuery #11153),0
Traverse its children,0
Do not include comment or processing instruction nodes,0
Can be adjusted by the user,0
Move the given value to match[3] whether quoted or unquoted,0
nth-* requires argument,0
numeric x and y parameters for Expr.filter.CHILD,0
remember that false/true cast respectively to 0/1,0
other types prohibit arguments,0
Accept quoted arguments as-is,0
Strip excess characters from unquoted arguments,0
Get excess from tokenize (recursively),0
advance to the next closing parenthesis,0
excess is a negative index,0
Return only captures needed by the pseudo filter method (type and argument),0
Shortcut for :nth-*(n),0
:(first|last|only)-(child|of-type),0
Reverse direction for :only-* (if we haven't yet done so),0
non-xml :nth-child(...) stores cache data on `parent`,0
Seek `elem` from a previously-cached index,0
...in a gzip-friendly way,0
Support: IE <9 only,0
Defend against cloned attroperties (jQuery gh-1709),0
Fallback to seeking `elem` from the start,0
"When found, cache indexes on `parent` and break",0
Use previously-cached element index if available,0
...in a gzip-friendly way,0
Support: IE <9 only,0
Defend against cloned attroperties (jQuery gh-1709),0
xml :nth-child(...),0
or :nth-last-child(...) or :nth(-last)?-of-type(...),0
Use the same loop as above to seek `elem` from the start,0
Cache the index of each encountered element,0
Support: IE <9 only,0
Defend against cloned attroperties (jQuery gh-1709),0
"Incorporate the offset, then check against cycle size",0
pseudo-class names are case-insensitive,0
http://www.w3.org/TR/selectors/#pseudo-classes,0
Prioritize by case sensitivity in case custom pseudos are added with uppercase letters,0
Remember that setFilters inherits from pseudos,0
The user may use createPseudo to indicate that,0
arguments are needed to create the filter function,0
just as Sizzle does,0
But maintain support for old signatures,0
Potentially complex pseudos,0
Trim the selector passed to compile,0
to avoid treating leading and trailing,0
spaces as combinators,0
Match elements unmatched by `matcher`,0
Don't keep the element (issue #299),0
"""Whether an element is represented by a :lang() selector",0
is based solely on the element's language value,0
"being equal to the identifier C,",0
"or beginning with the identifier C immediately followed by ""-"".",0
The matching of C against the element's language value is performed case-insensitively.,0
"The identifier C does not have to be a valid language name.""",0
http://www.w3.org/TR/selectors/#lang-pseudo,0
lang value must be a valid identifier,0
Miscellaneous,0
Boolean properties,0
"In CSS3, :checked should return both checked and selected elements",0
http://www.w3.org/TR/2011/REC-css3-selectors-20110929/#checked,0
Accessing this property makes selected-by-default,0
options in Safari work properly,0
Contents,0
http://www.w3.org/TR/selectors/#empty-pseudo,0
":empty is negated by element (1) or content nodes (text: 3; cdata: 4; entity ref: 5),",0
but not by others (comment: 8; processing instruction: 7; etc.),0
nodeType < 6 works because attributes (2) do not appear as children,0
Element/input types,0
Support: IE<8,0
"New HTML5 attribute values (e.g., ""search"") appear with elem.type === ""text""",0
Position-in-collection,0
Add button/input type pseudos,0
Easy API for creating new setFilters,0
Comma and first run,0
Don't consume trailing commas as valid,0
Combinators,0
Cast descendant combinators to space,0
Filters,0
Return the length of the invalid excess,0
if we're just parsing,0
"Otherwise, throw an error or return tokens",0
Cache the tokens,0
Check against closest ancestor/preceding element,0
Check against all ancestor/preceding elements,0
"We can't set arbitrary data on XML nodes, so they don't benefit from combinator caching",0
Support: IE <9 only,0
Defend against cloned attroperties (jQuery gh-1709),0
Assign to newCache so results back-propagate to previous elements,0
Reuse newcache so results back-propagate to previous elements,0
A match means we're done; a fail means we have to keep checking,0
Get initial elements from seed or context,0
"Prefilter to get matcher input, preserving a map for seed-results synchronization",0
"If we have a postFinder, or filtered seed, or non-seed postFilter or preexisting results,",0
...intermediate processing is necessary,0
...otherwise use results directly,0
Find primary matches,0
Apply postFilter,0
Un-match failing elements by moving them back to matcherIn,0
Get the final matcherOut by condensing this intermediate into postFinder contexts,0
Restore matcherIn since elem is not yet a final match,0
Move matched elements from seed to results to keep them synchronized,0
"Add elements to results, through postFinder if defined",0
The foundational matcher ensures that elements are reachable from top-level context(s),0
Avoid hanging onto element (issue #299),0
Return special upon seeing a positional matcher,0
Find the next relative operator (if any) for proper handling,0
"If the preceding token was a descendant combinator, insert an implicit any-element `*`",0
We must always have either seed elements or outermost context,0
Use integer dirruns iff this is the outermost matcher,0
Add elements passing elementMatchers directly to results,0
"Support: IE<9, Safari",0
"Tolerate NodeList properties (IE: ""length""; Safari: <number>) matching elements by id",0
Track unmatched elements for set filters,0
They will have gone through all possible matchers,0
"Lengthen the array for every element, matched or not",0
"`i` is now the count of elements visited above, and adding it to `matchedCount`",0
makes the latter nonnegative.,0
Apply set filters to unmatched elements,0
"NOTE: This can be skipped if there are no unmatched elements (i.e., `matchedCount`",0
"equals `i`), unless we didn't visit _any_ elements in the above loop because we have",0
no element matchers and no seed.,0
"Incrementing an initially-string ""0"" `i` allows `i` to remain a string only in that",0
"case, which will result in a ""00"" `matchedCount` that differs from `i` but is also",0
numerically zero.,0
Reintegrate element matches to eliminate the need for sorting,0
Discard index placeholder values to get only actual matches,0
Add matches to results,0
Seedless set matches succeeding multiple successful matchers stipulate sorting,0
Override manipulation of globals by nested matchers,0
Generate a function of recursive functions that can be used to check each element,0
Cache the compiled function,0
Save selector and tokenization,0
Try to minimize operations if there is only one selector in the list and no seed,0
(the latter of which guarantees us context),0
Reduce context if the leading compound selector is an ID,0
"Precompiled matchers will still verify ancestry, so step up a level",0
Fetch a seed set for right-to-left matching,0
Abort if we hit a combinator,0
"Search, expanding context for leading sibling combinators",0
"If seed is empty or no tokens remain, we can return early",0
Compile and execute a filtering function if one is not provided,0
Provide `match` to avoid retokenization if we modified the selector above,0
One-time assignments,0
Sort stability,0
Support: Chrome 14-35+,0
Always assume duplicates if they aren't passed to the comparison function,0
Initialize against the default document,0
Support: Webkit<537.32 - Safari 6.0.3/Chrome 25 (fixed in Chrome 27),0
Detached nodes confoundingly follow *each other*,0
"Should return 1, but returns 4 (following)",0
Support: IE<8,0
"Prevent attribute/property ""interpolation""",0
https://msdn.microsoft.com/en-us/library/ms536429%28VS.85%29.aspx,0
Support: IE<9,0
"Use defaultValue in place of getAttribute(""value"")",0
Support: IE<9,0
Use getAttributeNode to fetch booleans when getAttribute lies,0
Deprecated,0
Implement the identical functionality for filter and not,0
Single element,0
"Arraylike of elements (jQuery, arguments, Array)",0
Filtered directly for both simple and complex selectors,0
"If this is a positional/relative selector, check membership in the returned set",0
"so $(""p:first"").is(""p:last"") won't return true for a doc with two ""p"".",0
Initialize a jQuery object,0
A central reference to the root jQuery(document),0
A simple way to check for HTML strings,0
Prioritize #id over <tag> to avoid XSS via location.hash (#9521),0
Strict HTML recognition (#11290: must start with <),0
Shortcut simple #id case for speed,0
"HANDLE: $(""""), $(null), $(undefined), $(false)",0
Method init() accepts an alternate rootjQuery,0
so migrate can support jQuery.sub (gh-2101),0
Handle HTML strings,0
Assume that strings that start and end with <> are HTML and skip the regex check,0
Match html or make sure no context is specified for #id,0
HANDLE: $(html) -> $(array),0
Option to run scripts is true for back-compat,0
Intentionally let the error be thrown if parseHTML is not present,0
"HANDLE: $(html, props)",0
Properties of context are called as methods if possible,0
...and otherwise set as attributes,0
HANDLE: $(#id),0
Inject the element directly into the jQuery object,0
"HANDLE: $(expr, $(...))",0
"HANDLE: $(expr, context)",0
(which is just equivalent to: $(context).find(expr),0
HANDLE: $(DOMElement),0
HANDLE: $(function),0
Shortcut for document ready,0
Execute immediately if ready is not present,0
Give the init function the jQuery prototype for later instantiation,0
Initialize central reference,0
Methods guaranteed to produce a unique set when starting from a unique set,0
"Positional selectors never match, since there's no _selection_ context",0
Always skip document fragments,0
Don't pass non-elements to Sizzle,0
Determine the position of an element within the set,0
"No argument, return index in parent",0
Index in selector,0
Locate the position of the desired element,0
"If it receives a jQuery object, the first element is used",0
"Support: IE 9 - 11 only, iOS 7 only, Android Browser <=4.3 only",0
Treat the template element as a regular one in browsers that,0
don't support it.,0
Remove duplicates,0
Reverse order for parents* and prev-derivatives,0
Convert String-formatted options into Object-formatted ones,0
Convert options from String-formatted to Object-formatted if needed,0
(we check in cache first),0
Last fire value for non-forgettable lists,0
Flag to know if list was already fired,0
Flag to prevent firing,0
Actual callback list,0
Queue of execution data for repeatable lists,0
Index of currently firing callback (modified by add/remove as needed),0
Fire callbacks,0
Enforce single-firing,0
"Execute callbacks for all pending executions,",0
respecting firingIndex overrides and runtime changes,0
Run callback and check for early termination,0
Jump to end and forget the data so .add doesn't re-fire,0
Forget the data if we're done with it,0
Clean up if we're done firing for good,0
Keep an empty list if we have data for future add calls,0
"Otherwise, this object is spent",0
Actual Callbacks object,0
Add a callback or a collection of callbacks to the list,0
"If we have memory from a past run, we should fire after adding",0
Inspect recursively,0
Remove a callback from the list,0
Handle firing indexes,0
Check if a given callback is in the list.,0
"If no argument is given, return whether or not list has callbacks attached.",0
Remove all callbacks from the list,0
Disable .fire and .add,0
Abort any current/pending executions,0
Clear all callbacks and values,0
Disable .fire,0
Also disable .add unless we have memory (since it would have no effect),0
Abort any pending executions,0
Call all callbacks with the given context and arguments,0
Call all the callbacks with the given arguments,0
To know if the callbacks have already been called at least once,0
Check for promise aspect first to privilege synchronous behavior,0
Other thenables,0
Other non-thenables,0
Control `resolve` arguments by letting Array#slice cast boolean `noValue` to integer:,0
* false: [ value ].slice( 0 ) => resolve( value ),0
* true: [ value ].slice( 1 ) => resolve(),0
"For Promises/A+, convert exceptions into rejections",0
"Since jQuery.when doesn't unwrap thenables, we can skip the extra checks appearing in",0
Deferred#then to conditionally suppress rejection.,0
Support: Android 4.0 only,0
Strict mode functions invoked without .call/.apply get global-object context,0
"action, add listener, callbacks,",0
"... .then handlers, argument index, [final state]",0
Keep pipe for back-compat,0
"Map tuples (progress, done, fail) to arguments (done, fail, progress)",0
deferred.progress(function() { bind to newDefer or newDefer.notify }),0
deferred.done(function() { bind to newDefer or newDefer.resolve }),0
deferred.fail(function() { bind to newDefer or newDefer.reject }),0
Support: Promises/A+ section 2.3.3.3.3,0
https://promisesaplus.com/#point-59,0
Ignore double-resolution attempts,0
Support: Promises/A+ section 2.3.1,0
https://promisesaplus.com/#point-48,0
"Support: Promises/A+ sections 2.3.3.1, 3.5",0
https://promisesaplus.com/#point-54,0
https://promisesaplus.com/#point-75,0
Retrieve `then` only once,0
Support: Promises/A+ section 2.3.4,0
https://promisesaplus.com/#point-64,0
Only check objects and functions for thenability,0
Handle a returned thenable,0
Special processors (notify) just wait for resolution,0
Normal processors (resolve) also hook into progress,0
...and disregard older resolution values,0
Handle all other returned values,0
Only substitute handlers pass on context,0
and multiple values (non-spec behavior),0
Process the value(s),0
Default process is resolve,0
Only normal processors (resolve) catch and reject exceptions,0
Support: Promises/A+ section 2.3.3.3.4.1,0
https://promisesaplus.com/#point-61,0
Ignore post-resolution exceptions,0
Only substitute handlers pass on context,0
and multiple values (non-spec behavior),0
Support: Promises/A+ section 2.3.3.3.1,0
https://promisesaplus.com/#point-57,0
Re-resolve promises immediately to dodge false rejection from,0
subsequent errors,0
"Call an optional hook to record the stack, in case of exception",0
since it's otherwise lost when execution goes async,0
progress_handlers.add( ... ),0
fulfilled_handlers.add( ... ),0
rejected_handlers.add( ... ),0
Get a promise for this deferred,0
"If obj is provided, the promise aspect is added to the object",0
Add list-specific methods,0
promise.progress = list.add,0
promise.done = list.add,0
promise.fail = list.add,0
Handle state,0
"state = ""resolved"" (i.e., fulfilled)",0
"state = ""rejected""",0
rejected_callbacks.disable,0
fulfilled_callbacks.disable,0
rejected_handlers.disable,0
fulfilled_handlers.disable,0
progress_callbacks.lock,0
progress_handlers.lock,0
progress_handlers.fire,0
fulfilled_handlers.fire,0
rejected_handlers.fire,0
deferred.notify = function() { deferred.notifyWith(...) },0
deferred.resolve = function() { deferred.resolveWith(...) },0
deferred.reject = function() { deferred.rejectWith(...) },0
deferred.notifyWith = list.fireWith,0
deferred.resolveWith = list.fireWith,0
deferred.rejectWith = list.fireWith,0
Make the deferred a promise,0
Call given func if any,0
All done!,0
Deferred helper,0
count of uncompleted subordinates,0
count of unprocessed arguments,0
subordinate fulfillment data,0
the master Deferred,0
subordinate callback factory,0
Single- and empty arguments are adopted like Promise.resolve,0
Use .then() to unwrap secondary thenables (cf. gh-3000),0
Multiple arguments are aggregated like Promise.all array elements,0
"These usually indicate a programmer mistake during development,",0
warn about them ASAP rather than swallowing them by default.,0
Support: IE 8 - 9 only,0
"Console exists when dev tools are open, which can happen at any time",0
The deferred used on DOM ready,0
Wrap jQuery.readyException in a function so that the lookup,0
happens at the time of error handling instead of callback,0
registration.,0
Is the DOM ready to be used? Set to true once it occurs.,0
A counter to track how many items to wait for before,0
the ready event fires. See #6781,0
Handle when the DOM is ready,0
Abort if there are pending holds or we're already ready,0
Remember that the DOM is ready,0
"If a normal DOM Ready event fired, decrement, and wait if need be",0
"If there are functions bound, to execute",0
The ready event handler and self cleanup method,0
Catch cases where $(document).ready() is called,0
after the browser event has already occurred.,0
Support: IE <=9 - 10 only,0
"Older IE sometimes signals ""interactive"" too soon",0
Handle it asynchronously to allow scripts the opportunity to delay ready,0
Use the handy event callback,0
"A fallback to window.onload, that will always work",0
Multifunctional method to get and set values of a collection,0
The value/s can optionally be executed if it's a function,0
Sets many values,0
Sets one value,0
Bulk operations run against the entire set,0
...except when executing function values,0
Gets,0
Matches dashed string for camelizing,0
Used by camelCase as callback to replace(),0
Convert dashed to camelCase; used by the css and data modules,0
"Support: IE <=9 - 11, Edge 12 - 15",0
Microsoft forgot to hump their vendor prefix (#9572),0
Accepts only:,0
- Node,0
- Node.ELEMENT_NODE,0
- Node.DOCUMENT_NODE,0
- Object,0
- Any,0
Check if the owner object already has a cache,0
"If not, create one",0
"We can accept data for non-element nodes in modern browsers,",0
"but we should not, see #8335.",0
Always return an empty object.,0
If it is a node unlikely to be stringify-ed or looped over,0
use plain assignment,0
Otherwise secure it in a non-enumerable property,0
configurable must be true to allow the property to be,0
deleted when data is removed,0
"Handle: [ owner, key, value ] args",0
Always use camelCase key (gh-2257),0
"Handle: [ owner, { properties } ] args",0
Copy the properties one-by-one to the cache object,0
Always use camelCase key (gh-2257),0
In cases where either:,0
,0
1. No key was specified,0
"2. A string key was specified, but no value provided",0
,0
"Take the ""read"" path and allow the get method to determine",0
"which value to return, respectively either:",0
,0
1. The entire cache object,0
2. The data stored at the key,0
,0
"When the key is not a string, or both a key and value",0
"are specified, set or extend (existing objects) with either:",0
,0
1. An object of properties,0
2. A key and value,0
,0
"Since the ""set"" path can have two possible entry points",0
return the expected data based on which path was taken[*],0
Support array or space separated string of keys,0
If key is an array of keys...,0
"We always set camelCase keys, so remove that.",0
"If a key with the spaces exists, use it.",0
"Otherwise, create an array by matching non-whitespace",0
Remove the expando if there's no more data,0
Support: Chrome <=35 - 45,0
Webkit & Blink performance suffers when deleting properties,0
"from DOM nodes, so set to undefined instead",0
https://bugs.chromium.org/p/chromium/issues/detail?id=378607 (bug restricted),0
Implementation Summary,0
,0
1. Enforce API surface and semantic compatibility with 1.9.x branch,0
2. Improve the module's maintainability by reducing the storage,0
paths to a single mechanism.,0
"3. Use the same single mechanism to support ""private"" and ""user"" data.",0
"4. _Never_ expose ""private"" data to user code (TODO: Drop _data, _removeData)",1
5. Avoid exposing implementation details on user objects (eg. expando properties),0
6. Provide a clear path for implementation upgrade to WeakMap in 2014,0
Only convert to a number if it doesn't change the string,0
"If nothing was found internally, try to fetch any",0
data from the HTML5 data-* attribute,0
Make sure we set the data so it isn't changed later,0
TODO: Now that all calls to _data and _removeData have been replaced,1
"with direct calls to dataPriv methods, these can be deprecated.",0
Gets all values,0
Support: IE 11 only,0
The attrs elements can be null (#14894),0
Sets multiple values,0
The calling jQuery object (element matches) is not empty,0
(and therefore has an element appears at this[ 0 ]) and the,0
`value` parameter was not undefined. An empty jQuery object,0
will result in `undefined` for elem = this[ 0 ] which will,0
throw an exception if an attempt to read a data cache is made.,0
Attempt to get data from the cache,0
The key will always be camelCased in Data,0
"Attempt to ""discover"" the data in",0
HTML5 custom data-* attrs,0
"We tried really hard, but the data doesn't exist.",0
Set the data...,0
We always store the camelCased key,0
Speed up dequeue by getting out quickly if this is just a lookup,0
"If the fx queue is dequeued, always remove the progress sentinel",0
Add a progress sentinel to prevent the fx queue from being,0
automatically dequeued,0
Clear up the last queue stop function,0
"Not public - generate a queueHooks object, or return the current one",0
Ensure a hooks for this queue,0
Get a promise resolved when queues of a certain type,0
are emptied (fx is the type by default),0
Check attachment across shadow DOM boundaries when possible (gh-3504),0
isHiddenWithinTree might be called from jQuery#filter function;,0
"in that case, element will be second argument",0
Inline style trumps all,0
"Otherwise, check computed style",0
Support: Firefox <=43 - 45,0
"Disconnected elements can have computed display: none, so first confirm that elem is",0
in the document.,0
"Remember the old values, and insert the new ones",0
Revert the old values,0
Starting value computation is required for potential unit mismatches,0
Support: Firefox <=54,0
Halve the iteration target value to prevent interference from CSS upper bounds (gh-2144),0
Trust units reported by jQuery.css,0
Iteratively approximate from a nonzero starting point,0
Evaluate and update our best guess (doubling guesses that zero out).,0
Finish if the scale equals or crosses 1 (making the old*new product non-positive).,0
Make sure we update the tween properties later on,0
Apply relative offset (+=/-=) if specified,0
Determine new display value for elements that need to change,0
"Since we force visibility upon cascade-hidden elements, an immediate (and slow)",0
check is required in this first loop unless we have a nonempty display value (either,0
inline or about-to-be-restored),0
Remember what we're overwriting,0
Set the display of the elements in a second loop to avoid constant reflow,0
We have to close these tags to support XHTML (#13200),0
Support: IE <=9 only,0
XHTML parsers do not magically insert elements in the,0
same way that tag soup parsers do. So we cannot shorten,0
this by omitting <tbody> or other required elements.,0
Support: IE <=9 only,0
Support: IE <=9 - 11 only,0
Use typeof to avoid zero-argument method invocation on host objects (#15151),0
Mark scripts as having already been evaluated,0
Add nodes directly,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
"push.apply(_, arraylike) throws on ancient WebKit",0
Convert non-html into a text node,0
Convert html into DOM nodes,0
Deserialize a standard representation,0
Descend through wrappers to the right content,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
"push.apply(_, arraylike) throws on ancient WebKit",0
Remember the top-level container,0
Ensure the created nodes are orphaned (#12392),0
Remove wrapper from fragment,0
Skip elements already in the context collection (trac-4087),0
Append to fragment,0
Preserve script evaluation history,0
Capture executables,0
Support: Android 4.0 - 4.3 only,0
Check state lost if the name is set (#11217),0
Support: Windows Web Apps (WWA),0
`name` and `type` must use .setAttribute for WWA (#14901),0
Support: Android <=4.1 only,0
Older WebKit doesn't clone checked state correctly in fragments,0
Support: IE <=11 only,0
Make sure textarea (and checkbox) defaultValue is properly cloned,0
Support: IE <=9 - 11+,0
"focus() and blur() are asynchronous, except when they are no-op.",0
"So expect focus to be synchronous when the element is already active,",0
and blur to be synchronous when the element is not already active.,0
"(focus and blur are always synchronous in other supported browsers,",0
this just defines when we can count on it).,0
Support: IE <=9 only,0
Accessing document.activeElement can throw unexpectedly,0
https://bugs.jquery.com/ticket/13393,0
Types can be a map of types/handlers,0
"( types-Object, selector, data )",0
"( types-Object, data )",0
"( types, fn )",0
"( types, selector, fn )",0
"( types, data, fn )",0
"Can use an empty set, since event contains the info",0
Use same guid so caller can remove using origFn,0
Don't attach events to noData or text/comment nodes (but allow plain objects),0
Caller can pass in an object of custom data in lieu of the handler,0
Ensure that invalid selectors throw exceptions at attach time,0
"Evaluate against documentElement in case elem is a non-element node (e.g., document)",0
"Make sure that the handler has a unique ID, used to find/remove it later",0
"Init the element's event structure and main handler, if this is the first",0
Discard the second event of a jQuery.event.trigger() and,0
when an event is called after a page has unloaded,0
Handle multiple events separated by a space,0
"There *must* be a type, no attaching namespace-only handlers",0
"If event changes its type, use the special event handlers for the changed type",0
"If selector defined, determine special event api type, otherwise given type",0
Update special based on newly reset type,0
handleObj is passed to all event handlers,0
Init the event handler queue if we're the first,0
Only use addEventListener if the special events handler returns false,0
"Add to the element's handler list, delegates in front",0
"Keep track of which events have ever been used, for event optimization",0
Detach an event or set of events from an element,0
Once for each type.namespace in types; type may be omitted,0
"Unbind all events (on this namespace, if provided) for the element",0
Remove matching events,0
Remove generic event handler if we removed something and no more handlers exist,0
(avoids potential for endless recursion during removal of special event handlers),0
Remove data and the expando if it's no longer used,0
Make a writable jQuery.Event from the native event object,0
Use the fix-ed jQuery.Event rather than the (read-only) native event,0
"Call the preDispatch hook for the mapped type, and let it bail if desired",0
Determine handlers,0
Run delegates first; they may want to stop propagation beneath us,0
"If the event is namespaced, then each handler is only invoked if it is",0
specially universal or its namespaces are a superset of the event's.,0
Call the postDispatch hook for the mapped type,0
Find delegate handlers,0
Support: IE <=9,0
Black-hole SVG <use> instance trees (trac-13180),0
Support: Firefox <=42,0
Suppress spec-violating clicks indicating a non-primary pointer button (trac-3861),0
https://www.w3.org/TR/DOM-Level-3-Events/#event-type-click,0
Support: IE 11 only,0
"...but not arrow key ""clicks"" of radio inputs, which can have `button` -1 (gh-2343)",0
Don't check non-elements (#13208),0
"Don't process clicks on disabled elements (#6911, #8165, #11382, #11764)",0
Don't conflict with Object.prototype properties (#13203),0
Add the remaining (directly-bound) handlers,0
Prevent triggered image.load events from bubbling to window.load,0
Utilize native event to ensure correct state for checkable inputs,0
"For mutual compressibility with _default, replace `this` access with a local var.",0
`|| data` is dead code meant only to preserve the variable through minification.,0
Claim the first handler,0
"dataPriv.set( el, ""click"", ... )",0
Return false to allow normal processing in the caller,0
"For mutual compressibility with _default, replace `this` access with a local var.",0
`|| data` is dead code meant only to preserve the variable through minification.,0
Force setup before triggering a click,0
Return non-false to allow normal event-path propagation,0
"For cross-browser consistency, suppress native .click() on links",0
Also prevent it if we're currently inside a leveraged native-event stack,0
Support: Firefox 20+,0
Firefox doesn't alert if the returnValue field is not set.,0
Ensure the presence of an event listener that handles manually-triggered,0
synthetic events by interrupting progress until reinvoked in response to,0
"*native* events that it fires directly, ensuring that state changes have",0
already occurred before other listeners are invoked.,0
"Missing expectSync indicates a trigger call, which must force setup through jQuery.event.add",0
Register the controller as a special universal handler for all event namespaces,0
Interrupt processing of the outer synthetic .trigger()ed event,0
Store arguments for use when handling the inner native event,0
Trigger the native event and capture its result,0
Support: IE <=9 - 11+,0
focus() and blur() are asynchronous,0
Cancel the outer synthetic event,0
If this is an inner synthetic event for an event with a bubbling surrogate,0
"(focus or blur), assume that the surrogate already propagated from triggering the",0
native event and prevent that from happening again here.,0
This technically gets the ordering wrong w.r.t. to `.trigger()` (in which the,0
"bubbling surrogate propagates *after* the non-bubbling base), but that seems",0
less bad than duplication.,0
"If this is a native event triggered above, everything is now in order",0
Fire an inner synthetic event with the original arguments,0
...and capture the result,0
Support: IE <=9 - 11+,0
Extend with the prototype to reset the above stopImmediatePropagation(),0
Abort handling of the native event,0
"This ""if"" is needed for plain objects",0
Allow instantiation without the 'new' keyword,0
Event object,0
Events bubbling up the document may have been marked as prevented,0
by a handler lower down the tree; reflect the correct value.,0
Support: Android <=2.3 only,0
Create target properties,0
Support: Safari <=6 - 7 only,0
"Target should not be a text node (#504, #13143)",0
Event type,0
Put explicitly provided properties onto the event object,0
Create a timestamp if incoming event doesn't have one,0
Mark it as fixed,0
jQuery.Event is based on DOM3 Events as specified by the ECMAScript Language Binding,0
https://www.w3.org/TR/2003/WD-DOM-Level-3-Events-20030331/ecma-script-binding.html,0
Includes all common event props including KeyEvent and MouseEvent specific props,0
Add which for key events,0
Add which for click: 1 === left; 2 === middle; 3 === right,0
Utilize native event if possible so blur/focus sequence is correct,0
Claim the first handler,0
"dataPriv.set( this, ""focus"", ... )",0
"dataPriv.set( this, ""blur"", ... )",0
Return false to allow normal processing in the caller,0
Force setup before trigger,0
Return non-false to allow normal event-path propagation,0
Create mouseenter/leave events using mouseover/out and event-time checks,0
so that event delegation works in jQuery.,0
Do the same for pointerenter/pointerleave and pointerover/pointerout,0
,0
Support: Safari 7 only,0
Safari sends mouseenter too often; see:,0
https://bugs.chromium.org/p/chromium/issues/detail?id=470258,0
for the description of the bug (it existed in older Chrome versions as well).,0
For mouseenter/leave call the handler if related is outside the target.,0
NB: No relatedTarget if the mouse left/entered the browser window,0
( event )  dispatched jQuery.Event,0
"( types-object [, selector] )",0
"( types [, fn] )",0
See https://github.com/eslint/eslint/issues/3229,0
"Support: IE <=10 - 11, Edge 12 - 13 only",0
In IE/Edge using regex groups here causes severe slowdowns.,0
See https://connect.microsoft.com/IE/feedback/details/1736512/,0
"checked=""checked"" or checked",0
Prefer a tbody over its parent table for containing new rows,0
Replace/restore the type attribute of script elements for safe DOM manipulation,0
"1. Copy private data: events, handlers, etc.",0
2. Copy user data,0
"Fix IE bugs, see support tests",0
Fails to persist the checked state of a cloned checkbox or radio button.,0
Fails to return the selected option to the default selected state when cloning options,0
Flatten any nested arrays,0
"We can't cloneNode fragments that contain checked, in WebKit",0
Require either new content or an interest in ignored elements to invoke the callback,0
Use the original fragment for the last item,0
instead of the first because it can end up,0
being emptied incorrectly in certain situations (#8070).,0
Keep references to cloned scripts for later restoration,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
"push.apply(_, arraylike) throws on ancient WebKit",0
Reenable scripts,0
Evaluate executable scripts on first document insertion,0
"Optional AJAX dependency, but won't run scripts if not present",0
Fix IE cloning issues,0
We eschew Sizzle here for performance reasons: https://jsperf.com/getall-vs-sizzle/2,0
Copy the events from the original to the clone,0
Preserve script evaluation history,0
Return the cloned set,0
This is a shortcut to avoid jQuery.event.remove's overhead,0
Support: Chrome <=35 - 45+,0
"Assign undefined instead of using delete, see Data#remove",0
Support: Chrome <=35 - 45+,0
"Assign undefined instead of using delete, see Data#remove",0
Prevent memory leaks,0
Remove any remaining nodes,0
See if we can take a shortcut and just use innerHTML,0
Remove element nodes and prevent memory leaks,0
"If using innerHTML throws an exception, use the fallback method",0
"Make the changes, replacing each non-ignored context element with the new content",0
Force callback invocation,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
".get() because push.apply(_, arraylike) throws on ancient WebKit",0
"Support: IE <=11 only, Firefox <=30 (#15098, #14150)",0
IE throws on elements created in popups,0
"FF meanwhile throws on frame elements through ""defaultView.getComputedStyle""",0
Executing both pixelPosition & boxSizingReliable tests require only one layout,0
so they're executed at the same time to save the second computation.,0
"This is a singleton, we need to execute it only once",0
"Support: Android 4.0 - 4.3 only, Firefox <=3 - 44",0
"Support: Android 4.0 - 4.3 only, Safari <=9.1 - 10.1, iOS <=7.0 - 9.3",0
"Some styles come back with percentage values, even though they shouldn't",0
Support: IE 9 - 11 only,0
Detect misreporting of content dimensions for box-sizing:border-box elements,0
Support: IE 9 only,0
Detect overflow:scroll screwiness (gh-3699),0
Support: Chrome <=64,0
Don't get tricked when zoom affects offsetWidth (gh-4029),0
Nullify the div so it wouldn't be stored in the memory and,0
it will also be a sign that checks already performed,0
Finish early in limited (non-browser) environments,0
Support: IE <=9 - 11 only,0
Style of cloned element affects source element cloned (#8908),0
Support: Firefox 51+,0
Retrieving style before computed somehow,0
fixes an issue with getting wrong values,0
on detached elements,0
getPropertyValue is needed for:,0
".css('filter') (IE 9 only, #12537)",0
.css('--customProperty) (#3144),0
"A tribute to the ""awesome hack by Dean Edwards""",1
"Android Browser returns percentage for some values,",0
but width seems to be reliably pixels.,0
This is against the CSSOM draft spec:,0
https://drafts.csswg.org/cssom/#resolved-values,0
Remember the original values,0
Put in the new values to get a computed value out,0
Revert the changed values,0
Support: IE <=9 - 11 only,0
IE returns zIndex value as an integer.,0
"Define the hook, we'll check on the first run if it's really needed.",0
Hook not needed (or it's not possible to use it due,0
"to missing dependency), remove it.",0
Hook needed; redefine it so that the support test is not executed again.,0
Return a vendor-prefixed property or undefined,0
Check for vendor prefixed names,0
Return a potentially-mapped jQuery.cssProps or vendor prefixed property,0
Swappable if display is none or starts with table,0
"except ""table"", ""table-cell"", or ""table-caption""",0
See here for display values: https://developer.mozilla.org/en-US/docs/CSS/display,0
Any relative (+/-) values have already been,0
normalized at this point,0
"Guard against undefined ""subtract"", e.g., when used as in cssHooks",0
Adjustment may not be necessary,0
Both box models exclude margin,0
"If we get here with a content-box, we're seeking ""padding"" or ""border"" or ""margin""",0
Add padding,0
"For ""border"" or ""margin"", add border",0
But still keep track of it otherwise,0
"If we get here with a border-box (content + padding + border), we're seeking ""content"" or",0
"""padding"" or ""margin""",0
"For ""content"", subtract padding",0
"For ""content"" or ""padding"", subtract border",0
Account for positive content-box scroll gutter when requested by providing computedVal,0
"offsetWidth/offsetHeight is a rounded sum of content, padding, scroll gutter, and border",0
"Assuming integer scroll gutter, subtract the rest and round down",0
"If offsetWidth/offsetHeight is unknown, then we can't determine content-box scroll gutter",0
Use an explicit zero to avoid NaN (gh-3964),0
Start with computed style,0
"To avoid forcing a reflow, only fetch boxSizing if we need it (gh-4322).",0
Fake content-box until we know it's needed to know the true value.,0
Support: Firefox <=54,0
"Return a confounding non-pixel value or feign ignorance, as appropriate.",0
"Fall back to offsetWidth/offsetHeight when value is ""auto""",0
This happens for inline elements with no explicit setting (gh-3571),0
Support: Android <=4.1 - 4.3 only,0
Also use offsetWidth/offsetHeight for misreported inline dimensions (gh-3602),0
Support: IE 9-11 only,0
Also use offsetWidth/offsetHeight for when box sizing is unreliable,0
We use getClientRects() to check for hidden/disconnected.,0
"In those cases, the computed value can be trusted to be border-box",0
"Where available, offsetWidth/offsetHeight approximate border box dimensions.",0
"Where not available (e.g., SVG), assume unreliable box-sizing and interpret the",0
retrieved value as a content box dimension.,0
"Normalize """" and auto",0
Adjust for the element's box model,0
Provide the current computed size to request scroll gutter calculation (gh-3589),0
Add in style property hooks for overriding the default,0
behavior of getting and setting a style property,0
We should always get a number back from opacity,0
"Don't automatically add ""px"" to these possibly-unitless properties",0
Add in properties whose names you wish to fix before,0
setting or getting the value,0
Get and set the style property on a DOM Node,0
Don't set styles on text and comment nodes,0
Make sure that we're working with the right name,0
Make sure that we're working with the right name. We don't,0
want to query the value if it is a CSS custom property,0
since they are user-defined.,0
"Gets hook for the prefixed version, then unprefixed version",0
Check if we're setting a value,0
"Convert ""+="" or ""-="" to relative numbers (#7345)",0
Fixes bug #9237,0
Make sure that null and NaN values aren't set (#7116),0
"If a number was passed in, add the unit (except for certain CSS properties)",0
The isCustomProp check can be removed in jQuery 4.0 when we only auto-append,0
"""px"" to a few hardcoded values.",0
background-* props affect original clone's values,0
"If a hook was provided, use that value, otherwise just set the specified value",0
If a hook was provided get the non-computed value from there,0
Otherwise just get the value from the style object,0
Make sure that we're working with the right name. We don't,0
want to modify the value if it is a CSS custom property,0
since they are user-defined.,0
Try prefixed name followed by the unprefixed name,0
If a hook was provided get the computed value from there,0
"Otherwise, if a way to get the computed value exists, use that",0
"Convert ""normal"" to computed value",0
Make numeric if forced or a qualifier was provided and val looks numeric,0
Certain elements can have dimension info if we invisibly show them,0
but it must have a current display style that would benefit,0
Support: Safari 8+,0
Table columns in Safari have non-zero offsetWidth & zero,0
getBoundingClientRect().width unless display is changed.,0
Support: IE <=11 only,0
Running getBoundingClientRect on a disconnected node,0
in IE throws an error.,0
Only read styles.position if the test has a chance to fail,0
to avoid forcing a reflow.,0
"To avoid forcing a reflow, only fetch boxSizing if we need it (gh-3991)",0
Account for unreliable border-box dimensions by comparing offset* to computed and,0
faking a content-box to get border and padding (gh-3699),0
Convert to pixels if value adjustment is needed,0
These hooks are used by animate to expand properties,0
Assumes a single number if not a string,0
"Based off of the plugin by Clint Helfers, with permission.",0
https://web.archive.org/web/20100324014747/http://blindsignals.com/index.php/2009/07/jquery-delay/,0
Support: Android <=4.3 only,0
"Default value for a checkbox should be ""on""",0
Support: IE <=11 only,0
Must access selectedIndex to make default options select,0
Support: IE <=11 only,0
An input loses its value after becoming a radio,0
"Don't get/set attributes on text, comment and attribute nodes",0
Fallback to prop when attributes are not supported,0
Attribute hooks are determined by the lowercase version,0
Grab necessary hook if one is defined,0
"Non-existent attributes return null, we normalize to undefined",0
Attribute names can contain non-HTML whitespace characters,0
https://html.spec.whatwg.org/multipage/syntax.html#attributes-2,0
Hooks for boolean attributes,0
Remove boolean attributes when set to false,0
Avoid an infinite loop by temporarily removing this function from the getter,0
"Don't get/set properties on text, comment and attribute nodes",0
Fix name and attach hooks,0
Support: IE <=9 - 11 only,0
elem.tabIndex doesn't always return the,0
correct value when it hasn't been explicitly set,0
https://web.archive.org/web/20141116233347/http://fluidproject.org/blog/2008/01/09/getting-setting-and-removing-tabindex-values-with-javascript/,0
Use proper attribute retrieval(#12072),0
Support: IE <=11 only,0
Accessing the selectedIndex property,0
forces the browser to respect setting selected,0
on the option,0
The getter ensures a default option is selected,0
when in an optgroup,0
"eslint rule ""no-unused-expressions"" is disabled for this code",0
since it considers such accessions noop,0
Strip and collapse whitespace according to HTML spec,0
https://infra.spec.whatwg.org/#strip-and-collapse-ascii-whitespace,0
Only assign if different to avoid unneeded rendering.,0
This expression is here for better compressibility (see addClass),0
Remove *all* instances,0
Only assign if different to avoid unneeded rendering.,0
Toggle individual class names,0
"Check each className given, space separated list",0
Toggle whole class name,0
Store className if set,0
"If the element has a class name or if we're passed `false`,",0
"then remove the whole classname (if there was one, the above saved it).",0
"Otherwise bring back whatever was previously saved (if anything),",0
falling back to the empty string if nothing was stored.,0
Handle most common string cases,0
Handle cases where value is null/undef or number,0
"Treat null/undefined as """"; convert numbers to string",0
"If set returns undefined, fall back to normal setting",0
Support: IE <=10 - 11 only,0
"option.text throws exceptions (#14686, #14858)",0
Strip and collapse whitespace,0
https://html.spec.whatwg.org/#strip-and-collapse-whitespace,0
Loop through all the selected options,0
Support: IE <=9 only,0
IE8-9 doesn't update selected after form reset (#2551),0
Don't return options that are disabled or in a disabled optgroup,0
Get the specific value for the option,0
We don't need an array for one selects,0
Multi-Selects return an array,0
Force browsers to behave consistently when non-matching value is set,0
Radios and checkboxes getter/setter,0
Return jQuery for attributes-only inclusion,0
Don't do events on text and comment nodes,0
focus/blur morphs to focusin/out; ensure we're not firing them right now,0
Namespaced trigger; create a regexp to match event type in handle(),0
"Caller can pass in a jQuery.Event object, Object, or just an event type string",0
Trigger bitmask: & 1 for native handlers; & 2 for jQuery (always true),0
Clean up the event in case it is being reused,0
"Clone any incoming data and prepend the event, creating the handler arg list",0
Allow special events to draw outside the lines,0
"Determine event propagation path in advance, per W3C events spec (#9951)",0
"Bubble up to document, then to window; watch for a global ownerDocument var (#9724)",0
"Only add window if we got to document (e.g., not plain obj or detached DOM)",0
Fire handlers on the event path,0
jQuery handler,0
Native handler,0
"If nobody prevented the default action, do it now",0
Call a native DOM method on the target with the same name as the event.,0
"Don't do default actions on window, that's where global variables be (#6170)",0
Don't re-trigger an onFOO event when we call its FOO() method,0
"Prevent re-triggering of the same event, since we already bubbled it above",0
Piggyback on a donor event to simulate a different one,0
Used only for `focus(in | out)` events,0
Support: Firefox <=44,0
Firefox doesn't have focus(in | out) events,0
Related ticket - https://bugzilla.mozilla.org/show_bug.cgi?id=687787,0
,0
"Support: Chrome <=48 - 49, Safari <=9.0 - 9.1",0
"focus(in | out) events fire after focus & blur events,",0
which is spec violation - http://www.w3.org/TR/DOM-Level-3-Events/#events-focusevent-event-order,0
Related ticket - https://bugs.chromium.org/p/chromium/issues/detail?id=449857,0
Attach a single capturing handler on the document while someone wants focusin/focusout,0
Serialize array item.,0
Treat each array item as a scalar.,0
"Item is non-scalar (array or object), encode its numeric index.",0
Serialize object item.,0
Serialize scalar item.,0
Serialize an array of form elements or a set of,0
key/values into a query string,0
"If value is a function, invoke it and use its return value",0
"If an array was passed in, assume that it is an array of form elements.",0
Serialize the form elements,0
"If traditional, encode the ""old"" way (the way 1.3.2 or older",0
"did it), otherwise encode params recursively.",0
Return the resulting serialization,0
"Can add propHook for ""elements"" to filter or add form elements",0
"Use .is( "":disabled"" ) so that fieldset[disabled] works",0
The elements to wrap the target around,0
Support: Safari 8 only,0
In Safari 8 documents created via document.implementation.createHTMLDocument,0
collapse sibling forms: the second one becomes a child of the first one.,0
"Because of that, this security measure has to be disabled in Safari 8.",0
https://bugs.webkit.org/show_bug.cgi?id=137337,0
"Argument ""data"" should be string of html",0
"context (optional): If specified, the fragment will be created in this context,",0
defaults to document,0
"keepScripts (optional): If true, will include scripts passed in the html string",0
Stop scripts or inline event handlers from being executed immediately,0
by using document.implementation,0
Set the base href for the created document,0
so any parsed elements with URLs,0
are based on the document's URL (gh-2965),0
Single tag,0
"Set position first, in-case top/left are set even on static elem",0
Need to be able to calculate position if either,0
top or left is auto and position is either absolute or fixed,0
Use jQuery.extend here to allow modification of coordinates argument (gh-1848),0
offset() relates an element's border box to the document origin,0
Preserve chaining for setter,0
Return zeros for disconnected and hidden (display: none) elements (gh-2310),0
Support: IE <=11 only,0
Running getBoundingClientRect on a,0
disconnected node in IE throws an error,0
Get document-relative position by adding viewport scroll to viewport-relative gBCR,0
position() relates an element's margin box to its offset parent's padding box,0
This corresponds to the behavior of CSS absolute positioning,0
"position:fixed elements are offset from the viewport, which itself always has zero offset",0
Assume position:fixed implies availability of getBoundingClientRect,0
"Account for the *real* offset parent, which can be the document or its root element",0
when a statically positioned element is identified,0
"Incorporate borders into its offset, since they are outside its content origin",0
Subtract parent offsets and element margins,0
This method will return documentElement in the following cases:,0
"1) For the element inside the iframe without offsetParent, this method will return",0
documentElement of the parent window,0
2) For the hidden or detached element,0
"3) For body or html element, i.e. in case of the html node - it will return itself",0
,0
but those exceptions were never presented as a real life use-cases,0
and might be considered as more preferable results.,0
,0
"This logic, however, is not guaranteed and can change at any point in the future",0
Create scrollLeft and scrollTop methods,0
Coalesce documents and windows,0
"Support: Safari <=7 - 9.1, Chrome <=37 - 49",0
Add the top/left cssHooks using jQuery.fn.position,0
Webkit bug: https://bugs.webkit.org/show_bug.cgi?id=29084,0
Blink bug: https://bugs.chromium.org/p/chromium/issues/detail?id=589347,0
getComputedStyle returns percent when specified for top/left/bottom/right;,0
"rather than make the css module depend on the offset module, just check for it here",0
"If curCSS returns percentage, fallback to offset",0
"Create innerHeight, innerWidth, height, width, outerHeight and outerWidth methods",0
"Margin is only for outerHeight, outerWidth",0
$( window ).outerWidth/Height return w/h including scrollbars (gh-1729),0
Get document width or height,0
"Either scroll[Width/Height] or offset[Width/Height] or client[Width/Height],",0
whichever is greatest,0
"Get width or height on the element, requesting but not forcing parseFloat",0
Set width or height on the element,0
Handle event binding,0
"( namespace ) or ( selector, types [, fn] )",0
"Bind a function to a context, optionally partially applying any",0
arguments.,0
jQuery.proxy is deprecated to promote standards (specifically Function#bind),0
"However, it is not slated for removal any time soon",0
"Quick check to determine if target is callable, in the spec",0
"this throws a TypeError, but we will just return undefined.",0
Simulated bind,0
"Set the guid of unique handler to the same of original handler, so it can be removed",0
"As of jQuery 3.0, isNumeric is limited to",0
strings and numbers (primitives or objects),0
that can be coerced to finite numbers (gh-2662),0
"parseFloat NaNs numeric-cast false positives ("""")",0
"...but misinterprets leading-number strings, particularly hex literals (""0x..."")",0
subtraction forces infinities to NaN,0
"Register as a named AMD module, since jQuery can be concatenated with other",0
"files that may use define, but not via a proper concatenation script that",0
understands anonymous AMD modules. A named AMD is safest and most robust,0
way to register. Lowercase jquery is used because AMD module names are,0
"derived from file names, and jQuery is normally delivered in a lowercase",0
file name. Do this after creating the global so that if an AMD module wants,0
"to call noConflict to hide this version of jQuery, it will work.",0
"Note that for maximum portability, libraries that are not jQuery should",0
"declare themselves as anonymous modules, and avoid setting a global if an",0
"AMD loader is present. jQuery is a special case. For more information, see",0
https://github.com/jrburke/requirejs/wiki/Updating-existing-libraries#wiki-anon,0
Map over jQuery in case of overwrite,0
Map over the $ in case of overwrite,0
"Expose jQuery and $ identifiers, even in AMD",0
"(#7102#comment:10, https://github.com/jquery/jquery/pull/557)",0
and CommonJS for browser emulators (#13566),0
Implementation Summary,0
,0
1. Enforce API surface and semantic compatibility with 1.9.x branch,0
2. Improve the module's maintainability by reducing the storage,0
paths to a single mechanism.,0
"3. Use the same single mechanism to support ""private"" and ""user"" data.",0
"4. _Never_ expose ""private"" data to user code (TODO: Drop _data, _removeData)",1
5. Avoid exposing implementation details on user objects (eg. expando properties),0
6. Provide a clear path for implementation upgrade to WeakMap in 2014,0
Only convert to a number if it doesn't change the string,0
"If nothing was found internally, try to fetch any",0
data from the HTML5 data-* attribute,0
Make sure we set the data so it isn't changed later,0
TODO: Now that all calls to _data and _removeData have been replaced,1
"with direct calls to dataPriv methods, these can be deprecated.",0
Gets all values,0
Support: IE 11 only,0
The attrs elements can be null (#14894),0
Sets multiple values,0
The calling jQuery object (element matches) is not empty,0
(and therefore has an element appears at this[ 0 ]) and the,0
`value` parameter was not undefined. An empty jQuery object,0
will result in `undefined` for elem = this[ 0 ] which will,0
throw an exception if an attempt to read a data cache is made.,0
Attempt to get data from the cache,0
The key will always be camelCased in Data,0
"Attempt to ""discover"" the data in",0
HTML5 custom data-* attrs,0
"We tried really hard, but the data doesn't exist.",0
Set the data...,0
We always store the camelCased key,0
Convert String-formatted options into Object-formatted ones,0
Convert options from String-formatted to Object-formatted if needed,0
(we check in cache first),0
Last fire value for non-forgettable lists,0
Flag to know if list was already fired,0
Flag to prevent firing,0
Actual callback list,0
Queue of execution data for repeatable lists,0
Index of currently firing callback (modified by add/remove as needed),0
Fire callbacks,0
Enforce single-firing,0
"Execute callbacks for all pending executions,",0
respecting firingIndex overrides and runtime changes,0
Run callback and check for early termination,0
Jump to end and forget the data so .add doesn't re-fire,0
Forget the data if we're done with it,0
Clean up if we're done firing for good,0
Keep an empty list if we have data for future add calls,0
"Otherwise, this object is spent",0
Actual Callbacks object,0
Add a callback or a collection of callbacks to the list,0
"If we have memory from a past run, we should fire after adding",0
Inspect recursively,0
Remove a callback from the list,0
Handle firing indexes,0
Check if a given callback is in the list.,0
"If no argument is given, return whether or not list has callbacks attached.",0
Remove all callbacks from the list,0
Disable .fire and .add,0
Abort any current/pending executions,0
Clear all callbacks and values,0
Disable .fire,0
Also disable .add unless we have memory (since it would have no effect),0
Abort any pending executions,0
Call all callbacks with the given context and arguments,0
Call all the callbacks with the given arguments,0
To know if the callbacks have already been called at least once,0
Support: IE <=9 - 11+,0
"focus() and blur() are asynchronous, except when they are no-op.",0
"So expect focus to be synchronous when the element is already active,",0
and blur to be synchronous when the element is not already active.,0
"(focus and blur are always synchronous in other supported browsers,",0
this just defines when we can count on it).,0
Support: IE <=9 only,0
Accessing document.activeElement can throw unexpectedly,0
https://bugs.jquery.com/ticket/13393,0
Types can be a map of types/handlers,0
"( types-Object, selector, data )",0
"( types-Object, data )",0
"( types, fn )",0
"( types, selector, fn )",0
"( types, data, fn )",0
"Can use an empty set, since event contains the info",0
Use same guid so caller can remove using origFn,0
Don't attach events to noData or text/comment nodes (but allow plain objects),0
Caller can pass in an object of custom data in lieu of the handler,0
Ensure that invalid selectors throw exceptions at attach time,0
"Evaluate against documentElement in case elem is a non-element node (e.g., document)",0
"Make sure that the handler has a unique ID, used to find/remove it later",0
"Init the element's event structure and main handler, if this is the first",0
Discard the second event of a jQuery.event.trigger() and,0
when an event is called after a page has unloaded,0
Handle multiple events separated by a space,0
"There *must* be a type, no attaching namespace-only handlers",0
"If event changes its type, use the special event handlers for the changed type",0
"If selector defined, determine special event api type, otherwise given type",0
Update special based on newly reset type,0
handleObj is passed to all event handlers,0
Init the event handler queue if we're the first,0
Only use addEventListener if the special events handler returns false,0
"Add to the element's handler list, delegates in front",0
"Keep track of which events have ever been used, for event optimization",0
Detach an event or set of events from an element,0
Once for each type.namespace in types; type may be omitted,0
"Unbind all events (on this namespace, if provided) for the element",0
Remove matching events,0
Remove generic event handler if we removed something and no more handlers exist,0
(avoids potential for endless recursion during removal of special event handlers),0
Remove data and the expando if it's no longer used,0
Make a writable jQuery.Event from the native event object,0
Use the fix-ed jQuery.Event rather than the (read-only) native event,0
"Call the preDispatch hook for the mapped type, and let it bail if desired",0
Determine handlers,0
Run delegates first; they may want to stop propagation beneath us,0
"If the event is namespaced, then each handler is only invoked if it is",0
specially universal or its namespaces are a superset of the event's.,0
Call the postDispatch hook for the mapped type,0
Find delegate handlers,0
Support: IE <=9,0
Black-hole SVG <use> instance trees (trac-13180),0
Support: Firefox <=42,0
Suppress spec-violating clicks indicating a non-primary pointer button (trac-3861),0
https://www.w3.org/TR/DOM-Level-3-Events/#event-type-click,0
Support: IE 11 only,0
"...but not arrow key ""clicks"" of radio inputs, which can have `button` -1 (gh-2343)",0
Don't check non-elements (#13208),0
"Don't process clicks on disabled elements (#6911, #8165, #11382, #11764)",0
Don't conflict with Object.prototype properties (#13203),0
Add the remaining (directly-bound) handlers,0
Prevent triggered image.load events from bubbling to window.load,0
Utilize native event to ensure correct state for checkable inputs,0
"For mutual compressibility with _default, replace `this` access with a local var.",0
`|| data` is dead code meant only to preserve the variable through minification.,0
Claim the first handler,0
"dataPriv.set( el, ""click"", ... )",0
Return false to allow normal processing in the caller,0
"For mutual compressibility with _default, replace `this` access with a local var.",0
`|| data` is dead code meant only to preserve the variable through minification.,0
Force setup before triggering a click,0
Return non-false to allow normal event-path propagation,0
"For cross-browser consistency, suppress native .click() on links",0
Also prevent it if we're currently inside a leveraged native-event stack,0
Support: Firefox 20+,0
Firefox doesn't alert if the returnValue field is not set.,0
Ensure the presence of an event listener that handles manually-triggered,0
synthetic events by interrupting progress until reinvoked in response to,0
"*native* events that it fires directly, ensuring that state changes have",0
already occurred before other listeners are invoked.,0
"Missing expectSync indicates a trigger call, which must force setup through jQuery.event.add",0
Register the controller as a special universal handler for all event namespaces,0
Interrupt processing of the outer synthetic .trigger()ed event,0
Store arguments for use when handling the inner native event,0
Trigger the native event and capture its result,0
Support: IE <=9 - 11+,0
focus() and blur() are asynchronous,0
Cancel the outer synthetic event,0
If this is an inner synthetic event for an event with a bubbling surrogate,0
"(focus or blur), assume that the surrogate already propagated from triggering the",0
native event and prevent that from happening again here.,0
This technically gets the ordering wrong w.r.t. to `.trigger()` (in which the,0
"bubbling surrogate propagates *after* the non-bubbling base), but that seems",0
less bad than duplication.,0
"If this is a native event triggered above, everything is now in order",0
Fire an inner synthetic event with the original arguments,0
...and capture the result,0
Support: IE <=9 - 11+,0
Extend with the prototype to reset the above stopImmediatePropagation(),0
Abort handling of the native event,0
"This ""if"" is needed for plain objects",0
Allow instantiation without the 'new' keyword,0
Event object,0
Events bubbling up the document may have been marked as prevented,0
by a handler lower down the tree; reflect the correct value.,0
Support: Android <=2.3 only,0
Create target properties,0
Support: Safari <=6 - 7 only,0
"Target should not be a text node (#504, #13143)",0
Event type,0
Put explicitly provided properties onto the event object,0
Create a timestamp if incoming event doesn't have one,0
Mark it as fixed,0
jQuery.Event is based on DOM3 Events as specified by the ECMAScript Language Binding,0
https://www.w3.org/TR/2003/WD-DOM-Level-3-Events-20030331/ecma-script-binding.html,0
Includes all common event props including KeyEvent and MouseEvent specific props,0
Add which for key events,0
Add which for click: 1 === left; 2 === middle; 3 === right,0
Utilize native event if possible so blur/focus sequence is correct,0
Claim the first handler,0
"dataPriv.set( this, ""focus"", ... )",0
"dataPriv.set( this, ""blur"", ... )",0
Return false to allow normal processing in the caller,0
Force setup before trigger,0
Return non-false to allow normal event-path propagation,0
Create mouseenter/leave events using mouseover/out and event-time checks,0
so that event delegation works in jQuery.,0
Do the same for pointerenter/pointerleave and pointerover/pointerout,0
,0
Support: Safari 7 only,0
Safari sends mouseenter too often; see:,0
https://bugs.chromium.org/p/chromium/issues/detail?id=470258,0
for the description of the bug (it existed in older Chrome versions as well).,0
For mouseenter/leave call the handler if related is outside the target.,0
NB: No relatedTarget if the mouse left/entered the browser window,0
( event )  dispatched jQuery.Event,0
"( types-object [, selector] )",0
"( types [, fn] )",0
Defining this global in .eslintrc.json would create a danger of using the global,0
"unguarded in another place, it seems safer to define global only for this module",0
Define a local copy of jQuery,0
The jQuery object is actually just the init constructor 'enhanced',0
Need init if jQuery is called (just allow error to be thrown if not included),0
Support: Android <=4.0 only,0
Make sure we trim BOM and NBSP,0
The current version of jQuery being used,0
The default length of a jQuery object is 0,0
Get the Nth element in the matched element set OR,0
Get the whole matched element set as a clean array,0
Return all the elements in a clean array,0
Return just the one element from the set,0
Take an array of elements and push it onto the stack,0
(returning the new matched element set),0
Build a new jQuery matched element set,0
Add the old object onto the stack (as a reference),0
Return the newly-formed element set,0
Execute a callback for every element in the matched set.,0
For internal use only.,0
"Behaves like an Array's method, not like a jQuery method.",0
Handle a deep copy situation,0
Skip the boolean and the target,0
Handle case when target is a string or something (possible in deep copy),0
Extend jQuery itself if only one argument is passed,0
Only deal with non-null/undefined values,0
Extend the base object,0
Prevent Object.prototype pollution,0
Prevent never-ending loop,0
Recurse if we're merging plain objects or arrays,0
Ensure proper type for the source value,0
"Never move original objects, clone them",0
Don't bring in undefined values,0
Return the modified object,0
Unique for each copy of jQuery on the page,0
Assume jQuery is ready without the ready module,0
Detect obvious negatives,0
Use toString instead of jQuery.type to catch host objects,0
"Objects with no prototype (e.g., `Object.create( null )`) are plain",0
Objects with prototype are plain iff they were constructed by a global Object function,0
Evaluates a script in a global context,0
Support: Android <=4.0 only,0
results is for internal usage only,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
"push.apply(_, arraylike) throws on ancient WebKit",0
"Go through the array, only saving the items",0
that pass the validator function,0
arg is for internal usage only,0
"Go through the array, translating each of the items to their new values",0
"Go through every key on the object,",0
Flatten any nested arrays,0
A global GUID counter for objects,0
jQuery.support is not used in Core but other projects attach their,0
properties to it so it needs to exist.,0
Populate the class2type map,0
Support: real iOS 8.2 only (not reproducible in simulator),0
`in` check used to prevent JIT error (gh-2145),0
hasOwn isn't used here due to false negatives,0
regarding Nodelist length in IE,0
"Create innerHeight, innerWidth, height, width, outerHeight and outerWidth methods",0
"Margin is only for outerHeight, outerWidth",0
$( window ).outerWidth/Height return w/h including scrollbars (gh-1729),0
Get document width or height,0
"Either scroll[Width/Height] or offset[Width/Height] or client[Width/Height],",0
whichever is greatest,0
"Get width or height on the element, requesting but not forcing parseFloat",0
Set width or height on the element,0
Methods guaranteed to produce a unique set when starting from a unique set,0
"Positional selectors never match, since there's no _selection_ context",0
Always skip document fragments,0
Don't pass non-elements to Sizzle,0
Determine the position of an element within the set,0
"No argument, return index in parent",0
Index in selector,0
Locate the position of the desired element,0
"If it receives a jQuery object, the first element is used",0
"Support: IE 9 - 11 only, iOS 7 only, Android Browser <=4.3 only",0
Treat the template element as a regular one in browsers that,0
don't support it.,0
Remove duplicates,0
Reverse order for parents* and prev-derivatives,0
Return jQuery for attributes-only inclusion,0
Speed up dequeue by getting out quickly if this is just a lookup,0
"If the fx queue is dequeued, always remove the progress sentinel",0
Add a progress sentinel to prevent the fx queue from being,0
automatically dequeued,0
Clear up the last queue stop function,0
"Not public - generate a queueHooks object, or return the current one",0
Ensure a hooks for this queue,0
Get a promise resolved when queues of a certain type,0
are emptied (fx is the type by default),0
The elements to wrap the target around,0
Serialize array item.,0
Treat each array item as a scalar.,0
"Item is non-scalar (array or object), encode its numeric index.",0
Serialize object item.,0
Serialize scalar item.,0
Serialize an array of form elements or a set of,0
key/values into a query string,0
"If value is a function, invoke it and use its return value",0
"If an array was passed in, assume that it is an array of form elements.",0
Serialize the form elements,0
"If traditional, encode the ""old"" way (the way 1.3.2 or older",0
"did it), otherwise encode params recursively.",0
Return the resulting serialization,0
"Can add propHook for ""elements"" to filter or add form elements",0
"Use .is( "":disabled"" ) so that fieldset[disabled] works",0
"Set position first, in-case top/left are set even on static elem",0
Need to be able to calculate position if either,0
top or left is auto and position is either absolute or fixed,0
Use jQuery.extend here to allow modification of coordinates argument (gh-1848),0
offset() relates an element's border box to the document origin,0
Preserve chaining for setter,0
Return zeros for disconnected and hidden (display: none) elements (gh-2310),0
Support: IE <=11 only,0
Running getBoundingClientRect on a,0
disconnected node in IE throws an error,0
Get document-relative position by adding viewport scroll to viewport-relative gBCR,0
position() relates an element's margin box to its offset parent's padding box,0
This corresponds to the behavior of CSS absolute positioning,0
"position:fixed elements are offset from the viewport, which itself always has zero offset",0
Assume position:fixed implies availability of getBoundingClientRect,0
"Account for the *real* offset parent, which can be the document or its root element",0
when a statically positioned element is identified,0
"Incorporate borders into its offset, since they are outside its content origin",0
Subtract parent offsets and element margins,0
This method will return documentElement in the following cases:,0
"1) For the element inside the iframe without offsetParent, this method will return",0
documentElement of the parent window,0
2) For the hidden or detached element,0
"3) For body or html element, i.e. in case of the html node - it will return itself",0
,0
but those exceptions were never presented as a real life use-cases,0
and might be considered as more preferable results.,0
,0
"This logic, however, is not guaranteed and can change at any point in the future",0
Create scrollLeft and scrollTop methods,0
Coalesce documents and windows,0
"Support: Safari <=7 - 9.1, Chrome <=37 - 49",0
Add the top/left cssHooks using jQuery.fn.position,0
Webkit bug: https://bugs.webkit.org/show_bug.cgi?id=29084,0
Blink bug: https://bugs.chromium.org/p/chromium/issues/detail?id=589347,0
getComputedStyle returns percent when specified for top/left/bottom/right;,0
"rather than make the css module depend on the offset module, just check for it here",0
"If curCSS returns percentage, fallback to offset",0
"#7653, #8125, #8152: local protocol detection",0
Avoid comment-prolog char sequence (#10098); must appease lint and evade compression,0
Anchor tag for parsing the document origin,0
"Base ""constructor"" for jQuery.ajaxPrefilter and jQuery.ajaxTransport",0
"dataTypeExpression is optional and defaults to ""*""",0
For each dataType in the dataTypeExpression,0
Prepend if requested,0
Otherwise append,0
Base inspection function for prefilters and transports,0
A special extend for ajax options,0
"that takes ""flat"" options (not to be deep extended)",0
Fixes #9887,0
Remove auto dataType and get content-type in the process,0
Check if we're dealing with a known content-type,0
Check to see if we have a response for the expected dataType,0
Try convertible dataTypes,0
Or just use first one,0
If we found a dataType,0
We add the dataType to the list if needed,0
and return the corresponding response,0
Work with a copy of dataTypes in case we need to modify it for conversion,0
Create converters map with lowercased keys,0
Convert to each sequential dataType,0
Apply the dataFilter if provided,0
There's only work to do if current dataType is non-auto,0
Convert response if prev dataType is non-auto and differs from current,0
Seek a direct converter,0
"If none found, seek a pair",0
If conv2 outputs current,0
If prev can be converted to accepted input,0
Condense equivalence converters,0
"Otherwise, insert the intermediate dataType",0
Apply converter (if not an equivalence),0
"Unless errors are allowed to bubble, catch and return them",0
Counter for holding the number of active queries,0
Last-Modified header cache for next request,0
Data converters,0
"Keys separate source (or catchall ""*"") and destination types with a single space",0
Convert anything to text,0
Text to html (true = no transformation),0
Evaluate text as a json expression,0
Parse text as xml,0
For options that shouldn't be deep extended:,0
you can add your own custom options here if,0
and when you create one that shouldn't be,0
deep extended (see ajaxExtend),0
Creates a full fledged settings object into target,0
with both ajaxSettings and settings fields.,0
"If target is omitted, writes into ajaxSettings.",0
Building a settings object,0
Extending ajaxSettings,0
Main method,0
"If url is an object, simulate pre-1.5 signature",0
Force options to be an object,0
URL without anti-cache param,0
Response headers,0
timeout handle,0
Url cleanup var,0
Request state (becomes false upon send and true upon completion),0
To know if global events are to be dispatched,0
Loop variable,0
uncached part of the url,0
Create the final options object,0
Callbacks context,0
Context for global events is callbackContext if it is a DOM node or jQuery collection,0
Deferreds,0
Status-dependent callbacks,0
Headers (they are sent all at once),0
Default abort message,0
Fake xhr,0
Builds headers hashtable if needed,0
Raw string,0
Caches the header,0
Overrides response content-type header,0
Status-dependent callbacks,0
Execute the appropriate callbacks,0
Lazy-add the new callbacks in a way that preserves old ones,0
Cancel the request,0
Attach deferreds,0
Add protocol if not provided (prefilters might expect it),0
Handle falsy url in the settings object (#10093: consistency with old signature),0
We also use the url parameter if available,0
Alias method option to type as per ticket #12004,0
Extract dataTypes list,0
A cross-domain request is in order when the origin doesn't match the current origin.,0
"Support: IE <=8 - 11, Edge 12 - 15",0
"IE throws exception on accessing the href property if url is malformed,",0
e.g. http://example.com:80x/,0
Support: IE <=8 - 11 only,0
Anchor's host property isn't correctly set when s.url is relative,0
"If there is an error parsing the URL, assume it is crossDomain,",0
it can be rejected by the transport if it is invalid,0
Convert data if not already a string,0
Apply prefilters,0
"If request was aborted inside a prefilter, stop there",0
We can fire global events as of now if asked to,0
Don't fire events if jQuery.event is undefined in an AMD-usage scenario (#15118),0
Watch for a new set of requests,0
Uppercase the type,0
Determine if request has content,0
Save the URL in case we're toying with the If-Modified-Since,0
and/or If-None-Match header later on,0
Remove hash to simplify url manipulation,0
More options handling for requests with no content,0
Remember the hash so we can put it back,0
"If data is available and should be processed, append data to url",0
#9682: remove data so that it's not used in an eventual retry,0
Add or update anti-cache param if needed,0
Put hash and anti-cache on the URL that will be requested (gh-1732),0
Change '%20' to '+' if this is encoded form body content (gh-2658),0
"Set the If-Modified-Since and/or If-None-Match header, if in ifModified mode.",0
"Set the correct header, if data is being sent",0
"Set the Accepts header for the server, depending on the dataType",0
Check for headers option,0
Allow custom headers/mimetypes and early abort,0
Abort if not done already and return,0
Aborting is no longer a cancellation,0
Install callbacks on deferreds,0
Get transport,0
"If no transport, we auto-abort",0
Send global event,0
"If request was aborted inside ajaxSend, stop there",0
Timeout,0
Rethrow post-completion exceptions,0
Propagate others as results,0
Callback for when everything is done,0
Ignore repeat invocations,0
Clear timeout if it exists,0
Dereference transport for early garbage collection,0
(no matter how long the jqXHR object will be used),0
Cache response headers,0
Set readyState,0
Determine if successful,0
Get response data,0
Convert no matter what (that way responseXXX fields are always set),0
"If successful, handle type chaining",0
"Set the If-Modified-Since and/or If-None-Match header, if in ifModified mode.",0
if no content,0
if not modified,0
"If we have data, let's convert it",0
Extract error from statusText and normalize for non-aborts,0
Set data for the fake xhr object,0
Success/Error,0
Status-dependent callbacks,0
Complete,0
Handle the global AJAX counter,0
Shift arguments if data argument was omitted,0
The url can be an options object (which then must have .url),0
"( namespace ) or ( selector, types [, fn] )",0
"Bind a function to a context, optionally partially applying any",0
arguments.,0
jQuery.proxy is deprecated to promote standards (specifically Function#bind),0
"However, it is not slated for removal any time soon",0
"Quick check to determine if target is callable, in the spec",0
"this throws a TypeError, but we will just return undefined.",0
Simulated bind,0
"Set the guid of unique handler to the same of original handler, so it can be removed",0
"As of jQuery 3.0, isNumeric is limited to",0
strings and numbers (primitives or objects),0
that can be coerced to finite numbers (gh-2662),0
"parseFloat NaNs numeric-cast false positives ("""")",0
"...but misinterprets leading-number strings, particularly hex literals (""0x..."")",0
subtraction forces infinities to NaN,0
CSS string/identifier serialization,0
https://drafts.csswg.org/cssom/#common-serializing-idioms,0
U+0000 NULL becomes U+FFFD REPLACEMENT CHARACTER,0
Control characters and (dependent upon position) numbers get escaped as code points,0
Other potentially-special ASCII characters get backslash-escaped,0
Flag for duplicate removal,0
Sort on method existence if only one input has compareDocumentPosition,0
Calculate position if both inputs belong to the same document,0
Otherwise we know they are disconnected,0
Disconnected nodes,0
Choose the first element that is related to our preferred document,0
Maintain original order,0
Clear input after sorting to release objects,0
See https://github.com/jquery/sizzle/pull/225,0
Same basic safeguard as Sizzle,0
Early return if context is not an element or document,0
"If no nodeType, this is expected to be an array",0
Do not traverse comment nodes,0
Use textContent for elements,0
Do not include comment or processing instruction nodes,0
documentElement is verified for cases where it doesn't yet exist,0
(such as loading iframes in IE - #4833),0
Don't get fooled by Object.prototype properties (jQuery #13807),0
Deprecated,0
Animations created synchronously will run synchronously,0
Generate parameters to create a standard animation,0
"If we include width, step value is 1 to do all cssExpand values,",0
otherwise step value is 2 to skip over Left and Right,0
We're done with this property,0
Queue-skipping animations hijack the fx hooks,0
Ensure the complete handler is called before this completes,0
Detect show/hide animations,0
"Pretend to be hidden if this is a ""show"" and",0
there is still data from a stopped show/hide,0
Ignore all other no-op show/hide data,0
Bail out if this is a no-op like .hide().hide(),0
"Restrict ""overflow"" and ""display"" styles during box animations",0
"Support: IE <=9 - 11, Edge 12 - 15",0
Record all 3 overflow attributes because IE does not infer the shorthand,0
from identically-valued overflowX and overflowY and Edge just mirrors,0
the overflowX value there.,0
"Identify a display type, preferring old show/hide data over the CSS cascade",0
Get nonempty value(s) by temporarily forcing visibility,0
Animate inline elements as inline-block,0
Restore the original display value at the end of pure show/hide animations,0
Implement show/hide animations,0
General show/hide setup for this element animation,0
"Store hidden/visible for toggle so `.stop().toggle()` ""reverses""",0
Show elements before animating them,0
"The final step of a ""hide"" animation is actually hiding the element",0
Per-property setup,0
"camelCase, specialEasing and expand cssHook pass",0
"Not quite $.extend, this won't overwrite existing keys.",0
"Reusing 'index' because we have the correct ""name""",0
Don't match elem in the :animated selector,0
Support: Android 2.3 only,0
Archaic crash bug won't allow us to use `1 - ( 0.5 || 0 )` (#12497),0
"If there's more to do, yield",0
"If this was an empty animation, synthesize a final progress notification",0
Resolve the animation and report its conclusion,0
"If we are going to the end, we want to run all the tweens",0
otherwise we skip this part,0
"Resolve when we played the last frame; otherwise, reject",0
Attach callbacks from options,0
Go to the end state if fx are off,0
"Normalize opt.queue - true/undefined/null -> ""fx""",0
Queueing,0
Show any hidden elements after setting opacity to 0,0
Animate to the value specified,0
Operate on a copy of prop so per-property easing won't be lost,0
"Empty animations, or finishing resolves immediately",0
Start the next in the queue if the last step wasn't forced.,0
"Timers currently will call their complete callbacks, which",0
will dequeue but only if they were gotoEnd.,0
Enable finishing flag on private data,0
Empty the queue first,0
"Look for any active animations, and finish them",0
Look for any animations in the old queue and finish them,0
Turn off finishing flag,0
Generate shortcuts for custom animations,0
Run the timer and safely remove it when done (allowing for external removal),0
Default speed,0
Swappable if display is none or starts with table,0
"except ""table"", ""table-cell"", or ""table-caption""",0
See here for display values: https://developer.mozilla.org/en-US/docs/CSS/display,0
Any relative (+/-) values have already been,0
normalized at this point,0
"Guard against undefined ""subtract"", e.g., when used as in cssHooks",0
Adjustment may not be necessary,0
Both box models exclude margin,0
"If we get here with a content-box, we're seeking ""padding"" or ""border"" or ""margin""",0
Add padding,0
"For ""border"" or ""margin"", add border",0
But still keep track of it otherwise,0
"If we get here with a border-box (content + padding + border), we're seeking ""content"" or",0
"""padding"" or ""margin""",0
"For ""content"", subtract padding",0
"For ""content"" or ""padding"", subtract border",0
Account for positive content-box scroll gutter when requested by providing computedVal,0
"offsetWidth/offsetHeight is a rounded sum of content, padding, scroll gutter, and border",0
"Assuming integer scroll gutter, subtract the rest and round down",0
"If offsetWidth/offsetHeight is unknown, then we can't determine content-box scroll gutter",0
Use an explicit zero to avoid NaN (gh-3964),0
Start with computed style,0
"To avoid forcing a reflow, only fetch boxSizing if we need it (gh-4322).",0
Fake content-box until we know it's needed to know the true value.,0
Support: Firefox <=54,0
"Return a confounding non-pixel value or feign ignorance, as appropriate.",0
"Fall back to offsetWidth/offsetHeight when value is ""auto""",0
This happens for inline elements with no explicit setting (gh-3571),0
Support: Android <=4.1 - 4.3 only,0
Also use offsetWidth/offsetHeight for misreported inline dimensions (gh-3602),0
Support: IE 9-11 only,0
Also use offsetWidth/offsetHeight for when box sizing is unreliable,0
We use getClientRects() to check for hidden/disconnected.,0
"In those cases, the computed value can be trusted to be border-box",0
"Where available, offsetWidth/offsetHeight approximate border box dimensions.",0
"Where not available (e.g., SVG), assume unreliable box-sizing and interpret the",0
retrieved value as a content box dimension.,0
"Normalize """" and auto",0
Adjust for the element's box model,0
Provide the current computed size to request scroll gutter calculation (gh-3589),0
Add in style property hooks for overriding the default,0
behavior of getting and setting a style property,0
We should always get a number back from opacity,0
"Don't automatically add ""px"" to these possibly-unitless properties",0
Add in properties whose names you wish to fix before,0
setting or getting the value,0
Get and set the style property on a DOM Node,0
Don't set styles on text and comment nodes,0
Make sure that we're working with the right name,0
Make sure that we're working with the right name. We don't,0
want to query the value if it is a CSS custom property,0
since they are user-defined.,0
"Gets hook for the prefixed version, then unprefixed version",0
Check if we're setting a value,0
"Convert ""+="" or ""-="" to relative numbers (#7345)",0
Fixes bug #9237,0
Make sure that null and NaN values aren't set (#7116),0
"If a number was passed in, add the unit (except for certain CSS properties)",0
The isCustomProp check can be removed in jQuery 4.0 when we only auto-append,0
"""px"" to a few hardcoded values.",0
background-* props affect original clone's values,0
"If a hook was provided, use that value, otherwise just set the specified value",0
If a hook was provided get the non-computed value from there,0
Otherwise just get the value from the style object,0
Make sure that we're working with the right name. We don't,0
want to modify the value if it is a CSS custom property,0
since they are user-defined.,0
Try prefixed name followed by the unprefixed name,0
If a hook was provided get the computed value from there,0
"Otherwise, if a way to get the computed value exists, use that",0
"Convert ""normal"" to computed value",0
Make numeric if forced or a qualifier was provided and val looks numeric,0
Certain elements can have dimension info if we invisibly show them,0
but it must have a current display style that would benefit,0
Support: Safari 8+,0
Table columns in Safari have non-zero offsetWidth & zero,0
getBoundingClientRect().width unless display is changed.,0
Support: IE <=11 only,0
Running getBoundingClientRect on a disconnected node,0
in IE throws an error.,0
Only read styles.position if the test has a chance to fail,0
to avoid forcing a reflow.,0
"To avoid forcing a reflow, only fetch boxSizing if we need it (gh-3991)",0
Account for unreliable border-box dimensions by comparing offset* to computed and,0
faking a content-box to get border and padding (gh-3699),0
Convert to pixels if value adjustment is needed,0
These hooks are used by animate to expand properties,0
Assumes a single number if not a string,0
Check for promise aspect first to privilege synchronous behavior,0
Other thenables,0
Other non-thenables,0
Control `resolve` arguments by letting Array#slice cast boolean `noValue` to integer:,0
* false: [ value ].slice( 0 ) => resolve( value ),0
* true: [ value ].slice( 1 ) => resolve(),0
"For Promises/A+, convert exceptions into rejections",0
"Since jQuery.when doesn't unwrap thenables, we can skip the extra checks appearing in",0
Deferred#then to conditionally suppress rejection.,0
Support: Android 4.0 only,0
Strict mode functions invoked without .call/.apply get global-object context,0
"action, add listener, callbacks,",0
"... .then handlers, argument index, [final state]",0
Keep pipe for back-compat,0
"Map tuples (progress, done, fail) to arguments (done, fail, progress)",0
deferred.progress(function() { bind to newDefer or newDefer.notify }),0
deferred.done(function() { bind to newDefer or newDefer.resolve }),0
deferred.fail(function() { bind to newDefer or newDefer.reject }),0
Support: Promises/A+ section 2.3.3.3.3,0
https://promisesaplus.com/#point-59,0
Ignore double-resolution attempts,0
Support: Promises/A+ section 2.3.1,0
https://promisesaplus.com/#point-48,0
"Support: Promises/A+ sections 2.3.3.1, 3.5",0
https://promisesaplus.com/#point-54,0
https://promisesaplus.com/#point-75,0
Retrieve `then` only once,0
Support: Promises/A+ section 2.3.4,0
https://promisesaplus.com/#point-64,0
Only check objects and functions for thenability,0
Handle a returned thenable,0
Special processors (notify) just wait for resolution,0
Normal processors (resolve) also hook into progress,0
...and disregard older resolution values,0
Handle all other returned values,0
Only substitute handlers pass on context,0
and multiple values (non-spec behavior),0
Process the value(s),0
Default process is resolve,0
Only normal processors (resolve) catch and reject exceptions,0
Support: Promises/A+ section 2.3.3.3.4.1,0
https://promisesaplus.com/#point-61,0
Ignore post-resolution exceptions,0
Only substitute handlers pass on context,0
and multiple values (non-spec behavior),0
Support: Promises/A+ section 2.3.3.3.1,0
https://promisesaplus.com/#point-57,0
Re-resolve promises immediately to dodge false rejection from,0
subsequent errors,0
"Call an optional hook to record the stack, in case of exception",0
since it's otherwise lost when execution goes async,0
progress_handlers.add( ... ),0
fulfilled_handlers.add( ... ),0
rejected_handlers.add( ... ),0
Get a promise for this deferred,0
"If obj is provided, the promise aspect is added to the object",0
Add list-specific methods,0
promise.progress = list.add,0
promise.done = list.add,0
promise.fail = list.add,0
Handle state,0
"state = ""resolved"" (i.e., fulfilled)",0
"state = ""rejected""",0
rejected_callbacks.disable,0
fulfilled_callbacks.disable,0
rejected_handlers.disable,0
fulfilled_handlers.disable,0
progress_callbacks.lock,0
progress_handlers.lock,0
progress_handlers.fire,0
fulfilled_handlers.fire,0
rejected_handlers.fire,0
deferred.notify = function() { deferred.notifyWith(...) },0
deferred.resolve = function() { deferred.resolveWith(...) },0
deferred.reject = function() { deferred.rejectWith(...) },0
deferred.notifyWith = list.fireWith,0
deferred.resolveWith = list.fireWith,0
deferred.rejectWith = list.fireWith,0
Make the deferred a promise,0
Call given func if any,0
All done!,0
Deferred helper,0
count of uncompleted subordinates,0
count of unprocessed arguments,0
subordinate fulfillment data,0
the master Deferred,0
subordinate callback factory,0
Single- and empty arguments are adopted like Promise.resolve,0
Use .then() to unwrap secondary thenables (cf. gh-3000),0
Multiple arguments are aggregated like Promise.all array elements,0
See https://github.com/eslint/eslint/issues/3229,0
"Support: IE <=10 - 11, Edge 12 - 13 only",0
In IE/Edge using regex groups here causes severe slowdowns.,0
See https://connect.microsoft.com/IE/feedback/details/1736512/,0
"checked=""checked"" or checked",0
Prefer a tbody over its parent table for containing new rows,0
Replace/restore the type attribute of script elements for safe DOM manipulation,0
"1. Copy private data: events, handlers, etc.",0
2. Copy user data,0
"Fix IE bugs, see support tests",0
Fails to persist the checked state of a cloned checkbox or radio button.,0
Fails to return the selected option to the default selected state when cloning options,0
Flatten any nested arrays,0
"We can't cloneNode fragments that contain checked, in WebKit",0
Require either new content or an interest in ignored elements to invoke the callback,0
Use the original fragment for the last item,0
instead of the first because it can end up,0
being emptied incorrectly in certain situations (#8070).,0
Keep references to cloned scripts for later restoration,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
"push.apply(_, arraylike) throws on ancient WebKit",0
Reenable scripts,0
Evaluate executable scripts on first document insertion,0
"Optional AJAX dependency, but won't run scripts if not present",0
Fix IE cloning issues,0
We eschew Sizzle here for performance reasons: https://jsperf.com/getall-vs-sizzle/2,0
Copy the events from the original to the clone,0
Preserve script evaluation history,0
Return the cloned set,0
This is a shortcut to avoid jQuery.event.remove's overhead,0
Support: Chrome <=35 - 45+,0
"Assign undefined instead of using delete, see Data#remove",0
Support: Chrome <=35 - 45+,0
"Assign undefined instead of using delete, see Data#remove",0
Prevent memory leaks,0
Remove any remaining nodes,0
See if we can take a shortcut and just use innerHTML,0
Remove element nodes and prevent memory leaks,0
"If using innerHTML throws an exception, use the fallback method",0
"Make the changes, replacing each non-ignored context element with the new content",0
Force callback invocation,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
".get() because push.apply(_, arraylike) throws on ancient WebKit",0
"Define the hook, we'll check on the first run if it's really needed.",0
Hook not needed (or it's not possible to use it due,0
"to missing dependency), remove it.",0
Hook needed; redefine it so that the support test is not executed again.,0
Executing both pixelPosition & boxSizingReliable tests require only one layout,0
so they're executed at the same time to save the second computation.,0
"This is a singleton, we need to execute it only once",0
"Support: Android 4.0 - 4.3 only, Firefox <=3 - 44",0
"Support: Android 4.0 - 4.3 only, Safari <=9.1 - 10.1, iOS <=7.0 - 9.3",0
"Some styles come back with percentage values, even though they shouldn't",0
Support: IE 9 - 11 only,0
Detect misreporting of content dimensions for box-sizing:border-box elements,0
Support: IE 9 only,0
Detect overflow:scroll screwiness (gh-3699),0
Support: Chrome <=64,0
Don't get tricked when zoom affects offsetWidth (gh-4029),0
Nullify the div so it wouldn't be stored in the memory and,0
it will also be a sign that checks already performed,0
Finish early in limited (non-browser) environments,0
Support: IE <=9 - 11 only,0
Style of cloned element affects source element cloned (#8908),0
Support: Firefox 51+,0
Retrieving style before computed somehow,0
fixes an issue with getting wrong values,0
on detached elements,0
getPropertyValue is needed for:,0
".css('filter') (IE 9 only, #12537)",0
.css('--customProperty) (#3144),0
"A tribute to the ""awesome hack by Dean Edwards""",1
"Android Browser returns percentage for some values,",0
but width seems to be reliably pixels.,0
This is against the CSSOM draft spec:,0
https://drafts.csswg.org/cssom/#resolved-values,0
Remember the original values,0
Put in the new values to get a computed value out,0
Revert the changed values,0
Support: IE <=9 - 11 only,0
IE returns zIndex value as an integer.,0
Determine new display value for elements that need to change,0
"Since we force visibility upon cascade-hidden elements, an immediate (and slow)",0
check is required in this first loop unless we have a nonempty display value (either,0
inline or about-to-be-restored),0
Remember what we're overwriting,0
Set the display of the elements in a second loop to avoid constant reflow,0
Starting value computation is required for potential unit mismatches,0
Support: Firefox <=54,0
Halve the iteration target value to prevent interference from CSS upper bounds (gh-2144),0
Trust units reported by jQuery.css,0
Iteratively approximate from a nonzero starting point,0
Evaluate and update our best guess (doubling guesses that zero out).,0
Finish if the scale equals or crosses 1 (making the old*new product non-positive).,0
Make sure we update the tween properties later on,0
Apply relative offset (+=/-=) if specified,0
"Support: IE <=11 only, Firefox <=30 (#15098, #14150)",0
IE throws on elements created in popups,0
"FF meanwhile throws on frame elements through ""defaultView.getComputedStyle""",0
css is assumed,0
"isHiddenWithinTree reports if an element has a non-""none"" display style (inline and/or",0
"through the CSS cascade), which is useful in deciding whether or not to make it visible.",0
It differs from the :hidden selector (jQuery.expr.pseudos.hidden) in two important ways:,0
* A hidden ancestor does not force an element to be classified as hidden.,0
* Being disconnected from the document does not force an element to be classified as hidden.,0
These differences improve the behavior of .toggle() et al. when applied to elements that are,0
"detached or contained within hidden ancestors (gh-2404, gh-2863).",0
isHiddenWithinTree might be called from jQuery#filter function;,0
"in that case, element will be second argument",0
Inline style trumps all,0
"Otherwise, check computed style",0
Support: Firefox <=43 - 45,0
"Disconnected elements can have computed display: none, so first confirm that elem is",0
in the document.,0
A method for quickly swapping in/out CSS properties to get correct calculations.,0
"Remember the old values, and insert the new ones",0
Revert the old values,0
Don't do events on text and comment nodes,0
focus/blur morphs to focusin/out; ensure we're not firing them right now,0
Namespaced trigger; create a regexp to match event type in handle(),0
"Caller can pass in a jQuery.Event object, Object, or just an event type string",0
Trigger bitmask: & 1 for native handlers; & 2 for jQuery (always true),0
Clean up the event in case it is being reused,0
"Clone any incoming data and prepend the event, creating the handler arg list",0
Allow special events to draw outside the lines,0
"Determine event propagation path in advance, per W3C events spec (#9951)",0
"Bubble up to document, then to window; watch for a global ownerDocument var (#9724)",0
"Only add window if we got to document (e.g., not plain obj or detached DOM)",0
Fire handlers on the event path,0
jQuery handler,0
Native handler,0
"If nobody prevented the default action, do it now",0
Call a native DOM method on the target with the same name as the event.,0
"Don't do default actions on window, that's where global variables be (#6170)",0
Don't re-trigger an onFOO event when we call its FOO() method,0
"Prevent re-triggering of the same event, since we already bubbled it above",0
Piggyback on a donor event to simulate a different one,0
Used only for `focus(in | out)` events,0
Attach a bunch of functions for handling common AJAX events,0
Support: Firefox <=44,0
Firefox doesn't have focus(in | out) events,0
Related ticket - https://bugzilla.mozilla.org/show_bug.cgi?id=687787,0
,0
"Support: Chrome <=48 - 49, Safari <=9.0 - 9.1",0
"focus(in | out) events fire after focus & blur events,",0
which is spec violation - http://www.w3.org/TR/DOM-Level-3-Events/#events-focusevent-event-order,0
Related ticket - https://bugs.chromium.org/p/chromium/issues/detail?id=449857,0
Attach a single capturing handler on the document while someone wants focusin/focusout,0
Handle event binding,0
"Based off of the plugin by Clint Helfers, with permission.",0
https://web.archive.org/web/20100324014747/http://blindsignals.com/index.php/2009/07/jquery-delay/,0
Support: IE <=9 - 11 only,0
Use typeof to avoid zero-argument method invocation on host objects (#15151),0
Mark scripts as having already been evaluated,0
Support: Android 4.0 - 4.3 only,0
Check state lost if the name is set (#11217),0
Support: Windows Web Apps (WWA),0
`name` and `type` must use .setAttribute for WWA (#14901),0
Support: Android <=4.1 only,0
Older WebKit doesn't clone checked state correctly in fragments,0
Support: IE <=11 only,0
Make sure textarea (and checkbox) defaultValue is properly cloned,0
Add nodes directly,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
"push.apply(_, arraylike) throws on ancient WebKit",0
Convert non-html into a text node,0
Convert html into DOM nodes,0
Deserialize a standard representation,0
Descend through wrappers to the right content,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
"push.apply(_, arraylike) throws on ancient WebKit",0
Remember the top-level container,0
Ensure the created nodes are orphaned (#12392),0
Remove wrapper from fragment,0
Skip elements already in the context collection (trac-4087),0
Append to fragment,0
Preserve script evaluation history,0
Capture executables,0
We have to close these tags to support XHTML (#13200),0
Support: IE <=9 only,0
XHTML parsers do not magically insert elements in the,0
same way that tag soup parsers do. So we cannot shorten,0
this by omitting <tbody> or other required elements.,0
Support: IE <=9 only,0
"Make this explicit, since user can override this through ajaxSetup (#11264)",0
Only evaluate the response if it is successful (gh-4126),0
"dataFilter is not invoked for failure responses, so using it instead",0
of the default converter is kludgy but it works.,0
rtagName captures the name from the first start tag in a string of HTML,0
https://html.spec.whatwg.org/multipage/syntax.html#tag-open-state,0
https://html.spec.whatwg.org/multipage/syntax.html#tag-name-state,0
"These usually indicate a programmer mistake during development,",0
warn about them ASAP rather than swallowing them by default.,0
Support: IE 8 - 9 only,0
"Console exists when dev tools are open, which can happen at any time",0
Check if the owner object already has a cache,0
"If not, create one",0
"We can accept data for non-element nodes in modern browsers,",0
"but we should not, see #8335.",0
Always return an empty object.,0
If it is a node unlikely to be stringify-ed or looped over,0
use plain assignment,0
Otherwise secure it in a non-enumerable property,0
configurable must be true to allow the property to be,0
deleted when data is removed,0
"Handle: [ owner, key, value ] args",0
Always use camelCase key (gh-2257),0
"Handle: [ owner, { properties } ] args",0
Copy the properties one-by-one to the cache object,0
Always use camelCase key (gh-2257),0
In cases where either:,0
,0
1. No key was specified,0
"2. A string key was specified, but no value provided",0
,0
"Take the ""read"" path and allow the get method to determine",0
"which value to return, respectively either:",0
,0
1. The entire cache object,0
2. The data stored at the key,0
,0
"When the key is not a string, or both a key and value",0
"are specified, set or extend (existing objects) with either:",0
,0
1. An object of properties,0
2. A key and value,0
,0
"Since the ""set"" path can have two possible entry points",0
return the expected data based on which path was taken[*],0
Support array or space separated string of keys,0
If key is an array of keys...,0
"We always set camelCase keys, so remove that.",0
"If a key with the spaces exists, use it.",0
"Otherwise, create an array by matching non-whitespace",0
Remove the expando if there's no more data,0
Support: Chrome <=35 - 45,0
Webkit & Blink performance suffers when deleting properties,0
"from DOM nodes, so set to undefined instead",0
https://bugs.chromium.org/p/chromium/issues/detail?id=378607 (bug restricted),0
Accepts only:,0
- Node,0
- Node.ELEMENT_NODE,0
- Node.DOCUMENT_NODE,0
- Object,0
- Any,0
Only count HTML whitespace,0
Other whitespace should count in values,0
https://infra.spec.whatwg.org/#ascii-whitespace,0
[[Class]] -> type pairs,0
All support tests are defined in their respective modules.,0
"Support: Chrome <=57, Firefox <=52",0
"In some browsers, typeof returns ""function"" for HTML <object> elements",0
"(i.e., `typeof document.createElement( ""object"" ) === ""function""`).",0
We don't want to classify *any* DOM node as a function.,0
"Support: Firefox 64+, Edge 18+",0
"Some browsers don't support the ""nonce"" property on scripts.",0
"On the other hand, just using `getAttribute` is not enough as",0
the `nonce` attribute is reset to an empty string whenever it,0
becomes browsing-context connected.,0
See https://github.com/whatwg/html/issues/2369,0
See https://html.spec.whatwg.org/#nonce-attributes,0
The `node.getAttribute` check was added for the sake of,0
`jQuery.globalEval` so that it can fake a nonce-containing node,0
via an object.,0
Support: Safari 8 only,0
In Safari 8 documents created via document.implementation.createHTMLDocument,0
collapse sibling forms: the second one becomes a child of the first one.,0
"Because of that, this security measure has to be disabled in Safari 8.",0
https://bugs.webkit.org/show_bug.cgi?id=137337,0
This is the only module that needs core/support,0
"Argument ""data"" should be string of html",0
"context (optional): If specified, the fragment will be created in this context,",0
defaults to document,0
"keepScripts (optional): If true, will include scripts passed in the html string",0
Stop scripts or inline event handlers from being executed immediately,0
by using document.implementation,0
Set the base href for the created document,0
so any parsed elements with URLs,0
are based on the document's URL (gh-2965),0
Single tag,0
Matches dashed string for camelizing,0
Used by camelCase as callback to replace(),0
Convert dashed to camelCase; used by the css and data modules,0
"Support: IE <=9 - 11, Edge 12 - 15",0
Microsoft forgot to hump their vendor prefix (#9572),0
Multifunctional method to get and set values of a collection,0
The value/s can optionally be executed if it's a function,0
Sets many values,0
Sets one value,0
Bulk operations run against the entire set,0
...except when executing function values,0
Gets,0
Strip and collapse whitespace according to HTML spec,0
https://infra.spec.whatwg.org/#strip-and-collapse-ascii-whitespace,0
Initialize a jQuery object,0
A central reference to the root jQuery(document),0
A simple way to check for HTML strings,0
Prioritize #id over <tag> to avoid XSS via location.hash (#9521),0
Strict HTML recognition (#11290: must start with <),0
Shortcut simple #id case for speed,0
"HANDLE: $(""""), $(null), $(undefined), $(false)",0
Method init() accepts an alternate rootjQuery,0
so migrate can support jQuery.sub (gh-2101),0
Handle HTML strings,0
Assume that strings that start and end with <> are HTML and skip the regex check,0
Match html or make sure no context is specified for #id,0
HANDLE: $(html) -> $(array),0
Option to run scripts is true for back-compat,0
Intentionally let the error be thrown if parseHTML is not present,0
"HANDLE: $(html, props)",0
Properties of context are called as methods if possible,0
...and otherwise set as attributes,0
HANDLE: $(#id),0
Inject the element directly into the jQuery object,0
"HANDLE: $(expr, $(...))",0
"HANDLE: $(expr, context)",0
(which is just equivalent to: $(context).find(expr),0
HANDLE: $(DOMElement),0
HANDLE: $(function),0
Shortcut for document ready,0
Execute immediately if ready is not present,0
Give the init function the jQuery prototype for later instantiation,0
Initialize central reference,0
Support: Android <=2.3 only (functionish RegExp),0
Prevent errors from freezing future callback execution (gh-1823),0
Not backwards-compatible as this does not execute sync,0
Is the DOM ready to be used? Set to true once it occurs.,0
A counter to track how many items to wait for before,0
the ready event fires. See #6781,0
Abort if there are pending holds or we're already ready,0
Remember that the DOM is ready,0
"If a normal DOM Ready event fired, decrement, and wait if need be",0
Make jQuery.ready Promise consumable (gh-1778),0
Catch cases where $(document).ready() is called,0
after the browser event has already occurred.,0
Support: IE9-10 only,0
"Older IE sometimes signals ""interactive"" too soon",0
Handle it asynchronously to allow scripts the opportunity to delay ready,0
Use the handy event callback,0
"A fallback to window.onload, that will always work",0
The deferred used on DOM ready,0
Wrap jQuery.readyException in a function so that the lookup,0
happens at the time of error handling instead of callback,0
registration.,0
Is the DOM ready to be used? Set to true once it occurs.,0
A counter to track how many items to wait for before,0
the ready event fires. See #6781,0
Handle when the DOM is ready,0
Abort if there are pending holds or we're already ready,0
Remember that the DOM is ready,0
"If a normal DOM Ready event fired, decrement, and wait if need be",0
"If there are functions bound, to execute",0
The ready event handler and self cleanup method,0
Catch cases where $(document).ready() is called,0
after the browser event has already occurred.,0
Support: IE <=9 - 10 only,0
"Older IE sometimes signals ""interactive"" too soon",0
Handle it asynchronously to allow scripts the opportunity to delay ready,0
Use the handy event callback,0
"A fallback to window.onload, that will always work",0
rsingleTag matches a string consisting of a single HTML element with no attributes,0
and captures the element's name,0
"Register as a named AMD module, since jQuery can be concatenated with other",0
"files that may use define, but not via a proper concatenation script that",0
understands anonymous AMD modules. A named AMD is safest and most robust,0
way to register. Lowercase jquery is used because AMD module names are,0
"derived from file names, and jQuery is normally delivered in a lowercase",0
file name. Do this after creating the global so that if an AMD module wants,0
"to call noConflict to hide this version of jQuery, it will work.",0
"Note that for maximum portability, libraries that are not jQuery should",0
"declare themselves as anonymous modules, and avoid setting a global if an",0
"AMD loader is present. jQuery is a special case. For more information, see",0
https://github.com/jrburke/requirejs/wiki/Updating-existing-libraries#wiki-anon,0
Map over jQuery in case of overwrite,0
Map over the $ in case of overwrite,0
"Expose jQuery and $ identifiers, even in AMD",0
"(#7102#comment:10, https://github.com/jquery/jquery/pull/557)",0
and CommonJS for browser emulators (#13566),0
"Use a property on the element directly when it is not a DOM element,",0
or when there is no matching style property that exists.,0
Passing an empty string as a 3rd parameter to .css will automatically,0
attempt a parseFloat and fallback to a string if the parse fails.,0
"Simple values such as ""10px"" are parsed to Float;",0
"complex values such as ""rotate(1rad)"" are returned as-is.",0
"Empty strings, null, undefined and ""auto"" are converted to 0.",0
Use step hook for back compat.,0
Use cssHook if its there.,0
Use .style if available and use plain properties where available.,0
Support: IE <=9 only,0
Panic based approach to setting things on disconnected nodes,0
Back compat <1.8 extension point,0
Handle most common string cases,0
Handle cases where value is null/undef or number,0
"Treat null/undefined as """"; convert numbers to string",0
"If set returns undefined, fall back to normal setting",0
Support: IE <=10 - 11 only,0
"option.text throws exceptions (#14686, #14858)",0
Strip and collapse whitespace,0
https://html.spec.whatwg.org/#strip-and-collapse-whitespace,0
Loop through all the selected options,0
Support: IE <=9 only,0
IE8-9 doesn't update selected after form reset (#2551),0
Don't return options that are disabled or in a disabled optgroup,0
Get the specific value for the option,0
We don't need an array for one selects,0
Multi-Selects return an array,0
Force browsers to behave consistently when non-matching value is set,0
Radios and checkboxes getter/setter,0
"Don't get/set attributes on text, comment and attribute nodes",0
Fallback to prop when attributes are not supported,0
Attribute hooks are determined by the lowercase version,0
Grab necessary hook if one is defined,0
"Non-existent attributes return null, we normalize to undefined",0
Attribute names can contain non-HTML whitespace characters,0
https://html.spec.whatwg.org/multipage/syntax.html#attributes-2,0
Hooks for boolean attributes,0
Remove boolean attributes when set to false,0
Avoid an infinite loop by temporarily removing this function from the getter,0
Support: Android <=4.3 only,0
"Default value for a checkbox should be ""on""",0
Support: IE <=11 only,0
Must access selectedIndex to make default options select,0
Support: IE <=11 only,0
An input loses its value after becoming a radio,0
Only assign if different to avoid unneeded rendering.,0
This expression is here for better compressibility (see addClass),0
Remove *all* instances,0
Only assign if different to avoid unneeded rendering.,0
Toggle individual class names,0
"Check each className given, space separated list",0
Toggle whole class name,0
Store className if set,0
"If the element has a class name or if we're passed `false`,",0
"then remove the whole classname (if there was one, the above saved it).",0
"Otherwise bring back whatever was previously saved (if anything),",0
falling back to the empty string if nothing was stored.,0
"Don't get/set properties on text, comment and attribute nodes",0
Fix name and attach hooks,0
Support: IE <=9 - 11 only,0
elem.tabIndex doesn't always return the,0
correct value when it hasn't been explicitly set,0
https://web.archive.org/web/20141116233347/http://fluidproject.org/blog/2008/01/09/getting-setting-and-removing-tabindex-values-with-javascript/,0
Use proper attribute retrieval(#12072),0
Support: IE <=11 only,0
Accessing the selectedIndex property,0
forces the browser to respect setting selected,0
on the option,0
The getter ensures a default option is selected,0
when in an optgroup,0
"eslint rule ""no-unused-expressions"" is disabled for this code",0
since it considers such accessions noop,0
If it's a function,0
We assume that it's the callback,0
"Otherwise, build a param string",0
"If we have elements to modify, make the request",0
"If ""type"" variable is undefined, then ""GET"" method will be used.",0
Make value of this field explicit since,0
user can override it through ajaxSetup method,0
Save response for use in complete callback,0
"If a selector was specified, locate the right elements in a dummy div",0
Exclude scripts to avoid IE 'Permission Denied' errors,0
Otherwise use the full result,0
"If the request succeeds, this function gets ""data"", ""status"", ""jqXHR""",0
but they are ignored because response was set above.,0
"If it fails, this function gets ""jqXHR"", ""status"", ""error""",0
Cross-browser xml parsing,0
Support: IE 9 - 11 only,0
IE throws on parseFromString with invalid input.,0
Prevent auto-execution of scripts when no explicit dataType was provided (See gh-2432),0
Install script dataType,0
Handle cache's special case and crossDomain,0
Bind script tag hack transport,0
This transport only deals with cross domain or forced-by-attrs requests,0
Use native DOM manipulation to avoid our domManip AJAX trickery,0
Default jsonp settings,0
"Detect, normalize options and install callbacks for jsonp requests",0
"Handle iff the expected data type is ""jsonp"" or we have a parameter to set",0
"Get callback name, remembering preexisting value associated with it",0
Insert callback into url or form data,0
Use data converter to retrieve json after script execution,0
Force json dataType,0
Install callback,0
Clean-up function (fires after converters),0
If previous value didn't exist - remove it,0
Otherwise restore preexisting value,0
Save back as free,0
Make sure that re-using the options doesn't screw things around,0
Save the callback name for future use,0
Call if it was a function and we have a response,0
Delegate to script,0
"File protocol always yields status code 0, assume 200",0
Support: IE <=9 only,0
#1450: sometimes IE returns 1223 when it should be 204,0
Cross domain only allowed if supported through XMLHttpRequest,0
Apply custom fields if provided,0
Override mime type if needed,0
X-Requested-With header,0
"For cross-domain requests, seeing as conditions for a preflight are",0
"akin to a jigsaw puzzle, we simply never set it to be sure.",0
(it can always be set on a per-request basis or even using ajaxSetup),0
"For same-domain requests, won't change header if already provided.",0
Set headers,0
Callback,0
Support: IE <=9 only,0
"On a manual native abort, IE9 throws",0
errors on any property access that is not readyState,0
"File: protocol always yields status 0; see #8605, #14207",0
Support: IE <=9 only,0
IE9 has no XHR2 but throws on binary (trac-11426),0
"For XHR2 non-text, let the caller handle it (gh-2498)",0
Listen to events,0
Support: IE 9 only,0
Use onreadystatechange to replace onabort,0
to handle uncaught aborts,0
Check readyState before timeout as it changes,0
"Allow onerror to be called first,",0
but that will not handle a native abort,0
"Also, save errorCallback to a variable",0
as xhr.onerror cannot be accessed,0
Create the abort callback,0
Do send the request (this may raise an exception),0
#14683: Only rethrow if this hasn't been notified as an error yet,0
Implement the identical functionality for filter and not,0
Single element,0
"Arraylike of elements (jQuery, arguments, Array)",0
Filtered directly for both simple and complex selectors,0
"If this is a positional/relative selector, check membership in the returned set",0
"so $(""p:first"").is(""p:last"") won't return true for a doc with two ""p"".",0
# sourceMappingURL=sizzle.min.map,0
Local document vars,0
Instance-specific data,0
Instance methods,0
Use a stripped-down indexOf as it's faster than native,0
https://jsperf.com/thor-indexof-vs-for/5,0
Regular expressions,0
http://www.w3.org/TR/css3-selectors/#whitespace,0
http://www.w3.org/TR/CSS21/syndata.html#value-def-identifier,0
Attribute selectors: http://www.w3.org/TR/selectors/#attribute-selectors,0
Operator (capture 2),0
"""Attribute values must be CSS identifiers [capture 5] or strings [capture 3 or capture 4]""",0
"To reduce the number of selectors needing tokenize in the preFilter, prefer arguments:",0
1. quoted (capture 3; capture 4 or capture 5),0
2. simple (capture 6),0
3. anything else (capture 2),0
"Leading and non-escaped trailing whitespace, capturing some non-whitespace characters preceding the latter",0
For use in libraries implementing .is(),0
We use this for POS matching in `select`,0
Easily-parseable/retrievable ID or TAG or CLASS selectors,0
CSS escapes,0
http://www.w3.org/TR/CSS21/syndata.html#escaped-characters,0
NaN means non-codepoint,0
Support: Firefox<24,0
"Workaround erroneous numeric interpretation of +""0x""",1
BMP codepoint,0
Supplemental Plane codepoint (surrogate pair),0
CSS string/identifier serialization,0
https://drafts.csswg.org/cssom/#common-serializing-idioms,0
U+0000 NULL becomes U+FFFD REPLACEMENT CHARACTER,0
Control characters and (dependent upon position) numbers get escaped as code points,0
Other potentially-special ASCII characters get backslash-escaped,0
Used for iframes,0
See setDocument(),0
"Removing the function wrapper causes a ""Permission Denied""",0
error in IE,0
"Optimize for push.apply( _, NodeList )",0
Support: Android<4.0,0
Detect silently failing push.apply,0
Leverage slice if possible,0
Support: IE<9,0
Otherwise append directly,0
Can't trust NodeList.length,0
"nodeType defaults to 9, since context defaults to document",0
Return early from calls with invalid selector or context,0
Try to shortcut find operations (as opposed to filters) in HTML documents,0
"If the selector is sufficiently simple, try using a ""get*By*"" DOM method",0
"(excepting DocumentFragment context, where the methods don't exist)",0
ID selector,0
Document context,0
"Support: IE, Opera, Webkit",0
TODO: identify versions,1
getElementById can match elements by name instead of ID,0
Element context,0
"Support: IE, Opera, Webkit",0
TODO: identify versions,1
getElementById can match elements by name instead of ID,0
Type selector,0
Class selector,0
Take advantage of querySelectorAll,0
Support: IE 8 only,0
Exclude object elements,0
qSA considers elements outside a scoping root when evaluating child or,0
"descendant combinators, which is not what we want.",0
"In such cases, we work around the behavior by prefixing every selector in the",0
list with an ID selector referencing the scope context.,0
Thanks to Andrew Dupont for this technique.,0
"Capture the context ID, setting it first if necessary",0
Prefix every selector in the list,0
Expand context for sibling selectors,0
All others,0
"Use (key + "" "") to avoid collision with native prototype properties (see Issue #157)",0
Only keep the most recent entries,0
Remove from its parent by default,0
release memory in IE,0
Use IE sourceIndex if available on both nodes,0
Check if b follows a,0
Known :disabled false positives: fieldset[disabled] > legend:nth-of-type(n+2) :can-disable,0
Only certain elements can match :enabled or :disabled,0
https://html.spec.whatwg.org/multipage/scripting.html#selector-enabled,0
https://html.spec.whatwg.org/multipage/scripting.html#selector-disabled,0
Check for inherited disabledness on relevant non-disabled elements:,0
* listed form-associated elements in a disabled fieldset,0
https://html.spec.whatwg.org/multipage/forms.html#category-listed,0
https://html.spec.whatwg.org/multipage/forms.html#concept-fe-disabled,0
* option elements in a disabled optgroup,0
https://html.spec.whatwg.org/multipage/forms.html#concept-option-disabled,0
"All such elements have a ""form"" property.",0
Option elements defer to a parent optgroup if present,0
Support: IE 6 - 11,0
Use the isDisabled shortcut property to check for disabled fieldset ancestors,0
"Where there is no isDisabled, check manually",0
Try to winnow out elements that can't be disabled before trusting the disabled property.,0
"Some victims get caught in our net (label, legend, menu, track), but it shouldn't",0
"even exist on them, let alone have a boolean value.",0
Remaining elements are neither :enabled nor :disabled,0
Match elements found at the specified indexes,0
Expose support vars for convenience,0
Support: IE <=8,0
"Assume HTML when documentElement doesn't yet exist, such as inside loading iframes",0
https://bugs.jquery.com/ticket/4833,0
Return early if doc is invalid or already selected,0
Update global variables,0
"Support: IE 9-11, Edge",0
"Accessing iframe documents after unload throws ""permission denied"" errors (jQuery #13936)",0
"Support: IE 11, Edge",0
Support: IE 9 - 10 only,0
Support: IE<8,0
Verify that getAttribute really returns attributes and not properties,0
(excepting IE8 booleans),0
"Check if getElementsByTagName(""*"") returns only elements",0
Support: IE<9,0
Support: IE<10,0
Check if getElementById returns elements by name,0
"The broken getElementById methods don't pick up programmatically-set names,",1
so use a roundabout getElementsByName test,0
ID filter and find,0
Support: IE 6 - 7 only,0
getElementById is not reliable as a find shortcut,0
Verify the id attribute,0
Fall back on getElementsByName,0
Tag,0
DocumentFragment nodes don't have gEBTN,0
"By happy coincidence, a (broken) gEBTN appears on DocumentFragment nodes too",0
Filter out possible comments,0
Class,0
QSA and matchesSelector support,0
matchesSelector(:active) reports false when true (IE9/Opera 11.5),0
qSa(:focus) reports false when true (Chrome 21),0
We allow this because of a bug in IE8/9 that throws an error,0
whenever `document.activeElement` is accessed on an iframe,0
"So, we allow :focus to pass through QSA all the time to avoid the IE error",0
See https://bugs.jquery.com/ticket/13378,0
Build QSA regex,0
Regex strategy adopted from Diego Perini,0
Select is set to empty string on purpose,0
This is to test IE's treatment of not explicitly,0
"setting a boolean content attribute,",0
since its presence should be enough,0
https://bugs.jquery.com/ticket/12359,0
"Support: IE8, Opera 11-12.16",0
Nothing should be selected when empty strings follow ^= or $= or *=,0
"The test attribute must be unknown in Opera but ""safe"" for WinRT",0
https://msdn.microsoft.com/en-us/library/ie/hh465388.aspx#attribute_section,0
Support: IE8,0
"Boolean attributes and ""value"" are not treated correctly",0
"Support: Chrome<29, Android<4.4, Safari<7.0+, iOS<7.0+, PhantomJS<1.9.8+",0
Webkit/Opera - :checked should return selected option elements,0
http://www.w3.org/TR/2011/REC-css3-selectors-20110929/#checked,0
IE8 throws error here and will not see later tests,0
"Support: Safari 8+, iOS 8+",0
https://bugs.webkit.org/show_bug.cgi?id=136851,0
In-page `selector#id sibling-combinator selector` fails,0
Support: Windows 8 Native Apps,0
The type and name attributes are restricted during .innerHTML assignment,0
Support: IE8,0
Enforce case-sensitivity of name attribute,0
FF 3.5 - :enabled/:disabled and hidden elements (hidden elements are still enabled),0
IE8 throws error here and will not see later tests,0
Support: IE9-11+,0
IE's :disabled selector does not pick up the children of disabled fieldsets,0
Opera 10-11 does not throw on post-comma invalid pseudos,0
Check to see if it's possible to do matchesSelector,0
on a disconnected node (IE 9),0
This should fail with an exception,0
"Gecko does not error, returns false instead",0
Element contains another,0
Purposefully self-exclusive,0
"As in, an element does not contain itself",0
Document order sorting,0
Flag for duplicate removal,0
Sort on method existence if only one input has compareDocumentPosition,0
Calculate position if both inputs belong to the same document,0
Otherwise we know they are disconnected,0
Disconnected nodes,0
Choose the first element that is related to our preferred document,0
Maintain original order,0
Exit early if the nodes are identical,0
Parentless nodes are either documents or disconnected,0
"If the nodes are siblings, we can do a quick check",0
Otherwise we need full lists of their ancestors for comparison,0
Walk down the tree looking for a discrepancy,0
Do a sibling check if the nodes have a common ancestor,0
Otherwise nodes in our document sort first,0
Set document vars if needed,0
IE 9's matchesSelector returns false on disconnected nodes,0
"As well, disconnected nodes are said to be in a document",0
fragment in IE 9,0
Set document vars if needed,0
Set document vars if needed,0
Don't get fooled by Object.prototype properties (jQuery #13807),0
"Unless we *know* we can detect duplicates, assume their presence",0
Clear input after sorting to release objects,0
See https://github.com/jquery/sizzle/pull/225,0
"If no nodeType, this is expected to be an array",0
Do not traverse comment nodes,0
Use textContent for elements,0
innerText usage removed for consistency of new lines (jQuery #11153),0
Traverse its children,0
Do not include comment or processing instruction nodes,0
Can be adjusted by the user,0
Move the given value to match[3] whether quoted or unquoted,0
nth-* requires argument,0
numeric x and y parameters for Expr.filter.CHILD,0
remember that false/true cast respectively to 0/1,0
other types prohibit arguments,0
Accept quoted arguments as-is,0
Strip excess characters from unquoted arguments,0
Get excess from tokenize (recursively),0
advance to the next closing parenthesis,0
excess is a negative index,0
Return only captures needed by the pseudo filter method (type and argument),0
Shortcut for :nth-*(n),0
:(first|last|only)-(child|of-type),0
Reverse direction for :only-* (if we haven't yet done so),0
non-xml :nth-child(...) stores cache data on `parent`,0
Seek `elem` from a previously-cached index,0
...in a gzip-friendly way,0
Support: IE <9 only,0
Defend against cloned attroperties (jQuery gh-1709),0
Fallback to seeking `elem` from the start,0
"When found, cache indexes on `parent` and break",0
Use previously-cached element index if available,0
...in a gzip-friendly way,0
Support: IE <9 only,0
Defend against cloned attroperties (jQuery gh-1709),0
xml :nth-child(...),0
or :nth-last-child(...) or :nth(-last)?-of-type(...),0
Use the same loop as above to seek `elem` from the start,0
Cache the index of each encountered element,0
Support: IE <9 only,0
Defend against cloned attroperties (jQuery gh-1709),0
"Incorporate the offset, then check against cycle size",0
pseudo-class names are case-insensitive,0
http://www.w3.org/TR/selectors/#pseudo-classes,0
Prioritize by case sensitivity in case custom pseudos are added with uppercase letters,0
Remember that setFilters inherits from pseudos,0
The user may use createPseudo to indicate that,0
arguments are needed to create the filter function,0
just as Sizzle does,0
But maintain support for old signatures,0
Potentially complex pseudos,0
Trim the selector passed to compile,0
to avoid treating leading and trailing,0
spaces as combinators,0
Match elements unmatched by `matcher`,0
Don't keep the element (issue #299),0
"""Whether an element is represented by a :lang() selector",0
is based solely on the element's language value,0
"being equal to the identifier C,",0
"or beginning with the identifier C immediately followed by ""-"".",0
The matching of C against the element's language value is performed case-insensitively.,0
"The identifier C does not have to be a valid language name.""",0
http://www.w3.org/TR/selectors/#lang-pseudo,0
lang value must be a valid identifier,0
Miscellaneous,0
Boolean properties,0
"In CSS3, :checked should return both checked and selected elements",0
http://www.w3.org/TR/2011/REC-css3-selectors-20110929/#checked,0
Accessing this property makes selected-by-default,0
options in Safari work properly,0
Contents,0
http://www.w3.org/TR/selectors/#empty-pseudo,0
":empty is negated by element (1) or content nodes (text: 3; cdata: 4; entity ref: 5),",0
but not by others (comment: 8; processing instruction: 7; etc.),0
nodeType < 6 works because attributes (2) do not appear as children,0
Element/input types,0
Support: IE<8,0
"New HTML5 attribute values (e.g., ""search"") appear with elem.type === ""text""",0
Position-in-collection,0
Add button/input type pseudos,0
Easy API for creating new setFilters,0
Comma and first run,0
Don't consume trailing commas as valid,0
Combinators,0
Cast descendant combinators to space,0
Filters,0
Return the length of the invalid excess,0
if we're just parsing,0
"Otherwise, throw an error or return tokens",0
Cache the tokens,0
Check against closest ancestor/preceding element,0
Check against all ancestor/preceding elements,0
"We can't set arbitrary data on XML nodes, so they don't benefit from combinator caching",0
Support: IE <9 only,0
Defend against cloned attroperties (jQuery gh-1709),0
Assign to newCache so results back-propagate to previous elements,0
Reuse newcache so results back-propagate to previous elements,0
A match means we're done; a fail means we have to keep checking,0
Get initial elements from seed or context,0
"Prefilter to get matcher input, preserving a map for seed-results synchronization",0
"If we have a postFinder, or filtered seed, or non-seed postFilter or preexisting results,",0
...intermediate processing is necessary,0
...otherwise use results directly,0
Find primary matches,0
Apply postFilter,0
Un-match failing elements by moving them back to matcherIn,0
Get the final matcherOut by condensing this intermediate into postFinder contexts,0
Restore matcherIn since elem is not yet a final match,0
Move matched elements from seed to results to keep them synchronized,0
"Add elements to results, through postFinder if defined",0
The foundational matcher ensures that elements are reachable from top-level context(s),0
Avoid hanging onto element (issue #299),0
Return special upon seeing a positional matcher,0
Find the next relative operator (if any) for proper handling,0
"If the preceding token was a descendant combinator, insert an implicit any-element `*`",0
We must always have either seed elements or outermost context,0
Use integer dirruns iff this is the outermost matcher,0
Add elements passing elementMatchers directly to results,0
"Support: IE<9, Safari",0
"Tolerate NodeList properties (IE: ""length""; Safari: <number>) matching elements by id",0
Track unmatched elements for set filters,0
They will have gone through all possible matchers,0
"Lengthen the array for every element, matched or not",0
"`i` is now the count of elements visited above, and adding it to `matchedCount`",0
makes the latter nonnegative.,0
Apply set filters to unmatched elements,0
"NOTE: This can be skipped if there are no unmatched elements (i.e., `matchedCount`",0
"equals `i`), unless we didn't visit _any_ elements in the above loop because we have",0
no element matchers and no seed.,0
"Incrementing an initially-string ""0"" `i` allows `i` to remain a string only in that",0
"case, which will result in a ""00"" `matchedCount` that differs from `i` but is also",0
numerically zero.,0
Reintegrate element matches to eliminate the need for sorting,0
Discard index placeholder values to get only actual matches,0
Add matches to results,0
Seedless set matches succeeding multiple successful matchers stipulate sorting,0
Override manipulation of globals by nested matchers,0
Generate a function of recursive functions that can be used to check each element,0
Cache the compiled function,0
Save selector and tokenization,0
Try to minimize operations if there is only one selector in the list and no seed,0
(the latter of which guarantees us context),0
Reduce context if the leading compound selector is an ID,0
"Precompiled matchers will still verify ancestry, so step up a level",0
Fetch a seed set for right-to-left matching,0
Abort if we hit a combinator,0
"Search, expanding context for leading sibling combinators",0
"If seed is empty or no tokens remain, we can return early",0
Compile and execute a filtering function if one is not provided,0
Provide `match` to avoid retokenization if we modified the selector above,0
One-time assignments,0
Sort stability,0
Support: Chrome 14-35+,0
Always assume duplicates if they aren't passed to the comparison function,0
Initialize against the default document,0
Support: Webkit<537.32 - Safari 6.0.3/Chrome 25 (fixed in Chrome 27),0
Detached nodes confoundingly follow *each other*,0
"Should return 1, but returns 4 (following)",0
Support: IE<8,0
"Prevent attribute/property ""interpolation""",0
https://msdn.microsoft.com/en-us/library/ms536429%28VS.85%29.aspx,0
Support: IE<9,0
"Use defaultValue in place of getAttribute(""value"")",0
Support: IE<9,0
Use getAttributeNode to fetch booleans when getAttribute lies,0
EXPOSE,0
Sizzle requires that there be a global window in Common-JS like environments,0
EXPOSE,0
# sourceMappingURL=noty.min.js.map,0
Trim the opening space.,0
Replace the class name.,0
Trim the opening and closing spaces.,0
Opera 12.10 and Firefox 18 and later support,0
fix for Chrome < 45,0
"If len is 2, that means that we need to schedule an async flush.",0
"If additional callbacks are queued before the queue is flushed, they",0
will be processed by this flush that we are scheduling.,0
test for web worker but not in IE10,0
node,0
node version 0.10.x displays a deprecation warning when nextTick is used recursively,0
see https://github.com/cujojs/when/issues/410 for details,0
vertx,0
web worker,0
Store setTimeout reference so es6-promise will be unaffected by,0
other code modifying setTimeout (like sinon.useFakeTimers()),0
Decide what async method to use to triggering processing of queued callbacks:,0
value === 1,0
value === 1,0
noop,0
"The array here would be [ 1, 2, 3 ];",0
Code here never runs because there are rejected promises!,0
"error.message === ""2""",0
result === 'promise 2' because it was resolved before promise1,0
was resolved.,0
Code here never runs,0
reason.message === 'promise 2' because promise 2 became rejected before,0
promise 1 became fulfilled,0
Code here doesn't run because the promise is rejected!,0
reason.message === 'WHOOPS',0
Code here doesn't run because the promise is rejected!,0
reason.message === 'WHOOPS',0
on success,0
on failure,0
on fulfillment,0
on rejection,0
on fulfillment,0
on rejection,0
user is available,0
"user is unavailable, and you are given the reason why",0
"If `findUser` fulfilled, `userName` will be the user's name, otherwise it",0
will be `'default name'`,0
never reached,0
"if `findUser` fulfilled, `reason` will be 'Found user, but still unhappy'.",0
"If `findUser` rejected, `reason` will be '`findUser` rejected and we're unhappy'.",0
never reached,0
never reached,0
The `PedgagocialException` is propagated all the way down to here,0
The user's comments are now available,0
"If `findCommentsByAuthor` fulfills, we'll have the value here",0
"If `findCommentsByAuthor` rejects, we'll have the reason here",0
success,0
failure,0
failure,0
success,0
success,0
failure,0
success,0
failure,0
failure,0
success,0
found books,0
something went wrong,0
synchronous,0
something went wrong,0
async with promises,0
something went wrong,0
silently ignored,0
Strange compat..,0
# sourceMappingURL=es6-promise.map,0
removed by extract-text-webpack-plugin,0
bind button events if any,0
ugly fix for progressbar display bug,1
it's in the queue,0
API functions,0
Document visibility change controller,0
shim for using process in browser,0
cached from whatever global is present so that test runners that stub it,0
don't break things.  But we need to wrap it in a try catch in case it is,0
wrapped in strict mode code which doesn't define any globals.  It's inside a,0
function because try/catches deoptimize in certain engines.,0
normal enviroments in sane situations,0
if setTimeout wasn't available but was latter defined,0
when when somebody has screwed with setTimeout but no I.E. maddness,0
When we are in I.E. but the script has been evaled so I.E. doesn't trust the global object when called normally,0
"same as above but when it's a version of I.E. that must have the global object for 'this', hopfully our context correct otherwise it will throw a global error",0
normal enviroments in sane situations,0
if clearTimeout wasn't available but was latter defined,0
when when somebody has screwed with setTimeout but no I.E. maddness,0
When we are in I.E. but the script has been evaled so I.E. doesn't  trust the global object when called normally,0
"same as above but when it's a version of I.E. that must have the global object for 'this', hopfully our context correct otherwise it will throw a global error.",0
Some versions of I.E. have different rules for clearTimeout vs setTimeout,0
v8 likes predictible objects,0
This works in non-strict mode,0
This works if eval is allowed (see CSP),0
This works if the window reference is available,0
"g can still be undefined, but nothing to do about it...",0
"We return undefined, instead of nothing here, so it's",0
easier to handle this case. if(!global) { ...},0
# sourceMappingURL=noty.js.map,0
Trim the opening space.,0
Replace the class name.,0
Trim the opening and closing spaces.,0
Opera 12.10 and Firefox 18 and later support,0
fix for Chrome < 45,0
bind button events if any,0
ugly fix for progressbar display bug,1
it's in the queue,0
API functions,0
Document visibility change controller,0
# sourceMappingURL=bootstrap.bundle.min.js.map,0
# sourceMappingURL=bootstrap.min.js.map,0
eslint-disable-next-line no-bitwise,0
TODO: Remove in v5,1
Public,0
Public,0
Public,0
Don't call next when the page isn't visible,0
or the carousel or its parent isn't visible,0
"If it's a touch-enabled device, mouseenter/leave are fired as",0
part of the mouse compatibility events on first tap - the carousel,0
would stop cycling until user tapped out of it;,0
"here, we listen for touchend, explicitly pause the carousel",0
"(as if it's the second time we tap on it, mouseenter compat event",0
is NOT fired) and after a timeout (to allow for mouse compatibility,0
events to fire) we explicitly restart cycling,0
"Some weirdness is happening, so we bail",0
Public,0
preventDefault only for <a> elements (which change the URL) not inside the collapsible element,0
NOTE: 1 DOM access here,0
"Return body, `getScroll` will take care to get the correct `scrollTop` from it",0
Firefox want us to check `-x` and `-y` variations as well,0
NOTE: 1 DOM access here,0
Skip hidden elements which don't have an offsetParent,0
.offsetParent will return the closest TD or TABLE in case,0
"no offsetParent is present, I hate this job...",0
This check is needed to avoid errors in case one of the elements isn't defined for any reason,0
"Here we make sure to give as ""start"" the element that comes first in the DOM",0
Get common ancestor container,0
Both nodes are inside #document,0
"one of the nodes is inside shadowDOM, find which one",0
"IE10 10 FIX: Please, don't ask, the element isn't",0
considered in DOM in some circumstances...,0
This isn't reproducible in IE10 compatibility mode of IE11,0
subtract scrollbar size from sizes,0
"if an hypothetical scrollbar is detected, we must be sure it's not a `border`",0
we make this check conditional for performance reasons,0
"In cases where the parent is fixed, we must ignore negative scroll in offset calc",0
Subtract margins of documentElement in case it's being used as parent,0
we do this only on HTML because it's the only element that behaves,0
differently when margins are applied to it. The margins are included in,0
"the box of the documentElement, in the other cases not.",0
Attach marginTop and marginLeft because in some circumstances we may need them,0
This check is needed to avoid errors in case one of the elements isn't defined for any reason,0
NOTE: 1 DOM access here,0
Handle viewport case,0
Handle other cases based on DOM element used as boundaries,0
"In case of HTML, we need a different computation",0
"for all the other DOM elements, this one is good",0
Add paddings,0
Get popper node sizes,0
"Add position, width and height to our offsets object",0
depending by the popper placement we have to compute its offsets slightly differently,0
use native find if supported,0
use `filter` to obtain the same behavior of `find`,0
use native findIndex if supported,0
use `find` + `indexOf` if `findIndex` isn't supported,0
eslint-disable-line dot-notation,0
Add properties to offsets to make them a complete clientRect object,0
we do this before each modifier to make sure the previous one doesn't,0
mess with these values,0
"if popper is destroyed, don't perform any further update",0
compute reference element offsets,0
"compute auto placement, store placement inside the data object,",0
modifiers will be able to edit `placement` if needed,0
and refer to originalPlacement to know the original value,0
store the computed placement inside `originalPlacement`,0
compute the popper offsets,0
run the modifiers,0
the first `update` will call `onCreate` callback,0
the other ones will call `onUpdate` callback,0
touch DOM only if `applyStyle` modifier is enabled,0
remove the popper if user explicity asked for the deletion on destroy,0
do not use `remove` because IE11 doesn't support it,0
Resize event listener on window,0
Scroll event listener on scroll parents,0
Remove resize event listener on window,0
Remove scroll event listener on scroll parents,0
Reset state,0
add unit if the value is numeric and is one of the following,0
"any property present in `data.styles` will be applied to the popper,",0
in this way we can make the 3rd party modifiers add custom styles to it,0
"Be aware, modifiers could override the properties defined in the previous",0
lines of this modifier!,0
"any property present in `data.attributes` will be applied to the popper,",0
they will be set as HTML attributes of the element,0
if arrowElement is defined and arrowStyles has some properties,0
compute reference element offsets,0
"compute auto placement, store placement inside the data object,",0
modifiers will be able to edit `placement` if needed,0
and refer to originalPlacement to know the original value,0
Apply `position` to popper before anything else because,0
without the position applied we can't guarantee correct computations,0
Remove this legacy support in Popper.js v2,0
Styles,0
Avoid blurry text by using full pixel integers.,0
"For pixel-perfect positioning, top/bottom prefers rounded",0
"values, while left/right prefers floored values.",0
"if gpuAcceleration is set to `true` and transform is supported,",0
we use `translate3d` to apply the position to the popper we,0
automatically use the supported prefixed version if needed,0
"now, let's make a step back and look at this code closely (wtf?)",1
"If the content of the popper grows once it's been positioned, it",0
may happen that the popper gets misplaced because of the new content,0
overflowing its reference element,0
"To avoid this problem, we provide two options (x and y), which allow",0
the consumer to define the offset origin.,0
"If we position a popper on top of a reference element, we can set",0
`x` to `top` to make the popper grow towards its top instead of,0
its bottom.,0
"othwerise, we use the standard `top`, `left`, `bottom` and `right` properties",0
Attributes,0
"Update `data` attributes, styles and arrowStyles",0
arrow depends on keepTogether in order to work,0
"if arrowElement is a string, suppose it's a CSS selector",0
"if arrowElement is not found, don't run the modifier",0
if the arrowElement isn't a query selector we must check that the,0
provided DOM node is child of its popper node,0
,0
extends keepTogether behavior making sure the popper and its,0
reference have enough pixels in conjuction,0
,0
top/left side,0
bottom/right side,0
compute center of the popper,0
Compute the sideValue using the updated popper offsets,0
take popper margin in account because we don't have this info available,0
prevent arrowElement from being placed not contiguously to its popper,0
Get rid of `auto` `auto-start` and `auto-end`,0
"if `inner` modifier is enabled, we can't use the `flip` modifier",0
"seems like flip is trying to loop, probably there's not enough space on any of the flippable sides",0
using floor because the reference offsets may contain decimals we are not going to consider here,0
flip the variation if required,0
this boolean to detect any flip loop,0
"this object contains `position`, we want to preserve it along with",0
any additional property we may add in the future,0
separate value from unit,0
"If it's not a number it's an operator, I guess",0
"if is a vh or vw, we calculate the size based on the viewport",0
"if is an explicit pixel unit, we get rid of the unit and keep the value",0
"if is an implicit unit, it's px, and we return just the value",0
Use height if placement is left or right and index is 0 otherwise use width,0
in this way the first offset will use an axis and the second one,0
will use the other one,0
Split the offset string to obtain a list of values and operands,0
"The regex addresses values with the plus or minus sign in front (+10, -20, etc)",0
Detect if the offset string contains a pair of values or a single one,0
they could be separated by comma or space,0
"If divider is found, we divide the list of values and operands to divide",0
them by ofset X and Y.,0
Convert the values with units to absolute pixels to allow our computations,0
Most of the units rely on the orientation of the popper,0
This aggregates any `+` or `-` sign that aren't considered operators,0
"e.g.: 10 + +5 => [10, +, +5]",0
Here we convert the string values into number values (in px),0
Loop trough the offsets arrays and execute the operations,0
"If offsetParent is the reference element, we really want to",0
go one step up and use the next offsetParent as reference to,0
avoid to make this modifier completely useless and look like broken,0
NOTE: DOM access here,0
resets the popper's position so that the document size can be calculated excluding,0
the size of the popper element itself,0
NOTE: DOM access here,0
restores the original style properties after the offsets have been computed,0
"if shift shiftvariation is specified, run the modifier",0
Avoid unnecessary DOM access if visibility hasn't changed,0
Avoid unnecessary DOM access if visibility hasn't changed,0
Utils,0
Methods,0
"make update() debounced, so that it only runs at most once-per-tick",0
with {} we create a new object with the options inside it,0
init state,0
get reference and popper elements (allow jQuery wrappers),0
Deep merge modifiers options,0
Refactoring modifiers' list (Object => Array),0
sort the modifiers by order,0
modifiers have the ability to execute arbitrary code when Popper.js get inited,0
such code is executed in the same order of its modifier,0
they could add new properties to their options configuration,0
BE AWARE: don't add options to `options.modifiers.name` but to `modifierOptions`!,0
fire the first update to position the popper in the right place,0
"setup event listeners, they will take care of update the position in specific situations",0
We can't use class properties because they don't get listed in the,0
class prototype and break stuff like Sinon stubs,0
Public,0
"to allow the menu to ""escape"" the scroll parent's boundaries",0
https://github.com/twbs/bootstrap/issues/24251,0
empty mouseover listeners to the body's immediate children;,0
only needed because of broken event delegation on iOS,0
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html,0
empty mouseover listeners we added for iOS support,0
If not input/textarea:,0
- And not a key in REGEXP_KEYDOWN => not a dropdown command,0
If input/textarea:,0
- If space key => not a dropdown command,0
- If key is other than escape,0
- If key is not up or down => not a dropdown command,0
- If trigger inside the menu => not a dropdown command,0
Up,0
Down,0
Public,0
Don't move modal's DOM position,0
the following methods are used to handle overflowing modals,0
todo (fat): these should probably be refactored out of modal.js,1
----------------------------------------------------------------------,0
Note: DOMNode.style.paddingRight returns the actual value or '' if not set,0
while $(DOMNode).css('padding-right') returns the calculated value or 0 if not set,0
Restore fixed content padding,0
thx d.walsh,0
Only register focus restorer if modal will actually get shown,0
Public,0
empty mouseover listeners to the body's immediate children;,0
only needed because of broken event delegation on iOS,0
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html,0
empty mouseover listeners we added for iOS support,0
Content is a DOM node or a jQuery,0
Overrides,0
Getters,0
Public,0
TODO (fat): remove sketch reliance on jQuery position/offset,1
Set triggered link as active,0
With both <ul> and <nav> markup a parent is the previous sibling of any nav ancestor,0
Public,0
# sourceMappingURL=bootstrap.bundle.js.map,0
eslint-disable-next-line no-bitwise,0
TODO: Remove in v5,1
Public,0
Public,0
Public,0
Don't call next when the page isn't visible,0
or the carousel or its parent isn't visible,0
"If it's a touch-enabled device, mouseenter/leave are fired as",0
part of the mouse compatibility events on first tap - the carousel,0
would stop cycling until user tapped out of it;,0
"here, we listen for touchend, explicitly pause the carousel",0
"(as if it's the second time we tap on it, mouseenter compat event",0
is NOT fired) and after a timeout (to allow for mouse compatibility,0
events to fire) we explicitly restart cycling,0
"Some weirdness is happening, so we bail",0
Public,0
preventDefault only for <a> elements (which change the URL) not inside the collapsible element,0
Public,0
"to allow the menu to ""escape"" the scroll parent's boundaries",0
https://github.com/twbs/bootstrap/issues/24251,0
empty mouseover listeners to the body's immediate children;,0
only needed because of broken event delegation on iOS,0
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html,0
empty mouseover listeners we added for iOS support,0
If not input/textarea:,0
- And not a key in REGEXP_KEYDOWN => not a dropdown command,0
If input/textarea:,0
- If space key => not a dropdown command,0
- If key is other than escape,0
- If key is not up or down => not a dropdown command,0
- If trigger inside the menu => not a dropdown command,0
Up,0
Down,0
Public,0
Don't move modal's DOM position,0
the following methods are used to handle overflowing modals,0
todo (fat): these should probably be refactored out of modal.js,1
----------------------------------------------------------------------,0
Note: DOMNode.style.paddingRight returns the actual value or '' if not set,0
while $(DOMNode).css('padding-right') returns the calculated value or 0 if not set,0
Restore fixed content padding,0
thx d.walsh,0
Only register focus restorer if modal will actually get shown,0
Public,0
empty mouseover listeners to the body's immediate children;,0
only needed because of broken event delegation on iOS,0
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html,0
empty mouseover listeners we added for iOS support,0
Content is a DOM node or a jQuery,0
Overrides,0
Getters,0
Public,0
TODO (fat): remove sketch reliance on jQuery position/offset,1
Set triggered link as active,0
With both <ul> and <nav> markup a parent is the previous sibling of any nav ancestor,0
Public,0
# sourceMappingURL=bootstrap.js.map,0
Public,0
empty mouseover listeners to the body's immediate children;,0
only needed because of broken event delegation on iOS,0
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html,0
empty mouseover listeners we added for iOS support,0
Content is a DOM node or a jQuery,0
# sourceMappingURL=tooltip.js.map,0
Public,0
preventDefault only for <a> elements (which change the URL) not inside the collapsible element,0
# sourceMappingURL=collapse.js.map,0
Public,0
Don't move modal's DOM position,0
the following methods are used to handle overflowing modals,0
todo (fat): these should probably be refactored out of modal.js,1
----------------------------------------------------------------------,0
Note: DOMNode.style.paddingRight returns the actual value or '' if not set,0
while $(DOMNode).css('padding-right') returns the calculated value or 0 if not set,0
Restore fixed content padding,0
thx d.walsh,0
Only register focus restorer if modal will actually get shown,0
# sourceMappingURL=modal.js.map,0
Overrides,0
Getters,0
# sourceMappingURL=popover.js.map,0
Public,0
# sourceMappingURL=tab.js.map,0
Public,0
"to allow the menu to ""escape"" the scroll parent's boundaries",0
https://github.com/twbs/bootstrap/issues/24251,0
empty mouseover listeners to the body's immediate children;,0
only needed because of broken event delegation on iOS,0
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html,0
empty mouseover listeners we added for iOS support,0
If not input/textarea:,0
- And not a key in REGEXP_KEYDOWN => not a dropdown command,0
If input/textarea:,0
- If space key => not a dropdown command,0
- If key is other than escape,0
- If key is not up or down => not a dropdown command,0
- If trigger inside the menu => not a dropdown command,0
Up,0
Down,0
# sourceMappingURL=dropdown.js.map,0
Public,0
Don't call next when the page isn't visible,0
or the carousel or its parent isn't visible,0
"If it's a touch-enabled device, mouseenter/leave are fired as",0
part of the mouse compatibility events on first tap - the carousel,0
would stop cycling until user tapped out of it;,0
"here, we listen for touchend, explicitly pause the carousel",0
"(as if it's the second time we tap on it, mouseenter compat event",0
is NOT fired) and after a timeout (to allow for mouse compatibility,0
events to fire) we explicitly restart cycling,0
"Some weirdness is happening, so we bail",0
# sourceMappingURL=carousel.js.map,0
Public,0
# sourceMappingURL=alert.js.map,0
# sourceMappingURL=index.js.map,0
Public,0
# sourceMappingURL=button.js.map,0
Public,0
TODO (fat): remove sketch reliance on jQuery position/offset,1
Set triggered link as active,0
With both <ul> and <nav> markup a parent is the previous sibling of any nav ancestor,0
# sourceMappingURL=scrollspy.js.map,0
eslint-disable-next-line no-bitwise,0
TODO: Remove in v5,1
# sourceMappingURL=util.js.map,0
private,0
Protected,0
Getters,0
Public,0
If this is a touch-enabled device we add extra,0
empty mouseover listeners to the body's immediate children;,0
only needed because of broken event delegation on iOS,0
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html,0
If this is a touch-enabled device we remove the extra,0
empty mouseover listeners we added for iOS support,0
Protected,0
Content is a DOM node or a jQuery,0
Private,0
Static,0
Getters,0
Public,0
Private,0
It's a jQuery object,0
Static,0
preventDefault only for <a> elements (which change the URL) not inside the collapsible element,0
Getters,0
Public,0
Private,0
Don't move modal's DOM position,0
----------------------------------------------------------------------,0
the following methods are used to handle overflowing modals,0
todo (fat): these should probably be refactored out of modal.js,1
----------------------------------------------------------------------,0
Note: DOMNode.style.paddingRight returns the actual value or '' if not set,0
while $(DOMNode).css('padding-right') returns the calculated value or 0 if not set,0
Adjust fixed content padding,0
Adjust sticky content margin,0
Adjust body padding,0
Restore fixed content padding,0
Restore sticky content,0
Restore body padding,0
Static,0
Only register focus restorer if modal will actually get shown,0
Getters,0
Overrides,0
We use append for html objects to maintain js events,0
Private,0
Static,0
Getters,0
Public,0
Private,0
Static,0
Getters,0
Public,0
Disable totally Popper.js for Dropdown in Navbar,0
Check if it's jQuery element,0
"If boundary is not `scrollParent`, then set position to `static`",0
"to allow the menu to ""escape"" the scroll parent's boundaries",0
https://github.com/twbs/bootstrap/issues/24251,0
If this is a touch-enabled device we add extra,0
empty mouseover listeners to the body's immediate children;,0
only needed because of broken event delegation on iOS,0
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html,0
Private,0
Handle dropup,0
Disable Popper.js if we have a static display,0
Static,0
If this is a touch-enabled device we remove the extra,0
empty mouseover listeners we added for iOS support,0
eslint-disable-next-line complexity,0
If not input/textarea:,0
- And not a key in REGEXP_KEYDOWN => not a dropdown command,0
If input/textarea:,0
- If space key => not a dropdown command,0
- If key is other than escape,0
- If key is not up or down => not a dropdown command,0
- If trigger inside the menu => not a dropdown command,0
Getters,0
Public,0
Don't call next when the page isn't visible,0
or the carousel or its parent isn't visible,0
Private,0
"If it's a touch-enabled device, mouseenter/leave are fired as",0
part of the mouse compatibility events on first tap - the carousel,0
would stop cycling until user tapped out of it;,0
"here, we listen for touchend, explicitly pause the carousel",0
"(as if it's the second time we tap on it, mouseenter compat event",0
is NOT fired) and after a timeout (to allow for mouse compatibility,0
events to fire) we explicitly restart cycling,0
"Some weirdness is happening, so we bail",0
Static,0
Getters,0
Public,0
Private,0
Static,0
Getters,0
Public,0
Static,0
Getters,0
Public,0
TODO (fat): remove sketch reliance on jQuery position/offset,1
Private,0
eslint-disable-next-line arrow-body-style,0
Set triggered link as active,0
Set triggered links parents as active,0
With both <ul> and <nav> markup a parent is the previous sibling of any nav ancestor,0
Handle special case when .nav-link is inside .nav-item,0
Static,0
Shoutout AngusCroll (https://goo.gl/pxwQGp),0
eslint-disable-next-line no-bitwise,0
Get transition-duration of the element,0
Return 0 if element or transition duration is not found,0
"If multiple durations are defined, take the first",0
TODO: Remove in v5,1
"Determine the factor, which shifts the decimal point of x",0
just behind the last significant digit,0
"Shift decimal point by multiplicatipon with factor, flooring, and",0
division by factor,0
_ = pretty_print_object(obj),0
Sloppy that I'm making a module-wide change here...,1
Determine the coordinates of the intersection rectangle,0
The intersection of two axis-aligned bounding boxes is always an,0
axis-aligned bounding box,0
Compute the area of both AABBs,0
Compute the intersection over union by taking the intersection,0
area and dividing it by the sum of prediction + ground-truth,0
areas - the intersection area.,0
######,0
,0
cct_json_utils.py,0
,0
Utilities for working with COCO Camera Traps .json databases,0
,0
Format spec:,0
,0
https://github.com/Microsoft/CameraTraps/blob/master/data_management/README.md#coco-cameratraps-format,0
,0
######,0
%% Constants and imports,0
%% Classes,0
Collect all names,0
Make names unique and sort,0
cast location to string as the entries in locations are strings,0
Convert classnames to lowercase to simplify comparisons later,0
Normalize paths to simplify comparisons later,0
"Make custom replacements in filenames, typically used to",0
accommodate changes in root paths between DB construction and use,0
## Build useful mappings to facilitate working with the DB,0
Category ID <--> name,0
Image filename --> ID,0
"Each image can potentially multiple annotations, hence using lists",0
Image ID --> image object,0
Image ID --> annotations,0
...__init__,0
...class IndexedJsonDb,0
,0
cct_json_to_filename_json.py,0
,0
Converts a .json file in COCO Camera Traps format to a .json-formatted list of,0
relative file names.,0
,0
%% Constants and environment,0
%% Main function,0
"json.dump(s,open(outputFilename,'w'))",0
%% Command-line driver,0
%% Interactive driver,0
%%,0
,0
download_lila_subset.py,0
,0
"Example of how to download a list of files from LILA, e.g. all the files",0
in a data set corresponding to a particular species.,0
,0
%% Constants and imports,0
SAS URLs come from:,0
,0
http://lila.science/?attachment_id=792,0
,0
"In this example, we're using the Missouri Camera Traps data set",0
This assumes you've downloaded the metadata file from LILA,0
"We will demonstrate two approaches to downloading, one that loops over files",0
"and downloads directly in Python, another that uses AzCopy.",0
,0
AzCopy will generally be more performant and supports resuming if the,0
transfers are interrupted.  It assumes that azcopy is on the system path.,0
Number of concurrent download threads (when not using AzCopy),0
%% Environment prep and derived constants,0
%% Open the metadata file,0
%% Build a list of image files (relative path names) that match the target species,0
Retrieve the category ID we're interested in,0
Retrieve all the images that match that category,0
Retrieve image file names,0
%% Support functions,0
print('Skipping file {}'.format(fn)),0
"print('Downloading {} to {}'.format(url,target_file))",0
%% Download those image files,0
"Write out a list of files, and use the azcopy ""list-of-files"" option to download those files",0
this azcopy feature is unofficially documented at https://github.com/Azure/azure-storage-azcopy/wiki/Listing-specific-files-to-transfer,0
Loop over files,0
,0
add_bounding_boxes_to_megadb.py,0
,0
"Given pseudo-jsons containing the bounding box annotations made by iMerit, add them to",0
a list of entries from the MegaDB sequences table.,0
the category map that comes in the pseudo-jsons,0
annotation_path points to a directory containing annotation pseudo-jsons,0
annotation_path points to a single annotation pseudo-json,0
each row in this pseudo-json is a COCO formatted entry for an image sequence,0
iMerit calls this field image_id; some of these are URL encoded,0
dataset = image_ref.split('dataset')[1].split('.')[0]  # prior to batch 10,0
lower-case all image filenames !,0
image_filename = image_ref.split('.img')[1].lower()  # prior to batch 10,0
dataset = image_ref.split('dataset')[1].split('.')[0]  # prior to batch 10,0
image_filename = image_ref.split('.img')[1].lower()  # prior to batch 10,0
how many boxes of each category?,0
check that all sequences are for a single dataset; each may need adjustment to how image,0
identifiers are mapped,0
,0
add_bounding_boxes_to_json.py,0
,0
This script takes a image database in the COCO Camera Traps format and merges in a set of bounding,0
box annotations in the format that iMerit uses (a .json where actually only each row is a valid json).,0
,0
"If you need to update an existing bbox database, please get all the original annotation files and",0
re-generate from scratch,0
,0
%% Imports,0
%% Configurations and paths,0
images database,0
output bboxes database,0
annotation files (pseudo json) obtained from our annotation vendor that contain annotations for this dataset,0
None or a string or tuple of strings that is the prefix to all file_name of interest / in this dataset in the annotation files,0
functions for mapping the image_id in the annotation files (pseudo jsons) to the image_id used in the image DB,0
our img_id doesn't contain frame info,0
"batch3 - ""file_name"":""ASG0000019_0_S1_B06_R1_PICT0007.JPG""",0
"batch5 and 7 - ""file_name"":""datasetsnapshotserengeti.seqASG000002m-frame0.imgS1_B06_R1_PICT0056.JPG""",0
sometimes - 'datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG',0
"id in DB (old_token): 'S6/J01/J01_R1/S6_J01_R1_IMAG0001', 'S1/B05/B05_R1/S1_B05_R1_PICT0036'",0
specify which one to use for your dataset here,0
%% Load the image database and fill in DB info for the output bbox database,0
load the images database,0
%% Find the height and width of images from the annotation files,0
,0
...if they are not available in the images DB,0
each row in this pseudo-json is a COCO formatted entry for an image sequence,0
%% Other functions required by specific datasets,0
the IDFG image database does not include images from unlabeled folders that were annotated with bounding boxes,0
%% Create the bbox database from all annotation files pertaining to this dataset,0
"for the incoming annotations, look up by category name (common) and convert to the numerical id used in our databases",0
"for each annotation pseudo-json, check that the image it refers to exists in the original database",0
each row in this pseudo-json is a COCO formatted entry for an image sequence,0
check that entry is for this dataset,0
category map for this entry in the annotation file - usually the same across all entries but just in case,0
rspb_add_image_entry(img_id),0
use the image length and width in the image DB,0
"[top left x, top left y, width, height] in relative coordinates",0
"add all images that have been sent to annotation, some of which may be empty of bounding boxes",0
rspb_add_image_entry(db_image_id),0
print('Image not found in origin dir at {}'.format(origin_path)),0
currently only supports one endpoint,0
if need to re-download a dataset's images in case of corruption,0
file_list_to_download = [i for i in file_list_to_download if i['dataset'] == 'rspb_gola'],0
need to create a new blob service for this dataset,0
"the SAS token can be just for the container, not the storage account",0
- will be fine for accessing files in that container later,0
%% Common queries,0
This query is used when preparing tfrecords for object detector training,0
We do not want to get the whole seq obj where at least one image has bbox because some images in that sequence,0
will not be bbox labeled so will be confusing,0
"For public datasets to be converted to the CCT format, we get the whole seq object because",0
sequence level attributes need to be included too. megadb/converters/megadb_to_cct.py handles,0
the case of bbox-only JSONs with the flag exclude_species_class,0
%% Parameters,0
use False for when the results file will be too big to store in memory or in a single JSON.,0
%% Script,0
initialize Cosmos DB client,0
execute the query,0
loop through and save the results,0
schema already checks that the min possible value of frame_num is 0,0
"if there are more than one image item, each needs a frame_num",0
checks across all sequence items,0
per sequence item checks,0
load the schema,0
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory,0
load the schema,0
https://stackoverflow.com/questions/3718657/how-to-properly-determine-current-script-directory,0
checks across all datasets items,0
check for expiry date of the SAS keys,0
update the sequences,0
"`id` is from the database, as well as all attributes starting with _",0
"if valuable sequence information is available, add them to the image",0
required fields for an image object,0
add seq-level class labels for this image,0
add other sequence-level properties to each image too,0
add other image-level properties,0
... for im in seq['images'],0
... for seq in mega_db,0
consolidate categories,0
some property names have changed in the new schema,0
a dummy sequence ID will be generated if the image entry does not have a seq_id field,0
seq_id only needs to be unique within this dataset; MegaDB does not rely on it as the _id field,0
"""annotations"" fields are opened and have its sub-field surfaced one level up",0
set the `dataset` property on each sequence to the provided dataset_name,0
check that the location field is the same for all images in a sequence,0
check which fields in a CCT image entry are sequence-level,0
image-level properties that really should be sequence-level,0
need to add (misidentified) seq properties not present for each image in a sequence to img_level_properties,0
"(some properties act like flags - all have the same value, but not present on each img)",0
add the sequence-level properties to the sequence objects,0
not every sequence have to have all the seq_level_properties,0
get the value of this sequence-level property from the first image entry,0
check which fields are really dataset-level and should be included in the dataset table instead.,0
delete sequence-level properties that should be dataset-level,0
make all `class` fields lower-case; cast `seq_id` to type string in case they're integers,0
%% validation,0
"at first a dict of image_id: image_obj with annotations embedded, then it becomes",0
an array of image objects,0
%% integrate the image DB,0
takes in image entries and species and other annotations in the image DB,0
convert the species category to explicit string name,0
there may be other fields in the annotation object,0
these fields should already be gotten from the image object,0
%% integrate the bbox DB,0
add any images that are not in the image DB,0
also add any fields in the image object that are not present already,0
add bbox to the annotations field,0
for any newly added images,0
"'bbox_abs': bbox_anno['bbox'],",0
not keeping height and width,0
,0
jb_csv_to_json.py,0
,0
Convert a particular .csv file to CCT format.  Images were not available at,0
"the time I wrote this script, so this is much shorter than other scripts",0
in this folder.,0
,0
%% Constants and environment,0
%% Read source data,0
%% Confirm filename uniqueness (this data set has one label per image),0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
%% Create CCT dictionaries,0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
imageName = imageFilenames[0],0
"As per above, this is convenient and appears to be true; asserting to be safe",0
"Filenames look like ""290716114012001a1116.jpg""",0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
%% Create info struct,0
%% Write output,0
%% Sanity-check,0
%% Imports and constants,0
%% Load data,0
"with open(image_json,'r') as f:",0
data = json.load(f),0
%% Sanity-check data,0
%% Label previews,0
%% Collect images to annotate,0
%% Sort by sequence and frame,0
%% Copy to a folder by GUID,0
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension,0
im = images_to_annotate[0],0
...for each image,0
%% Write out the annotation list,0
,0
"Import a Snapshot Safari project (one project, one season)",0
,0
Before running this script:,0
,0
"* Mount the blob container where the images live, or copy the",0
images to local storage,0
,0
What this script does:,0
,0
* Creates a .json file,0
* Creates zip archives of the season without humans.,0
* Copies animals and humans to separate folders,0
,0
After running this script:,0
,0
* Create or update LILA page,0
* Push zipfile and unzipped images to LILA,0
* Push unzipped humans to wildlifeblobssc,0
* Delete images from UMN uplaod storage,0
%% Imports,0
From ai4eutils,0
From CameraTraps,0
%% Constants,0
project_name = 'XXX'; season_name = 'S1'; project_friendly_name = 'Snapshot Unknown',0
project_name = 'KRU'; season_name = 'S1'; project_friendly_name = 'Snapshot Kruger',0
project_name = 'CDB'; season_name = 'S1'; project_friendly_name = 'Snapshot Camdeboo',0
project_name = 'MTZ'; season_name = 'S1'; project_friendly_name = 'Snapshot Mountain Zebra',0
project_name = 'ENO'; season_name = 'S1'; project_friendly_name = 'Snapshot Enonkishu',0
project_name = 'KAR'; season_name = 'S1'; project_friendly_name = 'Snapshot Karoo',0
project_name = 'KGA'; season_name = 'S1'; project_friendly_name = 'Snapshot Kgalagadi',0
%% Folder/file creation,0
E.g. KRU_S1,0
E.g. Z:\KRU,0
E.g. Z:\KRU\KRU_S1,0
Contains annotations for each capture event (sequence),0
Maps image IDs to filenames; each line looks like:,0
,0
"KRU_S1#1#1#2,3,KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0004.JPG",0
"Total number of each answer to each question, e.g. total number of times each species was identified",0
,0
Not used here,1
Create output folders,0
Images will be placed in a season-specific folder inside this (the source data includes,0
this in path names),0
%% Load metadata files,0
%% Convert to dictionaries (prep),0
%% Convert to dictionaries (loops),0
"TODO: iterrows() is a terrible way to do this, but this is one of those days",1
"where I want to get this done, not get better at Python.",0
irow = 0; row = image_table.iloc[0],0
"Loaded as an int64, converting to int here",0
...for each row in the image table,0
Make sure image IDs are what we think they are,0
...for each row in the annotation table,0
%% Take a look at categories (just sanity-checking),0
print('\nCategories by species:'),0
pp.pprint(categories_by_species),0
%% Fill in some image fields we didn't have when we created the image table,0
"width, height, corrupt, seq_num_frames, location, datetime",0
Every annotation in this list should have the same sequence ID,0
"Figure out ""seq_num_frames"", which really should be done in a separate lopp;",0
there's no reason to do this redundantly for every image,0
Every image in this sequence should point back to the same equence,0
Every annotation in this list should have the same location,0
Every annotation in this list should have the same datetime,0
Is this image on disk?,0
iImage = 0; im = images[0],0
...for each image,0
"images_processed = pool.map(process_image, images)",0
"images_processed = list(tqdm(pool.imap_unordered(process_image, images), total=len(images)))",0
%% Count missing/corrupted images,0
%% Print distribution of sequence lengths (sanity-check),0
%% Replicate annotations across images,0
iAnn = 0; ann = annotations[iAnn],0
%% See what files are on disk but not annotated,0
%% Sanity-check image and annotation uniqueness,0
%% Minor updates to fields,0
%% Write .json file,0
%% Create a list of human files,0
ann = annotations[0],0
%% Create public archive and public/private folders,0
im = images[0],0
E.g. KRU_S1/1/1_R1/KRU_S1_1_R1_IMAG0001.JPG,0
Copy to private output folder,0
Add to zipfile,0
Possibly start a new archive,0
Copy to public output folder,0
...for each image,0
%% Sanity-check .json file,0
"This will produce some validation errors, because this zipfile doesn't include humans",0
%% Zip up .json and .csv files,0
%% When I skip to this part (using a pre-rendered .json file),0
%%,0
%%,0
ann = annotations[0],0
%% Summary prep for LILA,0
"%% Generate preview, sanity-check labels",0
"viz_options.classes_to_include = ['jackalblackbacked','bustardkori']",0
%% Scrap,0
%% Find annotations for a particular image,0
%% Write a list of missing images,0
,0
save_the_elephants_survey_A.py,0
,0
Convert the .csv file provided for the Save the Elephants Survey A data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
Handle all unstructured fields in the source data as extra fields in the annotations,0
"photo_type really should be an image property, but there are a few conflicts",0
that forced me to handle it as an annotation proprerty,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
Maps relative paths to row indices in input_metadata,0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
Ignore directories,0
%% Make sure the multiple-annotation cases make sense,0
%%,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
Force the empty category to be ID 0,0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image],0
Example filename:,0
,0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG',0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG',0
i_row = row_indices[0],0
There are a small handful of datetime mismatches across annotations,0
for the same image,0
assert im['datetime'] == timestamp,0
Special cases based on the 'photo type' field,0
Various spellings of 'community',0
Have we seen this category before?,0
Create an annotation,0
fieldname = list(mapped_fields.keys())[0],0
...for each row,0
...for each image,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
%% Scrap,0
%% Find unique photo types,0
##,0
,0
timelapse_csv_set_to_json.py,0
,0
Given a directory full of reasonably-consistent Timelapse-exported,0
".csvs, assemble a CCT .json.",0
,0
"Assumes that you have a list of all files in the directory tree, including",0
image and .csv files.,0
,0
##,0
%% Constants and imports,0
Text file with relative paths to all files (images and .csv files),0
"%% Read file list, make a list of all image files and all .csv files",0
"%% Verify column consistency, create a giant array with all rows from all .csv files",0
i_csv = 0; csv_filename = csv_files[0],0
%% Prepare some data structures we'll need for mapping image rows in .csv files to actual image files,0
Enumerate all folders containing image files,0
"In this data set, a site folder looks like:",0
,0
Processed Images\\site_name,0
%% Map .csv files to candidate camera folders,0
fn = valid_csv_files[0],0
"Some site folders appear as ""XXNNNN"", some appear as ""XXNNNN_complete""",0
...for each .csv file,0
%% Map camera folders to candidate image folders,0
%% Main loop over labels (prep),0
Force the empty category to be ID 0,0
Images that are marked empty and also have a species label,0
%% Main loop over labels (loop),0
i_row = 0; row = input_metadata.iloc[i_row],0
"for i_row,row in input_metadata.iterrows():",0
"Usually this is just a single folder name, sometimes it's a full path,",0
which we don't want,0
Check whether this file exists on disk,0
Retrieve image width and height,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each row in the big table of concatenated .csv files,0
%% Fix cases where an image was annotated as 'unlabeled' and as something else,0
This annotation is 'unlabeled',0
Was there another category associated with this image?,0
%% Check for un-annnotated images,0
Enumerate all images,0
list(relative_path_to_image.keys())[0],0
%% Create info struct,0
%% Write output,0
%% Sanity-check the database's integrity,0
%% Render a bunch of images to make sure the labels got carried along correctly,0
,0
auckland_doc_to_json.py,0
,0
Convert Auckland DOC data set to COCO camera traps format,0
,0
%% Constants and imports,0
Filenames will be stored in the output .json relative to this base dir,0
%% Enumerate files,0
%% Assemble dictionaries,0
Force the empty category to be ID 0,0
fn = image_files[0]; print(fn),0
Typically y:\Maukahuka_Auckland_Island\1_Training\Winter_Trial_2019\cat\cat\eat\20190903_IDdY_34_E3_tmp_201908240051.JPG,0
"This data set has two top-level folders, ""1_Training"" (which has class names encoded",0
"in paths) and ""2_Testing"" (which has no class information).",0
...for each image,0
%% Write output .json,0
%% Write train/test .jsons,0
%% Validate .json files,0
%% Preview labels,0
checkpoint,0
,0
awc_to_json.py,0
,0
Convert a particular .csv file to CCT format.,0
,0
%% Constants and environment,0
%% Read source data,0
%% Main loop over labels,0
Force the empty category to be ID 0,0
iRow = 0; row = input_metadata.iloc[iRow],0
"ImageID,FileName,FilePath,SpeciesID,CommonName",0
"This makes an assumption of one annotation per image, which happens to be",0
true in this data set.,0
Retrieve image width and height,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
list(relativePathToImage.keys())[0],0
%% Create info struct,0
%% Write output,0
%% Sanity-check the database's integrity,0
%% Render a bunch of images to make sure the labels got carried along correctly,0
,0
wellington_to_json.py,0
,0
Convert the .csv file provided for the Wellington data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
%% Read source data,0
"Filenames were provided as *.jpg, but images were *.JPG, converting here",0
"%% Map filenames to rows, verify image existence",0
"Takes ~30 seconds, since it's checking the existence of ~270k images",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
"I didn't expect this to be true a priori, but it appears to be true, and",0
it saves us the trouble of checking consistency across multiple occurrences,0
of an image.,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
imageName = imageFilenames[0],0
"As per above, this is convenient and appears to be true; asserting to be safe",0
"Filenames look like ""290716114012001a1116.jpg""",0
This gets imported as an int64,0
"These appear as ""image1"", ""image2"", etc.",0
"In the form ""001a""",0
Can be in the form '111' or 's46',0
"In the form ""7/29/2016 11:40""",0
Check image height and width,0
"Use 'empty', to be consistent with other data on lila",0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
,0
mcgill_to_json.py,0
,0
Convert the .csv file provided for the McGill test data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
Create an additional column for concatenated filenames,0
Maps relative filenames to rows,0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
row = input_metadata.iloc[0],0
"I didn't expect this to be true a priori, but it appears to be true, and",0
it saves us the trouble of checking consistency across multiple occurrences,0
of an image.,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
row = input_metadata.iloc[0],0
"Filenames look like ""290716114012001a1116.jpg""",0
"In the form ""001a""",0
Can be in the form '111' or 's46',0
"In the form ""7/29/2016 11:40""",0
Check image height and width,0
NaN is the only thing we should see that's not a string,0
NaN is the only thing we should see that's not a string,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
,0
missouri_to_json.py,0
,0
Create .json files from the original source files for the Missouri Camera Traps,0
data set.  Metadata was provided here in two formats:,0
,0
"1) In one subset of the data, folder names indicated species names.  In Set 1,",0
there are no empty sequences.  Set 1 has a metadata file to indicate image-level,0
bounding boxes.,0
,0
2) A subset of the data (overlapping with (1)) was annotated with bounding,0
"boxes, specified in a whitespace-delimited text file.  In set 2, there are",0
"some sequences omitted from the metadata file, which implied emptiness.",0
,0
"In the end, set 2 labels were not reliable enough to publish, so LILA includes only set 1.",0
,0
%% Constants and imports,0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Filenames will be stored in the output .json relative to this base dir,0
"%% Enumerate files, read image sizes (both sets)",0
"Takes a few minutes, since we're reading image sizes.",0
Each element will be a list of relative path/full path/width/height,0
"Only process leaf nodes corresponding to sequences, which look like:",0
,0
E:\wildlife_data\missouri_camera_traps\Set1\1.02-Agouti\SEQ75583,0
E:\wildlife_data\missouri_camera_traps\Set2\p1d101,0
,0
assert len(files) <= 2,0
Read the image,0
Not an image...,0
Store file info,0
"...if we didn't hit the max file limit, keep going",0
...for each file,0
%% Add sequence lengths (both sets),0
%% Load the set 1 metadata file,0
"List of lists, length varies according to number of bounding boxes",0
,0
Preserves original ordering,0
iLine = 0; line = metadataSet1Lines[0],0
"Lines should be filename, number of bounding boxes, boxes (four values per box)",0
Make sure we have image info for this image,0
%% Print missing files from Set 1 metadata,0
Manual changes I made to the metadata file:,0
,0
'IMG' --> 'IMG_',0
Red_Brocket_Deer --> Red_Deer,0
European-Hare --> European_Hare,0
Wood-Mouse --> Wood_Mouse,0
Coiban-Agouti --> Coiban_Agouti,0
%% Load the set 2 metadata file,0
"This metadata file contains most (but not all) images, and a class label (person/animal/empty)",0
"for each, plus bounding boxes.",0
"List of lists, length varies according to number of bounding boxes",0
,0
Preserves original ordering,0
"Create class IDs for each *sequence*, which we'll use to attach classes to",0
images for which we don't have metadata,0
,0
This only contains mappings for sequences that appear in the metadata.,0
iLine = 0; line = metadataSet2Lines[0],0
"Lines should be filename, number of bounding boxes, labeled boxes (five values per box)",0
,0
Empty images look like filename\t0\t0,0
E.g. 'Set2\\p1d101\\p1d101s100i10.JPG',0
Make sure we don't have mixed classes within an image,0
"Figure out what class this *sequence* is, so we know how to handle unlabeled",0
images from this sequence,0
Can't un-do a mixed sequence,0
Previously-empty sequences get the image class label,0
"If the sequence has a non-empty class, possibly change it",0
Make sure we have image info for this image,0
...for each line in the set 2 metadata file,0
%% What Set 2 images do I not have metadata for?,0
These are *mostly* empty images,0
iImage = 0; imageID = set2ImageIDs[iImage],0
%% Create categories and annotations for set 1,0
"Though we have no empty sequences, we do have empty images in this set",0
For each image,0
,0
iImage = 0; imageID = set1ImageIDs[iImage],0
E.g. Set1\\1.80-Coiban_Agouti\\SEQ83155\\SEQ83155_IMG_0010.JPG,0
Find the species name,0
This image may still be empty...,0
category['count'] = category['count'] + 1,0
"If we have bounding boxes, create image-level annotations",0
"filename, number of bounding boxes, boxes (four values per box)",0
"Make sure the relative filename matches, allowing for the fact that",0
some of the filenames in the metadata aren't quite right,0
This image is non-empty,0
"Some redundant bounding boxes crept in, don't add them twice",0
Check this bbox against previous bboxes,0
,0
Inefficient?  Yes.  In an important way?  No.,0
"Bounding box values are in absolute coordinates, with the origin",0
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1].",0
,0
"Convert to floats and to x/y/w/h, as per CCT standard",0
...for each box,0
if we do/don't have boxes for this image,0
Else create a sequence-level annotation,0
...for each image,0
%% Create categories and annotations for set 2,0
For each image,0
,0
iImage = 0; imageID = set2ImageIDs[iImage],0
E.g. 'Set2\\p1d100\\p1d100s10i1.JPG',0
"Find the sequence ID, sanity check filename against what we stored",0
"If we have bounding boxes or an explicit empty label, create image-level annotations",0
"filename, number of bounding boxes, labeled boxes (five values per box)",0
"Make sure the relative filename matches, allowing for the fact that",0
some of the filenames in the metadata aren't quite right,0
"Bounding box values are in absolute coordinates, with the origin",0
"at the upper-left of the image, as [xmin1 ymin1 xmax1 ymax1].",0
,0
Convert to floats and to x/y/w/h,0
...for each box,0
...if we do/don't have boxes for this image,0
Else create a sequence-level annotation,0
...for each image,0
"%% The 'count' field isn't really meaningful, delete it",0
"It's really the count of image-level annotations, not total images assigned to a class",0
%% Write output .json files,0
%% Sanity-check final set 1 .json file,0
"python sanity_check_json_db.py --bCheckImageSizes --baseDir ""E:\wildlife_data\missouri_camera_traps"" ""E:\wildlife_data\missouri_camera_traps\missouri_camera_traps_set1.json""",0
%% Generate previews,0
"Generate previewse:\wildlife_data\missouri_camera_traps\missouri_camera_traps_set1.json"" ""e:\wildlife_data\missouri_camera_traps\preview"" ""e:\wildlife_data\missouri_camera_traps"" --num_to_visualize 1000",0
,0
filenames_to_json.py,0
,0
Take a directory of images in which species labels are encoded by folder,0
"names, and produces a COCO-style .json file",0
,0
%% Constants and imports,0
from the ai4eutils repo,0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Filenames will be stored in the output .json relative to this base dir,0
"rawClassListFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_classes.csv')",0
"classMappingsFilename = os.path.join(baseDir,'bellevue_camera_traps.19.06.02.1320_class_mapping.csv')",0
"%% Enumerate files, read image sizes",0
Each element will be a list of relative path/full path/width/height,0
Read the image,0
Corrupt or not an image,0
Store file info,0
Write to output file,0
...for each image file,0
...csv file output,0
...if the file list is/isn't available,0
%% Enumerate classes,0
Maps classes to counts,0
We like 'empty' to be class 0,0
%% Assemble dictionaries,0
...for each category,0
Each element is a list of relative path/full path/width/height/className,0
...for each image,0
%% External class mapping,0
%% Write raw class table,0
cat = categories[0],0
%% Read the mapped class table,0
"id, source, count, target",0
"%% Make classMappings contain *all* classes, not just remapped classes",0
cat = categories[0],0
%% Create new class list,0
"Start at 1, explicitly assign 0 to ""empty""",0
One-off issue with character encoding,0
%% Re-map annotations,0
ann = annotations[0],0
%% Write output .json,0
%% Utilities,0
%%,0
Find images with a particular tag,0
%% Randomly sample annotations,0
,0
nacti_fieldname_adjustments.py,0
,0
"NACTI metadata was posted with ""filename"" in images instead of ""file_name"", and",0
"used string (rather than int) category IDs (in categories, but not in annotations).",0
,0
This script fixes those issues and rev's the version number.,0
,0
%% Constants and environment,0
%% Read .json file,0
"%% Rev version number, update field names and types",0
%% Write json file,0
,0
cct_field_adjustments.py,0
,0
CCT metadata was posted with int locations instead of strings.,0
,0
This script fixes those issues and rev's the version number.,0
,0
%% Constants and environment,0
%% Read .json file,0
"%% Rev version number, update field names and types",0
%% Write json file,0
%% Check output data file,0
,0
carrizo_shrubfree_2018.py,0
,0
Convert the .csv file provided for the Carrizo Mojave data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
%% Read source data,0
Original .csv file had superfluous spaces in column names,0
"%% Map filenames to rows, verify image existence",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
58 missing files (of 17652),0
%% Check for images that aren't included in the metadata file,0
3012 of 20606 files are not in metadata,0
%% Create CCT dictionaries,0
Map categories to integer IDs,0
,0
The category '0' is reserved for 'empty',0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
Each filename should just match one row,0
Don't include images that don't exist on disk,0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
,0
rspb_to_json.py,0
,0
Convert the .csv file provided for the RSPB data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
[location] is an obfuscation,0
%% Create info struct,0
%% Read source data,0
metadataTable.columns.values,0
,0
"array(['Project', 'inDir', 'FileName', 'Station', 'Camera',",0
"'StationCameraFileName', 'DateTimeOriginal', 'DateReadable',",0
"'outDir', 'filename_new', 'fileExistsAlready', 'CopyStatus',",0
"'Species'], dtype=object)",0
We'll populate these later,0
keys should be lowercase,0
"%% Enumerate images, confirm filename uniqueness",0
"%% Update metadata filenames to include site and camera folders, check existence",0
,0
Takes ~1min,0
iRow = 0; row = metadataTable.iloc[iRow],0
There's a bug in the metadata; the 'camera' column isn't correct.,0
camera = row['Camera'],0
"These appear as, e.g., '3.22e12'",0
camera = str(int(float(camera))),0
Let's pull this out of the file name instead,0
,0
Filenames look like one of the following:,0
,0
A1__03224850850507__2015-11-28__10-45-04(1).JPG,0
Bayama2PH__C05__NA(NA).JPG,0
assert(os.path.isfile(fullPath)),0
metadataTable.iloc[iRow] = row,0
Re-assemble into an updated table,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
Write to a text file,0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
iRow = 0; row = metadataTable.iloc[iRow],0
A1__03224850850507__2015-11-28__10-45-04(1).JPG,0
'A1\\03224850850507\\A1__03224850850507__2015-11-28__10-45-04(1).JPG',0
Not currently populated,0
"Often -1, sometimes a semi-meaningful int",0
A1,0
03224850850507,0
"In variable form, but sometimes '28/11/2015 10:45'",0
Check image height and width,0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Write output,0
%% Check database integrity,0
%% Preview a few images to make sure labels were passed along sensibly,0
%% One-time processing step: copy images to a flat directory for annotation,0
%%,0
,0
pc_to_json.py,0
,0
Convert a particular collection of .csv files to CCT format.,0
,0
%% Constants and environment,0
%% Read and concatenate source data,0
List files,0
"List of dataframes, one per .csv file; we'll concatenate later",0
i_file = 87; fn = input_files[i_file],0
Concatenate into a giant data frame,0
%% List files,0
%% Main loop over labels (prep),0
Force the empty category to be ID 0,0
%% Main loop over labels (loop),0
iRow = 0; row = input_metadata.iloc[iRow],0
"ImageID,FileName,FilePath,SpeciesID,CommonName",0
assert os.path.isfile(full_path),0
Retrieve image width and height,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
%% See what's up with missing files,0
s = list(image_relative_paths)[0],0
s = missing_files[0],0
%% Check for images that aren't included in the metadata file,0
%% Sample the database,0
%%,0
Collect the images we want,0
%% Create info struct,0
%% Write output,0
%% Sanity-check the database's integrity,0
%% Render a bunch of images to make sure the labels got carried along correctly,0
options.classes_to_exclude = ['unlabeled'],0
%% Write out a list of files to annotate,0
,0
save_the_elephants_survey_B.py,0
,0
Convert the .csv file provided for the Save the Elephants Survey B data set to a,0
COCO-camera-traps .json file,0
,0
%% Constants and environment,0
input_base = r'/mnt/blobfuse/wildlifeblobssc/ste_2019_08_drop',0
output_base = r'/home/gramener/survey_b',0
Handle all unstructured fields in the source data as extra fields in the annotations,0
"photo_type really should be an image property, but there are a few conflicts",0
that forced me to handle it as an annotation proprerty,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
"%% Map filenames to rows, verify image existence",0
Maps relative paths to row indices in input_metadata,0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
Ignore directories,0
%% Make sure the multiple-annotation cases make sense,0
%%,0
%% Check for images that aren't included in the metadata file,0
Enumerate all images,0
%% Create CCT dictionaries,0
Force the empty category to be ID 0,0
i_image = 0; image_name = list(filenames_to_rows.keys())[i_image],0
Example filename:,0
,0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2\100EK113\EK001382.JPG',0
'Site 1_Oloisukut_1\Oloisukut_A11_UP\Service_2.1\100EK113\EK001382.JPG',0
i_row = row_indices[0],0
timestamp = row['Date'],0
There are a small handful of datetime mismatches across annotations,0
for the same image,0
assert im['datetime'] == timestamp,0
Special cases based on the 'photo type' field,0
Various spellings of 'community',0
Have we seen this category before?,0
Create an annotation,0
fieldname = list(mapped_fields.keys())[0],0
...for each row,0
...for each image,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
%% Scrap,0
%% Find unique photo types,0
,0
carrizo_trail_cam_2017.py,0
,0
"Convert the .csv files provided for the ""Trail Cam Carrizo"" 2017 data set to",0
a COCO-camera-traps .json file.,0
,0
%% Constants and environment,0
%% Read source data,0
Removing the empty records,0
"%% Map filenames to rows, verify image existence",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
908 missing files (of 60562),0
%% Check for images that aren't included in the metadata file,0
105329 of 164983 files are not in metadata,0
%% Create CCT dictionaries,0
Map categories to integer IDs,0
,0
The category '0' is reserved for 'empty',0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
Each filename should just match one row,0
Don't include images that don't exist on disk,0
Have we seen this category before?,0
Create an annotation,0
"The Internet tells me this guarantees uniqueness to a reasonable extent, even",0
beyond the sheer improbability of collisions.,0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
%% Validate output,0
%% Preview labels,0
,0
ena24_to_json_2017.py,0
,0
Convert the ENA24 data set to a COCO-camera-traps .json file,0
,0
%% Constants and environment,0
Temporary folders for human and non-human images,0
Clean existing output folders/zipfiles,0
%% Support functions,0
%% Read source data,0
"%% Map filenames to rows, verify image existence",0
"Build up a map from filenames to a list of rows, checking image existence as we go",0
%% Create CCT dictionaries,0
"Also gets image sizes, so this takes ~6 minutes",0
,0
"Implicitly checks images for overt corruptness, i.e. by not crashing.",0
Map categories to integer IDs (that's what COCO likes),0
For each image,0
,0
"Because in practice images are 1:1 with annotations in this data set,",0
this is also a loop over annotations.,0
Check image height and width,0
"Each row is category, [box coordinates]",0
"If there's just one row, loadtxt reads it as a 1d array; make it a 2d array",0
with one row,0
Each row is a bounding box,0
Have we seen this category before?,0
Create an annotation,0
...for each bounding box,0
"This was here for debugging; nearly every instance is Human+Horse, Human+Vehicle,",0
"or Human+Dog, but there is one Rabbit+Opossium, and a few Deer+Chicken!",0
...for each image,0
Convert categories to a CCT-style dictionary,0
%% Create info struct,0
%% Write output,0
%% Create ZIP files for human and non human,0
%% Validate output,0
%% Preview labels,0
%% Imports and constants,0
configurations and paths,0
%% Helper functions,0
"dest_path = copy(source_path, dest_folder)",0
num_workers = multiprocessing.cpu_count(),0
pool = ThreadPool(num_workers),0
"results = pool.starmap(_copy_unzip, zip(sources, itertools.repeat(dest_folder)))",0
,0
print('Waiting for processes to finish...'),0
pool.close(),0
pool.join(),0
sequential,0
%% Command-line driver,0
if the blob container is already mounted on the VM,0
or you can download them using the storage Python SDK,0
key to the storage account should be stored in the environment variable AZ_STORAGE_KEY,0
,0
eMammal_helpers.py,0
,0
Support functions for processing eMammal metadata,0
,0
%% Constants and imports,0
%% Support functions,0
"pad to a total of 3 digits if < 1000, or 4 digits otherwise",0
img_frame is a string from the xml tree,0
"length 4 frame order is returned as is, others are left padded to be 3 digit long",0
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels,0
pad to a total of 4 digits,0
img_frame is a string from the xml tree,0
"length 4 frame order is returned as is, others are left padded to be 3 digit long",0
we need to make sure img_frame has length 3 when it's < 1000 so we can match it to the iMerit labels,0
others column,0
summer day hours: 6am - 7pm,0
others day hours: 7am - 6pm,0
,0
make_eMammal_json.py,0
,0
"Produces the COCO-formatted json database for an eMammal dataset, i.e. a",0
"collection of folders, each of which contains a deployment_manifest.xml file.",0
,0
"In this process, each image needs to be loaded to size it.",0
,0
"To add bounding box annotations to the resulting database, use",0
add_annotations_to_eMammal_json.py.,0
,0
%% Constants and imports,0
"Either add the eMammal directory to your path, or run from there",0
os.chdir(r'd:\git\CameraTraps\database_tools\eMammal'),0
import warnings,0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
warnings.filterwarnings('ignore'),0
Should we run the image size retrieval in parallel?,0
%% Support functions,0
%% Main loop (metadata processing; image sizes are retrieved later),0
deployment = folders[0],0
sequence = image_sequences[0],0
get species info for this sequence,0
add each image's info to database,0
img = images[0],0
"some manifests don't have the ImageOrder info, but the info is in the file name",0
full_img_id has no frame info,0
,0
frame number only used in requests to iMerit for ordering,0
...for each image,0
...for each sequence,0
...for each deployment,0
%% Get image sizes,0
"'tasks' is currently a list of 2-tuples, with each entry as [image dictionary,path].",0
,0
"Go through that and copy just the image dictionaries to 'db_images', adding size",0
information to each entry.  Takes a couple hours.,0
opening each image seems too fast for this multi-threaded version to be faster than sequential code.,0
%% Assemble top-level dictionaries,0
%% Write out .json,0
,0
make_full_SS_json.py,0
,0
Create a COCO-camera-traps .json file for Snapshot Serengeti data from,0
the original .csv files provided on Dryad.,0
,0
%% Imports and constants,0
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays",0
Count the number of images with multiple species,0
"%% Read image .csv file, format into a dictionary mapping images to capture events",0
%% Create CCT-style .json,0
still need image width and height,0
...for each image,0
%% Write output files,0
,0
make_per_season_SS_json.py,0
,0
Create a COCO-camera-traps .json file for each Snapshot Serengeti season from,0
the original .csv files provided on Dryad.,0
,0
%% Imports and constants,0
"%% Read annotation .csv file, format into a dictionary mapping field names to data arrays",0
"%% Read image .csv file, format into a dictionary mapping images to capture events",0
%% Create CCT-style .json,0
still need image width and height,0
...for each image ID,0
%% Write output files,0
...for each season,0
%% Configurations,0
"if the following two files are provided, only the tfrecord generation section will be executed",0
proceed to generate tfrecords if True,0
approximate fraction to split the new entries by,0
categories in the database to include,0
"in addition, any images with a 'group' label will not be included to avoid confusion",0
see 'image_contains_group' in create_tfrecords_format.py,0
%% Input validation,0
%% Convert the COCO Camera Trap format data to another json,0
that aligns with the fields in the resulting tfrecords,0
%% Make train/val/test splits,0
%% Write the tfrecords,0
want the file names of all tfrecords to be of the same format and length in each part,0
%% Parameters,0
%% Load the annotations queried from megadb,0
"%% Make the ""dataset"" required by create_tfrecords.py",0
%% Create tfrecords,0
Construct a Reader to read examples from the .tfrecords file,0
Basic info,0
Print out the per class image counts,0
Can we detect if there any missing classes?,0
"We expect class id for each value in the range [0, max_class_id]",0
So lets see if we are missing any of these values,0
Construct a Reader to read examples from the .tfrecords file,0
Reversed coordinates?,0
Too small of an area?,0
Basic info,0
"print(""Images with areas < 10:"")",0
for img_id in images_with_small_bboxes:,0
print(img_id),0
"print(""Images with reversed coordinates:"")",0
for img_id in images_with_reversed_coords:,0
print(img_id),0
for img_id in images_with_bbox_count_mismatch:,0
print(img_id),0
,0
read_from_tf_records.py,0
,0
"Reads detection results from a tfrecords file of the style generated by the TFODAPI inference script,",0
"and converts it to a .p file that's friendly to other tools in this repo, e.g. detection/detector_eval.",0
,0
"Detection and ground truth bounding box coordinates are in the format of [ymin, xmin, ymax, xmax].",0
,0
coding: utf-8,0
In[1]:,0
In[2]:,0
In[ ]:,0
Defaults are not specified since both keys are required.,0
"image, label, height, width",0
"print(sess.run([output['image/filename'], output['image/class/text'], output['image/class/label'], output['image/height'], output['image/width']]))",0
tasks = list(glob.glob('/data/lila/nacti/cropped_tfrecords/t*')),0
"for _ in tqdm.tqdm(p.imap(analyze_record, tasks, chunksize=10), total=len(tasks)):",0
pass,0
Parallel(n_jobs=8)(delayed(analyze_record)(path) for path in list(glob.glob('/data/lila/nacti/cropped_tfrecords/t*'))),0
,0
iterate_tf_records.py,0
,0
Inherited from Visipedia tfrecords repo.,0
,0
print(feature_key),0
return a dictionary of the features,0
Construct a Reader to read examples from the .tfrecords file,0
Copyright 2016 Google Inc. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Required,0
Class label for the whole image,0
Objects,0
Bounding Boxes,0
Parts,0
Areas,0
Ids,0
Any extra data (e.g. stringified json),0
Additional fields for the format needed by the Object Detection repository,0
"For explanation of the fields, see https://github.com/visipedia/tfrecords",0
Additional fields for the format needed by the Object Detection repository,0
Create a single Session to run all image coding calls.,0
Initializes function that converts PNG to JPEG data.,0
Initializes function that decodes RGB JPEG data.,0
Convert the image data from png to jpg,0
Decode the image data as a jpeg image,0
Read the image file.,0
Clean the dirty data.,0
Decode the RGB JPEG.,0
Check that image converted to RGB,0
Each thread produces N shards where N = int(num_shards / num_threads).,0
"For instance, if num_shards = 128, and the num_threads = 2, then the first",0
"thread would produce shards [0, 64).",0
"Generate a sharded version of the file name, e.g. 'train-00002-of-00010'",0
raise,0
Images in the tfrecords set must be shuffled properly,0
"Break all images into batches with a [ranges[i][0], ranges[i][1]].",0
Launch a thread for each batch.,0
Create a mechanism for monitoring when all threads are finished.,0
Create a generic TensorFlow-based utility for converting all image codings.,0
A Queue to hold the image examples that fail to process.,0
Wait for all the threads to terminate.,0
Collect the errors,0
,0
create_splits.py,0
,0
"Based on a tfrecords_format json file of the database, creates 3 splits according to",0
the specified fractions based on location (images from the same location should be in,0
one split) or based on images.,0
,0
"If a previous split is provided (append_to_previous_split is True), the entries in",0
"each split will be preserved, and new entries will be appended, so that new models",0
can warm start with a model trained on the original splits.,0
,0
%% Imports,0
%% Main function,0
"code below references 'locatioin' as the attribute to split on, but it works for any split_by attribute",0
present in the image entryes,0
"find new locations and assign them to a split, without reassigning any previous locations",0
do NOT sort the IDs to keep the shuffled order,0
,0
create_tfrecords_format.py,0
,0
This script converts a COCO formatted json database to another json file that would be the,0
input to create_tfrecords.py or similar scripts to create tf_records,0
,0
%% Imports and environment,0
%% Main tfrecord generation function,0
Remap category IDs; TF needs consecutive category ids,0
Sanity-check number of empty annotations and annotation-less images,0
Images without annotations don't have bounding boxes,0
Images with annotations *may* have bounding boxes,0
"prepend the dataset name to image_id because after inference on val set, records from",0
different datasets are stored in one tfrecord,0
Propagate optional metadata to tfrecords,0
checking to ignore any images that contain 'group' needs to happen before ignoring non-valid categories!,0
Only include valid categories,0
...for each annotation for the current image,0
...for each image,0
,0
create_tfrecords_from_coco.py,0
,0
This script creates a tfrecords file from a classification dataset in COCO format.,0
%% Imports and environment,0
%% Main tfrecord generation function,0
We remap all category IDs such that they are consecutive starting from zero,0
"If this is already the case for the input dataset, then the remapping will not",0
"have any effect, i.e. the order of the classes will remain unchanged",0
Propagate optional metadata to tfrecords,0
endfor each annotation for the current image,0
endfor each image,0
"Calculate number of shards to get the desired number of images per record,",0
ensure it is evenly divisible by the number of threads,0
%% Command-line driver,0
%% Driver,0
#######,0
,0
sanity_check_json_db.py,0
,0
"Does some sanity-checking and computes basic statistics on a db, specifically:",0
,0
* Verifies that required fields are present and have the right types,0
* Verifies that annotations refer to valid images,0
* Verifies that annotations refer to valid categories,0
"* Verifies that image, category, and annotation IDs are unique",0
,0
* Optionally checks file existence,0
,0
* Finds un-annotated images,0
* Finds unused categories,0
,0
* Prints a list of categories sorted by count,0
,0
#######,0
%% Constants and environment,0
%% Functions,0
"If baseDir is non-empty, checks image existence",0
This is used in a medium-hacky way to share modified options across threads,1
print('Image path {} does not exist'.format(filePath)),0
"#%% Read .json file if necessary, sanity-check fields",0
info = data['info'],0
"#%% Build dictionaries, checking ID uniqueness and internal validity as we go",0
Confirm that required fields are present,0
Confirm ID uniqueness,0
...for each category,0
image = images[0],0
Confirm that required fields are present,0
Confirm ID uniqueness,0
We previously supported ints here; this should be strings now,0
"assert isinstance(image['location'], str) or isinstance(image['location'], int), 'Illegal image location type'",0
Are we checking for unused images?,0
Recursively enumerate images,0
print('Image {} is unused'.format(p)),0
Are we checking file existence and/or image size?,0
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)",0
...for each image,0
Confirm that required fields are present,0
Confirm ID uniqueness,0
Confirm validity,0
...for each annotation,0
#%% Print statistics,0
Find un-annotated images and multi-annotation images,0
Find unused categories,0
Prints a list of categories sorted by count,0
https://stackoverflow.com/questions/72899/how-do-i-sort-a-list-of-dictionaries-by-a-value-of-the-dictionary,0
...def sanity_check_json_db(),0
%% Command-line driver,0
"python sanity_check_json_db.py ""e:\wildlife_data\wellington_data\wellington_camera_traps.json"" --baseDir ""e:\wildlife_data\wellington_data\images"" --bFindUnusedImages --bCheckImageSizes",0
"python sanity_check_json_db.py ""D:/wildlife_data/mcgill_test/mcgill_test.json"" --baseDir ""D:/wildlife_data/mcgill_test"" --bFindUnusedImages --bCheckImageSizes",0
"Here the '-u' prevents buffering, which makes tee happier",0
,0
python -u sanity_check_json_db.py '/datadrive1/nacti_metadata.json' --baseDir '/datadrive1/nactiUnzip/' --bFindUnusedImages --bCheckImageSizes | tee ~/nactiTest.out,0
%% Interactive driver(s),0
%%,0
Sanity-check .json files for LILA,0
Sanity-check one file with all the bells and whistles,0
options.iMaxNumImages = 10,0
,0
add_url_to_database.py,0
,0
"Adds a ""url"" field to a coco-camera-traps .json database, specifically to allow the db to",0
be reviewed in the Visipedia annotation tool.,0
,0
,0
remove_corrupted_images_from_database.py,0
,0
"Given a coco-camera-traps .json file, checks all images for TF-friendliness and generates",0
a new .json file that only contains the non-corrupted images.,0
,0
%% Imports and constants,0
import multiprocessing,0
"I leave this at an annoying low number, since by definition weird stuff will",0
"be happening in the TF kernel, and it's useful to keep having content in the console.",0
%% Function definitions,0
"I sometimes pass in a list of images, sometimes a dict with a single",0
element mapping a job ID to the list of images,0
"We're about to start a lot of TF sessions, and we don't want gobs",0
of debugging information printing out for every session.,0
At some point we were creating a single session and looping over images,0
"within that session, but the only way I found to reliably not run out",0
of GPU memory was to create a session per image and gc.collect() after,0
each session.,0
Map Image IDs to boolean (should I keep this image?),0
"Convert to lists, append job numbers to the image lists",0
"results = pool.imap_unordered(lambda x: fetch_url(x,nImages), indexedUrlList)",0
Merge results,0
%% Interactive driver,0
%%,0
base_dir = r'D:\temp\snapshot_serengeti_tfrecord_generation',0
Load annotations,0
Check for corruption,0
Write out only the uncorrupted data,0
%% Command-line driver,0
,0
combine_two_json_files.py,0
,0
"Merges two coco-camera-traps .json files. In particular, categories are combined and re-numbered.",0
,0
Combined Info,0
Combined Images,0
Combined Categories,0
## categories to merge,0
Combined Annotations,0
,0
make_detection_db_for_viewing.py,0
,0
"Given a .json file with ground truth bounding boxes, and a .p file containing detections for the same images,",0
"creates a new .json file with separate classes for ground truth and detection, suitable for viewing in the Visipedia",0
annotation tool.,0
,0
%% Imports and constants,0
%% Main function,0
im_id_to_im = {im['id']:im for im in images},0
make new categories to distinguish between ground truth and detections,0
"update all gt annotations to be class ""gt""",0
collect all detections by image,0
keep any detection with score above det_thresh,0
"need to convert bbox from [x1,y1,x2,y2] to [x,y,w,h]",0
"add ""info"" and ""licenses"" for annotation tools to function",0
create new db,0
%% Command-line handling,0
,0
analyze_json_database.py,0
,0
Plots location/class/etc. distributions for classes in a coco-camera-traps .json file.,0
,0
Currently includes some one-off code for specific species.,0
,0
%% Constants and imports,0
%% Path configuration,0
%% Load source data,0
%% Build image/category dictionaries,0
%% Make plot of category distribution,0
%% make plots of location distribution,0
"plt.title('Number of images per location, by category')",0
"plt.tight_layout(rect=[0,0,1,0.9])",0
#make plot of images per season,0
%% Make plot of lions per location,0
%% Make plot of elephants per location,0
for loc in sorted_by_total[:25]:,0
"print('Location:' + loc[0] +', Lions: ' + str(len(loc_to_lion_ims[loc[0]])) + ', Elephants: ' + str(len(loc_to_elephant_ims[loc[0]])) + ', Total ims: ' + str(len(loc_to_ims[loc[0]])))",0
,0
subset_json_db.py,0
,0
Select a subset of images (and associated annotations) from a .json file,0
in COCO Camera Traps format.,0
,0
"To subset the .json files produced by our batch processing API, see",0
subset_json_detector_output.py,0
,0
Sample invocation:,0
,0
"python subset_json_db.py ""E:\Statewide_wolf_container\idfg_20190409.json"" ""E:\Statewide_wolf_container\idfg_20190409_clearcreek.json"" ""clearcreek"" --ignore_case",0
,0
%% Constants and imports,0
%% Functions,0
Load the input file if necessary,0
Find images matching the query,0
Find annotations referring to those images,0
Write the output file if requested,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
,0
add_width_and_height_to_database.py,0
,0
Grabs width and height from actual image files for a .json database that is missing w/h.,0
,0
Originally used when we created a .json file for snapshot serengeti from .csv.,0
,0
"This file converts the JSON output of the batch processing API to a pickle file, which can be used by the",0
script ./make_classification_dataset.py,0
"Convert boxes from JSON   [x_min, y_min, width_of_box, height_of_box]",0
"to PICKLE [ymin,  xmin,  ymax,         xmax]",0
Write detections to file with pickle,0
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the",0
script ./make_classification_dataset.py,0
Write detections to file with pickle,0
"This file converts the CSV output of the batch processing API to a pickle file, which can be used by the",0
script ./make_classification_dataset.py,0
Assumes that the root of the CameraTrap repo is in the PYTHONPATH,0
Minimum threshold to put a detection into the output JSON file,0
Parameter check,0
Load detections from input,0
Load COCO style annotations,0
Build output JSON in format version 1.0,0
Adding the only known metadata info,0
"The pickle file does not contain category information, so we assume the default",0
For each image with detections,0
for each detection,0
"Convert boxes from [ymin, xmin, ymax, xmax] format to",0
"[x_min, y_min, width_of_box, height_of_box]",0
Write output json,0
"Batch file for applying an object detection graph to a COCO style dataset,",0
cropping images to the detected animals inside and creating a COCO-,0
style classification dataset out of it. It also saves the detections,0
to a file using pickle,0
#########################################################,0
## Configuration,0
Any model exported using the `export_inference_graph.py` tool can be loaded here simply by changing `PATH_TO_FROZEN_GRAPH` to point to a new .pb file.,0
"parser.add_argument('detections_output', type=str, default='detections_final.pkl',",0
"help='Pickle file with the detections, which can be used for cropping later on.')",0
#########################################################,0
## The actual code,0
Check arguments,0
/ai4edevfs/models/object_detection/faster_rcnn_inception_resnet_v2_atrous/megadetector/frozen_inference_graph.pb,0
"Detection threshold should be in [0,1]",0
Padding around the detected objects when cropping,0
1.3 for the cropping during test time and 1.3 for,0
the context that the CNN requires in the left-over,0
image,0
Fraction of locations used for testing,0
Create output directories,0
Load a (frozen) Tensorflow model into memory.,0
Load COCO style annotations from the input dataset,0
"Get all categories, their names, and create an updated ID for the json file",0
Prepare the coco-style json files,0
Split the dataset by locations,0
Load detections,0
TFRecords variables,0
The detection part,0
## Preparations: get all the output tensors,0
The following processing is only for single image,0
Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.,0
For all images listed in the annotations file,0
Path to the input image,0
Skip the image if it is annotated with more than one category,0
Get category ID for this image,0
... and the corresponding category name,0
The remapped category ID for our json file,0
Whether it belongs to a training or testing location,0
Skip excluded categories,0
"If we already have detection results, we can use them",0
Otherwise run detector,0
"We allow to skip images, which we do not have available right now",0
This is useful for processing parts of large datasets,0
Load image,0
Run inference,0
"all outputs are float32 numpy arrays, so convert types as appropriate",0
Add detections to the collection,0
Select detections with a confidence larger DETECTION_THRESHOLD,0
Skip if no detection selected,0
Get these boxes and convert normalized coordinates to pixel coordinates,0
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes",0
"However, we need to make sure that it box coordinates are still within the image",0
"For each detected bounding box with high confidence, we will",0
crop the image to the padded box and save it,0
"bbox is the detected box, crop_box the padded / enlarged box",0
The file path as it will appear in the annotation json,0
Add numbering to the original file name if there are multiple boxes,0
The absolute file path where we will store the image,0
Only used if an coco-style dataset is created,0
Create the category directories if necessary,0
"if COCO_OUTPUT_DIR is set, then we will only use the shape",0
of cropped_img in the following code. So instead of reading,0
cropped_img = np.array(Image.open(out_file)),0
we can speed everything up by reading only the size of the image,0
Read the image,0
Add annotations to the appropriate json,0
Propagate optional metadata to tfrecords,0
Write out COCO-style json files to the output directory,0
Write detections to file with pickle,0
json_file = TEST_JSON,0
js_keys = ['/'.join(im['file_name'].split('/')[1:])[:-4] for im in js['images']],0
for tk in js_keys:,0
"assert np.isclose(1, np.sum(detections[tk]['detection_scores'] > 0.5))",0
,0
ocr_sandbox.py,0
,0
sandbox for experimenting with using OCR to pull metadata from camera trap images,0
,0
The general approach is:,0
,0
"* Crop a fixed percentage from the top and bottom of an image, slightly larger",0
than the largest examples we've seen of how much space is used for metadata.,0
,0
"* Refine that crop by blurring a little, then looking for huge peaks in the",0
"color histogram suggesting a solid background, then finding rows that are",0
mostly that color.,0
,0
"* Crop to the refined crop, then run pytesseract to extract text",0
,0
"* Use regular expressions to find time and date, in the future can add, e.g.,",0
"temperature (which is often present *only* in the images, unlike time/date which",0
are also usually in EXIF but often wrong or lost in processing),0
,0
"The metadata extraction (EXIF, IPTC) here is just sample code that seemed to",0
belong in this file.,0
,0
Contact: Dan Morris (dan@microsoft.com),0
,0
%% Constants and imports,0
pip install pytesseract,0
,0
"Also intall tesseract from: https://github.com/UB-Mannheim/tesseract/wiki, and add",0
"the installation dir to your path (on Windows, typically C:\Program Files (x86)\Tesseract-OCR)",0
pip install IPTCInfo3,0
from the ai4eutils repo: https://github.com/Microsoft/ai4eutils,0
,0
"Only used for writing out a summary, not important for the core metadata extraction",0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Using a semi-arbitrary metric of how much it feels like we found the,0
"text-containing region, discard regions that appear to be extraction failures",0
Pad each crop with a few pixels to make tesseract happy,0
Discard text from the top,0
"When we're looking for pixels that match the background color, allow some",0
tolerance around the dominant color,0
We need to see a consistent color in at least this fraction of pixels in our rough,0
crop to believe that we actually found a candidate metadata region.,0
"What fraction of the [top,bottom] of the image should we use for our rough crop?",0
A row is considered a probable metadata row if it contains at least this fraction,0
"of the background color.  This is used only to find the top and bottom of the crop area,",0
"so it's not that *every* row needs to hit this criteria, only the rows that are generally",0
above and below the text.,0
%% Support functions,0
"%% Load some images, pull EXIF and IPTC data for fun",0
%% Rough crop,0
"This will be an nImages x 1 list of 2 x 1 lists (image top, image bottom)",0
image = images[0],0
"l,t,r,b",0
,0
"0,0 is upper-left",0
"%% Close-crop around the text, return a revised image and success metric",0
Did we find a sensible mode that looks like a background value?,0
"This looks very scientific, right?  Definitely a probability?",0
"print('Failed min background fraction test: {} of {}'.format(pBackGroundValue,minBackgroundFraction))",0
"Notes to self, things I tried that didn't really go anywhere...",0
"analysisImage = cv2.blur(analysisImage, (3,3))",0
"analysisImage = cv2.medianBlur(analysisImage,5)",0
"analysisImage = cv2.Canny(analysisImage,100,100)",0
imagePil = Image.fromarray(analysisImage); imagePil,0
Use row heuristics to refine the crop,0
,0
This egregious block of code makes me miss my fluency in Matlab.,1
"print('Cropping to {},{},{},{}'.format(x,y,w,h))",0
Crop the image,0
"For some reason, tesseract doesn't like characters really close to the edge",0
imagePil = Image.fromarray(croppedImage); imagePil,0
%% Go to OCR-town,0
"An nImages x 2 list of strings, extracted from the top and bottom of each image",0
An nImages x 2 list of cropped images,0
iImage = 0; iRegion = 1; regionSet = imageRegions[iImage]; region = regionSet[iRegion],0
text = pytesseract.image_to_string(region),0
pil --> cv2,0
"image = cv2.medianBlur(image, 3)",0
"image = cv2.erode(image, None, iterations=2)",0
"image = cv2.dilate(image, None, iterations=4)",0
"image = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]",0
"image = cv2.blur(image, (3,3))",0
"image = cv2.copyMakeBorder(image,10,10,10,10,cv2.BORDER_CONSTANT,value=[0,0,0])",0
"text = pytesseract.image_to_string(imagePil, lang='eng')",0
https://github.com/tesseract-ocr/tesseract/wiki/Command-Line-Usage,0
"psm 6: ""assume a single uniform block of text""",0
,0
...for each cropped region,0
...for each image,0
%% Extract dates and times,0
s = '1:22 pm',0
s = '1:23:44 pm',0
%% Write results to a handy html file,0
Add image name and resized image,0
Add results and individual region images,0
"textStyle = ""font-family:calibri,verdana,arial;font-weight:bold;font-size:150%;text-align:left;margin-left:50px;""",0
%% Scrap,0
Alternative approaches to finding the text/background  region,0
Using findCountours(),0
imagePil = Image.fromarray(analysisImage); imagePil,0
"analysisImage = cv2.erode(analysisImage, None, iterations=3)",0
"analysisImage = cv2.dilate(analysisImage, None, iterations=3)",0
"analysisImage = cv2.threshold(analysisImage, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]",0
Find object with the biggest bounding box,0
Using connectedComponents(),0
analysisImage = image,0
print('Found {} components'.format(nb_components)),0
We just want the *background* image,0
open the file,0
read it,0
do the substitution,0
matplotlib.use('Agg'),0
from UIComponents.DBObjects import *,0
Initialize Database,0
# database connection credentials,0
# try to connect as USER to database DB_NAME through peewee,0
Load the saved embedding model,0
dataset_query = Detection.select().limit(5),0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})",0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getalllabels()) , dataset.getallpaths(), {})",0
Random examples to start,0
"random_ids = np.random.choice(dataset.current_set, 1000, replace=False).tolist()",0
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)",0
print(random_ids),0
Move Records,0
"moveRecords(dataset, DetectionKind.ModelDetection.value, DetectionKind.UserDetection.value, random_ids)",0
#print([len(x) for x in dataset.set_indices]),0
# Finetune the embedding model,0
#dataset.set_kind(DetectionKind.UserDetection.value),0
#dataset.train(),0
"#train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)",0
"#finetune_embedding(model, checkpoint['loss_type'], dataset, 32, 4, 100)",0
#save_checkpoint({,0
"#        'arch': model.arch,",0
"#        'state_dict': model.state_dict(),",0
"#        'optimizer' : optimizer.state_dict(),",0
"#        'loss_type' : loss_type,",0
"#        }, False, ""%s%s_%s_%04d.tar""%('finetuned', loss_type, model.arch, len(dataset.set_indices[DetectionKind.UserDetection.value])))",0
Get indices of samples to get user to label,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
"kwargs[""already_selected""].extend(indices)",0
Train on samples that have been labeled so far,0
Test on the samples that have not been labeled,0
"'optimizer' : optimizer.state_dict(),",0
num_classes= len(train_dataset.getClassesInfo()[0]),0
"print(""Num Classes= ""+str(num_classes))",0
define loss function (criterion) and optimizer,0
"optimizer = torch.optim.SGD(params, momentum = 0.9, lr = args.lr, weight_decay = args.weight_decay)",0
optimizer.load_state_dict(checkpoint['optimizer']),0
train for one epoch,0
"adjust_lr(optimizer,epoch)",0
if epoch % 1 == 0 and epoch > 0:,0
"a, b, c = e.predict(train_embd_loader, load_info = True, dim = args.feat_dim)",0
"plot_embedding(reduce_dimensionality(a), b, c, {})",0
evaluate on validation set,0
"embedding_net = EmbeddingNet('resnet50', 256, True)",0
compute output,0
val_loader = train_dataset.getSingleLoader(batch_size = 8),0
"for a, b , c in val_loader:",0
print(b[0]),0
"plt.imshow(np.rollaxis(np.rollaxis(a[0].numpy(), 1, 0), 2, 1))",0
plt.show(),0
"print(np.rollaxis(a[0].numpy() , 1, 0).shape)",0
--------some stuff needed to get AJAX to work with bottle?--------#,0
-------------------------------------------------------------------------------- #,0
PREPARE TO QUEUE IMAGES FOR LABELING,0
-------------------------------------------------------------------------------- #,0
# Connect as USER to database DB_NAME through peewee and initialize database proxy,0
# Load embedding model,0
---------------------------------------------------------------------- #,0
CREATE QUEUE OF IMAGES TO LABEL,0
---------------------------------------------------------------------- #,0
Use classifier to generate predictions,0
# Update model predicted class in PostgreSQL database,0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?,0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
-------------------------------------------------------------------------------- #,0
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI,0
-------------------------------------------------------------------------------- #,0
"# static routes (to serve CSS, etc.)",0
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps",0
"return bottle.static_file(filename, root='../../../../../../../../../../../.')",0
# dynamic routes,0
if data['display_grayscale']:,0
indices_to_exclude.update(set(color_indices)),0
elif not data['display_grayscale']:,0
indices_to_exclude.update(set(grayscale_indices)),0
data['display_images'] = {},0
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices],0
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices],0
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices],0
data['display_images']['detection_categories'] = [],0
for i in indices:,0
if str(dataset.samples[i][1]) == 'None':,0
data['display_images']['detection_categories'].append('None'),0
else:,0
existing_category_entries = {cat.id: cat.name for cat in Category.select()},0
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()",0
data['display_images']['detection_categories'].append(cat_name),0
Use image ids in images_to_label to get the corresponding dataset indices,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
print(set(dataset.set_indices[4]).update(set(indices_to_label))),0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Use image id images_to_label to get the corresponding dataset index,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?,0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Train on samples that have been labeled so far,0
dataset.set_kind(DetectionKind.UserDetection.value),0
print(y_train),0
Predict on the samples that have not been labeled,0
print(y_pred),0
Update model predicted class in PostgreSQL database,0
timer = time.time(),0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
print('Updating the database took %0.2f seconds'%(time.time() - timer)),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids)),0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
"once the classifier has been trained the first time, switch to AL sampling",0
# sourceMappingURL=bootstrap.bundle.min.js.map,0
"print(ap_distances.size(),an_distances.size())",0
losses = -(((-ap_distances)/128)+1+1e-16).log() - (((-(128-an_distances))/128)+1+1e-16).log(),0
import pdb,0
pdb.set_trace(),0
losses = ap_distances - an_distances + self.margin,0
print(losses.size()),0
from UIComponents.DBObjects import *,0
TODO: should this also change self.kind?,1
get the embedding representations for all samples (i.e. set current_set to all indices),0
"return PILImage.open(os.path.join(self.img_base,path)).convert('RGB')",0
"print(self.labels_set, self.n_classes)",0
from sklearn.manifold import TSNE,0
embedding= TSNE(n_components=2).fit_transform(X),0
embedding= PCA(n_components=2).fit_transform(X),0
return X,0
"print(dir(event), type(sc))",0
"print(label,bgcolor)",0
"patches.append(mpatches.Patch(color=indexcolors[i], label=label))",0
"plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,",0
"ncol=12, mode=""expand"", borderaxespad=0., handles=patches)",0
plt.legend(handles=patches),0
"plt.xlabel('Dim 1', fontsize=12)",0
"plt.ylabel('Dim 2', fontsize=12)",0
plt.grid(True),0
"plt.xlabel('Dim 1', fontsize=12)",0
"plt.ylabel('Dim 2', fontsize=12)",0
plt.grid(True),0
print(thumb),0
"img.thumbnail((16, 12), PILImage.ANTIALIAS)",0
plt.show(),0
Add all negatives for all positive pairs,0
print(triplets.shape[0]),0
compute output,0
measure accuracy,0
compute loss on this batch,0
"train on a batch, record loss, and measure accuracy (if calc_accuracy)",0
compute output,0
measure accuracy and record loss,0
switch to evaluate mode,0
compute output,0
switch to evaluate mode,0
"self.fc13 = nn.Linear(128, 64)",0
self.bn2 = nn.BatchNorm1d(64),0
x = F.relu(self.fc12(x)),0
x = F.relu(self.bn1(self.fc13(x))),0
x = F.relu(self.fc13(x)),0
"x = F.dropout(x, training=self.training)",0
save features last FC layer,0
x = F.relu(x),0
save features last FC layer,0
import matplotlib.pyplot as plt,0
plt.switch_backend('agg'),0
class EmbeddingNet(nn.Module):,0
"def __init__(self, architecture, feat_dim, use_pretrained=False):",0
"super(EmbeddingNet, self).__init__()",0
self.feat_dim= feat_dim,0
self.inner_model = models.__dict__[architecture](pretrained=use_pretrained),0
if architecture.startswith('resnet'):,0
in_feats= self.inner_model.fc.in_features,0
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)",0
elif architecture.startswith('inception'):,0
in_feats= self.inner_model.fc.in_features,0
"self.inner_model.fc = nn.Linear(in_feats, feat_dim)",0
if architecture.startswith('densenet'):,0
in_feats= self.inner_model.classifier.in_features,0
"self.inner_model.classifier = nn.Linear(in_feats, feat_dim)",0
if architecture.startswith('vgg'):,0
in_feats= self.inner_model.classifier._modules['6'].in_features,0
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)",0
if architecture.startswith('alexnet'):,0
in_feats= self.inner_model.classifier._modules['6'].in_features,0
"self.inner_model.classifier._modules['6'] = nn.Linear(in_feats, feat_dim)",0
"def forward(self, x):",0
return self.inner_model.forward(x),0
class NormalizedEmbeddingNet(EmbeddingNet):,0
"def __init__(self, architecture, feat_dim, use_pretrained=False):",0
"EmbeddingNet.__init__(self, architecture, feat_dim, use_pretrained = use_pretrained)",0
"def forward(self, x):",0
embedding =  F.normalize(self.inner_model.forward(x))*10.0,0
"return embedding, embedding",0
"def get_random_images(num, image_dir, test_transforms):",0
"data = datasets.ImageFolder(image_dir, transform=test_transforms) # slight abuse; this expects subfolders corresponding to classes but we have no classes here",0
indices = list(range(len(data))),0
np.random.shuffle(indices),0
idx = indices[:num],0
from torch.utils.data.sampler import SubsetRandomSampler,0
sampler = SubsetRandomSampler(idx),0
"loader = torch.utils.data.DataLoader(data,",0
"sampler=sampler, batch_size=num)",0
dataiter = iter(loader),0
"images, labels = dataiter.next()",0
"return images, labels",0
"def predict_image(image, model, test_transforms):",0
"device = torch.device(""cuda"" if torch.cuda.is_available()",0
"else ""cpu"")",0
image_tensor = test_transforms(image).float(),0
image_tensor = image_tensor.unsqueeze_(0),0
input = Variable(image_tensor),0
input = input.to(device),0
output = model(input)[0],0
return output.data.cpu().numpy(),0
Connect to database and initialize db_proxy,0
# database connection credentials,0
# load the dataset,0
Load the saved embedding model from the checkpoint,0
# update the dataset embedding,0
# Create a folder for saving embedding visualizations with this model checkpoint,0
model_emb_dirname = os.path.basename(args.base_model).split('.')[0],0
"os.makedirs(model_emb_dirname, exist_ok=True)",0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')",0
dataset.embedding_mode(),0
"assert 2==3, 'break'",0
datasetindices = list(range(len(dataset))),0
np.random.shuffle(datasetindices),0
random_indices = datasetindices[:args.num],0
print(random_indices),0
"selected_sample_features = np.array([]).reshape(0, 256)",0
selected_sample_labels = [],0
for idx in random_indices:,0
"selected_sample_features = np.vstack([selected_sample_features, X_train[idx]])",0
selected_sample_labels.append(y_train[idx]),0
img_path = imagepaths[idx].split('.JPG')[0],0
image = dataset.loader(img_path),0
selected_sample_images.append(image),0
# TRY NEAREST NEIGHBORS WALK THROUGH EMBEDDING,0
nbrs = NearestNeighbors(n_neighbors=args.num).fit(selected_sample_features),0
"distances, indices = nbrs.kneighbors(selected_sample_features)",0
"plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')",0
"idx_w_closest_nbr = np.where(distances[:,1] == min(distances[:,1]))[0][0]",0
order = [idx_w_closest_nbr],0
for ii in range(len(distances)):,0
"distances[ii, 0] = np.inf",0
while len(order)<args.num:,0
curr_idx = order[-1],0
curr_neighbors = indices[curr_idx],0
curr_dists = list(distances[curr_idx]),0
# print(min(curr_dists)),0
next_closest_pos = curr_dists.index(min(curr_dists)),0
next_closest = curr_neighbors[next_closest_pos],0
order.append(next_closest),0
# make sure you can't revisit past nodes,0
for vi in order:,0
vi_pos = list(indices[next_closest]).index(vi),0
"distances[next_closest, vi_pos] = np.inf",0
for ii in range(len(order)):,0
imgidx = order[ii],0
image = selected_sample_images[imgidx],0
"image.save(model_emb_dirname+""/img""+str(ii)+""_""+str(selected_sample_labels[imgidx])+"".png"")",0
# Specify the transformations on the input images before inference,0
"# test_transforms = transforms.Compose([transforms.Resize([224, 224]), transforms.ToTensor()])",0
"test_transforms = transforms.Compose([transforms.Resize([256, 256]), transforms.RandomCrop([224, 224]), transforms.RandomHorizontalFlip(), transforms.ColorJitter(), transforms.ToTensor(), transforms.Normalize([0.407328, 0.407328, 0.407328], [0.118641, 0.118641, 0.118641])])",0
"images, labels = get_random_images(num, image_dir, test_transforms)",0
"all_features = np.array([]).reshape(0, 256)",0
for ii in range(len(images)):,0
image = to_pil(images[ii]),0
"features = predict_image(image, model, test_transforms)",0
"all_features = np.vstack([all_features, features])",0
# for ii in range(len(images)):,0
#     image = to_pil(images[ii]),0
"#     image.save(""img""+str(ii)+"".png"")",0
# TRY CLUSTERING,0
kmeans1 = KMeans(n_clusters=5).fit(StandardScaler().fit_transform(all_features)),0
print(kmeans1.labels_),0
for ii in range(len(images)):,0
image = to_pil(images[ii]),0
"filename = str(kmeans1.labels_[ii])+""/img""+str(ii)+"".png""",0
if not os.path.exists(os.path.dirname(filename)):,0
os.makedirs(os.path.dirname(filename)),0
image.save(filename),0
class SaveFeatures():,0
"def __init__(self, module):",0
self.hook = module.register_forward_hook(self.hook_fn),0
"def hook_fn(self, module, input, output):",0
"self.features = torch.tensor(output, requires_grad=True).cuda()",0
def close(self):,0
self.hook.remove(),0
Load the saved embedding model from the checkpoint,0
"Get a sample from the database, with eval transforms applied, etc.",0
Connect to database and sample a dataset,0
output = model.forward(sample_image.unsqueeze(0)),0
print(output),0
with torch.no_grad():,0
sample_image_input = sample_image.cuda(non_blocking=True),0
"_, output = model(sample_image_input) # compute output",0
print(output),0
sample_image = PILImage.open(sample_image_path).convert('RGB'),0
"sample_image = transforms.Compose([Resize([256, 256]), CenterCrop(([[224,224]])), ToTensor(), Normalize([0.369875, 0.388726, 0.347536], [0.136821, 0.143952, 0.145229])])(sample_image)",0
print(list(model_inner_resnet.children())),0
print(model_inner_resnet.fc),0
print(model_inner_resnet.fc0),0
# print(model_inner_resnet.layer4[0].conv2),0
# print(type(model)),0
# print(len(list(model_inner_resnet.children()))),0
# print(list(model.children())),0
# print(list(list(model.children())[0].children())),0
"img = np.uint8(np.random.uniform(150, 180, (56, 56, 3)))/255",0
"img_tensor = torch.unsqueeze(torch.from_numpy(img), 0)",0
full_out = model_inner_resnet.forward(img_tensor),0
print(full_out),0
model(img_tensor),0
activations = SaveFeatures(model_inner_resnet.layer4[0].conv2),0
print(activations.features),0
print(type(activations.features)),0
activations.close(),0
Connect to database and sample a dataset,0
Load the saved embedding model from the checkpoint,0
Update the dataset embedding,0
Get a random query image,0
# # # IMAGES IN THE SAME SEQUENCE # # # #,0
"assert 2==3, 'break'",0
# # # CLOSEST IN (EMBEDDING) FEATURE SPACE # # # #,0
Connect to database DB_NAME as USER and initialize tables,0
Populate Info table,0
Populate Category table,0
"# For now, we have a predefined list of species we expect to see in the camera trap database (e.g. maybe from a quick look through the images)",0
# TODO: allow user to update the class list through the labeling tool UI as they see different species,1
Populate Image and Detection tables,0
Image entry data,0
Detection entry data,0
Connect to database DB_NAME as USER and initialize tables,0
Populate Info table,0
Populate Category table,0
# Create category records from species present in the COCO camera trap classification dataset (COCO classes minus any excluded classes),0
# TODO: allow user to update the class list through the labeling tool UI as they see different species,1
Populate Image and Detection tables,0
"with open(os.path.join(args.crop_dir,'crops.json'), 'r') as infile:",0
crops_json = json.load(infile),0
counter = 0,0
timer = time.time(),0
num_detections = len(crops_json),0
for detectionid in crops_json:,0
counter += 1,0
detection_data = crops_json[detectionid],0
# Image entry data,0
existing_image_entries = Image.select().where((Image.file_name == detection_data['file_name'])),0
try:,0
existing_image_entry = existing_image_entries.get(),0
except:,0
"image_entry = Image.create(id=detectionid, file_name=detection_data['file_name'], width=detection_data['width'], height=detection_data['height'], grayscale=detection_data['grayscale'],",0
"source_file_name=detection_data['source_file_name'], relative_size=detection_data['relative_size'],",0
"seq_id=detection_data['seq_id'], seq_num_frames=detection_data['seq_num_frames'], frame_num=detection_data['frame_num'])",0
image_entry.save(),0
# Detection entry data,0
"detection_entry = Detection.create(id=detectionid, image=detectionid, bbox_confidence=detection_data['bbox_confidence'],",0
"bbox_X1=detection_data['bbox_X1'], bbox_Y1=detection_data['bbox_Y1'], bbox_X2=detection_data['bbox_X2'], bbox_Y2=detection_data['bbox_Y2'],",0
kind=DetectionKind.ModelDetection.value),0
detection_entry.save(),0
if counter%100 == 0:,0
"print('Updated database with Image and Detection table entries for %d out of %d crops in %0.2f seconds'%(counter, num_detections, time.time() - timer))",0
# data related to original image,0
Get class names from .txt list,0
Initialize Oracle table,0
Map filenames to classes (NOTE: we assume a single image does not contain more than one class),0
"For each detection, use source image path to get class",0
TODO update: Assumes that crops have already,1
been generated for the images using make_active_learning_classification_dataset.py. The created DB contains tables:,0
- info: information about the dataset,0
- image: images present in the dataset,0
- detections: crops of images with detections with confidence greater than a specified threshold,0
Initialize Database,0
# database connection credentials,0
HOST = 'localhost',0
PORT = 5432,0
"# first, make sure the (user, password) has been created",0
"# sudo -u postgres psql -c ""CREATE USER <db_user> WITH PASSWORD <db_password>;""",0
"# sudo -u postgres psql -c ""CREATE DATABASE <db_name> WITH OWNER <db_user> CONNECTION LIMIT -1;""",0
"# sudo -u postgres psql -c ""GRANT CONNECT ON DATABASE <db_name> TO <db_user>;""",0
"# sudo -u postgres psql -d <db_name> -c ""CREATE EXTENSION IF NOT EXISTS \""uuid-ossp\"";""",0
# Try to connect as USER to database DB_NAME through peewee,0
Populate Tables,0
# create Info table,0
# get class names for Category table,0
Faster anD available in Python 3.5 and above,0
# iterate through images in each class folder,0
killing this process after over 38 hours adding over 500k white-tailed deer crops from emammal,0
resuming for remaining classes,0
# get cropped image data for Image table,0
"if mean of each channel is about the same, image is likely grayscale",0
# still have no info on these:,0
seq_id = CharField(null= True)                # sequence identifier for the original image,0
seq_num_frames = IntegerField(null = True)    # number of frames in sequence,0
frame_num = IntegerField(null = True)         # which frame number in sequence,0
location = CharField(null = True)             # location of camera trap,0
datetime = DateTimeField(null = True),0
# store info about the detection corresponding to this image,0
# store info about the true labels for the detection,0
#  - for pretrain dataset this is the same as the detection_category if the detection categories,0
print(classes),0
Connect to database and sample a dataset,0
Load the saved embedding model from the checkpoint,0
Update the dataset embedding,0
save the images,0
save the features,0
"with open(os.path.join(args.output_dir, 'lastlayer_features.mat'), 'wb') as f:",0
"pickle.dump(sample_features, f)",0
"with open(os.path.join(args.output_dir, 'labels.mat'), 'wb') as f:",0
"pickle.dump(sample_labels, f)",0
"parser.add_argument('--db_name', default='missouricameratraps', type=str, help='Name of the training (target) data Postgres DB.')",0
"parser.add_argument('--db_user', default='user', type=str, help='Name of the user accessing the Postgres DB.')",0
"parser.add_argument('--db_password', default='password', type=str, help='Password of the user accessing the Postgres DB.')",0
"parser.add_argument('--base_model', type=str, help='Path to latest embedding model checkpoint.')",0
"parser.add_argument('--output_dir', type=str, help='Output directory for subset of crops')",0
Add json entry for this crop,0
Copy file for this crop to subset dataset crop dir,0
Copy file for its full-size source image to subset dataset image dir,0
Write crops.json to subset dataset crop dir,0
store info about the crops produced in a JSON file,0
------------------------------------------------------------------------------------------------------------#,0
COMMENT OUT IF NOT USING A SPECIFIC PROJECT WITHIN ROBERT LONG EMAMMAL DATASET,0
------------------------------------------------------------------------------------------------------------#,0
get some information about the source image,0
------------------------------------------------------------------------------------------------------------#,0
NOTE: EDIT THIS SECTION BASED ON DATASET SOURCE,0
get info about sequence the source image belongs to from path and directory,0
# missouricameratraps:,0
imgframenum = int(os.path.basename(imgfile).split('.JPG')[0].split('_')[-1]),0
imgseqid = int(os.path.split(os.path.dirname(imgfile))[-1]),0
"imgseqnumframes = len([name for name in os.listdir(os.path.dirname(imgfile)) if os.path.isfile(os.path.join(os.path.dirname(imgfile), name))])",0
# emammal:,0
------------------------------------------------------------------------------------------------------------#,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Variables for the hierarchical cluster,0
Variables for the AL algorithm,0
Data variables,0
"connectivity = kneighbors_graph(self.transformed_X,max_features)",0
Fit cluster and update cluster variables,0
The sklearn hierarchical clustering algo numbers leaves which correspond,0
to actual datapoints 0 to n_points - 1 and all internal nodes have,0
ids greater than n_points - 1 with the root having the highest node id,0
"If no labels have been observed, simply return uniform distribution",0
"If no observations, return worst possible upper lower bounds",0
Loop through generations from bottom to top,0
Update admissible labels for node,0
Calculate score,0
Determine if node should be split,0
Make sure label set for node so that we can flow to children,0
if necessary,0
Only split if all ancestors are admissible nodes,0
This is part  of definition of admissible pruning,0
Check that pruning covers all leave nodes,0
Fill in labels,0
Observe labels for previously recommended batches,0
TODO(lishal): implement multiple selection methods,1
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
A list of initialized samplers is allowed as an input because,0
"for AL_methods that search over different mixtures, may want mixtures to",0
have shared AL_methods so that initialization is only performed once for,0
computation intensive methods like HierarchicalClusteringAL and,0
states are shared between mixtures.,0
"If initialized samplers are not provided, initialize them ourselves.",0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copy these objects to make sure they are not modified while simulating,0
trajectories as they are used later by the main run_experiment script.,0
Assumes that model has already by fit using all labeled data so,0
the probabilities can be used immediately to hallucinate labels,0
All models need to have predict_proba method,0
Hallucinate labels for selected datapoints to be label,0
using class probabilities from model,0
"Not saving already_selected here, if saving then should sort",0
only for the input to fit but preserve ordering of indices in,0
already_selected,0
Useful to know how accuracy compares for model trained on hallucinated,0
labels vs trained on true labels.  But can remove this train to speed,0
up simulations.  Won't speed up significantly since many more models,0
are being trained inside the loop above.,0
Save trajectory for reference,0
Delete created copies,0
THE INPUTS CANNOT BE MODIFIED SO WE MAKE COPIES FOR THE CHECK LATER,0
Should check model but kernel_svm does not have coef_ so need better,0
handling here,0
Make sure that model object fed in did not change during simulations,0
Return indices based on return type specified,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Set gamma for gaussian kernel to be equal to 1/n_features,0
kneighbors graph is constructed using k=10,0
"Make connectivity matrix symmetric, if a point is a k nearest neighbor of",0
"another point, make it vice versa",0
Graph edges are weighted by applying gaussian kernel to manhattan dist.,0
"By default, gamma for rbf kernel is equal to 1/n_features but may",0
get better results if gamma is tuned.,0
Define graph density for an observation to be sum of weights for all,0
edges to the node representing the datapoint.  Normalize sum weights,0
by total number of neighbors.,0
"If a neighbor has already been sampled, reduce the graph density",0
for its direct neighbors to promote diversity.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Update observed reward and arm probabilities,0
Sample an arm,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
y only used for determining how many clusters there should be,0
probably not practical to assume we know # of classes before hand,0
should also probably scale with dimensionality of data,0
Probably okay to always use MiniBatchKMeans,0
Should standardize data before clustering,0
Can cluster on standardized data but train on raw features if desired,0
TODO(lishal): have MarginSampler and this share margin function,1
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Probably okay to always use MiniBatchKMeans,0
Should standardize data before clustering,0
Can cluster on standardized data but train on raw features if desired,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Update min_distances for all examples given new cluster center.,0
Assumes that the transform function takes in original data and not,0
flattened data.,0
Initialize centers with a randomly selected datapoint,0
New examples should not be in already selected since those points,0
should have min_distance of zero to a cluster center.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
This is uniform given the remaining pool but biased wrt the entire pool.,0
sample = [i for i in range(self.X.shape[0]) if i not in already_selected],0
return sample[0:N],0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Fields for hierarchical clustering AL,0
Setting parent and storing nodes in dict for fast access,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Copyright 2017 Google Inc.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
Faster and available in Python 3.5 and above,0
transform_list.append(CenterCrop((processed_size))),0
transform_list.append(Lambda(lambda X: normalize(X))),0
"print(self.labels_set, self.n_classes)",0
"from PyQt5 import QtCore, QtWidgets,QtGui",0
from collections import deque,0
from peewee import *,0
from UIComponents.Tag import Tag,0
policy.setHeightForWidth(True),0
"print(self.tab1.parentWidget(),self)",0
self.tab4.add.clicked.connect(self.addSpecies),0
self.tab4.update.clicked.connect(self.updateSpecies),0
checkpoint= load_checkpoint('../merge/triplet_model_0054.tar'),0
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))",0
"embedding_net = EmbeddingNet(checkpoint['arch'], checkpoint['feat_dim'])",0
if checkpoint['loss_type'].lower()=='center':,0
"model = torch.nn.DataParallel(ClassificationNet(embedding_net, n_classes=14)).cuda()",0
else:,0
model= torch.nn.DataParallel(embedding_net).cuda(),0
model.load_state_dict(checkpoint['state_dict']),0
self.parentWidget().progressBar.setMaximum(len(run_dataset)//2048),0
"e=Engine(model,None,None, verbose=True,progressBar= self.parentWidget().progressBar)",0
label = [ x[1] for x in run_dataset.samples],0
"print(indices,selected_set)",0
print(query.sql()),0
src.delete().where(src.image_id<<rList)),0
"det= UserDetection.create(category_id=0, id=str(index+label[1][2]),image_id=final[0], bbox_X=label[1][0], bbox_Y=label[1][1], bbox_W=label[1][2], bbox_H=label[1][3])",0
for x in self.tab1.grid.tags:,0
x.delete_instance(),0
db.create_tables([Detection]),0
This is simply to show the bar,0
"p = Process(target=ex.active, args=())",0
p.start(),0
p.join(),0
ex.active(),0
ex.centralWidget().setCurrentIndex(1),0
main(),0
"run_dataset.setup(Detection.select(Detection.id,Category.id).join(Category).where(Detection.kind==DetectionKind.ModelDetection.value).limit(250000))",0
print(row),0
"unq_id= ""crops_""+str(uuid.uuid1())",0
"print(line,imageWidth,imageHeight)",0
"print(""%s,%s,%s,%0.6f,%0.6f,%0.6f,%0.6f,%0.6f""%(line[0], line[1],line[2],float(line[3]),topRel,leftRel,bottomRel,rightRel))",0
if not os.path.exists(dest):,0
os.mkdir(dest),0
raise,0
out.close(),0
"print length,(i-1)*length,i*length",0
matplotlib.use('Agg'),0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
"conf.reset()""""""",0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
conf.reset(),0
for clf in classifiers:,0
"clf.fit(X_train, y_train)",0
preds= clf.predict_proba(X),0
uncertainty+= preds.max(axis=1),0
print(uncertainty[ind]),0
print(uncertainty[ind]),0
true_labels = y[unlabeled_indices],0
select up to 5 digit examples that the classifier is most uncertain about,0
remember best acc@1 and save checkpoint,0
"completeClassificationLoop(run_dataset, model,num_classes)",0
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)",0
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd),0
labels = db.labels_,0
"mapp=(find_probablemap(label,labels, K=args.K))",0
"print(""Clusters"")",0
"for i,x in enumerate(labels):",0
labels[i]= mapp[x],0
print(np.sum(labels == label)/labels.size),0
"print(""Confidence Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning(embd, label, idx)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(""Entropy Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning_entropy(embd, label, idx)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(idx,idx.shape)",0
for i in idx:,0
print(paths[i]),0
"print(""Silohette active learning"")",0
"idx= active_learning2(embd, 1000, args.num_clusters)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
embd= reduce_dimensionality(embd)#[0:10000]),0
labels= labels[0:10000],0
label= label[0:10000],0
paths= paths[0:10000],0
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])",0
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])",0
plt.show(),0
"np.save(args.name_prefix+""_embeddings.npy"",embd)",0
"np.save(args.name_prefix+""_labels.npy"",label)",0
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")",0
del output,0
define loss function (criterion) and optimizer,0
"optimizer = torch.optim.Adam(params, lr=args.lr, weight_decay= args.weight_decay)",0
optimizer.load_state_dict(checkpoint['optimizer']),0
train for one epoch,0
evaluate on validation set,0
matplotlib.use('Agg'),0
selected_set.add(rand_ind[i]),0
"embedding_net = EmbeddingNet('resnet50', 256, True)",0
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)",0
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)",0
print('Embedding Done'),0
sys.stdout.flush(),0
Random examples to start,0
"random_ids = noveltySamples(unlabeled_dataset.em, unlabeled_dataset.getIDs(), 1000)",0
print(random_ids),0
Move Records,0
Finetune the embedding model,0
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)",0
unlabeled_dataset.updateEmbedding(model),0
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})",0
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()",0
matplotlib.use('Agg'),0
"uncertainty= np.apply_along_axis(stats.entropy,1,probs) * (1 - probs.max(axis=1))",0
"copy(paths[srt[i]], ""active"")",0
"plot_together( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()), preds, base_ind, dataset.getpaths(), {})",0
"return np.random.choice(range(0,prob_list[0].shape[0]), 100, replace=False).tolist()",0
selected_set.add(rand_ind[i]),0
"query= Detection.update(kind = destKind.value).where(Detection.id.in_(rList), Detection.kind == srcKind.value)",0
query.execute(),0
"embedding_net = EmbeddingNet('resnet50', 256, True)",0
"unlabeledset_query= Detection.select(Detection.id,Oracle.label).join(Oracle).where(Detection.kind==DetectionKind.ModelDetection.value).order_by(fn.random()).limit(150000)",0
"unlabeled_dataset = SQLDataLoader(unlabeledset_query, os.path.join(args.run_data, ""crops""), is_training= False, num_workers= 8)",0
print('Embedding Done'),0
sys.stdout.flush(),0
"plot_embedding(dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})",0
Random examples to start,0
"random_ids = selectSamples(dataset.em[dataset.current_set], dataset.current_set, 2000)",0
print(random_ids),0
Move Records,0
Finetune the embedding model,0
"train_dataset = SQLDataLoader(trainset_query, os.path.join(args.run_data, 'crops'), is_training= True)",0
unlabeled_dataset.updateEmbedding(model),0
"print(dataset.em[dataset.current_set].shape, np.asarray(dataset.getlabels()).shape, len(dataset.getpaths()))",0
"plot_embedding( dataset.em[dataset.current_set], np.asarray(dataset.getlabels()) , dataset.getpaths(), {})",0
"plot_embedding( unlabeled_dataset.em, np.asarray(unlabeled_dataset.getlabels()) , unlabeled_dataset.getIDs(), {})",0
train_eval_classifier(),0
"clf_model = ClassificationNet(checkpoint['feat_dim'], 48).cuda()",0
"names = [""Linear SVM"", ""RBF SVM"", ""Random Forest"", ""Neural Net"", ""Naive Bayes""]",0
"classifiers = [SVC(kernel=""linear"", C=0.025, probability= True, class_weight='balanced'),",0
"SVC(gamma=2, C=1, probability= True, class_weight='balanced'),",0
"RandomForestClassifier(max_depth=None, n_estimators=100, class_weight='balanced'),",0
"MLPClassifier(alpha=1),",0
GaussianNB()],0
estimators= [],0
"for name, clf in zip(names, classifiers):",0
"estimators.append((name, clf))",0
"eclf1 = VotingClassifier(estimators= estimators, voting='hard')",0
"eclf2 = VotingClassifier(estimators= estimators, voting='soft')",0
"names.append(""ensemble hard"")",0
classifiers.append(eclf1),0
"names.append(""ensemble soft"")",0
classifiers.append(eclf2),0
dataset.image_mode(),0
dataset.updateEmbedding(model),0
y_pred= clf.predict(X_test),0
"print(confusion_matrix(y_test, y_pred))",0
paths= dataset.getpaths(),0
"for i, (yp, yt) in enumerate(zip(y_pred, y_test)):",0
if yp != yt:,0
"copy(paths[i],""mistakes"")",0
"print(yt, yp, paths[i],i)",0
"clf_output= clf_e.embedding(eval_loader, dim=48)",0
"self.train_transform = transforms.Compose([Resize(raw_size), CenterCrop((processed_size)), ToTensor(), Normalize(mean, std)])",0
"print(self.labels_set, self.n_classes)",0
"print(line,imageWidth,imageHeight)",0
"print length,(i-1)*length,i*length",0
#########################################################,0
## Configuration,0
#########################################################,0
## The actual code,0
Check arguments,0
Create output directories,0
Padding around the detected objects when cropping,0
1.3 for the cropping during test time and 1.3 for,0
the context that the CNN requires in the left-over,0
image,0
Load a (frozen) Tensorflow model into memory.,0
## Preparations: get all the output tensors,0
The following processing is only for single image,0
Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.,0
Follow the convention by adding back the batch dimension,0
For all images in the image directoryig,0
Load image,0
Run inference,0
"all outputs are float32 numpy arrays, so convert types as appropriate",0
Add detections to the collection,0
Get info about the image,0
Select detections with a confidence larger than DETECTION_CONFIDENCE,0
Get these boxes and convert normalized coordinates to pixel coordinates,0
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes",0
"However, we need to make sure that it box coordinates are still within the image",0
"For each detected bounding box with high confidence, we will",0
crop the image to the padded box and save it,0
generate a unique identifier for the detection,0
"bbox is the detected box, crop_box the padded / enlarged box",0
Add numbering to the original file name if there are multiple boxes,0
The absolute file path where we will store the image,0
"if COCO_OUTPUT_DIR is set, then we will only use the shape",0
of cropped_img in the following code. So instead of reading,0
cropped_img = np.array(Image.open(out_file)),0
we can speed everything up by reading only the size of the image,0
matplotlib.use('Agg'),0
conf= ConfusionMatrix(24),0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
get the inputs,0
zero the parameter gradients,0
forward + backward + optimize,0
get the inputs,0
forward + backward + optimize,0
print statistics,0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
"conf.reset()""""""",0
define loss function (criterion) and optimizer,0
conf= NPConfusionMatrix(10),0
all_indices= set(range(len(y))),0
diff= all_indices.difference(base_ind),0
"conf.update(preds_tr,y_train)",0
"classes= np.apply_along_axis(conf.classScore,0,preds.argmax(axis=1))",0
conf.reset(),0
for clf in classifiers:,0
"clf.fit(X_train, y_train)",0
preds= clf.predict_proba(X),0
uncertainty+= preds.max(axis=1),0
print(uncertainty[ind]),0
print(uncertainty[ind]),0
true_labels = y[unlabeled_indices],0
select up to 5 digit examples that the classifier is most uncertain about,0
"print(indices,selected_set)",0
print(query.sql()),0
remember best acc@1 and save checkpoint,0
"completeClassificationLoop(run_dataset, model,num_classes)",0
"embd, label, paths = extract_embeddings(run_loader, model)",0
"db = DBSCAN(eps=0.1, min_samples=5).fit(embd)",0
db = MiniBatchKMeans(n_clusters=args.num_clusters).fit(embd),0
labels = db.labels_,0
"mapp=(find_probablemap(label,labels, K=args.K))",0
"print(""Clusters"")",0
"for i,x in enumerate(labels):",0
labels[i]= mapp[x],0
print(np.sum(labels == label)/labels.size),0
"print(""Confidence Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning(embd, label, idx)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(""Entropy Active Learning"")",0
"idx = np.random.choice(np.arange(len(paths)), 100, replace=False)",0
for i in range(9):,0
"idx= active_learning_entropy(embd, label, idx)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"new_selected= selectSamples(embd,paths,3000)",0
"print(idx,idx.shape)",0
for i in idx:,0
print(paths[i]),0
"print(""Silohette active learning"")",0
"idx= active_learning2(embd, 1000, args.num_clusters)",0
print(idx.shape),0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"print(""Random"")",0
"idx = np.random.choice(np.arange(len(paths)), 1000, replace=False)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
"apply_different_methods(embd[idx], label[idx], embd, label)",0
embd= reduce_dimensionality(embd)#[0:10000]),0
labels= labels[0:10000],0
label= label[0:10000],0
paths= paths[0:10000],0
"plot_embedding(embd, label, paths, run_dataset.getClassesInfo()[1])",0
"plot_embedding(embd, labels, paths, run_dataset.getClassesInfo()[1])",0
plt.show(),0
"np.save(args.name_prefix+""_embeddings.npy"",embd)",0
"np.save(args.name_prefix+""_labels.npy"",label)",0
"np.savetxt(args.name_prefix+""_paths.txt"",paths, fmt=""%s"")",0
combo.setEnabled(not finalized),0
print(self.bbox),0
self.addWidget(self.child),0
Moving container with arrows,0
Left - Bottom,0
Right-Bottom,0
Left-Top,0
Right-Top,0
Left - Bottom,0
Right - Bottom,0
Left - Top,0
Right - Top,0
check cursor horizontal position,0
check cursor vertical position,0
self.resizeEvent=self.onResize,0
"print(""Parent"", parent, parent.width(), parent.height())",0
"self.setGeometry(0,0,410,307)",0
"print(w,h,""w,h"")",0
"print(""final"",tag.getFinal())",0
pass,0
"self.tags.append(TContainer(self,Category.get(-1),[0,0,0.1,0.1],True, Qt.red))",0
label= CharField(),0
fullname=str(self.model),0
"self.name= (fullname[fullname.find("":"")+2:fullname.find("">"")].strip()+'_set').lower()",0
"print(""Parent"", self.parentWidget().width(), self.parentWidget().height())",0
"print(self.model,self.name,query.sql())",0
self.tab4.speciesList.setModel(species),0
"self.tab4.speciesList.setRowHidden(len(species.stringList())-1, True)",0
self.speciesList.itemChanged.connect(self.itemChanged),0
###############,0
,0
iwildcam_dataset.py,0
,0
Loader for the iWildCam detection data set.,0
,0
###############,0
loads the taxonomy data and converts to ints,0
set up dummy data,0
create a dictionary of lists containing taxonomic labels,0
set up the filenames and annotations,0
This loop reads the bboxes and corresponding labels and assigns them,0
the correct image. Kind of slow at the moment...,0
"To speed up the loop, creating mapping for image_id to list index",0
"check that the image contains an animal, if not, don't append a box or label to the",0
image list,0
"Bboxes should have ('ymin', 'xmin', 'ymax', 'xmax') format",0
"Currently we take the label from the annotation file, non-consecutive-",0
label-support would be great,0
"self.bboxes[idx].append([-1.,-1.,0.,0.])",0
self.labels[idx].append(30),0
load classes,0
print out some stats,0
To make sure we loaded the bboxes correctly:,0
self.validate_bboxes(),0
Make sure all are greater equal 0,0
We have to add 1 as the framework assumes that labels start from 0,0
print(bboxes),0
###############,0
,0
util.py,0
,0
Image utilities used in the FasterRCNN framework.,0
,0
###############,0
"reshape (H, W) -> (1, H, W)",0
"transpose (H, W, C) -> (C, H, W)",0
pass in list of files,0
print(im_id),0
print(images[im_id]['seq_id']),0
get unique colors in segmentation,0
get box for each color,0
this is the background class,0
get a box around this color,0
"x1,y1 is top left corner, x2,y2 is bottom right corner",0
create coco-style json,0
In settings.json first activate computer vision mode:,0
https://github.com/Microsoft/AirSim/blob/master/docs/image_apis.md#computer-vision-mode,0
import setup_path,0
load animal class name lookup,0
set segmentation values for everything to 0,0
set segmentation for each animal to a different value,0
"client.simSetCameraOrientation(""0"", airsim.to_quaternion(-0.161799, 0, 0)); #radians",0
pose = client.simGetVehiclePose(),0
pp.pprint(pose),0
print('Pose ' + str(cam_num)),0
print(pose),0
"print(""Type %d, size %d, pos %s"" % (response.image_type, len(response.image_data_float), pprint.pformat(response.camera_position)))",0
"print(""Type %d, size %d, pos %s"" % (response.image_type, len(response.image_data_uint8), pprint.pformat(response.camera_position)))",0
pose = client.simGetVehiclePose(),0
pp.pprint(pose),0
currently reset() doesn't work in CV mode. Below is the workaround,1
"client.simSetPose(airsim.Pose(airsim.Vector3r(0, 0, 0), airsim.to_quaternion(0, 0, 0)), True)",0
environment_lookup = {},0
print(len(list(env_list))),0
save environment dict every time so you don't lose the info if airsim crashes,0
"%% Constants, imports, environment",0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
Numpy FutureWarnings from tensorflow import,0
%% Classes,0
Stick this into filenames before the extension for the rendered result,0
Number of decimal places to round to for confidence and bbox coordinates,0
"MegaDetector was trained with batch size of 1, and the resizing function is a part",0
of the inference graph,0
An enumeration of failure reasons,0
"change from [y1, x1, y2, x2] to [x1, y1, width_box, height_box]",0
convert numpy floats to Python floats,0
need to change the above line to the following if supporting a batch size > 1 and resizing to the same size,0
"np_images = [np.asarray(image, np.uint8) for image in images]",0
"images_stacked = np.stack(np_images, axis=0) if len(images) > 1 else np.expand_dims(np_images[0], axis=0)",0
performs inference,0
our batch size is 1; need to loop the batch dim if supporting batch size > 1,0
%% Main function,0
"load and run detector on target images, and visualize the results",0
"since we'll be writing a bunch of files to the same folder, rename",0
as necessary to avoid collisions,0
"the error code and message is written by generate_detections_one_image,",0
which is wrapped in a big try catch,0
image is modified in place,0
%% Command-line driver,0
"but for a single image, args.image_dir is also None",0
#####,0
,0
process_video.py,0
,0
"Split a video into frames, run the frames through run_tf_detector_batch.py, and",0
optionally stitch together results into a new video with detection boxes.,0
,0
#####,0
"%% Constants, imports, environment",0
%% Function for rendering frames to video and vice-versa,0
http://tsaith.github.io/combine-images-into-a-video-with-python-3-and-opencv-3.html,0
Determine the width and height from the first image,0
Define the codec and create VideoWriter object,0
https://stackoverflow.com/questions/33311153/python-extracting-and-saving-video-frames,0
%% Main function,0
Render detections to images,0
Combine into a video,0
%% Interactive driver,0
%% Load video and split into frames,0
"python process_video.py ""c:\temp\models\md_v4.0.0.pb"" ""c:\temp\LIFT0003.MP4"" --debug_max_frames=10 --render_output_video=True",0
%% Command-line driver,0
"%% Constants, imports, environment",0
from multiprocessing.pool import ThreadPool as workerpool,0
Numpy FutureWarnings from tensorflow import,0
%% Support functions for multiprocessing,0
Split a list into chunks of size n,0
Split a list into n even chunks,0
%% Main function,0
If we're not using multiprocessing...,0
Load the detector,0
"If we're using multiprocessing, let the workers load the model, just store",0
the model filename.,0
Does not count those already processed,0
Will not add additional entries not in the starter checkpoint,0
checkpoint,0
"This was modified in place, but we also return it for backwards-compatibility.",0
%% Command-line driver,0
Load the checkpoint if available,0
,0
Relative file names are only output at the end; all file paths in the checkpoint are,0
still full paths.,0
"Find the images to score; images can be a directory, may need to recurse",0
A json list of image paths,0
A single image file,0
Test that we can write to the output_file's dir if checkpointing requested,0
This script is taken from https://github.com/tensorflow/models/blob/master/research/object_detection/model_main.py,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
substitute the AML data reference mount points for relevant parts in the pipeline.config,0
substitute the AML data reference mount points for relevant parts in the pipeline.config and overwrite,0
The first eval input will be evaluated.,0
Currently only a single Eval Spec is allowed.,0
throttle_secs is not documented in eval.proto. This replaces eval_interval_secs somewhat,0
,0
copy_checkpoints.py,0
,0
Run this script with specified source_dir and target_dir while the model is training to make a copy,0
of every checkpoint (checkpoints are kept once an hour by default and is difficult to adjust),0
,0
do not copy event or evaluation results,0
"labels input to compute_object_detection_metrics() needs to start at 0, not 1",0
num_detections = len(dets['boxes']),0
to prevent 'Invalid dimensions for box data.' error,0
this box will not match any detections,0
compute one-class precision/recall/average precision (if every box is just of an object class),0
"iterate through each image in the gt file, not the detection file",0
ground truth,0
convert gt box coordinates to TFODAPI format,0
detections,0
only include a detection entry if that image had detections,0
%% Empty and non-empty classification at image level,0
%% Empty and non-empty classification at sequence level,0
TODO move detector_output_path specific code out so that this function evaluates only on classification results (confidences),1
evaluate on sequences that are present in both gt and the detector output file,0
%% Utilities,0
#####,0
,0
detect_and_predict_image.py,0
,0
"Functions to load a TensorFlow detection and a classification model, run inference,",0
"render bounding boxes on images, and write out the resulting",0
images (with bounding boxes and classes).,0
,0
"See the ""test driver"" cell for example invocation.",0
,0
,0
#####,0
"%% Constants, imports, environment",0
Minimum detection confidence for showing a bounding box on the output image,0
Stick this into filenames before the extension for the rendered result,0
Number of top-scoring classes to show at each bounding box,0
%% Core detection functions,0
Load images if they're not already numpy arrays,0
iImage = 0; image = images[iImage],0
"Load the image as an nparray of size h,w,nChannels",0
"There was a time when I was loading with PIL and switched to mpimg,",0
"but I can't remember why, and converting to RGB is a very good reason",0
"to load with PIL, since mpimg doesn't give any indication of color",0
"order, which basically breaks all .png files.",0
,0
"So if you find a bug related to using PIL, update this comment",0
"to indicate what it was, but also disable .png support.",0
image = mpimg.imread(image),0
This shouldn't be necessarily when loading with PIL and converting to RGB,0
Actual detection,0
...for each image,0
"Currently ""boxes"" is a list of length nImages, where each element is shaped as",0
,0
"1,nDetections,4",0
,0
"This implicitly banks on TF giving us back a fixed number of boxes, let's assert on this",0
to make sure this doesn't silently break in the future.,0
iBox = 0; box = boxes[iBox],0
"""scores"" is a length-nImages list of elements with size 1,nDetections",0
"""classes"" is a length-nImages list of elements with size 1,nDetections",0
,0
"Still as floats, but really representing ints",0
Squeeze out the empty axis,0
boxes is nImages x nDetections x 4,0
scores and classes are both nImages x nDetections,0
Get input and output tensors of classification model,0
"imsize = cur_image['width'], cur_image['height']",0
Select detections with a confidence larger 0.5,0
Get these boxes and convert normalized coordinates to pixel coordinates,0
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes",0
"However, we need to make sure that it box coordinates are still within the image",0
For convenience:,0
Create an array with contains the index of the corresponding crop_box for each selected box,0
i.e. [False False 0 False 1 2 3 False False],0
For each box,0
If this box should be classified,0
Run inference,0
if box should not be classified,0
...for each box,0
species_scores should have shape len(images) x len(boxes) x num_species,0
...for each image,0
...with tf.Session,0
with classification_graph,0
species_scores should have shape len(images) x len(boxes) x num_species,0
%% Rendering functions,0
Display the image,0
plt.show(),0
"top, left, bottom, right",0
,0
"x,y origin is the upper-left",0
Generate bounding box text,0
Choose color based on class,0
Location is the bottom-left of the rect,0
,0
Origin is the upper-left,0
iRight = x + w,0
iTop = y + h,0
Add the patch to the Axes,0
Add class description,0
First determine best location by finding the corner that is closest to the image center,0
relative corner coordinates,0
relative coordinates of image center,0
Compute pair-wise squared distance and get the index of the one with minimal distance,0
Get the corresponding coordinates ...,0
... and alignment for the text box,0
Plot the text box with background,0
...for each box,0
This is magic goop that removes whitespace around image plots (sort of),0
"plt.savefig(outputFileName, bbox_inches='tight', pad_inches=0.0, dpi=dpi, transparent=True)",0
os.startfile(outputFileName),0
...for each image,0
...def render_bounding_boxes,0
Load and run detector on target images,0
Read the name of all classes,0
remove empty lines,0
%% Interactive driver,0
%%,0
%%,0
%% File helper functions,0
%% Command-line driver,0
Hack to avoid running on already-detected images,1
,0
"Script for evaluating a frozen graph given a COCO-style dataset, which was generated by",0
the ../data_management/databases/classification/make_classification_dataset.py script.,0
This is useful for debugging as the accuracy reported by this script should match the,0
accuracy reported by the Tensorflow training.,0
,0
Check that all files exists for easier debugging,0
Load frozen graph,0
Collect tensors for input and output,0
Read image,0
Run inference,0
Check if correct,0
,0
Script for evaluating precision/recall on a two-class problem given,0
"a frozen graph and a COCO-style dataset, which was generated by",0
the ../data_management/databases/classification/make_classification_dataset.py script.,0
We assume the positive class is at index 0 (with a zero-based indexing),0
,0
Check that all files exists for easier debugging,0
Load frozen graph,0
Collect tensors for input and output,0
Read image,0
"with open(image_path, 'rb') as fi:",0
"image =  sess.run(tf.image.decode_jpeg(fi.read(), channels=3))",0
image = image / 255.,0
Run inference,0
predicted_class = np.argmax(predictions),0
Check if correct,0
if coco.imgToAnns[image_id][0]['category_id'] == predicted_class:,0
correct = correct + 1,0
"In matplotlib < 1.5, plt.fill_between does not have a 'step' argument",0
,0
Script for selecting testing images of a COCO-style dataset generated by the script,0
../data_management/databases/classification/make_classification_dataset.py in a consistent,0
manner and predicting the class for it.,0
,0
Mandatory parameters,0
Optional parameters,0
Validate parameters,0
Derived parameters,0
"We assume that the dataset was generated with the make_classification_dataset.py script,",0
hence the images should be located in the same folder as the json,0
Make seletion deterministic,0
Load frozen graph,0
Get dataset information,0
Get classes,0
...and the class list corresponding to the model outputs by assuming,0
that they are in order of their ids,0
Get images of each class,0
Shuffle the image list,0
Start the image sampling,0
"Set of avaiable class IDs, will be filled below",0
If there are still images left for that class,0
"Get image for the sampled class, we already shuffled the class images before so",0
we can simply pop(),0
Start prediction,0
Collect tensors for input and output,0
Read image,0
Run inference,0
Print output to log file,0
#####,0
,0
api_apply_classifier_single_node.py,0
,0
Takes the JSON file produced by the detection API and,0
classifies all boxes above a confidence threshold.,0
,0
#####,0
"%% Constants, imports, environment",0
Assumes that the root of the CameraTraps repo is on the PYTHONPATH,0
Minimum detection confidence for classifying an object,0
Number of top-scoring classes to show at each bounding box,0
Enlargment factor applied to boxes before passing them to the classifier,0
,0
Provides more context and can lead to better results,0
List of detection categories for which we will run the classification,0
,0
"Currently there are {""1"": ""animal"", ""2"": ""person"", ""4"": ""vehicle""}",0
,0
Should be a list of string-formatted ints.,0
Number of significant float digits in JSON output,0
%% Core detection functions,0
Read the name of all classes,0
remove empty lines,0
Create field with name *classification_categories*,0
Add classes using 0-based indexing,0
def add_classification_categories,0
Make sure we have the right json object,0
Get input and output tensors of classification model,0
For each image,0
Read image,0
"Scale pixel values to [0,1]",0
For each box,0
Skip detections with low confidence,0
Skip if detection category is not in whitelist,0
Skip if already classified,0
"Get current box in relative coordinates and format [x_min, y_min, width_of_box, height_of_box]",0
"Convert to [ymin, xmin, ymax, xmax] and store it as 1x4 numpy array so we can",0
re-use the generic multi-box padding code,0
Convert normalized coordinates to pixel coordinates,0
"Pad the detected animal to a square box and additionally by PADDING_FACTOR, the result will be in crop_boxes.",0
,0
"However, we need to make sure that it box coordinates are still within the image.",0
Get the first (and only) row as our bbox to classify,0
Get the image data for that box,0
Run inference,0
Add an empty list to the json for our predictions,0
Add the *num_annotated_classes* top scoring classes,0
...for each box,0
...for each image,0
...with tf.Session,0
with classification_graph,0
def classify_boxes,0
Load classification model,0
Load detector json,0
Add classes to detector_json,0
"Run classifier on all images, changes will be writting directly to the json",0
Write output json,0
def load_and_run_classifier,0
%% Command-line driver,0
,0
Script for selecting testing images of a COCO-style dataset generated by the script,0
../data_management/databases/classification/make_classification_dataset.py in a consistent,0
manner and predicting the class for it.,0
,0
Assumes the cameratraps repo root is on the path,0
Make seletion deterministic,0
Mandatory parameters,0
Optional parameters,0
Validate parameters,0
Tranfer parameters to post-processing format,0
Load frozen graph,0
Reading image list,0
Reading class list,0
Image sampling,0
Start prediction,0
Collect tensors for input and output,0
Array for collecting infos for rendering the html,0
Read image,0
"Scale pixel values to [0,1]",0
Run inference,0
Add links to all available classes,0
,0
predict_image.py,0
,0
"Given a pointer to a frozen detection graph, runs inference on a single image,",0
printing the top classes to the console,0
,0
%% Imports,0
%% Command-line processing,0
Check that all files exist for easier debugging,0
%% Inference,0
Load frozen graph,0
Load class list,0
Remove empty lines,0
Collect tensors for input and output,0
Read image,0
Run inference,0
Print output,0
,0
Mostly unmodified script for freezing a model,0
Added for convenience and for possible future optimizations,0
,0
Copyright 2015 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
'input_checkpoint' may be a prefix if we're using Saver V2 format,0
Remove all the explicit device specifications for this node. This helps to,0
make the graph more portable.,0
List of all partition variables. Because the condition is heuristic,0
"based, the list could include false positives.",0
This tensor doesn't exist in the graph (for example it's,0
'global_step' or a similar housekeeping element) so skip it.,0
`var_list` is required to be a map of variable names to Variable,0
tensors. Partition variables are Identity tensors that cannot be,0
handled by Saver.,0
Models that have been frozen previously do not contain Variables.,0
optimized_output_graph_def = optimize_for_inference_lib.optimize_for_inference(,0
"output_graph_def,",0
"input_node_names.replace("" "", """").split("",""),",0
"output_node_names.replace("" "", """").split("",""),",0
tf.float32.as_datatype_enum),0
Write GraphDef to file if output path has been given.,0
,0
"Creates a graph description, which is required to create a frozen graph.",0
Adapted from from ./tf-slim/export_inference_graph.py,0
Added preprocessing to the definition for easier handling,0
,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
tf.app.flags.DEFINE_integer(,0
"'batch_size', None,",0
"'Batch size for the exported model. Defaulted to ""None"" so batch size can '",0
'be specified at model runtime.'),0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
#####################,0
Optimization Flags #,0
#####################,0
######################,0
Learning Rate Flags #,0
######################,0
######################,0
Dataset Flags #,0
######################,0
####################,0
Fine-Tuning Flags #,0
####################,0
"Note: when num_clones is > 1, this will actually have each clone to go",0
over each epoch FLAGS.num_epochs_per_decay times. This is different,0
behavior from sync replicas and is expected to produce different results.,0
Warn the user if a checkpoint exists in the train_dir. Then we'll be,0
ignoring the checkpoint anyway.,0
TODO(sguada) variables.filter_variables(),1
######################,0
Config model_deploy #,0
######################,0
Create global_step,0
#####################,0
Select the dataset #,0
#####################,0
#####################,0
Select the network #,0
#####################,0
####################################,0
Select the preprocessing function #,0
####################################,0
#############################################################,0
Create a dataset provider that loads data from the dataset #,0
#############################################################,0
###################,0
Define the model #,0
###################,0
############################,0
Specify the loss function #,0
############################,0
Gather initial summaries.,0
"Gather update_ops from the first clone. These contain, for example,",0
the updates for the batch_norm variables created by network_fn.,0
Add summaries for end_points.,0
Add summaries for losses.,0
Add summaries for variables.,0
################################,0
Configure the moving averages #,0
################################,0
########################################,0
Configure the optimization procedure. #,0
########################################,0
"If sync_replicas is enabled, the averaging will be done in the chief",0
queue runner.,0
Update ops executed locally by trainer.,0
Variables to train.,0
and returns a train_tensor and summary_op,0
Add total_loss to summary.,0
Create gradient updates.,0
Add the summaries from the first clone. These contain the summaries,0
created by model_fn and either optimize_clones() or _gather_clone_loss().,0
Merge all summaries together.,0
##########################,0
Kicks off the training. #,0
##########################,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
#####################,0
Select the dataset #,0
#####################,0
###################,0
Select the model #,0
###################,0
#############################################################,0
Create a dataset provider that loads data from the dataset #,0
#############################################################,0
####################################,0
Select the preprocessing function #,0
####################################,0
###################,0
Define the model #,0
###################,0
Define the metrics:,0
Print the summaries to screen.,0
TODO(sguada) use num_epochs=1,1
This ensures that we make a single pass over all of the data.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Transform the image to floats.,0
"Randomly crop a [height, width] section of the image.",0
Randomly flip the image horizontally.,0
"Because these operations are not commutative, consider randomizing",0
the order their operation.,0
Subtract off the mean and divide by the variance of the pixels.,0
Transform the image to floats.,0
Resize and crop if needed.,0
Subtract off the mean and divide by the variance of the pixels.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Use tf.slice instead of crop_to_bounding box as it accepts tensors to,0
define the crop size.,0
Compute the rank assertions.,0
Create a random bounding box.,0
,0
Use tf.random_uniform and not numpy.random.rand as doing the former would,0
"generate random numbers at graph eval time, unlike the latter which",0
generates random numbers at graph definition time.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Pass the real x only to one of the func calls.,0
The random_* ops do not necessarily clamp.,0
"Each bounding box has shape [1, num_boxes, box coords] and",0
"the coordinates are ordered [ymin, xmin, ymax, xmax].",0
A large fraction of image datasets contain a human-annotated bounding,0
box delineating the region of the image containing the object of interest.,0
We choose to create a new bounding box for the object which is a randomly,0
distorted version of the human-annotated bounding box that obeys an,0
"allowed range of aspect ratios, sizes and overlap with the human-annotated",0
"bounding box. If no box is supplied, then we assume the bounding box is",0
the entire image.,0
Crop the image to the specified bounding box.,0
"Each bounding box has shape [1, num_boxes, box coords] and",0
"the coordinates are ordered [ymin, xmin, ymax, xmax].",0
Restore the shape since the dynamic slice based upon the bbox_size loses,0
the third dimension.,0
This resizing operation may distort the images because the aspect,0
ratio is not respected. We select a resize method in a round robin,0
fashion based on the thread number.,0
Note that ResizeMethod contains 4 enumerated resizing methods.,0
We select only 1 case for fast_mode bilinear.,0
Randomly flip the image horizontally.,0
Randomly distort the colors. There are 1 or 4 ways to do it.,0
Crop the central region of the image with an area containing 87.5% of,0
the original image.,0
Resize the image to the specified height and width.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TODO(sguada) docstring paragraph by (a) motivating the need for the file and,1
(b) defining clones.,0
TODO(sguada) describe the high-level components of model deployment.,1
"E.g. ""each model deployment is composed of several parts: a DeploymentConfig,",0
"which captures A, B and C, an input_fn which loads data.. etc",0
Set up DeploymentConfig,0
Create the global step on the device storing the variables.,0
Define the inputs,0
Define the optimizer.,0
Define the model including the loss.,0
Run training.,0
Namedtuple used to represent a clone during deployment.,0
"Namedtuple used to represent a DeployedModel, returned by deploy().",0
Default parameters for DeploymentConfig,0
Create clones.,0
The return value.,0
Individual components of the loss that will need summaries.,0
Compute and aggregate losses on the clone device.,0
Add the summaries out of the clone device block.,0
Only use regularization_losses for the first clone,0
Compute the total_loss summing all the clones_losses.,0
Sum the gradients across clones.,0
Gather initial summaries.,0
Create Clones.,0
"Gather update_ops from the first clone. These contain, for example,",0
the updates for the batch_norm variables created by model_fn.,0
Place the global step on the device storing the variables.,0
Compute the gradients for the clones.,0
Add summaries to the gradients.,0
Create gradient updates.,0
Only use regularization_losses for the first clone,0
Add the summaries from the first clone. These contain the summaries,0
created by model_fn and either optimize_clones() or _gather_clone_loss().,0
Add total_loss to summary.,0
Merge all summaries together.,0
Note that each grad_and_vars looks like the following:,0
"((grad_var0_clone0, var0), ... (grad_varN_cloneN, varN))",0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Create an easy training set:,0
Create an easy training set:,0
Create an easy training set:,0
clone function creates a fully_connected layer with a regularizer loss.,0
The model summary op should have a few summary inputs and all of them,0
should be on the CPU.,0
clone function creates a fully_connected layer with a regularizer loss.,0
"No optimizer here, it's an eval.",0
The model summary op should have a few summary inputs and all of them,0
should be on the CPU.,0
!/usr/bin/python,0
Copyright 2016 Google Inc. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Failed to find ""index"" occurrence of item.",0
pylint: disable=broad-except,0
pylint: enable=broad-except,0
Grab the 'index' annotation.,0
Some images contain bounding box annotations that,0
"extend outside of the supplied image. See, e.g.",0
n03127925/n03127925_147.xml,0
"Additionally, for some bounding boxes, the min > max",0
or the box is entirely outside of the image.,0
Example: <...>/n06470073/n00141669_6790.xml,0
Determine if the annotation is from an ImageNet Challenge label.,0
Note: There is a slight bug in the bounding box annotation data.,0
Many of the dog labels have the human label 'Scottish_deerhound',0
instead of the synset ID 'n02092002' in the bbox.label field. As a,0
"simple hack to overcome this issue, we only exclude bbox labels",1
*which are synset ID's* that do not match original synset label for,0
the XML file.,0
Guard against improperly specified boxes.,0
Note bbox.filename occasionally contains '%s' in the name. This is,0
data set noise that is fixed by just using the basename of the XML file.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The URLs where the MNIST data can be downloaded.,0
The names of the classes.,0
"First, process the training data:",0
"Next, process the testing data:",0
"Finally, write the labels file:",0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The URL where the Flowers data can be downloaded.,0
The number of images in the validation set.,0
Seed for repeatability.,0
The number of shards per dataset split.,0
Initializes function that decodes RGB JPEG data.,0
Read the filename:,0
Divide into train and test:,0
"First, convert the training and validation sets.",0
"Finally, write the labels file:",0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TODO(nsilberman): Add tfrecord file type once the script is updated.,1
"If set to false, will not try to set label_to_names in dataset",0
by reading them from labels.txt or github.,0
n01440764,0
n01443537,0
n02119247    black fox,0
n02119359    silver fox,0
pylint: disable=g-line-too-long,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The URL where the CIFAR data can be downloaded.,0
The number of training files.,0
The height and width of each image.,0
The names of the classes.,0
"First, process the training data:",0
"Next, process the testing data:",0
"Finally, write the labels file:",0
Copyright 2016 Google Inc. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The labels file contains a list of valid labels are held in this file.,0
Assumes that the file contains entries as such:,0
n01440764,0
n01443537,0
n01484850,0
where each line corresponds to a label expressed as a synset. We map,0
each synset contained in the file to an integer (based on the alphabetical,0
ordering). See below for details.,0
This file containing mapping from synset to human-readable label.,0
Assumes each line of the file looks like:,0
,0
n02119247    black fox,0
n02119359    silver fox,0
"n02119477    red fox, Vulpes fulva",0
,0
where each line corresponds to a unique mapping. Note that each line is,0
formatted as <synset>\t<human readable label>.,0
This file is the output of process_bounding_box.py,0
Assumes each line of the file looks like:,0
,0
"n00007846_64193.JPEG,0.0060,0.2620,0.7545,0.9940",0
,0
where each line corresponds to one bounding box annotation associated,0
with an image. Each line can be parsed as:,0
,0
"<JPEG file name>, <xmin>, <ymin>, <xmax>, <ymax>",0
,0
Note that there might exist mulitple bounding box annotations associated,0
with an image file.,0
pylint: disable=expression-not-assigned,0
pylint: enable=expression-not-assigned,0
Create a single Session to run all image coding calls.,0
Initializes function that converts PNG to JPEG data.,0
Initializes function that converts CMYK JPEG data to RGB JPEG data.,0
Initializes function that decodes RGB JPEG data.,0
File list from:,0
https://groups.google.com/forum/embed/?place=forum/torch7#!topic/torch7/fOSTXHIESSU,0
File list from:,0
https://github.com/cytsai/ilsvrc-cmyk-image-list,0
Read the image file.,0
Clean the dirty data.,0
1 image is a PNG.,0
22 JPEG images are in CMYK colorspace.,0
Decode the RGB JPEG.,0
Check that image converted to RGB,0
Each thread produces N shards where N = int(num_shards / num_threads).,0
"For instance, if num_shards = 128, and the num_threads = 2, then the first",0
"thread would produce shards [0, 64).",0
"Generate a sharded version of the file name, e.g. 'train-00002-of-00010'",0
"Break all images into batches with a [ranges[i][0], ranges[i][1]].",0
Launch a thread for each batch.,0
Create a mechanism for monitoring when all threads are finished.,0
Create a generic TensorFlow-based utility for converting all image codings.,0
Wait for all the threads to terminate.,0
Leave label index 0 empty as a background class.,0
Construct the list of JPEG files and labels.,0
Shuffle the ordering of all image files in order to guarantee,0
random ordering of the images with respect to label in the,0
saved TFRecord files. Make the randomization repeatable.,0
Build a map from synset to human-readable label.,0
Run it!,0
Allowing None in the signature so that dataset_factory can use the default.,0
!/usr/bin/python,0
Copyright 2016 Google Inc. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Read in the 50000 synsets associated with the validation data set.,0
Make all sub-directories in the validation data dir.,0
Move all of the image to the appropriate sub-directory.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
Allowing None in the signature so that dataset_factory can use the default.,0
"The dataset has classes with no images (empty and human), which have ID 0 and 1,",0
so we need to specify 49 here despite having only 47 classes with images,0
Allowing None in the signature so that dataset_factory can use the default.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
4 x Inception-A blocks,0
Reduction-A block,0
7 x Inception-B blocks,0
Reduction-A block,0
3 x Inception-C blocks,0
Logits and predictions,0
Force all Variables to reside on the device.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The endpoint shapes must be equal to the original shape even when the,0
separable convolution is replaced with a normal convolution.,0
"With the 'NCHW' data format, all endpoint activations have a transposed",0
shape from the original shape with the 'NHWC' layout.,0
'NCWH' data format is not supported.,0
'NCHW' data format is not supported for separable convolution.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Final pooling and prediction,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Check graph construction for a number of image size/depths and batch,0
sizes.,0
Check layer depths.,0
Check graph construction for a number of image size/depths and batch,0
sizes.,0
Check layer depths.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
The current_stride variable keeps track of the effective stride of the,0
activations. This allows us to invoke atrous convolution whenever applying,0
the next residual unit would result in the activations having stride larger,0
than the target output_stride.,0
The atrous convolution rate parameter.,0
Move stride from the block's last unit to the end of the block.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Collect activations at the block's end before performing subsampling.,0
Subsampling of the block's output activations.,0
"The following implies padding='SAME' for pool1, which makes feature",0
alignment easier for dense prediction tasks. This is also used in,0
https://github.com/facebook/fb.resnet.torch. However the accompanying,0
code of 'Deep Residual Learning for Image Recognition' uses,0
padding='VALID' for pool1. You can switch to that choice by setting,0
"slim.arg_scope([slim.max_pool2d], padding='VALID').",0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Collect outputs for conv2d, fully_connected and max_pool2d",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Input image.,0
Convolution kernel.,0
Input image.,0
Convolution kernel.,0
Test both odd and even input dimensions.,0
Dense feature extraction followed by subsampling.,0
Make the two networks use the same weights.,0
Feature extraction at the nominal network rate.,0
Test both odd and even input dimensions.,0
Subsampling at the last unit of the block.,0
Make the two networks use the same weights.,0
Subsample activations at the end of the blocks.,0
Make sure that the final output is the same.,0
Make sure that intermediate block activations in,0
output_end_points are subsampled versions of the corresponding,0
ones in expected_end_points.,0
"Like ResnetUtilsTest.testEndPointsV1(), but for the public API.",0
Dense feature extraction followed by subsampling.,0
Make the two networks use the same weights.,0
Feature extraction at the nominal network rate.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"inputs has shape [batch, 224, 224, 3]",0
"inputs has shape [batch, 513, 513, 3]",0
We do not include batch normalization or activation functions in,0
conv1 because the first ResNet unit will perform these. Cf.,0
Appendix of [2].,0
This is needed because the pre-activation variant does not have batch,1
normalization or activation functions in the residual unit output. See,0
Appendix of [2].,0
Convert end_points_collection into a dictionary of end_points.,0
Global average pooling.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
=============================================================================,0
"These parameters come from the online port, which don't necessarily match",0
those in the paper.,0
TODO(nsilberman): confirm these values with Philip.,1
##########,0
Encoder #,0
##########,0
"No normalizer for the first encoder layers as per 'Image-to-Image',",0
Section 5.1.1,0
First layer doesn't use normalizer_fn,0
Last layer doesn't use activation_fn nor normalizer_fn,0
##########,0
Decoder #,0
##########,0
"Dropout is used at both train and test time as per 'Image-to-Image',",0
Section 2.1 (last paragraph).,0
The Relu comes BEFORE the upsample op:,0
Explicitly set the normalizer_fn to None to override any default value,0
"that may come from an arg_scope, such as pix2pix_arg_scope.",0
No normalization on the input layer.,0
Stride 1 on the last layer.,0
"1-dim logits, stride 1, no activation, no normalization.",0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 Google Inc. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Most networks use 224 as their default_image_size,0
Most networks use 224 as their default_image_size,0
Most networks use 224 as their default_image_size,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TODO(joelshor): Add a test to check generator endpoints.,1
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Force all Variables to reside on the device.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
"Collect outputs for conv2d, fully_connected and max_pool2d.",0
Use conv2d instead of fully_connected layers.,0
Convert end_points_collection into a end_point dict.,0
Alias,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pylint: disable=unused-import,0
pylint: enable=unused-import,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Input image.,0
Convolution kernel.,0
Input image.,0
Convolution kernel.,0
Test both odd and even input dimensions.,0
Dense feature extraction followed by subsampling.,0
Make the two networks use the same weights.,0
Feature extraction at the nominal network rate.,0
"Like ResnetUtilsTest.testEndPointsV2(), but for the public API.",0
Dense feature extraction followed by subsampling.,0
Make the two networks use the same weights.,0
Feature extraction at the nominal network rate.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
If we are fine tuning a checkpoint we need to start at a lower learning,0
rate since we are farther along on training.,0
We can start quantizing immediately if we are finetuning.,0
We need to wait for the model to train a bit before we quantize if we are,0
training from scratch.,0
Call rewriter to produce graph with fake quant ops and folded batch norms,0
"quant_delay delays start of quantization till quant_delay steps, allowing",0
for better model accuracy.,0
Configure the learning rate using an exponential decay.,0
"When restoring from a floating point model, the min/max values for",0
quantized weights and activations are not present.,0
We instruct slim to ignore variables that are missing during restoration,0
by setting ignore_missing_vars=True,0
"If we are restoring from a floating point model, we need to initialize",0
the global step to zero for the exponential decay to result in,0
reasonable learning rates.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Orignaly, arg_scope = slim.arg_scope and layers = slim, now switch to more",0
update-to-date tf.contrib.* API.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Turns off fused batch norm.,0
collection containing the moving mean and moving variance.,0
"For inverted pyramid models, we start with gating switched off.",0
batch_size x 32 x 112 x 112 x 64,0
Separable conv is slow when used at first conv layer.,0
batch_size x 32 x 56 x 56 x 64,0
batch_size x 32 x 56 x 56 x 64,0
batch_size x 32 x 56 x 56 x 192,0
batch_size x 32 x 28 x 28 x 192,0
batch_size x 32 x 28 x 28 x 256,0
batch_size x 16 x 14 x 14 x 512,0
batch_size x 16 x 14 x 14 x 512,0
batch_size x 16 x 14 x 14 x 512,0
batch_size x 16 x 14 x 14 x 528,0
batch_size x 16 x 14 x 14 x 832,0
batch_size x 8 x 7 x 7 x 832,0
batch_size x 8 x 7 x 7 x 1024,0
Final pooling and prediction,0
Temporal average pooling.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
By default use stride=1 and SAME padding,0
299 x 299 x 3,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 160,0
71 x 71 x 192,0
35 x 35 x 384,0
4 x Inception-A blocks,0
35 x 35 x 384,0
Reduction-A block,0
17 x 17 x 1024,0
7 x Inception-B blocks,0
17 x 17 x 1024,0
Reduction-B block,0
8 x 8 x 1536,0
3 x Inception-C blocks,0
Auxiliary Head logits,0
17 x 17 x 1024,0
Final pooling and prediction,0
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which",1
can be set to False to disable pooling here (as in resnet_*()).,0
8 x 8 x 1536,0
1 x 1 x 1536,0
1536,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN,1
setups need the gradient of gradient FusedBatchNormGrad.,0
TODO(joelshor): Use fused batch norm by default. Investigate why some GAN,1
setups need the gradient of gradient FusedBatchNormGrad.,0
First upscaling is different because it takes the input vector.,0
Last layer has different normalizer and activation.,0
Convert to proper channels.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
Use clip_by_value to simulate bandpass activation.,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80,0
71 x 71 x 192,0
35 x 35 x 192,0
35 x 35 x 320,0
TODO(alemi): Register intermediate endpoints,1
"17 x 17 x 1088 if output_stride == 8,",0
33 x 33 x 1088 if output_stride == 16,0
TODO(alemi): register intermediate endpoints,1
TODO(gpapan): Properly support output_stride for the rest of the net.,1
8 x 8 x 2080,0
TODO(alemi): register intermediate endpoints,1
8 x 8 x 1536,0
"TODO(sguada,arnoegw): Consider adding a parameter global_pool which",1
can be set to False to disable pooling here (as in resnet_*()).,0
Set weight_decay for weights in conv2d and fully_connected layers.,0
Set activation_fn and parameters for batch_norm.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
collection containing update_ops.,0
use fused batch norm if possible.,0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"inputs has shape [batch, 224, 224, 3]",0
"inputs has shape [batch, 513, 513, 3]",0
Use clip_by_value to simulate bandpass activation.,0
Convert end_points_collection into a dictionary of end_points.,0
Global average pooling.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"end_points will collect relevant activations for external use, for example",0
summaries or losses.,0
299 x 299 x 3,0
149 x 149 x 32,0
147 x 147 x 32,0
147 x 147 x 64,0
73 x 73 x 64,0
73 x 73 x 80.,0
71 x 71 x 192.,0
35 x 35 x 192.,0
Inception blocks,0
mixed: 35 x 35 x 256.,0
mixed_1: 35 x 35 x 288.,0
mixed_2: 35 x 35 x 288.,0
mixed_3: 17 x 17 x 768.,0
mixed4: 17 x 17 x 768.,0
mixed_5: 17 x 17 x 768.,0
mixed_6: 17 x 17 x 768.,0
mixed_7: 17 x 17 x 768.,0
mixed_8: 8 x 8 x 1280.,0
mixed_9: 8 x 8 x 2048.,0
mixed_10: 8 x 8 x 2048.,0
Auxiliary Head logits,0
Shape of feature map before the final layer.,0
Final pooling and prediction,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 2048,0
2048,0
1000,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Reflection pad by 1 in spatial dimensions (axes 1, 2 = h, w) to make a 3x3",0
'valid' convolution produce an output with the same dimension as the,0
input.,0
This corrects 1 pixel offset for images with even width and height.,0
conv2d is left aligned and conv2d_transpose is right aligned for even,0
sized images (while doing 'SAME' padding).,0
Note: This doesn't reflect actual model in paper.,0
Neither dropout nor batch norm -> dont need is_training,0
##########,0
Encoder #,0
##########,0
7x7 input stage,0
##################,0
Residual Blocks #,0
##################,0
##########,0
Decoder #,0
##########,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"end_points will collect relevant activations for external use, for example",0
summaries or losses.,0
Used to find thinned depths for each layer.,0
Note that sizes in the comments below assume an input spatial size of,0
"224x224, however, the inputs can be of any size greater 32x32.",0
224 x 224 x 3,0
depthwise_multiplier here is different from depth_multiplier.,0
depthwise_multiplier determines the output channels of the initial,0
"depthwise conv (see docs for tf.nn.separable_conv2d), while",0
depth_multiplier controls the # channels of the subsequent 1x1,0
convolution. Must have,0
in_channels * depthwise_multipler <= out_channels,0
so that the separable convolution is not overparameterized.,0
Use a normal convolution instead of a separable convolution.,0
112 x 112 x 64,0
56 x 56 x 64,0
56 x 56 x 64,0
56 x 56 x 192,0
28 x 28 x 192,0
Inception module.,0
28 x 28 x 256,0
28 x 28 x 320,0
14 x 14 x 576,0
14 x 14 x 576,0
14 x 14 x 576,0
14 x 14 x 576,0
14 x 14 x 576,0
7 x 7 x 1024,0
7 x 7 x 1024,0
Final pooling and prediction,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 1024,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
=============================================================================,0
For the Conv2d_0 layer FaceNet has depth=16,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
=============================================================================,0
Tensorflow mandates these.,0
Conv and DepthSepConv namedtuple define layers of the MobileNet architecture,0
Conv defines 3x3 convolution layers,0
DepthSepConv defines 3x3 depthwise convolution followed by 1x1 convolution.,0
stride is the stride of the convolution,0
depth is the number of channels or filters in a layer,0
MOBILENETV1_CONV_DEFS specifies the MobileNet body,0
Used to find thinned depths for each layer.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
By passing filters=None,0
separable_conv2d produces only a depthwise convolution layer,0
Global average pooling.,0
Pooling with a fixed kernel size.,0
1 x 1 x 1024,0
Set weight_decay for weights in Conv and DepthSepConv layers.,0
Copyright 2016 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Turns off fused batch norm.,0
collection containing the moving mean and moving variance.,0
Final pooling and prediction,0
Temporal average pooling.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
=============================================================================,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"Orignaly, add_arg_scope = slim.add_arg_scope and layers = slim, now switch to",0
more update-to-date tf.contrib.* API.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
"Note: want to round down, we adjust each split to match the total.",0
"We use scope_depthwise, scope_pointwise for compatibility with V1 ckpts.",0
which provide numbered scopes.,0
b1 -> b2 * r -> b2,0
i -> (o * r) (bottleneck) -> o,0
"Note in contrast with expansion, we always have",0
projection to produce the desired output size.,0
stride check enforces that we don't add residuals when spatial,0
dimensions are None,0
Depth matches,0
Don't do any splitting if we end up with less than 8 filters,0
on either side.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Make sure that round down does not go down by more than 10%.,0
Set conv defs defaults and overrides.,0
a) Set the tensorflow scope,0
b) set padding to default: note we might consider removing this,0
since it is also set by mobilenet_scope,0
c) set all defaults,0
d) set all extra overrides.,0
The current_stride variable keeps track of the output stride of the,0
"activations, i.e., the running product of convolution strides up to the",0
current network layer. This allows us to invoke atrous convolution,0
whenever applying the next convolution would result in the activations,0
having output stride larger than the target output_stride.,0
The atrous convolution rate parameter.,0
Insert default parameters before the base scope which includes,0
any custom overrides set in mobilenet.,0
"If we have reached the target output_stride, then we need to employ",0
atrous convolution with stride=1 and multiply the atrous rate by the,0
current unit's stride for use in subsequent layers.,0
Update params.,0
Only insert rate to params if rate > 1.,0
Set padding,0
Add all tensors that end with 'output' to,0
endpoints,0
1 x 1 x num_classes,0
Note: legacy scope name.,0
"Recover output shape, for unknown shape.",0
the network created will be trainble with dropout/batch norm,0
initialized appropriately.,0
Note: do not introduce parameters that would change the inference,0
"model here (for example whether to use bias), modify conv_def instead.",0
Set weight_decay for weights in Conv and FC layers.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
This is mostly a sanity test. No deep reason for these particular,0
constants.,0
,0
"All but first 2 and last one have  two convolutions, and there is one",0
extra conv that is not in the spec. (logits),0
Check that depthwise are exposed.,0
"All but 3 op has 3 conv operatore, the remainign 3 have one",0
and there is one unaccounted.,0
Verifies that depth_multiplier arg scope actually works,0
if no default min_depth is provided.,0
Verifies that depth_multiplier arg scope actually works,0
if no default min_depth is provided.,0
"All convolutions will be 8->48, except for the last one.",0
Verifies that mobilenet_base returns pre-pooling layer.,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
pyformat: disable,0
Architecture: https://arxiv.org/abs/1801.04381,0
Note: these parameters of batch norm affect the architecture,0
that's why they are here and not in training_scope.,0
pyformat: enable,0
NB: do not set depth_args unless they are provided to avoid overriding,0
whatever default depth_multiplier might have thanks to arg_scope.,0
Wrappers for mobilenet v2 with depth-multipliers. Be noticed that,0
"'finegrain_classification_mode' is set to True, which means the embedding",0
layer will not be shrinked when given a depth-multiplier < 1.0.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Notes for training NASNet Cifar Model,0
-------------------------------------,0
batch_size: 32,0
learning rate: 0.025,0
cosine (single period) learning rate decay,0
auxiliary head loss weighting: 0.4,0
clip global norm of all gradients by 5,0
600 epochs with a batch size of 32,0
This is used for the drop path probabilities since it needs to increase,0
the drop out probability over the course of training.,0
Notes for training large NASNet model on ImageNet,0
-------------------------------------,0
batch size (per replica): 16,0
learning rate: 0.015 * 100,0
learning rate decay factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 100 replicas,0
auxiliary head loss weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Notes for training the mobile NASNet ImageNet model,0
-------------------------------------,0
batch size (per replica): 32,0
learning rate: 0.04 * 50,0
learning rate scaling factor: 0.97,0
num epochs per decay: 2.4,0
sync sgd with 50 replicas,0
auxiliary head weighting: 0.4,0
label smoothing: 0.1,0
clip global norm of all gradients by 10,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Decay for the moving averages.,0
epsilon to prevent 0s in variance.,0
Shape of feature map before the final layer.,0
149 x 149 x 32,0
Run the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Calculate the total number of cells in the network,0
Add 2 for the reduction cells,0
"If ImageNet, then add an additional two for the stem cells",0
Find where to place the reduction cells or stride normal cells,0
Setup for building in the auxiliary head.,0
Run the cells,0
true_cell_num accounts for the stem cells,0
Final softmax layer,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Find where to place the reduction cells or stride normal cells,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Setup for building in the auxiliary head.,0
Run the cells,0
true_cell_num accounts for the stem cells,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Final softmax layer,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Calculate the total number of cells in the network.,0
There is no distinction between reduction and normal cells in PNAS so the,0
total number of cells is equal to the number normal cells plus the number,0
of stem cells (two by default).,0
pylint: disable=protected-access,0
pylint: enable=protected-access,0
Calculate the total number of cells in the network.,0
There is no distinction between reduction and normal cells in PNAS so the,0
total number of cells is equal to the number normal cells plus the number,0
of stem cells (two by default).,0
Configuration for the PNASNet-5 model.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Copyright 2018 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Logits and predictions,0
Logits and predictions,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
Logits and predictions,0
Logits and predictions,0
Logits and predictions,0
Force all Variables to reside on the device.,0
Copyright 2017 The TensorFlow Authors. All Rights Reserved.,0
,0
"Licensed under the Apache License, Version 2.0 (the ""License"");",0
you may not use this file except in compliance with the License.,0
You may obtain a copy of the License at,0
,0
http://www.apache.org/licenses/LICENSE-2.0,0
,0
"Unless required by applicable law or agreed to in writing, software",0
"distributed under the License is distributed on an ""AS IS"" BASIS,",0
"WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.",0
See the License for the specific language governing permissions and,0
limitations under the License.,0
==============================================================================,0
"The cap for tf.clip_by_value, it's hinted from the activation distribution",0
"that the majority of activation values are in the range [-6, 6].",0
Skip path 1,0
Skip path 2,0
"First pad with 0's on the right and bottom, then shift the filter to",0
include those 0's that were added.,0
"If odd number of filters, add an additional one to the second path.",0
Concat and apply BN,0
Set the prev layer to the current layer if it is none,0
Check to be sure prev layer stuff is setup correctly,0
num_or_size_splits=1,0
Apply conv operations,0
Combine hidden states using 'add'.,0
Add hiddenstate to the list of hiddenstates we can choose from,0
Dont stride if this is not one of the original hiddenstates,0
"Check if a stride is needed, then use a strided 1x1 here",0
Determine if a reduction should be applied to make the number of,0
filters match.,0
Return the concat of all the states,0
Scale keep prob by layer number,0
The added 2 is for the reduction cells,0
Decrease the keep probability over time,0
,0
Script for generating a two-class dataset in COCO format for training an obscured image classifier,0
,0
Requires Python >= 3.6 because of the glob ** expression,0
,0
Collect images and labels,0
"Labels: clean = 0, obscured = 1",0
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils),0
from data_management.megadb.schema import sequences_schema_check,0
resize is for displaying them more quickly,0
dataset and seq_id are required fields,0
sort the images in the sequence,0
pool = ThreadPool(),0
"print('len of rendering_info', len(rendering_info))",0
"tqdm(pool.imap_unordered(render_image_info_partial, rendering_info), total=len(rendering_info))",0
options = write_html_image_list(),0
options['headerHtml'],0
print('Checking that the MegaDB entries conform to the schema...'),0
sequences_schema_check.sequences_schema_check(sequences),0
#######,0
,0
visualize_db.py,0
,0
Outputs an HTML page visualizing annotations (class labels and/or bounding boxes),0
on a sample of images in a database in the COCO Camera Traps format,0
,0
#######,0
%% Imports,0
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils),0
Assumes the cameratraps repo root is on the path,0
%% Settings,0
Set to None to visualize all images,0
Target size for rendering; set either dimension to -1 to preserve aspect ratio,0
These are mutually exclusive,0
We sometimes flatten image directories by replacing a path separator with,0
another character.  Leave blank for the typical case where this isn't necessary.,0
Control rendering parallelization,0
%% Helper functions,0
"Translate the file name in an image entry in the json database to a path, possibly doing",0
some manipulation of path separators,0
%% Core functions,0
"Optionally remove all images without bounding boxes, *before* sampling",0
"Optionally include/remove images with specific labels, *before* sampling",0
Put the annotations in a dataframe so we can select all annotations for a given image,0
Construct label map,0
Take a sample of images,0
Set of dicts representing inputs to render_db_bounding_boxes:,0
,0
"bboxes, boxClasses, image_path",0
iImage = 0,0
All the class labels we've seen for this image (with out without bboxes),0
Iterate over annotations for this image,0
iAnn = 0; anno = annos_i.iloc[iAnn],0
"We're adding html for an image before we render it, so it's possible this image will",0
fail to render.  For applications where this script is being used to debua a database,0
"(the common case?), this is useful behavior, for other applications, this is annoying.",0
,0
TODO: optionally write html only for images where rendering succeeded,1
...for each image,0
...def render_image_info,0
def process_images(...),0
%% Command-line driver,0
"Copy all fields from a Namespace (i.e., the output from parse_args) to an object.",0
,0
Skips fields starting with _.  Does not check existence in the target object.,0
Convert to an options object,0
%% Interactive driver(s),0
%%,0
os.startfile(htmlOutputFile),0
%% Constants and imports,0
%% Functions,0
PIL.Image.convert() returns a converted copy of this image,0
Null operation,0
Aspect ratio as width over height,0
ar = w / h,0
h = w / ar,0
ar = w / h,0
w = ar * h,0
The following three functions are modified versions of those at:,0
https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py,0
"To avoid duplicate colors with detection-only visualization, offset",0
the classification class index by the number of detection classes,0
...if we have detection results,0
...if the confidence of this detection is above threshold,0
...for each detection,0
"print('Input must be of size [N, 4], but is ' + str(boxes_shape))",0
If the total height of the display strings added to the top of the bounding,0
"box exceeds the top of the image, stack the strings below the bounding box",0
instead of above.,0
Each display_str has a top and bottom margin of 0.05x.,0
Reverse list and print from bottom to top.,0
plt.grid(False),0
"To fit the legend in, shrink current axis by 20%",0
Put a legend to the right of the current axis,0
####,0
,0
visualize_detector_output.py,0
,0
"Render images with bounding boxes annotated on them to a folder, based on a detector output result",0
file (json). The original images can be local or in Azure Blob Storage.,0
,0
####,0
%% Imports,0
%% Constants,0
convert category ID from int to str,0
%% Options class,0
## Required inputs,0
# Options,0
%% Helper functions and constants,0
%% Main function,0
%% Load detector output,0
"%% Load images, annotate them and save",0
max_conf = entry['max_detection_conf'],0
resize is for displaying them more quickly,0
%% Command-line driver,0
####,0
,0
visualize_incoming_annotations.py,0
,0
Spot-check the annotations received from iMerit by visualizing annotated bounding,0
boxes on a sample of images and display them in HTML.,0
,0
####,0
%% Imports,0
Assumes ai4eutils is on the path (github.com/Microsoft/ai4eutils),0
%% Settings - change everything in this section to match your task,0
'/datadrive/SS_annotated/imerit_batch7_snapshotserengeti_2018_10_26/images',0
'/home/yasiyu/mnt/wildlifeblobssc/rspb/gola/gola_camtrapr_data',0
'/datadrive/IDFG/IDFG_20190104_images_to_annotate',0
'/datadrive/emammal',0
functions for translating from image_id in the annotation files to path to images in images_dir,0
the dash between seq and frame is different among the batches,0
specify which of the above functions to use for your dataset,0
%% Read in the annotations,0
put the annotations in a dataframes so we can select all annotations for a given image,0
%% Get a numerical to English label map; note that both the numerical key and the name are str,0
%% Visualize the bboxes on a sample of images,0
if len(annos_i) < 20:,0
continue,0
if len(images_html) > 400:  # cap on maximum to save,0
break,0
only save images with a particular class,0
"classes = list(annos_i.loc[:, 'category_id'])",0
classes = [str(i) for i in classes],0
if '3' not in classes:  # only save images with the 'group' class,0
continue,0
%% Write to HTML,0
,0
prepare_api_output_for_timelapse.py,0
,0
Takes output from the batch API and does some conversions to prepare,0
it for use in Timelapse.,0
,0
Specifically:,0
,0
* Removes the class field from each bounding box,0
* Optionally does query-based subsetting of rows,0
* Optionally does a search and replace on filenames,0
* Replaces backslashes with forward slashes,0
"* Renames ""detections"" to ""predicted_boxes""",0
,0
"Note that ""relative"" paths as interpreted by Timelapse aren't strictly relative as",0
of 6/5/2019.  If your project is in:,0
,0
c:\myproject,0
,0
...and your .tdb file is:,0
,0
c:\myproject\blah.tdb,0
,0
...and you have an image at:,0
,0
c:\myproject\imagefolder1\img.jpg,0
,0
The .csv that Timelapse sees should refer to this as:,0
,0
myproject/imagefolder1/img.jpg,0
,0
...*not* as:,0
,0
imagefolder1/img.jpg,0
,0
Hence all the search/replace functionality in this script.  It's very straightforward,0
"once you get this and doesn't take time, but it's easy to forget to do this.  This will",0
be fixed in an upcoming release.,0
,0
%% Constants and imports,0
Python standard,0
pip-installable,0
"AI4E repos, expected to be available on the path",0
%% Helper classes,0
Only process rows matching this query (if not None); this is processed,0
after applying os.normpath to filenames.,0
"If not none, replace the query token with this",0
"If not none, prepend matching filenames with this",0
%% Helper functions,0
"If there's no query, we're just pre-pending",0
%% Main function,0
Create a temporary column we'll use to mark the rows we want to keep,0
This is the main loop over rows,0
Trim to matching rows,0
Timelapse legacy issue; we used to call this column 'predicted_boxes',0
Write output,0
"write_api_results(detectionResults,outputFilename)",0
%% Interactive driver,0
%%,0
%% Command-line driver (** outdated **),0
"Copy all fields from a Namespace (i.e., the output from parse_args) to an object.",0
,0
Skips fields starting with _.  Does not check existence in the target object.,0
"print('Setting {} to {}'.format(n,v))",0
Convert to an options object,0
--------some stuff needed to get AJAX to work with bottle?--------#,0
-------------------------------------------------------------------------------- #,0
PREPARE TO QUEUE IMAGES FOR LABELING,0
-------------------------------------------------------------------------------- #,0
# Connect as USER to database DB_NAME through peewee and initialize database proxy,0
# Load embedding model,0
---------------------------------------------------------------------- #,0
CREATE QUEUE OF IMAGES TO LABEL,0
---------------------------------------------------------------------- #,0
Use classifier to generate predictions,0
# Update model predicted class in PostgreSQL database,0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids))# switch where and update?,0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
-------------------------------------------------------------------------------- #,0
CREATE AND SET UP A BOTTLE APPLICATION FOR THE WEB UI,0
-------------------------------------------------------------------------------- #,0
"# static routes (to serve CSS, etc.)",0
"return bottle.static_file(filename, root='../../../../../../../../../.')## missouricameratraps",0
"return bottle.static_file(filename, root='../../../../../../../../../../../.')",0
# dynamic routes,0
if data['display_grayscale']:,0
indices_to_exclude.update(set(color_indices)),0
elif not data['display_grayscale']:,0
indices_to_exclude.update(set(grayscale_indices)),0
data['display_images'] = {},0
data['display_images']['image_ids'] = [dataset.samples[i][0] for i in indices],0
data['display_images']['image_file_names'] = [dataset.samples[i][5] for i in indices],0
data['display_images']['detection_kinds'] = [dataset.samples[i][2] for i in indices],0
data['display_images']['detection_categories'] = [],0
for i in indices:,0
if str(dataset.samples[i][1]) == 'None':,0
data['display_images']['detection_categories'].append('None'),0
else:,0
existing_category_entries = {cat.id: cat.name for cat in Category.select()},0
"cat_name = existing_category_entries[dataset.samples[i][1]].replace(""_"", "" "").title()",0
data['display_images']['detection_categories'].append(cat_name),0
Use image ids in images_to_label to get the corresponding dataset indices,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
print(set(dataset.set_indices[4]).update(set(indices_to_label))),0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Use image id images_to_label to get the corresponding dataset index,0
Update records in dataset dataloader but not in the PostgreSQL database,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]) # userdetection + confirmed detection?,0
Get the category id for the assigned label,0
Update entries in the PostgreSQL database,0
# get Detection table entries corresponding to the images being labeled,0
"# update category_id, category_confidence, and kind of each Detection entry in the PostgreSQL database",0
Update records in dataset dataloader,0
numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value]),0
Train on samples that have been labeled so far,0
dataset.set_kind(DetectionKind.UserDetection.value),0
print(y_train),0
Predict on the samples that have not been labeled,0
print(y_pred),0
Update model predicted class in PostgreSQL database,0
timer = time.time(),0
for pos in range(len(y_pred)):,0
idx = dataset.current_set[pos],0
det_id = dataset.samples[idx][0],0
matching_detection_entries = (Detection,0
".select(Detection.id, Detection.category_id)",0
.where((Detection.id == det_id))),0
mde = matching_detection_entries.get(),0
command = Detection.update(category_id=y_pred[pos]).where(Detection.id == mde.id),0
command.execute(),0
print('Updating the database took %0.2f seconds'%(time.time() - timer)),0
Alternative: batch update PostgreSQL database,0
timer = time.time(),0
det_ids = [dataset.samples[dataset.current_set[pos]][0] for pos in range(len(y_pred))],0
y_pred = [int(y) for y in y_pred],0
"det_id_pred_pairs = list(zip(det_ids, y_pred))",0
"case_statement = Case(Detection.id, det_id_pred_pairs)",0
command = Detection.update(category_id=case_statement).where(Detection.id.in_(det_ids)),0
command.execute(),0
print('Updating the database the other way took %0.2f seconds'%(time.time() - timer)),0
Update dataset dataloader,0
"once the classifier has been trained the first time, switch to AL sampling",0
,0
make_oneclass_json.py,0
,0
"Takes a coco-camera-traps .json database and collapses species classes to binary,",0
optionally removing labels from empty images (to be detector-friendly) (depending on,0
"""experiment_type"").",0
,0
"Assumes that empty images are labeled as ""empty"".",0
,0
%% Imports and environment,0
%% Core conversion function,0
"We're removing empty images from the annotation list, but not from",0
"the ""images"" list; they'll still get used in detector training.",0
print('Ignoring empty annotation'),0
%% Interactive driver,0
%%,0
Load annotations,0
Convert from multi-class to one-class,0
Write out the one-class data,0
%% Command-line driver,0
,0
plot_bounding_boxes.py,0
,0
Takes a .json database containing bounding boxes and renders those boxes on the,0
source images.,0
,0
"This assumes annotations in coco-camera-traps format, with absolute bbox",0
coordinates.,0
,0
%% Imports and environment,0
How many images should we process?  Set to -1 to process all images.,0
Should we randomize the image order?,0
%%  Read database and build up convenience mappings,0
Category ID to category name,0
Image ID to image info,0
Image ID to image path,0
Image ID to all annotations referring to this image,0
"%% Iterate over images, draw bounding boxes, write to file",0
For each image,0
image = images[0],0
Build up a list of bounding boxes to draw on this image,0
Load the image,0
Create figure and axes,0
Display the image,0
ann = imageAnnotations[0],0
"For each annotation associated with this image, render bounding box and label",0
"In the Rectangle() function, the first argument (""location"") is the bottom-left",0
of the rectangle.,0
,0
Origin is the upper-left of the image.,0
Add the patch to the Axes,0
Add a class label,0
This is magic goop that removes whitespace around image plots (sort of),0
Write the output image,0
...for each image,0
,0
plot_imerit_annotations.py,0
,0
Takes a .json file full of bounding box annotations and renders those boxes on the,0
source images.,0
,0
"This assumes annotations in the format we receive them, specifically:",0
,0
1) Relative bbox coordinates,0
"2) A list of .json objects, not a well-formatted .json file",0
,0
"I.e., don't use this on a COCO-style .json file.  See plot_bounding_boxes.py",0
for the same operation performed on a proper COCO-camera-traps database.,0
,0
%% Imports and environment,0
"os.makedirs(outputBase, exist_ok=True)",0
%%  Read all source images and build up a hash table from image name to full path,0
"This spans training and validation directories, so it's not the same as",0
just joining the image name to a base path,0
"%% Iterate over annotations, draw bounding boxes, write to file",0
annData has keys:,0
,0
"annotations, categories, images",0
,0
Each of these are lists of dictionaries,0
%% Render all annotations on each image in the sequence,0
%% Pull out image metadata,0
Build up a list of bounding boxes to draw on this image,0
Pull out just the image name from the filename,0
,0
File names look like:,0
,0
seq6efffac2-5567-11e8-b3fe-dca9047ef277.frame1.img59a94e52-23d2-11e8-a6a3-ec086b02610b.jpg,0
"m = re.findall(r'img(.*\.jpg)$', imgFileName, re.M|re.I)",0
print(m),0
assert(len(m) == 1),0
queryFileName = m[0],0
Map this image back to the original directory,0
"%% Loop over annotations, find annotations that match this image",0
%%,0
"x,y,w,h",0
,0
"x,y is the bottom-left of the rectangle",0
,0
"x,y origin is the upper-left",0
...for each annotation,0
%% Render with PIL (scrap),0
%% Render with Matplotlib,0
Create figure and axes,0
Display the image,0
Location is the bottom-left of the rect,0
,0
Origin is the upper-left,0
Add the patch to the Axes,0
This is magic goop that removes whitespace around image plots (sort of),0
os.startfile(outputFileName),0
%% Showing figures on-screen during debugging,0
plt.show(),0
Various (mostly unsuccessful) approaches to getting the plot window to show up,0
"in the foreground, which is a backend-specific operation...",0
,0
fig.canvas.manager.window.activateWindow(),0
fig.canvas.manager.window.raise_(),0
fm = plt.get_current_fig_manager(),0
"fm.window.attributes('-topmost', 1)",0
"fm.window.attributes('-topmost', 0)",0
,0
# This is the one that I found to be most robust... at like 80% robust.,0
plt.get_current_fig_manager().window.raise_(),0
%%,0
...for each image,0
...for each file,0
,0
convert_imerit_json_to_coco_json.py,0
,0
"Takes a .json file with bounding boxes but no class labels, and a .json file containing the",0
"class labels for those images, and creates a new json file with class labels and bounding",0
boxes.,0
,0
"The bounding box .json file is in the format returned by our annotators, which is not",0
actually a fully-formed .json file; rather it's a series of .json objects,0
,0
"Leaves filenames intact.  Stores imerit ""category IDs"" (empty/human/group/animal) in a new",0
"field called ""annotation_type"".",0
,0
%% Imports and constants,0
%% Configure files and paths,0
"For Snapshot Serengeti, we stored image IDs in our annotation files as:",0
,0
S1_B06_R1_PICT0008,0
,0
...but the corresponding ID in the master database is actually:,0
,0
S1\B06\R1\S1_B06_R1_PICT0008,0
,0
"If this is ""True"", we'll expand the former to the latter",0
Handling a one-off issue in which .'s were mysteriously replaced with -'s,0
"in our annotations.  This will be set dynamically, but I keep it here as",0
a constant to remind me to remove this code when we clean this issue up.,1
Used in the (rare) case where a bounding box was added to an image that was originally,0
annotated as empty,0
Used in the (rare) case where we added bounding boxes to an image with multiple species,0
"Should we include ambiguous bounding boxes (with the ""ambiguous"" category label)",0
in the output file?  Ambiguous boxes are boxes drawn on images with multiple species.,0
"%%  Read metadata from the master database, bounding boxes from the annotations file",0
"The bounding box .json file is in the format returned by our annotators, which is not",0
actually a fully-formed .json file; rather it's a series of .json objects,0
"Each element of annData is a dictionary corresponding to a single sequence, with keys:",0
,0
"annotations, categories, images",0
sequence = annData[0],0
%% Build convenience mappings,0
Image ID to images,0
Category ID to categories (referring to the database categories),0
"Image ID to categories (i.e., species labels)",0
Utility function we'll use to create annotations for images in empty,0
sequences (empty images in non-empty sequences already have annotations),0
"%% Reformat annotations, grabbing category IDs from the master database (prep)",0
iSequence = 0; sequence = annData[0],0
"%% Reformat annotations, grabbing category IDs from the master database (loop)",0
Make a copy here; we're going to manipulate the sequence annotations,0
when we need to add synthetic annotations for empty images,0
im = sequenceImages[0],0
Are there any annotations in this sequence?,0
Which images in this sequence have annotations?,0
For each image in this sequence...,0
imeritImageID = im['id'],0
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG,0
Confirm that the file exists,0
Hande a one-off issue with our annotations,0
datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG,0
,0
...had become:,0
,0
datasetsnapshotserengeti.seqASG000001a-frame0.imgS1_B06_R1_PICT0008.JPG,0
Does it look like we encountered this issue?,0
Convert:,0
,0
S1_B06_R1_PICT0008,0
,0
...to:,0
,0
S1/B06/B06_R1/S1_B06_R1_PICT0008,0
Make sure we only see each image once,0
Create empty annotations for empty images,0
,0
Here we use the *unmodified* file name,0
Create an empty annotation for this image,0
Annotations still use the annotation filename (not database ID) at this point;,0
these will get converted to database IDs below when we process the,0
whole sequence.,0
Sanity-check image size,0
"print('Warning: img {} was listed in DB as {}x{}, annotated as {}x{}, actual size{}x{}'.format(",0
"old_id,new_im['width'],new_im['height'],im['width'],im['height'],imgObj.width,imgObj.height))",0
...for each image in this sequence,0
For each annotation in this sequence...,0
ann = sequenceAnnotations[0],0
Prepare an annotation using the category ID from the database and,0
the bounding box from the annotations file,0
Maintain iMerit's annotation category,0
Generate an (arbitrary) ID for this annotation; the COCO format has a concept,0
"of annotation ID, but our annotation files don't",0
This was a one-off quirk with our file naming,1
We'll do special handling of images with multiple categories later,0
Store the annotation type (group/human/animal/empty),0
This annotation has no bounding box but the image wasn't originally,0
annotated as empty,0
This annotation has a bounding box but the image was originally,0
annotated as empty,0
unnormalize the bbox,0
... for each annotation in this sequence,0
... for each sequence,0
%% Post-processing,0
Count empty images,0
...for each file,0
%% Sanity-check empty images,0
,0
make_ss_annotation_image_folder.py,0
,0
Take a directory full of images with the very long filenames we give annotators:,0
,0
dataset[dataset_id].seq[sequence_id].frame[frame_number].img[img_id].extension,0
,0
"...along with a COCO-camera-traps database referring to those files, and:",0
,0
1) Creates a new COCO-camera-traps database with the original filenames in them,0
(copying the annotations),0
,0
2) Optionally creates a new directory with those images named according to the,0
"Snapshot Serengeti naming convention, including complete relative paths.",0
,0
See convert_imerit_json_to_coco_json to see how we get from the original annotation,0
.json to a COCO-camera-traps database.,0
,0
%% Constants and imports,0
%% Configure files/paths,0
%% Read the annotations (referring to the old filenames),0
"%% Update filenames, optionally copying files",0
im = data['images'][0],0
For each image...,0
E.g. datasetsnapshotserengeti.seqASG000001a.frame0.imgS1_B06_R1_PICT0008.JPG,0
"Find the image name, e.g. S1_B06_R1_PICT0008",0
Convert:,0
,0
S1_B06_R1_PICT0008.JPG,0
,0
...to:,0
,0
S1/B06/B06_R1/S1_B06_R1_PICT0008.JPG,0
...for each image,0
%% Write the revised database,0
,0
get_annotation_tool_link.py,0
,0
"Takes a COCO-camera-traps-style .json file with URLs already embedded, and prepares",0
a link to the visipedia annotation tool that reviews a subset of those images.,0
,0
,0
create_new_annotation_json.py,0
,0
"Creates a subset of a larger .json database, in this case specifically to pick some images",0
from Snapshot Serengeti.,0
,0
from utils import get_db_dicts,0
for seq in already_annotated:,0
seq_to_ims.pop(seq),0
remove already annotated images,0
add lion images,0
lion_seqs = cat_to_seqs[cat_to_id['lionMale']] + cat_to_seqs[cat_to_id['lionFemale']],0
#print(len(lion_seqs)),0
lion_seqs = [seq for seq in lion_seqs if seq not in already_annotated],0
#print(len(lion_seqs)),0
"lion_seqs_to_annotate.extend(random.sample(lion_seqs, min(len(lion_seqs),num_lions)))",0
print(len(lion_seqs_to_annotate)),0
num_elephants = 1000,0
elephant_seqs = cat_to_seqs[cat_to_id['elephant']],0
#print(len(lion_seqs)),0
elephant_seqs = [seq for seq in elephant_seqs if seq not in already_annotated],0
#print(len(lion_seqs)),0
"elephant_seqs_to_annotate = random.sample(elephant_seqs, num_elephants)",0
num_empty = 10,0
empty_seqs_to_annotate = [],0
for loc in loc_to_seqs:,0
empty_seqs = cats_per_location[loc][cat_to_id['empty']],0
empty_seqs = [seq for seq in empty_seqs if seq not in already_annotated],0
empty_seqs = [seq for seq in empty_seqs if seq_to_season[seq] in seasons_to_keep],0
"empty_seqs_to_annotate.extend(random.sample(empty_seqs, min(len(empty_seqs),num_empty)))",0
ims_to_annotate.extend(empty_ims_to_annotate),0
,0
filter_database.py,0
,0
"Look through a COCO-ct database and find images matching some crtieria, writing",0
a subset of images and annotations to a new file.,0
,0
%% Constants and imports,0
%% Configuration,0
%%  Read database and build up convenience mappings,0
Category ID to category name,0
Image ID to image info,0
Image ID to image path,0
%% Filter,0
ann = annotations[0],0
Is this a tiny box or a group annotation?,0
"x,y,w,h",0
All empty annotations should be classified as either empty or ambiguous,0
,0
"The ambiguous cases are basically minor misses on the annotators' part,",0
where two different small animals were present somewhere.,0
%% Write output file,0
,0
find_images_for_classes.py,0
,0
"Given a .json database, find images that are associated with one or more",0
classes.,0
,0
%% Constants and imports,0
%% Configuration,0
%%  Read database and build up convenience mappings,0
Category ID to category name,0
Image ID to image info,0
Image ID to image path,0
%% Look for target-class annotations,0
ann = annotations[0],0
,0
create_tfrecords_format.py,0
,0
This script creates a tfrecords file from a dataset in VOTT format.,0
%% Imports and environment,0
set up the filenames and annotations,0
This loop reads the bboxes and corresponding labels and assigns them,0
the correct image. Kind of slow at the moment...,0
If needed: merging all classes,0
bbox_labels = ['Animal' for _ in bbox_labels],0
BBox coords are stored in the format,0
"x_min (of width axis) y_min (of height axis), x_max, y_max",0
Coordinate system starts in top left corner,0
"In this framework, we need ('ymin', 'xmin', 'ymax', 'xmax') format",0
print out some stats,0
To make sure we loaded the bboxes correctly:,0
self.validate_bboxes(),0
For each image in the data set...,0
Make sure all are greater equal 0,0
%% Main tfrecord generation function,0
Propagate optional metadata to tfrecords,0
endfor each annotation for the current image,0
endfor each image,0
"Calculate number of shards to get the desired number of images per record,",0
ensure it is evenly divisible by the number of threads,0
%% Command-line driver,0
%% Driver,0
,0
make_tfrecords_cis_trans.py,0
,0
Given a .json file that contains a three-element list (train/val/test) of image IDs and a .json database that contains,0
"those image IDs, generates tfrecords whose filenames include ""train""/""val""/""test""",0
,0
"dataset = json.load(open('/ai4efs/databases/snapshotserengeti/oneclass/SnapshotSerengeti_Seasons_1_to_4_tfrecord_format_valid_ims.json','r'))",0
print('Creating trans_val tfrecords'),0
dataset = [im_id_to_im[idx] for idx in trans_val],0
,0
create_classification_tfrecords_from_json.py,0
,0
Called from make_tfrecords_cis_trans.py if you're running a classification experiment.,0
,0
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/',0
datafile = 'eccv_18_annotation_files_oneclass/CaltechCameraTrapsECCV18',0
image_file_root = datafolder+'eccv_18_all_images/',0
datafolder = '/teamscratch/findinganimals/data/iWildCam2018/',0
datafolder = '/data/iwildcam/',0
datafile = 'combined_iwildcam_annotations_oneclass/eccv_train_and_imerit_2',0
need consecutive category ids,0
old_cat_id_to_new_cat_id = {categories[idx]['id']:idx+1 for idx in range(len(categories))},0
print(old_cat_id_to_new_cat_id),0
remove multiclass images,0
print(images[0]),0
print(vis_data[0]),0
,0
make_tfrecords_from_json.py,0
,0
"Given a coco-camera-traps .json file, creates tfrecords",0
,0
Thin wrapper for create_tfrecords_from_json.,0
,0
%% Constants and imports,0
%% Constants and imports (interactive),0
%%,0
%% Main tfrecord generation function,0
"check whether the input file has already been converted to the tfrecords format,",0
"if not, convert",0
"Calculate number of shards to get the desired number of images per record,",0
ensure it is evenly divisible by the number of threads,0
%% Interactive driver,0
%%,0
%% Command-line driver,0
eMammal_make_tfrecords_train_val_test.py,0
,0
"From the tfrecords_format json version of the database, creates three splits",0
of tf_records according to a previously decided split of full image IDs.,0
configurations and paths,0
a tfrecord_format json,0
these are number of images,0
do not include empty images in the train set; note that some images from non-empty sequences,0
"end up being empty (no bbox can be labeled), so these will be included in train set anyways",0
eMammal_make_splits.py,0
,0
"Based on a tfrecords_format json file of the database, creates 3 splits according to",0
the specified fractions based on location (images from the same location should be in,0
one split) or based on images.,0
,0
"If a previous split is provided (append_to_previous_split is True), the entries in",0
"each split will be preserved, and new entries will be appended, so that new models",0
can warm start with a model trained on the original splits.,0
configurations and paths,0
approximate fraction for the new entries,0
read in the previous splits of image ID or location ID if available,0
"find new locations and assign them to a split, without reassigning any previous locations",0
"find out which images are new, shuffle and split them",0
do NOT sort the IDs to keep the shuffled order,0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
add empty category,0
"add all images that don't have annotations, with cat empty",0
print(detection_results.keys()),0
group the detections and gts by image id:,0
print(image_id),0
print(len(scores)),0
print(len(labels)),0
recall_thresh = 0.9,0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall]),0
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])",0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall]),0
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])",0
exp_name = 'eccv_train',0
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json',0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(len(detection_results['images'])),0
print(len(seqs)),0
"print(len(seqs[0]),len(seqs[",0
print(detection_results.keys()),0
group the detections by image id:,0
group the ground truth annotations by image id:,0
print(seq),0
"for image_id, dets in per_image_detections.iteritems():",0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
print(num_gts),0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
if sum(detected_class_labels)>0:,0
print('detected animal box'),0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])",0
"print(scores, tp_fp_labels)",0
num_total_gts += num_gts,0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
print('valid box'),0
"print(best_im, best_score)",0
exp_name = 'eccv_train',0
db_file = '/ai4efs/databases/caltechcameratraps/eccv_train_and_imerit_2.json',0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
need to loop over confidence values,0
"for each value, check if any detections on the image are > conf",0
"If so, that image gets class ""animal""",0
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0",0
"for image_id, dets in per_image_detections.iteritems():",0
"calc prec, rec for this confidence thresh",0
group the ground truth annotations by image id:,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(detection_results.keys()),0
group the detections and gts by image id:,0
print(image_id),0
print(len(scores)),0
print(len(labels)),0
recall_thresh = 0.9,0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in inter_recall]),0
"print('Cis prec. at ',inter_recall[recall_idx],' recall: ', inter_prec[recall_idx])",0
recall_idx = np.argmin([np.abs(x-recall_thresh) for x in loc_recall]),0
"print('Trans prec. at ',loc_recall[recall_idx],' recall: ', loc_prec[recall_idx])",0
print(sorted_ap),0
plt.bar(sorted_ap),0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(detection_results.keys()),0
group the detections by image id:,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
print(gts),0
print(num_gts),0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[:2])",0
"print(scores, tp_fp_labels)",0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
print('detected animal box'),0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])",0
"print(scores, tp_fp_labels)",0
num_total_gts += num_gts,0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
print('valid box'),0
best_im = np.argmax(max_im_scores),0
"print(best_im, best_score)",0
"temp_labels = np.zeros(len(im_detection_labels),  dtype=np.int32)",0
"temp_scores = np.zeros(len(im_detection_scores), dtype=np.float32)",0
"for j in range(min(im_num_gts, len(im_detection_labels))):",0
temp_labels[j] = True #TODO: this currently only works for oneclass?,1
temp_scores[j] = best_score,0
im_detection_labels = temp_labels,0
im_detection_scores = temp_scores,0
num_total_gts+=im_num_gts,0
"print(len(detection_scores), len(detection_scores[0]), len(detection_scores[1]))",0
exp_name = 'small_balanced_cct',0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
print(seq),0
"for image_id, dets in per_image_detections.iteritems():",0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
seq_num_gts.append(num_gts),0
print(num_gts),0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
num_total_gts += 1,0
print('valid box'),0
"print(best_im, best_score)",0
print('no valid box'),0
if sum(seq_num_gts)>0:,0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
need to loop over confidence values,0
"for each value, check if any detections on the image are > conf",0
"If so, that image gets class ""animal""",0
"then run prec, rec over the images to get the values for that confidence threshold, where gt is ""animal"" if num gt boxes > 0",0
"calc prec, rec for this confidence thresh",0
export PYTHONPATH=$PYTHONPATH:tfmodels/research,0
group the detections by image id:,0
group the ground truth annotations by image id:,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates.",0
detection scores for the boxes,0
0-indexed detection classes for the boxes,0
"[ymin, xmin, ymax, xmax] in absolute image coordinates",0
0-indexed groundtruth classes for the boxes,0
print('detected animal box'),0
"print(groundtruth_boxes, groundtruth_class_labels,detected_scores[0],detected_boxes[0], detected_class_labels[0])",0
"print(scores, tp_fp_labels)",0
num_total_gts += num_gts,0
if (tp_fp_labels[0].shape[0] != num_detections):,0
print('Incorrect label length'),0
if scores[0].shape[0] != num_detections:,0
print('Incorrect score length'),0
if tp_fp_labels[0].sum() > num_gts:,0
print('Too many correct detections'),0
print('valid box'),0
"print(best_im, best_score)",0
,0
evaluate_detections.py,0
,0
Adapted from analyze_detection.py which is now archived.,0
,0
%% Imports and constants,0
%% Functions,0
"labels input to compute_object_detection_metrics() needs to start at 0, not 1",0
num_detections = len(dets['boxes']),0
to prevent 'Invalid dimensions for box data.' error,0
this box will not match any detections,0
compute one-class precision/recall/average precision (if every box is just of an object class),0
%% Command-line driver,0
,0
detection_eval_utils.py,0
,0
Utility functions used in evaluate_detections.py,0
,0
group the ground truth annotations by image id,0
@task,0
def check_model_version(self):,0
"self.client.get('model_version', headers=headers, name='model_version')",0
"part is a BodyPart object with b'Content-Type', and b'Content-Disposition', the later includes 'name' and 'filename' info",0
images[part.headers['filename']] = part.content,0
upper limit on total content length (all images and parameters),0
Camera trap images are usually 4:3 width to height,0
The config of the model in use (model/pipeline.config) has min_dimension,0
"600 and max_dimension 1024 for the keep_aspect_ratio_resizer, which first resize an image so",0
"that the smaller edge is 600 pixels; if the longer edge is now more than 1024, it resizes such",0
that the longer edge is 1024 pixels,0
(https://github.com/tensorflow/models/issues/1794),0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"# /ai4e_api_tools has been added to the PYTHONPATH, so we can reference those",0
libraries directly.,0
Use the AI4EAppInsights library to send log messages. NOT REQURIED,0
"Use the AI4EService to execute your functions within a logging trace, which supports long-running/async",0
"functions, handles SIGTERM signals from AKS, etc., and handles concurrent requests.",0
function for processing the request data to the /detect endpoint. It loads data or files into a,0
dictionary for access in the API function. It is passed as a parameter to the API setup.,0
check that the content uploaded is not too big,0
request.content_length is the length of the total payload,0
validate detection confidence value,0
check that the number of images is acceptable for this synchronous API,0
read input images and parameters,0
file of type SpooledTemporaryFile has attributes content_type and a read() method,0
"if the number of requests exceed this limit, a 503 is returned to the caller.",0
consolidate the images into batches and perform detection on them,0
detections is an array of dicts,0
filter the detections by the confidence threshold,0
"each result is [ymin, xmin, ymax, xmax, confidence, category]",0
return results; optionally render the detections on the images and send the annotated images back,0
PIL.Image.convert() returns a converted copy of this image,0
performs inference,0
number of images should be small - all are loaded at once and a copy of resized version exists at one point,0
resize the images since the client side renders them as small images too,0
group the images into batches; image_batches is a list of lists,0
get the operators to go in the fetch list,0
the following two functions are from https://github.com/tensorflow/models/blob/master/research/object_detection/utils/visualization_utils.py,0
"print('Input must be of size [N, 4], but is ' + str(boxes_shape))",0
If the total height of the display strings added to the top of the bounding,0
"box exceeds the top of the image, stack the strings below the bounding box",0
instead of above.,0
Each display_str has a top and bottom margin of 0.05x.,0
Reverse list and print from bottom to top.,0
%% Constants and imports,0
Assumes ai4eutils is on the python path,0
https://github.com/Microsoft/ai4eutils,0
Assumes the cameratraps repo root is on the path,0
%% Options,0
"Make sure there is no overlap between the two sets, because this will cause",0
issues in the code,0
## Required inputs,0
## Options,0
Can be a folder or a SAS URL,0
These apply only when we're doing ground-truth comparisons,0
"A list of output sets that we should count, but not render images for.",0
,0
"Typically used to preview sets with lots of empties, where you don't want to",0
"subset but also don't want to render 100,000 empty images.",0
,0
"detections, non_detections",0
"detections_animal, detections_person, detections_vehicle",0
Used for summary statistics only,0
"Number of images to sample, -1 for ""all images""",0
"Random seed for sampling, or None",0
Optionally separate detections into categories (animal/vehicle/human),0
Optionally replace one or more strings in filenames with other strings;,0
this is useful for taking a set of results generated for one folder structure,0
and applying them to a slightly different folder structure.,0
Allow bypassing API output loading when operating on previously-loaded results,0
Should we also split out a separate report about the detections that were,0
just below our main confidence threshold?,0
,0
Currently only supported when ground truth is unavailable,0
Control rendering parallelization,0
Determines whether missing images force an error,0
...PostProcessingOptions,0
#%% Helper classes and functions,0
Flags used to mark images as positive or negative for P/R analysis (according,0
to ground truth and/or detector output),0
This image is a negative,0
This image is a positive,0
Anything greater than this isn't clearly positive or negative,0
This image has annotations suggesting both negative and positive,0
"This image is not annotated or is annotated with 'unknown', 'unlabeled', ETC.",0
This image has not yet been assigned a state,0
"In some analyses, we add an additional class that lets us look at detections just below",0
our main confidence threshold,0
Counter for the corresponding fields of class (actually enum) DetectionStatus,0
Check whether this image has unassigned-type labels,0
"assert image_has_unknown_labels is False, '{} has unknown labels'.format(annotations)",0
Check whether this image has negative-type labels,0
"Check whether this image has positive labels, i.e. whether it has labels that are neither",0
negative nor unknown,0
"If there are no image annotations, treat this as unknown",0
n_negative += 1,0
im['_detection_status'] = DetectionStatus.DS_NEGATIVE,0
"If the image has more than one type of labels, it's ambiguous",0
"note: booleans get automatically converted to 0/1, hence we can use the sum",0
"After the check above, we can be sure it's only one of positive, negative, or unknown",0
,0
Important: do not merge the following 'unknown' branch with the first 'unknown' branch,0
"above, where we were testing 'if len(image_categories) == 0'",0
,0
If the image has only unknown labels,0
If the image has only negative labels,0
If the images has only positive labels,0
"Annotate the category, if it is unambiguous",0
...for each image,0
...mark_detection_status(),0
"Leaving code in place for reading from blob storage, may support this",0
in the future.,0
isfile() is slow when mounting remote directories; much faster to just try/except,0
on the image open.,0
Render images to a flat folder... we can use os.sep here because we've,0
already normalized paths,0
"errno.ENAMETOOLONG doesn't get thrown properly on Windows, so",0
we awkwardly check against a hard-coded limit,0
Use slashes regardless of os,0
...render_bounding_boxes,0
Count items in each category,0
Optionally sort by filename before writing to html,0
Write the individual HTML files,0
...prepare_html_subpages(),0
%% Main function,0
#%% Expand some options for convenience,0
#%% Prepare output dir,0
#%% Load ground truth if available,0
Mark images in the ground truth as positive or negative,0
#%% Load detection (and possibly classification) results,0
Convert keys and values to lowercase,0
,0
"In practice, keys are string integers, but I'm angry at variable casing",0
so I'm converting those to lowercase too just to pound my fist.,0
"Add a column (pred_detection_label) to indicate predicted detection status, not separating out the classes",0
"#%% If we have ground truth, remove images we can't match to ground truth",0
fn = detector_files[0]; print(fn),0
"assert fn in ground_truth_indexed_db.filename_to_id, 'Could not find ground truth for row {} ({})'.format(i_fn,fn)",0
#%% Sample images for visualization,0
#%% Fork here depending on whether or not ground truth is available,0
"If we have ground truth, we'll compute precision/recall and sample tp/fp/tn/fn.",0
,0
Otherwise we'll just visualize detections/non-detections.,0
#%% Detection evaluation: compute precision/recall,0
numpy array of detection probabilities,0
"numpy array of bools (0.0/1.0), and -1 as null value",0
Don't include ambiguous/unknown ground truth in precision/recall analysis,0
"For completeness, include the result at a confidence threshold of 1.0",0
Compute and print summary statistics,0
Thresholds go up throughout precisions/recalls/thresholds; find the last,0
value where recall is at or above target.  That's our precision @ target recall.,0
Flatten the confusion matrix,0
"#%% Collect classification results, if they exist",0
Mapping of classnames to idx for the confusion matrix.,0
,0
"The lambda is actually kind of a hack, because we use assume that",1
the following code does not reassign classname_to_idx,0
Confusion matrix as defaultdict of defaultdict,0
,0
"Rows / first index is ground truth, columns / second index is predicted category",0
iDetection = 0; fn = detector_files[iDetection]; print(fn),0
"If this image has classification predictions, and an unambiguous class",0
"annotated, and is a positive image...",0
"The unambiguous category, we make this a set for easier handling afterward",0
"Compute the accuracy as intersection of union,",0
i.e. (# of categories in both prediciton and GT),0
divided by (# of categories in either prediction or GT,0
,0
"In case of only one GT category, the result will be 1.0, if",0
prediction is one category and this category matches GT,0
,0
"It is 1.0/(# of predicted top-1 categories), if the GT is",0
one of the predicted top-1 categories.,0
,0
"It is 0.0, if none of the predicted categories is correct",0
Distribute this accuracy across all predicted categories in the,0
confusion matrix,0
...for each file in the detection results,0
If we have classification results,0
Build confusion matrix as array from classifier_cm,0
Print some statistics,0
Prepare confusion matrix output,0
Get confusion matrix as string,0
Get fixed-size classname for each idx,0
Prepend class name on each line and add to the top,0
Print formatted confusion matrix,0
Plot confusion matrix,0
To manually add more space at bottom: plt.rcParams['figure.subplot.bottom'] = 0.1,0
,0
"Add 0.5 to figsize for every class. For two classes, this will result in",0
"fig = plt.figure(figsize=[4,4])",0
...if we have classification results,0
#%% Render output,0
Write p/r table to .csv file in output directory,0
Write precision/recall plot to .png file in output directory,0
plt.show(block=False),0
#%% Sampling,0
Sample true/false positives/negatives with correct/incorrect top-1,0
classification and render to html,0
Accumulate html image structs (in the format expected by write_html_image_lists),0
"for each category, e.g. 'tp', 'fp', ..., 'class_bird', ...",0
Add default entries by accessing them for the first time,0
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]",0
"Each element will be a three-tuple with elements file,max_conf,detections",0
"Assemble the information we need for rendering, so we can parallelize without",0
dealing with Pandas,0
i_row = 0; row = images_to_visualize.iloc[0],0
Filenames should already have been normalized to either '/' or '\',0
This should already have been normalized to either '/' or '\',0
...def render_image_with_gt(file_info),0
file_info = files_to_render[0],0
Map all the rendering results in the list rendering_results into the,0
dictionary images_html,0
Prepare the individual html image files,0
Write index.html,0
Show links to each GT class,0
,0
We could do this without classification results; currently we don't.,0
Add links to all available classes,0
Close body and html tags,0
...for each image,0
"#%% Otherwise, if we don't have ground truth...",0
#%% Sample detections/non-detections,0
Accumulate html image structs (in the format expected by write_html_image_list),0
for each category,0
Add default entries by accessing them for the first time,0
"Maps detection categories - e.g. ""human"" - to result set names, e.g.",0
"""detections_human""",0
Add a set of results for each category and combination of categories,0
Create output directories,0
"Each element will be a list of 2-tuples, with elements [collection name,html info struct]",0
"Each element will be a three-tuple with elements [file,max_conf,detections]",0
"Assemble the information we need for rendering, so we can parallelize without",0
dealing with Pandas,0
i_row = 0; row = images_to_visualize.iloc[0],0
Filenames should already have been normalized to either '/' or '\',0
Get unique categories above the threshold for this image,0
Local function for parallelization,0
...def render_image_no_gt(file_info):,0
Map all the rendering results in the list rendering_results into the,0
dictionary images_html,0
Prepare the individual html image files,0
Write index.HTML,0
Add links to all available classes,0
os.startfile(output_html_file),0
...if we do/don't have ground truth,0
...process_batch_results,0
%% Interactive driver(s),0
%%,0
os.start(ppresults.output_html_file),0
%% Command-line driver,0
,0
subset_json_detector_output.py,0
,0
"Creates one or more subsets of a detector API output file (.json), doing either",0
"or both of the following (if both are requested, they happen in this order):",0
,0
"1) Retrieve all elements where filenames contain a specified query string,",0
"optionally replacing that query with a replacement token. If the query is blank,",0
can also be used to prepend content to all filenames.,0
,0
"2) Create separate .jsons for each unique path, optionally making the filenames",0
"in those .json's relative paths.  In this case, you specify an output directory,",0
rather than an output path.  All images in the folder blah\foo\bar will end up,0
in a .json file called blah_foo_bar.json.,0
,0
##,0
,0
Sample invocations (splitting into multiple json's):,0
,0
"Read from ""1800_idfg_statewide_wolf_detections_w_classifications.json"", split up into",0
"individual .jsons in 'd:\temp\idfg\output', making filenames relative to their individual",0
folders:,0
,0
"python subset_json_detector_output.py ""d:\temp\idfg\1800_idfg_statewide_wolf_detections_w_classifications.json"" ""d:\temp\idfg\output"" --split_folders --make_folder_relative",0
,0
"Now do the same thing, but instead of writing .json's to d:\temp\idfg\output, write them to *subfolders*",0
corresponding to the subfolders for each .json file.,0
,0
"python subset_json_detector_output.py ""d:\temp\idfg\1800_detections_S2.json"" ""d:\temp\idfg\output_to_folders"" --split_folders --make_folder_relative --copy_jsons_to_folders",0
,0
##,0
,0
Sample invocations (creating a single subset matching a query):,0
,0
"Read from ""1800_detections.json"", write to ""1800_detections_2017.json""",0
,0
"Include only images matching ""2017"", and change ""2017"" to ""blah""",0
,0
"python subset_json_detector_output.py ""d:\temp\1800_detections.json"" ""d:\temp\1800_detections_2017_blah.json"" --query 2017 --replacement blah",0
,0
"Include all images, prepend with ""prefix/""",0
,0
"python subset_json_detector_output.py ""d:\temp\1800_detections.json"" ""d:\temp\1800_detections_prefix.json"" --replacement ""prefix/""",0
,0
##,0
,0
"To subset a COCO Camera Traps .json database, see subset_json_db.py",0
,0
%% Constants and imports,0
%% Helper classes,0
Only process files containing the token 'query',0
"Replace 'query' with 'replacement' if 'replacement' is not None.  If 'query' is None,",0
prepend 'replacement',0
Should we split output into individual .json files for each folder?,1
"Folder level to use for splitting ['bottom','top','n_from_bottom','dict']",0
,0
'dict' requires 'split_folder_param' to be a dictionary mapping each filename,0
to a token.,0
"When using the 'n_from_bottom' parameter to define folder splitting, this",0
defines the number of directories from the bottom.  'n_from_bottom' with,0
a parameter of zero is the same as 'bottom'.,0
,0
"When 'split_folder_mode' is 'dict', this should be a dictionary mapping each filename",0
to a token.,0
Only meaningful if split_folders is True: should we convert pathnames to be relative,0
the folder for each .json file?,0
"Only meaningful if split_folders and make_folder_relative are True: if not None,",0
"will copy .json files to their corresponding output directories, relative to",0
output_filename,0
Should we over-write .json files?,0
"If copy_jsons_to_folders is true, do we require that directories already exist?",0
Threshold on confidence,0
%% Main function,0
Format spec:,0
,0
https://github.com/microsoft/CameraTraps/tree/master/api/batch_processing,0
iImage = 0; im = images_in[0],0
Find all detections above threshold for this image,0
"If there are no detections above threshold, set the max probability",0
"to -1, unless it already had a negative probability.",0
Otherwise find the max confidence,0
Did this thresholding result in a max-confidence change?,0
"We should only be *lowering* max confidence values (i.e., making them negative)",0
...for each image,0
i_image = 0; im = images_in[0],0
Only take images that match the query,0
...for each image,0
"Path('/blah').parts is ('/','blah')",0
Handle paths like:,0
,0
"/, \, /stuff, c:, c:\stuff",0
Input validation,0
data = add_missing_detection_results_fields(data),0
Map images to unique folders,0
im = data['images'][0],0
Optionally make paths relative,0
dirname = list(folders_to_images.keys())[0],0
im = folders_to_images[dirname][0],0
dirname = list(folders_to_images.keys())[0],0
"Recycle the 'data' struct, replacing 'images' every time... medium-hacky, but",1
forward-compatible in that I don't take dependencies on the other fields,0
...for each directory,0
...if we're splitting folders,0
%% Interactive driver,0
%%,0
%% Subset a file without splitting,0
"%% Subset and split, but don't copy to individual folders",0
"input_filename = r""D:\temp\idfg\1800_detections_S2.json""",0
"%% Subset and split, copying to individual folders",0
%% Just do a filename replacement,0
"python subset_json_detector_output.py ""D:\temp\idfg\detections_idfg_20190625_refiltered.json"" ""D:\temp\idfg\detections_idfg_20190625_refiltered_renamed.json"" --query ""20190625-hddrop/"" --replacement """"",0
"python subset_json_detector_output.py ""D:\temp\idfg\detections_idfg_20190625_refiltered_renamed.json"" ""D:\temp\idfg\output"" --split_folders --make_folder_relative --copy_jsons_to_folders",0
%% Command-line driver,0
Convert to an options object,0
###,0
,0
combine_api_outputs.py,0
,0
"Merges two or more .json files in batch API output format, optionally",0
writing the results to another .json file.,0
"- Concatenates image lists, erroring if images are not unique.",0
- Errors if class lists are conflicting; errors on unrecognized fields.,0
"- Checks compatibility in info structs, within reason.",0
,0
File format:,0
,0
https://github.com/microsoft/CameraTraps/tree/master/api/batch_processing#batch-processing-api-output-format,0
,0
Command-line use:,0
,0
combine_api_outputs input1.json input2.json ... inputN.json output.json,0
,0
Also see combine_api_shard_files (not exposed via the command line yet) to combine the,0
intermediate files created by the API.,0
,0
###,0
%% Constants and imports,0
%% Merge functions,0
"Map image filenames to detections, we'll convert to a list later",0
Check compatibility of detection categories,0
Check compatibility of classification categories,0
"Merge image lists, checking uniqueness",0
"print('Warning, duplicate results for image: {}'.format(im['file']))",0
"Merge info dicts, within reason",0
Don't check completion time fields,0
...for each dictionary,0
Convert merged image dictionaries to a sorted list,0
detection_list = input_lists[0],0
d = detection_list[0],0
%% Driver,0
,0
load_api_results.py,0
,0
Loads the output of the batch processing API (json).,0
Also functions to group entries by seq_id.,0
,0
Includes the deprecated functions that worked with the old CSV API output format.,0
,0
%% Constants and imports,0
%% Functions for grouping by sequence_id,0
example,0
input 'file': 'SER/S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG',0
output 'id': 'S1/F08/F08_R3/S1_F08_R3_PICT1150.JPG',0
%% Functions for loading the result as a Pandas DataFrame,0
Sanity-check that this is really a detector output file,0
Fields in the API output json other than 'images',0
Normalize paths to simplify comparisons later,0
Pack the json output into a Pandas DataFrame,0
Optionally replace some path tokens to match local paths to the original blob structure,0
string_to_replace = list(options.detector_output_filename_replacements.keys())[0],0
"TODO: hit some silly issues with vectorized str() and escaped characters, vectorize",1
this later.,0
,0
"detection_results['image_path'].str.replace(string_to_replace,replacement_string)",0
i_row = 0,0
TODO: read double_precision from a config elsewhere,1
Sanity-check that this is really a detector output file,0
Normalize paths to simplify comparisons later,0
De-serialize detections,0
Optionally replace some path tokens to match local paths to the original blob structure,0
string_to_replace = list(options.detector_output_filename_replacements.keys())[0],0
"TODO: hit some silly issues with vectorized str() and escaped characters, vectorize",1
this later.,0
,0
"detection_results['image_path'].str.replace(string_to_replace,replacement_string)",0
iRow = 0,0
#######,0
,0
convert_output_format.py,0
,0
Converts between file formats output by our batch processing API.  Currently,0
"supports json <--> csv conversion, but this should be the landing place for any",0
conversion - including between future .json versions - that we support in the,0
future.,0
,0
#######,0
%% Imports,0
%% Conversion functions,0
"We add an output column for each class other than 'empty',",0
containing the maximum probability of  that class for each image,0
Skip sub-threshold detections,0
Our .json format is xmin/ymin/w/h,0
,0
Our .csv format was ymin/xmin/ymax/xmax,0
"Category 0 is empty, for which we don't have a column, so the max",0
confidence for category N goes in column N-1,0
...for each detection,0
...for each image,0
Format spec:,0
,0
https://github.com/microsoft/CameraTraps/tree/master/api/batch_processing,0
iFile = 0; row = df.iloc[iFile],0
Our .csv format was ymin/xmin/ymax/xmax,0
,0
Our .json format is xmin/ymin/w/h,0
...for each detection,0
...for each image,0
%% Interactive driver,0
%%,0
%%,0
%% Command-line driver,0
,0
separate_detections_into_folders.py,0
,0
"Given a .json file with batch processing results, separate the files in that",0
"set of results into folders that contain animals/people/vehicles/nothing,",0
according to per-class thresholds.,0
,0
Places images that are above threshold for multiple classes into 'multiple',0
folder.,0
,0
"Image files are copied, not moved.",0
,0
Preserves relative paths within each of those folders; cannot be used with .json,0
files that have absolute paths in them.,0
,0
"For example, if your .json file has these images:",0
,0
a/b/c/1.jpg,0
a/b/d/2.jpg,0
a/b/e/3.jpg,0
a/b/f/4.jpg,0
,0
And let's say:,0
,0
"* The results say that the first three images are empty/person/vehicle, respectively",0
"* The fourth image is above threshold for ""animal"" and ""person""",0
* You specify an output base folder of c:\out,0
,0
You will get the following files:,0
,0
c:\out\empty\a\b\c\1.jpg,0
c:\out\people\a\b\d\2.jpg,0
c:\out\vehicles\a\b\e\3.jpg,0
c:\out\multiple\a\b\f\4.jpg,0
,0
Hard-coded to work with MDv3 and MDv4 output files.  Not currently future-proofed,0
"past the classes in MegaDetector v4, not currently ready for species-level classification.",0
,0
%% Constants and imports,0
Occasionally we have near-zero confidence detections associated with COCO classes that,0
didn't quite get squeezed out of the model in training.  As long as they're near zero,0
"confidence, we just ignore them.",0
%% Options class,0
Inputs,0
Dictionary mapping categories (plus 'multiple' and 'empty') to output folders,0
%% Support functions,0
Find the maximum confidence for each category,0
,0
det = detections[0],0
"For zero-confidence detections, we occasionally have leftover goop",0
from COCO classes,0
assert det['conf'] < invalid_category_epsilon,0
Count the number of thresholds exceeded,0
Do we have a custom threshold for this category?,0
If this is above multiple thresholds,0
...def process_detection(),0
%% Main function,0
Create output folder if necessary,0
Load detection results,0
Map class names to output folders,0
i_image = 7600; d = detections[i_image]; print(d),0
%% Interactive driver,0
%%,0
%%,0
%% Find a particular file,0
%% Command-line driver,0
"python api\batch_processing\postprocessing\separate_detections_into_folders.py ""d:\temp\rspb_mini.json"" ""d:\temp\demo_images\rspb_2018_2019_mini"" ""d:\temp\separation_test"" --nthreads 2",0
Convert to an options object,0
#######,0
,0
remove_repeat_detections.py,0
,0
"Used after running find_repeat_detections, then manually filtering the results,",0
to create a final filtered output file.,0
,0
"If you want to use this script, we recommend that you read the user's guide:",0
,0
https://github.com/microsoft/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms,0
,0
#######,0
%% Constants and imports,0
%% Main function,0
%% Interactive driver,0
%%,0
"python remove_repeat_detections.py ""F:\wpz\6714_detections_wpz_all_20191015233705.SUCP_subset.json"" ""F:\wpz\6714_detections_wpz_all_20191015233705.SUCP_subset_filtered.json"" ""F:\wpz\rde\filtering_2019.10.24.16.52.54""",0
%% Command-line driver,0
#######,0
,0
repeat_detections_core.py,0
,0
Core utilities shared by find_repeat_detections and remove_repeat_detections.,0
,0
#######,0
%% Imports and environment,0
"from ai4eutils; this is assumed to be on the path, as per repo convention",0
Imports I'm not using but use when I tinker with parallelization,0
,0
from multiprocessing import Pool,0
from multiprocessing.pool import ThreadPool,0
import multiprocessing,0
import joblib,0
"ignoring all ""PIL cannot read EXIF metainfo for the images"" warnings",0
"Metadata Warning, tag 256 had too many entries: 42, expected 1",0
%% Constants,0
%% Classes,0
inputFlename = r'D:\temp\tigers_20190308_all_output.csv',0
Relevant for rendering HTML or filtering folder of images,0
,0
"imageBase can also be a SAS URL, in which case some error-checking is",0
disabled.,0
Don't consider detections with confidence lower than this as suspicious,0
Don't consider detections with confidence higher than this as suspicious,0
What's the IOU threshold for considering two boxes the same?,0
How many occurrences of a single location (as defined by the IOU threshold),0
are required before we declare it suspicious?,0
"Ignore ""suspicious"" detections larger than some size; these are often animals",0
taking up the whole image.  This is expressed as a fraction of the image size.,0
A list of classes we don't want to treat as suspicious. Each element is an int.,0
Set to zero to disable parallelism,0
Load detections from a filter file rather than finding them from the detector output,0
".json file containing detections, should be called detectionIndex.json in the filtering_* folder",0
produced in the first pass,0
(optional) List of filenames remaining after deletion of identified,0
repeated detections that are actually animals.  This should be a flat,0
"text file, one relative filename per line.  See enumerate_images().",0
Turn on/off optional outputs,0
State variables,0
"Replace filename tokens after reading, useful when the directory structure",0
has changed relative to the structure the detector saw,0
How many folders up from the leaf nodes should we be going to aggregate images?,0
"The data table (Pandas DataFrame), as loaded from the input json file via",0
load_api_results(),0
"The other fields in the input json file, loaded via load_api_results()",0
The data table after modification,0
dict mapping folder names to whole rows from the data table,0
dict mapping filenames to rows in the master table,0
"An array of length nDirs, where each element is a list of DetectionLocation",0
objects for that directory that have been flagged as suspicious,0
#%% Helper functions,0
#%% Look for matches (one directory) (function),0
List of DetectionLocations,0
iDirectoryRow = 0; row = rows.iloc[iDirectoryRow],0
Don't bother checking images with no detections above threshold,0
"Array of dict, where each element is",0
{,0
"'category': '1',  # str value, category ID",0
"'conf': 0.926,  # confidence of this detections",0
"'bbox': [x_min, y_min, width_of_box, height_of_box]  # (x_min, y_min) is upper-left,",0
all in relative coordinates and length,0
},0
For each detection in this image,0
Optionally exclude some classes from consideration as suspicious,0
Is this detection too big to be suspicious?,0
These are relative coordinates,0
print('Ignoring very large detection with area {}'.format(area)),0
For each detection in our candidate list,0
Is this a match?,0
"If so, add this example to the list for this detection",0
We *don't* break here; we allow this instance to possibly,0
match multiple candidates.  There isn't an obvious right or,0
wrong here.,0
...for each detection on our candidate list,0
"If we found no matches, add this to the candidate list",0
...for each detection,0
...for each row,0
...def find_matches_in_directory(dirName),0
#%% Render problematic locations to html (function),0
suspiciousDetectionsThisDir is a list of DetectionLocation objects,0
For each problematic detection in this directory,0
,0
iDetection = 0; detection = suspiciousDetectionsThisDir[iDetection];,0
iDetection = 0; detection = suspiciousDetectionsThisDir[0],0
_ = pretty_print_object(detection),0
Render images,0
iInstance = 0; instance = detection.instances[iInstance],0
...for each instance,0
Write html for this detection,0
Use the first image from this detection (arbitrary) as the canonical example,0
that we'll render for the directory-level page.,0
...for each detection,0
Write the html file for this directory,0
...def render_images_for_directory(iDir),0
"#%% Update the detection table based on suspicious results, write .csv output",0
"An array of length nDirs, where each element is a list of DetectionLocation",0
objects for that directory that have been flagged as suspicious,0
For each directory,0
For each suspicious detection group in this directory,0
For each instance of this suspicious detection,0
This should match the bbox for the detection event,0
The bbox for this instance should be almost the same as the bbox,0
"for this detection group, where ""almost"" is defined by the IOU",0
threshold.,0
if iou < options.iouThreshold:,0
"print('IOU warning: {},{}'.format(iou,options.iouThreshold))",0
Make sure the bounding box matches,0
"Make the probability negative, if it hasn't been switched by",0
another bounding box,0
...for each instance,0
...for each detection,0
...for each directory,0
Update maximum probabilities,0
For each row...,0
We should only be making detections *less* likely,0
row['max_confidence'] = str(maxP),0
"Negative probabilities should be the only reason maxP changed, so",0
we should have found at least one negative value,0
...if there was a meaningful change to the max probability for this row,0
...for each row,0
If we're also writing output...,0
"...def update_detection_table(RepeatDetectionResults,options)",0
#%% Main function,0
#%% Input handling,0
Check early to avoid problems with the output folder,0
Load file,0
"Before doing any real work, make sure we can *probably* access images",0
"This is just a cursory check on the first image, but it heads off most",0
"problems related to incorrect mount points, etc.  Better to do this before",0
spending 20 minutes finding repeat detections.,0
#%% Separate files into directories,0
This will be a map from a directory name to smaller data frames,0
This is a mapping back into the rows of the original table,0
"TODO: in the case where we're loading an existing set of FPs after manual filtering,",1
"we should load these data frames too, rather than re-building them from the input.",0
iRow = 0; row = detectionResults.iloc[0],0
Create a new DataFrame with just this row,0
rowsByDirectory[dirName] = pd.DataFrame(row),0
Convert lists of rows to proper DataFrames,0
#% Look for matches (or load them from file),0
length-nDirs list of lists of DetectionLocation objects,0
"Are we actually looking for matches, or just loading from a file?",0
We're actually looking for matches...,0
iDir = 0; dirName = dirsToSearch[iDir],0
#%% Find suspicious locations based on match results,0
For each directory,0
,0
iDir = 51,0
A list of DetectionLocation objects,0
A list of DetectionLocation objects,0
occurrenceList is a list of file/detection pairs,0
"Find the images corresponding to this bounding box, render boxes",0
Load the filtering file,0
"We're skipping detection-finding, but to see which images are actually legit false",0
"positives, we may be looking for physical files or loading from a text file.",0
For each directory,0
iDir = 0; detections = suspiciousDetections[0],0
,0
"suspiciousDetections is an array of DetectionLocation objects,",0
one per directory.,0
For each detection that was present before filtering,0
iDetection = 0; detection = detections[iDetection],0
Are we checking the directory to see whether detections were actually false,0
"positives, or reading from a list?",0
Is the image still there?,0
"If not, remove this from the list of suspicious detections",0
...for each detection,0
...for each directory,0
...if we are/aren't finding detections (vs. loading from file),0
Render problematic locations with html (loop),0
options.pbar = tqdm(total=nDirs),0
For each directory,0
iDir = 51,0
Add this directory to the master list of html files,0
...for each directory,0
Write master html file,0
Remove unicode characters before formatting,0
...if we're rendering html,0
Create filtering directory,0
iDir = 0; suspiciousDetectionsThisDir = suspiciousDetections[iDir],0
suspiciousDetectionsThisDir is a list of DetectionLocation objects,0
iDetection = 0; detection = suspiciousDetectionsThisDir[0],0
Write out the detection index,0
...if we're writing filtering info,0
...find_repeat_detections(),0
#######,0
,0
find_repeat_detections.py,0
,0
"If you want to use this script, we recommend that you read the user's guide:",0
,0
https://github.com/microsoft/CameraTraps/tree/master/api/batch_processing/postprocessing/repeat_detection_elimination.ms,0
,0
"Really, don't try to run this script without reading the user's guide, you'll think",0
it's more magical than it is.,0
,0
"This script looks through a sequence of detections in the API output json file, and finds",0
"candidates that might be ""repeated false positives"", i.e. that random branch that the",0
detector thinks is an animal/person/vehicle.,0
,0
"Typically after running this script, you would do a manual step to remove",0
"true positives, then run remove_repeat_detections to produce a final output file.",0
,0
There's no way that statement was self-explanatory; see the user's guide.,0
,0
#######,0
%% Constants and imports,0
%% Interactive driver,0
%%,0
"inputFilename = os.path.join(baseDir, '5570_blah_detections.json')",0
"outputFilename = mpt.insert_before_extension(inputFilename,",0
'filtered'),0
%% Command-line driver,0
With HTML (debug),0
"python find_repeat_detections.py ""D:\temp\tigers_20190308_all_output.json"" ""D:\temp\tigers_20190308_all_output.filtered.json"" --renderHtml --debugMaxDir 100 --imageBase ""d:\wildlife_data\tigerblobs"" --outputBase ""d:\temp\repeatDetections""",0
Without HTML (debug),0
"python find_repeat_detections.py ""D:\temp\tigers_20190308_all_output.json"" ""D:\temp\tigers_20190308_all_output.filtered.json"" --debugMaxDir 100 --imageBase ""d:\wildlife_data\tigerblobs"" --outputBase ""d:\temp\repeatDetections""",0
With HTML (for real),0
"python find_repeat_detections.py ""D:\temp\tigers_20190308_all_output.json"" ""D:\temp\tigers_20190308_all_output.filtered.json"" --renderHtml --imageBase ""d:\wildlife_data\tigerblobs"" --outputBase ""d:\temp\repeatDetections""",0
Convert to an options object,0
,0
If a request has been sent to AML for batch scoring but the monitoring thread of the API was,0
"interrupted (uncaught exception or having to re-start the API container), we could manually",0
"aggregate results from each shard using this script, assuming all jobs submitted to AML have finished.",0
,0
Need to have set environment variables STORAGE_ACCOUNT_NAME and STORAGE_ACCOUNT_KEY to those of the,0
"storage account backing the API. Also need to adjust the INTERNAL_CONTAINER, AML_CONTAINER and",0
AML_CONFIG fields in api_core/orchestrator_api/api_config.py to match the instance of the API that this,0
request was submitted to.,0
,0
May need to change the import statement in api_core/orchestrator_api/orchestrator.py,0
"""from sas_blob_utils import SasBlob"" to",0
"""from .sas_blob_utils import SasBlob"" to not confuse with the module in AI4Eutils;",0
"and change ""import api_config"" to",0
"""from api.batch_processing.api_core.orchestrator_api import api_config""",0
Execute this script from the root of the repository. You may need to add the repository to PYTHONPATH.,0
"list_jobs_submitted cannot be serialized (""can't pickle _thread.RLock objects ""), but",0
do not need it for aggregating results,0
name of the container in the internal storage account to store user facing files:,0
"image list, detection results and failed images list.",0
name of the container in the internal storage account to store outputs of each AML job,0
"if this number of times the thread wakes up to check is exceeded, stop the monitoring thread",0
number of retries in the monitoring thread for getting job status and aggregating results (each counted separately),0
lower case; must be tuple for endswith to take as arg,0
max number of images in a container to accept for processing,0
how many images are processed by each call to the scoring API,0
update API task manager after submitting x jobs to AML Compute,0
AML Compute,0
service principle for authenticating to AML,0
version of the detector model in use,0
max number of blobs to list in the output blob container,0
URLs to the 3 output files expires after this many days,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
Get the entire path with all slashes after the container,0
TODO - fix this for testing out only the query string,1
exhaustively listed all blobs in the container,0
list_blobs will have been returned by one of the two stopping conditions,0
Service principle authentication for AML,0
%% Utility functions,0
return the current UTC time in string format '2019-05-19 08:57:43',0
"return current UTC time in succinct format as a string, e.g. '20190519085759'",0
"image_paths will have length at least 1, otherwise would have ended before this step",0
%% AML Compute,0
default values are required and need to be literal values or data references as JSON,0
setting the overwrite flag to True overwrites any datastore that was created previously with that name,0
"internal_datastore stores all user-facing files: list of images, detection results, list of failed images",0
and it so happens that each job also needs the list of images as an input,0
"output_datastore stores the output from score.py in each job, which is another container",0
in the same storage account as interl_datastore,0
various attempts at getting the child_run's ID,0
child_run_id = None,0
"print('pipeline_run:', pipeline_run)",0
for child_run in pipeline_run.get_children():,0
child_run_id = child_run.id  # we can do this because there's only one step in the pipeline - not working,0
,0
print('=' * 20),0
"exp = Experiment(self.ws, job_id)",0
"run = Run(exp, pipeline_run.id)",0
"print('run:', run)",0
for c in run.get_children():,0
print('found run.get_children:'),0
print(c.id),0
child_run_id = c.id,0
print('=' * 20),0
list_jobs_active[job_id]['step_run_id']  = child_run_id  # this is the ID we can identify the output folder with,0
%% AML Monitor,0
list of images do not have request_name and timestamp in the file name so score.py can locate it easily,0
The more efficient method is to know the run_id which is the folder name that the result is written to.,0
"Since we can't reliably get the run_id after submitting the run, resort to listing all blobs in the output",0
container and match by the request_id,0
listing all (up to a large limit) because don't want to worry about generator next_marker,0
blob_path is azureml/run_id/output_requestID/out_file_name.json,0
"""request"" is part of the AML job_id",0
order the json output keys,0
upload aggregated results to output_store,0
Copyright (c) Microsoft Corporation. All rights reserved.,0
Licensed under the MIT License.,0
"ai4e_api_tools has been added to the PYTHONPATH, so we can reference those",0
libraries directly.,0
"Log requests, traces and exceptions to the Application Insights service",0
Use the AI4EAppInsights library to send log messages.,0
Use the internal-container AI for Earth Task Manager (not for production use!).,0
Use the AI4EWrapper to executes your functions within a logging trace.,0
"Also, helps support long-running/async functions.",0
Instantiate blob storage service to the internal container to put intermediate results and files,0
required params,0
"if use_url, then images_requested_json_sas is required",0
check model_version and request_name params are valid,0
check request_name has only allowed characters,0
TODO check that the expiry date of input_container_sas is at least a month into the future,1
TODO check images_requested_json_sas is a blob not a container,1
wrap_async_endpoint executes the function in a new thread and wraps it within a logging trace,0
request_name and request_submission_timestamp are for appending to output file names,0
"image_paths can be a list of strings (paths on Azure blobs or public URLs), or a list of lists,",0
"each of length 2 and is the [image_id, metadata] pair",0
case 1 - listing all images in the container,0
list all images to process,0
case 2 - user supplied a list of images to process; can include metadata,0
apply the first_n and sample_n filters,0
we sample by shuffling the image paths and take the first sample_n images,0
finalized image_paths is uploaded to internal_container; all sharding and scoring use the uploaded list,0
the list of images json does not have request_name or timestamp in the file name so that score.py can locate it,0
set up connection to AML Compute and data stores,0
do this for each request since pipeline step is associated with the data stores,0
start another thread to monitor the jobs and consolidate the results when they finish,0
time.sleep() blocks the current thread only,0
"check the status of the jobs, with retries",0
need to periodically check the enumerations are what AML returns - not the same as in their doc,0
"if all jobs finished, aggregate the results and return the URLs to the output files",0
"retrieve and join the output CSVs from each job, with retries",0
output_file_urls_str = json.dumps(output_file_urls),0
"not all jobs are finished, update the status with number of shards finished",0
"not all jobs are finished but the maximum number of checking cycle is reached, stop this thread",0
wrap_sync_endpoint wraps your function within a logging trace.,0
wrap_sync_endpoint wraps your function within a logging trace.,0
Number of decimal places to round to for confidence and bbox coordinates,0
PIL.Image.convert() returns a converted copy of this image,0
"change from [y1, x1, y2, x2] to [x1, y1, width_box, height_box]",0
convert numpy floats to Python floats,0
performs inference,0
number of images should be small - all are loaded at once and a copy of resized version exists at one point,0
2000 images are okay on a NC6s_v3,0
group the images into batches; image_batches is a list of lists,0
we keep track of the image_ids (and image_metas when available) to be able to output the list of failed images,0
start the TF session to process all images,0
get the operators,0
apply the confidence threshold,0
determine if there is metadata attached to each image_id,0
im_to_open will be a tempfile with a generated name,0
open is lazy; load() loads the image so we know it can be read successfully,0
self.image_ids does not include any failed images; self.image_ids is overwritten here,0
API's internal container where the list of image paths is stored,0
a json file containing a list of image paths this job should process,0
bool argument parsing is tricky - bool(any string) is True,0
get model from model registry,0
exclude the end_index; default is to process all images in this request,0
"items in this array can be strings or [image_id, metadata]",0
,0
prepare_api_submission.py,0
,0
"This module is somewhere between ""documentation"" and ""code"".  It is intended to",0
capture the steps the precede running a job via the AI for Earth Camera Trap,0
"Image Processing API, and it automates a couple of those steps.  We hope to",0
gradually automate all of these.,0
,0
Here's the stuff we usually do before submitting a job:,0
,0
"1) Upload data to Azure... we do this with azcopy, not addressed in this script",0
,0
2) List the files you want the API to process... this module supports that via,0
enumerate_blobs_to_file.,0
,0
3) Divide that list into chunks that will become individual API submissions...,0
this module supports that via divide_files_into_tasks.,0
"3) Put each .json file in a blob container, and generate a read-only SAS",0
URL for it.  Not automated right now.,0
,0
4) Generate the API query(ies) you'll submit to the API... this module supports that,0
via generate_api_queries.,0
,0
5) Submit the API query... I currently do this with Postman.,0
,0
6) Monitor task status,0
,0
7) Combine multiple API outputs,0
,0
"8) We're now into what we really call ""postprocessing"", rather than ""data_preparation"",",0
"but... possibly do some amount of partner-specific renaming, folder manipulation, etc.",0
"This is very partner-specific, but generally done via:",0
,0
find_repeat_detections.py,0
subset_json_detector_output.py.,0
postprocess_batch_results.py,0
,0
%% Imports and constants,0
assumes ai4eutils is on the path,0
%% File enumeration,0
print('Finished writing list {}'.format(output_file)),0
%% Dividing files into multiple tasks,0
https://www.geeksforgeeks.org/break-list-chunks-size-n-python/,0
i_chunk = 0; chunk = chunks[0],0
keep only whitelisted chars,0
i_url = 0; file_list_sas_url = file_list_sas_urls[0],0
%% Tools for working with API output,0
"I suspect this whole section will move to a separate file at some point,",0
so leaving these imports and constants here for now.,0
Download all three urls to temporary files,0
,0
"detections, failed_images, images",0
Load all three files,0
Diff submitted and processed images,0
Confirm that the failed images are a subset of the missing images,0
%% Interactive driver,0
%%,0
%%,0
%%,0
%%,0
,0
manage_api_submission.py,0
,0
Semi-automated process for submitting and managing camera trap,0
API jobs.,0
,0
%% Imports,0
%% Constants I set per job,0
These point to the same container; the read-only token is used for,0
accessing images; the write-enabled token is used for writing file lists,0
"Supported model_versions: '4', '3', '4_prelim'",0
,0
Also available at the /supported_model_versions and /default_model_version endpoints,0
,0
"additional_job_args = {""model_version"":""4_prelim""}",0
"%% Derived variables, path setup",0
Turn warnings into errors if more than this many images are missing,0
import clipboard; clipboard.copy(read_only_sas_url),0
configure mount point with rclone config,0
rclone mount mountname: z:,0
Not yet automated:,0
,0
Mounting the image source (see comment above),0
,0
"Submitting the jobs (code written below, but it doesn't really work)",0
,0
Handling failed jobs/shards/images (though most of the code exists in generate_resubmission_list),0
,0
Pushing the final results to shared storage and generating a SAS URL to share with the collaborator,0
,0
Pushing the previews to shared storage,0
%% Support functions,0
https://gist.github.com/zed/c2168b9c52b032b5fb7d,0
"scheme, netloc, path, query, fragment",0
%% Enumerate blobs to files,0
folder_name = folder_names[0],0
"If this is intended to be a folder, it needs to end in '/', otherwise files that start",0
with the same string will match too,0
%% Divide images into chunks for each folder,0
This will be a list of lists,0
list_file = list_files[0],0
%% Copy image lists to blob storage for each job,0
Maps  job name to a remote path,0
chunked_folder_files = folder_chunks[0]; chunk_file = chunked_folder_files[0],0
periods not allowed in job names,0
...for each task within this task group,0
...for each folder,0
%% Generate API calls for each job,0
job_name = list(job_name_to_list_url.keys())[0],0
%% Estimate total time,0
"%% Run the jobs (still in progress, doesn't actually work yet)",0
"Not working yet, something is wrong with my post call",0
import requests,0
task_group_request_strings = request_strings_by_task_group[0]; request_string = task_group_request_strings[0],0
"response = requests.post(submission_endpoint_url,json=request_string)",0
print(response.json()),0
"List of task IDs, grouped by logical job",0
%% Manually define task groups if we ran the jobs manually,0
%% Status check,0
"%% Look for failed shards or missing images, start new jobs if necessary",0
i_task_group = 0; task_group = task_groups[i_task_group]; task_id = task_group[0],0
assert n_failed_shards == 0,0
Each task group corresponds to one of our folders,0
...for each task,0
...for each task group,0
"%% Resubmit jobs for failed shards, add to appropriate task groups",0
%%,0
%% Pull results,0
i_task_group = 0; task_group = task_groups[i_task_group]; task_id = task_group[0],0
n_failed_shards = int(response['status']['message']['num_failed_shards']),0
assert n_failed_shards == 0,0
Each task group corresponds to one of our folders,0
...for each task,0
...for each task group,0
%% Combine results from task groups into final output files,0
i_folder = 0; folder_name = folder_names[i_folder],0
task_id = task_group[0],0
Check that we have (almost) all the images,0
Something has gone bonkers if there are images in the results that,0
aren't in the request,0
...for each folder,0
%% Post-processing (no ground truth),0
i_folder = 0; folder_name_raw = folder_names[i_folder],0
%% Manual processing follows,0
,0
"Everything after this should be considered mostly manual, and no longer includes",0
looping over folders.,0
,0
"%% Repeat detection elimination, phase 1",0
%% Manual RDE step,0
# DELETE THE ANIMALS ##,0
%% Re-filtering,0
%% Post-processing (post-RDE),0
i_folder = 0; folder_name_raw = folder_names[i_folder],0
api_output_file = folder_name_to_combined_output_file[folder_name],0
%% Subsetting,0
i_folder = 0; folder_name = folders[i_folder],0
img_file = BytesIO(urlopen.urlopen(url).read()),0
image = Image.open(img_file).convert('RGB'),0
image = mpimg.imread(url),0
Actual detection,0
calculate the size,0
img_file = BytesIO(urlopen.urlopen(inputFileName).read()),0
image = Image.open(img_file).convert('RGB'),0
Add the patch to the Axes,0
"plt.savefig(outputFileName, bbox_inches='tight', pad_inches=0.0, dpi=dpi, transparent=True)",0
model configuration,0
"if(path == ""/""):",0
"path = ""index""",0
#####,0
,0
run_tf_detector.py,0
,0
"Functions to load a TensorFlow detection model, run inference,",0
and render bounding boxes on images.,0
,0
"See the ""test driver"" cell for example invocation.",0
,0
#####,0
"%% Constants, imports, environment",0
%% Core detection functions,0
image = mpimg.imread(url),0
Actual detection,0
Read the image file,0
image = mpimg.imread(inputFileName),0
Display the image,0
"top, left, bottom, right",0
,0
"x,y origin is the upper-left",0
Location is the bottom-left of the rect,0
,0
Origin is the upper-left,0
Add the patch to the Axes,0
This is magic goop that removes whitespace around image plots (sort of),0
"plt.savefig(outputFileName, bbox_inches='tight', pad_inches=0.0, dpi=dpi, transparent=True)",0
%% Test driver,0
import os,0
MODEL_FILE = r'/Users/ranjanbalappa/backup/camera-trap/checkpoint/frozen_inference_graph.pb',0
TARGET_IMAGES = os.listdir('static/gallery'),0
TARGET_IMAGES = ['static/gallery/' + f for f in TARGET_IMAGES],0
# # Load and run detector on target images,0
detection_graph = load_model(MODEL_FILE),0
startTime = time.time(),0
"boxes,scores,classes,images = generate_detections(detection_graph,TARGET_IMAGES)",0
elapsed = time.time() - startTime,0
"print(""Done running detector on {} files in {}"".format(len(images),humanfriendly.format_timespan(elapsed)))",0
assert len(boxes) == len(TARGET_IMAGES),0
inputFileNames = TARGET_IMAGES,0
outputFileNames=[],0
confidenceThreshold=0.9,0
plt.ioff(),0
"render_bounding_boxes(boxes, scores, classes, TARGET_IMAGES)",0
print(TARGET_IMAGES[0]),0
output_img = {},0
for img_file in TARGET_IMAGES:,0
"box, score, clss = generate_image_detections(detection_graph, img_file)",0
"name, ext = os.path.splitext(img_file.split('/')[-1])",0
"num_objects, bboxes = draw_image_detections(box, score, clss, img_file, 'static/results/' + name )",0
output_img[img_file.split('/')[-1]] = {,0
"'num_objects': num_objects,",0
"'image_name': img_file.split('/')[-1],",0
"'result': 'Animal Detected' if num_objects > 0 else 'No Animal Detected',",0
'bboxes': bboxes,0
},0
import json,0
"with open('static/gallery_results/results.json', 'w') as res:",0
"json.dump(output_img, res)",0
from . import model,0
from . import aadConfig as aad,0
api_url = apiconfig.api['base_url'] + '/camera-trap/detect?confidence={1}&render={1}',0
routes for cameratrapassets as these are being loaded,0
from the cameratrapassets directory instead of the static directory,0
"def track_images(file, name):",0
print(str(e)),0
resize_images(images),0
"bbox points, confidence",0
print(img_result),0
redirect to home if no images to display,0
"gallery_images = random.sample(gallery_images, 12)",0
from . import aadConfig as aad,0
api url,0
Dropzone settings,0
app.config['AUTHORITY_URL'] =  aad.AUTHORITY_HOST_URL + '/' + aad.TENANT,0
app.config['DROPZONE_IN_FORM'] = True,0
app.config['DROPZONE_UPLOAD_ON_CLICK'] = True,0
app.config['DROPZONE_UPLOAD_BTN_ID'] =  'submit',0
app.config[' DROPZONE_UPLOAD_ACTION'] = 'processimages',0
Uploads settings,0
model configuration,0
# sourceMappingURL=popper.min.js.map,0
Noty.overrideDefaults({,0
"layout   : 'topRight',",0
"theme    : 'mint',",0
"closeWith: ['click', 'button'],",0
"timeout: 1500,",0
animation: {,0
"open : 'animated fadeInRight',",0
close: 'animated fadeOutRight',0
},0
});,0
Initialize,0
var bLazy = new Blazy({,0
container: '.scroll-class',0
});,0
timeout: 2500,0
Defining this global in .eslintrc.json would create a danger of using the global,0
"unguarded in another place, it seems safer to define global only for this module",0
Define a local copy of jQuery,0
The jQuery object is actually just the init constructor 'enhanced',0
Need init if jQuery is called (just allow error to be thrown if not included),0
Support: Android <=4.0 only,0
Make sure we trim BOM and NBSP,0
The current version of jQuery being used,0
The default length of a jQuery object is 0,0
Get the Nth element in the matched element set OR,0
Get the whole matched element set as a clean array,0
Return all the elements in a clean array,0
Return just the one element from the set,0
Take an array of elements and push it onto the stack,0
(returning the new matched element set),0
Build a new jQuery matched element set,0
Add the old object onto the stack (as a reference),0
Return the newly-formed element set,0
Execute a callback for every element in the matched set.,0
For internal use only.,0
"Behaves like an Array's method, not like a jQuery method.",0
Handle a deep copy situation,0
Skip the boolean and the target,0
Handle case when target is a string or something (possible in deep copy),0
Extend jQuery itself if only one argument is passed,0
Only deal with non-null/undefined values,0
Extend the base object,0
Prevent Object.prototype pollution,0
Prevent never-ending loop,0
Recurse if we're merging plain objects or arrays,0
Ensure proper type for the source value,0
"Never move original objects, clone them",0
Don't bring in undefined values,0
Return the modified object,0
Unique for each copy of jQuery on the page,0
Assume jQuery is ready without the ready module,0
Detect obvious negatives,0
Use toString instead of jQuery.type to catch host objects,0
"Objects with no prototype (e.g., `Object.create( null )`) are plain",0
Objects with prototype are plain iff they were constructed by a global Object function,0
Evaluates a script in a global context,0
Support: Android <=4.0 only,0
results is for internal usage only,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
"push.apply(_, arraylike) throws on ancient WebKit",0
"Go through the array, only saving the items",0
that pass the validator function,0
arg is for internal usage only,0
"Go through the array, translating each of the items to their new values",0
"Go through every key on the object,",0
Flatten any nested arrays,0
A global GUID counter for objects,0
jQuery.support is not used in Core but other projects attach their,0
properties to it so it needs to exist.,0
Populate the class2type map,0
Support: real iOS 8.2 only (not reproducible in simulator),0
`in` check used to prevent JIT error (gh-2145),0
hasOwn isn't used here due to false negatives,0
regarding Nodelist length in IE,0
For CommonJS and CommonJS-like environments where a proper `window`,0
"is present, execute the factory and get jQuery.",0
For environments that do not have a `window` with a `document`,0
"(such as Node.js), expose a factory as module.exports.",0
This accentuates the need for the creation of a real `window`.,0
"e.g. var jQuery = require(""jquery"")(window);",0
See ticket #14549 for more info.,0
Pass this if window is not defined yet,0
"Edge <= 12 - 13+, Firefox <=18 - 45+, IE 10 - 11, Safari 5.1 - 9+, iOS 6 - 9.1",0
"throw exceptions when non-strict code (e.g., ASP.NET 4.5) accesses strict mode",0
"arguments.callee.caller (trac-13335). But as of jQuery 3.0 (2016), strict mode should be common",0
enough that all such attempts are guarded in a try block.,0
"Support: Chrome <=57, Firefox <=52",0
"In some browsers, typeof returns ""function"" for HTML <object> elements",0
"(i.e., `typeof document.createElement( ""object"" ) === ""function""`).",0
We don't want to classify *any* DOM node as a function.,0
"Support: Firefox 64+, Edge 18+",0
"Some browsers don't support the ""nonce"" property on scripts.",0
"On the other hand, just using `getAttribute` is not enough as",0
the `nonce` attribute is reset to an empty string whenever it,0
becomes browsing-context connected.,0
See https://github.com/whatwg/html/issues/2369,0
See https://html.spec.whatwg.org/#nonce-attributes,0
The `node.getAttribute` check was added for the sake of,0
`jQuery.globalEval` so that it can fake a nonce-containing node,0
via an object.,0
Support: Android <=2.3 only (functionish RegExp),0
Defining this global in .eslintrc.json would create a danger of using the global,0
"unguarded in another place, it seems safer to define global only for this module",0
Define a local copy of jQuery,0
The jQuery object is actually just the init constructor 'enhanced',0
Need init if jQuery is called (just allow error to be thrown if not included),0
Support: Android <=4.0 only,0
Make sure we trim BOM and NBSP,0
The current version of jQuery being used,0
The default length of a jQuery object is 0,0
Get the Nth element in the matched element set OR,0
Get the whole matched element set as a clean array,0
Return all the elements in a clean array,0
Return just the one element from the set,0
Take an array of elements and push it onto the stack,0
(returning the new matched element set),0
Build a new jQuery matched element set,0
Add the old object onto the stack (as a reference),0
Return the newly-formed element set,0
Execute a callback for every element in the matched set.,0
For internal use only.,0
"Behaves like an Array's method, not like a jQuery method.",0
Handle a deep copy situation,0
Skip the boolean and the target,0
Handle case when target is a string or something (possible in deep copy),0
Extend jQuery itself if only one argument is passed,0
Only deal with non-null/undefined values,0
Extend the base object,0
Prevent Object.prototype pollution,0
Prevent never-ending loop,0
Recurse if we're merging plain objects or arrays,0
Ensure proper type for the source value,0
"Never move original objects, clone them",0
Don't bring in undefined values,0
Return the modified object,0
Unique for each copy of jQuery on the page,0
Assume jQuery is ready without the ready module,0
Detect obvious negatives,0
Use toString instead of jQuery.type to catch host objects,0
"Objects with no prototype (e.g., `Object.create( null )`) are plain",0
Objects with prototype are plain iff they were constructed by a global Object function,0
Evaluates a script in a global context,0
Support: Android <=4.0 only,0
results is for internal usage only,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
"push.apply(_, arraylike) throws on ancient WebKit",0
"Go through the array, only saving the items",0
that pass the validator function,0
arg is for internal usage only,0
"Go through the array, translating each of the items to their new values",0
"Go through every key on the object,",0
Flatten any nested arrays,0
A global GUID counter for objects,0
jQuery.support is not used in Core but other projects attach their,0
properties to it so it needs to exist.,0
Populate the class2type map,0
Support: real iOS 8.2 only (not reproducible in simulator),0
`in` check used to prevent JIT error (gh-2145),0
hasOwn isn't used here due to false negatives,0
regarding Nodelist length in IE,0
Local document vars,0
Instance-specific data,0
Instance methods,0
Use a stripped-down indexOf as it's faster than native,0
https://jsperf.com/thor-indexof-vs-for/5,0
Regular expressions,0
http://www.w3.org/TR/css3-selectors/#whitespace,0
http://www.w3.org/TR/CSS21/syndata.html#value-def-identifier,0
Attribute selectors: http://www.w3.org/TR/selectors/#attribute-selectors,0
Operator (capture 2),0
"""Attribute values must be CSS identifiers [capture 5] or strings [capture 3 or capture 4]""",0
"To reduce the number of selectors needing tokenize in the preFilter, prefer arguments:",0
1. quoted (capture 3; capture 4 or capture 5),0
2. simple (capture 6),0
3. anything else (capture 2),0
"Leading and non-escaped trailing whitespace, capturing some non-whitespace characters preceding the latter",0
For use in libraries implementing .is(),0
We use this for POS matching in `select`,0
Easily-parseable/retrievable ID or TAG or CLASS selectors,0
CSS escapes,0
http://www.w3.org/TR/CSS21/syndata.html#escaped-characters,0
NaN means non-codepoint,0
Support: Firefox<24,0
"Workaround erroneous numeric interpretation of +""0x""",1
BMP codepoint,0
Supplemental Plane codepoint (surrogate pair),0
CSS string/identifier serialization,0
https://drafts.csswg.org/cssom/#common-serializing-idioms,0
U+0000 NULL becomes U+FFFD REPLACEMENT CHARACTER,0
Control characters and (dependent upon position) numbers get escaped as code points,0
Other potentially-special ASCII characters get backslash-escaped,0
Used for iframes,0
See setDocument(),0
"Removing the function wrapper causes a ""Permission Denied""",0
error in IE,0
"Optimize for push.apply( _, NodeList )",0
Support: Android<4.0,0
Detect silently failing push.apply,0
Leverage slice if possible,0
Support: IE<9,0
Otherwise append directly,0
Can't trust NodeList.length,0
"nodeType defaults to 9, since context defaults to document",0
Return early from calls with invalid selector or context,0
Try to shortcut find operations (as opposed to filters) in HTML documents,0
"If the selector is sufficiently simple, try using a ""get*By*"" DOM method",0
"(excepting DocumentFragment context, where the methods don't exist)",0
ID selector,0
Document context,0
"Support: IE, Opera, Webkit",0
TODO: identify versions,1
getElementById can match elements by name instead of ID,0
Element context,0
"Support: IE, Opera, Webkit",0
TODO: identify versions,1
getElementById can match elements by name instead of ID,0
Type selector,0
Class selector,0
Take advantage of querySelectorAll,0
Support: IE 8 only,0
Exclude object elements,0
qSA considers elements outside a scoping root when evaluating child or,0
"descendant combinators, which is not what we want.",0
"In such cases, we work around the behavior by prefixing every selector in the",0
list with an ID selector referencing the scope context.,0
Thanks to Andrew Dupont for this technique.,0
"Capture the context ID, setting it first if necessary",0
Prefix every selector in the list,0
Expand context for sibling selectors,0
All others,0
"Use (key + "" "") to avoid collision with native prototype properties (see Issue #157)",0
Only keep the most recent entries,0
Remove from its parent by default,0
release memory in IE,0
Use IE sourceIndex if available on both nodes,0
Check if b follows a,0
Known :disabled false positives: fieldset[disabled] > legend:nth-of-type(n+2) :can-disable,0
Only certain elements can match :enabled or :disabled,0
https://html.spec.whatwg.org/multipage/scripting.html#selector-enabled,0
https://html.spec.whatwg.org/multipage/scripting.html#selector-disabled,0
Check for inherited disabledness on relevant non-disabled elements:,0
* listed form-associated elements in a disabled fieldset,0
https://html.spec.whatwg.org/multipage/forms.html#category-listed,0
https://html.spec.whatwg.org/multipage/forms.html#concept-fe-disabled,0
* option elements in a disabled optgroup,0
https://html.spec.whatwg.org/multipage/forms.html#concept-option-disabled,0
"All such elements have a ""form"" property.",0
Option elements defer to a parent optgroup if present,0
Support: IE 6 - 11,0
Use the isDisabled shortcut property to check for disabled fieldset ancestors,0
"Where there is no isDisabled, check manually",0
Try to winnow out elements that can't be disabled before trusting the disabled property.,0
"Some victims get caught in our net (label, legend, menu, track), but it shouldn't",0
"even exist on them, let alone have a boolean value.",0
Remaining elements are neither :enabled nor :disabled,0
Match elements found at the specified indexes,0
Expose support vars for convenience,0
Support: IE <=8,0
"Assume HTML when documentElement doesn't yet exist, such as inside loading iframes",0
https://bugs.jquery.com/ticket/4833,0
Return early if doc is invalid or already selected,0
Update global variables,0
"Support: IE 9-11, Edge",0
"Accessing iframe documents after unload throws ""permission denied"" errors (jQuery #13936)",0
"Support: IE 11, Edge",0
Support: IE 9 - 10 only,0
Support: IE<8,0
Verify that getAttribute really returns attributes and not properties,0
(excepting IE8 booleans),0
"Check if getElementsByTagName(""*"") returns only elements",0
Support: IE<9,0
Support: IE<10,0
Check if getElementById returns elements by name,0
"The broken getElementById methods don't pick up programmatically-set names,",1
so use a roundabout getElementsByName test,0
ID filter and find,0
Support: IE 6 - 7 only,0
getElementById is not reliable as a find shortcut,0
Verify the id attribute,0
Fall back on getElementsByName,0
Tag,0
DocumentFragment nodes don't have gEBTN,0
"By happy coincidence, a (broken) gEBTN appears on DocumentFragment nodes too",0
Filter out possible comments,0
Class,0
QSA and matchesSelector support,0
matchesSelector(:active) reports false when true (IE9/Opera 11.5),0
qSa(:focus) reports false when true (Chrome 21),0
We allow this because of a bug in IE8/9 that throws an error,0
whenever `document.activeElement` is accessed on an iframe,0
"So, we allow :focus to pass through QSA all the time to avoid the IE error",0
See https://bugs.jquery.com/ticket/13378,0
Build QSA regex,0
Regex strategy adopted from Diego Perini,0
Select is set to empty string on purpose,0
This is to test IE's treatment of not explicitly,0
"setting a boolean content attribute,",0
since its presence should be enough,0
https://bugs.jquery.com/ticket/12359,0
"Support: IE8, Opera 11-12.16",0
Nothing should be selected when empty strings follow ^= or $= or *=,0
"The test attribute must be unknown in Opera but ""safe"" for WinRT",0
https://msdn.microsoft.com/en-us/library/ie/hh465388.aspx#attribute_section,0
Support: IE8,0
"Boolean attributes and ""value"" are not treated correctly",0
"Support: Chrome<29, Android<4.4, Safari<7.0+, iOS<7.0+, PhantomJS<1.9.8+",0
Webkit/Opera - :checked should return selected option elements,0
http://www.w3.org/TR/2011/REC-css3-selectors-20110929/#checked,0
IE8 throws error here and will not see later tests,0
"Support: Safari 8+, iOS 8+",0
https://bugs.webkit.org/show_bug.cgi?id=136851,0
In-page `selector#id sibling-combinator selector` fails,0
Support: Windows 8 Native Apps,0
The type and name attributes are restricted during .innerHTML assignment,0
Support: IE8,0
Enforce case-sensitivity of name attribute,0
FF 3.5 - :enabled/:disabled and hidden elements (hidden elements are still enabled),0
IE8 throws error here and will not see later tests,0
Support: IE9-11+,0
IE's :disabled selector does not pick up the children of disabled fieldsets,0
Opera 10-11 does not throw on post-comma invalid pseudos,0
Check to see if it's possible to do matchesSelector,0
on a disconnected node (IE 9),0
This should fail with an exception,0
"Gecko does not error, returns false instead",0
Element contains another,0
Purposefully self-exclusive,0
"As in, an element does not contain itself",0
Document order sorting,0
Flag for duplicate removal,0
Sort on method existence if only one input has compareDocumentPosition,0
Calculate position if both inputs belong to the same document,0
Otherwise we know they are disconnected,0
Disconnected nodes,0
Choose the first element that is related to our preferred document,0
Maintain original order,0
Exit early if the nodes are identical,0
Parentless nodes are either documents or disconnected,0
"If the nodes are siblings, we can do a quick check",0
Otherwise we need full lists of their ancestors for comparison,0
Walk down the tree looking for a discrepancy,0
Do a sibling check if the nodes have a common ancestor,0
Otherwise nodes in our document sort first,0
Set document vars if needed,0
IE 9's matchesSelector returns false on disconnected nodes,0
"As well, disconnected nodes are said to be in a document",0
fragment in IE 9,0
Set document vars if needed,0
Set document vars if needed,0
Don't get fooled by Object.prototype properties (jQuery #13807),0
"Unless we *know* we can detect duplicates, assume their presence",0
Clear input after sorting to release objects,0
See https://github.com/jquery/sizzle/pull/225,0
"If no nodeType, this is expected to be an array",0
Do not traverse comment nodes,0
Use textContent for elements,0
innerText usage removed for consistency of new lines (jQuery #11153),0
Traverse its children,0
Do not include comment or processing instruction nodes,0
Can be adjusted by the user,0
Move the given value to match[3] whether quoted or unquoted,0
nth-* requires argument,0
numeric x and y parameters for Expr.filter.CHILD,0
remember that false/true cast respectively to 0/1,0
other types prohibit arguments,0
Accept quoted arguments as-is,0
Strip excess characters from unquoted arguments,0
Get excess from tokenize (recursively),0
advance to the next closing parenthesis,0
excess is a negative index,0
Return only captures needed by the pseudo filter method (type and argument),0
Shortcut for :nth-*(n),0
:(first|last|only)-(child|of-type),0
Reverse direction for :only-* (if we haven't yet done so),0
non-xml :nth-child(...) stores cache data on `parent`,0
Seek `elem` from a previously-cached index,0
...in a gzip-friendly way,0
Support: IE <9 only,0
Defend against cloned attroperties (jQuery gh-1709),0
Fallback to seeking `elem` from the start,0
"When found, cache indexes on `parent` and break",0
Use previously-cached element index if available,0
...in a gzip-friendly way,0
Support: IE <9 only,0
Defend against cloned attroperties (jQuery gh-1709),0
xml :nth-child(...),0
or :nth-last-child(...) or :nth(-last)?-of-type(...),0
Use the same loop as above to seek `elem` from the start,0
Cache the index of each encountered element,0
Support: IE <9 only,0
Defend against cloned attroperties (jQuery gh-1709),0
"Incorporate the offset, then check against cycle size",0
pseudo-class names are case-insensitive,0
http://www.w3.org/TR/selectors/#pseudo-classes,0
Prioritize by case sensitivity in case custom pseudos are added with uppercase letters,0
Remember that setFilters inherits from pseudos,0
The user may use createPseudo to indicate that,0
arguments are needed to create the filter function,0
just as Sizzle does,0
But maintain support for old signatures,0
Potentially complex pseudos,0
Trim the selector passed to compile,0
to avoid treating leading and trailing,0
spaces as combinators,0
Match elements unmatched by `matcher`,0
Don't keep the element (issue #299),0
"""Whether an element is represented by a :lang() selector",0
is based solely on the element's language value,0
"being equal to the identifier C,",0
"or beginning with the identifier C immediately followed by ""-"".",0
The matching of C against the element's language value is performed case-insensitively.,0
"The identifier C does not have to be a valid language name.""",0
http://www.w3.org/TR/selectors/#lang-pseudo,0
lang value must be a valid identifier,0
Miscellaneous,0
Boolean properties,0
"In CSS3, :checked should return both checked and selected elements",0
http://www.w3.org/TR/2011/REC-css3-selectors-20110929/#checked,0
Accessing this property makes selected-by-default,0
options in Safari work properly,0
Contents,0
http://www.w3.org/TR/selectors/#empty-pseudo,0
":empty is negated by element (1) or content nodes (text: 3; cdata: 4; entity ref: 5),",0
but not by others (comment: 8; processing instruction: 7; etc.),0
nodeType < 6 works because attributes (2) do not appear as children,0
Element/input types,0
Support: IE<8,0
"New HTML5 attribute values (e.g., ""search"") appear with elem.type === ""text""",0
Position-in-collection,0
Add button/input type pseudos,0
Easy API for creating new setFilters,0
Comma and first run,0
Don't consume trailing commas as valid,0
Combinators,0
Cast descendant combinators to space,0
Filters,0
Return the length of the invalid excess,0
if we're just parsing,0
"Otherwise, throw an error or return tokens",0
Cache the tokens,0
Check against closest ancestor/preceding element,0
Check against all ancestor/preceding elements,0
"We can't set arbitrary data on XML nodes, so they don't benefit from combinator caching",0
Support: IE <9 only,0
Defend against cloned attroperties (jQuery gh-1709),0
Assign to newCache so results back-propagate to previous elements,0
Reuse newcache so results back-propagate to previous elements,0
A match means we're done; a fail means we have to keep checking,0
Get initial elements from seed or context,0
"Prefilter to get matcher input, preserving a map for seed-results synchronization",0
"If we have a postFinder, or filtered seed, or non-seed postFilter or preexisting results,",0
...intermediate processing is necessary,0
...otherwise use results directly,0
Find primary matches,0
Apply postFilter,0
Un-match failing elements by moving them back to matcherIn,0
Get the final matcherOut by condensing this intermediate into postFinder contexts,0
Restore matcherIn since elem is not yet a final match,0
Move matched elements from seed to results to keep them synchronized,0
"Add elements to results, through postFinder if defined",0
The foundational matcher ensures that elements are reachable from top-level context(s),0
Avoid hanging onto element (issue #299),0
Return special upon seeing a positional matcher,0
Find the next relative operator (if any) for proper handling,0
"If the preceding token was a descendant combinator, insert an implicit any-element `*`",0
We must always have either seed elements or outermost context,0
Use integer dirruns iff this is the outermost matcher,0
Add elements passing elementMatchers directly to results,0
"Support: IE<9, Safari",0
"Tolerate NodeList properties (IE: ""length""; Safari: <number>) matching elements by id",0
Track unmatched elements for set filters,0
They will have gone through all possible matchers,0
"Lengthen the array for every element, matched or not",0
"`i` is now the count of elements visited above, and adding it to `matchedCount`",0
makes the latter nonnegative.,0
Apply set filters to unmatched elements,0
"NOTE: This can be skipped if there are no unmatched elements (i.e., `matchedCount`",0
"equals `i`), unless we didn't visit _any_ elements in the above loop because we have",0
no element matchers and no seed.,0
"Incrementing an initially-string ""0"" `i` allows `i` to remain a string only in that",0
"case, which will result in a ""00"" `matchedCount` that differs from `i` but is also",0
numerically zero.,0
Reintegrate element matches to eliminate the need for sorting,0
Discard index placeholder values to get only actual matches,0
Add matches to results,0
Seedless set matches succeeding multiple successful matchers stipulate sorting,0
Override manipulation of globals by nested matchers,0
Generate a function of recursive functions that can be used to check each element,0
Cache the compiled function,0
Save selector and tokenization,0
Try to minimize operations if there is only one selector in the list and no seed,0
(the latter of which guarantees us context),0
Reduce context if the leading compound selector is an ID,0
"Precompiled matchers will still verify ancestry, so step up a level",0
Fetch a seed set for right-to-left matching,0
Abort if we hit a combinator,0
"Search, expanding context for leading sibling combinators",0
"If seed is empty or no tokens remain, we can return early",0
Compile and execute a filtering function if one is not provided,0
Provide `match` to avoid retokenization if we modified the selector above,0
One-time assignments,0
Sort stability,0
Support: Chrome 14-35+,0
Always assume duplicates if they aren't passed to the comparison function,0
Initialize against the default document,0
Support: Webkit<537.32 - Safari 6.0.3/Chrome 25 (fixed in Chrome 27),0
Detached nodes confoundingly follow *each other*,0
"Should return 1, but returns 4 (following)",0
Support: IE<8,0
"Prevent attribute/property ""interpolation""",0
https://msdn.microsoft.com/en-us/library/ms536429%28VS.85%29.aspx,0
Support: IE<9,0
"Use defaultValue in place of getAttribute(""value"")",0
Support: IE<9,0
Use getAttributeNode to fetch booleans when getAttribute lies,0
Deprecated,0
Implement the identical functionality for filter and not,0
Single element,0
"Arraylike of elements (jQuery, arguments, Array)",0
Filtered directly for both simple and complex selectors,0
"If this is a positional/relative selector, check membership in the returned set",0
"so $(""p:first"").is(""p:last"") won't return true for a doc with two ""p"".",0
Initialize a jQuery object,0
A central reference to the root jQuery(document),0
A simple way to check for HTML strings,0
Prioritize #id over <tag> to avoid XSS via location.hash (#9521),0
Strict HTML recognition (#11290: must start with <),0
Shortcut simple #id case for speed,0
"HANDLE: $(""""), $(null), $(undefined), $(false)",0
Method init() accepts an alternate rootjQuery,0
so migrate can support jQuery.sub (gh-2101),0
Handle HTML strings,0
Assume that strings that start and end with <> are HTML and skip the regex check,0
Match html or make sure no context is specified for #id,0
HANDLE: $(html) -> $(array),0
Option to run scripts is true for back-compat,0
Intentionally let the error be thrown if parseHTML is not present,0
"HANDLE: $(html, props)",0
Properties of context are called as methods if possible,0
...and otherwise set as attributes,0
HANDLE: $(#id),0
Inject the element directly into the jQuery object,0
"HANDLE: $(expr, $(...))",0
"HANDLE: $(expr, context)",0
(which is just equivalent to: $(context).find(expr),0
HANDLE: $(DOMElement),0
HANDLE: $(function),0
Shortcut for document ready,0
Execute immediately if ready is not present,0
Give the init function the jQuery prototype for later instantiation,0
Initialize central reference,0
Methods guaranteed to produce a unique set when starting from a unique set,0
"Positional selectors never match, since there's no _selection_ context",0
Always skip document fragments,0
Don't pass non-elements to Sizzle,0
Determine the position of an element within the set,0
"No argument, return index in parent",0
Index in selector,0
Locate the position of the desired element,0
"If it receives a jQuery object, the first element is used",0
"Support: IE 9 - 11 only, iOS 7 only, Android Browser <=4.3 only",0
Treat the template element as a regular one in browsers that,0
don't support it.,0
Remove duplicates,0
Reverse order for parents* and prev-derivatives,0
Convert String-formatted options into Object-formatted ones,0
Convert options from String-formatted to Object-formatted if needed,0
(we check in cache first),0
Last fire value for non-forgettable lists,0
Flag to know if list was already fired,0
Flag to prevent firing,0
Actual callback list,0
Queue of execution data for repeatable lists,0
Index of currently firing callback (modified by add/remove as needed),0
Fire callbacks,0
Enforce single-firing,0
"Execute callbacks for all pending executions,",0
respecting firingIndex overrides and runtime changes,0
Run callback and check for early termination,0
Jump to end and forget the data so .add doesn't re-fire,0
Forget the data if we're done with it,0
Clean up if we're done firing for good,0
Keep an empty list if we have data for future add calls,0
"Otherwise, this object is spent",0
Actual Callbacks object,0
Add a callback or a collection of callbacks to the list,0
"If we have memory from a past run, we should fire after adding",0
Inspect recursively,0
Remove a callback from the list,0
Handle firing indexes,0
Check if a given callback is in the list.,0
"If no argument is given, return whether or not list has callbacks attached.",0
Remove all callbacks from the list,0
Disable .fire and .add,0
Abort any current/pending executions,0
Clear all callbacks and values,0
Disable .fire,0
Also disable .add unless we have memory (since it would have no effect),0
Abort any pending executions,0
Call all callbacks with the given context and arguments,0
Call all the callbacks with the given arguments,0
To know if the callbacks have already been called at least once,0
Check for promise aspect first to privilege synchronous behavior,0
Other thenables,0
Other non-thenables,0
Control `resolve` arguments by letting Array#slice cast boolean `noValue` to integer:,0
* false: [ value ].slice( 0 ) => resolve( value ),0
* true: [ value ].slice( 1 ) => resolve(),0
"For Promises/A+, convert exceptions into rejections",0
"Since jQuery.when doesn't unwrap thenables, we can skip the extra checks appearing in",0
Deferred#then to conditionally suppress rejection.,0
Support: Android 4.0 only,0
Strict mode functions invoked without .call/.apply get global-object context,0
"action, add listener, callbacks,",0
"... .then handlers, argument index, [final state]",0
Keep pipe for back-compat,0
"Map tuples (progress, done, fail) to arguments (done, fail, progress)",0
deferred.progress(function() { bind to newDefer or newDefer.notify }),0
deferred.done(function() { bind to newDefer or newDefer.resolve }),0
deferred.fail(function() { bind to newDefer or newDefer.reject }),0
Support: Promises/A+ section 2.3.3.3.3,0
https://promisesaplus.com/#point-59,0
Ignore double-resolution attempts,0
Support: Promises/A+ section 2.3.1,0
https://promisesaplus.com/#point-48,0
"Support: Promises/A+ sections 2.3.3.1, 3.5",0
https://promisesaplus.com/#point-54,0
https://promisesaplus.com/#point-75,0
Retrieve `then` only once,0
Support: Promises/A+ section 2.3.4,0
https://promisesaplus.com/#point-64,0
Only check objects and functions for thenability,0
Handle a returned thenable,0
Special processors (notify) just wait for resolution,0
Normal processors (resolve) also hook into progress,0
...and disregard older resolution values,0
Handle all other returned values,0
Only substitute handlers pass on context,0
and multiple values (non-spec behavior),0
Process the value(s),0
Default process is resolve,0
Only normal processors (resolve) catch and reject exceptions,0
Support: Promises/A+ section 2.3.3.3.4.1,0
https://promisesaplus.com/#point-61,0
Ignore post-resolution exceptions,0
Only substitute handlers pass on context,0
and multiple values (non-spec behavior),0
Support: Promises/A+ section 2.3.3.3.1,0
https://promisesaplus.com/#point-57,0
Re-resolve promises immediately to dodge false rejection from,0
subsequent errors,0
"Call an optional hook to record the stack, in case of exception",0
since it's otherwise lost when execution goes async,0
progress_handlers.add( ... ),0
fulfilled_handlers.add( ... ),0
rejected_handlers.add( ... ),0
Get a promise for this deferred,0
"If obj is provided, the promise aspect is added to the object",0
Add list-specific methods,0
promise.progress = list.add,0
promise.done = list.add,0
promise.fail = list.add,0
Handle state,0
"state = ""resolved"" (i.e., fulfilled)",0
"state = ""rejected""",0
rejected_callbacks.disable,0
fulfilled_callbacks.disable,0
rejected_handlers.disable,0
fulfilled_handlers.disable,0
progress_callbacks.lock,0
progress_handlers.lock,0
progress_handlers.fire,0
fulfilled_handlers.fire,0
rejected_handlers.fire,0
deferred.notify = function() { deferred.notifyWith(...) },0
deferred.resolve = function() { deferred.resolveWith(...) },0
deferred.reject = function() { deferred.rejectWith(...) },0
deferred.notifyWith = list.fireWith,0
deferred.resolveWith = list.fireWith,0
deferred.rejectWith = list.fireWith,0
Make the deferred a promise,0
Call given func if any,0
All done!,0
Deferred helper,0
count of uncompleted subordinates,0
count of unprocessed arguments,0
subordinate fulfillment data,0
the master Deferred,0
subordinate callback factory,0
Single- and empty arguments are adopted like Promise.resolve,0
Use .then() to unwrap secondary thenables (cf. gh-3000),0
Multiple arguments are aggregated like Promise.all array elements,0
"These usually indicate a programmer mistake during development,",0
warn about them ASAP rather than swallowing them by default.,0
Support: IE 8 - 9 only,0
"Console exists when dev tools are open, which can happen at any time",0
The deferred used on DOM ready,0
Wrap jQuery.readyException in a function so that the lookup,0
happens at the time of error handling instead of callback,0
registration.,0
Is the DOM ready to be used? Set to true once it occurs.,0
A counter to track how many items to wait for before,0
the ready event fires. See #6781,0
Handle when the DOM is ready,0
Abort if there are pending holds or we're already ready,0
Remember that the DOM is ready,0
"If a normal DOM Ready event fired, decrement, and wait if need be",0
"If there are functions bound, to execute",0
The ready event handler and self cleanup method,0
Catch cases where $(document).ready() is called,0
after the browser event has already occurred.,0
Support: IE <=9 - 10 only,0
"Older IE sometimes signals ""interactive"" too soon",0
Handle it asynchronously to allow scripts the opportunity to delay ready,0
Use the handy event callback,0
"A fallback to window.onload, that will always work",0
Multifunctional method to get and set values of a collection,0
The value/s can optionally be executed if it's a function,0
Sets many values,0
Sets one value,0
Bulk operations run against the entire set,0
...except when executing function values,0
Gets,0
Matches dashed string for camelizing,0
Used by camelCase as callback to replace(),0
Convert dashed to camelCase; used by the css and data modules,0
"Support: IE <=9 - 11, Edge 12 - 15",0
Microsoft forgot to hump their vendor prefix (#9572),0
Accepts only:,0
- Node,0
- Node.ELEMENT_NODE,0
- Node.DOCUMENT_NODE,0
- Object,0
- Any,0
Check if the owner object already has a cache,0
"If not, create one",0
"We can accept data for non-element nodes in modern browsers,",0
"but we should not, see #8335.",0
Always return an empty object.,0
If it is a node unlikely to be stringify-ed or looped over,0
use plain assignment,0
Otherwise secure it in a non-enumerable property,0
configurable must be true to allow the property to be,0
deleted when data is removed,0
"Handle: [ owner, key, value ] args",0
Always use camelCase key (gh-2257),0
"Handle: [ owner, { properties } ] args",0
Copy the properties one-by-one to the cache object,0
Always use camelCase key (gh-2257),0
In cases where either:,0
,0
1. No key was specified,0
"2. A string key was specified, but no value provided",0
,0
"Take the ""read"" path and allow the get method to determine",0
"which value to return, respectively either:",0
,0
1. The entire cache object,0
2. The data stored at the key,0
,0
"When the key is not a string, or both a key and value",0
"are specified, set or extend (existing objects) with either:",0
,0
1. An object of properties,0
2. A key and value,0
,0
"Since the ""set"" path can have two possible entry points",0
return the expected data based on which path was taken[*],0
Support array or space separated string of keys,0
If key is an array of keys...,0
"We always set camelCase keys, so remove that.",0
"If a key with the spaces exists, use it.",0
"Otherwise, create an array by matching non-whitespace",0
Remove the expando if there's no more data,0
Support: Chrome <=35 - 45,0
Webkit & Blink performance suffers when deleting properties,0
"from DOM nodes, so set to undefined instead",0
https://bugs.chromium.org/p/chromium/issues/detail?id=378607 (bug restricted),0
Implementation Summary,0
,0
1. Enforce API surface and semantic compatibility with 1.9.x branch,0
2. Improve the module's maintainability by reducing the storage,0
paths to a single mechanism.,0
"3. Use the same single mechanism to support ""private"" and ""user"" data.",0
"4. _Never_ expose ""private"" data to user code (TODO: Drop _data, _removeData)",1
5. Avoid exposing implementation details on user objects (eg. expando properties),0
6. Provide a clear path for implementation upgrade to WeakMap in 2014,0
Only convert to a number if it doesn't change the string,0
"If nothing was found internally, try to fetch any",0
data from the HTML5 data-* attribute,0
Make sure we set the data so it isn't changed later,0
TODO: Now that all calls to _data and _removeData have been replaced,1
"with direct calls to dataPriv methods, these can be deprecated.",0
Gets all values,0
Support: IE 11 only,0
The attrs elements can be null (#14894),0
Sets multiple values,0
The calling jQuery object (element matches) is not empty,0
(and therefore has an element appears at this[ 0 ]) and the,0
`value` parameter was not undefined. An empty jQuery object,0
will result in `undefined` for elem = this[ 0 ] which will,0
throw an exception if an attempt to read a data cache is made.,0
Attempt to get data from the cache,0
The key will always be camelCased in Data,0
"Attempt to ""discover"" the data in",0
HTML5 custom data-* attrs,0
"We tried really hard, but the data doesn't exist.",0
Set the data...,0
We always store the camelCased key,0
Speed up dequeue by getting out quickly if this is just a lookup,0
"If the fx queue is dequeued, always remove the progress sentinel",0
Add a progress sentinel to prevent the fx queue from being,0
automatically dequeued,0
Clear up the last queue stop function,0
"Not public - generate a queueHooks object, or return the current one",0
Ensure a hooks for this queue,0
Get a promise resolved when queues of a certain type,0
are emptied (fx is the type by default),0
Check attachment across shadow DOM boundaries when possible (gh-3504),0
isHiddenWithinTree might be called from jQuery#filter function;,0
"in that case, element will be second argument",0
Inline style trumps all,0
"Otherwise, check computed style",0
Support: Firefox <=43 - 45,0
"Disconnected elements can have computed display: none, so first confirm that elem is",0
in the document.,0
"Remember the old values, and insert the new ones",0
Revert the old values,0
Starting value computation is required for potential unit mismatches,0
Support: Firefox <=54,0
Halve the iteration target value to prevent interference from CSS upper bounds (gh-2144),0
Trust units reported by jQuery.css,0
Iteratively approximate from a nonzero starting point,0
Evaluate and update our best guess (doubling guesses that zero out).,0
Finish if the scale equals or crosses 1 (making the old*new product non-positive).,0
Make sure we update the tween properties later on,0
Apply relative offset (+=/-=) if specified,0
Determine new display value for elements that need to change,0
"Since we force visibility upon cascade-hidden elements, an immediate (and slow)",0
check is required in this first loop unless we have a nonempty display value (either,0
inline or about-to-be-restored),0
Remember what we're overwriting,0
Set the display of the elements in a second loop to avoid constant reflow,0
We have to close these tags to support XHTML (#13200),0
Support: IE <=9 only,0
XHTML parsers do not magically insert elements in the,0
same way that tag soup parsers do. So we cannot shorten,0
this by omitting <tbody> or other required elements.,0
Support: IE <=9 only,0
Support: IE <=9 - 11 only,0
Use typeof to avoid zero-argument method invocation on host objects (#15151),0
Mark scripts as having already been evaluated,0
Add nodes directly,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
"push.apply(_, arraylike) throws on ancient WebKit",0
Convert non-html into a text node,0
Convert html into DOM nodes,0
Deserialize a standard representation,0
Descend through wrappers to the right content,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
"push.apply(_, arraylike) throws on ancient WebKit",0
Remember the top-level container,0
Ensure the created nodes are orphaned (#12392),0
Remove wrapper from fragment,0
Skip elements already in the context collection (trac-4087),0
Append to fragment,0
Preserve script evaluation history,0
Capture executables,0
Support: Android 4.0 - 4.3 only,0
Check state lost if the name is set (#11217),0
Support: Windows Web Apps (WWA),0
`name` and `type` must use .setAttribute for WWA (#14901),0
Support: Android <=4.1 only,0
Older WebKit doesn't clone checked state correctly in fragments,0
Support: IE <=11 only,0
Make sure textarea (and checkbox) defaultValue is properly cloned,0
Support: IE <=9 - 11+,0
"focus() and blur() are asynchronous, except when they are no-op.",0
"So expect focus to be synchronous when the element is already active,",0
and blur to be synchronous when the element is not already active.,0
"(focus and blur are always synchronous in other supported browsers,",0
this just defines when we can count on it).,0
Support: IE <=9 only,0
Accessing document.activeElement can throw unexpectedly,0
https://bugs.jquery.com/ticket/13393,0
Types can be a map of types/handlers,0
"( types-Object, selector, data )",0
"( types-Object, data )",0
"( types, fn )",0
"( types, selector, fn )",0
"( types, data, fn )",0
"Can use an empty set, since event contains the info",0
Use same guid so caller can remove using origFn,0
Don't attach events to noData or text/comment nodes (but allow plain objects),0
Caller can pass in an object of custom data in lieu of the handler,0
Ensure that invalid selectors throw exceptions at attach time,0
"Evaluate against documentElement in case elem is a non-element node (e.g., document)",0
"Make sure that the handler has a unique ID, used to find/remove it later",0
"Init the element's event structure and main handler, if this is the first",0
Discard the second event of a jQuery.event.trigger() and,0
when an event is called after a page has unloaded,0
Handle multiple events separated by a space,0
"There *must* be a type, no attaching namespace-only handlers",0
"If event changes its type, use the special event handlers for the changed type",0
"If selector defined, determine special event api type, otherwise given type",0
Update special based on newly reset type,0
handleObj is passed to all event handlers,0
Init the event handler queue if we're the first,0
Only use addEventListener if the special events handler returns false,0
"Add to the element's handler list, delegates in front",0
"Keep track of which events have ever been used, for event optimization",0
Detach an event or set of events from an element,0
Once for each type.namespace in types; type may be omitted,0
"Unbind all events (on this namespace, if provided) for the element",0
Remove matching events,0
Remove generic event handler if we removed something and no more handlers exist,0
(avoids potential for endless recursion during removal of special event handlers),0
Remove data and the expando if it's no longer used,0
Make a writable jQuery.Event from the native event object,0
Use the fix-ed jQuery.Event rather than the (read-only) native event,0
"Call the preDispatch hook for the mapped type, and let it bail if desired",0
Determine handlers,0
Run delegates first; they may want to stop propagation beneath us,0
"If the event is namespaced, then each handler is only invoked if it is",0
specially universal or its namespaces are a superset of the event's.,0
Call the postDispatch hook for the mapped type,0
Find delegate handlers,0
Support: IE <=9,0
Black-hole SVG <use> instance trees (trac-13180),0
Support: Firefox <=42,0
Suppress spec-violating clicks indicating a non-primary pointer button (trac-3861),0
https://www.w3.org/TR/DOM-Level-3-Events/#event-type-click,0
Support: IE 11 only,0
"...but not arrow key ""clicks"" of radio inputs, which can have `button` -1 (gh-2343)",0
Don't check non-elements (#13208),0
"Don't process clicks on disabled elements (#6911, #8165, #11382, #11764)",0
Don't conflict with Object.prototype properties (#13203),0
Add the remaining (directly-bound) handlers,0
Prevent triggered image.load events from bubbling to window.load,0
Utilize native event to ensure correct state for checkable inputs,0
"For mutual compressibility with _default, replace `this` access with a local var.",0
`|| data` is dead code meant only to preserve the variable through minification.,0
Claim the first handler,0
"dataPriv.set( el, ""click"", ... )",0
Return false to allow normal processing in the caller,0
"For mutual compressibility with _default, replace `this` access with a local var.",0
`|| data` is dead code meant only to preserve the variable through minification.,0
Force setup before triggering a click,0
Return non-false to allow normal event-path propagation,0
"For cross-browser consistency, suppress native .click() on links",0
Also prevent it if we're currently inside a leveraged native-event stack,0
Support: Firefox 20+,0
Firefox doesn't alert if the returnValue field is not set.,0
Ensure the presence of an event listener that handles manually-triggered,0
synthetic events by interrupting progress until reinvoked in response to,0
"*native* events that it fires directly, ensuring that state changes have",0
already occurred before other listeners are invoked.,0
"Missing expectSync indicates a trigger call, which must force setup through jQuery.event.add",0
Register the controller as a special universal handler for all event namespaces,0
Interrupt processing of the outer synthetic .trigger()ed event,0
Store arguments for use when handling the inner native event,0
Trigger the native event and capture its result,0
Support: IE <=9 - 11+,0
focus() and blur() are asynchronous,0
Cancel the outer synthetic event,0
If this is an inner synthetic event for an event with a bubbling surrogate,0
"(focus or blur), assume that the surrogate already propagated from triggering the",0
native event and prevent that from happening again here.,0
This technically gets the ordering wrong w.r.t. to `.trigger()` (in which the,0
"bubbling surrogate propagates *after* the non-bubbling base), but that seems",0
less bad than duplication.,0
"If this is a native event triggered above, everything is now in order",0
Fire an inner synthetic event with the original arguments,0
...and capture the result,0
Support: IE <=9 - 11+,0
Extend with the prototype to reset the above stopImmediatePropagation(),0
Abort handling of the native event,0
"This ""if"" is needed for plain objects",0
Allow instantiation without the 'new' keyword,0
Event object,0
Events bubbling up the document may have been marked as prevented,0
by a handler lower down the tree; reflect the correct value.,0
Support: Android <=2.3 only,0
Create target properties,0
Support: Safari <=6 - 7 only,0
"Target should not be a text node (#504, #13143)",0
Event type,0
Put explicitly provided properties onto the event object,0
Create a timestamp if incoming event doesn't have one,0
Mark it as fixed,0
jQuery.Event is based on DOM3 Events as specified by the ECMAScript Language Binding,0
https://www.w3.org/TR/2003/WD-DOM-Level-3-Events-20030331/ecma-script-binding.html,0
Includes all common event props including KeyEvent and MouseEvent specific props,0
Add which for key events,0
Add which for click: 1 === left; 2 === middle; 3 === right,0
Utilize native event if possible so blur/focus sequence is correct,0
Claim the first handler,0
"dataPriv.set( this, ""focus"", ... )",0
"dataPriv.set( this, ""blur"", ... )",0
Return false to allow normal processing in the caller,0
Force setup before trigger,0
Return non-false to allow normal event-path propagation,0
Create mouseenter/leave events using mouseover/out and event-time checks,0
so that event delegation works in jQuery.,0
Do the same for pointerenter/pointerleave and pointerover/pointerout,0
,0
Support: Safari 7 only,0
Safari sends mouseenter too often; see:,0
https://bugs.chromium.org/p/chromium/issues/detail?id=470258,0
for the description of the bug (it existed in older Chrome versions as well).,0
For mouseenter/leave call the handler if related is outside the target.,0
NB: No relatedTarget if the mouse left/entered the browser window,0
( event )  dispatched jQuery.Event,0
"( types-object [, selector] )",0
"( types [, fn] )",0
See https://github.com/eslint/eslint/issues/3229,0
"Support: IE <=10 - 11, Edge 12 - 13 only",0
In IE/Edge using regex groups here causes severe slowdowns.,0
See https://connect.microsoft.com/IE/feedback/details/1736512/,0
"checked=""checked"" or checked",0
Prefer a tbody over its parent table for containing new rows,0
Replace/restore the type attribute of script elements for safe DOM manipulation,0
"1. Copy private data: events, handlers, etc.",0
2. Copy user data,0
"Fix IE bugs, see support tests",0
Fails to persist the checked state of a cloned checkbox or radio button.,0
Fails to return the selected option to the default selected state when cloning options,0
Flatten any nested arrays,0
"We can't cloneNode fragments that contain checked, in WebKit",0
Require either new content or an interest in ignored elements to invoke the callback,0
Use the original fragment for the last item,0
instead of the first because it can end up,0
being emptied incorrectly in certain situations (#8070).,0
Keep references to cloned scripts for later restoration,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
"push.apply(_, arraylike) throws on ancient WebKit",0
Reenable scripts,0
Evaluate executable scripts on first document insertion,0
"Optional AJAX dependency, but won't run scripts if not present",0
Fix IE cloning issues,0
We eschew Sizzle here for performance reasons: https://jsperf.com/getall-vs-sizzle/2,0
Copy the events from the original to the clone,0
Preserve script evaluation history,0
Return the cloned set,0
This is a shortcut to avoid jQuery.event.remove's overhead,0
Support: Chrome <=35 - 45+,0
"Assign undefined instead of using delete, see Data#remove",0
Support: Chrome <=35 - 45+,0
"Assign undefined instead of using delete, see Data#remove",0
Prevent memory leaks,0
Remove any remaining nodes,0
See if we can take a shortcut and just use innerHTML,0
Remove element nodes and prevent memory leaks,0
"If using innerHTML throws an exception, use the fallback method",0
"Make the changes, replacing each non-ignored context element with the new content",0
Force callback invocation,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
".get() because push.apply(_, arraylike) throws on ancient WebKit",0
"Support: IE <=11 only, Firefox <=30 (#15098, #14150)",0
IE throws on elements created in popups,0
"FF meanwhile throws on frame elements through ""defaultView.getComputedStyle""",0
Executing both pixelPosition & boxSizingReliable tests require only one layout,0
so they're executed at the same time to save the second computation.,0
"This is a singleton, we need to execute it only once",0
"Support: Android 4.0 - 4.3 only, Firefox <=3 - 44",0
"Support: Android 4.0 - 4.3 only, Safari <=9.1 - 10.1, iOS <=7.0 - 9.3",0
"Some styles come back with percentage values, even though they shouldn't",0
Support: IE 9 - 11 only,0
Detect misreporting of content dimensions for box-sizing:border-box elements,0
Support: IE 9 only,0
Detect overflow:scroll screwiness (gh-3699),0
Support: Chrome <=64,0
Don't get tricked when zoom affects offsetWidth (gh-4029),0
Nullify the div so it wouldn't be stored in the memory and,0
it will also be a sign that checks already performed,0
Finish early in limited (non-browser) environments,0
Support: IE <=9 - 11 only,0
Style of cloned element affects source element cloned (#8908),0
Support: Firefox 51+,0
Retrieving style before computed somehow,0
fixes an issue with getting wrong values,0
on detached elements,0
getPropertyValue is needed for:,0
".css('filter') (IE 9 only, #12537)",0
.css('--customProperty) (#3144),0
"A tribute to the ""awesome hack by Dean Edwards""",1
"Android Browser returns percentage for some values,",0
but width seems to be reliably pixels.,0
This is against the CSSOM draft spec:,0
https://drafts.csswg.org/cssom/#resolved-values,0
Remember the original values,0
Put in the new values to get a computed value out,0
Revert the changed values,0
Support: IE <=9 - 11 only,0
IE returns zIndex value as an integer.,0
"Define the hook, we'll check on the first run if it's really needed.",0
Hook not needed (or it's not possible to use it due,0
"to missing dependency), remove it.",0
Hook needed; redefine it so that the support test is not executed again.,0
Return a vendor-prefixed property or undefined,0
Check for vendor prefixed names,0
Return a potentially-mapped jQuery.cssProps or vendor prefixed property,0
Swappable if display is none or starts with table,0
"except ""table"", ""table-cell"", or ""table-caption""",0
See here for display values: https://developer.mozilla.org/en-US/docs/CSS/display,0
Any relative (+/-) values have already been,0
normalized at this point,0
"Guard against undefined ""subtract"", e.g., when used as in cssHooks",0
Adjustment may not be necessary,0
Both box models exclude margin,0
"If we get here with a content-box, we're seeking ""padding"" or ""border"" or ""margin""",0
Add padding,0
"For ""border"" or ""margin"", add border",0
But still keep track of it otherwise,0
"If we get here with a border-box (content + padding + border), we're seeking ""content"" or",0
"""padding"" or ""margin""",0
"For ""content"", subtract padding",0
"For ""content"" or ""padding"", subtract border",0
Account for positive content-box scroll gutter when requested by providing computedVal,0
"offsetWidth/offsetHeight is a rounded sum of content, padding, scroll gutter, and border",0
"Assuming integer scroll gutter, subtract the rest and round down",0
"If offsetWidth/offsetHeight is unknown, then we can't determine content-box scroll gutter",0
Use an explicit zero to avoid NaN (gh-3964),0
Start with computed style,0
"To avoid forcing a reflow, only fetch boxSizing if we need it (gh-4322).",0
Fake content-box until we know it's needed to know the true value.,0
Support: Firefox <=54,0
"Return a confounding non-pixel value or feign ignorance, as appropriate.",0
"Fall back to offsetWidth/offsetHeight when value is ""auto""",0
This happens for inline elements with no explicit setting (gh-3571),0
Support: Android <=4.1 - 4.3 only,0
Also use offsetWidth/offsetHeight for misreported inline dimensions (gh-3602),0
Support: IE 9-11 only,0
Also use offsetWidth/offsetHeight for when box sizing is unreliable,0
We use getClientRects() to check for hidden/disconnected.,0
"In those cases, the computed value can be trusted to be border-box",0
"Where available, offsetWidth/offsetHeight approximate border box dimensions.",0
"Where not available (e.g., SVG), assume unreliable box-sizing and interpret the",0
retrieved value as a content box dimension.,0
"Normalize """" and auto",0
Adjust for the element's box model,0
Provide the current computed size to request scroll gutter calculation (gh-3589),0
Add in style property hooks for overriding the default,0
behavior of getting and setting a style property,0
We should always get a number back from opacity,0
"Don't automatically add ""px"" to these possibly-unitless properties",0
Add in properties whose names you wish to fix before,0
setting or getting the value,0
Get and set the style property on a DOM Node,0
Don't set styles on text and comment nodes,0
Make sure that we're working with the right name,0
Make sure that we're working with the right name. We don't,0
want to query the value if it is a CSS custom property,0
since they are user-defined.,0
"Gets hook for the prefixed version, then unprefixed version",0
Check if we're setting a value,0
"Convert ""+="" or ""-="" to relative numbers (#7345)",0
Fixes bug #9237,0
Make sure that null and NaN values aren't set (#7116),0
"If a number was passed in, add the unit (except for certain CSS properties)",0
The isCustomProp check can be removed in jQuery 4.0 when we only auto-append,0
"""px"" to a few hardcoded values.",0
background-* props affect original clone's values,0
"If a hook was provided, use that value, otherwise just set the specified value",0
If a hook was provided get the non-computed value from there,0
Otherwise just get the value from the style object,0
Make sure that we're working with the right name. We don't,0
want to modify the value if it is a CSS custom property,0
since they are user-defined.,0
Try prefixed name followed by the unprefixed name,0
If a hook was provided get the computed value from there,0
"Otherwise, if a way to get the computed value exists, use that",0
"Convert ""normal"" to computed value",0
Make numeric if forced or a qualifier was provided and val looks numeric,0
Certain elements can have dimension info if we invisibly show them,0
but it must have a current display style that would benefit,0
Support: Safari 8+,0
Table columns in Safari have non-zero offsetWidth & zero,0
getBoundingClientRect().width unless display is changed.,0
Support: IE <=11 only,0
Running getBoundingClientRect on a disconnected node,0
in IE throws an error.,0
Only read styles.position if the test has a chance to fail,0
to avoid forcing a reflow.,0
"To avoid forcing a reflow, only fetch boxSizing if we need it (gh-3991)",0
Account for unreliable border-box dimensions by comparing offset* to computed and,0
faking a content-box to get border and padding (gh-3699),0
Convert to pixels if value adjustment is needed,0
These hooks are used by animate to expand properties,0
Assumes a single number if not a string,0
"Use a property on the element directly when it is not a DOM element,",0
or when there is no matching style property that exists.,0
Passing an empty string as a 3rd parameter to .css will automatically,0
attempt a parseFloat and fallback to a string if the parse fails.,0
"Simple values such as ""10px"" are parsed to Float;",0
"complex values such as ""rotate(1rad)"" are returned as-is.",0
"Empty strings, null, undefined and ""auto"" are converted to 0.",0
Use step hook for back compat.,0
Use cssHook if its there.,0
Use .style if available and use plain properties where available.,0
Support: IE <=9 only,0
Panic based approach to setting things on disconnected nodes,0
Back compat <1.8 extension point,0
Animations created synchronously will run synchronously,0
Generate parameters to create a standard animation,0
"If we include width, step value is 1 to do all cssExpand values,",0
otherwise step value is 2 to skip over Left and Right,0
We're done with this property,0
Queue-skipping animations hijack the fx hooks,0
Ensure the complete handler is called before this completes,0
Detect show/hide animations,0
"Pretend to be hidden if this is a ""show"" and",0
there is still data from a stopped show/hide,0
Ignore all other no-op show/hide data,0
Bail out if this is a no-op like .hide().hide(),0
"Restrict ""overflow"" and ""display"" styles during box animations",0
"Support: IE <=9 - 11, Edge 12 - 15",0
Record all 3 overflow attributes because IE does not infer the shorthand,0
from identically-valued overflowX and overflowY and Edge just mirrors,0
the overflowX value there.,0
"Identify a display type, preferring old show/hide data over the CSS cascade",0
Get nonempty value(s) by temporarily forcing visibility,0
Animate inline elements as inline-block,0
Restore the original display value at the end of pure show/hide animations,0
Implement show/hide animations,0
General show/hide setup for this element animation,0
"Store hidden/visible for toggle so `.stop().toggle()` ""reverses""",0
Show elements before animating them,0
"The final step of a ""hide"" animation is actually hiding the element",0
Per-property setup,0
"camelCase, specialEasing and expand cssHook pass",0
"Not quite $.extend, this won't overwrite existing keys.",0
"Reusing 'index' because we have the correct ""name""",0
Don't match elem in the :animated selector,0
Support: Android 2.3 only,0
Archaic crash bug won't allow us to use `1 - ( 0.5 || 0 )` (#12497),0
"If there's more to do, yield",0
"If this was an empty animation, synthesize a final progress notification",0
Resolve the animation and report its conclusion,0
"If we are going to the end, we want to run all the tweens",0
otherwise we skip this part,0
"Resolve when we played the last frame; otherwise, reject",0
Attach callbacks from options,0
Go to the end state if fx are off,0
"Normalize opt.queue - true/undefined/null -> ""fx""",0
Queueing,0
Show any hidden elements after setting opacity to 0,0
Animate to the value specified,0
Operate on a copy of prop so per-property easing won't be lost,0
"Empty animations, or finishing resolves immediately",0
Start the next in the queue if the last step wasn't forced.,0
"Timers currently will call their complete callbacks, which",0
will dequeue but only if they were gotoEnd.,0
Enable finishing flag on private data,0
Empty the queue first,0
"Look for any active animations, and finish them",0
Look for any animations in the old queue and finish them,0
Turn off finishing flag,0
Generate shortcuts for custom animations,0
Run the timer and safely remove it when done (allowing for external removal),0
Default speed,0
"Based off of the plugin by Clint Helfers, with permission.",0
https://web.archive.org/web/20100324014747/http://blindsignals.com/index.php/2009/07/jquery-delay/,0
Support: Android <=4.3 only,0
"Default value for a checkbox should be ""on""",0
Support: IE <=11 only,0
Must access selectedIndex to make default options select,0
Support: IE <=11 only,0
An input loses its value after becoming a radio,0
"Don't get/set attributes on text, comment and attribute nodes",0
Fallback to prop when attributes are not supported,0
Attribute hooks are determined by the lowercase version,0
Grab necessary hook if one is defined,0
"Non-existent attributes return null, we normalize to undefined",0
Attribute names can contain non-HTML whitespace characters,0
https://html.spec.whatwg.org/multipage/syntax.html#attributes-2,0
Hooks for boolean attributes,0
Remove boolean attributes when set to false,0
Avoid an infinite loop by temporarily removing this function from the getter,0
"Don't get/set properties on text, comment and attribute nodes",0
Fix name and attach hooks,0
Support: IE <=9 - 11 only,0
elem.tabIndex doesn't always return the,0
correct value when it hasn't been explicitly set,0
https://web.archive.org/web/20141116233347/http://fluidproject.org/blog/2008/01/09/getting-setting-and-removing-tabindex-values-with-javascript/,0
Use proper attribute retrieval(#12072),0
Support: IE <=11 only,0
Accessing the selectedIndex property,0
forces the browser to respect setting selected,0
on the option,0
The getter ensures a default option is selected,0
when in an optgroup,0
"eslint rule ""no-unused-expressions"" is disabled for this code",0
since it considers such accessions noop,0
Strip and collapse whitespace according to HTML spec,0
https://infra.spec.whatwg.org/#strip-and-collapse-ascii-whitespace,0
Only assign if different to avoid unneeded rendering.,0
This expression is here for better compressibility (see addClass),0
Remove *all* instances,0
Only assign if different to avoid unneeded rendering.,0
Toggle individual class names,0
"Check each className given, space separated list",0
Toggle whole class name,0
Store className if set,0
"If the element has a class name or if we're passed `false`,",0
"then remove the whole classname (if there was one, the above saved it).",0
"Otherwise bring back whatever was previously saved (if anything),",0
falling back to the empty string if nothing was stored.,0
Handle most common string cases,0
Handle cases where value is null/undef or number,0
"Treat null/undefined as """"; convert numbers to string",0
"If set returns undefined, fall back to normal setting",0
Support: IE <=10 - 11 only,0
"option.text throws exceptions (#14686, #14858)",0
Strip and collapse whitespace,0
https://html.spec.whatwg.org/#strip-and-collapse-whitespace,0
Loop through all the selected options,0
Support: IE <=9 only,0
IE8-9 doesn't update selected after form reset (#2551),0
Don't return options that are disabled or in a disabled optgroup,0
Get the specific value for the option,0
We don't need an array for one selects,0
Multi-Selects return an array,0
Force browsers to behave consistently when non-matching value is set,0
Radios and checkboxes getter/setter,0
Return jQuery for attributes-only inclusion,0
Don't do events on text and comment nodes,0
focus/blur morphs to focusin/out; ensure we're not firing them right now,0
Namespaced trigger; create a regexp to match event type in handle(),0
"Caller can pass in a jQuery.Event object, Object, or just an event type string",0
Trigger bitmask: & 1 for native handlers; & 2 for jQuery (always true),0
Clean up the event in case it is being reused,0
"Clone any incoming data and prepend the event, creating the handler arg list",0
Allow special events to draw outside the lines,0
"Determine event propagation path in advance, per W3C events spec (#9951)",0
"Bubble up to document, then to window; watch for a global ownerDocument var (#9724)",0
"Only add window if we got to document (e.g., not plain obj or detached DOM)",0
Fire handlers on the event path,0
jQuery handler,0
Native handler,0
"If nobody prevented the default action, do it now",0
Call a native DOM method on the target with the same name as the event.,0
"Don't do default actions on window, that's where global variables be (#6170)",0
Don't re-trigger an onFOO event when we call its FOO() method,0
"Prevent re-triggering of the same event, since we already bubbled it above",0
Piggyback on a donor event to simulate a different one,0
Used only for `focus(in | out)` events,0
Support: Firefox <=44,0
Firefox doesn't have focus(in | out) events,0
Related ticket - https://bugzilla.mozilla.org/show_bug.cgi?id=687787,0
,0
"Support: Chrome <=48 - 49, Safari <=9.0 - 9.1",0
"focus(in | out) events fire after focus & blur events,",0
which is spec violation - http://www.w3.org/TR/DOM-Level-3-Events/#events-focusevent-event-order,0
Related ticket - https://bugs.chromium.org/p/chromium/issues/detail?id=449857,0
Attach a single capturing handler on the document while someone wants focusin/focusout,0
Cross-browser xml parsing,0
Support: IE 9 - 11 only,0
IE throws on parseFromString with invalid input.,0
Serialize array item.,0
Treat each array item as a scalar.,0
"Item is non-scalar (array or object), encode its numeric index.",0
Serialize object item.,0
Serialize scalar item.,0
Serialize an array of form elements or a set of,0
key/values into a query string,0
"If value is a function, invoke it and use its return value",0
"If an array was passed in, assume that it is an array of form elements.",0
Serialize the form elements,0
"If traditional, encode the ""old"" way (the way 1.3.2 or older",0
"did it), otherwise encode params recursively.",0
Return the resulting serialization,0
"Can add propHook for ""elements"" to filter or add form elements",0
"Use .is( "":disabled"" ) so that fieldset[disabled] works",0
"#7653, #8125, #8152: local protocol detection",0
Avoid comment-prolog char sequence (#10098); must appease lint and evade compression,0
Anchor tag for parsing the document origin,0
"Base ""constructor"" for jQuery.ajaxPrefilter and jQuery.ajaxTransport",0
"dataTypeExpression is optional and defaults to ""*""",0
For each dataType in the dataTypeExpression,0
Prepend if requested,0
Otherwise append,0
Base inspection function for prefilters and transports,0
A special extend for ajax options,0
"that takes ""flat"" options (not to be deep extended)",0
Fixes #9887,0
Remove auto dataType and get content-type in the process,0
Check if we're dealing with a known content-type,0
Check to see if we have a response for the expected dataType,0
Try convertible dataTypes,0
Or just use first one,0
If we found a dataType,0
We add the dataType to the list if needed,0
and return the corresponding response,0
Work with a copy of dataTypes in case we need to modify it for conversion,0
Create converters map with lowercased keys,0
Convert to each sequential dataType,0
Apply the dataFilter if provided,0
There's only work to do if current dataType is non-auto,0
Convert response if prev dataType is non-auto and differs from current,0
Seek a direct converter,0
"If none found, seek a pair",0
If conv2 outputs current,0
If prev can be converted to accepted input,0
Condense equivalence converters,0
"Otherwise, insert the intermediate dataType",0
Apply converter (if not an equivalence),0
"Unless errors are allowed to bubble, catch and return them",0
Counter for holding the number of active queries,0
Last-Modified header cache for next request,0
Data converters,0
"Keys separate source (or catchall ""*"") and destination types with a single space",0
Convert anything to text,0
Text to html (true = no transformation),0
Evaluate text as a json expression,0
Parse text as xml,0
For options that shouldn't be deep extended:,0
you can add your own custom options here if,0
and when you create one that shouldn't be,0
deep extended (see ajaxExtend),0
Creates a full fledged settings object into target,0
with both ajaxSettings and settings fields.,0
"If target is omitted, writes into ajaxSettings.",0
Building a settings object,0
Extending ajaxSettings,0
Main method,0
"If url is an object, simulate pre-1.5 signature",0
Force options to be an object,0
URL without anti-cache param,0
Response headers,0
timeout handle,0
Url cleanup var,0
Request state (becomes false upon send and true upon completion),0
To know if global events are to be dispatched,0
Loop variable,0
uncached part of the url,0
Create the final options object,0
Callbacks context,0
Context for global events is callbackContext if it is a DOM node or jQuery collection,0
Deferreds,0
Status-dependent callbacks,0
Headers (they are sent all at once),0
Default abort message,0
Fake xhr,0
Builds headers hashtable if needed,0
Raw string,0
Caches the header,0
Overrides response content-type header,0
Status-dependent callbacks,0
Execute the appropriate callbacks,0
Lazy-add the new callbacks in a way that preserves old ones,0
Cancel the request,0
Attach deferreds,0
Add protocol if not provided (prefilters might expect it),0
Handle falsy url in the settings object (#10093: consistency with old signature),0
We also use the url parameter if available,0
Alias method option to type as per ticket #12004,0
Extract dataTypes list,0
A cross-domain request is in order when the origin doesn't match the current origin.,0
"Support: IE <=8 - 11, Edge 12 - 15",0
"IE throws exception on accessing the href property if url is malformed,",0
e.g. http://example.com:80x/,0
Support: IE <=8 - 11 only,0
Anchor's host property isn't correctly set when s.url is relative,0
"If there is an error parsing the URL, assume it is crossDomain,",0
it can be rejected by the transport if it is invalid,0
Convert data if not already a string,0
Apply prefilters,0
"If request was aborted inside a prefilter, stop there",0
We can fire global events as of now if asked to,0
Don't fire events if jQuery.event is undefined in an AMD-usage scenario (#15118),0
Watch for a new set of requests,0
Uppercase the type,0
Determine if request has content,0
Save the URL in case we're toying with the If-Modified-Since,0
and/or If-None-Match header later on,0
Remove hash to simplify url manipulation,0
More options handling for requests with no content,0
Remember the hash so we can put it back,0
"If data is available and should be processed, append data to url",0
#9682: remove data so that it's not used in an eventual retry,0
Add or update anti-cache param if needed,0
Put hash and anti-cache on the URL that will be requested (gh-1732),0
Change '%20' to '+' if this is encoded form body content (gh-2658),0
"Set the If-Modified-Since and/or If-None-Match header, if in ifModified mode.",0
"Set the correct header, if data is being sent",0
"Set the Accepts header for the server, depending on the dataType",0
Check for headers option,0
Allow custom headers/mimetypes and early abort,0
Abort if not done already and return,0
Aborting is no longer a cancellation,0
Install callbacks on deferreds,0
Get transport,0
"If no transport, we auto-abort",0
Send global event,0
"If request was aborted inside ajaxSend, stop there",0
Timeout,0
Rethrow post-completion exceptions,0
Propagate others as results,0
Callback for when everything is done,0
Ignore repeat invocations,0
Clear timeout if it exists,0
Dereference transport for early garbage collection,0
(no matter how long the jqXHR object will be used),0
Cache response headers,0
Set readyState,0
Determine if successful,0
Get response data,0
Convert no matter what (that way responseXXX fields are always set),0
"If successful, handle type chaining",0
"Set the If-Modified-Since and/or If-None-Match header, if in ifModified mode.",0
if no content,0
if not modified,0
"If we have data, let's convert it",0
Extract error from statusText and normalize for non-aborts,0
Set data for the fake xhr object,0
Success/Error,0
Status-dependent callbacks,0
Complete,0
Handle the global AJAX counter,0
Shift arguments if data argument was omitted,0
The url can be an options object (which then must have .url),0
"Make this explicit, since user can override this through ajaxSetup (#11264)",0
Only evaluate the response if it is successful (gh-4126),0
"dataFilter is not invoked for failure responses, so using it instead",0
of the default converter is kludgy but it works.,0
The elements to wrap the target around,0
"File protocol always yields status code 0, assume 200",0
Support: IE <=9 only,0
#1450: sometimes IE returns 1223 when it should be 204,0
Cross domain only allowed if supported through XMLHttpRequest,0
Apply custom fields if provided,0
Override mime type if needed,0
X-Requested-With header,0
"For cross-domain requests, seeing as conditions for a preflight are",0
"akin to a jigsaw puzzle, we simply never set it to be sure.",0
(it can always be set on a per-request basis or even using ajaxSetup),0
"For same-domain requests, won't change header if already provided.",0
Set headers,0
Callback,0
Support: IE <=9 only,0
"On a manual native abort, IE9 throws",0
errors on any property access that is not readyState,0
"File: protocol always yields status 0; see #8605, #14207",0
Support: IE <=9 only,0
IE9 has no XHR2 but throws on binary (trac-11426),0
"For XHR2 non-text, let the caller handle it (gh-2498)",0
Listen to events,0
Support: IE 9 only,0
Use onreadystatechange to replace onabort,0
to handle uncaught aborts,0
Check readyState before timeout as it changes,0
"Allow onerror to be called first,",0
but that will not handle a native abort,0
"Also, save errorCallback to a variable",0
as xhr.onerror cannot be accessed,0
Create the abort callback,0
Do send the request (this may raise an exception),0
#14683: Only rethrow if this hasn't been notified as an error yet,0
Prevent auto-execution of scripts when no explicit dataType was provided (See gh-2432),0
Install script dataType,0
Handle cache's special case and crossDomain,0
Bind script tag hack transport,0
This transport only deals with cross domain or forced-by-attrs requests,0
Use native DOM manipulation to avoid our domManip AJAX trickery,0
Default jsonp settings,0
"Detect, normalize options and install callbacks for jsonp requests",0
"Handle iff the expected data type is ""jsonp"" or we have a parameter to set",0
"Get callback name, remembering preexisting value associated with it",0
Insert callback into url or form data,0
Use data converter to retrieve json after script execution,0
Force json dataType,0
Install callback,0
Clean-up function (fires after converters),0
If previous value didn't exist - remove it,0
Otherwise restore preexisting value,0
Save back as free,0
Make sure that re-using the options doesn't screw things around,0
Save the callback name for future use,0
Call if it was a function and we have a response,0
Delegate to script,0
Support: Safari 8 only,0
In Safari 8 documents created via document.implementation.createHTMLDocument,0
collapse sibling forms: the second one becomes a child of the first one.,0
"Because of that, this security measure has to be disabled in Safari 8.",0
https://bugs.webkit.org/show_bug.cgi?id=137337,0
"Argument ""data"" should be string of html",0
"context (optional): If specified, the fragment will be created in this context,",0
defaults to document,0
"keepScripts (optional): If true, will include scripts passed in the html string",0
Stop scripts or inline event handlers from being executed immediately,0
by using document.implementation,0
Set the base href for the created document,0
so any parsed elements with URLs,0
are based on the document's URL (gh-2965),0
Single tag,0
If it's a function,0
We assume that it's the callback,0
"Otherwise, build a param string",0
"If we have elements to modify, make the request",0
"If ""type"" variable is undefined, then ""GET"" method will be used.",0
Make value of this field explicit since,0
user can override it through ajaxSetup method,0
Save response for use in complete callback,0
"If a selector was specified, locate the right elements in a dummy div",0
Exclude scripts to avoid IE 'Permission Denied' errors,0
Otherwise use the full result,0
"If the request succeeds, this function gets ""data"", ""status"", ""jqXHR""",0
but they are ignored because response was set above.,0
"If it fails, this function gets ""jqXHR"", ""status"", ""error""",0
Attach a bunch of functions for handling common AJAX events,0
"Set position first, in-case top/left are set even on static elem",0
Need to be able to calculate position if either,0
top or left is auto and position is either absolute or fixed,0
Use jQuery.extend here to allow modification of coordinates argument (gh-1848),0
offset() relates an element's border box to the document origin,0
Preserve chaining for setter,0
Return zeros for disconnected and hidden (display: none) elements (gh-2310),0
Support: IE <=11 only,0
Running getBoundingClientRect on a,0
disconnected node in IE throws an error,0
Get document-relative position by adding viewport scroll to viewport-relative gBCR,0
position() relates an element's margin box to its offset parent's padding box,0
This corresponds to the behavior of CSS absolute positioning,0
"position:fixed elements are offset from the viewport, which itself always has zero offset",0
Assume position:fixed implies availability of getBoundingClientRect,0
"Account for the *real* offset parent, which can be the document or its root element",0
when a statically positioned element is identified,0
"Incorporate borders into its offset, since they are outside its content origin",0
Subtract parent offsets and element margins,0
This method will return documentElement in the following cases:,0
"1) For the element inside the iframe without offsetParent, this method will return",0
documentElement of the parent window,0
2) For the hidden or detached element,0
"3) For body or html element, i.e. in case of the html node - it will return itself",0
,0
but those exceptions were never presented as a real life use-cases,0
and might be considered as more preferable results.,0
,0
"This logic, however, is not guaranteed and can change at any point in the future",0
Create scrollLeft and scrollTop methods,0
Coalesce documents and windows,0
"Support: Safari <=7 - 9.1, Chrome <=37 - 49",0
Add the top/left cssHooks using jQuery.fn.position,0
Webkit bug: https://bugs.webkit.org/show_bug.cgi?id=29084,0
Blink bug: https://bugs.chromium.org/p/chromium/issues/detail?id=589347,0
getComputedStyle returns percent when specified for top/left/bottom/right;,0
"rather than make the css module depend on the offset module, just check for it here",0
"If curCSS returns percentage, fallback to offset",0
"Create innerHeight, innerWidth, height, width, outerHeight and outerWidth methods",0
"Margin is only for outerHeight, outerWidth",0
$( window ).outerWidth/Height return w/h including scrollbars (gh-1729),0
Get document width or height,0
"Either scroll[Width/Height] or offset[Width/Height] or client[Width/Height],",0
whichever is greatest,0
"Get width or height on the element, requesting but not forcing parseFloat",0
Set width or height on the element,0
Handle event binding,0
"( namespace ) or ( selector, types [, fn] )",0
"Bind a function to a context, optionally partially applying any",0
arguments.,0
jQuery.proxy is deprecated to promote standards (specifically Function#bind),0
"However, it is not slated for removal any time soon",0
"Quick check to determine if target is callable, in the spec",0
"this throws a TypeError, but we will just return undefined.",0
Simulated bind,0
"Set the guid of unique handler to the same of original handler, so it can be removed",0
"As of jQuery 3.0, isNumeric is limited to",0
strings and numbers (primitives or objects),0
that can be coerced to finite numbers (gh-2662),0
"parseFloat NaNs numeric-cast false positives ("""")",0
"...but misinterprets leading-number strings, particularly hex literals (""0x..."")",0
subtraction forces infinities to NaN,0
"Register as a named AMD module, since jQuery can be concatenated with other",0
"files that may use define, but not via a proper concatenation script that",0
understands anonymous AMD modules. A named AMD is safest and most robust,0
way to register. Lowercase jquery is used because AMD module names are,0
"derived from file names, and jQuery is normally delivered in a lowercase",0
file name. Do this after creating the global so that if an AMD module wants,0
"to call noConflict to hide this version of jQuery, it will work.",0
"Note that for maximum portability, libraries that are not jQuery should",0
"declare themselves as anonymous modules, and avoid setting a global if an",0
"AMD loader is present. jQuery is a special case. For more information, see",0
https://github.com/jrburke/requirejs/wiki/Updating-existing-libraries#wiki-anon,0
Map over jQuery in case of overwrite,0
Map over the $ in case of overwrite,0
"Expose jQuery and $ identifiers, even in AMD",0
"(#7102#comment:10, https://github.com/jquery/jquery/pull/557)",0
and CommonJS for browser emulators (#13566),0
For CommonJS and CommonJS-like environments where a proper `window`,0
"is present, execute the factory and get jQuery.",0
For environments that do not have a `window` with a `document`,0
"(such as Node.js), expose a factory as module.exports.",0
This accentuates the need for the creation of a real `window`.,0
"e.g. var jQuery = require(""jquery"")(window);",0
See ticket #14549 for more info.,0
Pass this if window is not defined yet,0
"Edge <= 12 - 13+, Firefox <=18 - 45+, IE 10 - 11, Safari 5.1 - 9+, iOS 6 - 9.1",0
"throw exceptions when non-strict code (e.g., ASP.NET 4.5) accesses strict mode",0
"arguments.callee.caller (trac-13335). But as of jQuery 3.0 (2016), strict mode should be common",0
enough that all such attempts are guarded in a try block.,0
"Support: Chrome <=57, Firefox <=52",0
"In some browsers, typeof returns ""function"" for HTML <object> elements",0
"(i.e., `typeof document.createElement( ""object"" ) === ""function""`).",0
We don't want to classify *any* DOM node as a function.,0
"Support: Firefox 64+, Edge 18+",0
"Some browsers don't support the ""nonce"" property on scripts.",0
"On the other hand, just using `getAttribute` is not enough as",0
the `nonce` attribute is reset to an empty string whenever it,0
becomes browsing-context connected.,0
See https://github.com/whatwg/html/issues/2369,0
See https://html.spec.whatwg.org/#nonce-attributes,0
The `node.getAttribute` check was added for the sake of,0
`jQuery.globalEval` so that it can fake a nonce-containing node,0
via an object.,0
Support: Android <=2.3 only (functionish RegExp),0
Defining this global in .eslintrc.json would create a danger of using the global,0
"unguarded in another place, it seems safer to define global only for this module",0
Define a local copy of jQuery,0
The jQuery object is actually just the init constructor 'enhanced',0
Need init if jQuery is called (just allow error to be thrown if not included),0
Support: Android <=4.0 only,0
Make sure we trim BOM and NBSP,0
The current version of jQuery being used,0
The default length of a jQuery object is 0,0
Get the Nth element in the matched element set OR,0
Get the whole matched element set as a clean array,0
Return all the elements in a clean array,0
Return just the one element from the set,0
Take an array of elements and push it onto the stack,0
(returning the new matched element set),0
Build a new jQuery matched element set,0
Add the old object onto the stack (as a reference),0
Return the newly-formed element set,0
Execute a callback for every element in the matched set.,0
For internal use only.,0
"Behaves like an Array's method, not like a jQuery method.",0
Handle a deep copy situation,0
Skip the boolean and the target,0
Handle case when target is a string or something (possible in deep copy),0
Extend jQuery itself if only one argument is passed,0
Only deal with non-null/undefined values,0
Extend the base object,0
Prevent Object.prototype pollution,0
Prevent never-ending loop,0
Recurse if we're merging plain objects or arrays,0
Ensure proper type for the source value,0
"Never move original objects, clone them",0
Don't bring in undefined values,0
Return the modified object,0
Unique for each copy of jQuery on the page,0
Assume jQuery is ready without the ready module,0
Detect obvious negatives,0
Use toString instead of jQuery.type to catch host objects,0
"Objects with no prototype (e.g., `Object.create( null )`) are plain",0
Objects with prototype are plain iff they were constructed by a global Object function,0
Evaluates a script in a global context,0
Support: Android <=4.0 only,0
results is for internal usage only,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
"push.apply(_, arraylike) throws on ancient WebKit",0
"Go through the array, only saving the items",0
that pass the validator function,0
arg is for internal usage only,0
"Go through the array, translating each of the items to their new values",0
"Go through every key on the object,",0
Flatten any nested arrays,0
A global GUID counter for objects,0
jQuery.support is not used in Core but other projects attach their,0
properties to it so it needs to exist.,0
Populate the class2type map,0
Support: real iOS 8.2 only (not reproducible in simulator),0
`in` check used to prevent JIT error (gh-2145),0
hasOwn isn't used here due to false negatives,0
regarding Nodelist length in IE,0
Local document vars,0
Instance-specific data,0
Instance methods,0
Use a stripped-down indexOf as it's faster than native,0
https://jsperf.com/thor-indexof-vs-for/5,0
Regular expressions,0
http://www.w3.org/TR/css3-selectors/#whitespace,0
http://www.w3.org/TR/CSS21/syndata.html#value-def-identifier,0
Attribute selectors: http://www.w3.org/TR/selectors/#attribute-selectors,0
Operator (capture 2),0
"""Attribute values must be CSS identifiers [capture 5] or strings [capture 3 or capture 4]""",0
"To reduce the number of selectors needing tokenize in the preFilter, prefer arguments:",0
1. quoted (capture 3; capture 4 or capture 5),0
2. simple (capture 6),0
3. anything else (capture 2),0
"Leading and non-escaped trailing whitespace, capturing some non-whitespace characters preceding the latter",0
For use in libraries implementing .is(),0
We use this for POS matching in `select`,0
Easily-parseable/retrievable ID or TAG or CLASS selectors,0
CSS escapes,0
http://www.w3.org/TR/CSS21/syndata.html#escaped-characters,0
NaN means non-codepoint,0
Support: Firefox<24,0
"Workaround erroneous numeric interpretation of +""0x""",1
BMP codepoint,0
Supplemental Plane codepoint (surrogate pair),0
CSS string/identifier serialization,0
https://drafts.csswg.org/cssom/#common-serializing-idioms,0
U+0000 NULL becomes U+FFFD REPLACEMENT CHARACTER,0
Control characters and (dependent upon position) numbers get escaped as code points,0
Other potentially-special ASCII characters get backslash-escaped,0
Used for iframes,0
See setDocument(),0
"Removing the function wrapper causes a ""Permission Denied""",0
error in IE,0
"Optimize for push.apply( _, NodeList )",0
Support: Android<4.0,0
Detect silently failing push.apply,0
Leverage slice if possible,0
Support: IE<9,0
Otherwise append directly,0
Can't trust NodeList.length,0
"nodeType defaults to 9, since context defaults to document",0
Return early from calls with invalid selector or context,0
Try to shortcut find operations (as opposed to filters) in HTML documents,0
"If the selector is sufficiently simple, try using a ""get*By*"" DOM method",0
"(excepting DocumentFragment context, where the methods don't exist)",0
ID selector,0
Document context,0
"Support: IE, Opera, Webkit",0
TODO: identify versions,1
getElementById can match elements by name instead of ID,0
Element context,0
"Support: IE, Opera, Webkit",0
TODO: identify versions,1
getElementById can match elements by name instead of ID,0
Type selector,0
Class selector,0
Take advantage of querySelectorAll,0
Support: IE 8 only,0
Exclude object elements,0
qSA considers elements outside a scoping root when evaluating child or,0
"descendant combinators, which is not what we want.",0
"In such cases, we work around the behavior by prefixing every selector in the",0
list with an ID selector referencing the scope context.,0
Thanks to Andrew Dupont for this technique.,0
"Capture the context ID, setting it first if necessary",0
Prefix every selector in the list,0
Expand context for sibling selectors,0
All others,0
"Use (key + "" "") to avoid collision with native prototype properties (see Issue #157)",0
Only keep the most recent entries,0
Remove from its parent by default,0
release memory in IE,0
Use IE sourceIndex if available on both nodes,0
Check if b follows a,0
Known :disabled false positives: fieldset[disabled] > legend:nth-of-type(n+2) :can-disable,0
Only certain elements can match :enabled or :disabled,0
https://html.spec.whatwg.org/multipage/scripting.html#selector-enabled,0
https://html.spec.whatwg.org/multipage/scripting.html#selector-disabled,0
Check for inherited disabledness on relevant non-disabled elements:,0
* listed form-associated elements in a disabled fieldset,0
https://html.spec.whatwg.org/multipage/forms.html#category-listed,0
https://html.spec.whatwg.org/multipage/forms.html#concept-fe-disabled,0
* option elements in a disabled optgroup,0
https://html.spec.whatwg.org/multipage/forms.html#concept-option-disabled,0
"All such elements have a ""form"" property.",0
Option elements defer to a parent optgroup if present,0
Support: IE 6 - 11,0
Use the isDisabled shortcut property to check for disabled fieldset ancestors,0
"Where there is no isDisabled, check manually",0
Try to winnow out elements that can't be disabled before trusting the disabled property.,0
"Some victims get caught in our net (label, legend, menu, track), but it shouldn't",0
"even exist on them, let alone have a boolean value.",0
Remaining elements are neither :enabled nor :disabled,0
Match elements found at the specified indexes,0
Expose support vars for convenience,0
Support: IE <=8,0
"Assume HTML when documentElement doesn't yet exist, such as inside loading iframes",0
https://bugs.jquery.com/ticket/4833,0
Return early if doc is invalid or already selected,0
Update global variables,0
"Support: IE 9-11, Edge",0
"Accessing iframe documents after unload throws ""permission denied"" errors (jQuery #13936)",0
"Support: IE 11, Edge",0
Support: IE 9 - 10 only,0
Support: IE<8,0
Verify that getAttribute really returns attributes and not properties,0
(excepting IE8 booleans),0
"Check if getElementsByTagName(""*"") returns only elements",0
Support: IE<9,0
Support: IE<10,0
Check if getElementById returns elements by name,0
"The broken getElementById methods don't pick up programmatically-set names,",1
so use a roundabout getElementsByName test,0
ID filter and find,0
Support: IE 6 - 7 only,0
getElementById is not reliable as a find shortcut,0
Verify the id attribute,0
Fall back on getElementsByName,0
Tag,0
DocumentFragment nodes don't have gEBTN,0
"By happy coincidence, a (broken) gEBTN appears on DocumentFragment nodes too",0
Filter out possible comments,0
Class,0
QSA and matchesSelector support,0
matchesSelector(:active) reports false when true (IE9/Opera 11.5),0
qSa(:focus) reports false when true (Chrome 21),0
We allow this because of a bug in IE8/9 that throws an error,0
whenever `document.activeElement` is accessed on an iframe,0
"So, we allow :focus to pass through QSA all the time to avoid the IE error",0
See https://bugs.jquery.com/ticket/13378,0
Build QSA regex,0
Regex strategy adopted from Diego Perini,0
Select is set to empty string on purpose,0
This is to test IE's treatment of not explicitly,0
"setting a boolean content attribute,",0
since its presence should be enough,0
https://bugs.jquery.com/ticket/12359,0
"Support: IE8, Opera 11-12.16",0
Nothing should be selected when empty strings follow ^= or $= or *=,0
"The test attribute must be unknown in Opera but ""safe"" for WinRT",0
https://msdn.microsoft.com/en-us/library/ie/hh465388.aspx#attribute_section,0
Support: IE8,0
"Boolean attributes and ""value"" are not treated correctly",0
"Support: Chrome<29, Android<4.4, Safari<7.0+, iOS<7.0+, PhantomJS<1.9.8+",0
Webkit/Opera - :checked should return selected option elements,0
http://www.w3.org/TR/2011/REC-css3-selectors-20110929/#checked,0
IE8 throws error here and will not see later tests,0
"Support: Safari 8+, iOS 8+",0
https://bugs.webkit.org/show_bug.cgi?id=136851,0
In-page `selector#id sibling-combinator selector` fails,0
Support: Windows 8 Native Apps,0
The type and name attributes are restricted during .innerHTML assignment,0
Support: IE8,0
Enforce case-sensitivity of name attribute,0
FF 3.5 - :enabled/:disabled and hidden elements (hidden elements are still enabled),0
IE8 throws error here and will not see later tests,0
Support: IE9-11+,0
IE's :disabled selector does not pick up the children of disabled fieldsets,0
Opera 10-11 does not throw on post-comma invalid pseudos,0
Check to see if it's possible to do matchesSelector,0
on a disconnected node (IE 9),0
This should fail with an exception,0
"Gecko does not error, returns false instead",0
Element contains another,0
Purposefully self-exclusive,0
"As in, an element does not contain itself",0
Document order sorting,0
Flag for duplicate removal,0
Sort on method existence if only one input has compareDocumentPosition,0
Calculate position if both inputs belong to the same document,0
Otherwise we know they are disconnected,0
Disconnected nodes,0
Choose the first element that is related to our preferred document,0
Maintain original order,0
Exit early if the nodes are identical,0
Parentless nodes are either documents or disconnected,0
"If the nodes are siblings, we can do a quick check",0
Otherwise we need full lists of their ancestors for comparison,0
Walk down the tree looking for a discrepancy,0
Do a sibling check if the nodes have a common ancestor,0
Otherwise nodes in our document sort first,0
Set document vars if needed,0
IE 9's matchesSelector returns false on disconnected nodes,0
"As well, disconnected nodes are said to be in a document",0
fragment in IE 9,0
Set document vars if needed,0
Set document vars if needed,0
Don't get fooled by Object.prototype properties (jQuery #13807),0
"Unless we *know* we can detect duplicates, assume their presence",0
Clear input after sorting to release objects,0
See https://github.com/jquery/sizzle/pull/225,0
"If no nodeType, this is expected to be an array",0
Do not traverse comment nodes,0
Use textContent for elements,0
innerText usage removed for consistency of new lines (jQuery #11153),0
Traverse its children,0
Do not include comment or processing instruction nodes,0
Can be adjusted by the user,0
Move the given value to match[3] whether quoted or unquoted,0
nth-* requires argument,0
numeric x and y parameters for Expr.filter.CHILD,0
remember that false/true cast respectively to 0/1,0
other types prohibit arguments,0
Accept quoted arguments as-is,0
Strip excess characters from unquoted arguments,0
Get excess from tokenize (recursively),0
advance to the next closing parenthesis,0
excess is a negative index,0
Return only captures needed by the pseudo filter method (type and argument),0
Shortcut for :nth-*(n),0
:(first|last|only)-(child|of-type),0
Reverse direction for :only-* (if we haven't yet done so),0
non-xml :nth-child(...) stores cache data on `parent`,0
Seek `elem` from a previously-cached index,0
...in a gzip-friendly way,0
Support: IE <9 only,0
Defend against cloned attroperties (jQuery gh-1709),0
Fallback to seeking `elem` from the start,0
"When found, cache indexes on `parent` and break",0
Use previously-cached element index if available,0
...in a gzip-friendly way,0
Support: IE <9 only,0
Defend against cloned attroperties (jQuery gh-1709),0
xml :nth-child(...),0
or :nth-last-child(...) or :nth(-last)?-of-type(...),0
Use the same loop as above to seek `elem` from the start,0
Cache the index of each encountered element,0
Support: IE <9 only,0
Defend against cloned attroperties (jQuery gh-1709),0
"Incorporate the offset, then check against cycle size",0
pseudo-class names are case-insensitive,0
http://www.w3.org/TR/selectors/#pseudo-classes,0
Prioritize by case sensitivity in case custom pseudos are added with uppercase letters,0
Remember that setFilters inherits from pseudos,0
The user may use createPseudo to indicate that,0
arguments are needed to create the filter function,0
just as Sizzle does,0
But maintain support for old signatures,0
Potentially complex pseudos,0
Trim the selector passed to compile,0
to avoid treating leading and trailing,0
spaces as combinators,0
Match elements unmatched by `matcher`,0
Don't keep the element (issue #299),0
"""Whether an element is represented by a :lang() selector",0
is based solely on the element's language value,0
"being equal to the identifier C,",0
"or beginning with the identifier C immediately followed by ""-"".",0
The matching of C against the element's language value is performed case-insensitively.,0
"The identifier C does not have to be a valid language name.""",0
http://www.w3.org/TR/selectors/#lang-pseudo,0
lang value must be a valid identifier,0
Miscellaneous,0
Boolean properties,0
"In CSS3, :checked should return both checked and selected elements",0
http://www.w3.org/TR/2011/REC-css3-selectors-20110929/#checked,0
Accessing this property makes selected-by-default,0
options in Safari work properly,0
Contents,0
http://www.w3.org/TR/selectors/#empty-pseudo,0
":empty is negated by element (1) or content nodes (text: 3; cdata: 4; entity ref: 5),",0
but not by others (comment: 8; processing instruction: 7; etc.),0
nodeType < 6 works because attributes (2) do not appear as children,0
Element/input types,0
Support: IE<8,0
"New HTML5 attribute values (e.g., ""search"") appear with elem.type === ""text""",0
Position-in-collection,0
Add button/input type pseudos,0
Easy API for creating new setFilters,0
Comma and first run,0
Don't consume trailing commas as valid,0
Combinators,0
Cast descendant combinators to space,0
Filters,0
Return the length of the invalid excess,0
if we're just parsing,0
"Otherwise, throw an error or return tokens",0
Cache the tokens,0
Check against closest ancestor/preceding element,0
Check against all ancestor/preceding elements,0
"We can't set arbitrary data on XML nodes, so they don't benefit from combinator caching",0
Support: IE <9 only,0
Defend against cloned attroperties (jQuery gh-1709),0
Assign to newCache so results back-propagate to previous elements,0
Reuse newcache so results back-propagate to previous elements,0
A match means we're done; a fail means we have to keep checking,0
Get initial elements from seed or context,0
"Prefilter to get matcher input, preserving a map for seed-results synchronization",0
"If we have a postFinder, or filtered seed, or non-seed postFilter or preexisting results,",0
...intermediate processing is necessary,0
...otherwise use results directly,0
Find primary matches,0
Apply postFilter,0
Un-match failing elements by moving them back to matcherIn,0
Get the final matcherOut by condensing this intermediate into postFinder contexts,0
Restore matcherIn since elem is not yet a final match,0
Move matched elements from seed to results to keep them synchronized,0
"Add elements to results, through postFinder if defined",0
The foundational matcher ensures that elements are reachable from top-level context(s),0
Avoid hanging onto element (issue #299),0
Return special upon seeing a positional matcher,0
Find the next relative operator (if any) for proper handling,0
"If the preceding token was a descendant combinator, insert an implicit any-element `*`",0
We must always have either seed elements or outermost context,0
Use integer dirruns iff this is the outermost matcher,0
Add elements passing elementMatchers directly to results,0
"Support: IE<9, Safari",0
"Tolerate NodeList properties (IE: ""length""; Safari: <number>) matching elements by id",0
Track unmatched elements for set filters,0
They will have gone through all possible matchers,0
"Lengthen the array for every element, matched or not",0
"`i` is now the count of elements visited above, and adding it to `matchedCount`",0
makes the latter nonnegative.,0
Apply set filters to unmatched elements,0
"NOTE: This can be skipped if there are no unmatched elements (i.e., `matchedCount`",0
"equals `i`), unless we didn't visit _any_ elements in the above loop because we have",0
no element matchers and no seed.,0
"Incrementing an initially-string ""0"" `i` allows `i` to remain a string only in that",0
"case, which will result in a ""00"" `matchedCount` that differs from `i` but is also",0
numerically zero.,0
Reintegrate element matches to eliminate the need for sorting,0
Discard index placeholder values to get only actual matches,0
Add matches to results,0
Seedless set matches succeeding multiple successful matchers stipulate sorting,0
Override manipulation of globals by nested matchers,0
Generate a function of recursive functions that can be used to check each element,0
Cache the compiled function,0
Save selector and tokenization,0
Try to minimize operations if there is only one selector in the list and no seed,0
(the latter of which guarantees us context),0
Reduce context if the leading compound selector is an ID,0
"Precompiled matchers will still verify ancestry, so step up a level",0
Fetch a seed set for right-to-left matching,0
Abort if we hit a combinator,0
"Search, expanding context for leading sibling combinators",0
"If seed is empty or no tokens remain, we can return early",0
Compile and execute a filtering function if one is not provided,0
Provide `match` to avoid retokenization if we modified the selector above,0
One-time assignments,0
Sort stability,0
Support: Chrome 14-35+,0
Always assume duplicates if they aren't passed to the comparison function,0
Initialize against the default document,0
Support: Webkit<537.32 - Safari 6.0.3/Chrome 25 (fixed in Chrome 27),0
Detached nodes confoundingly follow *each other*,0
"Should return 1, but returns 4 (following)",0
Support: IE<8,0
"Prevent attribute/property ""interpolation""",0
https://msdn.microsoft.com/en-us/library/ms536429%28VS.85%29.aspx,0
Support: IE<9,0
"Use defaultValue in place of getAttribute(""value"")",0
Support: IE<9,0
Use getAttributeNode to fetch booleans when getAttribute lies,0
Deprecated,0
Implement the identical functionality for filter and not,0
Single element,0
"Arraylike of elements (jQuery, arguments, Array)",0
Filtered directly for both simple and complex selectors,0
"If this is a positional/relative selector, check membership in the returned set",0
"so $(""p:first"").is(""p:last"") won't return true for a doc with two ""p"".",0
Initialize a jQuery object,0
A central reference to the root jQuery(document),0
A simple way to check for HTML strings,0
Prioritize #id over <tag> to avoid XSS via location.hash (#9521),0
Strict HTML recognition (#11290: must start with <),0
Shortcut simple #id case for speed,0
"HANDLE: $(""""), $(null), $(undefined), $(false)",0
Method init() accepts an alternate rootjQuery,0
so migrate can support jQuery.sub (gh-2101),0
Handle HTML strings,0
Assume that strings that start and end with <> are HTML and skip the regex check,0
Match html or make sure no context is specified for #id,0
HANDLE: $(html) -> $(array),0
Option to run scripts is true for back-compat,0
Intentionally let the error be thrown if parseHTML is not present,0
"HANDLE: $(html, props)",0
Properties of context are called as methods if possible,0
...and otherwise set as attributes,0
HANDLE: $(#id),0
Inject the element directly into the jQuery object,0
"HANDLE: $(expr, $(...))",0
"HANDLE: $(expr, context)",0
(which is just equivalent to: $(context).find(expr),0
HANDLE: $(DOMElement),0
HANDLE: $(function),0
Shortcut for document ready,0
Execute immediately if ready is not present,0
Give the init function the jQuery prototype for later instantiation,0
Initialize central reference,0
Methods guaranteed to produce a unique set when starting from a unique set,0
"Positional selectors never match, since there's no _selection_ context",0
Always skip document fragments,0
Don't pass non-elements to Sizzle,0
Determine the position of an element within the set,0
"No argument, return index in parent",0
Index in selector,0
Locate the position of the desired element,0
"If it receives a jQuery object, the first element is used",0
"Support: IE 9 - 11 only, iOS 7 only, Android Browser <=4.3 only",0
Treat the template element as a regular one in browsers that,0
don't support it.,0
Remove duplicates,0
Reverse order for parents* and prev-derivatives,0
Convert String-formatted options into Object-formatted ones,0
Convert options from String-formatted to Object-formatted if needed,0
(we check in cache first),0
Last fire value for non-forgettable lists,0
Flag to know if list was already fired,0
Flag to prevent firing,0
Actual callback list,0
Queue of execution data for repeatable lists,0
Index of currently firing callback (modified by add/remove as needed),0
Fire callbacks,0
Enforce single-firing,0
"Execute callbacks for all pending executions,",0
respecting firingIndex overrides and runtime changes,0
Run callback and check for early termination,0
Jump to end and forget the data so .add doesn't re-fire,0
Forget the data if we're done with it,0
Clean up if we're done firing for good,0
Keep an empty list if we have data for future add calls,0
"Otherwise, this object is spent",0
Actual Callbacks object,0
Add a callback or a collection of callbacks to the list,0
"If we have memory from a past run, we should fire after adding",0
Inspect recursively,0
Remove a callback from the list,0
Handle firing indexes,0
Check if a given callback is in the list.,0
"If no argument is given, return whether or not list has callbacks attached.",0
Remove all callbacks from the list,0
Disable .fire and .add,0
Abort any current/pending executions,0
Clear all callbacks and values,0
Disable .fire,0
Also disable .add unless we have memory (since it would have no effect),0
Abort any pending executions,0
Call all callbacks with the given context and arguments,0
Call all the callbacks with the given arguments,0
To know if the callbacks have already been called at least once,0
Check for promise aspect first to privilege synchronous behavior,0
Other thenables,0
Other non-thenables,0
Control `resolve` arguments by letting Array#slice cast boolean `noValue` to integer:,0
* false: [ value ].slice( 0 ) => resolve( value ),0
* true: [ value ].slice( 1 ) => resolve(),0
"For Promises/A+, convert exceptions into rejections",0
"Since jQuery.when doesn't unwrap thenables, we can skip the extra checks appearing in",0
Deferred#then to conditionally suppress rejection.,0
Support: Android 4.0 only,0
Strict mode functions invoked without .call/.apply get global-object context,0
"action, add listener, callbacks,",0
"... .then handlers, argument index, [final state]",0
Keep pipe for back-compat,0
"Map tuples (progress, done, fail) to arguments (done, fail, progress)",0
deferred.progress(function() { bind to newDefer or newDefer.notify }),0
deferred.done(function() { bind to newDefer or newDefer.resolve }),0
deferred.fail(function() { bind to newDefer or newDefer.reject }),0
Support: Promises/A+ section 2.3.3.3.3,0
https://promisesaplus.com/#point-59,0
Ignore double-resolution attempts,0
Support: Promises/A+ section 2.3.1,0
https://promisesaplus.com/#point-48,0
"Support: Promises/A+ sections 2.3.3.1, 3.5",0
https://promisesaplus.com/#point-54,0
https://promisesaplus.com/#point-75,0
Retrieve `then` only once,0
Support: Promises/A+ section 2.3.4,0
https://promisesaplus.com/#point-64,0
Only check objects and functions for thenability,0
Handle a returned thenable,0
Special processors (notify) just wait for resolution,0
Normal processors (resolve) also hook into progress,0
...and disregard older resolution values,0
Handle all other returned values,0
Only substitute handlers pass on context,0
and multiple values (non-spec behavior),0
Process the value(s),0
Default process is resolve,0
Only normal processors (resolve) catch and reject exceptions,0
Support: Promises/A+ section 2.3.3.3.4.1,0
https://promisesaplus.com/#point-61,0
Ignore post-resolution exceptions,0
Only substitute handlers pass on context,0
and multiple values (non-spec behavior),0
Support: Promises/A+ section 2.3.3.3.1,0
https://promisesaplus.com/#point-57,0
Re-resolve promises immediately to dodge false rejection from,0
subsequent errors,0
"Call an optional hook to record the stack, in case of exception",0
since it's otherwise lost when execution goes async,0
progress_handlers.add( ... ),0
fulfilled_handlers.add( ... ),0
rejected_handlers.add( ... ),0
Get a promise for this deferred,0
"If obj is provided, the promise aspect is added to the object",0
Add list-specific methods,0
promise.progress = list.add,0
promise.done = list.add,0
promise.fail = list.add,0
Handle state,0
"state = ""resolved"" (i.e., fulfilled)",0
"state = ""rejected""",0
rejected_callbacks.disable,0
fulfilled_callbacks.disable,0
rejected_handlers.disable,0
fulfilled_handlers.disable,0
progress_callbacks.lock,0
progress_handlers.lock,0
progress_handlers.fire,0
fulfilled_handlers.fire,0
rejected_handlers.fire,0
deferred.notify = function() { deferred.notifyWith(...) },0
deferred.resolve = function() { deferred.resolveWith(...) },0
deferred.reject = function() { deferred.rejectWith(...) },0
deferred.notifyWith = list.fireWith,0
deferred.resolveWith = list.fireWith,0
deferred.rejectWith = list.fireWith,0
Make the deferred a promise,0
Call given func if any,0
All done!,0
Deferred helper,0
count of uncompleted subordinates,0
count of unprocessed arguments,0
subordinate fulfillment data,0
the master Deferred,0
subordinate callback factory,0
Single- and empty arguments are adopted like Promise.resolve,0
Use .then() to unwrap secondary thenables (cf. gh-3000),0
Multiple arguments are aggregated like Promise.all array elements,0
"These usually indicate a programmer mistake during development,",0
warn about them ASAP rather than swallowing them by default.,0
Support: IE 8 - 9 only,0
"Console exists when dev tools are open, which can happen at any time",0
The deferred used on DOM ready,0
Wrap jQuery.readyException in a function so that the lookup,0
happens at the time of error handling instead of callback,0
registration.,0
Is the DOM ready to be used? Set to true once it occurs.,0
A counter to track how many items to wait for before,0
the ready event fires. See #6781,0
Handle when the DOM is ready,0
Abort if there are pending holds or we're already ready,0
Remember that the DOM is ready,0
"If a normal DOM Ready event fired, decrement, and wait if need be",0
"If there are functions bound, to execute",0
The ready event handler and self cleanup method,0
Catch cases where $(document).ready() is called,0
after the browser event has already occurred.,0
Support: IE <=9 - 10 only,0
"Older IE sometimes signals ""interactive"" too soon",0
Handle it asynchronously to allow scripts the opportunity to delay ready,0
Use the handy event callback,0
"A fallback to window.onload, that will always work",0
Multifunctional method to get and set values of a collection,0
The value/s can optionally be executed if it's a function,0
Sets many values,0
Sets one value,0
Bulk operations run against the entire set,0
...except when executing function values,0
Gets,0
Matches dashed string for camelizing,0
Used by camelCase as callback to replace(),0
Convert dashed to camelCase; used by the css and data modules,0
"Support: IE <=9 - 11, Edge 12 - 15",0
Microsoft forgot to hump their vendor prefix (#9572),0
Accepts only:,0
- Node,0
- Node.ELEMENT_NODE,0
- Node.DOCUMENT_NODE,0
- Object,0
- Any,0
Check if the owner object already has a cache,0
"If not, create one",0
"We can accept data for non-element nodes in modern browsers,",0
"but we should not, see #8335.",0
Always return an empty object.,0
If it is a node unlikely to be stringify-ed or looped over,0
use plain assignment,0
Otherwise secure it in a non-enumerable property,0
configurable must be true to allow the property to be,0
deleted when data is removed,0
"Handle: [ owner, key, value ] args",0
Always use camelCase key (gh-2257),0
"Handle: [ owner, { properties } ] args",0
Copy the properties one-by-one to the cache object,0
Always use camelCase key (gh-2257),0
In cases where either:,0
,0
1. No key was specified,0
"2. A string key was specified, but no value provided",0
,0
"Take the ""read"" path and allow the get method to determine",0
"which value to return, respectively either:",0
,0
1. The entire cache object,0
2. The data stored at the key,0
,0
"When the key is not a string, or both a key and value",0
"are specified, set or extend (existing objects) with either:",0
,0
1. An object of properties,0
2. A key and value,0
,0
"Since the ""set"" path can have two possible entry points",0
return the expected data based on which path was taken[*],0
Support array or space separated string of keys,0
If key is an array of keys...,0
"We always set camelCase keys, so remove that.",0
"If a key with the spaces exists, use it.",0
"Otherwise, create an array by matching non-whitespace",0
Remove the expando if there's no more data,0
Support: Chrome <=35 - 45,0
Webkit & Blink performance suffers when deleting properties,0
"from DOM nodes, so set to undefined instead",0
https://bugs.chromium.org/p/chromium/issues/detail?id=378607 (bug restricted),0
Implementation Summary,0
,0
1. Enforce API surface and semantic compatibility with 1.9.x branch,0
2. Improve the module's maintainability by reducing the storage,0
paths to a single mechanism.,0
"3. Use the same single mechanism to support ""private"" and ""user"" data.",0
"4. _Never_ expose ""private"" data to user code (TODO: Drop _data, _removeData)",1
5. Avoid exposing implementation details on user objects (eg. expando properties),0
6. Provide a clear path for implementation upgrade to WeakMap in 2014,0
Only convert to a number if it doesn't change the string,0
"If nothing was found internally, try to fetch any",0
data from the HTML5 data-* attribute,0
Make sure we set the data so it isn't changed later,0
TODO: Now that all calls to _data and _removeData have been replaced,1
"with direct calls to dataPriv methods, these can be deprecated.",0
Gets all values,0
Support: IE 11 only,0
The attrs elements can be null (#14894),0
Sets multiple values,0
The calling jQuery object (element matches) is not empty,0
(and therefore has an element appears at this[ 0 ]) and the,0
`value` parameter was not undefined. An empty jQuery object,0
will result in `undefined` for elem = this[ 0 ] which will,0
throw an exception if an attempt to read a data cache is made.,0
Attempt to get data from the cache,0
The key will always be camelCased in Data,0
"Attempt to ""discover"" the data in",0
HTML5 custom data-* attrs,0
"We tried really hard, but the data doesn't exist.",0
Set the data...,0
We always store the camelCased key,0
Speed up dequeue by getting out quickly if this is just a lookup,0
"If the fx queue is dequeued, always remove the progress sentinel",0
Add a progress sentinel to prevent the fx queue from being,0
automatically dequeued,0
Clear up the last queue stop function,0
"Not public - generate a queueHooks object, or return the current one",0
Ensure a hooks for this queue,0
Get a promise resolved when queues of a certain type,0
are emptied (fx is the type by default),0
Check attachment across shadow DOM boundaries when possible (gh-3504),0
isHiddenWithinTree might be called from jQuery#filter function;,0
"in that case, element will be second argument",0
Inline style trumps all,0
"Otherwise, check computed style",0
Support: Firefox <=43 - 45,0
"Disconnected elements can have computed display: none, so first confirm that elem is",0
in the document.,0
"Remember the old values, and insert the new ones",0
Revert the old values,0
Starting value computation is required for potential unit mismatches,0
Support: Firefox <=54,0
Halve the iteration target value to prevent interference from CSS upper bounds (gh-2144),0
Trust units reported by jQuery.css,0
Iteratively approximate from a nonzero starting point,0
Evaluate and update our best guess (doubling guesses that zero out).,0
Finish if the scale equals or crosses 1 (making the old*new product non-positive).,0
Make sure we update the tween properties later on,0
Apply relative offset (+=/-=) if specified,0
Determine new display value for elements that need to change,0
"Since we force visibility upon cascade-hidden elements, an immediate (and slow)",0
check is required in this first loop unless we have a nonempty display value (either,0
inline or about-to-be-restored),0
Remember what we're overwriting,0
Set the display of the elements in a second loop to avoid constant reflow,0
We have to close these tags to support XHTML (#13200),0
Support: IE <=9 only,0
XHTML parsers do not magically insert elements in the,0
same way that tag soup parsers do. So we cannot shorten,0
this by omitting <tbody> or other required elements.,0
Support: IE <=9 only,0
Support: IE <=9 - 11 only,0
Use typeof to avoid zero-argument method invocation on host objects (#15151),0
Mark scripts as having already been evaluated,0
Add nodes directly,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
"push.apply(_, arraylike) throws on ancient WebKit",0
Convert non-html into a text node,0
Convert html into DOM nodes,0
Deserialize a standard representation,0
Descend through wrappers to the right content,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
"push.apply(_, arraylike) throws on ancient WebKit",0
Remember the top-level container,0
Ensure the created nodes are orphaned (#12392),0
Remove wrapper from fragment,0
Skip elements already in the context collection (trac-4087),0
Append to fragment,0
Preserve script evaluation history,0
Capture executables,0
Support: Android 4.0 - 4.3 only,0
Check state lost if the name is set (#11217),0
Support: Windows Web Apps (WWA),0
`name` and `type` must use .setAttribute for WWA (#14901),0
Support: Android <=4.1 only,0
Older WebKit doesn't clone checked state correctly in fragments,0
Support: IE <=11 only,0
Make sure textarea (and checkbox) defaultValue is properly cloned,0
Support: IE <=9 - 11+,0
"focus() and blur() are asynchronous, except when they are no-op.",0
"So expect focus to be synchronous when the element is already active,",0
and blur to be synchronous when the element is not already active.,0
"(focus and blur are always synchronous in other supported browsers,",0
this just defines when we can count on it).,0
Support: IE <=9 only,0
Accessing document.activeElement can throw unexpectedly,0
https://bugs.jquery.com/ticket/13393,0
Types can be a map of types/handlers,0
"( types-Object, selector, data )",0
"( types-Object, data )",0
"( types, fn )",0
"( types, selector, fn )",0
"( types, data, fn )",0
"Can use an empty set, since event contains the info",0
Use same guid so caller can remove using origFn,0
Don't attach events to noData or text/comment nodes (but allow plain objects),0
Caller can pass in an object of custom data in lieu of the handler,0
Ensure that invalid selectors throw exceptions at attach time,0
"Evaluate against documentElement in case elem is a non-element node (e.g., document)",0
"Make sure that the handler has a unique ID, used to find/remove it later",0
"Init the element's event structure and main handler, if this is the first",0
Discard the second event of a jQuery.event.trigger() and,0
when an event is called after a page has unloaded,0
Handle multiple events separated by a space,0
"There *must* be a type, no attaching namespace-only handlers",0
"If event changes its type, use the special event handlers for the changed type",0
"If selector defined, determine special event api type, otherwise given type",0
Update special based on newly reset type,0
handleObj is passed to all event handlers,0
Init the event handler queue if we're the first,0
Only use addEventListener if the special events handler returns false,0
"Add to the element's handler list, delegates in front",0
"Keep track of which events have ever been used, for event optimization",0
Detach an event or set of events from an element,0
Once for each type.namespace in types; type may be omitted,0
"Unbind all events (on this namespace, if provided) for the element",0
Remove matching events,0
Remove generic event handler if we removed something and no more handlers exist,0
(avoids potential for endless recursion during removal of special event handlers),0
Remove data and the expando if it's no longer used,0
Make a writable jQuery.Event from the native event object,0
Use the fix-ed jQuery.Event rather than the (read-only) native event,0
"Call the preDispatch hook for the mapped type, and let it bail if desired",0
Determine handlers,0
Run delegates first; they may want to stop propagation beneath us,0
"If the event is namespaced, then each handler is only invoked if it is",0
specially universal or its namespaces are a superset of the event's.,0
Call the postDispatch hook for the mapped type,0
Find delegate handlers,0
Support: IE <=9,0
Black-hole SVG <use> instance trees (trac-13180),0
Support: Firefox <=42,0
Suppress spec-violating clicks indicating a non-primary pointer button (trac-3861),0
https://www.w3.org/TR/DOM-Level-3-Events/#event-type-click,0
Support: IE 11 only,0
"...but not arrow key ""clicks"" of radio inputs, which can have `button` -1 (gh-2343)",0
Don't check non-elements (#13208),0
"Don't process clicks on disabled elements (#6911, #8165, #11382, #11764)",0
Don't conflict with Object.prototype properties (#13203),0
Add the remaining (directly-bound) handlers,0
Prevent triggered image.load events from bubbling to window.load,0
Utilize native event to ensure correct state for checkable inputs,0
"For mutual compressibility with _default, replace `this` access with a local var.",0
`|| data` is dead code meant only to preserve the variable through minification.,0
Claim the first handler,0
"dataPriv.set( el, ""click"", ... )",0
Return false to allow normal processing in the caller,0
"For mutual compressibility with _default, replace `this` access with a local var.",0
`|| data` is dead code meant only to preserve the variable through minification.,0
Force setup before triggering a click,0
Return non-false to allow normal event-path propagation,0
"For cross-browser consistency, suppress native .click() on links",0
Also prevent it if we're currently inside a leveraged native-event stack,0
Support: Firefox 20+,0
Firefox doesn't alert if the returnValue field is not set.,0
Ensure the presence of an event listener that handles manually-triggered,0
synthetic events by interrupting progress until reinvoked in response to,0
"*native* events that it fires directly, ensuring that state changes have",0
already occurred before other listeners are invoked.,0
"Missing expectSync indicates a trigger call, which must force setup through jQuery.event.add",0
Register the controller as a special universal handler for all event namespaces,0
Interrupt processing of the outer synthetic .trigger()ed event,0
Store arguments for use when handling the inner native event,0
Trigger the native event and capture its result,0
Support: IE <=9 - 11+,0
focus() and blur() are asynchronous,0
Cancel the outer synthetic event,0
If this is an inner synthetic event for an event with a bubbling surrogate,0
"(focus or blur), assume that the surrogate already propagated from triggering the",0
native event and prevent that from happening again here.,0
This technically gets the ordering wrong w.r.t. to `.trigger()` (in which the,0
"bubbling surrogate propagates *after* the non-bubbling base), but that seems",0
less bad than duplication.,0
"If this is a native event triggered above, everything is now in order",0
Fire an inner synthetic event with the original arguments,0
...and capture the result,0
Support: IE <=9 - 11+,0
Extend with the prototype to reset the above stopImmediatePropagation(),0
Abort handling of the native event,0
"This ""if"" is needed for plain objects",0
Allow instantiation without the 'new' keyword,0
Event object,0
Events bubbling up the document may have been marked as prevented,0
by a handler lower down the tree; reflect the correct value.,0
Support: Android <=2.3 only,0
Create target properties,0
Support: Safari <=6 - 7 only,0
"Target should not be a text node (#504, #13143)",0
Event type,0
Put explicitly provided properties onto the event object,0
Create a timestamp if incoming event doesn't have one,0
Mark it as fixed,0
jQuery.Event is based on DOM3 Events as specified by the ECMAScript Language Binding,0
https://www.w3.org/TR/2003/WD-DOM-Level-3-Events-20030331/ecma-script-binding.html,0
Includes all common event props including KeyEvent and MouseEvent specific props,0
Add which for key events,0
Add which for click: 1 === left; 2 === middle; 3 === right,0
Utilize native event if possible so blur/focus sequence is correct,0
Claim the first handler,0
"dataPriv.set( this, ""focus"", ... )",0
"dataPriv.set( this, ""blur"", ... )",0
Return false to allow normal processing in the caller,0
Force setup before trigger,0
Return non-false to allow normal event-path propagation,0
Create mouseenter/leave events using mouseover/out and event-time checks,0
so that event delegation works in jQuery.,0
Do the same for pointerenter/pointerleave and pointerover/pointerout,0
,0
Support: Safari 7 only,0
Safari sends mouseenter too often; see:,0
https://bugs.chromium.org/p/chromium/issues/detail?id=470258,0
for the description of the bug (it existed in older Chrome versions as well).,0
For mouseenter/leave call the handler if related is outside the target.,0
NB: No relatedTarget if the mouse left/entered the browser window,0
( event )  dispatched jQuery.Event,0
"( types-object [, selector] )",0
"( types [, fn] )",0
See https://github.com/eslint/eslint/issues/3229,0
"Support: IE <=10 - 11, Edge 12 - 13 only",0
In IE/Edge using regex groups here causes severe slowdowns.,0
See https://connect.microsoft.com/IE/feedback/details/1736512/,0
"checked=""checked"" or checked",0
Prefer a tbody over its parent table for containing new rows,0
Replace/restore the type attribute of script elements for safe DOM manipulation,0
"1. Copy private data: events, handlers, etc.",0
2. Copy user data,0
"Fix IE bugs, see support tests",0
Fails to persist the checked state of a cloned checkbox or radio button.,0
Fails to return the selected option to the default selected state when cloning options,0
Flatten any nested arrays,0
"We can't cloneNode fragments that contain checked, in WebKit",0
Require either new content or an interest in ignored elements to invoke the callback,0
Use the original fragment for the last item,0
instead of the first because it can end up,0
being emptied incorrectly in certain situations (#8070).,0
Keep references to cloned scripts for later restoration,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
"push.apply(_, arraylike) throws on ancient WebKit",0
Reenable scripts,0
Evaluate executable scripts on first document insertion,0
"Optional AJAX dependency, but won't run scripts if not present",0
Fix IE cloning issues,0
We eschew Sizzle here for performance reasons: https://jsperf.com/getall-vs-sizzle/2,0
Copy the events from the original to the clone,0
Preserve script evaluation history,0
Return the cloned set,0
This is a shortcut to avoid jQuery.event.remove's overhead,0
Support: Chrome <=35 - 45+,0
"Assign undefined instead of using delete, see Data#remove",0
Support: Chrome <=35 - 45+,0
"Assign undefined instead of using delete, see Data#remove",0
Prevent memory leaks,0
Remove any remaining nodes,0
See if we can take a shortcut and just use innerHTML,0
Remove element nodes and prevent memory leaks,0
"If using innerHTML throws an exception, use the fallback method",0
"Make the changes, replacing each non-ignored context element with the new content",0
Force callback invocation,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
".get() because push.apply(_, arraylike) throws on ancient WebKit",0
"Support: IE <=11 only, Firefox <=30 (#15098, #14150)",0
IE throws on elements created in popups,0
"FF meanwhile throws on frame elements through ""defaultView.getComputedStyle""",0
Executing both pixelPosition & boxSizingReliable tests require only one layout,0
so they're executed at the same time to save the second computation.,0
"This is a singleton, we need to execute it only once",0
"Support: Android 4.0 - 4.3 only, Firefox <=3 - 44",0
"Support: Android 4.0 - 4.3 only, Safari <=9.1 - 10.1, iOS <=7.0 - 9.3",0
"Some styles come back with percentage values, even though they shouldn't",0
Support: IE 9 - 11 only,0
Detect misreporting of content dimensions for box-sizing:border-box elements,0
Support: IE 9 only,0
Detect overflow:scroll screwiness (gh-3699),0
Support: Chrome <=64,0
Don't get tricked when zoom affects offsetWidth (gh-4029),0
Nullify the div so it wouldn't be stored in the memory and,0
it will also be a sign that checks already performed,0
Finish early in limited (non-browser) environments,0
Support: IE <=9 - 11 only,0
Style of cloned element affects source element cloned (#8908),0
Support: Firefox 51+,0
Retrieving style before computed somehow,0
fixes an issue with getting wrong values,0
on detached elements,0
getPropertyValue is needed for:,0
".css('filter') (IE 9 only, #12537)",0
.css('--customProperty) (#3144),0
"A tribute to the ""awesome hack by Dean Edwards""",1
"Android Browser returns percentage for some values,",0
but width seems to be reliably pixels.,0
This is against the CSSOM draft spec:,0
https://drafts.csswg.org/cssom/#resolved-values,0
Remember the original values,0
Put in the new values to get a computed value out,0
Revert the changed values,0
Support: IE <=9 - 11 only,0
IE returns zIndex value as an integer.,0
"Define the hook, we'll check on the first run if it's really needed.",0
Hook not needed (or it's not possible to use it due,0
"to missing dependency), remove it.",0
Hook needed; redefine it so that the support test is not executed again.,0
Return a vendor-prefixed property or undefined,0
Check for vendor prefixed names,0
Return a potentially-mapped jQuery.cssProps or vendor prefixed property,0
Swappable if display is none or starts with table,0
"except ""table"", ""table-cell"", or ""table-caption""",0
See here for display values: https://developer.mozilla.org/en-US/docs/CSS/display,0
Any relative (+/-) values have already been,0
normalized at this point,0
"Guard against undefined ""subtract"", e.g., when used as in cssHooks",0
Adjustment may not be necessary,0
Both box models exclude margin,0
"If we get here with a content-box, we're seeking ""padding"" or ""border"" or ""margin""",0
Add padding,0
"For ""border"" or ""margin"", add border",0
But still keep track of it otherwise,0
"If we get here with a border-box (content + padding + border), we're seeking ""content"" or",0
"""padding"" or ""margin""",0
"For ""content"", subtract padding",0
"For ""content"" or ""padding"", subtract border",0
Account for positive content-box scroll gutter when requested by providing computedVal,0
"offsetWidth/offsetHeight is a rounded sum of content, padding, scroll gutter, and border",0
"Assuming integer scroll gutter, subtract the rest and round down",0
"If offsetWidth/offsetHeight is unknown, then we can't determine content-box scroll gutter",0
Use an explicit zero to avoid NaN (gh-3964),0
Start with computed style,0
"To avoid forcing a reflow, only fetch boxSizing if we need it (gh-4322).",0
Fake content-box until we know it's needed to know the true value.,0
Support: Firefox <=54,0
"Return a confounding non-pixel value or feign ignorance, as appropriate.",0
"Fall back to offsetWidth/offsetHeight when value is ""auto""",0
This happens for inline elements with no explicit setting (gh-3571),0
Support: Android <=4.1 - 4.3 only,0
Also use offsetWidth/offsetHeight for misreported inline dimensions (gh-3602),0
Support: IE 9-11 only,0
Also use offsetWidth/offsetHeight for when box sizing is unreliable,0
We use getClientRects() to check for hidden/disconnected.,0
"In those cases, the computed value can be trusted to be border-box",0
"Where available, offsetWidth/offsetHeight approximate border box dimensions.",0
"Where not available (e.g., SVG), assume unreliable box-sizing and interpret the",0
retrieved value as a content box dimension.,0
"Normalize """" and auto",0
Adjust for the element's box model,0
Provide the current computed size to request scroll gutter calculation (gh-3589),0
Add in style property hooks for overriding the default,0
behavior of getting and setting a style property,0
We should always get a number back from opacity,0
"Don't automatically add ""px"" to these possibly-unitless properties",0
Add in properties whose names you wish to fix before,0
setting or getting the value,0
Get and set the style property on a DOM Node,0
Don't set styles on text and comment nodes,0
Make sure that we're working with the right name,0
Make sure that we're working with the right name. We don't,0
want to query the value if it is a CSS custom property,0
since they are user-defined.,0
"Gets hook for the prefixed version, then unprefixed version",0
Check if we're setting a value,0
"Convert ""+="" or ""-="" to relative numbers (#7345)",0
Fixes bug #9237,0
Make sure that null and NaN values aren't set (#7116),0
"If a number was passed in, add the unit (except for certain CSS properties)",0
The isCustomProp check can be removed in jQuery 4.0 when we only auto-append,0
"""px"" to a few hardcoded values.",0
background-* props affect original clone's values,0
"If a hook was provided, use that value, otherwise just set the specified value",0
If a hook was provided get the non-computed value from there,0
Otherwise just get the value from the style object,0
Make sure that we're working with the right name. We don't,0
want to modify the value if it is a CSS custom property,0
since they are user-defined.,0
Try prefixed name followed by the unprefixed name,0
If a hook was provided get the computed value from there,0
"Otherwise, if a way to get the computed value exists, use that",0
"Convert ""normal"" to computed value",0
Make numeric if forced or a qualifier was provided and val looks numeric,0
Certain elements can have dimension info if we invisibly show them,0
but it must have a current display style that would benefit,0
Support: Safari 8+,0
Table columns in Safari have non-zero offsetWidth & zero,0
getBoundingClientRect().width unless display is changed.,0
Support: IE <=11 only,0
Running getBoundingClientRect on a disconnected node,0
in IE throws an error.,0
Only read styles.position if the test has a chance to fail,0
to avoid forcing a reflow.,0
"To avoid forcing a reflow, only fetch boxSizing if we need it (gh-3991)",0
Account for unreliable border-box dimensions by comparing offset* to computed and,0
faking a content-box to get border and padding (gh-3699),0
Convert to pixels if value adjustment is needed,0
These hooks are used by animate to expand properties,0
Assumes a single number if not a string,0
"Based off of the plugin by Clint Helfers, with permission.",0
https://web.archive.org/web/20100324014747/http://blindsignals.com/index.php/2009/07/jquery-delay/,0
Support: Android <=4.3 only,0
"Default value for a checkbox should be ""on""",0
Support: IE <=11 only,0
Must access selectedIndex to make default options select,0
Support: IE <=11 only,0
An input loses its value after becoming a radio,0
"Don't get/set attributes on text, comment and attribute nodes",0
Fallback to prop when attributes are not supported,0
Attribute hooks are determined by the lowercase version,0
Grab necessary hook if one is defined,0
"Non-existent attributes return null, we normalize to undefined",0
Attribute names can contain non-HTML whitespace characters,0
https://html.spec.whatwg.org/multipage/syntax.html#attributes-2,0
Hooks for boolean attributes,0
Remove boolean attributes when set to false,0
Avoid an infinite loop by temporarily removing this function from the getter,0
"Don't get/set properties on text, comment and attribute nodes",0
Fix name and attach hooks,0
Support: IE <=9 - 11 only,0
elem.tabIndex doesn't always return the,0
correct value when it hasn't been explicitly set,0
https://web.archive.org/web/20141116233347/http://fluidproject.org/blog/2008/01/09/getting-setting-and-removing-tabindex-values-with-javascript/,0
Use proper attribute retrieval(#12072),0
Support: IE <=11 only,0
Accessing the selectedIndex property,0
forces the browser to respect setting selected,0
on the option,0
The getter ensures a default option is selected,0
when in an optgroup,0
"eslint rule ""no-unused-expressions"" is disabled for this code",0
since it considers such accessions noop,0
Strip and collapse whitespace according to HTML spec,0
https://infra.spec.whatwg.org/#strip-and-collapse-ascii-whitespace,0
Only assign if different to avoid unneeded rendering.,0
This expression is here for better compressibility (see addClass),0
Remove *all* instances,0
Only assign if different to avoid unneeded rendering.,0
Toggle individual class names,0
"Check each className given, space separated list",0
Toggle whole class name,0
Store className if set,0
"If the element has a class name or if we're passed `false`,",0
"then remove the whole classname (if there was one, the above saved it).",0
"Otherwise bring back whatever was previously saved (if anything),",0
falling back to the empty string if nothing was stored.,0
Handle most common string cases,0
Handle cases where value is null/undef or number,0
"Treat null/undefined as """"; convert numbers to string",0
"If set returns undefined, fall back to normal setting",0
Support: IE <=10 - 11 only,0
"option.text throws exceptions (#14686, #14858)",0
Strip and collapse whitespace,0
https://html.spec.whatwg.org/#strip-and-collapse-whitespace,0
Loop through all the selected options,0
Support: IE <=9 only,0
IE8-9 doesn't update selected after form reset (#2551),0
Don't return options that are disabled or in a disabled optgroup,0
Get the specific value for the option,0
We don't need an array for one selects,0
Multi-Selects return an array,0
Force browsers to behave consistently when non-matching value is set,0
Radios and checkboxes getter/setter,0
Return jQuery for attributes-only inclusion,0
Don't do events on text and comment nodes,0
focus/blur morphs to focusin/out; ensure we're not firing them right now,0
Namespaced trigger; create a regexp to match event type in handle(),0
"Caller can pass in a jQuery.Event object, Object, or just an event type string",0
Trigger bitmask: & 1 for native handlers; & 2 for jQuery (always true),0
Clean up the event in case it is being reused,0
"Clone any incoming data and prepend the event, creating the handler arg list",0
Allow special events to draw outside the lines,0
"Determine event propagation path in advance, per W3C events spec (#9951)",0
"Bubble up to document, then to window; watch for a global ownerDocument var (#9724)",0
"Only add window if we got to document (e.g., not plain obj or detached DOM)",0
Fire handlers on the event path,0
jQuery handler,0
Native handler,0
"If nobody prevented the default action, do it now",0
Call a native DOM method on the target with the same name as the event.,0
"Don't do default actions on window, that's where global variables be (#6170)",0
Don't re-trigger an onFOO event when we call its FOO() method,0
"Prevent re-triggering of the same event, since we already bubbled it above",0
Piggyback on a donor event to simulate a different one,0
Used only for `focus(in | out)` events,0
Support: Firefox <=44,0
Firefox doesn't have focus(in | out) events,0
Related ticket - https://bugzilla.mozilla.org/show_bug.cgi?id=687787,0
,0
"Support: Chrome <=48 - 49, Safari <=9.0 - 9.1",0
"focus(in | out) events fire after focus & blur events,",0
which is spec violation - http://www.w3.org/TR/DOM-Level-3-Events/#events-focusevent-event-order,0
Related ticket - https://bugs.chromium.org/p/chromium/issues/detail?id=449857,0
Attach a single capturing handler on the document while someone wants focusin/focusout,0
Serialize array item.,0
Treat each array item as a scalar.,0
"Item is non-scalar (array or object), encode its numeric index.",0
Serialize object item.,0
Serialize scalar item.,0
Serialize an array of form elements or a set of,0
key/values into a query string,0
"If value is a function, invoke it and use its return value",0
"If an array was passed in, assume that it is an array of form elements.",0
Serialize the form elements,0
"If traditional, encode the ""old"" way (the way 1.3.2 or older",0
"did it), otherwise encode params recursively.",0
Return the resulting serialization,0
"Can add propHook for ""elements"" to filter or add form elements",0
"Use .is( "":disabled"" ) so that fieldset[disabled] works",0
The elements to wrap the target around,0
Support: Safari 8 only,0
In Safari 8 documents created via document.implementation.createHTMLDocument,0
collapse sibling forms: the second one becomes a child of the first one.,0
"Because of that, this security measure has to be disabled in Safari 8.",0
https://bugs.webkit.org/show_bug.cgi?id=137337,0
"Argument ""data"" should be string of html",0
"context (optional): If specified, the fragment will be created in this context,",0
defaults to document,0
"keepScripts (optional): If true, will include scripts passed in the html string",0
Stop scripts or inline event handlers from being executed immediately,0
by using document.implementation,0
Set the base href for the created document,0
so any parsed elements with URLs,0
are based on the document's URL (gh-2965),0
Single tag,0
"Set position first, in-case top/left are set even on static elem",0
Need to be able to calculate position if either,0
top or left is auto and position is either absolute or fixed,0
Use jQuery.extend here to allow modification of coordinates argument (gh-1848),0
offset() relates an element's border box to the document origin,0
Preserve chaining for setter,0
Return zeros for disconnected and hidden (display: none) elements (gh-2310),0
Support: IE <=11 only,0
Running getBoundingClientRect on a,0
disconnected node in IE throws an error,0
Get document-relative position by adding viewport scroll to viewport-relative gBCR,0
position() relates an element's margin box to its offset parent's padding box,0
This corresponds to the behavior of CSS absolute positioning,0
"position:fixed elements are offset from the viewport, which itself always has zero offset",0
Assume position:fixed implies availability of getBoundingClientRect,0
"Account for the *real* offset parent, which can be the document or its root element",0
when a statically positioned element is identified,0
"Incorporate borders into its offset, since they are outside its content origin",0
Subtract parent offsets and element margins,0
This method will return documentElement in the following cases:,0
"1) For the element inside the iframe without offsetParent, this method will return",0
documentElement of the parent window,0
2) For the hidden or detached element,0
"3) For body or html element, i.e. in case of the html node - it will return itself",0
,0
but those exceptions were never presented as a real life use-cases,0
and might be considered as more preferable results.,0
,0
"This logic, however, is not guaranteed and can change at any point in the future",0
Create scrollLeft and scrollTop methods,0
Coalesce documents and windows,0
"Support: Safari <=7 - 9.1, Chrome <=37 - 49",0
Add the top/left cssHooks using jQuery.fn.position,0
Webkit bug: https://bugs.webkit.org/show_bug.cgi?id=29084,0
Blink bug: https://bugs.chromium.org/p/chromium/issues/detail?id=589347,0
getComputedStyle returns percent when specified for top/left/bottom/right;,0
"rather than make the css module depend on the offset module, just check for it here",0
"If curCSS returns percentage, fallback to offset",0
"Create innerHeight, innerWidth, height, width, outerHeight and outerWidth methods",0
"Margin is only for outerHeight, outerWidth",0
$( window ).outerWidth/Height return w/h including scrollbars (gh-1729),0
Get document width or height,0
"Either scroll[Width/Height] or offset[Width/Height] or client[Width/Height],",0
whichever is greatest,0
"Get width or height on the element, requesting but not forcing parseFloat",0
Set width or height on the element,0
Handle event binding,0
"( namespace ) or ( selector, types [, fn] )",0
"Bind a function to a context, optionally partially applying any",0
arguments.,0
jQuery.proxy is deprecated to promote standards (specifically Function#bind),0
"However, it is not slated for removal any time soon",0
"Quick check to determine if target is callable, in the spec",0
"this throws a TypeError, but we will just return undefined.",0
Simulated bind,0
"Set the guid of unique handler to the same of original handler, so it can be removed",0
"As of jQuery 3.0, isNumeric is limited to",0
strings and numbers (primitives or objects),0
that can be coerced to finite numbers (gh-2662),0
"parseFloat NaNs numeric-cast false positives ("""")",0
"...but misinterprets leading-number strings, particularly hex literals (""0x..."")",0
subtraction forces infinities to NaN,0
"Register as a named AMD module, since jQuery can be concatenated with other",0
"files that may use define, but not via a proper concatenation script that",0
understands anonymous AMD modules. A named AMD is safest and most robust,0
way to register. Lowercase jquery is used because AMD module names are,0
"derived from file names, and jQuery is normally delivered in a lowercase",0
file name. Do this after creating the global so that if an AMD module wants,0
"to call noConflict to hide this version of jQuery, it will work.",0
"Note that for maximum portability, libraries that are not jQuery should",0
"declare themselves as anonymous modules, and avoid setting a global if an",0
"AMD loader is present. jQuery is a special case. For more information, see",0
https://github.com/jrburke/requirejs/wiki/Updating-existing-libraries#wiki-anon,0
Map over jQuery in case of overwrite,0
Map over the $ in case of overwrite,0
"Expose jQuery and $ identifiers, even in AMD",0
"(#7102#comment:10, https://github.com/jquery/jquery/pull/557)",0
and CommonJS for browser emulators (#13566),0
Implementation Summary,0
,0
1. Enforce API surface and semantic compatibility with 1.9.x branch,0
2. Improve the module's maintainability by reducing the storage,0
paths to a single mechanism.,0
"3. Use the same single mechanism to support ""private"" and ""user"" data.",0
"4. _Never_ expose ""private"" data to user code (TODO: Drop _data, _removeData)",1
5. Avoid exposing implementation details on user objects (eg. expando properties),0
6. Provide a clear path for implementation upgrade to WeakMap in 2014,0
Only convert to a number if it doesn't change the string,0
"If nothing was found internally, try to fetch any",0
data from the HTML5 data-* attribute,0
Make sure we set the data so it isn't changed later,0
TODO: Now that all calls to _data and _removeData have been replaced,1
"with direct calls to dataPriv methods, these can be deprecated.",0
Gets all values,0
Support: IE 11 only,0
The attrs elements can be null (#14894),0
Sets multiple values,0
The calling jQuery object (element matches) is not empty,0
(and therefore has an element appears at this[ 0 ]) and the,0
`value` parameter was not undefined. An empty jQuery object,0
will result in `undefined` for elem = this[ 0 ] which will,0
throw an exception if an attempt to read a data cache is made.,0
Attempt to get data from the cache,0
The key will always be camelCased in Data,0
"Attempt to ""discover"" the data in",0
HTML5 custom data-* attrs,0
"We tried really hard, but the data doesn't exist.",0
Set the data...,0
We always store the camelCased key,0
Convert String-formatted options into Object-formatted ones,0
Convert options from String-formatted to Object-formatted if needed,0
(we check in cache first),0
Last fire value for non-forgettable lists,0
Flag to know if list was already fired,0
Flag to prevent firing,0
Actual callback list,0
Queue of execution data for repeatable lists,0
Index of currently firing callback (modified by add/remove as needed),0
Fire callbacks,0
Enforce single-firing,0
"Execute callbacks for all pending executions,",0
respecting firingIndex overrides and runtime changes,0
Run callback and check for early termination,0
Jump to end and forget the data so .add doesn't re-fire,0
Forget the data if we're done with it,0
Clean up if we're done firing for good,0
Keep an empty list if we have data for future add calls,0
"Otherwise, this object is spent",0
Actual Callbacks object,0
Add a callback or a collection of callbacks to the list,0
"If we have memory from a past run, we should fire after adding",0
Inspect recursively,0
Remove a callback from the list,0
Handle firing indexes,0
Check if a given callback is in the list.,0
"If no argument is given, return whether or not list has callbacks attached.",0
Remove all callbacks from the list,0
Disable .fire and .add,0
Abort any current/pending executions,0
Clear all callbacks and values,0
Disable .fire,0
Also disable .add unless we have memory (since it would have no effect),0
Abort any pending executions,0
Call all callbacks with the given context and arguments,0
Call all the callbacks with the given arguments,0
To know if the callbacks have already been called at least once,0
Support: IE <=9 - 11+,0
"focus() and blur() are asynchronous, except when they are no-op.",0
"So expect focus to be synchronous when the element is already active,",0
and blur to be synchronous when the element is not already active.,0
"(focus and blur are always synchronous in other supported browsers,",0
this just defines when we can count on it).,0
Support: IE <=9 only,0
Accessing document.activeElement can throw unexpectedly,0
https://bugs.jquery.com/ticket/13393,0
Types can be a map of types/handlers,0
"( types-Object, selector, data )",0
"( types-Object, data )",0
"( types, fn )",0
"( types, selector, fn )",0
"( types, data, fn )",0
"Can use an empty set, since event contains the info",0
Use same guid so caller can remove using origFn,0
Don't attach events to noData or text/comment nodes (but allow plain objects),0
Caller can pass in an object of custom data in lieu of the handler,0
Ensure that invalid selectors throw exceptions at attach time,0
"Evaluate against documentElement in case elem is a non-element node (e.g., document)",0
"Make sure that the handler has a unique ID, used to find/remove it later",0
"Init the element's event structure and main handler, if this is the first",0
Discard the second event of a jQuery.event.trigger() and,0
when an event is called after a page has unloaded,0
Handle multiple events separated by a space,0
"There *must* be a type, no attaching namespace-only handlers",0
"If event changes its type, use the special event handlers for the changed type",0
"If selector defined, determine special event api type, otherwise given type",0
Update special based on newly reset type,0
handleObj is passed to all event handlers,0
Init the event handler queue if we're the first,0
Only use addEventListener if the special events handler returns false,0
"Add to the element's handler list, delegates in front",0
"Keep track of which events have ever been used, for event optimization",0
Detach an event or set of events from an element,0
Once for each type.namespace in types; type may be omitted,0
"Unbind all events (on this namespace, if provided) for the element",0
Remove matching events,0
Remove generic event handler if we removed something and no more handlers exist,0
(avoids potential for endless recursion during removal of special event handlers),0
Remove data and the expando if it's no longer used,0
Make a writable jQuery.Event from the native event object,0
Use the fix-ed jQuery.Event rather than the (read-only) native event,0
"Call the preDispatch hook for the mapped type, and let it bail if desired",0
Determine handlers,0
Run delegates first; they may want to stop propagation beneath us,0
"If the event is namespaced, then each handler is only invoked if it is",0
specially universal or its namespaces are a superset of the event's.,0
Call the postDispatch hook for the mapped type,0
Find delegate handlers,0
Support: IE <=9,0
Black-hole SVG <use> instance trees (trac-13180),0
Support: Firefox <=42,0
Suppress spec-violating clicks indicating a non-primary pointer button (trac-3861),0
https://www.w3.org/TR/DOM-Level-3-Events/#event-type-click,0
Support: IE 11 only,0
"...but not arrow key ""clicks"" of radio inputs, which can have `button` -1 (gh-2343)",0
Don't check non-elements (#13208),0
"Don't process clicks on disabled elements (#6911, #8165, #11382, #11764)",0
Don't conflict with Object.prototype properties (#13203),0
Add the remaining (directly-bound) handlers,0
Prevent triggered image.load events from bubbling to window.load,0
Utilize native event to ensure correct state for checkable inputs,0
"For mutual compressibility with _default, replace `this` access with a local var.",0
`|| data` is dead code meant only to preserve the variable through minification.,0
Claim the first handler,0
"dataPriv.set( el, ""click"", ... )",0
Return false to allow normal processing in the caller,0
"For mutual compressibility with _default, replace `this` access with a local var.",0
`|| data` is dead code meant only to preserve the variable through minification.,0
Force setup before triggering a click,0
Return non-false to allow normal event-path propagation,0
"For cross-browser consistency, suppress native .click() on links",0
Also prevent it if we're currently inside a leveraged native-event stack,0
Support: Firefox 20+,0
Firefox doesn't alert if the returnValue field is not set.,0
Ensure the presence of an event listener that handles manually-triggered,0
synthetic events by interrupting progress until reinvoked in response to,0
"*native* events that it fires directly, ensuring that state changes have",0
already occurred before other listeners are invoked.,0
"Missing expectSync indicates a trigger call, which must force setup through jQuery.event.add",0
Register the controller as a special universal handler for all event namespaces,0
Interrupt processing of the outer synthetic .trigger()ed event,0
Store arguments for use when handling the inner native event,0
Trigger the native event and capture its result,0
Support: IE <=9 - 11+,0
focus() and blur() are asynchronous,0
Cancel the outer synthetic event,0
If this is an inner synthetic event for an event with a bubbling surrogate,0
"(focus or blur), assume that the surrogate already propagated from triggering the",0
native event and prevent that from happening again here.,0
This technically gets the ordering wrong w.r.t. to `.trigger()` (in which the,0
"bubbling surrogate propagates *after* the non-bubbling base), but that seems",0
less bad than duplication.,0
"If this is a native event triggered above, everything is now in order",0
Fire an inner synthetic event with the original arguments,0
...and capture the result,0
Support: IE <=9 - 11+,0
Extend with the prototype to reset the above stopImmediatePropagation(),0
Abort handling of the native event,0
"This ""if"" is needed for plain objects",0
Allow instantiation without the 'new' keyword,0
Event object,0
Events bubbling up the document may have been marked as prevented,0
by a handler lower down the tree; reflect the correct value.,0
Support: Android <=2.3 only,0
Create target properties,0
Support: Safari <=6 - 7 only,0
"Target should not be a text node (#504, #13143)",0
Event type,0
Put explicitly provided properties onto the event object,0
Create a timestamp if incoming event doesn't have one,0
Mark it as fixed,0
jQuery.Event is based on DOM3 Events as specified by the ECMAScript Language Binding,0
https://www.w3.org/TR/2003/WD-DOM-Level-3-Events-20030331/ecma-script-binding.html,0
Includes all common event props including KeyEvent and MouseEvent specific props,0
Add which for key events,0
Add which for click: 1 === left; 2 === middle; 3 === right,0
Utilize native event if possible so blur/focus sequence is correct,0
Claim the first handler,0
"dataPriv.set( this, ""focus"", ... )",0
"dataPriv.set( this, ""blur"", ... )",0
Return false to allow normal processing in the caller,0
Force setup before trigger,0
Return non-false to allow normal event-path propagation,0
Create mouseenter/leave events using mouseover/out and event-time checks,0
so that event delegation works in jQuery.,0
Do the same for pointerenter/pointerleave and pointerover/pointerout,0
,0
Support: Safari 7 only,0
Safari sends mouseenter too often; see:,0
https://bugs.chromium.org/p/chromium/issues/detail?id=470258,0
for the description of the bug (it existed in older Chrome versions as well).,0
For mouseenter/leave call the handler if related is outside the target.,0
NB: No relatedTarget if the mouse left/entered the browser window,0
( event )  dispatched jQuery.Event,0
"( types-object [, selector] )",0
"( types [, fn] )",0
Defining this global in .eslintrc.json would create a danger of using the global,0
"unguarded in another place, it seems safer to define global only for this module",0
Define a local copy of jQuery,0
The jQuery object is actually just the init constructor 'enhanced',0
Need init if jQuery is called (just allow error to be thrown if not included),0
Support: Android <=4.0 only,0
Make sure we trim BOM and NBSP,0
The current version of jQuery being used,0
The default length of a jQuery object is 0,0
Get the Nth element in the matched element set OR,0
Get the whole matched element set as a clean array,0
Return all the elements in a clean array,0
Return just the one element from the set,0
Take an array of elements and push it onto the stack,0
(returning the new matched element set),0
Build a new jQuery matched element set,0
Add the old object onto the stack (as a reference),0
Return the newly-formed element set,0
Execute a callback for every element in the matched set.,0
For internal use only.,0
"Behaves like an Array's method, not like a jQuery method.",0
Handle a deep copy situation,0
Skip the boolean and the target,0
Handle case when target is a string or something (possible in deep copy),0
Extend jQuery itself if only one argument is passed,0
Only deal with non-null/undefined values,0
Extend the base object,0
Prevent Object.prototype pollution,0
Prevent never-ending loop,0
Recurse if we're merging plain objects or arrays,0
Ensure proper type for the source value,0
"Never move original objects, clone them",0
Don't bring in undefined values,0
Return the modified object,0
Unique for each copy of jQuery on the page,0
Assume jQuery is ready without the ready module,0
Detect obvious negatives,0
Use toString instead of jQuery.type to catch host objects,0
"Objects with no prototype (e.g., `Object.create( null )`) are plain",0
Objects with prototype are plain iff they were constructed by a global Object function,0
Evaluates a script in a global context,0
Support: Android <=4.0 only,0
results is for internal usage only,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
"push.apply(_, arraylike) throws on ancient WebKit",0
"Go through the array, only saving the items",0
that pass the validator function,0
arg is for internal usage only,0
"Go through the array, translating each of the items to their new values",0
"Go through every key on the object,",0
Flatten any nested arrays,0
A global GUID counter for objects,0
jQuery.support is not used in Core but other projects attach their,0
properties to it so it needs to exist.,0
Populate the class2type map,0
Support: real iOS 8.2 only (not reproducible in simulator),0
`in` check used to prevent JIT error (gh-2145),0
hasOwn isn't used here due to false negatives,0
regarding Nodelist length in IE,0
"Create innerHeight, innerWidth, height, width, outerHeight and outerWidth methods",0
"Margin is only for outerHeight, outerWidth",0
$( window ).outerWidth/Height return w/h including scrollbars (gh-1729),0
Get document width or height,0
"Either scroll[Width/Height] or offset[Width/Height] or client[Width/Height],",0
whichever is greatest,0
"Get width or height on the element, requesting but not forcing parseFloat",0
Set width or height on the element,0
Methods guaranteed to produce a unique set when starting from a unique set,0
"Positional selectors never match, since there's no _selection_ context",0
Always skip document fragments,0
Don't pass non-elements to Sizzle,0
Determine the position of an element within the set,0
"No argument, return index in parent",0
Index in selector,0
Locate the position of the desired element,0
"If it receives a jQuery object, the first element is used",0
"Support: IE 9 - 11 only, iOS 7 only, Android Browser <=4.3 only",0
Treat the template element as a regular one in browsers that,0
don't support it.,0
Remove duplicates,0
Reverse order for parents* and prev-derivatives,0
Return jQuery for attributes-only inclusion,0
Speed up dequeue by getting out quickly if this is just a lookup,0
"If the fx queue is dequeued, always remove the progress sentinel",0
Add a progress sentinel to prevent the fx queue from being,0
automatically dequeued,0
Clear up the last queue stop function,0
"Not public - generate a queueHooks object, or return the current one",0
Ensure a hooks for this queue,0
Get a promise resolved when queues of a certain type,0
are emptied (fx is the type by default),0
The elements to wrap the target around,0
Serialize array item.,0
Treat each array item as a scalar.,0
"Item is non-scalar (array or object), encode its numeric index.",0
Serialize object item.,0
Serialize scalar item.,0
Serialize an array of form elements or a set of,0
key/values into a query string,0
"If value is a function, invoke it and use its return value",0
"If an array was passed in, assume that it is an array of form elements.",0
Serialize the form elements,0
"If traditional, encode the ""old"" way (the way 1.3.2 or older",0
"did it), otherwise encode params recursively.",0
Return the resulting serialization,0
"Can add propHook for ""elements"" to filter or add form elements",0
"Use .is( "":disabled"" ) so that fieldset[disabled] works",0
"Set position first, in-case top/left are set even on static elem",0
Need to be able to calculate position if either,0
top or left is auto and position is either absolute or fixed,0
Use jQuery.extend here to allow modification of coordinates argument (gh-1848),0
offset() relates an element's border box to the document origin,0
Preserve chaining for setter,0
Return zeros for disconnected and hidden (display: none) elements (gh-2310),0
Support: IE <=11 only,0
Running getBoundingClientRect on a,0
disconnected node in IE throws an error,0
Get document-relative position by adding viewport scroll to viewport-relative gBCR,0
position() relates an element's margin box to its offset parent's padding box,0
This corresponds to the behavior of CSS absolute positioning,0
"position:fixed elements are offset from the viewport, which itself always has zero offset",0
Assume position:fixed implies availability of getBoundingClientRect,0
"Account for the *real* offset parent, which can be the document or its root element",0
when a statically positioned element is identified,0
"Incorporate borders into its offset, since they are outside its content origin",0
Subtract parent offsets and element margins,0
This method will return documentElement in the following cases:,0
"1) For the element inside the iframe without offsetParent, this method will return",0
documentElement of the parent window,0
2) For the hidden or detached element,0
"3) For body or html element, i.e. in case of the html node - it will return itself",0
,0
but those exceptions were never presented as a real life use-cases,0
and might be considered as more preferable results.,0
,0
"This logic, however, is not guaranteed and can change at any point in the future",0
Create scrollLeft and scrollTop methods,0
Coalesce documents and windows,0
"Support: Safari <=7 - 9.1, Chrome <=37 - 49",0
Add the top/left cssHooks using jQuery.fn.position,0
Webkit bug: https://bugs.webkit.org/show_bug.cgi?id=29084,0
Blink bug: https://bugs.chromium.org/p/chromium/issues/detail?id=589347,0
getComputedStyle returns percent when specified for top/left/bottom/right;,0
"rather than make the css module depend on the offset module, just check for it here",0
"If curCSS returns percentage, fallback to offset",0
"#7653, #8125, #8152: local protocol detection",0
Avoid comment-prolog char sequence (#10098); must appease lint and evade compression,0
Anchor tag for parsing the document origin,0
"Base ""constructor"" for jQuery.ajaxPrefilter and jQuery.ajaxTransport",0
"dataTypeExpression is optional and defaults to ""*""",0
For each dataType in the dataTypeExpression,0
Prepend if requested,0
Otherwise append,0
Base inspection function for prefilters and transports,0
A special extend for ajax options,0
"that takes ""flat"" options (not to be deep extended)",0
Fixes #9887,0
Remove auto dataType and get content-type in the process,0
Check if we're dealing with a known content-type,0
Check to see if we have a response for the expected dataType,0
Try convertible dataTypes,0
Or just use first one,0
If we found a dataType,0
We add the dataType to the list if needed,0
and return the corresponding response,0
Work with a copy of dataTypes in case we need to modify it for conversion,0
Create converters map with lowercased keys,0
Convert to each sequential dataType,0
Apply the dataFilter if provided,0
There's only work to do if current dataType is non-auto,0
Convert response if prev dataType is non-auto and differs from current,0
Seek a direct converter,0
"If none found, seek a pair",0
If conv2 outputs current,0
If prev can be converted to accepted input,0
Condense equivalence converters,0
"Otherwise, insert the intermediate dataType",0
Apply converter (if not an equivalence),0
"Unless errors are allowed to bubble, catch and return them",0
Counter for holding the number of active queries,0
Last-Modified header cache for next request,0
Data converters,0
"Keys separate source (or catchall ""*"") and destination types with a single space",0
Convert anything to text,0
Text to html (true = no transformation),0
Evaluate text as a json expression,0
Parse text as xml,0
For options that shouldn't be deep extended:,0
you can add your own custom options here if,0
and when you create one that shouldn't be,0
deep extended (see ajaxExtend),0
Creates a full fledged settings object into target,0
with both ajaxSettings and settings fields.,0
"If target is omitted, writes into ajaxSettings.",0
Building a settings object,0
Extending ajaxSettings,0
Main method,0
"If url is an object, simulate pre-1.5 signature",0
Force options to be an object,0
URL without anti-cache param,0
Response headers,0
timeout handle,0
Url cleanup var,0
Request state (becomes false upon send and true upon completion),0
To know if global events are to be dispatched,0
Loop variable,0
uncached part of the url,0
Create the final options object,0
Callbacks context,0
Context for global events is callbackContext if it is a DOM node or jQuery collection,0
Deferreds,0
Status-dependent callbacks,0
Headers (they are sent all at once),0
Default abort message,0
Fake xhr,0
Builds headers hashtable if needed,0
Raw string,0
Caches the header,0
Overrides response content-type header,0
Status-dependent callbacks,0
Execute the appropriate callbacks,0
Lazy-add the new callbacks in a way that preserves old ones,0
Cancel the request,0
Attach deferreds,0
Add protocol if not provided (prefilters might expect it),0
Handle falsy url in the settings object (#10093: consistency with old signature),0
We also use the url parameter if available,0
Alias method option to type as per ticket #12004,0
Extract dataTypes list,0
A cross-domain request is in order when the origin doesn't match the current origin.,0
"Support: IE <=8 - 11, Edge 12 - 15",0
"IE throws exception on accessing the href property if url is malformed,",0
e.g. http://example.com:80x/,0
Support: IE <=8 - 11 only,0
Anchor's host property isn't correctly set when s.url is relative,0
"If there is an error parsing the URL, assume it is crossDomain,",0
it can be rejected by the transport if it is invalid,0
Convert data if not already a string,0
Apply prefilters,0
"If request was aborted inside a prefilter, stop there",0
We can fire global events as of now if asked to,0
Don't fire events if jQuery.event is undefined in an AMD-usage scenario (#15118),0
Watch for a new set of requests,0
Uppercase the type,0
Determine if request has content,0
Save the URL in case we're toying with the If-Modified-Since,0
and/or If-None-Match header later on,0
Remove hash to simplify url manipulation,0
More options handling for requests with no content,0
Remember the hash so we can put it back,0
"If data is available and should be processed, append data to url",0
#9682: remove data so that it's not used in an eventual retry,0
Add or update anti-cache param if needed,0
Put hash and anti-cache on the URL that will be requested (gh-1732),0
Change '%20' to '+' if this is encoded form body content (gh-2658),0
"Set the If-Modified-Since and/or If-None-Match header, if in ifModified mode.",0
"Set the correct header, if data is being sent",0
"Set the Accepts header for the server, depending on the dataType",0
Check for headers option,0
Allow custom headers/mimetypes and early abort,0
Abort if not done already and return,0
Aborting is no longer a cancellation,0
Install callbacks on deferreds,0
Get transport,0
"If no transport, we auto-abort",0
Send global event,0
"If request was aborted inside ajaxSend, stop there",0
Timeout,0
Rethrow post-completion exceptions,0
Propagate others as results,0
Callback for when everything is done,0
Ignore repeat invocations,0
Clear timeout if it exists,0
Dereference transport for early garbage collection,0
(no matter how long the jqXHR object will be used),0
Cache response headers,0
Set readyState,0
Determine if successful,0
Get response data,0
Convert no matter what (that way responseXXX fields are always set),0
"If successful, handle type chaining",0
"Set the If-Modified-Since and/or If-None-Match header, if in ifModified mode.",0
if no content,0
if not modified,0
"If we have data, let's convert it",0
Extract error from statusText and normalize for non-aborts,0
Set data for the fake xhr object,0
Success/Error,0
Status-dependent callbacks,0
Complete,0
Handle the global AJAX counter,0
Shift arguments if data argument was omitted,0
The url can be an options object (which then must have .url),0
"( namespace ) or ( selector, types [, fn] )",0
"Bind a function to a context, optionally partially applying any",0
arguments.,0
jQuery.proxy is deprecated to promote standards (specifically Function#bind),0
"However, it is not slated for removal any time soon",0
"Quick check to determine if target is callable, in the spec",0
"this throws a TypeError, but we will just return undefined.",0
Simulated bind,0
"Set the guid of unique handler to the same of original handler, so it can be removed",0
"As of jQuery 3.0, isNumeric is limited to",0
strings and numbers (primitives or objects),0
that can be coerced to finite numbers (gh-2662),0
"parseFloat NaNs numeric-cast false positives ("""")",0
"...but misinterprets leading-number strings, particularly hex literals (""0x..."")",0
subtraction forces infinities to NaN,0
CSS string/identifier serialization,0
https://drafts.csswg.org/cssom/#common-serializing-idioms,0
U+0000 NULL becomes U+FFFD REPLACEMENT CHARACTER,0
Control characters and (dependent upon position) numbers get escaped as code points,0
Other potentially-special ASCII characters get backslash-escaped,0
Flag for duplicate removal,0
Sort on method existence if only one input has compareDocumentPosition,0
Calculate position if both inputs belong to the same document,0
Otherwise we know they are disconnected,0
Disconnected nodes,0
Choose the first element that is related to our preferred document,0
Maintain original order,0
Clear input after sorting to release objects,0
See https://github.com/jquery/sizzle/pull/225,0
Same basic safeguard as Sizzle,0
Early return if context is not an element or document,0
"If no nodeType, this is expected to be an array",0
Do not traverse comment nodes,0
Use textContent for elements,0
Do not include comment or processing instruction nodes,0
documentElement is verified for cases where it doesn't yet exist,0
(such as loading iframes in IE - #4833),0
Don't get fooled by Object.prototype properties (jQuery #13807),0
Deprecated,0
Animations created synchronously will run synchronously,0
Generate parameters to create a standard animation,0
"If we include width, step value is 1 to do all cssExpand values,",0
otherwise step value is 2 to skip over Left and Right,0
We're done with this property,0
Queue-skipping animations hijack the fx hooks,0
Ensure the complete handler is called before this completes,0
Detect show/hide animations,0
"Pretend to be hidden if this is a ""show"" and",0
there is still data from a stopped show/hide,0
Ignore all other no-op show/hide data,0
Bail out if this is a no-op like .hide().hide(),0
"Restrict ""overflow"" and ""display"" styles during box animations",0
"Support: IE <=9 - 11, Edge 12 - 15",0
Record all 3 overflow attributes because IE does not infer the shorthand,0
from identically-valued overflowX and overflowY and Edge just mirrors,0
the overflowX value there.,0
"Identify a display type, preferring old show/hide data over the CSS cascade",0
Get nonempty value(s) by temporarily forcing visibility,0
Animate inline elements as inline-block,0
Restore the original display value at the end of pure show/hide animations,0
Implement show/hide animations,0
General show/hide setup for this element animation,0
"Store hidden/visible for toggle so `.stop().toggle()` ""reverses""",0
Show elements before animating them,0
"The final step of a ""hide"" animation is actually hiding the element",0
Per-property setup,0
"camelCase, specialEasing and expand cssHook pass",0
"Not quite $.extend, this won't overwrite existing keys.",0
"Reusing 'index' because we have the correct ""name""",0
Don't match elem in the :animated selector,0
Support: Android 2.3 only,0
Archaic crash bug won't allow us to use `1 - ( 0.5 || 0 )` (#12497),0
"If there's more to do, yield",0
"If this was an empty animation, synthesize a final progress notification",0
Resolve the animation and report its conclusion,0
"If we are going to the end, we want to run all the tweens",0
otherwise we skip this part,0
"Resolve when we played the last frame; otherwise, reject",0
Attach callbacks from options,0
Go to the end state if fx are off,0
"Normalize opt.queue - true/undefined/null -> ""fx""",0
Queueing,0
Show any hidden elements after setting opacity to 0,0
Animate to the value specified,0
Operate on a copy of prop so per-property easing won't be lost,0
"Empty animations, or finishing resolves immediately",0
Start the next in the queue if the last step wasn't forced.,0
"Timers currently will call their complete callbacks, which",0
will dequeue but only if they were gotoEnd.,0
Enable finishing flag on private data,0
Empty the queue first,0
"Look for any active animations, and finish them",0
Look for any animations in the old queue and finish them,0
Turn off finishing flag,0
Generate shortcuts for custom animations,0
Run the timer and safely remove it when done (allowing for external removal),0
Default speed,0
Swappable if display is none or starts with table,0
"except ""table"", ""table-cell"", or ""table-caption""",0
See here for display values: https://developer.mozilla.org/en-US/docs/CSS/display,0
Any relative (+/-) values have already been,0
normalized at this point,0
"Guard against undefined ""subtract"", e.g., when used as in cssHooks",0
Adjustment may not be necessary,0
Both box models exclude margin,0
"If we get here with a content-box, we're seeking ""padding"" or ""border"" or ""margin""",0
Add padding,0
"For ""border"" or ""margin"", add border",0
But still keep track of it otherwise,0
"If we get here with a border-box (content + padding + border), we're seeking ""content"" or",0
"""padding"" or ""margin""",0
"For ""content"", subtract padding",0
"For ""content"" or ""padding"", subtract border",0
Account for positive content-box scroll gutter when requested by providing computedVal,0
"offsetWidth/offsetHeight is a rounded sum of content, padding, scroll gutter, and border",0
"Assuming integer scroll gutter, subtract the rest and round down",0
"If offsetWidth/offsetHeight is unknown, then we can't determine content-box scroll gutter",0
Use an explicit zero to avoid NaN (gh-3964),0
Start with computed style,0
"To avoid forcing a reflow, only fetch boxSizing if we need it (gh-4322).",0
Fake content-box until we know it's needed to know the true value.,0
Support: Firefox <=54,0
"Return a confounding non-pixel value or feign ignorance, as appropriate.",0
"Fall back to offsetWidth/offsetHeight when value is ""auto""",0
This happens for inline elements with no explicit setting (gh-3571),0
Support: Android <=4.1 - 4.3 only,0
Also use offsetWidth/offsetHeight for misreported inline dimensions (gh-3602),0
Support: IE 9-11 only,0
Also use offsetWidth/offsetHeight for when box sizing is unreliable,0
We use getClientRects() to check for hidden/disconnected.,0
"In those cases, the computed value can be trusted to be border-box",0
"Where available, offsetWidth/offsetHeight approximate border box dimensions.",0
"Where not available (e.g., SVG), assume unreliable box-sizing and interpret the",0
retrieved value as a content box dimension.,0
"Normalize """" and auto",0
Adjust for the element's box model,0
Provide the current computed size to request scroll gutter calculation (gh-3589),0
Add in style property hooks for overriding the default,0
behavior of getting and setting a style property,0
We should always get a number back from opacity,0
"Don't automatically add ""px"" to these possibly-unitless properties",0
Add in properties whose names you wish to fix before,0
setting or getting the value,0
Get and set the style property on a DOM Node,0
Don't set styles on text and comment nodes,0
Make sure that we're working with the right name,0
Make sure that we're working with the right name. We don't,0
want to query the value if it is a CSS custom property,0
since they are user-defined.,0
"Gets hook for the prefixed version, then unprefixed version",0
Check if we're setting a value,0
"Convert ""+="" or ""-="" to relative numbers (#7345)",0
Fixes bug #9237,0
Make sure that null and NaN values aren't set (#7116),0
"If a number was passed in, add the unit (except for certain CSS properties)",0
The isCustomProp check can be removed in jQuery 4.0 when we only auto-append,0
"""px"" to a few hardcoded values.",0
background-* props affect original clone's values,0
"If a hook was provided, use that value, otherwise just set the specified value",0
If a hook was provided get the non-computed value from there,0
Otherwise just get the value from the style object,0
Make sure that we're working with the right name. We don't,0
want to modify the value if it is a CSS custom property,0
since they are user-defined.,0
Try prefixed name followed by the unprefixed name,0
If a hook was provided get the computed value from there,0
"Otherwise, if a way to get the computed value exists, use that",0
"Convert ""normal"" to computed value",0
Make numeric if forced or a qualifier was provided and val looks numeric,0
Certain elements can have dimension info if we invisibly show them,0
but it must have a current display style that would benefit,0
Support: Safari 8+,0
Table columns in Safari have non-zero offsetWidth & zero,0
getBoundingClientRect().width unless display is changed.,0
Support: IE <=11 only,0
Running getBoundingClientRect on a disconnected node,0
in IE throws an error.,0
Only read styles.position if the test has a chance to fail,0
to avoid forcing a reflow.,0
"To avoid forcing a reflow, only fetch boxSizing if we need it (gh-3991)",0
Account for unreliable border-box dimensions by comparing offset* to computed and,0
faking a content-box to get border and padding (gh-3699),0
Convert to pixels if value adjustment is needed,0
These hooks are used by animate to expand properties,0
Assumes a single number if not a string,0
Check for promise aspect first to privilege synchronous behavior,0
Other thenables,0
Other non-thenables,0
Control `resolve` arguments by letting Array#slice cast boolean `noValue` to integer:,0
* false: [ value ].slice( 0 ) => resolve( value ),0
* true: [ value ].slice( 1 ) => resolve(),0
"For Promises/A+, convert exceptions into rejections",0
"Since jQuery.when doesn't unwrap thenables, we can skip the extra checks appearing in",0
Deferred#then to conditionally suppress rejection.,0
Support: Android 4.0 only,0
Strict mode functions invoked without .call/.apply get global-object context,0
"action, add listener, callbacks,",0
"... .then handlers, argument index, [final state]",0
Keep pipe for back-compat,0
"Map tuples (progress, done, fail) to arguments (done, fail, progress)",0
deferred.progress(function() { bind to newDefer or newDefer.notify }),0
deferred.done(function() { bind to newDefer or newDefer.resolve }),0
deferred.fail(function() { bind to newDefer or newDefer.reject }),0
Support: Promises/A+ section 2.3.3.3.3,0
https://promisesaplus.com/#point-59,0
Ignore double-resolution attempts,0
Support: Promises/A+ section 2.3.1,0
https://promisesaplus.com/#point-48,0
"Support: Promises/A+ sections 2.3.3.1, 3.5",0
https://promisesaplus.com/#point-54,0
https://promisesaplus.com/#point-75,0
Retrieve `then` only once,0
Support: Promises/A+ section 2.3.4,0
https://promisesaplus.com/#point-64,0
Only check objects and functions for thenability,0
Handle a returned thenable,0
Special processors (notify) just wait for resolution,0
Normal processors (resolve) also hook into progress,0
...and disregard older resolution values,0
Handle all other returned values,0
Only substitute handlers pass on context,0
and multiple values (non-spec behavior),0
Process the value(s),0
Default process is resolve,0
Only normal processors (resolve) catch and reject exceptions,0
Support: Promises/A+ section 2.3.3.3.4.1,0
https://promisesaplus.com/#point-61,0
Ignore post-resolution exceptions,0
Only substitute handlers pass on context,0
and multiple values (non-spec behavior),0
Support: Promises/A+ section 2.3.3.3.1,0
https://promisesaplus.com/#point-57,0
Re-resolve promises immediately to dodge false rejection from,0
subsequent errors,0
"Call an optional hook to record the stack, in case of exception",0
since it's otherwise lost when execution goes async,0
progress_handlers.add( ... ),0
fulfilled_handlers.add( ... ),0
rejected_handlers.add( ... ),0
Get a promise for this deferred,0
"If obj is provided, the promise aspect is added to the object",0
Add list-specific methods,0
promise.progress = list.add,0
promise.done = list.add,0
promise.fail = list.add,0
Handle state,0
"state = ""resolved"" (i.e., fulfilled)",0
"state = ""rejected""",0
rejected_callbacks.disable,0
fulfilled_callbacks.disable,0
rejected_handlers.disable,0
fulfilled_handlers.disable,0
progress_callbacks.lock,0
progress_handlers.lock,0
progress_handlers.fire,0
fulfilled_handlers.fire,0
rejected_handlers.fire,0
deferred.notify = function() { deferred.notifyWith(...) },0
deferred.resolve = function() { deferred.resolveWith(...) },0
deferred.reject = function() { deferred.rejectWith(...) },0
deferred.notifyWith = list.fireWith,0
deferred.resolveWith = list.fireWith,0
deferred.rejectWith = list.fireWith,0
Make the deferred a promise,0
Call given func if any,0
All done!,0
Deferred helper,0
count of uncompleted subordinates,0
count of unprocessed arguments,0
subordinate fulfillment data,0
the master Deferred,0
subordinate callback factory,0
Single- and empty arguments are adopted like Promise.resolve,0
Use .then() to unwrap secondary thenables (cf. gh-3000),0
Multiple arguments are aggregated like Promise.all array elements,0
See https://github.com/eslint/eslint/issues/3229,0
"Support: IE <=10 - 11, Edge 12 - 13 only",0
In IE/Edge using regex groups here causes severe slowdowns.,0
See https://connect.microsoft.com/IE/feedback/details/1736512/,0
"checked=""checked"" or checked",0
Prefer a tbody over its parent table for containing new rows,0
Replace/restore the type attribute of script elements for safe DOM manipulation,0
"1. Copy private data: events, handlers, etc.",0
2. Copy user data,0
"Fix IE bugs, see support tests",0
Fails to persist the checked state of a cloned checkbox or radio button.,0
Fails to return the selected option to the default selected state when cloning options,0
Flatten any nested arrays,0
"We can't cloneNode fragments that contain checked, in WebKit",0
Require either new content or an interest in ignored elements to invoke the callback,0
Use the original fragment for the last item,0
instead of the first because it can end up,0
being emptied incorrectly in certain situations (#8070).,0
Keep references to cloned scripts for later restoration,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
"push.apply(_, arraylike) throws on ancient WebKit",0
Reenable scripts,0
Evaluate executable scripts on first document insertion,0
"Optional AJAX dependency, but won't run scripts if not present",0
Fix IE cloning issues,0
We eschew Sizzle here for performance reasons: https://jsperf.com/getall-vs-sizzle/2,0
Copy the events from the original to the clone,0
Preserve script evaluation history,0
Return the cloned set,0
This is a shortcut to avoid jQuery.event.remove's overhead,0
Support: Chrome <=35 - 45+,0
"Assign undefined instead of using delete, see Data#remove",0
Support: Chrome <=35 - 45+,0
"Assign undefined instead of using delete, see Data#remove",0
Prevent memory leaks,0
Remove any remaining nodes,0
See if we can take a shortcut and just use innerHTML,0
Remove element nodes and prevent memory leaks,0
"If using innerHTML throws an exception, use the fallback method",0
"Make the changes, replacing each non-ignored context element with the new content",0
Force callback invocation,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
".get() because push.apply(_, arraylike) throws on ancient WebKit",0
"Define the hook, we'll check on the first run if it's really needed.",0
Hook not needed (or it's not possible to use it due,0
"to missing dependency), remove it.",0
Hook needed; redefine it so that the support test is not executed again.,0
Executing both pixelPosition & boxSizingReliable tests require only one layout,0
so they're executed at the same time to save the second computation.,0
"This is a singleton, we need to execute it only once",0
"Support: Android 4.0 - 4.3 only, Firefox <=3 - 44",0
"Support: Android 4.0 - 4.3 only, Safari <=9.1 - 10.1, iOS <=7.0 - 9.3",0
"Some styles come back with percentage values, even though they shouldn't",0
Support: IE 9 - 11 only,0
Detect misreporting of content dimensions for box-sizing:border-box elements,0
Support: IE 9 only,0
Detect overflow:scroll screwiness (gh-3699),0
Support: Chrome <=64,0
Don't get tricked when zoom affects offsetWidth (gh-4029),0
Nullify the div so it wouldn't be stored in the memory and,0
it will also be a sign that checks already performed,0
Finish early in limited (non-browser) environments,0
Support: IE <=9 - 11 only,0
Style of cloned element affects source element cloned (#8908),0
Support: Firefox 51+,0
Retrieving style before computed somehow,0
fixes an issue with getting wrong values,0
on detached elements,0
getPropertyValue is needed for:,0
".css('filter') (IE 9 only, #12537)",0
.css('--customProperty) (#3144),0
"A tribute to the ""awesome hack by Dean Edwards""",1
"Android Browser returns percentage for some values,",0
but width seems to be reliably pixels.,0
This is against the CSSOM draft spec:,0
https://drafts.csswg.org/cssom/#resolved-values,0
Remember the original values,0
Put in the new values to get a computed value out,0
Revert the changed values,0
Support: IE <=9 - 11 only,0
IE returns zIndex value as an integer.,0
Determine new display value for elements that need to change,0
"Since we force visibility upon cascade-hidden elements, an immediate (and slow)",0
check is required in this first loop unless we have a nonempty display value (either,0
inline or about-to-be-restored),0
Remember what we're overwriting,0
Set the display of the elements in a second loop to avoid constant reflow,0
Starting value computation is required for potential unit mismatches,0
Support: Firefox <=54,0
Halve the iteration target value to prevent interference from CSS upper bounds (gh-2144),0
Trust units reported by jQuery.css,0
Iteratively approximate from a nonzero starting point,0
Evaluate and update our best guess (doubling guesses that zero out).,0
Finish if the scale equals or crosses 1 (making the old*new product non-positive).,0
Make sure we update the tween properties later on,0
Apply relative offset (+=/-=) if specified,0
"Support: IE <=11 only, Firefox <=30 (#15098, #14150)",0
IE throws on elements created in popups,0
"FF meanwhile throws on frame elements through ""defaultView.getComputedStyle""",0
css is assumed,0
"isHiddenWithinTree reports if an element has a non-""none"" display style (inline and/or",0
"through the CSS cascade), which is useful in deciding whether or not to make it visible.",0
It differs from the :hidden selector (jQuery.expr.pseudos.hidden) in two important ways:,0
* A hidden ancestor does not force an element to be classified as hidden.,0
* Being disconnected from the document does not force an element to be classified as hidden.,0
These differences improve the behavior of .toggle() et al. when applied to elements that are,0
"detached or contained within hidden ancestors (gh-2404, gh-2863).",0
isHiddenWithinTree might be called from jQuery#filter function;,0
"in that case, element will be second argument",0
Inline style trumps all,0
"Otherwise, check computed style",0
Support: Firefox <=43 - 45,0
"Disconnected elements can have computed display: none, so first confirm that elem is",0
in the document.,0
A method for quickly swapping in/out CSS properties to get correct calculations.,0
"Remember the old values, and insert the new ones",0
Revert the old values,0
Don't do events on text and comment nodes,0
focus/blur morphs to focusin/out; ensure we're not firing them right now,0
Namespaced trigger; create a regexp to match event type in handle(),0
"Caller can pass in a jQuery.Event object, Object, or just an event type string",0
Trigger bitmask: & 1 for native handlers; & 2 for jQuery (always true),0
Clean up the event in case it is being reused,0
"Clone any incoming data and prepend the event, creating the handler arg list",0
Allow special events to draw outside the lines,0
"Determine event propagation path in advance, per W3C events spec (#9951)",0
"Bubble up to document, then to window; watch for a global ownerDocument var (#9724)",0
"Only add window if we got to document (e.g., not plain obj or detached DOM)",0
Fire handlers on the event path,0
jQuery handler,0
Native handler,0
"If nobody prevented the default action, do it now",0
Call a native DOM method on the target with the same name as the event.,0
"Don't do default actions on window, that's where global variables be (#6170)",0
Don't re-trigger an onFOO event when we call its FOO() method,0
"Prevent re-triggering of the same event, since we already bubbled it above",0
Piggyback on a donor event to simulate a different one,0
Used only for `focus(in | out)` events,0
Attach a bunch of functions for handling common AJAX events,0
Support: Firefox <=44,0
Firefox doesn't have focus(in | out) events,0
Related ticket - https://bugzilla.mozilla.org/show_bug.cgi?id=687787,0
,0
"Support: Chrome <=48 - 49, Safari <=9.0 - 9.1",0
"focus(in | out) events fire after focus & blur events,",0
which is spec violation - http://www.w3.org/TR/DOM-Level-3-Events/#events-focusevent-event-order,0
Related ticket - https://bugs.chromium.org/p/chromium/issues/detail?id=449857,0
Attach a single capturing handler on the document while someone wants focusin/focusout,0
Handle event binding,0
"Based off of the plugin by Clint Helfers, with permission.",0
https://web.archive.org/web/20100324014747/http://blindsignals.com/index.php/2009/07/jquery-delay/,0
Support: IE <=9 - 11 only,0
Use typeof to avoid zero-argument method invocation on host objects (#15151),0
Mark scripts as having already been evaluated,0
Support: Android 4.0 - 4.3 only,0
Check state lost if the name is set (#11217),0
Support: Windows Web Apps (WWA),0
`name` and `type` must use .setAttribute for WWA (#14901),0
Support: Android <=4.1 only,0
Older WebKit doesn't clone checked state correctly in fragments,0
Support: IE <=11 only,0
Make sure textarea (and checkbox) defaultValue is properly cloned,0
Add nodes directly,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
"push.apply(_, arraylike) throws on ancient WebKit",0
Convert non-html into a text node,0
Convert html into DOM nodes,0
Deserialize a standard representation,0
Descend through wrappers to the right content,0
"Support: Android <=4.0 only, PhantomJS 1 only",0
"push.apply(_, arraylike) throws on ancient WebKit",0
Remember the top-level container,0
Ensure the created nodes are orphaned (#12392),0
Remove wrapper from fragment,0
Skip elements already in the context collection (trac-4087),0
Append to fragment,0
Preserve script evaluation history,0
Capture executables,0
We have to close these tags to support XHTML (#13200),0
Support: IE <=9 only,0
XHTML parsers do not magically insert elements in the,0
same way that tag soup parsers do. So we cannot shorten,0
this by omitting <tbody> or other required elements.,0
Support: IE <=9 only,0
"Make this explicit, since user can override this through ajaxSetup (#11264)",0
Only evaluate the response if it is successful (gh-4126),0
"dataFilter is not invoked for failure responses, so using it instead",0
of the default converter is kludgy but it works.,0
rtagName captures the name from the first start tag in a string of HTML,0
https://html.spec.whatwg.org/multipage/syntax.html#tag-open-state,0
https://html.spec.whatwg.org/multipage/syntax.html#tag-name-state,0
"These usually indicate a programmer mistake during development,",0
warn about them ASAP rather than swallowing them by default.,0
Support: IE 8 - 9 only,0
"Console exists when dev tools are open, which can happen at any time",0
Check if the owner object already has a cache,0
"If not, create one",0
"We can accept data for non-element nodes in modern browsers,",0
"but we should not, see #8335.",0
Always return an empty object.,0
If it is a node unlikely to be stringify-ed or looped over,0
use plain assignment,0
Otherwise secure it in a non-enumerable property,0
configurable must be true to allow the property to be,0
deleted when data is removed,0
"Handle: [ owner, key, value ] args",0
Always use camelCase key (gh-2257),0
"Handle: [ owner, { properties } ] args",0
Copy the properties one-by-one to the cache object,0
Always use camelCase key (gh-2257),0
In cases where either:,0
,0
1. No key was specified,0
"2. A string key was specified, but no value provided",0
,0
"Take the ""read"" path and allow the get method to determine",0
"which value to return, respectively either:",0
,0
1. The entire cache object,0
2. The data stored at the key,0
,0
"When the key is not a string, or both a key and value",0
"are specified, set or extend (existing objects) with either:",0
,0
1. An object of properties,0
2. A key and value,0
,0
"Since the ""set"" path can have two possible entry points",0
return the expected data based on which path was taken[*],0
Support array or space separated string of keys,0
If key is an array of keys...,0
"We always set camelCase keys, so remove that.",0
"If a key with the spaces exists, use it.",0
"Otherwise, create an array by matching non-whitespace",0
Remove the expando if there's no more data,0
Support: Chrome <=35 - 45,0
Webkit & Blink performance suffers when deleting properties,0
"from DOM nodes, so set to undefined instead",0
https://bugs.chromium.org/p/chromium/issues/detail?id=378607 (bug restricted),0
Accepts only:,0
- Node,0
- Node.ELEMENT_NODE,0
- Node.DOCUMENT_NODE,0
- Object,0
- Any,0
Only count HTML whitespace,0
Other whitespace should count in values,0
https://infra.spec.whatwg.org/#ascii-whitespace,0
[[Class]] -> type pairs,0
All support tests are defined in their respective modules.,0
"Support: Chrome <=57, Firefox <=52",0
"In some browsers, typeof returns ""function"" for HTML <object> elements",0
"(i.e., `typeof document.createElement( ""object"" ) === ""function""`).",0
We don't want to classify *any* DOM node as a function.,0
"Support: Firefox 64+, Edge 18+",0
"Some browsers don't support the ""nonce"" property on scripts.",0
"On the other hand, just using `getAttribute` is not enough as",0
the `nonce` attribute is reset to an empty string whenever it,0
becomes browsing-context connected.,0
See https://github.com/whatwg/html/issues/2369,0
See https://html.spec.whatwg.org/#nonce-attributes,0
The `node.getAttribute` check was added for the sake of,0
`jQuery.globalEval` so that it can fake a nonce-containing node,0
via an object.,0
Support: Safari 8 only,0
In Safari 8 documents created via document.implementation.createHTMLDocument,0
collapse sibling forms: the second one becomes a child of the first one.,0
"Because of that, this security measure has to be disabled in Safari 8.",0
https://bugs.webkit.org/show_bug.cgi?id=137337,0
This is the only module that needs core/support,0
"Argument ""data"" should be string of html",0
"context (optional): If specified, the fragment will be created in this context,",0
defaults to document,0
"keepScripts (optional): If true, will include scripts passed in the html string",0
Stop scripts or inline event handlers from being executed immediately,0
by using document.implementation,0
Set the base href for the created document,0
so any parsed elements with URLs,0
are based on the document's URL (gh-2965),0
Single tag,0
Matches dashed string for camelizing,0
Used by camelCase as callback to replace(),0
Convert dashed to camelCase; used by the css and data modules,0
"Support: IE <=9 - 11, Edge 12 - 15",0
Microsoft forgot to hump their vendor prefix (#9572),0
Multifunctional method to get and set values of a collection,0
The value/s can optionally be executed if it's a function,0
Sets many values,0
Sets one value,0
Bulk operations run against the entire set,0
...except when executing function values,0
Gets,0
Strip and collapse whitespace according to HTML spec,0
https://infra.spec.whatwg.org/#strip-and-collapse-ascii-whitespace,0
Initialize a jQuery object,0
A central reference to the root jQuery(document),0
A simple way to check for HTML strings,0
Prioritize #id over <tag> to avoid XSS via location.hash (#9521),0
Strict HTML recognition (#11290: must start with <),0
Shortcut simple #id case for speed,0
"HANDLE: $(""""), $(null), $(undefined), $(false)",0
Method init() accepts an alternate rootjQuery,0
so migrate can support jQuery.sub (gh-2101),0
Handle HTML strings,0
Assume that strings that start and end with <> are HTML and skip the regex check,0
Match html or make sure no context is specified for #id,0
HANDLE: $(html) -> $(array),0
Option to run scripts is true for back-compat,0
Intentionally let the error be thrown if parseHTML is not present,0
"HANDLE: $(html, props)",0
Properties of context are called as methods if possible,0
...and otherwise set as attributes,0
HANDLE: $(#id),0
Inject the element directly into the jQuery object,0
"HANDLE: $(expr, $(...))",0
"HANDLE: $(expr, context)",0
(which is just equivalent to: $(context).find(expr),0
HANDLE: $(DOMElement),0
HANDLE: $(function),0
Shortcut for document ready,0
Execute immediately if ready is not present,0
Give the init function the jQuery prototype for later instantiation,0
Initialize central reference,0
Support: Android <=2.3 only (functionish RegExp),0
Prevent errors from freezing future callback execution (gh-1823),0
Not backwards-compatible as this does not execute sync,0
Is the DOM ready to be used? Set to true once it occurs.,0
A counter to track how many items to wait for before,0
the ready event fires. See #6781,0
Abort if there are pending holds or we're already ready,0
Remember that the DOM is ready,0
"If a normal DOM Ready event fired, decrement, and wait if need be",0
Make jQuery.ready Promise consumable (gh-1778),0
Catch cases where $(document).ready() is called,0
after the browser event has already occurred.,0
Support: IE9-10 only,0
"Older IE sometimes signals ""interactive"" too soon",0
Handle it asynchronously to allow scripts the opportunity to delay ready,0
Use the handy event callback,0
"A fallback to window.onload, that will always work",0
The deferred used on DOM ready,0
Wrap jQuery.readyException in a function so that the lookup,0
happens at the time of error handling instead of callback,0
registration.,0
Is the DOM ready to be used? Set to true once it occurs.,0
A counter to track how many items to wait for before,0
the ready event fires. See #6781,0
Handle when the DOM is ready,0
Abort if there are pending holds or we're already ready,0
Remember that the DOM is ready,0
"If a normal DOM Ready event fired, decrement, and wait if need be",0
"If there are functions bound, to execute",0
The ready event handler and self cleanup method,0
Catch cases where $(document).ready() is called,0
after the browser event has already occurred.,0
Support: IE <=9 - 10 only,0
"Older IE sometimes signals ""interactive"" too soon",0
Handle it asynchronously to allow scripts the opportunity to delay ready,0
Use the handy event callback,0
"A fallback to window.onload, that will always work",0
rsingleTag matches a string consisting of a single HTML element with no attributes,0
and captures the element's name,0
"Register as a named AMD module, since jQuery can be concatenated with other",0
"files that may use define, but not via a proper concatenation script that",0
understands anonymous AMD modules. A named AMD is safest and most robust,0
way to register. Lowercase jquery is used because AMD module names are,0
"derived from file names, and jQuery is normally delivered in a lowercase",0
file name. Do this after creating the global so that if an AMD module wants,0
"to call noConflict to hide this version of jQuery, it will work.",0
"Note that for maximum portability, libraries that are not jQuery should",0
"declare themselves as anonymous modules, and avoid setting a global if an",0
"AMD loader is present. jQuery is a special case. For more information, see",0
https://github.com/jrburke/requirejs/wiki/Updating-existing-libraries#wiki-anon,0
Map over jQuery in case of overwrite,0
Map over the $ in case of overwrite,0
"Expose jQuery and $ identifiers, even in AMD",0
"(#7102#comment:10, https://github.com/jquery/jquery/pull/557)",0
and CommonJS for browser emulators (#13566),0
"Use a property on the element directly when it is not a DOM element,",0
or when there is no matching style property that exists.,0
Passing an empty string as a 3rd parameter to .css will automatically,0
attempt a parseFloat and fallback to a string if the parse fails.,0
"Simple values such as ""10px"" are parsed to Float;",0
"complex values such as ""rotate(1rad)"" are returned as-is.",0
"Empty strings, null, undefined and ""auto"" are converted to 0.",0
Use step hook for back compat.,0
Use cssHook if its there.,0
Use .style if available and use plain properties where available.,0
Support: IE <=9 only,0
Panic based approach to setting things on disconnected nodes,0
Back compat <1.8 extension point,0
Handle most common string cases,0
Handle cases where value is null/undef or number,0
"Treat null/undefined as """"; convert numbers to string",0
"If set returns undefined, fall back to normal setting",0
Support: IE <=10 - 11 only,0
"option.text throws exceptions (#14686, #14858)",0
Strip and collapse whitespace,0
https://html.spec.whatwg.org/#strip-and-collapse-whitespace,0
Loop through all the selected options,0
Support: IE <=9 only,0
IE8-9 doesn't update selected after form reset (#2551),0
Don't return options that are disabled or in a disabled optgroup,0
Get the specific value for the option,0
We don't need an array for one selects,0
Multi-Selects return an array,0
Force browsers to behave consistently when non-matching value is set,0
Radios and checkboxes getter/setter,0
"Don't get/set attributes on text, comment and attribute nodes",0
Fallback to prop when attributes are not supported,0
Attribute hooks are determined by the lowercase version,0
Grab necessary hook if one is defined,0
"Non-existent attributes return null, we normalize to undefined",0
Attribute names can contain non-HTML whitespace characters,0
https://html.spec.whatwg.org/multipage/syntax.html#attributes-2,0
Hooks for boolean attributes,0
Remove boolean attributes when set to false,0
Avoid an infinite loop by temporarily removing this function from the getter,0
Support: Android <=4.3 only,0
"Default value for a checkbox should be ""on""",0
Support: IE <=11 only,0
Must access selectedIndex to make default options select,0
Support: IE <=11 only,0
An input loses its value after becoming a radio,0
Only assign if different to avoid unneeded rendering.,0
This expression is here for better compressibility (see addClass),0
Remove *all* instances,0
Only assign if different to avoid unneeded rendering.,0
Toggle individual class names,0
"Check each className given, space separated list",0
Toggle whole class name,0
Store className if set,0
"If the element has a class name or if we're passed `false`,",0
"then remove the whole classname (if there was one, the above saved it).",0
"Otherwise bring back whatever was previously saved (if anything),",0
falling back to the empty string if nothing was stored.,0
"Don't get/set properties on text, comment and attribute nodes",0
Fix name and attach hooks,0
Support: IE <=9 - 11 only,0
elem.tabIndex doesn't always return the,0
correct value when it hasn't been explicitly set,0
https://web.archive.org/web/20141116233347/http://fluidproject.org/blog/2008/01/09/getting-setting-and-removing-tabindex-values-with-javascript/,0
Use proper attribute retrieval(#12072),0
Support: IE <=11 only,0
Accessing the selectedIndex property,0
forces the browser to respect setting selected,0
on the option,0
The getter ensures a default option is selected,0
when in an optgroup,0
"eslint rule ""no-unused-expressions"" is disabled for this code",0
since it considers such accessions noop,0
If it's a function,0
We assume that it's the callback,0
"Otherwise, build a param string",0
"If we have elements to modify, make the request",0
"If ""type"" variable is undefined, then ""GET"" method will be used.",0
Make value of this field explicit since,0
user can override it through ajaxSetup method,0
Save response for use in complete callback,0
"If a selector was specified, locate the right elements in a dummy div",0
Exclude scripts to avoid IE 'Permission Denied' errors,0
Otherwise use the full result,0
"If the request succeeds, this function gets ""data"", ""status"", ""jqXHR""",0
but they are ignored because response was set above.,0
"If it fails, this function gets ""jqXHR"", ""status"", ""error""",0
Cross-browser xml parsing,0
Support: IE 9 - 11 only,0
IE throws on parseFromString with invalid input.,0
Prevent auto-execution of scripts when no explicit dataType was provided (See gh-2432),0
Install script dataType,0
Handle cache's special case and crossDomain,0
Bind script tag hack transport,0
This transport only deals with cross domain or forced-by-attrs requests,0
Use native DOM manipulation to avoid our domManip AJAX trickery,0
Default jsonp settings,0
"Detect, normalize options and install callbacks for jsonp requests",0
"Handle iff the expected data type is ""jsonp"" or we have a parameter to set",0
"Get callback name, remembering preexisting value associated with it",0
Insert callback into url or form data,0
Use data converter to retrieve json after script execution,0
Force json dataType,0
Install callback,0
Clean-up function (fires after converters),0
If previous value didn't exist - remove it,0
Otherwise restore preexisting value,0
Save back as free,0
Make sure that re-using the options doesn't screw things around,0
Save the callback name for future use,0
Call if it was a function and we have a response,0
Delegate to script,0
"File protocol always yields status code 0, assume 200",0
Support: IE <=9 only,0
#1450: sometimes IE returns 1223 when it should be 204,0
Cross domain only allowed if supported through XMLHttpRequest,0
Apply custom fields if provided,0
Override mime type if needed,0
X-Requested-With header,0
"For cross-domain requests, seeing as conditions for a preflight are",0
"akin to a jigsaw puzzle, we simply never set it to be sure.",0
(it can always be set on a per-request basis or even using ajaxSetup),0
"For same-domain requests, won't change header if already provided.",0
Set headers,0
Callback,0
Support: IE <=9 only,0
"On a manual native abort, IE9 throws",0
errors on any property access that is not readyState,0
"File: protocol always yields status 0; see #8605, #14207",0
Support: IE <=9 only,0
IE9 has no XHR2 but throws on binary (trac-11426),0
"For XHR2 non-text, let the caller handle it (gh-2498)",0
Listen to events,0
Support: IE 9 only,0
Use onreadystatechange to replace onabort,0
to handle uncaught aborts,0
Check readyState before timeout as it changes,0
"Allow onerror to be called first,",0
but that will not handle a native abort,0
"Also, save errorCallback to a variable",0
as xhr.onerror cannot be accessed,0
Create the abort callback,0
Do send the request (this may raise an exception),0
#14683: Only rethrow if this hasn't been notified as an error yet,0
Implement the identical functionality for filter and not,0
Single element,0
"Arraylike of elements (jQuery, arguments, Array)",0
Filtered directly for both simple and complex selectors,0
"If this is a positional/relative selector, check membership in the returned set",0
"so $(""p:first"").is(""p:last"") won't return true for a doc with two ""p"".",0
# sourceMappingURL=sizzle.min.map,0
Local document vars,0
Instance-specific data,0
Instance methods,0
Use a stripped-down indexOf as it's faster than native,0
https://jsperf.com/thor-indexof-vs-for/5,0
Regular expressions,0
http://www.w3.org/TR/css3-selectors/#whitespace,0
http://www.w3.org/TR/CSS21/syndata.html#value-def-identifier,0
Attribute selectors: http://www.w3.org/TR/selectors/#attribute-selectors,0
Operator (capture 2),0
"""Attribute values must be CSS identifiers [capture 5] or strings [capture 3 or capture 4]""",0
"To reduce the number of selectors needing tokenize in the preFilter, prefer arguments:",0
1. quoted (capture 3; capture 4 or capture 5),0
2. simple (capture 6),0
3. anything else (capture 2),0
"Leading and non-escaped trailing whitespace, capturing some non-whitespace characters preceding the latter",0
For use in libraries implementing .is(),0
We use this for POS matching in `select`,0
Easily-parseable/retrievable ID or TAG or CLASS selectors,0
CSS escapes,0
http://www.w3.org/TR/CSS21/syndata.html#escaped-characters,0
NaN means non-codepoint,0
Support: Firefox<24,0
"Workaround erroneous numeric interpretation of +""0x""",1
BMP codepoint,0
Supplemental Plane codepoint (surrogate pair),0
CSS string/identifier serialization,0
https://drafts.csswg.org/cssom/#common-serializing-idioms,0
U+0000 NULL becomes U+FFFD REPLACEMENT CHARACTER,0
Control characters and (dependent upon position) numbers get escaped as code points,0
Other potentially-special ASCII characters get backslash-escaped,0
Used for iframes,0
See setDocument(),0
"Removing the function wrapper causes a ""Permission Denied""",0
error in IE,0
"Optimize for push.apply( _, NodeList )",0
Support: Android<4.0,0
Detect silently failing push.apply,0
Leverage slice if possible,0
Support: IE<9,0
Otherwise append directly,0
Can't trust NodeList.length,0
"nodeType defaults to 9, since context defaults to document",0
Return early from calls with invalid selector or context,0
Try to shortcut find operations (as opposed to filters) in HTML documents,0
"If the selector is sufficiently simple, try using a ""get*By*"" DOM method",0
"(excepting DocumentFragment context, where the methods don't exist)",0
ID selector,0
Document context,0
"Support: IE, Opera, Webkit",0
TODO: identify versions,1
getElementById can match elements by name instead of ID,0
Element context,0
"Support: IE, Opera, Webkit",0
TODO: identify versions,1
getElementById can match elements by name instead of ID,0
Type selector,0
Class selector,0
Take advantage of querySelectorAll,0
Support: IE 8 only,0
Exclude object elements,0
qSA considers elements outside a scoping root when evaluating child or,0
"descendant combinators, which is not what we want.",0
"In such cases, we work around the behavior by prefixing every selector in the",0
list with an ID selector referencing the scope context.,0
Thanks to Andrew Dupont for this technique.,0
"Capture the context ID, setting it first if necessary",0
Prefix every selector in the list,0
Expand context for sibling selectors,0
All others,0
"Use (key + "" "") to avoid collision with native prototype properties (see Issue #157)",0
Only keep the most recent entries,0
Remove from its parent by default,0
release memory in IE,0
Use IE sourceIndex if available on both nodes,0
Check if b follows a,0
Known :disabled false positives: fieldset[disabled] > legend:nth-of-type(n+2) :can-disable,0
Only certain elements can match :enabled or :disabled,0
https://html.spec.whatwg.org/multipage/scripting.html#selector-enabled,0
https://html.spec.whatwg.org/multipage/scripting.html#selector-disabled,0
Check for inherited disabledness on relevant non-disabled elements:,0
* listed form-associated elements in a disabled fieldset,0
https://html.spec.whatwg.org/multipage/forms.html#category-listed,0
https://html.spec.whatwg.org/multipage/forms.html#concept-fe-disabled,0
* option elements in a disabled optgroup,0
https://html.spec.whatwg.org/multipage/forms.html#concept-option-disabled,0
"All such elements have a ""form"" property.",0
Option elements defer to a parent optgroup if present,0
Support: IE 6 - 11,0
Use the isDisabled shortcut property to check for disabled fieldset ancestors,0
"Where there is no isDisabled, check manually",0
Try to winnow out elements that can't be disabled before trusting the disabled property.,0
"Some victims get caught in our net (label, legend, menu, track), but it shouldn't",0
"even exist on them, let alone have a boolean value.",0
Remaining elements are neither :enabled nor :disabled,0
Match elements found at the specified indexes,0
Expose support vars for convenience,0
Support: IE <=8,0
"Assume HTML when documentElement doesn't yet exist, such as inside loading iframes",0
https://bugs.jquery.com/ticket/4833,0
Return early if doc is invalid or already selected,0
Update global variables,0
"Support: IE 9-11, Edge",0
"Accessing iframe documents after unload throws ""permission denied"" errors (jQuery #13936)",0
"Support: IE 11, Edge",0
Support: IE 9 - 10 only,0
Support: IE<8,0
Verify that getAttribute really returns attributes and not properties,0
(excepting IE8 booleans),0
"Check if getElementsByTagName(""*"") returns only elements",0
Support: IE<9,0
Support: IE<10,0
Check if getElementById returns elements by name,0
"The broken getElementById methods don't pick up programmatically-set names,",1
so use a roundabout getElementsByName test,0
ID filter and find,0
Support: IE 6 - 7 only,0
getElementById is not reliable as a find shortcut,0
Verify the id attribute,0
Fall back on getElementsByName,0
Tag,0
DocumentFragment nodes don't have gEBTN,0
"By happy coincidence, a (broken) gEBTN appears on DocumentFragment nodes too",0
Filter out possible comments,0
Class,0
QSA and matchesSelector support,0
matchesSelector(:active) reports false when true (IE9/Opera 11.5),0
qSa(:focus) reports false when true (Chrome 21),0
We allow this because of a bug in IE8/9 that throws an error,0
whenever `document.activeElement` is accessed on an iframe,0
"So, we allow :focus to pass through QSA all the time to avoid the IE error",0
See https://bugs.jquery.com/ticket/13378,0
Build QSA regex,0
Regex strategy adopted from Diego Perini,0
Select is set to empty string on purpose,0
This is to test IE's treatment of not explicitly,0
"setting a boolean content attribute,",0
since its presence should be enough,0
https://bugs.jquery.com/ticket/12359,0
"Support: IE8, Opera 11-12.16",0
Nothing should be selected when empty strings follow ^= or $= or *=,0
"The test attribute must be unknown in Opera but ""safe"" for WinRT",0
https://msdn.microsoft.com/en-us/library/ie/hh465388.aspx#attribute_section,0
Support: IE8,0
"Boolean attributes and ""value"" are not treated correctly",0
"Support: Chrome<29, Android<4.4, Safari<7.0+, iOS<7.0+, PhantomJS<1.9.8+",0
Webkit/Opera - :checked should return selected option elements,0
http://www.w3.org/TR/2011/REC-css3-selectors-20110929/#checked,0
IE8 throws error here and will not see later tests,0
"Support: Safari 8+, iOS 8+",0
https://bugs.webkit.org/show_bug.cgi?id=136851,0
In-page `selector#id sibling-combinator selector` fails,0
Support: Windows 8 Native Apps,0
The type and name attributes are restricted during .innerHTML assignment,0
Support: IE8,0
Enforce case-sensitivity of name attribute,0
FF 3.5 - :enabled/:disabled and hidden elements (hidden elements are still enabled),0
IE8 throws error here and will not see later tests,0
Support: IE9-11+,0
IE's :disabled selector does not pick up the children of disabled fieldsets,0
Opera 10-11 does not throw on post-comma invalid pseudos,0
Check to see if it's possible to do matchesSelector,0
on a disconnected node (IE 9),0
This should fail with an exception,0
"Gecko does not error, returns false instead",0
Element contains another,0
Purposefully self-exclusive,0
"As in, an element does not contain itself",0
Document order sorting,0
Flag for duplicate removal,0
Sort on method existence if only one input has compareDocumentPosition,0
Calculate position if both inputs belong to the same document,0
Otherwise we know they are disconnected,0
Disconnected nodes,0
Choose the first element that is related to our preferred document,0
Maintain original order,0
Exit early if the nodes are identical,0
Parentless nodes are either documents or disconnected,0
"If the nodes are siblings, we can do a quick check",0
Otherwise we need full lists of their ancestors for comparison,0
Walk down the tree looking for a discrepancy,0
Do a sibling check if the nodes have a common ancestor,0
Otherwise nodes in our document sort first,0
Set document vars if needed,0
IE 9's matchesSelector returns false on disconnected nodes,0
"As well, disconnected nodes are said to be in a document",0
fragment in IE 9,0
Set document vars if needed,0
Set document vars if needed,0
Don't get fooled by Object.prototype properties (jQuery #13807),0
"Unless we *know* we can detect duplicates, assume their presence",0
Clear input after sorting to release objects,0
See https://github.com/jquery/sizzle/pull/225,0
"If no nodeType, this is expected to be an array",0
Do not traverse comment nodes,0
Use textContent for elements,0
innerText usage removed for consistency of new lines (jQuery #11153),0
Traverse its children,0
Do not include comment or processing instruction nodes,0
Can be adjusted by the user,0
Move the given value to match[3] whether quoted or unquoted,0
nth-* requires argument,0
numeric x and y parameters for Expr.filter.CHILD,0
remember that false/true cast respectively to 0/1,0
other types prohibit arguments,0
Accept quoted arguments as-is,0
Strip excess characters from unquoted arguments,0
Get excess from tokenize (recursively),0
advance to the next closing parenthesis,0
excess is a negative index,0
Return only captures needed by the pseudo filter method (type and argument),0
Shortcut for :nth-*(n),0
:(first|last|only)-(child|of-type),0
Reverse direction for :only-* (if we haven't yet done so),0
non-xml :nth-child(...) stores cache data on `parent`,0
Seek `elem` from a previously-cached index,0
...in a gzip-friendly way,0
Support: IE <9 only,0
Defend against cloned attroperties (jQuery gh-1709),0
Fallback to seeking `elem` from the start,0
"When found, cache indexes on `parent` and break",0
Use previously-cached element index if available,0
...in a gzip-friendly way,0
Support: IE <9 only,0
Defend against cloned attroperties (jQuery gh-1709),0
xml :nth-child(...),0
or :nth-last-child(...) or :nth(-last)?-of-type(...),0
Use the same loop as above to seek `elem` from the start,0
Cache the index of each encountered element,0
Support: IE <9 only,0
Defend against cloned attroperties (jQuery gh-1709),0
"Incorporate the offset, then check against cycle size",0
pseudo-class names are case-insensitive,0
http://www.w3.org/TR/selectors/#pseudo-classes,0
Prioritize by case sensitivity in case custom pseudos are added with uppercase letters,0
Remember that setFilters inherits from pseudos,0
The user may use createPseudo to indicate that,0
arguments are needed to create the filter function,0
just as Sizzle does,0
But maintain support for old signatures,0
Potentially complex pseudos,0
Trim the selector passed to compile,0
to avoid treating leading and trailing,0
spaces as combinators,0
Match elements unmatched by `matcher`,0
Don't keep the element (issue #299),0
"""Whether an element is represented by a :lang() selector",0
is based solely on the element's language value,0
"being equal to the identifier C,",0
"or beginning with the identifier C immediately followed by ""-"".",0
The matching of C against the element's language value is performed case-insensitively.,0
"The identifier C does not have to be a valid language name.""",0
http://www.w3.org/TR/selectors/#lang-pseudo,0
lang value must be a valid identifier,0
Miscellaneous,0
Boolean properties,0
"In CSS3, :checked should return both checked and selected elements",0
http://www.w3.org/TR/2011/REC-css3-selectors-20110929/#checked,0
Accessing this property makes selected-by-default,0
options in Safari work properly,0
Contents,0
http://www.w3.org/TR/selectors/#empty-pseudo,0
":empty is negated by element (1) or content nodes (text: 3; cdata: 4; entity ref: 5),",0
but not by others (comment: 8; processing instruction: 7; etc.),0
nodeType < 6 works because attributes (2) do not appear as children,0
Element/input types,0
Support: IE<8,0
"New HTML5 attribute values (e.g., ""search"") appear with elem.type === ""text""",0
Position-in-collection,0
Add button/input type pseudos,0
Easy API for creating new setFilters,0
Comma and first run,0
Don't consume trailing commas as valid,0
Combinators,0
Cast descendant combinators to space,0
Filters,0
Return the length of the invalid excess,0
if we're just parsing,0
"Otherwise, throw an error or return tokens",0
Cache the tokens,0
Check against closest ancestor/preceding element,0
Check against all ancestor/preceding elements,0
"We can't set arbitrary data on XML nodes, so they don't benefit from combinator caching",0
Support: IE <9 only,0
Defend against cloned attroperties (jQuery gh-1709),0
Assign to newCache so results back-propagate to previous elements,0
Reuse newcache so results back-propagate to previous elements,0
A match means we're done; a fail means we have to keep checking,0
Get initial elements from seed or context,0
"Prefilter to get matcher input, preserving a map for seed-results synchronization",0
"If we have a postFinder, or filtered seed, or non-seed postFilter or preexisting results,",0
...intermediate processing is necessary,0
...otherwise use results directly,0
Find primary matches,0
Apply postFilter,0
Un-match failing elements by moving them back to matcherIn,0
Get the final matcherOut by condensing this intermediate into postFinder contexts,0
Restore matcherIn since elem is not yet a final match,0
Move matched elements from seed to results to keep them synchronized,0
"Add elements to results, through postFinder if defined",0
The foundational matcher ensures that elements are reachable from top-level context(s),0
Avoid hanging onto element (issue #299),0
Return special upon seeing a positional matcher,0
Find the next relative operator (if any) for proper handling,0
"If the preceding token was a descendant combinator, insert an implicit any-element `*`",0
We must always have either seed elements or outermost context,0
Use integer dirruns iff this is the outermost matcher,0
Add elements passing elementMatchers directly to results,0
"Support: IE<9, Safari",0
"Tolerate NodeList properties (IE: ""length""; Safari: <number>) matching elements by id",0
Track unmatched elements for set filters,0
They will have gone through all possible matchers,0
"Lengthen the array for every element, matched or not",0
"`i` is now the count of elements visited above, and adding it to `matchedCount`",0
makes the latter nonnegative.,0
Apply set filters to unmatched elements,0
"NOTE: This can be skipped if there are no unmatched elements (i.e., `matchedCount`",0
"equals `i`), unless we didn't visit _any_ elements in the above loop because we have",0
no element matchers and no seed.,0
"Incrementing an initially-string ""0"" `i` allows `i` to remain a string only in that",0
"case, which will result in a ""00"" `matchedCount` that differs from `i` but is also",0
numerically zero.,0
Reintegrate element matches to eliminate the need for sorting,0
Discard index placeholder values to get only actual matches,0
Add matches to results,0
Seedless set matches succeeding multiple successful matchers stipulate sorting,0
Override manipulation of globals by nested matchers,0
Generate a function of recursive functions that can be used to check each element,0
Cache the compiled function,0
Save selector and tokenization,0
Try to minimize operations if there is only one selector in the list and no seed,0
(the latter of which guarantees us context),0
Reduce context if the leading compound selector is an ID,0
"Precompiled matchers will still verify ancestry, so step up a level",0
Fetch a seed set for right-to-left matching,0
Abort if we hit a combinator,0
"Search, expanding context for leading sibling combinators",0
"If seed is empty or no tokens remain, we can return early",0
Compile and execute a filtering function if one is not provided,0
Provide `match` to avoid retokenization if we modified the selector above,0
One-time assignments,0
Sort stability,0
Support: Chrome 14-35+,0
Always assume duplicates if they aren't passed to the comparison function,0
Initialize against the default document,0
Support: Webkit<537.32 - Safari 6.0.3/Chrome 25 (fixed in Chrome 27),0
Detached nodes confoundingly follow *each other*,0
"Should return 1, but returns 4 (following)",0
Support: IE<8,0
"Prevent attribute/property ""interpolation""",0
https://msdn.microsoft.com/en-us/library/ms536429%28VS.85%29.aspx,0
Support: IE<9,0
"Use defaultValue in place of getAttribute(""value"")",0
Support: IE<9,0
Use getAttributeNode to fetch booleans when getAttribute lies,0
EXPOSE,0
Sizzle requires that there be a global window in Common-JS like environments,0
EXPOSE,0
# sourceMappingURL=noty.min.js.map,0
Trim the opening space.,0
Replace the class name.,0
Trim the opening and closing spaces.,0
Opera 12.10 and Firefox 18 and later support,0
fix for Chrome < 45,0
"If len is 2, that means that we need to schedule an async flush.",0
"If additional callbacks are queued before the queue is flushed, they",0
will be processed by this flush that we are scheduling.,0
test for web worker but not in IE10,0
node,0
node version 0.10.x displays a deprecation warning when nextTick is used recursively,0
see https://github.com/cujojs/when/issues/410 for details,0
vertx,0
web worker,0
Store setTimeout reference so es6-promise will be unaffected by,0
other code modifying setTimeout (like sinon.useFakeTimers()),0
Decide what async method to use to triggering processing of queued callbacks:,0
value === 1,0
value === 1,0
noop,0
"The array here would be [ 1, 2, 3 ];",0
Code here never runs because there are rejected promises!,0
"error.message === ""2""",0
result === 'promise 2' because it was resolved before promise1,0
was resolved.,0
Code here never runs,0
reason.message === 'promise 2' because promise 2 became rejected before,0
promise 1 became fulfilled,0
Code here doesn't run because the promise is rejected!,0
reason.message === 'WHOOPS',0
Code here doesn't run because the promise is rejected!,0
reason.message === 'WHOOPS',0
on success,0
on failure,0
on fulfillment,0
on rejection,0
on fulfillment,0
on rejection,0
user is available,0
"user is unavailable, and you are given the reason why",0
"If `findUser` fulfilled, `userName` will be the user's name, otherwise it",0
will be `'default name'`,0
never reached,0
"if `findUser` fulfilled, `reason` will be 'Found user, but still unhappy'.",0
"If `findUser` rejected, `reason` will be '`findUser` rejected and we're unhappy'.",0
never reached,0
never reached,0
The `PedgagocialException` is propagated all the way down to here,0
The user's comments are now available,0
"If `findCommentsByAuthor` fulfills, we'll have the value here",0
"If `findCommentsByAuthor` rejects, we'll have the reason here",0
success,0
failure,0
failure,0
success,0
success,0
failure,0
success,0
failure,0
failure,0
success,0
found books,0
something went wrong,0
synchronous,0
something went wrong,0
async with promises,0
something went wrong,0
silently ignored,0
Strange compat..,0
# sourceMappingURL=es6-promise.map,0
removed by extract-text-webpack-plugin,0
bind button events if any,0
ugly fix for progressbar display bug,1
it's in the queue,0
API functions,0
Document visibility change controller,0
shim for using process in browser,0
cached from whatever global is present so that test runners that stub it,0
don't break things.  But we need to wrap it in a try catch in case it is,0
wrapped in strict mode code which doesn't define any globals.  It's inside a,0
function because try/catches deoptimize in certain engines.,0
normal enviroments in sane situations,0
if setTimeout wasn't available but was latter defined,0
when when somebody has screwed with setTimeout but no I.E. maddness,0
When we are in I.E. but the script has been evaled so I.E. doesn't trust the global object when called normally,0
"same as above but when it's a version of I.E. that must have the global object for 'this', hopfully our context correct otherwise it will throw a global error",0
normal enviroments in sane situations,0
if clearTimeout wasn't available but was latter defined,0
when when somebody has screwed with setTimeout but no I.E. maddness,0
When we are in I.E. but the script has been evaled so I.E. doesn't  trust the global object when called normally,0
"same as above but when it's a version of I.E. that must have the global object for 'this', hopfully our context correct otherwise it will throw a global error.",0
Some versions of I.E. have different rules for clearTimeout vs setTimeout,0
v8 likes predictible objects,0
This works in non-strict mode,0
This works if eval is allowed (see CSP),0
This works if the window reference is available,0
"g can still be undefined, but nothing to do about it...",0
"We return undefined, instead of nothing here, so it's",0
easier to handle this case. if(!global) { ...},0
# sourceMappingURL=noty.js.map,0
Trim the opening space.,0
Replace the class name.,0
Trim the opening and closing spaces.,0
Opera 12.10 and Firefox 18 and later support,0
fix for Chrome < 45,0
bind button events if any,0
ugly fix for progressbar display bug,1
it's in the queue,0
API functions,0
Document visibility change controller,0
# sourceMappingURL=bootstrap.bundle.min.js.map,0
# sourceMappingURL=bootstrap.min.js.map,0
eslint-disable-next-line no-bitwise,0
TODO: Remove in v5,1
Public,0
Public,0
Public,0
Don't call next when the page isn't visible,0
or the carousel or its parent isn't visible,0
"If it's a touch-enabled device, mouseenter/leave are fired as",0
part of the mouse compatibility events on first tap - the carousel,0
would stop cycling until user tapped out of it;,0
"here, we listen for touchend, explicitly pause the carousel",0
"(as if it's the second time we tap on it, mouseenter compat event",0
is NOT fired) and after a timeout (to allow for mouse compatibility,0
events to fire) we explicitly restart cycling,0
"Some weirdness is happening, so we bail",0
Public,0
preventDefault only for <a> elements (which change the URL) not inside the collapsible element,0
NOTE: 1 DOM access here,0
"Return body, `getScroll` will take care to get the correct `scrollTop` from it",0
Firefox want us to check `-x` and `-y` variations as well,0
NOTE: 1 DOM access here,0
Skip hidden elements which don't have an offsetParent,0
.offsetParent will return the closest TD or TABLE in case,0
"no offsetParent is present, I hate this job...",0
This check is needed to avoid errors in case one of the elements isn't defined for any reason,0
"Here we make sure to give as ""start"" the element that comes first in the DOM",0
Get common ancestor container,0
Both nodes are inside #document,0
"one of the nodes is inside shadowDOM, find which one",0
"IE10 10 FIX: Please, don't ask, the element isn't",0
considered in DOM in some circumstances...,0
This isn't reproducible in IE10 compatibility mode of IE11,0
subtract scrollbar size from sizes,0
"if an hypothetical scrollbar is detected, we must be sure it's not a `border`",0
we make this check conditional for performance reasons,0
"In cases where the parent is fixed, we must ignore negative scroll in offset calc",0
Subtract margins of documentElement in case it's being used as parent,0
we do this only on HTML because it's the only element that behaves,0
differently when margins are applied to it. The margins are included in,0
"the box of the documentElement, in the other cases not.",0
Attach marginTop and marginLeft because in some circumstances we may need them,0
This check is needed to avoid errors in case one of the elements isn't defined for any reason,0
NOTE: 1 DOM access here,0
Handle viewport case,0
Handle other cases based on DOM element used as boundaries,0
"In case of HTML, we need a different computation",0
"for all the other DOM elements, this one is good",0
Add paddings,0
Get popper node sizes,0
"Add position, width and height to our offsets object",0
depending by the popper placement we have to compute its offsets slightly differently,0
use native find if supported,0
use `filter` to obtain the same behavior of `find`,0
use native findIndex if supported,0
use `find` + `indexOf` if `findIndex` isn't supported,0
eslint-disable-line dot-notation,0
Add properties to offsets to make them a complete clientRect object,0
we do this before each modifier to make sure the previous one doesn't,0
mess with these values,0
"if popper is destroyed, don't perform any further update",0
compute reference element offsets,0
"compute auto placement, store placement inside the data object,",0
modifiers will be able to edit `placement` if needed,0
and refer to originalPlacement to know the original value,0
store the computed placement inside `originalPlacement`,0
compute the popper offsets,0
run the modifiers,0
the first `update` will call `onCreate` callback,0
the other ones will call `onUpdate` callback,0
touch DOM only if `applyStyle` modifier is enabled,0
remove the popper if user explicity asked for the deletion on destroy,0
do not use `remove` because IE11 doesn't support it,0
Resize event listener on window,0
Scroll event listener on scroll parents,0
Remove resize event listener on window,0
Remove scroll event listener on scroll parents,0
Reset state,0
add unit if the value is numeric and is one of the following,0
"any property present in `data.styles` will be applied to the popper,",0
in this way we can make the 3rd party modifiers add custom styles to it,0
"Be aware, modifiers could override the properties defined in the previous",0
lines of this modifier!,0
"any property present in `data.attributes` will be applied to the popper,",0
they will be set as HTML attributes of the element,0
if arrowElement is defined and arrowStyles has some properties,0
compute reference element offsets,0
"compute auto placement, store placement inside the data object,",0
modifiers will be able to edit `placement` if needed,0
and refer to originalPlacement to know the original value,0
Apply `position` to popper before anything else because,0
without the position applied we can't guarantee correct computations,0
Remove this legacy support in Popper.js v2,0
Styles,0
Avoid blurry text by using full pixel integers.,0
"For pixel-perfect positioning, top/bottom prefers rounded",0
"values, while left/right prefers floored values.",0
"if gpuAcceleration is set to `true` and transform is supported,",0
we use `translate3d` to apply the position to the popper we,0
automatically use the supported prefixed version if needed,0
"now, let's make a step back and look at this code closely (wtf?)",1
"If the content of the popper grows once it's been positioned, it",0
may happen that the popper gets misplaced because of the new content,0
overflowing its reference element,0
"To avoid this problem, we provide two options (x and y), which allow",0
the consumer to define the offset origin.,0
"If we position a popper on top of a reference element, we can set",0
`x` to `top` to make the popper grow towards its top instead of,0
its bottom.,0
"othwerise, we use the standard `top`, `left`, `bottom` and `right` properties",0
Attributes,0
"Update `data` attributes, styles and arrowStyles",0
arrow depends on keepTogether in order to work,0
"if arrowElement is a string, suppose it's a CSS selector",0
"if arrowElement is not found, don't run the modifier",0
if the arrowElement isn't a query selector we must check that the,0
provided DOM node is child of its popper node,0
,0
extends keepTogether behavior making sure the popper and its,0
reference have enough pixels in conjuction,0
,0
top/left side,0
bottom/right side,0
compute center of the popper,0
Compute the sideValue using the updated popper offsets,0
take popper margin in account because we don't have this info available,0
prevent arrowElement from being placed not contiguously to its popper,0
Get rid of `auto` `auto-start` and `auto-end`,0
"if `inner` modifier is enabled, we can't use the `flip` modifier",0
"seems like flip is trying to loop, probably there's not enough space on any of the flippable sides",0
using floor because the reference offsets may contain decimals we are not going to consider here,0
flip the variation if required,0
this boolean to detect any flip loop,0
"this object contains `position`, we want to preserve it along with",0
any additional property we may add in the future,0
separate value from unit,0
"If it's not a number it's an operator, I guess",0
"if is a vh or vw, we calculate the size based on the viewport",0
"if is an explicit pixel unit, we get rid of the unit and keep the value",0
"if is an implicit unit, it's px, and we return just the value",0
Use height if placement is left or right and index is 0 otherwise use width,0
in this way the first offset will use an axis and the second one,0
will use the other one,0
Split the offset string to obtain a list of values and operands,0
"The regex addresses values with the plus or minus sign in front (+10, -20, etc)",0
Detect if the offset string contains a pair of values or a single one,0
they could be separated by comma or space,0
"If divider is found, we divide the list of values and operands to divide",0
them by ofset X and Y.,0
Convert the values with units to absolute pixels to allow our computations,0
Most of the units rely on the orientation of the popper,0
This aggregates any `+` or `-` sign that aren't considered operators,0
"e.g.: 10 + +5 => [10, +, +5]",0
Here we convert the string values into number values (in px),0
Loop trough the offsets arrays and execute the operations,0
"If offsetParent is the reference element, we really want to",0
go one step up and use the next offsetParent as reference to,0
avoid to make this modifier completely useless and look like broken,0
NOTE: DOM access here,0
resets the popper's position so that the document size can be calculated excluding,0
the size of the popper element itself,0
NOTE: DOM access here,0
restores the original style properties after the offsets have been computed,0
"if shift shiftvariation is specified, run the modifier",0
Avoid unnecessary DOM access if visibility hasn't changed,0
Avoid unnecessary DOM access if visibility hasn't changed,0
Utils,0
Methods,0
"make update() debounced, so that it only runs at most once-per-tick",0
with {} we create a new object with the options inside it,0
init state,0
get reference and popper elements (allow jQuery wrappers),0
Deep merge modifiers options,0
Refactoring modifiers' list (Object => Array),0
sort the modifiers by order,0
modifiers have the ability to execute arbitrary code when Popper.js get inited,0
such code is executed in the same order of its modifier,0
they could add new properties to their options configuration,0
BE AWARE: don't add options to `options.modifiers.name` but to `modifierOptions`!,0
fire the first update to position the popper in the right place,0
"setup event listeners, they will take care of update the position in specific situations",0
We can't use class properties because they don't get listed in the,0
class prototype and break stuff like Sinon stubs,0
Public,0
"to allow the menu to ""escape"" the scroll parent's boundaries",0
https://github.com/twbs/bootstrap/issues/24251,0
empty mouseover listeners to the body's immediate children;,0
only needed because of broken event delegation on iOS,0
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html,0
empty mouseover listeners we added for iOS support,0
If not input/textarea:,0
- And not a key in REGEXP_KEYDOWN => not a dropdown command,0
If input/textarea:,0
- If space key => not a dropdown command,0
- If key is other than escape,0
- If key is not up or down => not a dropdown command,0
- If trigger inside the menu => not a dropdown command,0
Up,0
Down,0
Public,0
Don't move modal's DOM position,0
the following methods are used to handle overflowing modals,0
todo (fat): these should probably be refactored out of modal.js,1
----------------------------------------------------------------------,0
Note: DOMNode.style.paddingRight returns the actual value or '' if not set,0
while $(DOMNode).css('padding-right') returns the calculated value or 0 if not set,0
Restore fixed content padding,0
thx d.walsh,0
Only register focus restorer if modal will actually get shown,0
Public,0
empty mouseover listeners to the body's immediate children;,0
only needed because of broken event delegation on iOS,0
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html,0
empty mouseover listeners we added for iOS support,0
Content is a DOM node or a jQuery,0
Overrides,0
Getters,0
Public,0
TODO (fat): remove sketch reliance on jQuery position/offset,1
Set triggered link as active,0
With both <ul> and <nav> markup a parent is the previous sibling of any nav ancestor,0
Public,0
# sourceMappingURL=bootstrap.bundle.js.map,0
eslint-disable-next-line no-bitwise,0
TODO: Remove in v5,1
Public,0
Public,0
Public,0
Don't call next when the page isn't visible,0
or the carousel or its parent isn't visible,0
"If it's a touch-enabled device, mouseenter/leave are fired as",0
part of the mouse compatibility events on first tap - the carousel,0
would stop cycling until user tapped out of it;,0
"here, we listen for touchend, explicitly pause the carousel",0
"(as if it's the second time we tap on it, mouseenter compat event",0
is NOT fired) and after a timeout (to allow for mouse compatibility,0
events to fire) we explicitly restart cycling,0
"Some weirdness is happening, so we bail",0
Public,0
preventDefault only for <a> elements (which change the URL) not inside the collapsible element,0
Public,0
"to allow the menu to ""escape"" the scroll parent's boundaries",0
https://github.com/twbs/bootstrap/issues/24251,0
empty mouseover listeners to the body's immediate children;,0
only needed because of broken event delegation on iOS,0
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html,0
empty mouseover listeners we added for iOS support,0
If not input/textarea:,0
- And not a key in REGEXP_KEYDOWN => not a dropdown command,0
If input/textarea:,0
- If space key => not a dropdown command,0
- If key is other than escape,0
- If key is not up or down => not a dropdown command,0
- If trigger inside the menu => not a dropdown command,0
Up,0
Down,0
Public,0
Don't move modal's DOM position,0
the following methods are used to handle overflowing modals,0
todo (fat): these should probably be refactored out of modal.js,1
----------------------------------------------------------------------,0
Note: DOMNode.style.paddingRight returns the actual value or '' if not set,0
while $(DOMNode).css('padding-right') returns the calculated value or 0 if not set,0
Restore fixed content padding,0
thx d.walsh,0
Only register focus restorer if modal will actually get shown,0
Public,0
empty mouseover listeners to the body's immediate children;,0
only needed because of broken event delegation on iOS,0
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html,0
empty mouseover listeners we added for iOS support,0
Content is a DOM node or a jQuery,0
Overrides,0
Getters,0
Public,0
TODO (fat): remove sketch reliance on jQuery position/offset,1
Set triggered link as active,0
With both <ul> and <nav> markup a parent is the previous sibling of any nav ancestor,0
Public,0
# sourceMappingURL=bootstrap.js.map,0
Public,0
empty mouseover listeners to the body's immediate children;,0
only needed because of broken event delegation on iOS,0
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html,0
empty mouseover listeners we added for iOS support,0
Content is a DOM node or a jQuery,0
# sourceMappingURL=tooltip.js.map,0
Public,0
preventDefault only for <a> elements (which change the URL) not inside the collapsible element,0
# sourceMappingURL=collapse.js.map,0
Public,0
Don't move modal's DOM position,0
the following methods are used to handle overflowing modals,0
todo (fat): these should probably be refactored out of modal.js,1
----------------------------------------------------------------------,0
Note: DOMNode.style.paddingRight returns the actual value or '' if not set,0
while $(DOMNode).css('padding-right') returns the calculated value or 0 if not set,0
Restore fixed content padding,0
thx d.walsh,0
Only register focus restorer if modal will actually get shown,0
# sourceMappingURL=modal.js.map,0
Overrides,0
Getters,0
# sourceMappingURL=popover.js.map,0
Public,0
# sourceMappingURL=tab.js.map,0
Public,0
"to allow the menu to ""escape"" the scroll parent's boundaries",0
https://github.com/twbs/bootstrap/issues/24251,0
empty mouseover listeners to the body's immediate children;,0
only needed because of broken event delegation on iOS,0
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html,0
empty mouseover listeners we added for iOS support,0
If not input/textarea:,0
- And not a key in REGEXP_KEYDOWN => not a dropdown command,0
If input/textarea:,0
- If space key => not a dropdown command,0
- If key is other than escape,0
- If key is not up or down => not a dropdown command,0
- If trigger inside the menu => not a dropdown command,0
Up,0
Down,0
# sourceMappingURL=dropdown.js.map,0
Public,0
Don't call next when the page isn't visible,0
or the carousel or its parent isn't visible,0
"If it's a touch-enabled device, mouseenter/leave are fired as",0
part of the mouse compatibility events on first tap - the carousel,0
would stop cycling until user tapped out of it;,0
"here, we listen for touchend, explicitly pause the carousel",0
"(as if it's the second time we tap on it, mouseenter compat event",0
is NOT fired) and after a timeout (to allow for mouse compatibility,0
events to fire) we explicitly restart cycling,0
"Some weirdness is happening, so we bail",0
# sourceMappingURL=carousel.js.map,0
Public,0
# sourceMappingURL=alert.js.map,0
# sourceMappingURL=index.js.map,0
Public,0
# sourceMappingURL=button.js.map,0
Public,0
TODO (fat): remove sketch reliance on jQuery position/offset,1
Set triggered link as active,0
With both <ul> and <nav> markup a parent is the previous sibling of any nav ancestor,0
# sourceMappingURL=scrollspy.js.map,0
eslint-disable-next-line no-bitwise,0
TODO: Remove in v5,1
# sourceMappingURL=util.js.map,0
private,0
Protected,0
Getters,0
Public,0
If this is a touch-enabled device we add extra,0
empty mouseover listeners to the body's immediate children;,0
only needed because of broken event delegation on iOS,0
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html,0
If this is a touch-enabled device we remove the extra,0
empty mouseover listeners we added for iOS support,0
Protected,0
Content is a DOM node or a jQuery,0
Private,0
Static,0
Getters,0
Public,0
Private,0
It's a jQuery object,0
Static,0
preventDefault only for <a> elements (which change the URL) not inside the collapsible element,0
Getters,0
Public,0
Private,0
Don't move modal's DOM position,0
----------------------------------------------------------------------,0
the following methods are used to handle overflowing modals,0
todo (fat): these should probably be refactored out of modal.js,1
----------------------------------------------------------------------,0
Note: DOMNode.style.paddingRight returns the actual value or '' if not set,0
while $(DOMNode).css('padding-right') returns the calculated value or 0 if not set,0
Adjust fixed content padding,0
Adjust sticky content margin,0
Adjust body padding,0
Restore fixed content padding,0
Restore sticky content,0
Restore body padding,0
Static,0
Only register focus restorer if modal will actually get shown,0
Getters,0
Overrides,0
We use append for html objects to maintain js events,0
Private,0
Static,0
Getters,0
Public,0
Private,0
Static,0
Getters,0
Public,0
Disable totally Popper.js for Dropdown in Navbar,0
Check if it's jQuery element,0
"If boundary is not `scrollParent`, then set position to `static`",0
"to allow the menu to ""escape"" the scroll parent's boundaries",0
https://github.com/twbs/bootstrap/issues/24251,0
If this is a touch-enabled device we add extra,0
empty mouseover listeners to the body's immediate children;,0
only needed because of broken event delegation on iOS,0
https://www.quirksmode.org/blog/archives/2014/02/mouse_event_bub.html,0
Private,0
Handle dropup,0
Disable Popper.js if we have a static display,0
Static,0
If this is a touch-enabled device we remove the extra,0
empty mouseover listeners we added for iOS support,0
eslint-disable-next-line complexity,0
If not input/textarea:,0
- And not a key in REGEXP_KEYDOWN => not a dropdown command,0
If input/textarea:,0
- If space key => not a dropdown command,0
- If key is other than escape,0
- If key is not up or down => not a dropdown command,0
- If trigger inside the menu => not a dropdown command,0
Getters,0
Public,0
Don't call next when the page isn't visible,0
or the carousel or its parent isn't visible,0
Private,0
"If it's a touch-enabled device, mouseenter/leave are fired as",0
part of the mouse compatibility events on first tap - the carousel,0
would stop cycling until user tapped out of it;,0
"here, we listen for touchend, explicitly pause the carousel",0
"(as if it's the second time we tap on it, mouseenter compat event",0
is NOT fired) and after a timeout (to allow for mouse compatibility,0
events to fire) we explicitly restart cycling,0
"Some weirdness is happening, so we bail",0
Static,0
Getters,0
Public,0
Private,0
Static,0
Getters,0
Public,0
Static,0
Getters,0
Public,0
TODO (fat): remove sketch reliance on jQuery position/offset,1
Private,0
eslint-disable-next-line arrow-body-style,0
Set triggered link as active,0
Set triggered links parents as active,0
With both <ul> and <nav> markup a parent is the previous sibling of any nav ancestor,0
Handle special case when .nav-link is inside .nav-item,0
Static,0
Shoutout AngusCroll (https://goo.gl/pxwQGp),0
eslint-disable-next-line no-bitwise,0
Get transition-duration of the element,0
Return 0 if element or transition duration is not found,0
"If multiple durations are defined, take the first",0
TODO: Remove in v5,1
